although relevance feedback technique have been investigated for more than year hardly any of these technique ha been implemented in a commercial full text document retrieval system in addition to pure performance problem this is due to the fact that the application of relevance feedback technique increase the complexity of the user interface and thus also the use of a document retrieval system in this paper we concentrate on a relevance feedback technique that allows easily understandable and manageable user interface and at the same time provides high quality retrieval result moreover the relevance feedback technique introduced unifies a well a improves other well known relevance feedback technique 
researcher have found relevance feedback to be effective in interactive information retrieval although few formal user experiment have been made in order to run a user experiment on a large document collection experiment were performed at nist to complete some of the missing link found in using the probabilistic retrieval model these experiment using the cranfield collection showed the importance of query expansion in addition to query reweighting and showed that adding a few a well selected term could result in performance improvement of over additionally it wa shown that performing multiple iteration of feedback is highly effective 
example based programming is a form of software reuse in which existing code example are modified to meet current task need example based programming system that have enough exampies to be useful present the problem of finding relevant example a prototype system named codefinder which explores issue of retrieving software object relevant to the design task is presented codefinder support human computer dialogue by providing the mean to incrementally construct a query and by providing associative cue that are compatible with human memory retrieval principle 
a well constructed thesaurus ha long been recognized a a valuable tool in the effective operation of an information retrieval system this paper report the result of experiment designed to determine the validity of an approach to the automatic construction of global thesaurus described originally by crouch in and based on a clustering of the document collection the author validate the approach by showing that the use of thesaurus generated by this method result in substantial improvement in retrieval effectiveness in four test collection the term discrimination value theory used in the thesaurus generation algorithm to determine a term s membership in a particular thesaurus class is found not to be useful in distinguishing a good from an indifferent or poor thesaurus class in conclusion the author suggest an alternate approach to automatic thesaurus construction which greatly simplifies the work of producing viable thesaurus class experimental result show that the alternate approach described herein in some case produce thesaurus which are comparable in retrieval effectiveness to those produced by the first method at much lower cost 
although relevance feedback technique have been investigated for more than year hardly any of these technique ha been implemented in a commercial full text document retrieval system in addition to pure performance problem this is due to the fact that the application of relevance feedback technique increase the complexity of the user interface and thus also the use of a document retrieval system in this paper we concentrate on a relevance feedback technique that allows easily understandable and manageable user interface and at the same time provides high quality retrieval result moreover the relevance feedback technique introduced unifies a well a improves other well known relevance feedback technique 
researcher have found relevance feedback to be effective in interactive information retrieval although few formal user experiment have been made in order to run a user experiment on a large document collection experiment were performed at nist to complete some of the missing link found in using the probabilistic retrieval model these experiment using the cranfield collection showed the importance of query expansion in addition to query reweighting and showed that adding a few a well selected term could result in performance improvement of over additionally it wa shown that performing multiple iteration of feedback is highly effective 
example based programming is a form of software reuse in which existing code example are modified to meet current task need example based programming system that have enough exampies to be useful present the problem of finding relevant example a prototype system named codefinder which explores issue of retrieving software object relevant to the design task is presented codefinder support human computer dialogue by providing the mean to incrementally construct a query and by providing associative cue that are compatible with human memory retrieval principle 
a well constructed thesaurus ha long been recognized a a valuable tool in the effective operation of an information retrieval system this paper report the result of experiment designed to determine the validity of an approach to the automatic construction of global thesaurus described originally by crouch in and based on a clustering of the document collection the author validate the approach by showing that the use of thesaurus generated by this method result in substantial improvement in retrieval effectiveness in four test collection the term discrimination value theory used in the thesaurus generation algorithm to determine a term s membership in a particular thesaurus class is found not to be useful in distinguishing a good from an indifferent or poor thesaurus class in conclusion the author suggest an alternate approach to automatic thesaurus construction which greatly simplifies the work of producing viable thesaurus class experimental result show that the alternate approach described herein in some case produce thesaurus which are comparable in retrieval effectiveness to those produced by the first method at much lower cost 
this paper is a report of a study investigating the validity of the multiple poisson np model of word distribution in document collection an np distribution is a mixture of n poisson distribution with different mean we describe a practical algorithm for determining if a certain word is distributed acording to an np distribution and computing the distribution parameter the algorithm wa applied to every word in four different document collection it wa found that over of frequently occurring word and term indeed behave according to the np distribution the result indicate that the proportion of np word depends on the collection size document length and the frequency of the individual word most of the np word recognised are distributed according to the mixture of relatively few single poisson distribution two three or four there is an indication that the number of single poisson component in the mixture of relatively few single poisson distribution two three or four there is an indication that the number of single poisson component in the mixture depends on the collection frequency of word 
evaluation of information retrieval system should be based on measure of the information provided by the retrieval process informativeness measure which take into account the interactive and full text nature of present day system and the different type of question which are asked of them desirable property for an informativeness measure are developed including context sensitivity user centrality and logarithmic response a hypergraph based framework for measuring the informativeness of a retrieval process is presented and a measure developed which satisfies the desired property the measure is compared to previously developed information measure and illustrated via an application 
we describe a method for classifying news story using memory based reasoning mbr a k nearest neighbor method that doe not require manual topic definition using an already coded training database of about story from the dow jones press release news wire and seeker stanfill a text retrieval system that support relevance feedback a the underlying match engine code are assigned to new unseen story with a recall of about and precision of about there are about different code to be assigned using a massively parallel supercomputer we leverage the information already contained in the thousand of coded story and are able to code a story in about second given seeker the text retrieval system we achieved these result in about two person month we believe this approach is effective in reducing the development time to implement classification system involving large number of topic for the purpose of classification message routing etc 
syntactic phrase indexing and term clustering have been widely explored a text representation technique for text retrieval in this paper we study the property of phrasal and clustered indexing language on a text categorization task enabling u to study their property in isolation from query interpretation issue we show that optimal effectiveness occurs when using only a small proportion of the indexing term available and that effectiveness peak at a higher feature set size and lower effectiveness level for a syntactic phrase indexing than for word based indexing we also present result suggesting that traditional term clustering method are unlikely to provide significantly improved text representation an improved probabilistic text categorization method is also presented 
this paper discus the application of algorithmic spelling correction technique to the identification of those word in a database of th century english text that are most similar to a query word in modern english the experiment have used n gram matching non phonetic coding and dynamic programming method for spelling correction and have demonstrated that high recall search can be carried out although some of the search are very demanding of computational resource the method are in principle applicable to historical text in many language and from many diffeent period 
the first part of this paper briefly describes a mathematical framework called the containment model that provides the operation and data structure for a text dominated database with a hierarchical structure the database is considered to be a hierarchical collection of continuous extent each extent being a word word phrase text element or non text element the filter operation making up a search command are expressed in term of containment criterion that specify whether a contiguous extent will be selected or rejected during a search this formalism comprised of the mathematical framework and it associated language defines a conceptual layer upon which we can construct a well defined higher level layer specifically the user interface that serf to provide a level of functionality that is closer to the need of the user and the application domain with the conceptual layer established we go on to describe the design and implementation of a versatile interface which handle query that search and navigate a heterogeneous collection of structured document interface functionality is provided by a set of worker module supported by an environment that is the same for all interface the interface environment allows a worker to communicate with the underlying text retrieval engine using a well defined command protocol that is based on a small set of filter operator the overall design emphasizes a interface flexibility for a variety of search and browsing capability b the modular independence of the interface with respect to it underlying retrieval engine and c the advantage to be accrued by defining retrieval command using operator that are part of a text algebra that provides a sound theoretical foundation for the database 
the manuscript submitted for the hypertext conference were assigned to member of the review committee using a variety of automated method based on information retrieval principle and latent semantic indexing fifteen reviewer provided exhaustive rating for the submitted abstract indicating how well each abstract matched their interest the automated method do a fairly good job of assigning relevant paper for review but they are still somewhat poorer than assignment made manually by human expert and substantially poorer than an assignment perfectly matching the reviewer own ranking of the paper a new automated assignment method called n of n achieves better performance than human expert by sending reviewer more paper than they actually have to review and then allowing them to choose part of their review load themselves 
saw the lifting of the security restriction on large number of scientific and technical report which had been written during world war two pre war virtually all publication had been in journal and the report format wa strange and unfamiliar both for the scientific community and for librarian a such they presented new challenge the administrative problem of actually being able to obtain copy of the report wa tackled by setting up new government agency with direct 
cognitive ability of fifty university student were tested using eight test from the kit of factor referenced cognitive test all student searched for reference on the same topic using a standard computerized index and performance in the search wa analyzed using a variety of measure effect for cognitive difference a well a for difference in demographic characteristic and knowledge were identified using multiple regression perceptual speed had an effect on the quality of search and logical reasoning verbal comprehension and spatial scanning ability influenced search tactic it is suggested that information retrieval system can be made more accessible to user with different level of cognitive ability through improvement that will assist user to scan list of term choose appropriate vocabulary for searching and select useful reference 
latent semantic indexing lsi is a technique for representing document query and term a vector in a multidimensional real valued space the representtions are approximation to the original term space encoding and are found using the matrix technique of singular value decomposition in comparison multidimensional scaling md is a class of data analysis technique for representing data point a point in a multidimensional real valued space the object are represented so that inter point similarity in the space match inter object similarity information provided by the researcher we illustrate how the document representation given by lsi are equivalent to the optimal representation found when solving a particular md problem in which the given inter object similarity information is provided by the inner product similarity between the document themselves we further analyze a more general md problem in which the interdocument similarity information although still in inner product form is arbitrary with respect to the vector space encoding of the document 
our previous research on one probe access to large collection of data indexed by alphanumeric key ha produced the first practical minimal perfect hash function for this problem here a new algorithm is described for quickly finding minimal perfect hash function whose specification space is very close to the theoretical lower bound i e around bit per key the various stage of processing are detailed along with analytical and empirical result including timing for a set of over million key that wa processed on a nextstation in about hour 
most natural language based document retrieval system use the syntax structure of constituent phrase of document a index term many of these system also attempt to reduce the syntactic variability of natural language by some normalisation procedure applied to these syntax structure however the retrieval performance of such system remains fairly disappointing some system therefore use a meaning representation language to index and retrieve document in this paper a system is presented that us horn clause logic a meaning representation language employ advanced technique from natural language processing to achieve incremental extensibility and us method from logic programming to achieve robustness in the face of insufficient data 
document clustering ha not been well received a an information retrieval tool objection to it use fall into two main category first that clustering is too slow for large corpus with running time often quadratic in the number of document and second that clustering doe not appreciably improve retrieval we argue that these problem arise only when clustering is used in an attempt to improve conventional search technique however looking at clustering a an information access tool in it own right obviates these objection and provides a powerful new access paradigm we present a document browsing technique that employ document clustering a it primary operation we also present fast linear time clustering algorithm which support this interactive browsing paradigm 
the dominant approach to information retrieval system design are based on rational theory and cognitive engineering however these theory a well a approach in other discipline reviewed in this paper do not account for communication or interaction among design participant which is critical to design outcome this research attempt to develop a descriptive design model that account for communication among user designer and developer throughout the design process a pilot study ha been completed and a preliminary model that represents a first step in understanding participant evolving perception and expectation of the design process and it outcome is described in this paper 
semantic based approach to information retrieval make a query evaluation similar to an inference process based on semantic relation semantic based approach find out hidden semantic relationship between a document and a query but quantitative estimation of the correspondence between them is often empiric on the other hand probabilistic approach usually consider only statistical relationship between term it is expected that improvement may be brought by integrating these two approach this paper demonstrates using some particular probabilistic model which are strongly related to modal logic that such an integration is feasible and natural a new model is developed on the basis of an extended modal logic it ha the advantage of augmenting a semantic based approach with a probabilistic measurement and augmenting a probabilistic approach with finer semantic relation than just statistical one it is shown that this model verifies most of the condition for an absolute probability function 
information retrieval is concerned with selecting document from a collection that will be of interest to a user with a stated information need or query research aimed at improving the performance of retrieval system that is selecting those document most likely to match the user s information need remains an area of considerable theoretical and practical importance this dissertation describes a new formal retrieval model that us probabilistic inference network to represent document 
in this paper we present an approach to the incorporation of object versioning into a distributed full text information retrieval system we propose an implementation based on partially versioned index set arguing that it space overhead and query time performance make it suitable for full text ir with it heavy dependence on inverted indexing we develop algorithm for computing both historical query and time range query and show how these algorithm can be applied to a number of problem in distributed information management such a data replication caching transactional consistency and hybrid medium repository 
in this paper a model for combining text and fact retrieval is described a query is a set of condition where a single condition is either a text or fact condition fact condition can be interpreted a being vague thus leading to nonbinary weight for fact condition with respect to database object for text condition we use description of the occurence of term in document instead of precomputed indexing weight thus treating term similar to attribute probabilistic indexing weight for condition are computed by introducing the notion of correctness or acceptability of a condition w r t an object these indexing weight are used in retrieval for a probabilistic ranking of object based on the retrieval for a probabilistic ranking of object based on the retrieval with probabilistic indexing rpi model for which a new derivation is given here 
we describe work on the visualization of bibliographic data and to aid in this task the application of numerical technique for multidimensional scaling many area of scientific research involve complex multivariate data one example of this is information retrieval document comparison may be done using a large number of variable such condition do not favour the more well known method of visualization and graphical analysis a it is rarely feasible to map each variable onto one aspect of even a three dimensional coloured and textured space bead is a prototype system for the graphically based exploration of information in this system article in a bibliography are represented by particle in space by using physically based modelling technique to take advantage of fast method for the approximation of potential field we represent the relationship between article by their relative spatial position inter particle force tend to make similar article move closer to one another and dissimilar one move apart the result is a d scene which can be used to visualize pattern in the high d information space 
in this paper we describe an automated method of classifying research project description a human expert classifies a sample set of project into a set of disjoint and pre defined class and then the computer learns from this sample how to classify new project into these class both textual and non textual information associated with the project are used in the learning and classification phase textual information is processed by two method of analysis a natural language analysis followed by a statistical analysis non textual information is processed by a symbolic learning technique we present the result of some experiment done on real data two different classification of our research project 
computer program that access significant amount of text usually include code that manipulates the textual object that comprise it such program include electronic mail reader typesetter and in particular full text information retrieval system such code is often unsatisfying in that access to textual object is either efficient or flexible but not both a programming language like awk or perl provides very general facility for describing textual object but at the cost of rescanning the text for every textual object at the other extreme full text information retrieval system usually offer access to a very limited number of kind of textual object but this access is very efficient the system described in this paper is a programming tool for managing textual object it provides a great deal of flexibility giving access to very complex document structure with a large number of constituent kind of textual object further it provides access to these object very efficiently both in term of time and auxiliary space by being very careful to access secondary storage only when absolutely necessary 
full text retrieval system often use either a bitmap or an inverted file to identify which document contain which term so that the document containing any combination of query term can be quickly located bitmap of term occurrence are large but are usually sparse and thus are amenable to a variety of compression technique here we consider technique in which the encoding of each bitvector within the bitmap is parameterised so that a different code can be used for each bitvector our experimental result show that the new method yield better compression than previous technique 
this paper analyzes the property structure and limitation of vector based model for information retrieval from the computational geometry point of view it is shown that both the pseudo cosine and the standard vector space model can be viewed a special case of a generalized linear model more importantly both the necessary and sufficient condition have been identified under which ranking function such a the inner product cosine pseudo cosine dice covariance and product moment correlation measure can be used to rank the document the structure of the solution region for acceptable ranking is analyzed and an algorithm for finding all the solution vector is suggested 
for free text search over rapidly evolving corpus dynamic update of inverted index is a basic requirement b tree are an effective tool in implementing such index the zipfian distribution of posting suggests space and time optimization unique to this task in particular we present two novel optimization merge update which performs better than straight forward block update and pulsing which significantly reduces space requirement without sacrificing performance inverted index 
the retrieval capability of the signature file access method have become very attractive for many data processing application dealing with both formatted and unformatted data however performance is still a problem mainly when large file are used and fast response required in this paper a high performance signature file organization is proposed integrating the latest development both in storage structure and parallel computing architecture it combine horizontal and vertical approach to the signature file fragmentation in this way a new mixed decomposition scheme particularly suitable for parallel implementation is achieved the organization based on this fragmentation scheme is called fragmented signature file performance analysis show that this organization provides very good and relatively stable performance covering the full range of possible query for the same degree of parallelism it outperforms any other parallel signature file organization that ha been defined so far the proposed method also ha other important advantage concerning processing of dynamic file adaptability to the number of available processor load balancing and to some extent fault tolerant query processing 
when dictionary for specific application or subject field are derived from a text collection the frequency distribution of the term in the collection give information about the expected completeness of the dictionary if only a subset of the term in the collection is to be included in the dictionary the completeness of the dictionary can be optimized with respect to dictionary size in this paper formula for the relationship between the frequency distribution of the term in the collection and expected dictionary completeness are derived first we regard one dimensional dictionary where the non trivial term occurring in the text are to be included in the dictionary then we describe the case of two dimensional dictionary which are needed for example for automatic indexing with a controlled vocabulary here relationship between text term and descriptor from the prescribed vocabulary have to be stored in the dictionary for both case formula for the interpolation and extrapolation with respect to different collection size are derived we give experimental result for one dimensional dictionary and show how the completeness can be estimated and optimized 
term clustering and syntactic phrase formation are method for transforming natural language text both have had only mixed success a strategy for improving the quality of text representation for document retrieval since the strength of these method are complementary we have explored combining them to produce superior representation in this paper wc discus our implementation of a syntactic phrase generator a well a our preliminary experiment with producing phrase cluster these experiment show small improvement in retrieval effectiveness resulting from the use of phrase cluster but it is clear that corpus much larger than standard information retrieval test coliections will be required to thoroughly evaluate the use of thin technique 
present seven set of laboratory result testing variable in term position ranking which produce a phrase effect by weighting the distance between proximate term result of the test conducted by this project are included covering variant term position algorithm sentence boundary stopword counting every pair testing field selection and combination of algorithm including collection frequency record frequency and searcher weighted the discussion includes the result of test by fagan and by croft the need for term stemming proximity a a precision device comparison with boolean and the quality of test collection 
we designed implemented and evaluated a new concept for visualizing and searching database utilizing direct manipulation called dynamic query dynamic query allow user to formulate query by adjusting graphical widget such a slider and see the result immediately by providing a graphical visualization of the database and search result user can find trend and exception easily user testing wa done with eighteen undergraduate student who performed significantly faster using a dynamic query interface compared to both a natural language system and paper printout the interface were used to explore a real estate database and find home meeting specific search criterion 
an information retrieval model is presented for the retrieval of speech document i e audio recording containing speech the indexing vocabulary consists of indexing feature that have the following characteristic first they are easy to recognize by speech recognition method second the number of different indexing feature is small such that a reasonable amount of training data is sufficent to train the hidden markov model that are used by the speech recognition process third the retrieval method based on such indexing feature achieves an acceptable retrieval effectiveness a shown by experiment on text collection fourth these indexing feature cannot only be identified in speech document but also in text document from the last characteristic follows that speech document and text document can be retrieved simultaneously analogously the query may contain either speech or text thus we have a simple multimedia retrieval model where two different medias are indexed coherently we also describe a prototype retrieval system under development 
this paper present searching approach and user interface capability of duo an online public access catalogue opac designed to permit the user of three university of the northeast of italy different subject searching access to the co operative multi discipline library catalogue database the co operative catalogue database is managed by one of the software system developed under the italian national project for library automation the sbn project since the sbn database ha not been designed to be efficiently accessed for end user search the duo database ha been designed to avoid duplication of the sbn database data and to be usable for making efficient subject access to the catalogue document the duo design choice are presented in particular the main choice of designing a virtual document that corresponds to each sbn document and that ha unstructured data usable for subject search purpose the paper present a new kind of user opac dialogue that make available to the user different search approach and on line dictionary in particular the user during the interaction with the search tool can represent his information need with the support of interface capability that are based on retrieval path history and word and code on line dictionary duo is the first italian opac that ha been made openly available to user of university and research institution for this reason it is also the first time that opac log data is going to be collected in italy this work mainly intends to make a modern opac available to the user of a sbn catalogue database but it is going to permit also to build up a knowledge on opac usage in italy 
abstract in this paper we propose a method for specifying the functionality of an intelligent interface to large scale information retrieval system erational and for implementing those function in an r 
the goal of a probabilistic retrieval system design is to rank the element of the search universe in descending order of their estimated probability of usefulness to the user previously explored method for computing such a ranking have involved the use of statistical independence assumption and multiple regression analysis on a learning sample in this paper these technique are recombined in a new way to achieve greater accuracy of probabilistic estimate without undue additional computational complexity the novel element of the proposed design is that the regression analysis be carried out in two or more level or stage such an approach allows composite or grouped retrieval clue to be analyzed in an orderly manner first within group and then between it compensates automatically for systematic bias introduced by the statistical simplifying assumption and give rise to search algorithm of reasonable computational efficiency 
one aspect of world knowledge essential to information retrieval is knowing when two word are related knowing word relatedness allows a system given a user s query term to retrieve relevant document not containing those exact term two word can be said to be related if they appear in the same context document co occurrence give a measure of word relatedness that ha proved to be too rough to be useful the relatively recent apparition of on line dictionary and robust and rapid parser permit the extraction of finer word context from large corpus in this paper we will describe such an extraction technique that us only coarse syntactic analysis and no domain knowledge this technique produce list of word related to any work appearing in a corpus when the closest related term were used in query expansion of a standard information retrieval testbed the result were much better than that given by document co occurence technique and slightly better than using unexpanded query supporting the contention that semantically similar word were indeed extracted by this technique 
document management system are needed for many business application this type of system would combine the functionality of a database system for describing storing and maintaining document with complex structure and relationship with a text retrieval system for effective retrieval based on full text the retrieval model for a document management system is complicated by the variety and complexity of the object that are represented in this paper we describe an approach to complex object retrieval using a probabilistic inference net model and an implementation of this approach using a loose coupling of an object oriented database system iris and a text retrieval system based on inference net inquery the resulting system is used to store long structured document and can retrieve document component section figure etc based on their content or the content of related component the lesson learnt from the implementation are discussed 
