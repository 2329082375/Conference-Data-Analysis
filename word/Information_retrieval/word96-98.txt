browser l are now among the most popular software for people to surf web page and retrieve information from internet for their own interest however people usually find the information they search for are scattered in several web page and cannot specify which part of content are interesting and how these part are presented in other word today s browser are weak in personalization to enhance the capability of personalization browser must have more power in managing content of web page in ihis demo we apply natural language processing nlp technique to create a browser that can be personalized personal browser personal browser allows user to record the interesting part of web page a chunk which are then analyzed through nlp technique for future information retrieval user can also bundle these chunk coming from different web page together to suit their preference to update information in these chunk personal browser pull relevant web page filter these web page to produce the desired information according to the recorded chunk and update the information in the original page in personal browser user can organize different information chunk in the page much like composing a personal home page for example a user who is interested in australia singapore and hong kong stock quote find web page that list current price of international stock market the user can copy the text that show the three stock quote to personal browser personal browser then analyzes the html source text according to the marked text and record the result a chunk applying nlp tool to process the html source text should show that this web page is relevant to stock price and therefore the number after a national name such a australia is a price quote which change very often the url marked text and analyzed result are all recorded in a chunk for future information retrieval user can assign a priority to each information chunk in the prioritized retrieval personal browser will pull relevant web page applying information retrieval technique s to match the information chunk with the web page to produce the desired information and present the information in a single page 
ranking based on passage address some of the shortcoming ofwhole document ranking it provides convenient unit of text toreturn to the user avoids the difficulty of comparing documentsof different length and enables identification of short block ofrelevant material amongst otherwise irrelevant text in this paperwe explore the potential of passage retrieval based on anexperimental evaluation of the ability of passage to identifyrelevant document we compare our scheme of arbitrary passageretrieval to several other document retrieval and passage retrievalmethods we show experimentally that compared to these method ranking via fixed length passage is robust and effective ourexperiments also show that compared to whole document ranking ranking via fixed length arbitrary passage significantly improvesretrieval effectiveness by for trec disk and and by for the federal register collection 
this paper describes the application of distributional clustering to document classification this approach cluster word into group based on the distribution of class label associated with each word thus unlike some other unsupervised dimensionalityreduction technique such a latent semantic indexing we are able to compress the feature space much more aggressively while still maintaining high document classification accuracy experimental result obtained on three real world data set show that we can reduce the feature dimensional y by three order of magnitude and lose only accuracy significantly better than latent semantic indexing class based clustering l feature selection by mutual information or markov blanket based feature selection we also show that le aggressive clustering sometimes result in improved classification accuracy over classification without clustering 
browser l are now among the most popular software for people to surf web page and retrieve information from internet for their own interest however people usually find the information they search for are scattered in several web page and cannot specify which part of content are interesting and how these part are presented in other word today s browser are weak in personalization to enhance the capability of personalization browser must have more power in managing content of web page in ihis demo we apply natural language processing nlp technique to create a browser that can be personalized personal browser personal browser allows user to record the interesting part of web page a chunk which are then analyzed through nlp technique for future information retrieval user can also bundle these chunk coming from different web page together to suit their preference to update information in these chunk personal browser pull relevant web page filter these web page to produce the desired information according to the recorded chunk and update the information in the original page in personal browser user can organize different information chunk in the page much like composing a personal home page for example a user who is interested in australia singapore and hong kong stock quote find web page that list current price of international stock market the user can copy the text that show the three stock quote to personal browser personal browser then analyzes the html source text according to the marked text and record the result a chunk applying nlp tool to process the html source text should show that this web page is relevant to stock price and therefore the number after a national name such a australia is a price quote which change very often the url marked text and analyzed result are all recorded in a chunk for future information retrieval user can assign a priority to each information chunk in the prioritized retrieval personal browser will pull relevant web page applying information retrieval technique s to match the information chunk with the web page to produce the desired information and present the information in a single page 
ranking based on passage address some of the shortcoming ofwhole document ranking it provides convenient unit of text toreturn to the user avoids the difficulty of comparing documentsof different length and enables identification of short block ofrelevant material amongst otherwise irrelevant text in this paperwe explore the potential of passage retrieval based on anexperimental evaluation of the ability of passage to identifyrelevant document we compare our scheme of arbitrary passageretrieval to several other document retrieval and passage retrievalmethods we show experimentally that compared to these method ranking via fixed length passage is robust and effective ourexperiments also show that compared to whole document ranking ranking via fixed length arbitrary passage significantly improvesretrieval effectiveness by for trec disk and and by for the federal register collection 
this paper describes the application of distributional clustering to document classification this approach cluster word into group based on the distribution of class label associated with each word thus unlike some other unsupervised dimensionalityreduction technique such a latent semantic indexing we are able to compress the feature space much more aggressively while still maintaining high document classification accuracy experimental result obtained on three real world data set show that we can reduce the feature dimensional y by three order of magnitude and lose only accuracy significantly better than latent semantic indexing class based clustering l feature selection by mutual information or markov blanket based feature selection we also show that le aggressive clustering sometimes result in improved classification accuracy over classification without clustering 
a system to retrieve image using a syntactic description of appearance is presented a multiscaleinvariant vector representation is obtained by first filtering image in the database withgaussian derivative filter at several scale and then computing low order differential invariant the multi scale representation is indexed for rapid retrieval query are designed by the usersfrom an example image by selecting appropriate region the invariant vector corresponding tothese region are 
document length normalization is an important aspect of term weight assignment in an automatic information retrieval system in this study we observe that a normalization scheme that retrieves document of all length with similar chance a their likelihood of relevance will outperform another scheme which retrieves document with chance very different from their likelihood of relevance we show that the retrieval probability for a particular normalization method deviate systematically from the relevance probability across different collection we present pivoted normalization a technique that can be used to reduce the gap between the relevance and the retrieval probability training pivoted normalization on one collection we can successfully use it on other new text collection yielding a robust collection independent normalization technique we use the idea of pivoting with the well known cosine normalization scheme we point out some shortcoming of the cosine normalization function and present two new normalization function pivoted unique normalization and pivoted byte size normalization 
we present a corpus based system to expand multi wordindex term using a part of speech tagger and a full fledgedderivational morphological system combined with a shallowparser the system ha been applied to french the uniquecontribution of the research is in using these linguisticallybased tool with safety filter in order to avoid the problemsof degradation typically associated with derivational analysisand generation the successful expansion and thus conflationof term 
this article is concerned with the design and implementation of information retrieval system irs we show how theory and model from the domain of human computer interaction hci can be applied to the design of irs we first study the user s task by modelling the mental activity of the user while accomplishing a task adopting a system perspective we consider the processing task of an irs and organize them in a design space we then build upon the design space to consider the implication of such data processing and level of abstraction on software design finally we present pac amodeus a software architecture model and illustrate the applicability of the approach with the implementation of an irs the tiapri system 
to properly handle concurrent access to document by update and query in information retrieval ir system effort are on to integrate ir feature with database management system dbms feature however initial research ha revealed that dbms feature optimized for traditional database display degraded performance while handling text database since efficiency is critical in ir system infrastructural extension are necessary for several dbms feature transaction support being one of them this paper focus on developing efficient transaction support for ir system where update and query arrive dynamically by exploiting the data characteristic of the index a well a of the query and update that access the index result of performance test on a prototype system demonstrate the superior performance of our algorithm 
model of document indexing and document retrieval have been extensively studied the integration of these two class of model ha been the goal of several researcher but it is a very difficult problem we argue that much of the reason for this is the lack of an adequate indexing model this suggests that perhaps a better indexing model would help solve the problem however we feel that making unwarranted parametric assumption will not lead to better retrieval performance furthermore making prior assumption about the similarity of document is not warranted either instead we propose an approach to retrieval based on probabilistic language modeling we estimate model for each document individually our approach to modeling is non parametric and integrates document indexing and document retrieval into a single model one advantage of our approach is that collection statistic which are used heuristically in many other retrieval model are an integral part of our model we have implemented our model and tested it empirically our approach significantly outperforms standard tf idf weighting on two different collection and query set 
three representation method are empirically investigated for chinese information retrieval gram single character bigram two contiguous overlapping character and short word indexing based on a simple segmentation of the text the retrieval collection is the approximately mb trec chinese corpus of news article and query that are long and rich in wording evaluation show that gram indexing is good but not sufficiently competitive while bigram indexing work surprisingly well bigram indexing lead to a large index term space three time that of short word indexing but is a good a short word indexing in precision and about better in relevants retrieved the best average non interpolated precision is about better than gram indexing and quite high for a mainly statistical approach 
abstract one of the main hurdle to improved clir ef fectiveness is resolving ambiguity associated with translation availability of resource is also a problem first we present a technique based on co occurrence statistic from unlinked cor pora which can be used to reduce the ambiguity associated with phrasal and term translation we then combine this method with other technique for reducing ambiguity and achieve more than monolingual effectiveness finally we compare the co occurrence method with parallel corpus and machine trans lation technique and show that good retrieval effectiveness can be achieved without complex resource 
partial collection replication improves performance and scalability of a large scale distributed information retrieval system by distributing excessive workload reducing network latency and restricting some search to a small percentage of data in this paper we first examine query from real system log and show that there is sufficient query locality in real system to justify partial collection replication we then present a method for constructing a hierarchy of partial replica from a collection where each replica is a subset of all larger replica and extend the inference network model to rank and select partial replica we compare our new selection algorithm to previous work on collection selection over a range of tuning parameter for a given query our replica selection algorithm correctly determines the most relevant of the replica or original collection and thus maintains the highest retrieval effectiveness while searching the least data a compared with the other ranking function simulation result show that with load balancing partial replication consistently improves performance over collection partitioning on multiple disk of a shared memory multiprocessor and it requires only modest query locality 
the multilingual information retrieval system of the future will need to be able to retrieve document across language boundary this extension of the classical ir problem is particularly challenging ax significant resource are required to perform query translation at xerox we are working to build a multilingual ir system and conducting a series of experiment to understand what factor are most important in making the system work using translated query and a bilingual transfer dictionary we have learned that crosslartguage multilingual ir is feasible although performance lag considerably behind the monolingual standard the experiment suggest that correct identification and translation of multi word terminology is the single most important source of error in the system although amblguit y in translation also contributes to poor performance 
the effect of using paat query to improve automatic query expansion wa examined in the trec environment automatic feedback of document identified from similar past query wa compared with standard top document feedback and with no feedback a new query similarity metric wa used based on comparing result list and using probability of relevance our top document feedback method showed small improvement over no feedback method consistent with past study on recall precision and average precision measure past query feedback yielded performance superior to that of top document feedback the past query feedback method also lends itself to tunable threshold such that better performance can be obtained by automatically deciding when and when not to apply the expansion automatic past query feedback actually improved top document precision in this experiment 
this paper introduces a novel user interface that integrates search and browsing of very large category hierarchy with their associated text collection a key component is the separate but simultaneous display of the representation of the category and the retrieved document another key component is the display of multapfe selected category simultaneously complete with their hierarchical context the prototype implementation us animation and a three dimensional graphical workspace to accommodate the category hierarchy and to store intermediate search result query specification in this d environment is accomplished via a novel method for painting boolean query over a combination of category label and free text example are shown on a collection of medical text 
it ha been known that different representation of a query retrieve different set of document recentwork suggests that significant improvement in retrieval performance can be achieved by combiningmultiple representation of an information need however little effort ha been made to understandthe reason why combining multiple source of evidence improves retrieval effectiveness in this paperwe analyze why improvement can be achieved with evidence combination and investigate how 
three different type of classifier were investigatedin the context of a text categorization problem in the medical domain the automatic assignment of icd code to dictated inpatient discharge summary k nearest neighbor relevance feedback and bayesian independence classifier were applied individually and in combination a coknbination of different classifier produced better result than any single type of classifier for this specific medical categorization problem new query formulation and weighting method used in the k nearest neighbor classifier improved performance 
we report on the design and evaluation of a visualization tool for information retrieval ir system that aim to help the end user in the following respect a an indicator of document relevance the tool graphically provides specific query related information about individual document a a diagnosis tool it graphically provides aggregate information about the query result that could help in identifying how the different query term influence the retrieval and ranking of document two different experiment using trec data were conducted to evaluate the effectiveness of this tool result while mixed indicate that visualization of this sort may provide useful support for judging the relevance of document in particular by enabling user to make more accurate decision about which document to inspect in detail problem in evaluation of such tool in interactive environment are discussed 
clustering is increasing in importance but linearand even constant time clustering algorithm are often too slow for real time application a simple way to speed up clustering is to speed up the distance calculation at the heart of clustering routine we study two technique for improving the cost of distance calculation lsi and truncation and determine both how much these technique speed up clustering and how much they affect the quality of the resulting cluster we find that the 
abstract there is strong empirical and theoretic evidence that combination of retrieval method can improve performance in this paper we systematically compare combination strategy in the context of document filtering using query from the tipster reference corpus we find that simple averaging strategy do indeed improve performance but that direet averaging of probability estimate is not the correet approach instead the probabijit y estimate must be renormalized using logistic regression on the known relevance judgment we examine more complex combination strat gy but find them le successful due to the high correlation among our filtering method which are optimized over the same training data and employ similar document represerttations introduction a text filtering system monitor au incoming document 
system for text retrieval routing categorization and other ir task rely heavily on linear classifier we propose that two machine learning algorithm the widrow hoff and eg algorithm be used in training linear text classifier in contrast to most ir method theoretical analysis provides performance guarantee and guidance on parameter setting for these algorithm experimental data is presented showing widrow hoff and eg to be more effective than the widely used rocchio algorithm on several categorization and routing task 
technique of exploratory data analysis areused to study the weight of evidence that the occurrenceof a query term provides in support of the hypothesisthat a document is relevant to an information need inparticular the relationship between the document frequencyand the weight of evidence is investigated acorrelation between document frequency normalized bycollection size and the mutual information between relevanceand term occurrence is uncovered this correlationis found to be 
abstract 
document ojlen display a structure determined by the author e g several chapter each with several sub chapter and so on taking into account the structure of a document allows the retrieval process to focus on those part of the document that are most relevant to an information need chiaramella et al advanced a model for indexing and retrieving structured document their aim wa to express the model within a framework based on formal logic with associated theory they developed the logical formalism of the model this paper add to this model a theory of uncertainty the dempster shafer theory of evidence it is shown that the theory provides a rule the dempster s combination rule that a low the expression of the uncertainty with respect to part of a document and that is compatible with the iogica model developed by chiaramella et al 
we use data from the trec routing experiment to explore how relevance feedback can be applied incrementally using a few judged document each time to achieve result that are a good a if the feedback occurred in one pas we show that relatively few judgment are needed to get highquality result we also demonstrate method that reduce the amount of information archived from past judged document without adversely affecting effectiveness a novel simulation show that such 
a high performance information filtering system ha three mainrequirements it must be effective in supplying user with usefulinformation it must do so in a timely fashion and it must be ableto handle a large throughput of information and a large number ofuser profile efficiently these three requirement pose adifficult problem and to our knowledge no existing system iscapable of meeting all three in this paper we describe a systemwhich combine a number of technique from other informationretrieval and filtering system and is capable of providing highperformance on a typical workstation platform we provide estimatesof computing resource usage and show that our system is alsoscalable 
two stage in measurement of technique for informationretrieval are gathering of document for relevance assessment anduse of the assessment to numerically evaluate effectiveness weconsider both of these stage in the context of the trecexperiments to determine whether they lead to measurement thatare trustworthy and fair our detailed empirical investigation ofthe trec result show that the measured relative performance ofsystems appears to be reliable but that recall is overestimated it is likely that many relevant document have not been found wepropose a new pooling strategy that can significantly increasethe number of relevant document found for given effort withoutcompromising fairness 
the paper is the acceptance address for the acm sigir gerard salton award for excellence in research in the preamble the approach of dealing with the broader context of information science when considering information retrieval ir is justified the first part contains personal reflection of the author related to the major event and issue that formed his professional life and research agenda the second and major part considers the broad aspect of information science a a field origin problem addressed area of study structure specialty paradigm split and education problem the third part discus the limit of information science in term of internal limit imposed by the activity in the field and external limit imposed by the very human nature of information processing and use throughout issue related to user and use are transposed a being of primary concern 
