this paper introduces to the finite state calculus a family of directed replace operator in contrast to the simple replace expression upper larr lower defined in karttunen the new directed version upper larr lower yield an unambiguous transducer if the lower language consists of a single string it transduces the input string from left to right making only the longest possible replacement at each point a new type of replacement expression upper larr prefix suffix yield a transducer that insert text around string that are instance of upper the symbol denotes the matching part of the input which itself remains unchanged prefix and suffix are regular expression describing the insertion expression of the type upper larr prefix suffix may be used to compose a deterministic parser for a local grammar in the sense of gross other useful application of directed replacement include tokenization and filtering of text stream 
based on empirical evidence from a free word order language german we propose a fundamental revision of the principle guiding the ordering of discourse entity in the forward looking center within the centering model we claim that grammatical role criterion should be replaced by indicator of the functional information structure of the utterance i e the distinction between context bound and unbound discourse element this claim is backed up by an empirical evaluation of functional centering 
chart constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical form and that measure are taken to curtail generation path containing semantically incomplete phrase 
this paper discus the treatment of fixed word expression developed for our it frenc english translation system this treatment make a clear distinction between compound i e multiword expression of x level in which the chunk are adjacent and idiomatic phrase i e multiword expression of phrasal category where the chunk are not necessarily adjacent in our system compound are handled during the lexical analysis while idiom are treated in the syntax where they are treated a specialized lexeme once recognized an idiom can be transfered according to the specification of the bilingual dictionary we will show several case of transfer to corresponding idiom in the target language or to simple lexeme the complete system including several hundred of compound and idiom can be consulted on the internet http latl unige ch itsweb html 
a critical path in the development of natural language understanding nlu module lie in the difficulty of defining a mapping from word to semantics usually it take in the order of year of highly skilled labor to develop a semantic mapping e g in the form of a semantic grammar that is comprehensive enough for a given domain yet due to the very nature of human language such mapping invariably fail to achieve full coverage on unseen data acknowledging the impossibility of stating a priori all the surface form by which a concept can be expressed we present gsg an empathic computer system for the rapid deployment of nlu front end and their dynamic customization by non expert end user given a new domain for which an nlu front end is to be developed two stage are involved in the end user paraphrase a baseline version of gsg ha been implemented and preliminary experiment show promising result 
string transformation system have been introduced in brill and have several application in natural language processing in this work we consider the computational problem of automatically learning from a given corpus the set of transformation presenting the best evidence we introduce an original data structure and efficient algorithm that learn some family of transformation that are relevant for part of speech tagging and phonological rule system we also show that the same learning problem becomes np hard in case of an unbounded use of don t care symbol in a transformation 
recent work on the syntax semantics interface see e g dalrymple et al us a fragment of linear logic a a glue language for assembling meaning compositionally this paper present a glue language account of how negative polarity item e g ever any get licensed within the scope of negative or downward entailing context ladusaw e g nobody ever left this treatment of licensing operates precisely at the syntax semantics interface since it is carried out entirely within the interface glue language linear logic in addition to the account of negative polarity licensing we show in detail how linear logic proof net girard gallier can be used for efficient meaning deduction within this glue language framework 
most previous corpus based algorithm disambiguate a word with a classifier trained from previous usage of the same word separate classifier have to be trained for different word we present an algorithm that us the same knowledge source to disambiguate different word the algorithm doe not require a sense tagged corpus and exploit the fact that two different word are likely to have similar meaning if they occur in identical local context 
for language that have no explicit word boundary such a thai chinese and japanese correcting word in text is harder than in english because of additional ambiguity in locating error word the traditional method handle this by hypothesizing that every substring in the input sentence could be error word and trying to correct all of them in this paper we propose the idea of reducing the scope of spelling correction by focusing only on dubious area in the input sentence boundary of these dubious area could be obtained approximately by applying word segmentation algorithm and finding word sequence with low probability to generate the candidate correction word we used a modified edit distance which reflects the characteristic of thai ocr error finally a part of speech trigram model and winnow algorithm are combined to determine the most probable correction 
this paper describes a system that lead u to believe in the feasibility of constructing natural spoken dialogue system in task oriented domain it specifically address the issue of robust interpretation of speech in the presence of recognition error robustness is achieved by a combination of statistical error post correction syntacticallyand semantically driven robust parsing and extensive use of the dialogue context we present an evaluation of the system using time to completion and the quality of the final solution that suggests that most native speaker of english can use the system successfully with virtually no training 
we present a novel new word extraction methodfrom japanese text based on expected wordfrequencies first we compute expected wordfrequencies from japanese text using a robuststochastic n best word segmenter we then extractnew word by filtering out erroneous wordhypotheses whose expected word frequency arelower than the predefined threshold the methodis derived from an approximation of the generalizedversion of the forward backward algorithm when the japanese word segmenter is 
this paper introduces to the finite state calculus a family of directed replace operator in contrast to the simple replace expression upper larr lower defined in karttunen the new directed version upper larr lower yield an unambiguous transducer if the lower language consists of a single string it transduces the input string from left to right making only the longest possible replacement at each point a new type of replacement expression upper larr prefix suffix yield a transducer that insert text around string that are instance of upper the symbol denotes the matching part of the input which itself remains unchanged prefix and suffix are regular expression describing the insertion expression of the type upper larr prefix suffix may be used to compose a deterministic parser for a local grammar in the sense of gross other useful application of directed replacement include tokenization and filtering of text stream 
based on empirical evidence from a free word order language german we propose a fundamental revision of the principle guiding the ordering of discourse entity in the forward looking center within the centering model we claim that grammatical role criterion should be replaced by indicator of the functional information structure of the utterance i e the distinction between context bound and unbound discourse element this claim is backed up by an empirical evaluation of functional centering 
chart constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical form and that measure are taken to curtail generation path containing semantically incomplete phrase 
this paper discus the treatment of fixed word expression developed for our it frenc english translation system this treatment make a clear distinction between compound i e multiword expression of x level in which the chunk are adjacent and idiomatic phrase i e multiword expression of phrasal category where the chunk are not necessarily adjacent in our system compound are handled during the lexical analysis while idiom are treated in the syntax where they are treated a specialized lexeme once recognized an idiom can be transfered according to the specification of the bilingual dictionary we will show several case of transfer to corresponding idiom in the target language or to simple lexeme the complete system including several hundred of compound and idiom can be consulted on the internet http latl unige ch itsweb html 
a critical path in the development of natural language understanding nlu module lie in the difficulty of defining a mapping from word to semantics usually it take in the order of year of highly skilled labor to develop a semantic mapping e g in the form of a semantic grammar that is comprehensive enough for a given domain yet due to the very nature of human language such mapping invariably fail to achieve full coverage on unseen data acknowledging the impossibility of stating a priori all the surface form by which a concept can be expressed we present gsg an empathic computer system for the rapid deployment of nlu front end and their dynamic customization by non expert end user given a new domain for which an nlu front end is to be developed two stage are involved in the end user paraphrase a baseline version of gsg ha been implemented and preliminary experiment show promising result 
string transformation system have been introduced in brill and have several application in natural language processing in this work we consider the computational problem of automatically learning from a given corpus the set of transformation presenting the best evidence we introduce an original data structure and efficient algorithm that learn some family of transformation that are relevant for part of speech tagging and phonological rule system we also show that the same learning problem becomes np hard in case of an unbounded use of don t care symbol in a transformation 
recent work on the syntax semantics interface see e g dalrymple et al us a fragment of linear logic a a glue language for assembling meaning compositionally this paper present a glue language account of how negative polarity item e g ever any get licensed within the scope of negative or downward entailing context ladusaw e g nobody ever left this treatment of licensing operates precisely at the syntax semantics interface since it is carried out entirely within the interface glue language linear logic in addition to the account of negative polarity licensing we show in detail how linear logic proof net girard gallier can be used for efficient meaning deduction within this glue language framework 
most previous corpus based algorithm disambiguate a word with a classifier trained from previous usage of the same word separate classifier have to be trained for different word we present an algorithm that us the same knowledge source to disambiguate different word the algorithm doe not require a sense tagged corpus and exploit the fact that two different word are likely to have similar meaning if they occur in identical local context 
for language that have no explicit word boundary such a thai chinese and japanese correcting word in text is harder than in english because of additional ambiguity in locating error word the traditional method handle this by hypothesizing that every substring in the input sentence could be error word and trying to correct all of them in this paper we propose the idea of reducing the scope of spelling correction by focusing only on dubious area in the input sentence boundary of these dubious area could be obtained approximately by applying word segmentation algorithm and finding word sequence with low probability to generate the candidate correction word we used a modified edit distance which reflects the characteristic of thai ocr error finally a part of speech trigram model and winnow algorithm are combined to determine the most probable correction 
this paper describes a system that lead u to believe in the feasibility of constructing natural spoken dialogue system in task oriented domain it specifically address the issue of robust interpretation of speech in the presence of recognition error robustness is achieved by a combination of statistical error post correction syntacticallyand semantically driven robust parsing and extensive use of the dialogue context we present an evaluation of the system using time to completion and the quality of the final solution that suggests that most native speaker of english can use the system successfully with virtually no training 
we present a novel new word extraction methodfrom japanese text based on expected wordfrequencies first we compute expected wordfrequencies from japanese text using a robuststochastic n best word segmenter we then extractnew word by filtering out erroneous wordhypotheses whose expected word frequency arelower than the predefined threshold the methodis derived from an approximation of the generalizedversion of the forward backward algorithm when the japanese word segmenter is 
this paper explores the possibility and limit of a discourse grammar applied to spontaneous speech most discourse grammar e g sdrt asher rst mann thompson tend to be descriptive theory of written discourse which presuppose a coherent structure this structure is the outcome of a goal directed planning process on the part of the producer in order to obtain a better understanding of the planning process we analyse spoken discourse elicited in an experimental setting subject describe the pixel per pixel development of sketch map on a computer screen this force the speaker to conceptualise the perceived state of affair plan their discourse and produce a description of the drawing at the same time thus we find evidence for the planning process in the recorded data and can show that the discourse structure are le globally coherent than those underlying written text in our paper we discus to what extent a flexible discourse grammar based on a tree description grammar tdg schilder can handle such data 
this paper however our approximation of a subclass ofthese relation proved helpful for a number of query a strong example of the part whole relation occurswhen a country is mentioned in the query and aprovince or city within that country is mentioned in thedocument for example 
we give a new treatment of tabular lr parsing which is an alternative to tomita s generalized lr algorithm the advantage is twofold firstly our treatment is conceptually more attractive because it us simpler concept such a grammar transformation and standard tabulation technique also know a secondly the static and dynamic complexity of parsing both in space and time is significantly reduced 
in transformation based parsing a finite sequence of tree rewriting rule are checked for application to an input structure since in practice only a small percentage of rule are applied to any particular structure the naive parsing algorithm is rather inefficient we exploit this sparseness in rule application to derive an algorithm two to three order of magnitude faster than the standard parsing algorithm 
the paper present a language model that develops syntatic structure and us it to extract meaningful information from the word history thus enabling the use of long distance dependency the model assigns probability to every joint sequence of word binary parse structure with headword annotation the model it probabilistic parametrization and a set of experiment meant to evaluate it predictive power are presented 
most traditional approach to anaphora resolution rely heavily on linguistic and domain knowledge one of the disadvantage of developing a knowledgebased system however is that it is a very labourintensive and time consuming task this paper present a robust knowledge poor approach to resolving pronoun in technical manual this approach is a modification of the practical approach mitkov a and operates on text pre processed by a partof speech tagger input is checked against agreement and a number of antecedent indicator candidate are assigned score by each indicator and the candidate with the highest aggregate score is returned a the antecedent we propose this approach a a platform for multilingual pronoun resolution the robust approach wa initially developed and tested for english but we have also adapted and tested it for polish and arabic for both language we found that adaptation required minimum modification and that further even if used unmodified the approach delivers acceptable success rate preliminary evaluation report high success rate in the range of and over 
this paper discus the design of the eurowordnet database in which semantic database like wordnetl for several language are combined via a so called inter lingual index in this database language independent data is shared and language specific property are maintained a well a special interface ha been developed to compare the semantic configuration across language and to track down difference the pragmatic design of the database make it possible to gather empirical evidence for a common cross linguistic ontology 
many current approach to statistical language modeling rely on independence assumption between the different explanatory variable this result in model which are computationally simple but which only model the main effect of the explanatory variable on the response variable this paper present an argument in favor of a statistical approach that also model the interaction between the explanatory variable the argument rest on empirical evidence from two series of experimetns concerning automatic ambiguity resolution 
framenet is a three year nsf supported project in corpus based computational lexicography now in it second year nsf iri tool for lexicon building the project s key feature are a a commitment to corpus evidence for semantic and syntactic generalization and b the representation of the valence of it target word mostly noun adjective and verb in which the semantic portion make use of frame semantics the resulting database will contain a description of the semantic frame underlying the meaning of the word described and b the valence representation semantic and syntactic of several thousand word and phrase each accompanied by c a representative collection of annotated corpus attestation which jointly exemplify the observed linkings between frame element and their syntactic realization e g grammatical function phrase type and other syntactic trait this report will present the project s goal and workflow and information about the computational tool that have been adapted or created in house for this work 
recent empirical research ha shown conclusive advantage of multimodal interaction over speech only interaction for map based task this paper describes a multimodal language processing architecture which support interface allowing simultaneous input from speech and gesture recognition integration of spoken and gestural input is driven by unification of typed feature structure representing the semantic contribution of the different mode this integration method allows the component modality to mutually compensate for each others error it is implemented in quick set a multimodal pen voice system that enables user to set up and control distributed interactive simulation 
in order to realize their full potential multimodal system need to support not just input from multiple mode but also synchronized integration of mode johnston et al model this integration using a unification operation over typed feature structure this is an effective solution for a broad class of system but limit multimodal utterance to combination of a single spoken phrase with a single gesture we show how the unification based approach can be scaled up to provide a full multimodal grammar formalism in conjunction with a multidimensional chart parser this approach support integration of multiple element distributed across the spatial temporal and acoustic dimension of multimodal interaction integration strategy are stated in a high level unification based rule formalism supporting rapid prototyping and iterative development of multimodal system 
in synchronous rewriting the production of two rewriting system are paired and applied synchronously in the derivation of a pair of string we present a new synchronous rewriting system and argue that it can handle certain phenomenon that are not covered by existing synchronous system we also prove some interesting formal computational property of our system 
we report in this paper the observation of that is the same critical fragment in different sentence from the same source almost always realize one and the same of it many possible tokenizations this observation is demonstrated very helpful in sentence tokenization practice and is argued to be with far reaching implication in natural language processing 
many different metric exist for evaluating parsing result including viterbi crossing bracket rate zero crossing bracket rate and several others however most parsing algorithm including the viterbi algorithm attempt to optimize the same metric namely the probability of getting the correct labelled tree by choosing a parsing algorithm appropriate for the evaluation metric better performance can be achieved we present two new algorithm the labelled recall algorithm which maximizes the expected labelled recall rate and the bracketed recall algorithm which maximizes the bracketed recall rate experimental result are given showing that the two new algorithm have improved performance over the viterbi algorithm on many criterion especially the one that they optimize 
traditional account of quantifier scope employ qualitative constraint or rule to account for scoping preference this paper outline a feature based parsing algorithm for a grammar with multiple simultaneous level of representation one of which corresponds to a partial ordering among quantifier according to scope the optimal such ordering a well a the ranking of other ordering is determined in this grammar not by absolute constraint but by stochastic heuristic based on the degree of alignment among the representational level a prolog implementation is described and it accuracy is compared with that of other account 
a method for resolving the ellipsis that appear in japanese dialogue is proposed this method resolve not only the subject ellipsis but also those in object and other grammatical case in this approach a machine learning algorithm is used to select the attribute necessary for a resolution a decision tree is built and used a the actual ellipsis resolver the result of blind test have shown that the proposed method wa able to provide a resolution accuracy of for indirect object and for subject with a verb predicate by investigating the decision tree we found that topic dependent attribute are necessary to obtain high performance resolution and that indispensable attribute vary according to the grammatical case the problem of data size relative to decision tree training is also discussed 
this paper present the lilfes system an efficient feature structure description language for hpsg the core engine of lilfes is an abstract machine for attribute value logic proposed by carpenter and qu basic design policy the current status and performance evaluation of the lilfes system are described the paper discus two implementation of the lilfes the first one is based on an emulator of the abstract machine while the second one us a native code compiler and therefore is much more efficient than the first one 
this paper describes robotag an advanced prototype for a machine learningbased multilingual information extraction system first we describe a general client server architecture used in learning from observation then we give a detailed description of our novel decision tree tagging approach robotag performance for the proper noun tagging task in english and japanese is compared against humantagged key and to the best hand coded pattern performance a reported in the muc and met evaluation result related work and future direction are presented 
we propose a unified framework in which to treat semantic underspecification and parallelism phenomenon in discourse the framework employ a constraint language that can express equality and subtree relation between finite tree in addition our constraint language can express the equality up to relation over tree which capture parallelism between them the constraint are solved by context unification we demonstrate the use of our framework at the example of quantifier scope ellipsis and their interaction 
this paper deal with the reference choice involved in the generation of argumentative text since a natual segmentation of discourse into attentional space is needed to carry out this task this paper first proposes an architecture for natural language generation that combine hierarchical planning and focus guided navigation a work in it own right while hierarchical planning span out an attentional hierarchy of the discourse produced local navigation fill detail into the primitive discourse space the usefulness of this architecure actually go beyond the particular domain of application for which it is developed a piece of argumentative text such a the proof of a mathematical theorem conveys a sequence of derivation for each step of derivation the premise derived in the previous context and the inference method such a the application of a particular theorem or definition must be made clear although not restricted to nominal phrase our reference decision are similar to those concerning nominal subsequent referring expression based on the work of reichmann this paper present a discourse theory that handle reference choice by taking into account both textual distance a well a the attentional hierarchy 
we identify and validate from a large corpus constraint from conjunction on the positive or negative semantic orientation of the conjoined adjective a log linear regression model us these constraint to predict whether conjoined adjective are of same or different orientation achieving accuracy in this task when each conjunction is considered independently combining the constraint across many adjective a clustering algorithm separate the adjective into group of different orientation and finally adjective are labeled positive or negative evaluation on real data and simulation experiment indicate high level of performance classification precision is more than for adjective that occur in a modest number of conjunction in the corpus 
the information used for the extraction of term can be considered a rather internal i e coming from the candidate string itself this paper present the incorporation of external information derived from the context of the candidate string it is embedded to the c value approach for automatic term recognition atr in the form of weight constructed from statistical characteristic of the context word of the candidate string 
this paper introduces primitive optimality theory otp a linguistically motivated formalization of ot otp specifies the class of autosegmental representation the universal generator gen and the two simple family of permissible constraint in contrast to le restricted theory using generalized alignment otp s optimal surface form can be generated with finite state method adapted from ellison unfortunately these method take time exponential on the size of the grammar indeed the generation problem is shown np complete in this sense however technique are discussed for making ellison s approach fast in the typical case including a simple trick that alone provides a fold speedup on a grammar fragment of moderate size one avenue for future improvement is a new finite state notion factored automaton where regular language are represented compactly via formal intersection ki ai of fsas 
this paper describes a prototype disambiguation module kankei which us two corpus of the train project in ambiguous verb phrase of form v np pp or v np adverb s the two corpus have very different pp and adverb attachment pattern in the first the correct attachment is to the vp of the time while in the second the correct attachment is to the np of the time kankei us various n gram pattern of the phrase head around these ambiguity and assigns parse tree with these ambiguity a score based on a linear combination of the frequency with which these pattern appear with np and vp attachment in the train corpus unlike previous statistical disambiguation system this technique thus combine evidence from bigram trigram and the gram around an ambiguous attachment in the current experiment equal weight are used for simplicity but result are still good on the train corpus and accuracy despite the large statistical difference in attachment preference in the two corpus training on the first corpus and testing on the second give an accuracy of these result suggest that our technique capture attachment pattern that are useful across corpus 
we introduce a typed feature logic system providing both universal implicational principle a well a definite clause over feature term we show that such an architecture support a modular encoding of linguistic theory and allows for a compact representation using underspecification the system is fully implemented and ha been used a a workbench to develop and test large hpsg grammar the technique described in this paper are not restricted to a specific implementation but could be added to many current feature based grammar development system 
we investigate the effect of lexicon size and stopwords on chinese information retrieval using our method of short word segmentation based on simple language usage rule and statistic these rule allow u to employ a small lexicon of only entry and provide quite admirable retrieval result it is noticed that accurate segmentation is not essential for good retrieval larger lexicon can lead to incremental improvement the presence of stopwords do not contribute much noise to ir their removal risk elimination of crucial word in a query and adversely affect retrieval especially when the query are short short query of a few word perform more than worse than paragraph size query 
we describe here an algorithm for detecting subject boundary within text based on a statistical lexical similarity measure hearst ha already tackled this problem with good result hearst one of her main assumption is that a change in subject is accompanied by a change in vocabulary using this assumption but by introducing a new measure of word significance we have been able to build a robust and reliable algorithm which exhibit improved accuracy without sacrificing language independency 
this paper deal with the discovery representation and use of lexical rule lr during large scale semi automatic computational lexicon acquisition the analysis is based on a set of lr implemented and tested on the basis of spanish and english businessand finance related corpus we show that though the use of lr is justified they do not come cost free semi automatic output checking is required even with blocking and preemtion procedure built in nevertheless large scope lr are justified because they facilitate the unavoidable process of large scale semi automatic lexical acquisition we also argue that the place of lr in the computational process is a complex issue 
in this paper we present result on developing robust natural language interface by combining shallow and partial interpretation with dialogue management the key issue is to reduce the effort needed to adapt the knowledge source for parsing and interpretation to a necessary minimum in the paper we identify different type of information and present corresponding computational model the approach utilizes an automatically generated lexicon which is updated with information from a corpus of simulated dialogue the grammar is developed manually from the same knowledge source we also present result from evaluation that support the approach 
we derive the rhetorical structure of text by mean of two new surface form based algorithm one that identifies discourse usage of cue phrase and break sentence into clause and one that produce valid rhetorical structure tree for unrestricted natural language text the algorithm use information that wa derived from a corpus analysis of cue phrase 
in this paper we describe a dynamic programming dp based search algorithm for statistical translation and present experimental result the statistical translation us two source of information a translation model and a language model the language model used is a standard bigram model for the translation model the alignment probability are made dependent on the difference in the alignment position rather than on the absolute position thus the approach amount to a first order hidden markov model hmm a they are used successfully in speech recognition for the time alignment problem under the assumption that the alignment is monotone with respect to the word order in both language an efficient search strategy for translation can be formulated the detail of the search algorithm are described experiment on the eutrans corpus produced a word error rate of 
prose rhythm is a widely observed but scarcely quantified phenomenon we describe an information theoretic model for measuring the regularity of lexical stress in english text and use it in combination with trigram language model to demonstrate a relationship between the probability of word sequence in english and the amount of rhythm present in them we find that the stream of lexical stress in text from the wall street journal ha an entropy rate of le than bit per syllable for common sentence we observe that the average number of syllable per word is greater for rarer word sequence and to normalize for this effect we run control experiment to show that the choice of word order contributes significantly to stress regularity and increasingly with lexical probability 
abstract we consider the problem of assigning level number weight to hierarchically organized category during the process of text categorization these level control the ability of the category to attract document during the categorization process the level are adjusted in order to obtain a balance between recall and precision for each category if a category s recall exceeds it precision the category is too strong and it level is reduced conversely a category s level is increased to strengthen it if it precision exceeds it recall the categorization algorithm used is a supervised learning procedure that us a linear 
this paper describes an approach to extract the aspectual information of japanese verb phrase from a monoligual corpus we classify verb into six category by mean of the aspectual feature which are defined on the basis of the possibility of co occurrence with aspectual form and adverb a unique category could be identified for of the target verb to evaluate the result of the experiment we examined the meaning of teiru which is one of the most fundamental aspectual marker in japanese and obtained the correct recognition score of for the sentence 
we use a statistical method to select the most probable structure or parse for a given sentence it take a input the dependency structure generated for the sentence by a dependency grammar find all triple of modifier particle and modificant relation calculates mutual information of each relation and chooses the structure for which the product of the mutual information of it relation is the highest 
the view that communication is a form of action serving a variety of specific function ha had a tremendous impact on the philosophy of language and on computational linguistics yet this mode of analysis ha been applied to only a narrow range of exchange e g those whose primary purpose is transferring information or coordinating task while exchange meant to manage interpersonal relationship maintain face or simply to convey thanks sympathy and so on have been largely ignored we present a model of such social perlocutions that integrates previous work in natural language generation social psychology and communication study this model ha been implemented in a system that generates socially appropriate e mail in response to user specified communicative goal 
it is important to use pattern information e g tv newscast and textual information e g newspaper together for this purpose we describe a method for aligning article in tv newscast and newspaper in order to align article the alignment system us word extracted from telops in tv newscast the recall and the precision of the alignment process are and respectively in addition using the result of the alignment process we develop a browsing and retrieval system for article in tv newscast and newspaper 
we introduce a polynomial time algorithm for statistical machine translation this algorithm can be used in place of the expensive slow best first search strategy in current statistical translation architecture the approach employ the stochastic bracketing transduction grammar sbtg model we recently introduced to replace earlier word alignment channel model while retaining a bigram language model the new algorithm in our experience yield major speed improvement with no significant loss of accuracy 
in data oriented language processing an annotated language corpus is used a a stochastic grammar the most probable analysis of a new sentence is constructed by combining fragment from the corpus in the most probable way this approach ha been successfully used for syntactic analysis using corpus with syntactic annotation such a the penn tree bank if a corpus with semantically annotated sentence is used the same approach can also generate the most probable semantic interpretation of an input sentence the present paper explains this semantic interpretation method a data oriented semantic interpretation algorithm wa tested on two semantically annotated corpus the english atis corpus and the dutch ovis corpus experiment show an increase in semantic accuracy if larger corpus fragment are taken into consideration 
a compositional account of the semantics of german prefix verb in hpsg is outlined we consider only those verb that are formed by productive synchronic rule rule are fully productive if they apply to all base verb which satisfy a common description prefix can be polysemous and have separate highly underspecified lexical entry adequate base are determined via selection restriction 
we compare the effectiveness of two related machine translation model applied to the same limited domain task one is a transfer model with monolingual head automaton for analysis and generation the other is a direct transduction model based on bilingual head transducer we conclude that the head transducer model is more effective according to measure of accuracy computational requirement model size and development effort 
this paper describes the incremental generation of parse table for the lr type parsing of tree adjoining language tals the algorithm presented handle modification to the input grammar by updating the parser generated so far in this paper a lazy generation of lr type parser for tals is defined in which parse table are created by need while parsing we then describe an incremental parser generator for tals which responds to modification of the input grammar by updating parse table built so far 
this paper present a method for word sense disambiguation and coherence understanding of prepositional relation the method relies on information provided by wordnet we first classify prepositional attachment according to semantic equivalence of phrase head and then apply inferential heuristic for understanding the validity of prepositional structure 
this paper discus research on distinguishing word meaning in the context of information retrieval system we conducted experiment with three source of evidence for making these distinction morphology part of speech and phrase we have focused on the distinction between homonymy and polysemy unrelated v related meaning our result support the need to distinguish homonymy and polysemy we found grouping morphological variant make a significant improvement in retrieval performance that more than half of all word in a dictionary that differ in part of speech are related in meaning and that it is crucial to assign credit to the component word of a phrase these experiment provide better understanding of word based method and suggest where natural language processing can provide further improvement in retrieval performance 
in language modeling for speech recognition the goal is to constrain the search of the speech recognizer by providing a model which can given a context indicate what the next most likely word will be in this paper we explore how the addition of information to the text in particular part of speech and dysfluency annotation can be used to build more complex language model in particular we ask two question first in conversational speech where there is a le clear notion of sentence than in written text doe segmenting the text into linguistically or semantically based unit contribute to a better language model than merely segmenting based on broad acoustic information such a pause second is the sentence itself a good unit to be modeling or should we look at smaller unit for example dividing a sentence into a given and new portion and segmenting out acknowledgment and reply to answer these question we present a variety of kind of analysis from vocabulary distribution to perplexity on language model the next step will be modeling conversation and incorporating those model into a speech recognizer 
we introduce an algorithm for scope resolution in underspecified semantic representation scope preference are suggested on the basis of semantic argument structure the major novelty of this approach is that while maintaining an scopally underspecified semantic representation we at the same time suggest a resolution possibility the algorithm ha been implemented and tested in a large scale system and fared quite well of the utterance were ambiguous of these were correctly interpreted leaving error in only of the utterance set 
how similar are two corpus a measure of corpus similarity would be very useful for nlp for many purpose such a estimating the work involved in porting a system from one domain to another first we discus difculties in identifying what we mean by corpus similarity human similarity judgement are not negrained enough corpus similarity is inherently multidimensional and similarity can only be interpreted in the light of corpus homogeneity we then present an operational denition of corpus similarity which address or circumvents the problem using purposebuilt set of known similarity corpus these ksc set can be used to evaluate the measure we evaluate the measure described in the literature including three variant of the information theoretic measure perplexity a based measure using word frequency is shown to be the best of those tested 
this paper present a trainable rule based algorithm for performing word segmentation the algorithm provides a simple language independent alternative to large scale lexical based segmenters requiring large amount of knowledge engineering a a stand alone segmenter we show our algorithm to produce high performance chinese segmentation in addition we show the transformation based algorithm to be effective in improving the output of several existing word segmentation algorithm in three different language 
we present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a lexicalized tree adjoining grammar ltag this approach capture naturally and elegantly the interaction between pragmatic and syntactic constraint on description in a sentence and the inferential interaction between multiple description in a sentence at the same time it exploit linguistically motivated declarative specification of the discourse function of syntactic construction to make contextually appropriate syntactic choice 
this paper present an algorithm for the compilation of regular formalism with rule feature into finite state automaton rule feature are incorporated into the right context of rule this general notion can also be applied to other algorithm which compile regular rewrite rule into automaton 
recent work ha seen the emergence of a common framework for parsing categorial grammar cg formalism that fall within the type logical tradition such a the lambek calculus and related system whereby some method of linear logic theorem proving is used in combination with a system of labelling that ensures only deduction appropriate to the relevant grammatical logic are allowed the approach realising this framework however have not so far addressed the task of incremental parsing a key issue in earlier work with flexible categorial grammar in this paper the approach of hepple is modified to yield a linear deduction system that doe allow flexible deduction and hence incremental processing but that hence also suffers the problem of spurious ambiguity this problem is avoided via normalisation 
in this paper we study the problem of combininga chinese thesaurus with a chinese dictionary bylinking the word entry in the thesaurus with theword sens in the dictionary and propose asimilar word strategy to solve the problem themethod is based on the definition given in thedictionary but without any syntactic parsing orsense disambiguation on them at all a a result their combination make the thesaurus specify thesimilarity between sens which account for thesimilarity 
in this paper we study the problem of combining a chinese thesaurus with a chinese dictionary by linking the word entry in the thesaurus with the word sens in the dictionary and propose a similar word strategy to solve the problem the method is based on the definition given in the dictionary but without any syntactic parsing or sense disambiguation on them at all a a result their combination make the thesaurus specify the similarity between sens which account for the similarity between word produce a kind of semantic classification of the sens defined in the dictionary and provides reliable information about the lexical item on which the resource don t conform with each other 
this paper report on corpus based research into the relationship between intonational variation and discourse structure we examine the effect of speaking style read versus spontaneous and of discourse segmentation method text alone versus text and speech on the nature of this relationship we also compare the acoustic prosodic feature of initial medial and final utterance in a discourse segment 
many multilingual nlp application need to translate word between different language but cannot afford the computational expense of inducing or applying a full translation model for thesis application we have designed a fast algorithm for estimating a partial translation model which account for translational equivalence only at the word level the model s precision recall trade off can be directly controlled via one threshold parameter this feature make the model more suitable for application that are not fully statistical the model s hidden parameter can be easily conditioned on information extrinsic to the model providing an easy way to integrate pre existing knowledge such a part of speech dictionary word order etc our model can link word token in parallel text a well a other translation model in the literature unlike other translation model it can automatically produce dictionary sized translation lexicon and it can do so with over accuracy 
we describe a formal framework for interpretation of word and compound in a discourse context which integrates a symbolic lexicon grammar word sense probability and a pragmatic component the approach is motivated by the need to handle productive word use in this paper we concentrate on compound nominal we discus the inadequacy of approach which consider compund interpretation a either wholly lexico grammatical or wholly pragmatic and provide an alternative integrated account 
we describe a formal framework for interpretation of word and compound in a discourse context which integrates a symbolic lexicon grammar word sense probability and a pragmatic component the approach is motivated by the need to handle productive word use in this paper we concentrate on compound nominal we discus the inadequacy of approach which consider compund interpretation a either wholly lexico grammatical or wholly pragmatic and provide an alternative integrated account 
a system for the automatic production of controlled index term is presented using linguistically motivated technique this includes a finite state part of speech tagger a derivational morphological processor for analysis and generation and a unification based shallow level parser using transformational rule over syntactic pattern the contribution of this research is the successful combination of parsing over a seed term list coupled with derivational morphology to achieve greater coverage of multi word term for indexing and retrieval final result are evaluated for precision and recall and implication for indexing and retrieval are discussed 
this paper describes novel result on the characteristic of three party dialogue by quantitatively comparing them with those of two party in previous dialogue research two party dialogue are mainly focussed because data collection of multi party dialogue is difficult and there are very few theory handling them although research on multi party dialogue is expected to be of much use in building computer supported collaborative work environment and computer assisted instruction system in this paper firstly we describe our data collection method of multi party dialogue using a meeting scheduling task which enables u to compare three party dialogue with those of two party then we quantitively compare these two kind of dialogue such a the number of character and turn and pattern of information exchange lastly we show that pattern of information exchange in speaker alternation and initiative taking can be used to characterise three party dialogue 
this paper present a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system streak to incrementally generate newswire sport summary the evaluation consists of searching a test corpus of stock market report for sentence pair whose semantic and syntactic structure respectively match the triggering condition and application result of each revision rule the result show that at least of all rule class are fully portable with at least another partially portable 
few attention ha been paid to terminology extraction for what concern the possibility it offer to corpus linguistics and lexical acquisition the problem of detecting term in textual corpus ha been approached in a complex framework terminology is seen a the acquisition of domain specific knowledge i e semantic feature selectional restriction for complex term and or unknown word this ha useful implication on more complex text processing task e g information extraction an hybrid symbolic and probabilistic approach to terminology extraction ha been defined the proposed inductive method put a specific attention to the linguistic description of what term are a well a to the statistical characterization of term a complex unit of information typical of domain sub language experimental evidence of the proposed method are discussed 
this paper proposes a two layered model of dialogue structure for task oriented dialogue that process contextual information and disambiguates speech act the final goal is to improve translation quality in a speech to speech translation system 
this paper present such a model g grammar for quot group grammar quot and argues that the standard group theoretic notion of conjugacy which is central in g grammar is well suited toa uniform description of commutative and noncommutativeaspects of language the use of conjugacy provides an elegant approachto long distance dependency and scoping phenomenon both in parsing and in generation g grammar give a symmetrical account of thesemantics phonology relation from which it is 
we present and experimentally evaluate a new model of prounciation by analogy the paradigmatic cascade model given a pronunciation lexicon this algorithm first extract the most productive paradigmatic mapping in the graphemic domain and pair them statistically with their correlate s in the phonemic domain these mapping are used to search and retrieve in the lexical database the most promising analog of unseen word we finally apply to the analog pronunciation the correlated series of mapping in the phonemic domain to get the desired pronunciation 
we present an architecture and an on line learning algorithm and apply it to the problem of part of speech tagging the architecture presented is a network of linear separator in the feature space utilizing the winnow update algorithm multiplicative weight update algorithm such a winnow have been shown to have exceptionally good behavior when applied to very high dimensional problem and especially when the target concept depend on only a small subset of the feature in the feature space in this paper we describe an architecture that utilizes this mistake driven algorithm for multi class prediction selecting the part of speech of a word the experimental analysis presented here provides more evidence to that these algorithm are suitable for natural language problem the algorithm used is an on line algorithm every example is used by the algorithm only once and is then discarded this ha significance in term of efficiency a well a quick adaptation to new context we present an extensive experimental study of our algorithm under various condition in particular it is shown that the algorithm performs comparably to the best known algorithm for po 
little work ha been done in nlp on the subject of punctuation owing mainly to a lack of a good theory on which computational treatment could be based this paper described early work in progress to try to construct such a theory two approach to finding the syntactic function of punctuation mark are discussed and procedure are described by which the result from these approach can be tested and evaluated both against each other a well a against other work suggestion are made for the use of these result and for future work 
most algorithm dedicated to the generation of referential description widely suffer from a fundamental problem they make too strong assumption about adjacent processing component resulting in a limited coordination with their perceptive and linguistics data that is the provider for object descriptor and the lexical expression by which the chosen descriptor is ultimately realized motivated by this deficit we present a new algorithm that allows for a widely unconstrained incremental and goal driven selection of descriptor integrates linguistic constraint to ensure the expressibility of the chosen descriptor and provides mean to control the appearance of the created referring expression hence the main achievement of our approach lie in providing a core algorithm that make few assumption about other processing component and improves the flow of control between module 
the analysis of nominal compound construction ha proven to be a recalcitrant problemfor linguistic semantics and pose serious challenge for natural language processing system 
we present an extensive empirical comparisonof several smoothing technique inthe domain of language modeling includingthose described by jelinek and mercer katz and church andgale we investigate for the rsttime how factor such a training datasize corpus e g brown versus wall streetjournal and n gram order bigram versustrigram aect the relative performance ofthese method which we measure throughthe cross entropy of test data in addition we 
this paper describes a new statistical parser which is based on probability of dependency between head word in the parse tree standard bigram probability estimation technique are extended to calculate probability of dependency between pair of word test using wall street journal data show that the method performs at least a well a spatter magerman jelinek et al which ha the best published result for a statistical parser on this task the simplicity of the approach mean the model train on sentence in under minute with a beam search strategy parsing speed can be improved to over sentence a minute with negligible loss in accuracy 
we propose a treatment of coordination based on the concept of functor argument and subcategorization it formalization comprises two part which are conceptually independent on one hand we have extended the feature structure unification to disjunctive and set value in order to check the compatibility and the satisfiability of subcategorization requirement by structured complement on the other hand we have considered the conjunction and the general schema of a head saturation both part have been encoded within hpsg using the same resource that is the subcategorization and it principle which we have just extended 
this paper examines the phenomenon of consonant spreading in arabic stem each spreading involves a local surface copying of an underlying consonant and in certain phonological context spreading alternate productively with consonant lengthening or gemination the morphophonemic trigger of spreading lie in the pattern or even in the root themselves and the combination of a spreading root and a spreading pattern cause a consonant to be copied multiple time the interdigitation of arabic stem and the realization of consonant spreading are formalized using finite state morphotactics and variation rule and this approach ha been successfully implemented in a large scale arabic morphological analyzer which is available for testing on the internet 
this paper proposes a mistake driven mixture method for learning a tag model the method iteratively performs two procedure constructing a tag model based on the current data distribution and updating the distribution by focusing on data that are not well predicted by the constructed model the final tag model is constructed by mixing all the model according to their performance to well reflect the data distribution we represent each tag model a a hierarchical tag i e ntt context tree by using the hierarchical tag context tree the constituent of sequential tag model gradually change from broad coverage tag e g noun to specific exceptional word that cannot be captured by general tag in other word the method incorporates not only frequent connetions but also infrequent one that are often considered to be collocational we evaluate several tag model by implementing japanese part of speech tagger that share all other condition i e dictionary and word model other than their tag model the experimental result show the proposed method significantly outperforms both hand crafted and conventional statistical method 
this paper look at representing paraphrase using the formalism of synchronous tag it look particularly at comparison with machine translation and the modification it is necessary to make to synchronous tag for paraphrasing a more detailed version is in dras a 
we describe the early stage of our methodology of knowledge acquisition from technical text first a partial morpho syntactic analysis is performed to extract candidate term then the knowledge engineer assisted by an automatic clustering tool build the conceptual field of the domain we focus on this conceptual analysis stage describe the data prepared from the result of the morpho syntactic analysis and show the result of the clustering module and their interpretation we found that syntactic link represent good descriptor for candidate term clustering since the cluster are often easily interpreted a conceptual field 
we propose an algorithm to resolve anaphor tackling mainly the problem of intrasentential antecedent we base our methodology on the fact that such antecedent are likely to occur in embedded sentence sidner s focusing mechanism is used a the basic algorithm in a more complete approach the proposed algorithm ha been tested and implemented a a part of a conceptual analyser mainly to process pronoun detail of an evaluation are given 
a term list is a list of content word that characterize a consistent text or a concept this paper present a new method for translating a term list by using a corpus in the target language the method first retrieves alternative translation for each input word from a bilingual dictionary it then determines the most coherent combination of alternative translation where the coherence of a set of word is defined a the proximity among multi dimensional vector produced from the word on the basis of co occurrence statistic the method wa applied to term list extracted from newspaper article and achieved translation accuracy for ambiguous word i e word with multiple translation 
we present a natural language interface system which is based entirely on trained statistical model the system consists of three stage of processing parsing semantic interpretation and discourse each of these stage is modeled a a statistical process the model are fully integrated resulting in an end to end system that map input utterance into meaning representation frame 
this paper describes an on going study which applies the concept of transitivity to news discourse for text processing task the complex notion of transitivity is defined and the relatioship between transitivity and information foregrounding is explained a sample corpus of news article ha been coded for transitivity the corpus is being used in two text processing experiment 
it is common in nlp that the category into whichtext is classified do not have fully objective definition example of such category are lexicaldistinctions such a part of speech tag and wordsensedistinctions sentence level distinction suchas phrase attachment and discourse level distinctionssuch a topic or speech act categorization this paper present an approach to analyzing theagreement among human judge for the purposeof formulating a refined and more reliable set of 
decoding algorithm is a crucial part in statistical machine translation we describe a stack decoding algorithm in this paper we present the hypothesis scoring method and the heuristic used in our algorithm we report several technique deployed to improve the performance of the decoder we also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process we evaluate and compare these technique model in our statistical machine translation system 
this paper present a new approach to bitext correspondence problem bcp of noisy bilingual corpus based on image processing ip technique by using one of several way of estimating the lexical translation probability ltp between pair of source and target word we can turn a bitext into a discrete gray level image we contend that the bcp when seen in the light bear a striking resemblance to the line detection problem in ip therefore bcps including sentence and word alignment can benefit from a wealth of effective well established ip technique including convolution based filter texture analysis and hough transform this paper describes a new program plotalign that produce a word level bitext map for noisy or non literal bitext based on these technique 
in this paper we examine mechanism for automatic dialogue initiative setting we show how to incorporate initiative changing in a task oriented human computer dialogue system and we evaluate the effect of initiative both analytically and via computer computer dialogue simulation 
language itself he argues that stochasticgrammars play an important role in the handling of a number of theoretical linguisticissues the principal emphasis of the paper is on syntactic issue with little descriptiongiven of the incorporation of semantic and pragmatic factor into a statistical system or of their interaction with syntax in contrast to abney s general discussion of the contribution of statistic to theoreticallinguistics the work presented by kapur and clark in 
we present an approach to the parsing of dependency structure which brings together the notion of parsing a candidate elimination the use of graded constraint and the parallel disambiguation of related structural representation the approach aim at an increased level of robustness by accepting constraint violation in a controlled way combining redundant and possibly conflicting information on different representational level and facilitating partial parsing a a natural mode of behavior 
optimality theory a constraint based phonology and morphology paradigm ha allowed linguist to make elegant analysis of many phenomenon including infixation and reduplication in this work in progress we build on the work of ellison to investigate the possibility of using ot a a parsing tool that derives underlying form from surface form 
we are proposing a new framework of statistical language modeling which integrates lexical association statistic with syntactic preference while maintaining the modularity of those different statistic type facilitating both training of the model and analysis of it behavior in this paper we report the result of an empirical evaluation of our model where the model is applied to disambiguation of dependency structure of japanese sentence we also discussed the room remained for further improvement based on our error analysis 
this paper describes novel and practical japanese parser that us decision tree first we construct a single decision tree to estimate modification probability how one phrase tends to modify another next we introduce a boosting algorithm in which several decision tree are constructed and then combined for probability estimation the two constructed parser are evaluated by using the edr japanese annotated corpus the single tree method outperforms the conventional japanese stochastic method by moreover the boosting version is shown to have significant advantage better parsing accuracy than it single tree counterpart for any amount of training data and no over fitting to data for various iteration 
we describe a method for interpreting abstract flat syntactic representation lfg f structure a underspecified semantic representation here underspecified discourse representation structure udrss the method establishes a one to one correspondence between subset of the lfg and udrs formalism it provides a model theoretic interpretation and an inferential component which operates directly on underspecified representation for f structure through the translation image of f structure a udrss 
this paper address the issue of algorithm v representation for case based learning of linguistic knowledge we first present empirical evidence that the success of case based learning method for natural language processing task depends to a large degree on the feature set used to describe the training instance next we present a technique for automating feature set selection for case based learning of linguistic knowledge given a input a baseline case representation the method modifies the representation in response to a number of predefined linguistic bias by adding deleting and weighting feature appropriately we apply the linguistic bias approach to feature set selection to the problem of relative pronoun disambiguation and show that the casebased learning algorithm improves a relevant bias are incorporated into the underlying instance representation finally we argue that the linguistic bias approach to feature set selection offer new possibility for case based learning of natural language it simplifies the process of instance representation design and in theory obviates the need for separate instance representation for each linguistic knowledge acquisition task more importantly the approach offer a mechanism for explicitly combining the frequency information available from corpus based technique with linguistic bias information employed in traditional linguistic and knowledge based approach to natural language processing 
this paper present paradise paradigm for dialogue system evaluation a general framework for evaluating spoken dialogue agent the framework decouples task requirement from an agent s dialogue behavior support comparison among dialogue strategy enables the calculation of performance over subdialogues and whole dialogue specifies the relative contribution of various factor to performance and make it possible to compare agent performing different task by normalizing for task complexity 
this paper stem from an ongoing research project on verb phrase ellipsis the project s goal are to implement a verb phrase ellipsis resolution algorithm automatically test the algorithm on corpus data then automatically evaluate the algorithm against human generated answer the paper will establish the current status of the algorithm based on this automatic evaluation categorizing current problem situation an algorithm to handle one of these problem the case of subdeletion will be described and evaluated the algorithm attempt to detect and solve subdeletion by locating adjunct of similar type in a verb phrase ellipsis and corresponding antecedent 
being able to predict the placement of contrastive accent is essential for the assignment of correct accentuation pattern in spoken language generation i discus two approach to the generation of contrastive accent and propose and alternative method that is feasible and computationally attractive in data to speech system 
in general a certain range of sentence in a text is widely assumed to form a coherent unit which is called a discourse segment identifying the segment boundary is a first step to recognize the structure of a text in this paper we describe a method for identifying segment boundary of a japanese text with the aid of multiple surface linguistic cue though our experiment might be small scale we also present a method of training the weight for multiple linguistic cue automatically without the overfitting problem 
a logical recasting of binding theory is performed a an enhancing step for the purpose of it full and lean declarative implementation a new insight on sentential anaphoric process is presented which may suggestively be captured by the slogan 
we present preliminary result concerning robust technique for resolving bridging definite description we report our analysis of a collection of wall street journal article from the penn treebank corpus and our experiment with wordnet to identify relation between bridging description and their antecedent 
abstract supervised method for ambiguity resolution learn in sterile environment in absence of syntactic noise however in many language engineering application manually tagged corpus are not available nor easily implemented on the other side the exportability of disambiguation cue acquired from a given noise free domain e g the wall street journal to other domain is not obvious unsupervised method of lexical learning have just a well many inherent limitation first the type of syntactic ambiguity phenomenon occurring in real domain are much more complex than the standard v n pp pattern analyzed in literature second especially in sublanguages syntactic noise seems to be a systematic phenomenon because many ambiguity occur within identical phrase in such case there is little hope to acquire a higher statistical evidence of the correct attachment class based model may reduce this problem only to a certain degree depending upon the richness of the sublanguage and upon the size of the application corpus because of these inherent difficulty we believe that syntactic learning should be a gradual process in which the most difficult decision are made a late a possible using increasingly refined level of knowledge 
this paper present a statistical model which train from a corpus annotated with part ofspeech tag and assigns them to previously unseen text with state of the art accuracy the model can be classified a a maximum entropy model and simultaneously us many contextual feature to predict the po tag furthermore this paper demonstrates the use of specialized feature to model difficult tagging decision discus the corpus consistency problem discovered during the implementation of these feature and proposes a training strategy that mitigates these problem 
several recent effort in statistical natural language understanding nlu have focused on generating clump of english word from semantic meaning concept miller et al levin and pieracini epstein et al epstein this paper extends the ibm machine translation group s concept of fertility brown et al to the generation of clump for natural language understanding the basic underlying intuition is that a single concept may be expressed in english a many disjoint clump of word we present two fertility model which attempt to capture this phenomenon the first is a poisson model which lead to appealing computational simplicity the second is a general nonparametric fertility model the general model s parameter are boot strapped from the poisson model and updated by the em algorithm these fertility model can be used to impose clump fertility structure on top of preexisting clump generation model here we present result for adding fertility structure to unigram bigram and headword clump generation model on arpa s air travel information service atis domain 
the selectional preference of verbal predicate are an important component of lexical information useful for a number of nlp task including disambigliation of word sens approach to selectional preference acquisition without word sense disambiguation are reported to be prone to error arising from erroneous word sens large scale automatic semantic tagging of text in sufficient quantity for preference acquisition ha received little attention a most research in word sense disambiguation ha concentrated on quality word sense disambiguation of a handful of target word the work described here concentrate on adapting semantic tagging method that do not require a massive overhead of manual semantic tagging and that strike a reasonable compromise between accuracy and cost so that large amount of text can be tagged relatively quickly the result of some of these adaptation are described here along with a comparison of the selectional preference acquired with and without one of these method result of a bootstrapping approach are also outlined in which the preference obtained are used for coarse grained sense disambiguation and then the partially disambiguated data is fed back into the preference acquisition system 
this paper present a genetic algorithm based approach to the automatic discovery of finite state automaton fsas from positive data fsas are commonly used in computational phonology but given the limited learnability of fsas from arbitrary language subset are usually constructed manually the approach presented here offer a practical automatic method that help reduce the cost of manual fsa construction 
we present an extensive empirical comparison of several smoothing technique in the domain of language modeling including those described by jelinek and mercer katz and church and gale we investigate for the first time how factor such a training data size corpus gram order bigram versus trigram affect the relative performance of these method which we measure through the cross entropy of test data in addition we introduce two novel smoothing technique one a variation of jelinek mercer smoothing and one a very simple linear interpolation technique both of which outperform existing method 
lexicalized tree adjoining grammar have proved useful for nlp however numerous redundancy problem face ltags developer a highlighted by vijay shanker and schabes we present and a tool that automatically generates the tree family of an ltag it start from a compact hierarchical organization of syntactic description that is linguistically motivated and carry out all the relevant combination of linguistic phenomenon 
this paper show that a class of combinatory categorial grammar ccgs augmented with a linguistically motivated form of type raising involving variable is weakly equivalent to the standard ccgs not involving variable the proof is based on the idea that any instance of such a grammar can be simulated by a standard ccg 
the development of natural language processing nlp system that perform machine translation mt and information retrieval ir ha highlighted the need for the automatic recognition of proper name while various name recognizers have been developed they suffer from being too limited some only recognize one name class and all are language specific this work develops an approach to multilingual name recognition that us machine learning and a portable framework to simplify the porting task by maximizing reuse and automation 
in this paper we describe a method for automatically retrieving collocation from large text corpus this method retrieve collocation in the following stage extracting string of character a unit of collocation extracting recurrent combination of string in accordance with their word order in a corpus a collocation through the method various range of collocation especially domain specific collocation are retrieved the method is practical because it us plain text without any information dependent on a language such a lexical knowledge and part of speech 
this paper proposes an expansion of set of primitiveconstraints available within the primitiveoptimality theory framework eisner a this expansion consists of the addition of anew family of constraint existential implicationalconstraints which allow the specificationof faithfulness constraint that can be satisfiedat a distance and the definition of twoways to combine simple constraint into complexconstraints that is constraint disjunction crowhurst and hewitt 
verbal and compositional lexical aspect provide the underlying temporal structure of event knowledge of lexical aspect e g a telicity is therefore required for interpreting event sequence in discourse dowty moens and steedman passoneau interfacing to temporal database androutsopoulos processing temporal modifier antonisse describing allowable alternation and their semantic effect resnik tenny and selecting tense and lexical item for natural language generation dorr and olsen klavans and chodorow cf slobin and bocaz we show that it is possible to represent lexical aspect both verbal and compositional on a large scale using lexical conceptual structure lcs representation of verb in the class cataloged by levin we show how proper consideration of these universal piece of verb meaning may be used to refine lexical representation and derive a range of meaning from combination of lcs representation a single algorithm may therefore be used to determine lexical aspect class and feature at both verbal and sentence level finally we illustrate how knowledge of lexical aspect facilitates the interpretation of event in nlp application 
in the past the evaluation of machine translation system ha focused on single system evaluation because there were only few system available but now there are several commercial system for the same language pair this requires new method of comparative evaluation in the paper we propose a black box method for comparing the lexical coverage of mt system the method is based on list of word from different frequency class it is shown how these word list can be compiled and used for testing we also present the result of using our method on mt system that translate between english and german 
telegraphic message with numerous instance of omission pose a new challenge to parsing in that a sentence with omission cause a higher degree of ambiguity than a sentence without omission misparsing induced by omission ha a far reaching consequence in machine translation namely a misparse of the input often lead to a translation into the target language which ha incoherent meaning in the given context this is more frequently the case if the structure of the source and target language are quite different a in english and korean thus the question of how we parse telegraphic message accurately and efficiently becomes a critical issue in machine translation in this paper we describe a technical solution for the issue and present the performance evaluation of a machine translation system on telegraphic message before and after adopting the proposed solution the solution lie in a grammar design in which lexicalized grammar rule defined in term of semantic category and syntactic rule defined in term of part of speech are utilized together the proposed grammar achieves a higher parsing coverage without increasing the amount of ambiguity misparsing when compared with a purely lexicalized semantic grammar and achieves a lower degree of ambiguity misparses without decreasing the parsing coverage when compared with a purely syntactic grammar 
in this paper we propose a framework structured semantic space a a foundation for word sense disarnbiguation task and present a strategy to identify the correct sense of a word in some context based on the space the semantic space is a set of multidimensional real valued vector which formally describe the context of word instead of locating all word sens in the space we only make use of mono sense word to outline it we design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the node sense cluster corresponding with set of similar sens in the dendrogram given a word in a particular context the context would activate some cluster in the dendrogram based on it similarity with the context of the word in the cluster then the correct sense of the word could be determined by comparing it definition with those of the word in the cluster 
abstract this paper present an original method and it implementation to extract terminology from corpus by combining linguistic filter and statistical method starting from a linguistic study of the term of telecommunication domain we designed a number of filter which enable u to obtain a first selection of sequence that may be considered a term various statistical score are applied to this selection and result are evaluated this method ha been applied to french and to english but this paper deal only with french 
