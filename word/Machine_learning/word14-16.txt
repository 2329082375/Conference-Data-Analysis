using random graph to model network ha a rich history in this paper we analyze and improve the multifractal network generator mfng introduced by palla et al we provide a new result on the probability of subgraphs existing in graph generated with mfng this allows u to quickly compute moment of an important set of graph property such a the expected number of edge star and clique for graph generated using mfng specifically we show how to compute these moment in time complexity independent of the size of the graph and the number of recursive level in the generative model we leverage this theory to propose a new method of moment algorithm for fitting mfng to large network empirically this new approach effectively simulates property of several social and information network in term of matching subgraph count our method outperforms similar algorithm used with the stochastic kronecker graph model furthermore we present a fast approximation algorithm to generate graph instance following the multifractal structure the approximation scheme is an improvement over previous method which ran in time complexity quadratic in the number of vertex combined our method of moment and fast sampling scheme provide the first scalable framework for effectively modeling large network with mfng 
using random graph to model network ha a rich history in this paper we analyze and improve the multifractal network generator mfng introduced by palla et al we provide a new result on the probability of subgraphs existing in graph generated with mfng this allows u to quickly compute moment of an important set of graph property such a the expected number of edge star and clique for graph generated using mfng specifically we show how to compute these moment in time complexity independent of the size of the graph and the number of recursive level in the generative model we leverage this theory to propose a new method of moment algorithm for fitting mfng to large network empirically this new approach effectively simulates property of several social and information network in term of matching subgraph count our method outperforms similar algorithm used with the stochastic kronecker graph model furthermore we present a fast approximation algorithm to generate graph instance following the multifractal structure the approximation scheme is an improvement over previous method which ran in time complexity quadratic in the number of vertex combined our method of moment and fast sampling scheme provide the first scalable framework for effectively modeling large network with mfng 
support vector machine svm ha been one of the most popular learning algorithm with the central idea of maximizing the minimum margin i e the smallest distance from the instance to the classification boundary recent theoretical result however disclosed that maximizing the minimum margin doe not necessarily lead to better generalization performance and instead the margin distribution ha been proven to be more crucial in this paper we propose the large margin distribution machine ldm which try to achieve a better generalization performance by optimizing the margin distribution we characterize the margin distribution by the firstand second order statistic i e the margin mean and variance the ldm is a general learning approach which can be used in any place where svm can be applied and it superiority is verified both theoretically and empirically in this paper 
we introduce a new problem the online selective anomaly detection osad to model a specific scenario emerging from research in sleep science scientist have segmented sleep into several stage and stage two is characterized by two pattern or anomaly in the eeg time series recorded on sleep subject these two pattern are sleep spindle s and k complex the osad problem wa introduced to design a residual system where all anomaly known and unknown are detected but the system only trigger an alarm when non s anomaly appear the solution of the osad problem required u to combine technique from both data mining and control theory experiment on data from real subject attest to the effectiveness of our approach 
we describe a completely automated large scale visual recommendation system for fashion our focus is to efficiently harness the availability of large quantity of online fashion image and their rich meta data specifically we propose two class of data driven model in the deterministic fashion recommenders dfr and stochastic fashion recommenders sfr for solving this problem we analyze relative merit and pitfall of these algorithm through extensive experimentation on a large scale data set and baseline them against existing idea from color science we also illustrate key fashion insight learned through these experiment and show how they can be employed to design better recommendation system the industrial applicability of proposed model is in the context of mobile fashion shopping finally we also outline a large scale annotated data set of fashion image fashion k that can be exploited for future research in data driven visual fashion 
statistical model with constrained probability distribution are abundant in machine learning some example include regression model with norm constraint e g lasso probit model many copula model and latent dirichlet allocation lda model bayesian inference involving probability distribution confined to constrained domain could be quite challenging for commonly used sampling algorithm for such problem we propose a novel markov chain monte carlo mcmc method that provides a general and computationally efficient framework for handling boundary condition our method first map the d dimensional constrained domain of parameter to the unit ball formula see text then augments it to a d dimensional sphere s d such that the original boundary corresponds to the equator of s d this way our method handle the constraint implicitly by moving freely on the sphere generating proposal that remain within boundary when mapped back to the original space to improve the computational efficiency of our algorithm we divide the dynamic into several part such that the resulting split dynamic ha a partial analytical solution a a geodesic flow on the sphere we apply our method to several example including truncated gaussian bayesian lasso bayesian bridge regression and a copula model for identifying synchrony among multiple neuron our result show that the proposed method can provide a natural and efficient framework for handling several type of constraint on target distribution 
we describe the design implementation and evaluation of ember an automated x continuous system for forecasting civil unrest across country of latin america using open source indicator such a tweet news source blog economic indicator and other data source unlike retrospective study ember ha been making forecast into the future since nov which have been and continue to be evaluated by an independent t e team mitre of note ember ha successfully forecast the june protest in brazil and feb violent protest in venezuela we outline the system architecture of ember individual model that leverage specific data source and a fusion and suppression engine that support trading off specific evaluation criterion ember also provides an audit trail interface that enables the investigation of why specific prediction were made along with the data utilized for forecasting through numerous evaluation we demonstrate the superiority of ember over baserate method and it capability to forecast significant societal happening 
the heat kernel is a type of graph diffusion that like the much used personalized pagerank diffusion is useful in identifying a community nearby a starting seed node we present the first deterministic local algorithm to compute this diffusion and use that algorithm to study the community that it produce our algorithm is formally a relaxation method for solving a linear system to estimate the matrix exponential in a degree weighted norm we prove that this algorithm stay localized in a large graph and ha a worst case constant runtime that depends only on the parameter of the diffusion not the size of the graph on large graph our experiment indicate that the community produced by this method have better conductance than those produced by pagerank although they take slightly longer to compute on a real world community identification task the heat kernel community perform better than those from the pagerank diffusion 
petabyte of data about human movement transaction and communication pattern are being generated by everyday technology such a mobile phone credit card this unprecedented volume of information facilitates a novel set of research question applicable to a wide range of development issue in collaboration involving mobile phone operator across country jana s mobile technology platform can instantly poll and compensate billion active mobile subscription this talk will discus how insight gained from living in kenya became the genesis of a technology company currently working with global client in over country including p g google unilever danone general mill nestle johnson johnson microsoft the world bank and the united nation after providing an overview of the mobile and social medium landscape in emerging market we discus a system that implement poll mobile subscription compensation the presentation will conclude by emphasizing the value of consumer data in underserved and understudied region of the world 
we propose sparfa trace a new machine learning based framework for time varying learning and content analytics for educational application we develop a novel message passing based blind approximate kalman filter for sparse factor analysis sparfa that jointly trace learner concept knowledge over time analyzes learner concept knowledge state transition induced by interacting with learning resource such a textbook section lecture video etc or the forgetting effect and estimate the content organization and difficulty of the question in assessment these quantity are estimated solely from binary valued correct incorrect graded learner response data and the specific action each learner performs e g answering a question or studying a learning resource at each time instant experimental result on two online course datasets demonstrate that sparfa trace is capable of tracing each learner s concept knowledge evolution over time analyzing the quality and content organization of learning resource and estimating the question concept association and the question difficulty moreover we show that sparfa trace achieves comparable or better performance in predicting unobserved learner response compared to existing collaborative filtering and knowledge tracing method 
we introduce a new algorithm for off policy temporal difference learning with function approximation that ha lower variance and requires le knowledge of the behavior policy than prior method we develop the notion of a recognizer a filter on action that distorts the behavior policy to produce a related target policy with low variance importance sampling correction we also consider target policy that are deviation from the state distribution of the behavior policy such a potential temporally abstract option which further reduces variance this paper introduces recognizers and their potential advantage then develops a full algorithm for linear function approximation and prof that it update are in the same direction a on policy td update which implies asymptotic convergence even though our algorithm is based on importance sampling we prove that it requires absolutely no knowledge of the behavior policy for the case of state aggregation function approximators 
massive online open course have the potential to revolutionize higher education with their wide outreach and accessibility but they require instructor to come up with scalable alternate to traditional student evaluation peer grading having student ass each other is a promising approach to tackling the problem of evaluation at scale since the number of grader naturally scale with the number of student however student are not trained in grading which mean that one cannot expect the same level of grading skill a in traditional setting drawing on broad evidence that ordinal feedback is easier to provide and more reliable than cardinal feedback it is therefore desirable to allow peer grader to make ordinal statement e g project x is better than project y and not require them to make cardinal statement e g project x is a b thus in this paper we study the problem of automatically inferring student grade from ordinal peer feedback a opposed to existing method that require cardinal peer feedback we formulate the ordinal peer grading problem a a type of rank aggregation problem and explore several probabilistic model under which to estimate student grade and grader reliability we study the applicability of these method using peer grading data collected from a real class with instructor and ta grade a a baseline and demonstrate the efficacy of ordinal feedback technique in comparison to existing cardinal peer grading method finally we compare these peer grading technique to traditional evaluation technique 
the vast majority of real world classification problem are imbalanced meaning there are far fewer data from the class of interest the positive class than from other class we propose two machine learning algorithm to handle highly imbalanced classification problem the classifier are disjunction of conjunction and are created a union of parallel axis rectangle around the positive example and thus have the benefit of being interpretable the first algorithm us mixed integer programming to optimize a weighted balance between positive and negative class accuracy regularization is introduced to improve generalization performance the second method us an approximation in order to assist with scalability specifically it follows a textit characterize then discriminate approach where the positive class is characterized first by box and then each box boundary becomes a separate discriminative classifier this method ha the computational advantage that it can be easily parallelized and considers only the relevant region of feature space 
we present deepwalk a novel approach for learning latent representation of vertex in a network these latent representation encode social relation in a continuous vector space which is easily exploited by statistical model deepwalk generalizes recent advancement in language modeling and unsupervised feature learning or deep learning from sequence of word to graph deepwalk us local information obtained from truncated random walk to learn latent representation by treating walk a the equivalent of sentence we demonstrate deepwalk s latent representation on several multi label network classification task for social network such a blogcatalog flickr and youtube our result show that deepwalk outperforms challenging baseline which are allowed a global view of the network especially in the presence of missing information deepwalk s representation can provide f score up to higher than competing method when labeled data is sparse in some experiment deepwalk s representation are able to outperform all baseline method while using le training data deepwalk is also scalable it is an online learning algorithm which build useful incremental result and is trivially parallelizable these quality make it suitable for a broad class of real world application such a network classification and anomaly detection 
a topic propagating in a social network reach it tipping point if the number of user discussing it in the network exceeds a critical threshold such that a wide cascade on the topic is likely to occur in this paper we consider the task of selecting initial seed user of a topic with minimum size so that em with a guaranteed probability the number of user discussing the topic would reach a given threshold we formulate the task a an optimization problem called em seed minimization with probabilistic coverage guarantee sm pcg this problem departs from the previous study on social influence maximization or seed minimization because it considers influence coverage with em probabilistic guarantee instead of guarantee on em expected influence coverage we show that the problem is not submodular and thus is harder than previously studied problem based on submodular function optimization we provide an approximation algorithm and show that it approximates the optimal solution with both a multiplicative ratio and an additive error the multiplicative ratio is tight while the additive error would be small if influence coverage distribution of certain seed set are well concentrated for one way bipartite graph we analytically prove the concentration condition and obtain an approximation algorithm with an o log n multiplicative ratio and an o sqrt n additive error where n is the total number of node in the social graph moreover we empirically verify the concentration condition in real world network and experimentally demonstrate the effectiveness of our proposed algorithm comparing to commonly adopted benchmark algorithm 
we consider the problem of offline pool based active semi supervised learning on graph this problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available the data point are represented by the vertex of an undirected graph with the similarity between them captured by the edge weight given a target number of node to label the goal is to choose those node that are most informative and then predict the unknown label we propose a novel framework for this problem based on our recent result on sampling theory for graph signal a graph signal is a real valued function defined on each node of the graph a notion of frequency for such signal can be defined using the spectrum of the graph laplacian matrix the sampling theory for graph signal aim to extend the traditional nyquist shannon sampling theory by allowing u to identify the class of graph signal that can be reconstructed from their value on a subset of vertex this approach allows u to define a criterion for active learning based on sampling set selection which aim at maximizing the frequency of the signal that can be reconstructed from their sample on the set experiment show the effectiveness of our method 
sampling is a standard approach in big graph analytics the goal is to efficiently estimate the graph property by consulting a sample of the whole population a perfect sample is assumed to mirror every property of the whole population unfortunately such a perfect sample is hard to collect in complex population such a graph e g web graph social network where an underlying network connects the unit of the population therefore a good sample will be representative in the sense that graph property of interest can be estimated with a known degree of accuracy while previous work focused particularly on sampling scheme to estimate certain graph property e g triangle count much le is known for the case when we need to estimate various graph property with the same sampling scheme in this paper we propose a generic stream sampling framework for big graph analytics called graph sample and hold gsh which sample from massive graph sequentially in a single pas one edge at a time while maintaining a small state in memory we use a horvitz thompson construction in conjunction with a scheme that sample arriving edge without adjacency to previously sampled edge with probability p and hold edge with adjacency with probability q our sample and hold framework facilitates the accurate estimation of subgraph pattern by enabling the dependence of the sampling process to vary based on previous history within our framework we show how to produce statistically unbiased estimator for various graph property from the sample given that the graph analytics will run on a sample instead of the whole population the runtime complexity is kept under control moreover given that the estimator are unbiased the approximation error is also kept under control finally we test the performance of the proposed framework gsh on various type of graph showing that from a sample with k edge it produce estimate with relative error 
sparse support vector machine svm is a robust predictive model that can effectively remove noise and preserve signal like lasso it can efficiently learn a solution path based on a set of predefined parameter and therefore provides strong support for model selection sparse svm ha been successfully applied in a variety of data mining application including text mining bioinformatics and image processing the emergence of big data analysis pose new challenge for model selection with large scale data that consist of ten of million sample and feature in this paper a novel screening technique is proposed to accelerate model selection for l regularized l svm and effectively improve it scalability this technique can precisely identify inactive feature in the optimal solution of a l regularized l svm model and remove them before training the technique make use of the variational inequality and provides a closed form solution for screening inactive feature in different situation every feature that is removed by the screening technique is guaranteed to be inactive in the optimal solution therefore when l regularized l svm us the feature selected by the technique it achieves exactly the same result a when it us the full feature set because the technique can remove a large number of inactive feature it can greatly increase the efficiency of model selection for l regularized l svm experimental result on five high dimensional benchmark data set demonstrate the power of the proposed technique 
in the paper we consider the problem of link prediction in time evolving graph we assume that certain graph feature such a the node degree follow a vector autoregressive var model and we propose to use this information to improve the accuracy of prediction our strategy involves a joint optimization procedure over the space of adjacency matrix and var matrix on the adjacency matrix it take into account both sparsity and low rank property and on the var it encodes the sparsity the analysis involves oracle inequality that illustrate the trade offs in the choice of smoothing parameter when modeling the joint effect of sparsity and low rank the estimate is computed efficiently using proximal method and evaluated through numerical experiment 
estimating similarity between vertex is a fundamental issue in network analysis across various domain such a social network and biological network method based on common neighbor and structural context have received much attention however both category of method are difficult to scale up to handle large network with billion of node in this paper we propose a sampling method that provably and accurately estimate the similarity between vertex the algorithm is based on a novel idea of random path and an extended method is also presented to enhance the structural similarity when two vertex are completely disconnected we provide theoretical proof for the error bound and confidence of the proposed algorithm we perform extensive empirical study and show that our algorithm can obtain top k similar vertex for any vertex in a network approximately faster than state of the art method we also use identity resolution and structural hole spanner finding two important application in social network to evaluate the accuracy of the estimated similarity our experimental result demonstrate that the proposed algorithm achieves clearly better performance than several alternative method 
citation recommendation is an interesting but challenging research problem most existing study assume that all paper adopt the same criterion and follow the same behavioral pattern in deciding relevance and authority of a paper however in reality paper have distinct citation behavioral pattern when looking for different reference depending on paper content author and target venue in this study we investigate the problem in the context of heterogeneous bibliographic network and propose a novel cluster based citation recommendation framework called cluscite which explores the principle that citation tend to be softly clustered into interest group based on multiple type of relationship in the network therefore we predict each query s citation based on related interest group each having it own model for paper authority and relevance specifically we learn group membership for object and the significance of relevance feature for each interest group while also propagating relative authority between object by solving a joint optimization problem experiment on both dblp and pubmed datasets demonstrate the power of the proposed approach with improvement in recall and growth in mrr over the best performing baseline 
the present article serf a an erratum to our paper of the same title which wa presented and published in the kdd conference in that article we claimed falsely that the objective function defined in section is non monotone submodular we are deeply indebted to debmalya mandal jean pouget abadie and yaron singer for bringing to our attention a counter example to that claim subsequent to becoming aware of the counter example we have shown that the objective function is in fact np hard to approximate to within a factor of o n for any in an attempt to fix the record the present article combine the problem motivation model and experimental result section from the original incorrect article with the new hardness result we would like reader to only cite and use this version which will remain an unpublished note instead of the incorrect conference version 
we study the interplay between a dynamic process and the structure of the network on which it is defined specifically we examine the impact of this interaction on the quality measure of network cluster and node centrality this enables u to effectively identify network community and important node participating in the dynamic a the first step towards this objective we introduce an umbrella framework for defining and characterizing an ensemble of dynamic process on a network this framework generalizes the traditional laplacian framework to continuous time biased random walk and also allows u to model some epidemic process over a network for each dynamic process in our framework we can define a function that measure the quality of every subset of node a a potential cluster or community with respect to this process on a given network this subset quality function generalizes the traditional conductance measure for graph partitioning we partially justify our choice of the quality function by showing that the classic cheeger s inequality which relates the conductance of the best cluster in a network with a spectral quantity of it laplacian matrix can be extended from the laplacian conductance setting to this more general setting 
can we learn the influence of a set of people in a social network from cascade of information diffusion this question is often addressed by a two stage approach first learn a diffusion model and then calculate the influence based on the learned model thus the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data in this paper we exploit the insight that the influence function in many diffusion model are coverage function and propose a novel parameterization of such function using a convex combination of random basis function moreover we propose an efficient maximum likelihood based algorithm to learn such function directly from cascade data and hence bypass the need to specify a particular diffusion model in advance we provide both theoretical and empirical analysis for our approach showing that the proposed approach can provably learn the influence function with low sample complexity be robust to the unknown diffusion model and significantly outperform existing approach in both synthetic and real world data 
it ha been shown that graphical model can be used to leverage the dependence in large scale multiple testing problem with significantly improved performance sun amp amp cai liu et al these graphical model are fully parametric and require that we know the parameterization of f the density function of the test statistic under the alternative hypothesis however in practice f is often heterogeneous and cannot be estimated with a simple parametric distribution we propose a novel semiparametric approach for multiple testing under dependence which estimate f adaptively this semiparametric approach exactly generalizes the local fdr procedure efron et al and connects with the bh procedure benjamini amp amp hochberg a variety of simulation show that our semiparametric approach outperforms classical procedure which assume independence and the parametric approach which capture dependence 
the recent blossom of social network and communication service in both public and corporate setting have generated a staggering amount of network data of all kind unlike the bio network and the chemical compound graph data often used in traditional network mining and analysis the new network data grown out of the social application are characterized by their rich attribute high heterogeneity enormous size and complex pattern of various semantic meaning all of which have posed significant research challenge to the graph network mining community in this tutorial we aim to examine some recent advance in network mining and analysis for social application covering a diverse collection of methodology and application from the perspective of event relationship collaboration and network pattern we would present the problem setting the challenge the recent research advance and some future direction for each perspective topic include but are not limited to correlation mining iceberg finding anomaly detection relationship discovery information flow task routing and pattern mining 
in recent year with the widespread usage of web technique crowdsourcing play an important role in offering human intelligence in various service website such a yahoo answer and quora with the increasing amount of crowd oriented service data an important task is to analyze latest hot topic and track topic evolution over time however the existing technique in text mining cannot effectively work due to the unique structure of crowd oriented service data task response pair which consists of the task and it corresponding response in particular existing approach become ineffective with the ever increasing crowd oriented service data that accumulate along the time in this paper we first study the problem of discovering topic over crowd oriented service data then we propose a new probabilistic topic model the topic crowd service model tc model to effectively discover latent topic from massive crowd oriented service data in particular in order to train tc efficiently we design a novel parameter inference algorithm the bucket parameter estimation bpe which utilizes belief propagation and a new sketching technique called pairwise sketch psketch finally we conduct extensive experiment to verify the effectiveness and efficiency of the tc model and the bpe algorithm 
in this paper we study networked bandit a new bandit problem where a set of interrelated arm varies over time and given the contextual information that selects one arm invokes other correlated arm this problem remains under investigated in spite of it applicability to many practical problem for instance in social network an arm can obtain payoff from both the selected user and it relation since they often share the content through the network we examine whether it is possible to obtain multiple payoff from several correlated arm based on the relationship in particular we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm but also the relationship between arm our algorithm is optimism in face of uncertainty style in that it decides an arm depending on integrated confidence set constructed from historical data we analyze the performance in simulation experiment and on two real world offline datasets the experimental result demonstrate our algorithm s effectiveness in the networked bandit setting 
topic modeling ha been widely used to mine topic from document however a key weakness of topic modeling is that it need a large amount of data e g thousand of document to provide reliable statistic to generate coherent topic however in practice many document collection do not have so many document given a small number of document the classic topic model lda generates very poor topic even with a large volume of data unsupervised learning of topic model can still produce unsatisfactory result in recently year knowledge based topic model have been proposed which ask human user to provide some prior domain knowledge to guide the model to produce better topic our research take a radically different approach we propose to learn a human do i e retaining the result learned in the past and using them to help future learning when faced with a new task we first mine some reliable prior knowledge from the past learning modeling result and then use it to guide the model inference to generate more coherent topic this approach is possible because of the big data readily available on the web the proposed algorithm mine two form of knowledge must link meaning that two word should be in the same topic and cannot link meaning that two word should not be in the same topic it also deal with two problem of the automatically mined knowledge i e wrong knowledge and knowledge transitivity experimental result using review document from product domain show that the proposed approach make dramatic improvement over state of the art baseline 
the explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advance in video retrieval recommendation and discovery in this paper we focus on the task of video suggestion commonly found in many online application the current state of the art video suggestion technique are based on the collaborative filtering analysis and suggest video that are likely to be co viewed with the watched video in this paper we propose augmenting the collaborative filtering analysis with the topical representation of the video content to suggest related video we propose two novel method for topical video representation the first method us information retrieval heuristic such a tf idf while the second method learns the optimal topical representation based on the implicit user feedback available in the online scenario we conduct a large scale live experiment on youtube traffic and demonstrate that augmenting collaborative filtering with topical representation significantly improves the quality of the related video suggestion in a live setting especially for category with fresh and topically rich video content such a news video in addition we show that employing user feedback for learning the optimal topical video representation can increase the user engagement by more than over the standard information retrieval representation when compared to the collaborative filtering baseline 
in this talk drew will examine data science through the lens of the social scientist he will discus how the various skill and discipline combine into data science drew will also present a motivating example directly from his work a a senior advisor to nyc s mayor s office of analytics 
the diffusion of information rumor and disease are assumed to be probabilistic process over some network structure an event start at one node of the network and then spread to the edge of the network in most case the underlying network structure that generates the diffusion process is unobserved and we only observe the time at which each node is altered influenced by the process this paper proposes a probabilistic model for inferring the diffusion network which we call probabilistic latent network visualization plnv it is based on cascade data a record of observed time of node influence an important characteristic of our approach is to infer the network by embedding it into a low dimensional visualization space we assume that each node in the network ha latent coordinate in the visualization space and diffusion is more likely to occur between node that are placed close together our model us maximum a posteriori estimation to learn the latent coordinate of node that best explain the observed cascade data the latent coordinate of node in the visualization space can enable the system to suggest network layout most suitable for browsing and lead to high accuracy in inferring the underlying network when analyzing the diffusion process of new or rare information rumor and disease 
kernel based regression represents an important family of learning technique for solving challenging regression task with non linear pattern despite being studied extensively most of the existing work suffers from two major drawback i they are often designed for solving regression task in a batch learning setting making them not only computationally inefficient and but also poorly scalable in real world application where data arrives sequentially and ii they usually assume a fixed kernel function is given prior to the learning task which could result in poor performance if the chosen kernel is inappropriate to overcome these drawback this paper present a novel scheme of online multiple kernel regression omkr which sequentially learns the kernel based regressor in an online and scalable fashion and dynamically explore a pool of multiple diverse kernel to avoid suffering from a single fixed poor kernel so a to remedy the drawback of manual heuristic kernel selection the omkr problem is more challenging than regular kernel based regression task since we have to on the fly determine both the optimal kernel based regressor for each individual kernel and the best combination of the multiple kernel regressors in this paper we propose a family of omkr algorithm for regression and discus their application to time series prediction task we also analyze the theoretical bound of the proposed omkr method and conduct extensive experiment to evaluate it empirical performance on both real world regression and time series prediction task 
balanced edge partition ha emerged a a new approach to partition an input graph data for the purpose of scaling out parallel computation which is of interest for several modern data analytics computation platform including platform for iterative computation machine learning problem and graph database this new approach stand in a stark contrast to the traditional approach of balanced vertex partition where for given number of partition the problem is to minimize the number of edge cut subject to balancing the vertex cardinality of partition in this paper we first characterize the expected cost of vertex and edge partition with and without aggregation of message for the commonly deployed policy of placing a vertex or an edge uniformly at random to one of the partition we then obtain the first approximation algorithm for the balanced edge partition problem which for the case of no aggregation match the best known approximation ratio for the balanced vertex partition problem and show that this remains to hold for the case with aggregation up to factor that is equal to the maximum in degree of a vertex we report result of an extensive empirical evaluation on a set of real world graph which quantifies the benefit of edgevs vertex partition and demonstrates efficiency of natural greedy online assignment for the balanced edge partition problem with and with no aggregation 
discovering temporal dependence structure from multivariate time series ha established it importance in many application we observe that when we look in reversed order of time the temporal dependence structure of the time series is usually preserved after switching the role of cause and effect inspired by this observation we create a new time series by reversing the time stamp of original time series and combine both time series to improve the performance of temporal dependence recovery we also provide theoretical justification for the proposed algorithm for several existing time series model we test our approach on both synthetic and real world datasets the experimental result confirm that this surprisingly simple approach is indeed effective under various circumstance 
human emotional state are not independent but rather proceed along systematic path governed by both internal cognitive factor and external social one for example anxiety often transition to disappointment which is likely to sink to depression before rising to happiness and relaxation and these state are conditioned by the state of others in our community modeling these complex dependency can yield insight into human emotion and support more powerful sentiment technology we develop a theory of conditional dependency between emotional state in which emotion are characterized not only by valence polarity and arousal intensity but also by the role they play in state transition and social relationship we implement this theory using conditional random field crfs that synthesize textual information with information about previous emotional state and the emotional state of others to ass the power of affective transition we evaluate our model in a collection of mood update from the experience project to ass the power of social factor we use a corpus of product review from a website in which the community dynamic encourage reviewer to be influenced by each other in both setting our model yield improvement of statistical and practical significance over one that classify each text independently of it emotional or social context 
shapelets are discriminative sub sequence of time series that best predict the target variable for this reason shapelet discovery ha recently attracted considerable interest within the time series research community currently shapelets are found by evaluating the prediction quality of numerous candidate extracted from the series segment in contrast to the state of the art this paper proposes a novel perspective in term of learning shapelets a new mathematical formalization of the task via a classification objective function is proposed and a tailored stochastic gradient learning algorithm is applied the proposed method enables learning near to optimal shapelets directly without the need to try out lot of candidate furthermore our method can learn true top k shapelets by capturing their interaction extensive experimentation demonstrates statistically significant improvement in term of win and rank against baseline over time series datasets 
social sensing is based on the idea that community or group of people can provide a set of information similar to those obtainable from a sensor network emergency management is a candidate field of application for social sensing in this work we describe the design implementation and deployment of a decision support system for the detection and the damage assessment of earthquake in italy our system exploit the message shared in real time on twitter one of the most popular social network in the world data mining and natural language processing technique are employed to select meaningful and comprehensive set of tweet we then apply a burst detection algorithm in order to promptly identify outbreaking seismic event detected event are automatically broadcasted by our system via a dedicated twitter account and by email notification in addition we mine the content of the message associated to an event to discover knowledge on it consequence finally we compare our result with official data provided by the national institute of geophysics and volcanology ingv the authority responsible for monitoring seismic event in italy the ingv network detects shaking level produced by the earthquake but can only model the damage scenario by using empirical relationship this scenario can be greatly improved with direct information site by site result show that the system ha a great ability to detect event of a magnitude in the region of with relatively low occurrence of false positive earthquake detection mostly occurs within second of the event and far earlier than the notification shared by ingv or by other official channel thus we are able to alert interested party promptly information discovered by our system can be extremely useful to all the government agency interested in mitigating the impact of earthquake a well a the news agency looking for fresh information to publish 
this paper instantly infers the gas consumption and pollution emission of vehicle traveling on a city s road network in a current time slot using gps trajectory from a sample of vehicle e g taxicab the knowledge can be used to suggest cost efficient driving route a well a identifying road segment where gas ha been wasted significantly the instant estimation of the emission from vehicle can enable pollution alert and help diagnose the root cause of air pollution in the long run in our method we first compute the travel speed of each road segment using the gps trajectory received recently a many road segment are not traversed by trajectory i e data sparsity we propose a travel speed estimation tse model based on a context aware matrix factorization approach tse leverage feature learned from other data source e g map data and historical trajectory to deal with the data sparsity problem we then propose a traffic volume inference tvi model to infer the number of vehicle passing each road segment per minute tvi is an unsupervised bayesian network that incorporates multiple factor such a travel speed weather condition and geographical feature of a road given the travel speed and traffic volume of a road segment gas consumption and emission can be calculated based on existing environmental theory we evaluate our method based on extensive experiment using gps trajectory generated by over taxi in beijing over a period of two month the result demonstrate the advantage of our method over baseline validating the contribution of it component and finding interesting discovery for the benefit of society 
for decade large corporation a well a labor placement service have maintained extensive yet static resume databanks online professional network like linkedin have taken these resume databanks to a dynamic constantly updated and massive scale professional profile dataset spanning career record from hundred of industry million of company and hundred of million of people worldwide using this professional profile dataset this paper attempt to model profile of individual a a sequence of position held by them a a time series of node each of which represents one particular position or job experience in the individual s career trajectory these career trajectory model can be employed in various utility application including career trajectory planning for student in school university using knowledge inferred from real world career outcome they can also be employed for decoding sequence to uncover path leading to certain professional milestone from a user s current professional status we deploy the proposed technique to ascertain professional similarity between two individual by developing a similarity measure simcareers similar career path the measure employ sequence alignment between two career trajectory to quantify professional similarity between career path to the best of our knowledge simcareers is the first framework to model professional similarity between two people taking account their career trajectory information we posit that using the temporal and structural feature of a career trajectory for modeling profile similarity is a far more superior approach than using similarity measure on semi structured attribute representation of a profile for this application we validate our hypothesis by extensive quantitative evaluation on a gold dataset of similar profile generated from recruiting activity log from actual recruiter using linkedin in addition we show significant improvement in engagement by running an a b test on a real world application called similar profile on linkedin world s largest online professional network 
chronic disease such a alzheimer s disease diabetes and chronic obstructive pulmonary disease usually progress slowly over a long period of time causing increasing burden to the patient their family and the healthcare system a better understanding of their progression is instrumental in early diagnosis and personalized care modeling disease progression based on real world evidence is a very challenging task due to the incompleteness and irregularity of the observation a well a the heterogeneity of the patient condition in this paper we propose a probabilistic disease progression model that address these challenge a compared to existing disease progression model the advantage of our model is three fold it learns a continuous time progression model from discrete time observation with non equal interval it learns the full progression trajectory from a set of incomplete record that only cover short segment of the progression it learns a compact set of medical concept a the bridge between the hidden progression process and the observed medical evidence which are usually extremely sparse and noisy we demonstrate the capability of our model by applying it to a real world copd patient cohort and deriving some interesting clinical insight 
given a social network can we quickly zoom out of the graph is there a smaller equivalent representation of the graph that preserve it propagation characteristic can we group node together based on their influence property these are important problem with application to influence analysis epidemiology and viral marketing application in this paper we first formulate a novel graph coarsening problem to find a succinct representation of any graph while preserving key characteristic for diffusion process on that graph we then provide a fast and effective near linear time in node and edge algorithm coarsenet for the same using extensive experiment on multiple real datasets we demonstrate the quality and scalability of coarsenet enabling u to reduce the graph by in some case without much loss of information finally we also show how our method can help in diverse application like influence maximization and detecting pattern of propagation at the level of automatically created group on real cascade data 
modeling the movement of information within social medium outlet like twitter is key to understanding to how idea spread but quantifying such movement run into several difficulty two specific area that elude a clear characterization are i the intrinsic random nature of individual to potentially adopt and subsequently broadcast a twitter topic and ii the dissemination of information via non twitter source such a news outlet and word of mouth and it impact on twitter propagation these distinct yet inter connected area must be incorporated to generate a comprehensive model of information diffusion we propose a bispace model to capture propagation in the union of exclusively twitter and non twitter environment to quantify the stochastic nature of twitter topic propagation we combine principle of geometric brownian motion and traditional network graph theory we apply poisson process function to model information diffusion outside of the twitter mention network we discus technique to unify the two sub model to accurately model information dissemination we demonstrate the novel application of these technique on real twitter datasets related to mass protest adoption in social community 
it s not often we get to peer into the detail of how big corporates use predictive modeling in practice in this talk sprint s head of predictive modeling tracey de poalo will talk about the process she developed using sa and logistic regression to build a wide range of model jeremy howard will discus his experience a a consultant at sprint comparing r and random forest to the existing process and will show the pro and con of each approach the talk will also cover the real world issue that tracey ha dealt with in creating a reusable process including data mart development sampling testing reporting and implementation with internal customer sprint s existing process is the best that jeremy ha seen in industry and show a lot of best practice in data structure documentation testing automation and customer interaction and modeling 
singular value decomposition svd is computationally costly and therefore a naive implementation doe not scale to the need of scenario where data evolves continuously while there are various on line analysis and incremental decomposition technique these may not accurately represent the data or may be slow for the need of many application to address these challenge in this paper we propose a low rank windowed incremental svd lwi svd algorithm which a leverage efficient and accurate low rank approximation to speed up incremental svd update and b us a window based approach to aggregate multiple incoming update insertion or deletion of row and column and thus reduces online processing cost we also present an lwi svd with restarts lwi svd algorithm which leverage a novel highly efficient partial reconstruction based change detection scheme to support timely refreshing of the decomposition with significant change in the data and prevent accumulation of error over time experiment result including comparison to other state of the art technique on different data set and under different parameter setting confirm that lwi svd and lwi svd are both efficient and accurate in maintaining decomposition 
we introduce a novel graphical model the collaborative score topic model cstm for personal recommendation of textual document cstm s chief novelty lie in it learned model of individual library or set of document associated with each user overall cstm is a joint directed probabilistic model of user item score rating and the textual side information in the user library and the item creating a generative description of score and the text allows cstm to perform well in a wide variety of data regime smoothly combining the side information with observed rating a the number of rating available for a given user range from none to many experiment on real world datasets demonstrate cstm s performance we further demonstrate it utility in an application for personal recommendation of poster which we deployed at the nip conference 
community detection is an important task for social network which help u understand the functional module on the whole network among different community detection method based on graph structure modularity based method are very popular recently but suffer a well known resolution limit problem this paper connects modularity based method with correlation analysis by subtly reformatting their math formula and investigates how to fully make use of correlation analysis to change the objective function of modularity based method which provides a more natural and effective way to solve the resolution limit problem in addition a novel theoretical analysis on the upper bound of different objective function help u understand their bias to different community size and experiment are conducted on both real life and simulated data to validate our finding 
the u department of defense dictionary of military term defines the information environment a the aggregate of individual organization and system that collect process disseminate or act on information the decision and action we take both a individual and collectively simultaneously shape and are shaped by the information environment in which we live the nature of our interaction with the information environment is rapidly evolving and old model are becoming irrelevant faster than we can develop new one this result in uncertainty that leave u exposed to dangerous influence without proper defense the purpose of this talk is to help frame a new science of information environment security y whose goal is to create and apply the tool needed to discover and maintain fundamental model of our ever changing information environment and to defend u in that environment both a individual and collectively against intentional a well a unintentional attempt to deceive misinform and otherwise manipulate u y is an interdisciplinary science that will require bringing together expert working in area such a cognitive science computer science social science security marketing political campaigning public policy and psychology to develop a theoretical a well a an applied engineering methodology for managing the full spectrum of information environment security issue 
we consider the problem of open domain question answering open qa over massive knowledge base kb existing approach use either manually curated kb like freebase or kb automatically extracted from unstructured text in this paper we present oqa the first approach to leverage both curated and extracted kb a key technical challenge is designing system that are robust to the high variability in both natural language question and massive kb oqa achieves robustness by decomposing the full open qa problem into smaller sub problem including question paraphrasing and query reformulation oqa solves these sub problem by mining million of rule from an unlabeled question corpus and across multiple kb oqa then learns to integrate these rule by performing discriminative training on question answer pair using a latent variable structured perceptron algorithm we evaluate oqa on three benchmark question set and demonstrate that it achieves up to twice the precision and recall of a state of the art open qa system 
social scientist increasingly criticize the use of machine learning technique to understand human behavior criticism include they are atheoretical and hence of limited scientific value they do not address causality and are hence of limited policy value and they are uninterpretable and hence of limited generalizability value outside context very narrowly similar to the training dataset these criticism i argue miss the enormous opportunity offered by ml technique to fundamentally improve the practice of empirical social science yet each criticism doe contain a grain of truth and overcoming them will require innovation to existing methodology some of these innovation are being developed today and some are yet to be tackled i will in this talk sketch what these innovation look like or should look like why they are needed and the technical challenge they raise i will illustrate my point using a set of application that range from financial market to social policy problem to computational model of basic psychological process this talk describes joint work with jon kleinberg and individual project with himabindu lakkaraju jure leskovec jens ludwig anuj shah chenhao tan mike yeoman and tom zimmerman 
rating data is ubiquitous on website such a amazon tripadvisor or yelp since rating are not static but given at various point in time a temporal analysis of rating data provides deeper insight into the evolution of a product s quality in this work we tackle the following question given the time stamped rating data for a product or service how can we detect the general rating behavior of user a well a time interval where the rating behave anomalous we propose a bayesian model that represents the rating data a sequence of categorical mixture model in contrast to existing method our method doe not require any aggregation of the input but it operates on the original time stamped data to capture the dynamic effect of the rating the categorical mixture are temporally constrained anomaly can occur in specific time interval only and the general rating behavior should evolve smoothly over time our method automatically determines the interval where anomaly occur and it capture the temporal effect of the general behavior by using a state space model on the natural parameter of the categorical distribution for learning our model we propose an efficient algorithm combining principle from variational inference and dynamic programming in our experimental study we show the effectiveness of our method and we present interesting discovery on multiple real world datasets 
in this paper we report the first empirical study and live test of the reserve price optimisation problem in the context of real time bidding rtb display advertising from an operational environment a reserve price is the minimum that the auctioneer would accept from bidder in auction and in a second price auction it could potentially uplift the auctioneer s revenue by charging winner the reserve price instead of the second highest bid a such it ha been used for sponsored search and been well studied in that context however comparing with sponsored search and contextual advertising this problem in the rtb context is le understood yet more critical for publisher because bidder have to submit a bid for each individual impression which mostly is associated with user data that is subject to change over time this coupled with practical constraint such a the budget campaign life time etc make the theoretical result from optimal auction theory not necessarily applicable and a further empirical study is required to confirm it optimality from the real world system in rtb an advertiser is facing nearly unlimited supply and the auction is almost done in last second which encourages spending le on the high cost ad placement this could imply the loss of bid volume over time if a correct reserve price is not in place in this paper we empirically examine several commonly adopted algorithm for setting up a reserve price we report our result of a large scale online experiment in a production platform the result suggest the our proposed game theory based oneshot algorithm performed the best and the superiority is significant in most case 
online health community are a valuable source of information for patient and physician however such user generated resource are often plagued by inaccuracy and misinformation in this work we propose a method for automatically establishing the credibility of user generated medical statement and the trustworthiness of their author by exploiting linguistic cue and distant supervision from expert source to this end we introduce a probabilistic graphical model that jointly learns user trustworthiness statement credibility and language objectivity we apply this methodology to the task of extracting rare or unknown side effect of medical drug this being one of the problem where large scale non expert data ha the potential to complement expert medical knowledge we show that our method can reliably extract side effect and filter out false statement while identifying trustworthy user that are likely to contribute valuable medical information 
large enterprise it information technology infrastructure component generate large volume of alert and incident ticket these are manually screened but it is otherwise difficult to extract information automatically from them to gain insight in order to improve operational efficiency we propose a framework to cluster alert and incident ticket based on the text in them using unsupervised machine learning this would be a step towards eliminating manual classification of the alert and incident which is very labor intense and costly our framework can handle the semi structured text in alert generated by it infrastructure component such a storage device network device server etc a well a the unstructured text in incident ticket created manually by operation support personnel after text pre processing and application of appropriate distance metric we apply different graph theoretic approach to cluster the alert and incident ticket based on their semi structured and unstructured text respectively for automated interpretation and read ability on semi structured text cluster we propose a method to visualize cluster that preserve the structure and human readability of the text data a compared to traditional word cloud where the text structure is not preserved for unstructured text cluster we find a simple way to define prototype of cluster for easy interpretation this framework for clustering and visualization will enable enterprise to prioritize the issue in their it infrastructure and improve the reliability and availability of their service 
clustering categorical data pose some unique challenge due to missing order and spacing among the category selecting a suitable similarity measure is a difficult task many existing technique require the user to specify input parameter which are difficult to estimate moreover many technique are limited to detect cluster in the full dimensional data space only few method exist for subspace clustering and they produce highly redundant result therefore we propose rocat relevant overlapping subspace cluster on categorical data a novel technique based on the idea of data compression following the minimum description length principle rocat automatically detects the most relevant subspace cluster without any input parameter the relevance of each cluster is validated by it contribution to compress the data optimizing the trade off between goodness of fit and model complexity rocat automatically determines a meaningful number of cluster to represent the data rocat is especially designed to detect subspace cluster on categorical data which may overlap in object and or attribute i e object can be assigned to different cluster in different subspace and attribute may contribute to different subspace containing cluster rocat naturally avoids undesired redundancy in cluster and subspace by allowing overlap only if it improves the compression rate extensive experiment demonstrate the effectiveness and efficiency of our approach 
short text clustering ha become an increasingly important task with the popularity of social medium like twitter google and facebook it is a challenging problem due to it sparse high dimensional and large volume characteristic in this paper we proposed a collapsed gibbs sampling algorithm for the dirichlet multinomial mixture model for short text clustering abbr to gsdmm we found that gsdmm can infer the number of cluster automatically with a good balance between the completeness and homogeneity of the clustering result and is fast to converge gsdmm can also cope with the sparse and high dimensional problem of short text and can obtain the representative word of each cluster our extensive experimental study show that gsdmm can achieve significantly better performance than three other clustering model 
a recent pandemic such a sars and the swine flu outbreak have shown disease spread very fast in today s interconnected world making public health an important research area some of the basic question are how can an outbreak be contained before it becomes an epidemic and what disease surveillance strategy should be implemented these problem have been studied traditionally using differential equation method which are amenable to analysis and closed form solution however these model are based on complete mixing assumption which do not hold for realistic population thereby limiting their utility in this tutorial we focus on an approach based on diffusion process on complex network this capture more realistic population but lead to novel mathematical and computational challenge the structure of the underlying network ha a significant impact on the dynamical property motivating the need for improved network model and efficient algorithm for computing network and dynamical property that scale to large network we provide an overview of the state of the art in computational epidemiology which is a multi disciplinary research area that overlap different area in computer science including data mining machine learning high performance computing and theoretical computer science a well a mathematics economics and statistic specifically we will discus mathematical and computational model problem of inference forecasting and state assessment and epidemic containment 
we consider the problem of collaborative permutation recovery i e recovering multiple permutation over object e g preference ranking over different option from limited pairwise comparison we tackle both the problem of how to recover multiple related permutation from limited observation and the active learning problem of which pairwise comparison query to ask so a to allow better recovery there ha been much work on recovering single permutation from pairwise comparison but we show that considering several related permutation jointly we can leverage their relatedness so a to reduce the number of comparison needed compared to reconstructing each permutation separately to do so we take a collaborative filtering matrix completion approach and use a trace norm or max norm regularized matrix learning model our approach can also be seen a a collaborative learning version of jamieson and nowak s recent work on constrained permutation recovery where instead of basing the recovery on known feature we learn the best feature de novo 
recent year have witnessed a proliferation of large scale knowledge graph such a freebase yago google s knowledge graph and microsoft s satori whereas there is a large body of research on mining homogeneous graph this new generation of information network are highly heterogeneous with thousand of entity and relation type and billion of instance of vertex and edge in this tutorial we will present the state of the art in constructing mining and growing knowledge graph the purpose of the tutorial is to equip newcomer to this exciting field with an understanding of the basic concept tool and methodology available datasets and open research challenge a publicly available knowledge base freebase will be used throughout the tutorial to exemplify the different technique 
in many application we have a social network of people and would like to identify the member of an interesting but unlabeled group or community we start with a small number of exemplar group member they may be follower of a political ideology or fan of a music genre and need to use those example to discover the additional member this problem give rise to the seed expansion problem in community detection given example community member how can the social graph be used to predict the identity of remaining hidden community member in contrast with global community detection graph partitioning or covering seed expansion is best suited for identifying community locally concentrated around node of interest a growing body of work ha used seed expansion a a scalable mean of detecting overlapping community yet despite growing interest in seed expansion there are divergent approach in the literature and there still isn t a systematic understanding of which approach work best in different domain here we evaluate several variant and uncover subtle trade offs between different approach we explore which property of the seed set can improve performance focusing on heuristic that one can control in practice a a consequence of this systematic understanding we have found several opportunity for performance gain we also consider an adaptive version in which request are made for additional membership label of particular node such a one find in field study of social community this lead to interesting connection and contrast with active learning and the trade offs of exploration and exploitation finally we explore topological property of community and seed set that correlate with algorithm performance and explain these empirical observation with theoretical one we evaluate our method across multiple domain using publicly available datasets with labeled ground truth community 
we consider a search task a a set of query that serve the same user information need analyzing search task from user query stream play an important role in building a set of modern tool to improve search engine performance in this paper we propose a probabilistic method for identifying and labeling search task based on the following intuitive observation query that are issued temporally close by user in many sequence of query are likely to belong to the same search task meanwhile different user having the same information need tend to submit topically coherent search query to capture the above intuition we directly model query temporal pattern using a special class of point process called hawkes process and combine topic model with hawkes process for simultaneously identifying and labeling search task essentially hawkes process utilize their self exciting property to identify search task if influence exists among a sequence of query for individual user while the topic model exploit query co occurrence across different user to discover the latent information needed for labeling search task more importantly there is mutual reinforcement between hawkes process and the topic model in the unified model that enhances the performance of both we evaluate our method based on both synthetic data and real world query log data in addition we also apply our model to query clustering and search task identification by comparing with state of the art method the result demonstrate that the improvement in our proposed approach is consistent and promising 
we often care about people s degree of belief about certain event e g causality between an action and the outcome odds distribution among the outcome of a horse race and so on it is well recognized that the best form to elicit opinion from human is probability distribution instead of simple voting because the form of distribution retains the delicate information that an opinion express in the past opinion elicitation ha relied on expert who are expensive and not always available more recently crowdsourcing ha gained prominence a an inexpensive way to get a great deal of human input however traditional crowdsourcing ha primarily focused on issuing very simple e g binary decision task to the crowd in this paper we study how to use crowd for opinion elicitation there are three major challenge to eliciting opinion information in the form of probability distribution how to measure the quality of distribution how to aggregate the distribution and how to strategically implement such a system to address these challenge we design and implement cope crowd powered opinion elicitation market cope model crowdsourced work a a trading market where the worker behave like trader to maximize their profit by presenting their opinion among the innovative feature in this system we design cope updating to combine the multiple elicited distribution following a bayesian scheme also to provide more flexibility while running cope we propose a series of efficient algorithm and a slope based strategy to manage the ending condition of cope we then demonstrate the implementation of cope and report experimental result running on real commercial platform to demonstrate the practical value of this system 
inferring phenotypic pattern from population scale clinical data is a core computational task in the development of personalized medicine one important source of data on which to conduct this type of research is patient electronic medical record emr however the patient emrs are typically sparse and noisy which creates significant challenge if we use them directly to represent patient phenotype in this paper we propose a data driven phenotyping framework called pacifier patient record densifier where we interpret the longitudinal emr data of each patient a a sparse matrix with a feature dimension and a time dimension and derive more robust patient phenotype by exploring the latent structure of those matrix specifically we assume that each derived phenotype is composed of a subset of the medical feature contained in original patient emr whose value evolves smoothly over time we propose two formulation to achieve such goal one is individual basis approach iba which assumes the phenotype are different for every patient the other is shared basis approach sba which assumes the patient population share a common set of phenotype we develop an efficient optimization algorithm that is capable of resolving both problem efficiently finally we validate pacifier on two real world emr cohort for the task of early prediction of congestive heart failure chf and end stage renal disease esrd our result show that the predictive performance in both task can be improved significantly by the proposed algorithm average auc score improved from to on chf and from to on esrd respectively on diagnosis group granularity we also illustrate some interesting phenotype derived from our data 
selecting an informative subset of feature ha important application in many data mining task especially for high dimensional data recently simultaneous selection of feature and feature group a k a bi level selection becomes increasingly popular since it not only reduces the number of feature but also unveils the underlying grouping effect in the data which is a valuable functionality in many application such a bioinformatics and web data mining one major challenge of bi level selection or even feature selection only is that computing a globally optimal solution requires a prohibitive computational cost to overcome such a challenge current research mainly fall into two category the first one focus on finding suitable continuous computational surrogate for the discrete function and this lead to various convex and nonconvex optimization model although efficient convex model usually deliver sub optimal performance while nonconvex model on the other hand require significantly more computational effort another direction is to use greedy algorithm to solve the discrete optimization directly however existing algorithm are proposed to handle single level selection only and it remains challenging to extend these method to handle bi level selection in this paper we fulfill this gap by introducing an efficient sparse group hard thresholding algorithm our main contribution are we propose a novel bi level selection model and show that the key combinatorial problem admits a globally optimal solution using dynamic programming we provide an error bound between our solution and the globally optimal under the rip restricted isometry property theoretical framework our experiment on synthetic and real data demonstrate that the proposed algorithm produce encouraging performance while keeping comparable computational efficiency to convex relaxation model 
internet display advertising is a critical revenue source for publisher and online content provider and is supported by massive amount of user and publisher data targeting display ad can be improved substantially with machine learning method but building many model on massive data becomes prohibitively expensive computationally this paper present a combination of strategy deployed by the online advertising firm dstillery for learning many model from extremely high dimensional data efficiently and without human intervention this combination includes i a method for simple yet effective transfer learning where a model learned from data that is relatively abundant and cheap is taken a a prior for bayesian logistic regression trained with stochastic gradient descent sgd from the more expensive target data ii a new update rule for automatic learning rate adaptation to support learning from sparse high dimensional data a well a the integration with adaptive regularization we present an experimental analysis across different ad campaign showing that the transfer learning indeed improves performance across a large number of them especially at the start of the campaign the combined hand free method need no fiddling with the sgd learning rate and we show that it is just a effective a using expensive grid search to set the regularization parameter for each campaign 
strategic planning and talent management in large enterprise composed of knowledge worker requires complete accurate and up to date representation of the expertise of employee in a form that integrates with business process like other similar organization operating in dynamic environment the ibm corporation strives to maintain such current and correct information specifically assessment of employee against job role and skill set from it expertise taxonomy in this work we deploy an analytics driven solution that infers the expertise of employee through the mining of enterprise and social data that is not specifically generated and collected for expertise inference we consider job role and specialty prediction and pose them a supervised classification problem we evaluate a large number of feature set predictive model and postprocessing algorithm and choose a combination for deployment this expertise analytics system ha been deployed for key employee population segment yielding large reduction in manual effort and the ability to continually and consistently serve up to date and accurate data for several business function this expertise management system is in the process of being deployed throughout the corporation 
web site owner from small web site to the largest property that include amazon facebook google linkedin microsoft and yahoo attempt to improve their web site optimizing for criterion ranging from repeat usage time on site to revenue having been involved in running thousand of controlled experiment at amazon booking com linkedin and multiple microsoft property we share seven rule of thumb for experimenter which we have generalized from these experiment and their result these are principle that we believe have broad applicability in web optimization and analytics outside of controlled experiment yet they are not provably correct and in some case exception are known to support these rule of thumb we share multiple real example most being shared in a public paper for the first time some rule of thumb have previously been stated such a speed matter but we describe the assumption in the experimental design and share additional experiment that improved our understanding of where speed matter more certain area of the web page are more critical this paper serf two goal first it can guide experimenter with rule of thumb that can help them optimize their site second it provides the kdd community with new research challenge on the applicability exception and extension to these one of the goal for kdd s industrial track 
chronic obstructive pulmonary disease copd is a lung disease characterized by airflow limitation usually associated with an inflammatory response to noxious particle such a cigarette smoke copd is currently the third leading cause of death in the united state and is the only leading cause of death that is increasing in prevalence it also represents an enormous financial burden to society costing ten of billion of dollar annually in the u s it is widely accepted by the medical community that copd is a heterogeneous disease with substantial evidence indicating that genetic variation contributes to varying level of disease susceptibility this heterogeneity make it difficult to predict health decline and develop targeted treatment for better patient care although researcher have made several attempt to discover disease subtypes result have been inconclusive in part because standard clustering method have not properly dealt with disease manifestation that may worsen with increased exposure in this paper we introduce a transformative way of looking at the copd subtyping task specifically we model the relationship between risk factor such a age and smoke exposure and manifestation of disease severity using gaussian process which allow u to represent so called disease trajectory we also posit that individual can be associated with multiple disease type latent cluster which we assume are influenced by genetics furthermore we predict that only subset of the numerous disease related quantitative feature are useful for describing each latent subtype we model these association using two separate beta process prior and we describe a variational inference approach to discover the most probable latent cluster assignment result are validated with association to genetic marker 
million of people use social network everyday to talk about a variety of subject publish opinion and share information understanding this data to infer user s topical interest is a challenging problem with application in various data powered product in this paper we present lasta large scale topic assignment a full production system used at klout inc which mine topical interest from five social network and assigns over topic to hundred of million of user on a daily basis the system continuously collect stream of user data and is reactive to fresh information updating topic for user a interest shift lasta generates over distinct feature derived from signal such a user generated post and profile user reaction such a comment and retweets user attribution such a list tag and endorsement a well a signal based on social graph connection we show that using this diverse set of feature lead to a better representation of a user s topical interest a compared to using only generated text or only graph based feature we also show that using cross network information for a user lead to a more complete and accurate understanding of the user s topic a compared to using any single network we evaluate lasta s topic assignment system on an internal labeled corpus of user topic label generated from real user 
the analysis of network connection diffusion process and cascade requires evaluating property of the diffusion network property of interest often involve variable that are not explicitly observed in real world diffusion connection strength in the network and diffusion path of infection over the network are example of such hidden variable these hidden variable therefore need to be estimated for these property to be evaluated in this paper we propose and study this novel problem in a bayesian framework by capturing the posterior distribution of these hidden variable given the observed cascade and computing the expectation of these property under this posterior distribution we identify and characterize interesting network diffusion property whose expectation can be computed exactly and efficiently either wholly or in part for property that are not nice in this sense we propose a gibbs sampling framework for monte carlo integration in detailed experiment using various network diffusion property over multiple synthetic and real datasets we demonstrate that the proposed approach is significantly more accurate than a frequentist plug in baseline we also propose a map reduce implementation of our framework and demonstrate that this can analyze cascade with million of infection in minute 
many clustering method partition the data group based on the input data similarity matrix thus the clustering result highly depend on the data similarity learning because the similarity measurement and data clustering are often conducted in two separated step the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal result in this paper we propose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously our new model learns the data similarity matrix by assigning the adaptive and optimal neighbor for each data point based on the local distance meanwhile the new rank constraint is imposed to the laplacian matrix of the data similarity matrix such that the connected component in the resulted similarity matrix are exactly equal to the cluster number we derive an efficient algorithm to optimize the proposed challenging problem and show the theoretical analysis on the connection between our method and the k mean clustering and spectral clustering we also further extend the new clustering model for the projected clustering to handle the high dimensional data extensive empirical result on both synthetic data and real world benchmark data set show that our new clustering method consistently outperforms the related clustering approach 
the hierarchical dirichlet process hdp is an intuitive and elegant technique to model data with latent group however it ha not been widely used for practical application due to the high computational cost associated with inference in this paper we propose an effective parallel gibbs sampling algorithm for hdp by exploring it connection with the gamma gamma poisson process specifically we develop a novel framework that combine bootstrap and reversible jump mcmc algorithm to enable parallel variable update we also provide theoretical convergence analysis based on gibbs sampling with asynchronous variable update experiment result on both synthetic datasets and two large scale text collection show that our algorithm can achieve considerable speedup a well a better inference accuracy for hdp compared with existing parallel sampling algorithm 
behavioral pattern discovery is increasingly being studied to understand human behavior and the discovered pattern can be used in many real world application such a web search recommender system and advertisement targeting traditional method usually consider the behavior a simple user and item connection or represent them with a static model in real world however human behavior are actually complex and dynamic they include correlation between user and multiple type of object and also continuously evolve along time these characteristic cause severe data sparsity and computational complexity problem which pose great challenge to human behavioral analysis and prediction in this paper we propose a flexible evolutionary multi faceted analysis fema framework for both behavior prediction and pattern mining fema utilizes a flexible and dynamic factorization scheme for analyzing human behavioral data sequence which can incorporate various knowledge embedded in different object domain to alleviate the sparsity problem we give approximation algorithm for efficiency where the bound of approximation loss is theoretically proved we extensively evaluate the proposed method in two real datasets for the prediction of human behavior the proposed fema significantly outperforms other state of the art baseline method by moreover fema is able to discover quite a number of interesting multi faceted temporal pattern on human behavior with good interpretability more importantly it can reduce the run time from hour to minute which is significant for industry to serve real time application 
this paper present an efficient active transductive approach for classification a common approach of active learning algorithm is to focus on querying point near the class boundary in order to refine it however for certain data distribution this approach ha been shown to lead to uninformative sample more recent approach consider combining data exploration with traditional refinement technique these technique typically require tuning sampling of unexplored region with refinement of detected class boundary they also involve significant computational cost for the exploration of informative query candidate we present a novel iterative active learning algorithm designed to overcome these shortcoming by using a linear running time active transductive learning approach that naturally switch from exploration to refinement the passive classifier employed in our algorithm build a random walk on the data graph based on a modified graph geometry that combine the data distribution with current label hypothesis while the query component us the uncertainty of the evolving hypothesis our supporting theory draw the link between the spectral property of our iteration matrix and a solution to the minimal cut problem for a fused hypothesis data graph experiment demonstrate computational complexity that is order of magnitude lower than state of the art and competitive result on benchmark data and real churn prediction data 
spreadsheet contain valuable data on many topic however spreadsheet are difficult to integrate with other data source converting spreadsheet data to the relational model would allow data analyst to use relational integration tool we propose a two phase semiautomatic system that extract accurate relational metadata while minimizing user effort based on an undirected graphical model our system enables downstream spreadsheet integration application first the automatic extractor us hint from spreadsheet graphical style and recovered metadata to extract the spreadsheet data a accurately a possible second the interactive repair identifies similar region in distinct spreadsheet scattered across large spreadsheet corpus allowing a user s single manual repair to be amortized over many possible extraction error our experiment show that a human can obtain the accurate extraction with just of the manual operation required by a standard classification based technique on two real world datasets 
recommendation and review site offer a wealth of information beyond rating for instance on imdb user leave review commenting on different aspect of a movie e g actor plot visual effect and expressing their sentiment positive or negative on these aspect in their review this suggests that uncovering aspect and sentiment will allow u to gain a better understanding of user movie and the process involved in generating rating the ability to answer question such a doe this user care more about the plot or about the special effect or what is the quality of the movie in term of acting help u to understand why certain rating are generated this can be used to provide more meaningful recommendation in this work we propose a probabilistic model based on collaborative filtering and topic modeling it allows u to capture the interest distribution of user and the content distribution for movie it provides a link between interest and relevance on a per aspect basis and it allows u to differentiate between positive and negative sentiment on a per aspect basis unlike prior work our approach is entirely unsupervised and doe not require knowledge of the aspect specific rating or genre for inference we evaluate our model on a live copy crawled from imdb our model offer superior performance by joint modeling moreover we are able to address the cold start problem by utilizing the information inherent in review our model demonstrates improvement for new user and movie 
in data mining application such a crowdsourcing and privacy preserving data mining one may wish to obtain consolidated prediction out of multiple model without access to feature of the data besides multiple model usually carry complementary predictive information model combination can potentially provide more robust and accurate prediction by correcting independent error from individual model various method have been proposed to combine prediction such that the final prediction are maximally agreed upon by multiple base model though this maximum consensus principle ha been shown to be successful simply maximizing consensus can lead to le discriminative prediction and overfit the inevitable noise due to imperfect base model we argue that proper regularization for model combination approach is needed to alleviate such overfitting effect specifically we analyze the hypothesis space of several model combination method and identify the trade off between model consensus and generalization ability we propose a novel model called regularized consensus maximization rcm which is formulated a an optimization problem to combine the maximum consensus and large margin principle we theoretically show that rcm ha a smaller upper bound on generalization error compared to the version without regularization experiment show that the proposed algorithm outperforms a wide spectrum of state of the art model combination method on task 
large scale data center network are complex comprising several thousand network device and several hundred thousand link and form the critical infrastructure upon which all higher level service depend on despite the built in redundancy in data center network performance issue and device or link failure in the network can lead to user perceived service interruption therefore determining and localizing user impacting availability and performance issue in the network in near real time is crucial traditionally both passive and active monitoring approach have been used for failure localization however data from passive monitoring is often too noisy and doe not effectively capture silent or gray failure whereas active monitoring is potent in detecting fault but limited in it ability to isolate the exact fault location depending on it scale and granularity our key idea is to use statistical data mining technique on large scale active monitoring data to determine a ranked list of suspect cause which we refine with passive monitoring signal in particular we compute a failure probability for device and link in near real time using data from active monitoring and look for statistically significant increase in the failure probability we also correlate the probabilistic output with other failure signal from passive monitoring to increase the confidence of the probabilistic analysis we have implemented our approach in the window azure production environment and have validated it effectiveness in term of localization accuracy precision and time to localization using known network incident over the past three month the correlated ranked list of device and link is surfaced a a report that is used by network operator to investigate current issue and identify probable root cause 
online social network offering various service have become ubiquitous in our daily life meanwhile user nowadays are usually involved in multiple online social network simultaneously to enjoy specific service provided by different network formally social network that share some common user are named a partially aligned network in this paper we want to predict the formation of social link in multiple partially aligned social network at the same time which is formally defined a the multi network link formation prediction problem in multiple partially aligned social network user can be extensively correlated with each other by various connection to categorize these diverse connection among user intra network social meta path and category of inter network social meta path are proposed in this paper these social meta path can cover a wide variety of connection information in the network some of which can be helpful for solving the multi network link prediction problem but some can be not to utilize useful connection a subset of the most informative social meta path are picked the process of which is formally defined a social meta path selection in this paper an effective general link formation prediction framework mli multi network link identifier is proposed in this paper to solve the multi network link formation prediction problem built with heterogenous topological feature extracted based on the selected social meta path in the multiple partially aligned social network mli can help refine and disambiguate the prediction result reciprocally in all aligned network extensive experiment conducted on real world partially aligned heterogeneous network foursquare and twitter demonstrate that mli can solve the multi network link prediction problem very well 
identifying interpretable discriminative high order feature interaction given limited training data in high dimension is challenging in both machine learning and data mining in this paper we propose a factorization based sparse learning framework termed fhim for identifying high order feature interaction in linear and logistic regression model and study several optimization method for solving them unlike previous sparse learning method our model fhim recovers both the main effect and the interaction term accurately without imposing tree structured hierarchical constraint furthermore we show that fhim ha oracle property when extended to generalized linear regression model with pairwise interaction experiment on simulated data show that fhim outperforms the state of the art sparse lear ning technique further experiment on our experimentally generated data from patient blood sample using a novel somamer slow off rate modified aptamer technology show that fhim performs blood based cancer diagnosis and bio marker discovery for renal cell carcinoma much better than other competing method and it identifies interpretable block wise high order gene interaction predictive of cancer stage of sample a literature survey show that the interaction identified by fhim play important role in cancer development 
purchasing decision in many product category are heavily influenced by the shopper s aesthetic preference it s insufficient to simply match a shopper with popular item from the category in question a successful shopping experience also identifies product that match those aesthetic the challenge of capturing shopper style becomes more difficult a the size and diversity of the marketplace increase at etsy an online marketplace for handmade and vintage good with over million diverse listing the problem of capturing taste is particularly important user come to the site specifically to find item that match their eclectic style in this paper we describe our method and experiment for deploying two new style based recommender system on the etsy site we use latent dirichlet allocation lda to discover trending category and style on etsy which are then used to describe a user s interest profile we also explore hashing method to perform fast nearest neighbor search on a map reduce framework in order to efficiently obtain recommendation these technique have been implemented successfully at very large scale substantially improving many key business metric 
core decomposition ha proven to be a useful primitive for a wide range of graph analysis one of it most appealing feature is that unlike other notion of dense subgraphs it can be computed linearly in the size of the input graph in this paper we provide an analogous tool for uncertain graph i e graph whose edge are assigned a probability of existence the fact that core decomposition can be computed efficiently in deterministic graph doe not guarantee efficiency in uncertain graph where even the simplest graph operation may become computationally intensive here we show that core decomposition of uncertain graph can be carried out efficiently a well we extensively evaluate our definition and method on a number of real world datasets and application such a influence maximization and task driven team formation 
solving the missing value mv problem with small estimation error in big data environment is a notoriously resource demanding task a datasets and their user community continuously grow the problem can only be exacerbated assume that it is possible to have a single machine godzilla which can store the massive dataset and support an ever growing community submitting mv imputation request is it possible to replace godzilla by employing a large number of cohort machine so that imputation can be performed much faster engaging cohort in parallel each of which access much smaller partition of the original dataset if so it would be preferable for obvious performance reason to access only a subset of all cohort per imputation in this case can we decide swiftly which is the desired subset of cohort to engage per imputation but efficiency and scalability is just one key concern is it possible to do the above while ensuring comparable or even better than godzilla s imputation estimation error in this paper we derive answer to these fundamental question and develop principled method and a framework which offer large performance speed ups and better or comparable error to that of godzilla independently of which missing value imputation algorithm is used our contribution involve pythia a framework and algorithm for providing the answer to the above question and for engaging the appropriate subset of cohort per mv imputation request pythia functionality rest on two pillar i dataset partition signature one per cohort and ii similarity notion and algorithm which can identify the appropriate subset of cohort to engage comprehensive experimentation with real and synthetic datasets showcase our efficiency scalability and accuracy claim 
the detection of abnormal moving object over high volume trajectory stream is critical for real time application ranging from military surveillance to transportation management yet this problem remains largely unexplored in this work we first propose class of novel trajectory outlier definition that model the anomalous behavior of moving object for a large range of real time application our theoretical analysis and empirical study on the beijing taxi and gmti ground moving target indicator datasets demonstrate it effectiveness in capturing abnormal moving object furthermore we propose a general strategy for efficiently detecting the new outlier class it feature three fundamental optimization principle designed to minimize the detection cost our comprehensive experimental study demonstrate that our proposed strategy drive the detection cost fold down into practical realm for application producing high volume trajectory stream to utilize 
e commerce ha largely been a pull model to date offline retailer have nailed discovery delight serendipity and impulse purchase in person with greater success than online commerce site however in an always on mobile first world company like groupon have the opportunity to push the frontier even further than offline retailer or comprehensive site due to the fact that our smartphones are always with u the challenge is to provide the right deal to the right user at the right time that involves learning about the user and their location their personal preference and predicting which deal are likely to delight them presenting diversity discovery and engaging ux to gather user preference and semantic graph approach for user deal matching this presentation will give insight into how groupon manages to grapple with these challenge via a data driven system in order to delight and surprise customer 
in the era of ehrs it is possible to examine the outcome of decision made by doctor during clinical practice to identify pattern of care generating evidence based on the collective practice of expert we will discus method that use unstructured patient data to monitor for adverse drug event profile specific drug identify off label drug usage uncover natural experiment and generate practice based evidence for difficult to test clinical hypothesis we will describe how to detect association among drug and their adverse event several year before an alert is issued a well a compute the true rate of drug drug interaction we will present approach to identify novel off label us of drug using the patient feature matrix along with prior knowledge about drug disease and known usage we will review a natural experiment where a subset of congestive heart failure patient who were prescribed cilostazol despite it black box warning and profile it safety we will discus the testing of a clinical hypothesis about an association between allergic condition and chronic uveitis in patient with juvenile idiopathic arthritis 
deep learning ha rapidly moved from a marginal approach in the machine learning community le than ten year ago to one that ha strong industrial impact in particular for high dimensional perceptual data such a speech and image but also natural language the demand for expert in deep learning is growing very fast faster than we can graduate phd thereby considerably increasing their market value deep learning is based on the idea of learning multiple level of representation with higher level computed a a function of lower level and corresponding to more abstract concept automatically discovered by the learner deep learning arose out of research on artificial neural network and graphical model and the literature on that subject ha considerably grown in recent year culminating in the creation of a dedicated conference iclr the tutorial will introduce some of the basic algorithm both on the supervised and unsupervised side a well a discus some of the guideline for successfully using them in practice finally it will introduce current research question regarding the challenge of scaling up deep learning to much larger model that can successfully extract information from huge datasets 
with the rapid development of online social network a growing number of people are willing to share their group activity e g having dinner with colleague and watching movie with spouse this motivates the study on group recommendation which aim to recommend item for a group of user group recommendation is a challenging problem because different group member have different preference and how to make a trade off among their preference for recommendation is still an open problem in this paper we propose a probabilistic model named com consensus model to model the generative process of group activity and make group recommendation intuitively user in a group may have different influence and those who are expert in topic relevant to the group are usually more influential in addition user in a group may behave differently a group member from a individual com is designed based on these intuition and is able to incorporate both user selection history and personal consideration of content factor when making recommendation com estimate the preference of a group to an item by aggregating the preference of the group member with different weight we conduct extensive experiment on four datasets and the result show that the proposed model is effective in making group recommendation and outperforms baseline method significantly 
the objective in extreme multi label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of label from a large label set extreme multi label classification is an important research problem since not only doe it enable the tackling of application with many label but it also allows the reformulation of ranking problem with certain advantage over existing formulation our objective in this paper is to develop an extreme multi label classifier that is faster to train and more accurate at prediction than the state of the art multi label random forest mlrf algorithm and the label partitioning for sub linear ranking lpsr algorithm mlrf and lpsr learn a hierarchy to deal with the large number of label but optimize task independent measure such a the gini index or clustering error in order to learn the hierarchy our proposed fastxml algorithm achieves significantly higher accuracy by directly optimizing an ndcg based ranking loss function we also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation experiment reveal that fastxml can be trained on problem with more than a million label on a standard desktop in eight hour using a single core and in an hour using multiple core 
processing large volume of streaming data in near real time is becoming increasingly important a the internet sensor network and network traffic grow online machine learning is a typical mean of dealing with streaming data since it allows the classification model to learn one instance of data at a time although many online learning method have been developed since the development of the perceptron algorithm existing online method assume that the number of class is available in advance of classification process however this assumption is unrealistic for large scale or streaming data set this work proposes an online chinese restaurant process crp algorithm which is an online and nonparametric algorithm to tackle this problem this work proposes a relaxing function a part of the prior and update the parameter with the likelihood function in term of the consistency between the true label information and predicted result this work present two gibbs sampling algorithm to perform posterior inference in the experiment the online crp is applied to three massive data set and compared with several online learning and batch learning algorithm one of the data set is obtained from wikipedia which comprises approximately two million document the experimental result reveal that the proposed online crp performs well and efficiently on massive data set finally this work proposes two method to update the hyperparameter alpha of the online crp the first method is based on the posterior distribution of alpha and the second exploit the property of online learning namely adapting to change to adjust alpha dynamically 
crime reduction and prevention strategy are essential to increase public safety and reduce the crime cost to society law enforcement agency have long realized the importance of analyzing co offending network network of offender who have committed crime together for this purpose although network structure can contribute significantly to co offence prediction research in this area is very limited here we address this important problem by proposing a framework for co offence prediction using supervised learning considering the available information about offender we introduce social geographic geo social and similarity feature set which are used for classifying potential negative and positive pair of offender similar to other social network co offending network also suffer from a highly skewed distribution of positive and negative pair to address the class imbalance problem we identify three type of criminal cooperation opportunity which help to reduce the class imbalance ratio significantly while keeping half of the co offence the proposed framework is evaluated on a large crime dataset for the province of british columbia canada our experimental evaluation of four different feature set show that the novel geo social feature are the best predictor overall we experimentally show the high effectiveness of the proposed co offence prediction framework we believe that our framework will not only allow law enforcement agency to improve their crime reduction and prevention strategy but also offer new criminological insight into criminal link formation between offender 
accurate knowledge of a patient s disease state and trajectory is critical in a clinical setting modern electronic healthcare record contain an increasingly large amount of data and the ability to automatically identify the factor that influence patient outcome stand to greatly improve the efficiency and quality of care we examined the use of latent variable model viz latent dirichlet allocation to decompose free text hospital note into meaningful feature and the predictive power of these feature for patient mortality we considered three prediction regime baseline prediction dynamic time varying outcome prediction and retrospective outcome prediction in each our prediction task differs from the familiar time varying situation whereby data accumulates since fewer patient have long icu stay a we move forward in time fewer patient are available and the prediction task becomes increasingly difficult we found that latent topic derived feature were effective in determining patient mortality under three timeline inhospital day post discharge and year post discharge mortality our result demonstrated that the latent topic feature important in predicting hospital mortality are very different from those that are important in post discharge mortality in general latent topic feature were more predictive than structured feature and a combination of the two performed best the time varying model that combined latent topic feature and baseline feature had auc that reached and for in hospital day post discharge and year post discharge mortality respectively our result agreed with other work suggesting that the first hour of patient information are often the most predictive of hospital mortality retrospective model that used a combination of latent topic feature and structured feature achieved auc of and for in hospital day and year mortality prediction our work focus on the dynamic time varying setting because model from this regime could facilitate an on going severity stratification system that help direct care staff resource and inform treatment strategy 
analyzing biomedical big data bbd is computationally expensive due to high dimensionality and large data volume performance and scalability issue of traditional database management system dbms often limit the usage of more sophisticated and complex data query and analytic model moreover in the conventional setting data management and analysis use separate software platform exporting and importing large amount of data across platform require a significant amount of computational and i o resource a well a potentially putting sensitive data at a security risk in this tutorial the participant will learn the difference between in memory dbms and traditional dbms through hand on exercise using sap s cloud based hana in memory dbms in conjunction with the multi parameter intelligent monitoring in intensive care mimic dataset mimic is an open access critical care ehr archive over tb in size and consists of structured unstructured and waveform data furthermore this tutorial will seek to educate the participant on how a combination of dynamic querying and in memory dbms may enhance the management and analysis of complex clinical data 
the rapid growth of information source on the web ha intensified the problem of data quality in particular the same real world entity may be described by different source in various way with overlapping information and possibly conflicting or even erroneous value in order to obtain a more complete and accurate picture for a real world entity we need to collate the data record that refer to the entity a well a correct any erroneous value we observe that these two task are often tightly coupled rectifying erroneous value will facilitate data collation while linking similar record provides u with a clearer view of the data and additional evidence for error correction in this paper we present a framework called comet that interleaf record linkage with error correction taking into consideration the source reliability on various attribute the proposed framework first utilizes confidence based matching to discriminate record in term of ambiguity and source reliability then it performs adaptive matching to reduce the impact of erroneous value experiment result demonstrate that comet outperforms the state of the art technique and is able to build complete and accurate profile for real world entity 
given a class of large number of student each exhibiting a different ability level how can we group them into section so that the overall gain for student is maximized this question ha been a topic of central concern and debate amongst social scientist and policy maker for a long time we propose a framework for rigorously studying this question taking a computational perspective we present a formal definition of the grouping problem and investigate some of it variant such variant are determined by the desired number of group a well a the definition of the gain for each student in the group we focus on two natural instantiation of the gain function and we show that for both of them the problem of identifying a single group of student that maximizes the gain among it member can be solved in polynomial time the corresponding partitioning problem where the goal is to partition the student into non overlapping group appear to be much harder however the algorithm for the single group version can be leveraged for solving the more complex partitioning problem our experiment with generated data coming from different distribution demonstrate that our algorithm is significantly better than the current strategy in vogue for dividing student in a class into section 
distance query are a basic tool in data analysis they are used for detection and localization of change for the purpose of anomaly detection monitoring or planning distance query are particularly useful when data set such a measurement snapshot of a system content traffic matrix and activity log are collected repeatedly random sampling which can be efficiently performed over streamed or distributed data is an important tool for scalable data analysis the sample constitutes an extremely flexible summary which naturally support domain query and scalable estimation of statistic which can be specified after the sample is generated the effectiveness of a sample a a summary however hinge on the estimator we have we derive novel estimator for estimating l p distance from sampled data our estimator apply with the most common weighted sampling scheme poisson probability proportional to size pps and it fixed sample size variant they also apply when the sample of different data set are independent or coordinated our estimator are admissible pareto optimal in term of variance and have compelling property we study the performance of our manhattan and euclidean distance p estimator on diverse datasets demonstrating scalability and accuracy even when a small fraction of the data is sampled our work for the first time facilitates effective distance estimation over sampled data 
people s interest are dynamically evolving often affected by external factor such a trend promoted by the medium or adopted by their friend in this work we model interest evolution through dynamic interest cascade we consider a scenario where a user s interest may be affected by a the interest of other user in her social circle a well a b suggestion she receives from a recommender system in the latter case we model user reaction through either attraction or aversion towards past suggestion we study this interest evolution process and the utility accrued by recommendation a a function of the system s recommendation strategy we show that in steady state the optimal strategy can be computed a the solution of a semi definite program sdp using datasets of user rating we provide evidence for the existence of aversion and attraction in real life data and show that our optimal strategy can lead to significantly improved recommendation over system that ignore aversion and attraction 
given a large collection of epidemiological data consisting of the count of d contagious disease for l location of duration n how can we find pattern rule and outlier for example the project tycho provides open access to the count infection for u s state from to for contagious disease e g measles influenza which include missing value possible recording error sudden spike or dive of infection etc so how can we find a combined model for all these disease location and time tick in this paper we present funnel a unifying analytical model for large scale epidemiological data a well a a novel fitting algorithm funnelfit which solves the above problem our method ha the following property a sense making it detects important pattern of epidemic such a periodicity the appearance of vaccine external shock event and more b parameter free our modeling framework free the user from providing parameter value c scalable funnelfit is carefully designed to be linear on the input size d general our model is general and practical which can be applied to various type of epidemic including computer virus propagation a well a human disease extensive experiment on real data demonstrate that funnelfit doe indeed discover important property of epidemic p disease seasonality e g influenza spike in january lyme disease spike in july and the absence of yearly periodicity for gonorrhea p disease reduction effect e g the appearance of vaccine p local state level sensitivity e g many measles case in ny p external shock event e g historical flu pandemic p detect incongruous value i e data reporting error 
this paper is concerned with the problem of personalized diversification of search result with the goal of enhancing the performance of both plain diversification and plain personalization algorithm in previous work the problem ha mainly been tackled by mean of unsupervised learning to further enhance the performance we propose a supervised learning strategy specifically we set up a structured learning framework for conducting supervised personalized diversification in which we add feature extracted directly from the token of document and those utilized by unsupervised personalized diversification algorithm and importantly those generated from our proposed user interest latent dirichlet topic model based on our proposed topic model whether a document can cater to a user s interest can be estimated in our learning strategy we also define two constraint in our structured learning framework to ensure that search result are both diversified and consistent with a user s interest we conduct experiment on an open personalized diversification dataset and find that our supervised learning strategy outperforms unsupervised personalized diversification method a well a other plain personalization and plain diversification method 
hashing ha enjoyed a great success in large scale similarity search recently researcher have studied the multi modal hashing to meet the need of similarity search across different type of medium however most of the existing method are applied to search across multi view among which explicit bridge information is provided given a heterogeneous medium search task we observe that abundant multi view data can be found on the web which can serve a an auxiliary bridge in this paper we propose a heterogeneous translated hashing hth method with such auxiliary bridge incorporated not only to improve current multi view search but also to enable similarity search across heterogeneous medium which have no direct correspondence hth simultaneously learns hash function embedding heterogeneous medium into different hamming space and translator aligning these space unlike almost all existing method that map heterogeneous data in a common hamming space mapping to different space provides more flexible and discriminative ability we empirically verify the effectiveness and efficiency of our algorithm on two real world large datasets one publicly available dataset of flickr and the other mirflickr yahoo answer dataset 
the rapidly increasing availability of electronic health record ehrs from multiple heterogeneous source ha spearheaded the adoption of data driven approach for improved clinical research decision making prognosis and patient management unfortunately ehr data do not always directly and reliably map to phenotype or medical concept that clinical researcher need or use existing phenotyping approach typically require labor intensive supervision from medical expert we propose marble a novel sparse non negative tensor factorization method to derive phenotype candidate with virtually no human supervision marble decomposes the observed tensor into two term a bias tensor and an interaction tensor the bias tensor represents the baseline characteristic common amongst the overall population and the interaction tensor defines the phenotype we demonstrate the capability of our proposed model on both simulated and patient data from a publicly available clinical database our result show that marble derived phenotype provide at least a reduction in the number of non zero element and also retains predictive power for classification purpose furthermore the resulting phenotype and baseline characteristic from real ehr data are consistent with known characteristic of the patient population thus it can potentially be used to rapidly characterize predict and manage a large number of disease thereby promising a novel data driven solution that can benefit very large segment of the population 
in this paper we propose a novel supervised learning method fast flux discriminant ffd for large scale nonlinear classification compared with other existing method ffd ha unmatched advantage a it attains the efficiency and interpretability of linear model a well a the accuracy of nonlinear model it is also sparse and naturally handle mixed data type it work by decomposing the kernel density estimation in the entire feature space into selected low dimensional subspace since there are many possible subspace we propose a submodular optimization framework for subspace selection the selected subspace prediction are then transformed to new feature on which a linear model can be learned besides since the transformed feature naturally expect non negative weight we only require smooth optimization even with the l regularization unlike other nonlinear model such a kernel method the ffd model is interpretable a it give importance weight on the original feature it training and testing are also much faster than traditional kernel model we carry out extensive empirical study on real world datasets and show that the proposed model achieves state of the art classification result with sparsity interpretability and exceptional scalability our model can be learned in minute on datasets with million of sample for which most existing nonlinear method will be prohibitively expensive in space and time 
deep learning ha catapulted to the front page of the new york time formed the core of the so called google brain and achieved impressive result in vision speech recognition and elsewhere yet researcher have offered simple conundrum that deep learning doesn t address for example consider the sentence the large ball crashed right through the table because it wa made of styrofoam what wa made of styrofoam the large ball or the table the answer is obviously the table but if we change the word styrofoam to steel the answer is clearly the large ball to automatically answer this type of question our computer require an extensive body of knowledge we believe that text mining can provide the requisite body of knowledge my talk will describe work at the new allen institute for ai towards building the next generation of text mining system 
food safety is an important health issue in singapore a the number of food poisoning case have increased significantly over the past few decade the national environment agency of singapore nea is the primary government agency responsible for monitoring and mitigating the food safety risk in an effort to pro actively monitor emerging food safety issue and to stay abreast with development related to food safety in the world nea track the world wide web a a source of news feed to identify food safety related article however such information gathering is a difficult and time consuming process due to information overload in this paper we present foodsis a system for end to end web information gathering for food safety foodsis improves efficiency of such focused information gathering process with the use of machine learning technique to identify and rank relevant content we discus the challenge in building such a system and describe how thoughtful system design and recent advance in machine learning provide a framework that synthesizes interactive learning with classification to provide a system that is used in daily operation we conduct experiment and demonstrate that our classification approach result in improving the efficiency by average compared to a conventional approach and the ranking approach lead to average improvement in elevating the rank of relevant article 
user on an online social network site generate a large number of heterogeneous activity ranging from connecting with other user to sharing content to updating their profile the set of activity within a user s network neighborhood form a stream of update for the user s consumption in this paper we report our experience with the problem of ranking activity in the linkedin homepage feed in particular we provide a taxonomy of social network activity describe a system architecture with a number of key component open sourced that support fast iteration in model development demonstrate a number of key factor for effective ranking and report experimental result from extensive online bucket test 
time sync video tagging aim to automatically generate tag for each video shot it can improve the user s experience in previewing a video s timeline structure compared to traditional scheme that tag an entire video clip in this paper we propose a new application which extract time sync video tag by automatically exploiting crowdsourced comment from video website such a nico nico douga where video are commented on by online crowd user in a time sync manner the challenge of the proposed application is that user with bias interact with one another frequently and bring noise into the data while the comment are too sparse to compensate for the noise previous technique are unable to handle this task well a they consider video semantics independently which may overfit the sparse comment in each shot and thus fail to provide accurate modeling to resolve these issue we propose a novel temporal and personalized topic model that jointly considers temporal dependency between video semantics user interaction in commenting and user preference a prior knowledge our proposed model share knowledge across video shot via user to enrich the short comment and peel off user interaction and user bias to solve the noisy comment problem log likelihood analysis and user study on large datasets show that the proposed model outperforms several state of the art baseline in video tagging quality case study also demonstrate our model s capability of extracting tag from the crowdsourced short and noisy comment 
this paper outline the approach developed together with the radio network strategy design department of a large european telecom operator in order to forecast the air interface load in their g network which is used for planning network upgrade and budgeting purpose it is based on large scale intelligent data analysis and modeling at the level of thousand of individual radio cell resulting in model per day it ha been embedded into a scenario simulation framework that is used by end user not experienced in data mining for studying and simulating the behavior of this complex networked system a an example of a systematic approach to the deployment step in the kdd process this system is already in use for two year in the country where it wa developed and it is a part of a standard business process in the last six month this national operator became a competence center for predictive modeling for micro simulation of g air interface load for four other operator of the same parent company 
visualization of high dimensional data such a text document is widely applicable the traditional mean is to find an appropriate embedding of the high dimensional representation in a low dimensional visualizable space a topic modeling is a useful form of dimensionality reduction that preserve the semantics in document recent approach aim for a visualization that is consistent with both the original word space a well a the semantic topic space in this paper we address the semantic visualization problem given a corpus of document the objective is to simultaneously learn the topic distribution a well a the visualization coordinate of document we propose to develop a semantic visualization model that approximates l normalized data directly the key is to associate each document with three representation a coordinate in the visualization space a multinomial distribution in the topic space and a directional vector in a high dimensional unit hypersphere in the word space we join these representation in a unified generative model and describe it parameter estimation through variational inference comprehensive experiment on real life text datasets show that the proposed method outperforms the existing baseline on objective evaluation metric for visualization quality and topic interpretability 
this paper address geospatial interpolation for meteorological measurement in which we estimate the value of climatic metric at unsampled site with existing observation providing climatological and meteorological condition covering a large region is potentially useful in many application such a smart grid however existing research work on interpolation either cause a large number of complex calculation or are lack of high accuracy we propose a bayesian compressed sensing based non parametric statistical model to efficiently perform the spatial interpolation task student t prior are employed to model the sparsity of unknown signal coefficient and the approximated variational inference avi method is provided for effective and fast learning the presented model ha been deployed at ibm targeting for aiding the intelligent management of smart grid the evaluation on two real world datasets demonstrate that our algorithm achieves state of the art performance in both effectiveness and efficiency 
over of column in hundred of million of web table contain numeric quantity table are a richer source of structured knowledge than free text we harness web table to answer query whose target is a quantity with natural variation such a net worth of zuckerburg battery life of ipad half life of plutonium and calorie in pizza our goal is to respond to such query with a ranked list of quantity distribution suitably represented apart from the challenge of informal schema and noisy extraction which have been known since table were used for non quantity information extraction we face additional problem of noisy number format a well a unit specification that are often contextual and ambiguous early hardening of extraction decision at a table level lead to poor accuracy instead we use a probabilistic context free grammar pcfg based unit extractor on the table and retain several top scoring extraction of quantity and numeral then we inject these into a new collective inference framework that make global decision about the relevance of candidate table snippet the interpretation of the query s target quantity type the value distribution to be ranked and presented and the degree of consensus that can be built to support the proposed quantity distribution experiment with over million web table and diverse query show robust large benefit from our quantity catalog unit extractor and collective inference 
advanced manufacturing such a aerospace semi conductor and flat display device often involves complex production process and generates large volume of production data in general the production data come from product with different level of quality assembly line with complex flow and equipment and processing craft with massive controlling parameter the scale and complexity of data is beyond the analytic power of traditional it infrastructure to achieve better manufacturing performance it is imperative to explore the underlying dependency of the production data and exploit analytic insight to improve the production process however few research and industrial effort have been reported on providing manufacturer with integrated data analytical solution to reveal potential and optimize the production process from data driven perspective in this paper we design implement and deploy an integrated solution named pdp miner which is a data analytics platform customized for process optimization in plasma display panel pdp manufacturing the system utilizes the latest advance in data mining technology and big data infrastructure to create a complete analytical solution besides our proposed system is capable of supporting automatically configuring and scheduling analysis task and balancing heterogeneous computing resource the system and the analytic strategy can be applied to other advanced manufacturing field to enable complex data analysis task since pdp miner ha been deployed a the data analysis platform of changhong coc by taking the advantage of our system the overall pdp yield rate ha increased from to the monthly production is boosted by panel which brings more than million rmb of revenue improvement per year 
how can one summarize a massive data set on the fly i e without even having seen it in it entirety in this paper we address the problem of extracting representative element from a large stream of data i e we would like to select a subset of say k data point from the stream that are most representative according to some objective function many natural notion of representativeness satisfy submodularity an intuitive notion of diminishing return thus such problem can be reduced to maximizing a submodular set function subject to a cardinality constraint classical approach to submodular maximization require full access to the data set we develop the first efficient streaming algorithm with constant factor approximation guarantee to the optimum solution requiring only a single pas through the data and memory independent of data size in our experiment we extensively evaluate the effectiveness of our approach on several application including training large scale kernel method and exemplar based clustering on million of data point we observe that our streaming method while achieving practically the same utility value run about time faster than previous work 
traditional data mining technique are designed to model a single type of heterogeneity such a multi task learning for modeling task heterogeneity multi view learning for modeling view heterogeneity etc recently a variety of real application emerged which exhibit dual heterogeneity namely both task heterogeneity and view heterogeneity example include insider threat detection across multiple organization web image classification in different domain etc existing method for addressing such problem typically assume that multiple task are equally related and multiple view are equally consistent which limit their application in complex setting with varying task relatedness and view consistency in this paper we advance state of the art technique by adaptively modeling task relatedness and view consistency via a nonparametric bayes model we model task relatedness using normal penalty with sparse covariance and view consistency using matrix dirichlet process based on this model we propose the noble algorithm using an efficient gibbs sampler experimental result on multiple real data set demonstrate the effectiveness of the proposed algorithm 
in this paper we study a variant of the social network maximum influence problem and it application to intelligently approaching individual gang member with incentive to leave a gang the goal is to identify individual who when influenced to leave gang will propagate this action we study this emerging application by exploring specific facet of the problem that must be addressed when modeling this particular situation we formulate a new influence maximization variant the social incentive influence sii problem and study it both formally and in the context of the law enforcement domain using new technique from unconstrained submodular maximization we develop an approximation algorithm for sii and present a suite of experimental result including test on real world police data from chicago 
location based data is increasingly prevalent with the rapid increase and adoption of mobile device in this paper we address the problem of learning spatial density model focusing specifically on individual level data modeling and predicting a spatial distribution for an individual is a challenging problem given both a the typical sparsity of data at the individual level and b the heterogeneity of spatial mobility pattern across individual we investigate the application of kernel density estimation kde to this problem using a mixture model approach that can interpolate between an individual s data and broader pattern in the population a a whole the mixture kde approach is evaluated on two large geolocation check in data set from twitter and gowalla with comparison to non kde baseline using both log likelihood and detection of simulated identity theft a evaluation metric our experimental result indicate that the mixture kde method provides a useful and accurate methodology for capturing and predicting individual level spatial pattern in the presence of noisy and sparse data 
in netflix announced a m prize competition to advance recommendation algorithm the recommendation problem wa simplified a the accuracy in predicting a user rating measured by the root mean squared error while that formulation helped get the attention of the research community in the area it may have put an excessive focus on what is simply one of possible approach to recommendation in this tutorial we will describe different component of modern recommender system such a personalized ranking similarity explanation context awareness or search a recommendation in the first part we will use the netflix use case a a driving example of a prototypical industrial scale recommender system we will also review the usage of modern algorithmic approach that include algorithm such a factorization machine restricted boltzmann machine simrank deep neural network or listwise learning to rank in the second part we will focus on the area of context aware recommendation where the two dimensional user item recommender problem is turned into an n dimensional space 
multi label classification of heterogeneous information network ha received renewed attention in social network analysis in this paper we present an activity edge centric multi label classification framework for analyzing heterogeneous information network with three unique feature first we model a heterogeneous information network in term of a collaboration graph and multiple associated activity graph we introduce a novel concept of vertex edge homophily in term of both vertex label and edge label and transform a general collaboration graph into an activity based collaboration multigraph by augmenting it edge with class label from each activity graph through activity based edge classification second we utilize the label vicinity to capture the pairwise vertex closeness based on the labeling on the activity based collaboration multigraph we incorporate both the structure affinity and the label vicinity into a unified classifier to speed up the classification convergence third we design an iterative learning algorithm aeclass to dynamically refine the classification result by continuously adjusting the weight on different activity based edge classification scheme from multiple activity graph while constantly learning the contribution of the structure affinity and the label vicinity in the unified classifier extensive evaluation on real datasets demonstrates that aeclass outperforms existing representative method in term of both effectiveness and efficiency 
deep learning well demonstrates it potential in learning latent feature representation recent year have witnessed an increasing enthusiasm for regularizing deep neural network by incorporating various side information such a user provided label or pairwise constraint however the effectiveness and parameter sensitivity of such algorithm have been major obstacle for putting them into practice the major contribution of our work is the exposition of a novel supervised deep learning algorithm which distinguishes from two unique trait first it regularizes the network construction by utilizing similarity or dissimilarity constraint between data pair rather than sample specific annotation such kind of side information is more flexible and greatly mitigates the workload of annotator secondly unlike prior work our proposed algorithm decouples the supervision information and intrinsic data structure we design two heterogeneous network each of which encodes either supervision or unsupervised data structure respectively specifically we term the supervision oriented network a auxiliary network since it is principally used for facilitating the parameter learning of the other one and will be removed when handling out of sample data the two network are complementary to each other and bridged by enforcing the correlation of their parameter we name the proposed algorithm supervision guided autoencoder sugar comparing prior work on unsupervised deep network and supervised learning sugar better balance numerical tractability and the flexible utilization of supervision information the classification performance on mnist digit and eight benchmark datasets demonstrates that sugar can effectively improve the performance by using the auxiliary network on both shallow and deep architecture particularly when multiple sugar are stacked the performance is significantly boosted on the selected benchmark ours achieve up to relative accuracy improvement compared to the state of the art model 
we study the problem of active learning for multilabel classification we focus on the real world scenario where the average number of positive relevant label per data point is small leading to positive label sparsity carrying out mutual information based near optimal active learning in this setting is a challenging task since the computational complexity involved is exponential in the total number of label we propose a novel inference algorithm for the sparse bayesian multilabel model of the benefit of this alternate inference scheme is that it enables a natural approximation of the mutual information objective we prove that the approximation lead to an identical solution to the exact optimization problem but at a fraction of the optimization cost this allows u to carry out efficient non myopic and near optimal active learning for sparse multilabel classification extensive experiment reveal the effectiveness of the method 
in this paper we study bid optimisation for real time bidding rtb based display advertising rtb allows advertiser to bid on a display ad impression in real time when it is being generated it go beyond contextual advertising by motivating the bidding focused on user data and it is different from the sponsored search auction where the bid price is associated with keywords for the demand side a fundamental technical challenge is to automate the bidding process based on the budget the campaign objective and various information gathered in runtime and in history in this paper the programmatic bidding is cast a a functional optimisation problem under certain dependency assumption we derive simple bidding function that can be calculated in real time our finding show that the optimal bid ha a non linear relationship with the impression level evaluation such a the click through rate and the conversion rate which are estimated in real time from the impression level feature this is different from previous work that is mainly focused on a linear bidding function our mathematical derivation suggests that optimal bidding strategy should try to bid more impression rather than focus on a small set of high valued impression because according to the current rtb market data compared to the higher evaluated impression the lower evaluated one are more cost effective and the chance of winning them are relatively higher aside from the theoretical insight offline experiment on a real dataset and online experiment on a production rtb system verify the effectiveness of our proposed optimal bidding strategy and the functional optimisation framework 
ambiguous query which are typical on search engine and recommendation system often return a large number of result from multiple interpretation given that many user often perform their search on limited size screen e g mobile phone an important problem is which result to display first recent work ha suggested displaying a set of result top k based on their relevance score with respect to the query and their diversity with respect to each other however previous work balance relevance and diversity mostly by a predefined fixed way in this paper we show that for different search task there is a different ideal balance of relevance and diversity we propose a principled method for adaptive diversification of query result that minimizes the user effort to find the desired result by dynamically balancing the relevance and diversity at each query step e g when refining the query or viewing the next page of result we introduce a navigation cost model a a mean to estimate the effort required to navigate the query result and show that the problem of estimating the ideal amount of diversification at each step is np hard we propose an efficient approximate algorithm to select a near optimal subset of the query result that minimizes the expected user effort finally we demonstrate the efficacy and efficiency of our solution in minimizing user effort compared to state of the art ranking method by mean of an extensive experimental evaluation and a comprehensive user study on amazon mechanical turk 
when data driven improvement involve personally identifiable data or even data that can be used to infer sensitive information about individual we face the dilemma that we potentially risk compromising privacy a we see increased emphasis on using data mining to effect improvement in a range of socially beneficial activity from improving matching of talented student to opportunity for higher education or improving allocation of fund across competing school program or reducing hospitalization time following surgery the dilemma can often be especially acute the data involved often is personally identifiable or revealing and sensitive and many of the institution that must be involved in gathering and maintaining custody of the data are not equipped to adequately secure the data raising the risk of privacy breach how should we approach this trade off can we ass the risk can we control or mitigate them can we develop guideline for when the risk is or is not worthwhile and for how best to handle data in different common scenario chair raghu ramakrishnan and geoffrey i webb bring this panel of leading data miner and privacy expert together to address these critical issue 
histogram construction is a fundamental problem in data management and a good histogram support numerous mining operation recent work ha extended histogram to probabilistic data however constructing histogram for probabilistic data can be extremely expensive and existing study suffer from limited scalability this work design novel approximation method to construct scalable histogram on probabilistic data we show that our method provide constant approximation compared to the optimal histogram produced by the state of the art in the worst case we also extend our method to parallel and distributed setting so that they can run gracefully in a cluster of commodity machine we introduced novel synopsis to reduce communication cost when running our method in such setting extensive experiment on large real data set have demonstrated the superb scalability and efficiency achieved by our method when compared to the state of the art method they also achieved excellent approximation quality in practice 
given a simple noun such a em apple and a question such a is it edible what process take place in the human brain more specifically given the stimulus what are the interaction between group of neuron also known a functional connectivity and how can we automatically infer those interaction given measurement of the brain activity furthermore how doe this connectivity differ across different human subject in this work we present a simple novel good enough brain model or gebm in short and a novel algorithm sparse sysid which are able to effectively model the dynamic of the neuron interaction and infer the functional connectivity moreover gebm is able to simulate basic psychological phenomenon such a habituation and priming whose definition we provide in the main text we evaluate gebm by using both synthetic and real brain data using the real data gebm produce brain activity pattern that are strikingly similar to the real one and the inferred functional connectivity is able to provide neuroscientific insight towards a better understanding of the way that neuron interact with each other a well a detect regularity and outlier in multi subject brain activity measurement 
given a directed graph of million of node how can we automatically spot anomalous suspicious node judging only from their connectivity pattern suspicious graph pattern show up in many application from twitter user who buy fake follower manipulating the social network to botnet member performing distributed denial of service attack disturbing the network traffic graph we propose a fast and effective method catchsync which exploit two of the tell tale sign left in graph by fraudsters a synchronized behavior suspicious node have extremely similar behavior pattern because they are often required to perform some task together such a follow the same user and b rare behavior their connectivity pattern are very different from the majority we introduce novel measure to quantify both concept synchronicity and normality and we propose a parameter free algorithm that work on the resulting synchronicity normality plot thanks to careful design catchsync ha the following desirable property a it is scalable to large datasets being linear on the graph size b it is parameter free and c it is side information oblivious it can operate using only the topology without needing labeled data nor timing information etc while still capable of using side information if available we applied catchsync on two large real datasets billion edge twitter social graph and billion edge tencent weibo social graph and several synthetic one catchsync consistently outperforms existing competitor both in detection accuracy by on twitter and on tencent weibo a well a in speed 
in classification if a small number of instance is added or removed incremental and decremental technique can be applied to quickly update the model however the design of incremental and decremental algorithm involves many consideration in this paper we focus on linear classifier including logistic regression and linear svm because of their simplicity over kernel or other method by applying a warm start strategy we investigate issue such a using primal or dual formulation choosing optimization method and creating practical implementation through theoretical analysis and practical experiment we conclude that a warm start setting on a high order optimization method for primal formulation is more suitable than others for incremental and decremental learning of linear classification 
graph clustering and graph outlier detection have been studied extensively on plain graph with various application recently algorithm have been extended to graph with attribute a often observed in the real world however all of these technique fail to incorporate the user preference into graph mining and thus lack the ability to steer algorithm to more interesting part of the attributed graph in this work we overcome this limitation and introduce a novel user oriented approach for mining attributed graph the key aspect of our approach is to infer user preference by the so called focus attribute through a set of user provided exemplar node in this new problem setting cluster and outlier are then simultaneously mined according to this user preference specifically our focusco algorithm identifies the focus extract focused cluster and detects outlier moreover focusco scale well with graph size since we perform a local clustering of interest to the user rather than global partitioning of the entire graph we show the effectiveness and scalability of our method on synthetic and real world graph a compared to both existing graph clustering and outlier detection approach 
on street parking just a any publicly owned utility is used inefficiently if access is free or priced very far from market rate this paper introduces a novel demand management solution using data from dedicated occupancy sensor an iteration scheme update parking rate to better match demand the new rate encourage parker to avoid peak hour and peak location and reduce congestion and underuse the solution is deliberately simple so that it is easy to understand easily seen to be fair and lead to parking policy that are easy to remember and act upon we study the convergence property of the iteration scheme and prove that it converges to a reasonable distribution for a very large class of model the algorithm is in use to change parking rate in over space in downtown los angeles since june a part of the la express park project initial result are encouraging with a reduction of congestion and underuse while in more location rate were decreased than increased 
online platform such a meetup and plancast have recently become popular for planning gathering and event organization however there is a surprising lack of study on how to effectively and efficiently organize social event for a large group of people through such platform in this paper we study the key computational problem involved in organization of social event to our best knowledge for the first time we propose the social event organization seo problem a one of assigning a set of event for a group of user to attend where the user are socially connected with each other and have innate level of interest in those event a a first step toward social event organization we introduce a formal definition of a restricted version of the problem and show that it is np hard and is hard to approximate we propose efficient heuristic algorithm that improve upon simple greedy algorithm by incorporating the notion of phantom event and by using look ahead estimation using synthetic datasets and three real datasets including those from the platform meetup and plancast we experimentally demonstrate that our greedy heuristic are scalable and furthermore outperform the baseline algorithm significantly in term of achieving superior social welfare 
advance in real time location system rtls solution have enabled u to collect massive amount of fine grained semantically rich location trace which provide unparalleled opportunity for understanding human activity and discovering useful knowledge this in turn delivers intelligence for real time decision making in various field such a workflow management indeed it is a new paradigm for workflow modeling by the knowledge discovery in location trace to that end in this paper we provide a focused study of workflow modeling by the integrated analysis of indoor location trace in the hospital environment in comparison with conventional workflow modeling based on passive workflow log one salient feature of our approach is that it can proactively unravel the workflow pattern hidden in the location trace by automatically constructing the workflow state and estimating parameter describing the transition pattern of moving object specifically to determine a meaningful granularity for the model the workflow state are first constructed a region associated with specific healthcare activity then we transform the original indoor location trace to the sequence of workflow state and model the workflow transition pattern by finite state machine furthermore we leverage the correlation in the location trace between related type of medical device to reinforce the modeling performance and enable more application the result show that the proposed framework can not only model the workflow pattern effectively but also have managerial application in workflow monitoring auditing and inspection of workflow compliance which are critical in the healthcare industry 
one response to the proliferation of large datasets ha been to develop ingenious way to throw resource at the problem using massive fault tolerant storage architecture parallel and graphical computation model such a mapreduce pregel and giraph however not all environment can support this scale of resource and not all query need an exact response this motivates the use of sampling to generate summary datasets that support rapid query and prolong the useful life of the data in storage to be effective sampling must mediate the tension between resource constraint data characteristic and the required query accuracy the state of the art in sampling go far beyond simple uniform selection of element to maximize the usefulness of the resulting sample this tutorial review progress in sample design for large datasets including streaming and graph structured data application are discussed to sampling network traffic and social network 
this paper proposes multi task copula mtc that can handle a much wider class of task than mean regression with gaussian noise in most former multi task learning mtl while former mtl emphasizes shared structure among model mtc aim at joint prediction to exploit inter output correlation given input the output of mtc are allowed to follow arbitrary joint continuous distribution mtc capture the joint likelihood of multi output by learning the marginal of each output firstly and then a sparse and smooth output dependency graph function while the former can be achieved by classical mtl learning graph dynamically varying with input is quite a challenge we address this issue by developing sparse graph regression spagraphr a non parametric estimator incorporating kernel smoothing maximum likelihood and sparse graph structure to gain fast learning algorithm it start from a few seed graph on a few input point and then update the graph on other input point by a fast operator via coarse to fine propagation due to the power of copula in modeling semi parametric distribution spagraphr can model a rich class of dynamic non gaussian correlation we show that mtc can address more flexible and difficult task that do not fit the assumption of former mtl nicely and can fully exploit their relatedness experiment on robotic control and stock price prediction justify it appealing performance in challenging mtl problem 
frequent itemset mining is a core data mining task and ha been studied extensively although by their nature frequent itemsets are aggregate over many individual and would not seem to pose a privacy threat an attacker with strong background information can learn private individual information from frequent itemsets this ha lead to differentially private frequent itemset mining which protects privacy by giving inexact answer we give an approach that first identifies top k frequent itemsets then us them to construct a compact differentially private fp tree once the noisy fp tree is built the privatized support of all frequent itemsets can be derived from it without access to the original data experimental result show that the proposed algorithm give substantially better result than prior approach especially for high level of privacy 
data glitch are unusual observation that do not conform to data quality expectation be they logical semantic or statistical by applying data integrity constraint potentially large section of data could be flagged a being noncompliant ignoring or repairing significant section of the data could fundamentally bias the result and conclusion drawn from analysis in the context of big data where large number and volume of feed from disparate source are integrated it is likely that significant portion of seemingly noncompliant data are actually legitimate usable data in this paper we introduce the notion of empirical glitch explanation concise multi dimensional description of subset of potentially dirty data and propose a scalable method for empirically generating such explanatory characterization the explanation could serve two valuable function provide a way of identifying legitimate data and releasing it back into the pool of clean data in doing so we reduce cleaning related statistical distortion of the data used to refine existing data quality constraint and generate and formalize domain knowledge we conduct experiment using real and simulated data to demonstrate the scalability of our method and the robustness of explanation in addition we use two real world example to demonstrate the utility of the explanation where we reclaim over of the suspicious data keeping data repair related statistical distortion close to 
we show how to programmatically model process that human use when extracting answer to query e g who invented typewriter list of washington national park from semi structured web page returned by a search engine this modeling enables various application including automating repetitive search task and helping search engine developer design micro segment of factoid question we describe the design and implementation of a domain specific language that enables extracting data from a webpage based on it structure visual layout and linguistic pattern we also describe an algorithm to rank multiple answer extracted from multiple webpage on query across micro segment obtained from bing log our system laseweb answered query with an average recall of also the desired answer s were present in top suggestion for case 
demographic are widely used in marketing to characterize different type of customer however in practice demographic information such a age gender and location is usually unavailable due to privacy and other reason in this paper we aim to harness the power of big data to automatically infer user demographic based on their daily mobile communication pattern our study is based on a real world large mobile network of more than user and over communication record call and sm we discover several interesting social strategy that mobile user frequently use to maintain their social connection first young people are very active in broadening their social circle while senior tend to keep close but more stable connection second female user put more attention on cross generation interaction than male user though interaction between male and female user are frequent third a persistent same gender triadic pattern over one s lifetime is discovered for the first time while more complex opposite gender triadic pattern are only exhibited among young people we further study to what extent user demographic can be inferred from their mobile communication a a special case we formalize a problem of double dependent variable prediction inferring user gender and age simultaneously we propose the whoami method a double dependent variable factor graph model to address this problem by considering not only the effect of feature on gender age but also the interrelation between gender and age our experiment show that the proposed whoami method significantly improves the prediction accuracy by up to compared with several alternative method 
speaker of more than language have access to internet and communication technology the majority of phone tablet and computer now ship with language enabled capability like speech recognition and intelligent auto correction and people increasingly interact with data intensive cloud based language technology like search engine and spam filter for both personal and large scale technology the service quality drop or disappears entirely outside of a handful of language speaker of low resource language correlate with lower access to healthcare education and higher vulnerability to disaster serving the broadest possible range of language is crucial to ensuring equitable participation in the global information economy i will present example of how natural language processing and distributed human computing are improving the life of speaker of all the world s language in area including education disaster response health and access to employment when applying natural language processing to the full diversity of the world s communication we need to go beyond simple keyword analysis and implement complex technology that require human in the loop processing to ensure usable accuracy in recent work where more than a million human judgment were collected on unstructured text and imagery data around natural disaster i will present observation that debunk recent over optimistic claim about the utility of social medium following disaster on the positive side i will share result that show how for profit technology are improving people s life by providing sustainable economic growth opportunity when they support more language aligning business objective with global diversity 
with the fast growth of smart device and social network a lot of computing system collect data that record different type of activity an important computational challenge is to analyze these data extract pattern and understand activity trend we consider the problem of mining activity network to identify interesting event such a a big concert or a demonstration in a city or a trending keyword in a user community in a social network we define an event to be a subset of node in the network that are close to each other and have high activity level we formalize the problem of event detection using two graph theoretic formulation the first one capture the compactness of an event using the sum of distance among all pair of the event node we show that this formulation can be mapped to the maxcut problem and thus it can be solved by applying standard semidefinite programming technique the second formulation capture compactness using a minimum distance tree this formulation lead to the prize collecting steiner tree problem which we solve by adapting existing approximation algorithm for the two problem we introduce we also propose efficient and effective greedy approach and we prove performance guarantee for one of them we experiment with the proposed algorithm on real datasets from a public bicycling system and a geolocation enabled social network dataset collected from twitter the result show that our method are able to detect meaningful event 
we describe an automated system for the large scale monitoring of web site that serve a online storefront for spam advertised good our system is developed from an extensive crawl of black market web site that deal in illegal pharmaceutical replica luxury good and counterfeit software the operational goal of the system is to identify the affiliate program of online merchant behind these web site the system itself is part of a larger effort to improve the tracking and targeting of these affiliate program there are two main challenge in this domain the first is that appearance can be deceiving web page that render very differently are often linked to the same affiliate program of merchant the second is the difficulty of acquiring training data the manual labeling of web page though necessary to some degree is a laborious and time consuming process our approach in this paper is to extract feature that reveal when web page linked to the same affiliate program share a similar underlying structure using these feature which are mined from a small initial seed of labeled data we are able to profile the web site of forty four distinct affiliate program that account collectively for hundred of million of dollar in illicit e commerce our work also highlight several broad challenge that arise in the large scale empirical study of malicious activity on the web 
since data is often multi faceted in it very nature it might not adequately be summarized by just a single clustering to better capture the data s complexity method aiming at the detection of multiple alternative clustering have been proposed independent of this research area semi supervised clustering technique have shown to substantially improve clustering result for single view clustering by integrating prior knowledge in this paper we join both research area and present a solution for integrating prior knowledge in the process of detecting multiple clustering we propose a bayesian framework modeling multiple clustering of the data by multiple mixture distribution each responsible for an individual set of relevant dimension in addition our model is able to handle prior knowledge in the form of instance level constraint indicating which object should or should not be grouped together since a priori the assignment of constraint to specific view is not necessarily known our technique automatically determines their membership for efficient learning we propose the algorithm smvc using variational bayesian method with experiment on various real world data we demonstrate smvc s potential to detect multiple clustering view and it capability to improve the result by exploiting prior knowledge 
the unintentional transport of invasive specie i e non native and harmful specie that adversely affect habitat and native specie through the global shipping network gsn cause substantial loss to social and economic welfare e g annual loss due to ship borne invasion in the laurentian great lake is estimated to be a high a usd million despite the huge negative impact management of such invasion remains challenging because of the complex process that lead to specie transport and establishment numerous difficulty associated with quantitative risk assessment e g inadequate characterization of invasion process lack of crucial data large uncertainty associated with available data etc have hampered the usefulness of such estimate in the task of supporting the authority who are battling to manage invasion with limited resource we present here an approach for addressing the problem at hand via creative use of computational technique and multiple data source thus illustrating how data mining can be used for solving crucial yet very complex problem towards social good by modeling implicit specie exchange a a network that we refer to a the specie flow network sfn large scale specie flow dynamic are studied via a graph clustering approach that decomposes the sfn into cluster of port and inter cluster connection we then exploit this decomposition to discover crucial knowledge on how pattern in gsn affect aquatic invasion and then illustrate how such knowledge can be used to devise effective and economical invasive specie management strategy by experimenting on actual gsn traffic data for year we have discovered crucial knowledge that can significantly aid the management authority 
in the past few year there ha been an explosion of social network in the online world user flock these network creating profile and linking themselves to other individual connecting online ha a small cost compared to the physical world leading to a proliferation of connection many of which carry little value or importance understanding the strength and nature of these relationship is paramount to anyone interesting in making use of the online social network data in this paper we use the principle of strong triadic closure to characterize the strength of relationship in social network the strong triadic closure principle stipulates that it is not possible for two individual to have a strong relationship with a common friend and not know each other we consider the problem of labeling the tie of a social network a strong or weak so a to enforce the strong triadic closure property we formulate the problem a a novel combinatorial optimization problem and we study it theoretically although the problem is np hard we are able to identify case where there exist efficient algorithm with provable approximation guarantee we perform experiment on real data and we show that there is a correlation between the labeling we obtain and empirical metric of tie strength and that weak edge act a bridge between different community in the network finally we study extension and variation of our problem both theoretically and experimentally 
for document scoring although learning to rank and domain adaptation are treated a two different problem in previous work we discover that they actually share the same challenge of adapting keyword contribution across different query or domain in this paper we propose to study the cross task document scoring problem where a task refers to a query to rank or a domain to adapt to a the first attempt to unify these two problem existing solution for learning to rank and domain adaptation either leave the heavy burden of adapting keyword contribution to feature designer or are difficult to be generalized to resolve such limitation we abstract the keyword scoring principle pointing out that the contribution of a keyword essentially depends on first it importance to a task and second it importance to the document for determining these two aspect of keyword importance we further propose the concept of feature decoupling suggesting using two type of easy to design feature meta feature and intra feature towards learning a scorer based on the decoupled feature we require that our framework fulfill inferred sparsity to eliminate the interference of noisy keywords and employ distant supervision to tackle the lack of keyword label we propose the tree structured boltzmann machine t rbm a novel two stage markov network a our solution experiment on three different application confirm the effectiveness of t rbm which achieves significant improvement compared with four state of the art baseline method 
mining phrase entity concept topic and hierarchy from massive text corpus is an essential problem in the age of big data text data in electronic form are ubiquitous ranging from scientific article to social network enterprise log news article social medium and general web page it is highly desirable but challenging to bring structure to unstructured text data uncover underlying hierarchy relationship pattern and trend and gain knowledge from such data in this tutorial we provide a comprehensive survey on the state of the art of data driven method that automatically mine phrase extract and infer latent structure from text corpus and construct multi granularity topical grouping and hierarchy of the underlying theme we study their principle methodology algorithm and application using several real datasets including research paper and news article and demonstrate how these method work and how the uncovered latent entity structure may help text understanding knowledge discovery and management 
network are prevalent and have posed many fascinating research question how can we spot similar user e g virtual identical twin in cleveland for a new yorker given a query disease how can we prioritize it candidate gene by incorporating the tissue specific protein interaction network of those similar disease in most if not all of the existing network ranking method the node are the ranking object with the finest granularity in this paper we propose a new network data model a network of network non where each node of the main network itself can be further represented a another domain specific network this new data model enables to compare the node in a broader context and rank them at a finer granularity moreover such an non model enables much more efficient search when the ranking target reside in a certain domain specific network we formulate ranking on non a a regularized optimization problem propose efficient algorithm and provide theoretical analysis such a optimality convergence complexity and equivalence extensive experimental evaluation demonstrate the effectiveness and the efficiency of our method 
it is traditionally a challenge for home buyer to understand compare and contrast the investment value of real estate while a number of estate appraisal method have been developed to value real property the performance of these method have been limited by the traditional data source for estate appraisal however with the development of new way of collecting estate related mobile data there is a potential to leverage geographic dependency of estate for enhancing estate appraisal indeed the geographic dependency of the value of an estate can be from the characteristic of it own neighborhood individual the value of it nearby estate peer and the prosperity of the affiliated latent business area zone to this end in this paper we propose a geographic method named clusranking for estate appraisal by leveraging the mutual enforcement of ranking and clustering power clusranking is able to exploit geographic individual peer and zone dependency in a probabilistic ranking model specifically we first extract the geographic utility of estate from geography data estimate the neighborhood popularity of estate by mining taxicab trajectory data and model the influence of latent business area via clusranking also we use a linear model to fuse these three influential factor and predict estate investment value moreover we simultaneously consider individual peer and zone dependency and derive an estate specific ranking likelihood a the objective function finally we conduct a comprehensive evaluation with real world estate related data and the experimental result demonstrate the effectiveness of our method 
the increasing sophistication of malicious software call for new defensive technique that are harder to evade and are capable of protecting user against novel threat we present aesop a scalable algorithm that identifies malicious executable file by applying aesop s moral that a man is known by the company he keep we use a large dataset voluntarily contributed by the member of norton community watch consisting of partial list of the file that exist on their machine to identify close relationship between file that often appear together on machine aesop leverage locality sensitive hashing to measure the strength of these inter file relationship to construct a graph on which it performs large scale inference by propagating information from the labeled file a benign or malicious to the preponderance of unlabeled file aesop attained early labeling of of benign file and of malicious file over a week before they are labeled by the state of the art technique with a true positive rate at flagging malware at false positive rate 
in the past few year the government and other agency have publicly released a prodigious amount of data that can be potentially mined to benefit the society at large however data such a health record are typically only provided at aggregated level e g per state per hospital referral region etc to protect privacy unfortunately aggregation can severely diminish the utility of such data when modeling or analysis is desired at a per individual basis so not surprisingly despite the increasing abundance of aggregate data there have been very few successful attempt in exploiting them for individual level analysis this paper introduces ludia a novel low rank approximation algorithm that utilizes aggregation constraint in addition to auxiliary information in order to estimate or reconstruct the original individual level value from aggregate data if the reconstructed data are statistically similar to the original individual level data off the shelf individual level model can be readily and reliably applied for subsequent predictive or descriptive analytics ludia is more robust to nonlinear estimate and random effect than other reconstruction algorithm it solves a sylvester equation and leverage multi level also known a hierarchical or mixed effect modeling approach efficiently a novel graphical model is also introduced to provide a probabilistic viewpoint of ludia experimental result using a texas inpatient dataset show that individual level data can be reasonably reconstructed from county hospital and zip code level aggregate data several factor affecting the reconstruction quality are discussed along with the implication of this work for current aggregation guideline 
tourism industry ha become a key economic driver for singapore understanding the behavior of tourist is very important for the government and private sector e g restaurant hotel and advertising company to improve their existing service or create new business opportunity in this joint work with singapore s land transport authority lta we innovatively apply machine learning technique to identity the tourist among public commuter using the public transportation data provided by lta on successful identification the travelling pattern of tourist are then revealed and thus allow further analysis to be carried out such a on their favorite destination region of stay etc technically we model the tourist identification a a classification problem and design an iterative learning algorithm to perform inference with limited prior knowledge and labeled data we show the superiority of our algorithm with performance evaluation and comparison with other state of the art learning algorithm further we build an interactive web based system for answering query regarding the moving pattern of the tourist which can be used by stakeholder to gain insight into tourist travelling behavior in singapore 
information network such a social medium and email network often contain sensitive information releasing such network data could seriously jeopardize individual privacy therefore we need to sanitize network data before the release in this paper we present a novel data sanitization solution that infers a network s structure in a differentially private manner we observe that by estimating the connection probability between vertex instead of considering the observed edge directly the noise scale enforced by differential privacy can be greatly reduced our proposed method infers the network structure by using a statistical hierarchical random graph hrg model the guarantee of differential privacy is achieved by sampling possible hrg structure in the model space via markov chain monte carlo mcmc we theoretically prove that the sensitivity of such inference is only o log n where n is the number of vertex in a network this bound implies le noise to be injected than those of existing work we experimentally evaluate our approach on four real life network datasets and show that our solution effectively preserve essential network structural property like degree distribution shortest path length distribution and influential node 
ideal point estimation that estimate legislator ideological position and understands their voting behavior ha attracted study from political science and computer science typically a legislator is assigned a global ideal point based on her voting or other social behavior however it is quite normal that people may have different position on different policy dimension for example some people may be more liberal on economic issue while more conservative on cultural issue in this paper we propose a novel topic factorized ideal point estimation model for a legislative voting network in a unified framework first we model the ideal point of legislator and bill for each topic instead of assigning them to a global one second the generation of topic are guided by the voting matrix in addition to the text information contained in bill a unified model that combine voting behavior modeling and topic modeling is presented and an iterative learning algorithm is proposed to learn the topic of bill a well a the topic factorized ideal point of legislator and bill by comparing with the state of the art ideal point estimation model our method ha a much better explanation power in term of held out log likelihood and other measure besides case study show that the topic factorized ideal point coincide with human intuition finally we illustrate how to use these topic factorized ideal point to predict voting result for unseen bill 
distance metric learning dml aim to learn a distance metric better than euclidean distance it ha been successfully applied to various task e g classification clustering and information retrieval many dml algorithm suffer from the over fitting problem because of a large number of parameter to be determined in dml in this paper we exploit the dropout technique which ha been successfully applied in deep learning to alleviate the over fitting problem for dml different from the previous study that only apply dropout to training data we apply dropout to both the learned metric and the training data we illustrate that application of dropout to dml is essentially equivalent to matrix norm based regularization compared with the standard regularization scheme in dml dropout is advantageous in simulating the structured regularizers which have shown consistently better performance than non structured regularizers we verify both empirically and theoretically that dropout is effective in regulating the learned metric to avoid the over fitting problem last we examine the idea of wrapping the dropout technique in the state of art dml method and observe that the dropout technique can significantly improve the performance of the original dml method 
it is often crucial for manufacturer to decide what product to produce so that they can increase their market share in an increasingly fierce market to decide which product to produce manufacturer need to analyze the consumer requirement and how consumer make their purchase decision so that the new product will be competitive in the market in this paper we first present a general distance based product adoption model to capture consumer purchase behavior using this model various distance metric can be used to describe different real life purchase behavior we then provide a learning algorithm to decide which set of distance metric one should use when we are given some historical purchase data based on the product adoption model we formalize the k most marketable product or k mmp selection problem and formally prove that the problem is np hard to tackle this problem we propose an efficient greedy based approximation algorithm with a provable solution guarantee using submodularity analysis we prove that our approximation algorithm can achieve at least of the optimal solution we apply our algorithm on both synthetic datasets and real world datasets tripadvisor com and show that our algorithm can easily achieve five or more order of speedup over the exhaustive search and achieve about of the optimal solution on average our experiment also show the significant impact of different distance metric on the result and how proper distance metric can improve the accuracy of product selection 
contextual advertising is a form of textual advertising usually displayed on third party web page one of the main problem with contextual advertising is determining how to select ad that are relevant to the page content and or the user information in order to achieve both effective advertising and a positive user experience typically the relevance of an ad to page content is indicated by a tf idf score that measure the word overlap between the page and the ad content so this problem is transformed into a similarity search in a vector space however such an approach is not useful if the vocabulary used on the page is expected to be different from that in the ad there have been study proposing the use of semantic category or hidden class to overcome this problem with these approach it is necessary to expand the ad retrieval system or build new index to handle the category or class and it is not always easy to maintain the number of category and class required for business need in this work we propose a translation method that learns the mapping of the contextual information to the textual feature of ad by using past click data the contextual information includes the user s demographic information and behavioral information a well a page content information the proposed method is able to retrieve more preferable ad while maintaining the sparsity of the inverted index and the performance of the ad retrieval system in addition it is easy to implement and there is no need to modify an existing ad retrieval system we evaluated this approach offline on a data set based on log from an ad network our method achieved better result than existing method we also applied our approach with a real ad serving system and compared the online performance using a b testing our approach achieved an improvement over the existing production system 
we present a clustering algorithm for discovering rare yet significant recurring class across a batch of sample in the presence of random effect we model each sample data by an infinite mixture of dirichlet process gaussian mixture model dpms with each dpm representing the noisy realization of it corresponding class distribution in a given sample we introduce dependency across multiple sample by placing a global dirichlet process prior over individual dpms this hierarchical prior introduces a sharing mechanism across sample and allows for identifying local realization of class across sample we use collapsed gibbs sampler for inference to recover local dpms and identify their class association we demonstrate the utility of the proposed algorithm processing a flow cytometry data set containing two extremely rare cell population and report result that significantly outperform competing technique the source code of the proposed algorithm is available on the web via the link http c iupui edu dundar aspire htm 
the pervasive use of social medium generates massive data in an unprecedented rate and the information overload problem becomes increasingly severe for social medium user recommendation ha been proven to be effective in mitigating the information overload problem demonstrated it strength in improving the quality of user experience and positively impacted the success of social medium new type of data introduced by social medium not only provide more information to advance traditional recommender system but also manifest new research possibility for recommendation in this tutorial we aim to provide a comprehensive overview of various recommendation task in social medium especially their recent advance and new frontier we introduce basic concept review state of the art algorithm and deliberate the emerging challenge and opportunity finally we summarize the tutorial with discussion on open issue and challenge about recommendation in social medium updated information about the tutorial can be found at url http www public asu edu jtang recommendation htm 
most current mutual information mi based feature selection technique are greedy in nature thus are prone to sub optimal decision potential performance improvement could be gained by systematically posing mi based feature selection a a global optimization problem a rare attempt at providing a global solution for the mi based feature selection is the recently proposed quadratic programming feature selection qpfs approach we point out that the qpfs formulation face several non trivial issue in particular how to properly treat feature self redundancy while ensuring the convexity of the objective function in this paper we take a systematic approach to the problem of global mi based feature selection we show how the resulting np hard global optimization problem could be efficiently approximately solved via spectral relaxation and semi definite programming technique we experimentally demonstrate the efficiency and effectiveness of these novel feature selection framework 
we are interested in organizing a continuous stream of sparse and noisy text known a tweet in real time into an ontology of hundred of topic with measurable and stringently high precision this inference is performed over a full scale stream of twitter data whose statistical distribution evolves rapidly over time the implementation in an industrial setting with the potential of affecting and being visible to real user made it necessary to overcome a host of practical challenge we present a spectrum of topic modeling technique that contribute to a deployed system these include non topical tweet detection automatic labeled data acquisition evaluation with human computation diagnostic and corrective learning and most importantly high precision topic inference the latter represents a novel two stage training algorithm for tweet text classification and a close loop inference mechanism for combining text with additional source of information the resulting system achieves precision at substantial overall coverage 
the gps technology and new form of urban geography have changed the paradigm for mobile service a such the abundant availability of gps trace ha enabled new way of doing taxi business indeed recent effort have been made on developing mobile recommender system for taxi driver using taxi gps trace these system can recommend a sequence of pick up point for the purpose of maximizing the probability of identifying a customer with the shortest driving distance however in the real world the income of taxi driver is strongly correlated with the effective driving hour in other word it is more critical for taxi driver to know the actual driving route to minimize the driving time before finding a customer to this end in this paper we propose to develop a cost effective recommender system for taxi driver the design goal is to maximize their profit when following the recommended route for finding passenger specifically we first design a net profit objective function for evaluating the potential profit of the driving route then we develop a graph representation of road network by mining the historical taxi gps trace and provide a brute force strategy to generate optimal driving route for recommendation however a critical challenge along this line is the high computational cost of the graph based approach therefore we develop a novel recursion strategy based on the special form of the net profit function for searching optimal candidate route efficiently particularly instead of recommending a sequence of pick up point and letting the driver decide how to get to those point our recommender system is capable of providing an entire driving route and the driver are able to find a customer for the largest potential profit by following the recommendation this make our recommender system more practical and profitable than other existing recommender system finally we carry out extensive experiment on a real world data set collected from the san francisco bay area and the experimental result clearly validate the effectiveness of the proposed recommender system 
the frequency and intensity of natural disaster ha significantly increased over the past decade and this trend is predicted to continue facing these possible and unexpected disaster accurately predicting human emergency behavior and their mobility will become the critical issue for planning effective humanitarian relief disaster management and long term societal reconstruction in this paper we build up a large human mobility database gps record of million user over one year and several different datasets to capture and analyze human emergency behavior and their mobility following the great east japan earthquake and fukushima nuclear accident based on our empirical analysis through these data we find that human behavior and their mobility following large scale disaster sometimes correlate with their mobility pattern during normal time and are also highly impacted by their social relationship intensity of disaster damage level government appointed shelter news reporting large population flow and etc on the basis of these finding we develop a model of human behavior that take into account these factor for accurately predicting human emergency behavior and their mobility following large scale disaster the experimental result and validation demonstrate the efficiency of our behavior model and suggest that human behavior and their movement during disaster may be significantly more predictable than previously thought 
correlation clustering is a basic primitive in data miner s toolkit with application ranging from entity matching to social network analysis the goal in correlation clustering is given a graph with signed edge partition the node into cluster to minimize the number of disagreement in this paper we obtain a new algorithm for correlation clustering our algorithm is easily implementable in computational model such a mapreduce and streaming and run in a small number of round in addition we show that our algorithm obtains an almost approximation to the optimal correlation clustering experiment on huge graph demonstrate the scalability of our algorithm and it applicability to data mining problem 
betweenness centrality measure the importance of a vertex by quantifying the number of time it act a a midpoint of the shortest path between other vertex this measure is widely used in network analysis in many application we wish to choose the k vertex with the maximum adaptive betweenness centrality which is the betweenness centrality without considering the shortest path that have been taken into account by already chosen vertex all previous method are designed to compute the betweenness centrality in a fixed graph thus to solve the above task we have to run these method k time in this paper we present a method that directly solves the task with an almost linear runtime no matter how large the value of k our method first construct a hypergraph that encodes the betweenness centrality and then computes the adaptive betweenness centrality by examining this graph our technique can be utilized to handle other centrality measure we theoretically prove that our method is very accurate and experimentally confirm that it is three order of magnitude faster than previous method relying on the scalability of our method we experimentally demonstrate that strategy based on adaptive betweenness centrality are effective in important application studied in the network science and database community 
with billion of database generated page on the web where consumer can readily add priced product offering to their virtual shopping cart several opportunity will become possible once we can automatically recognize what exactly is being offered for sale on each page we present a case study of a deployed data driven system that first chunk individual title into semantically classified sub segment and then us this information to improve a hyperlink insertion service to accomplish this process we propose an annotation structure that is general enough to apply to offering title from most e commerce industry while also being specific enough to identify useful semantics about each offer to automate the parsing task we apply the best practice approach of training a supervised conditional random field model and discover that creating separate prediction model for some of the industry along with the use of model ensemble achieves the best performance to date we further report on a real world application of the trained parser to the task of growing a lexical dictionary of product related term which critically provides background knowledge to an affiliate marketing hyperlink insertion service on a regular basis we apply the parser to offering title to produce a large set of labeled term from these candidate we select the most confidently predicted novel term for review by crowd sourced annotator the agreed on term are then added into a dictionary which significantly improves the performance of the link insertion service finally to continually improve system performance we retrain the model in an online fashion by performing additional annotation on title with incorrect prediction on each batch 
it is extremely important in many application domain to have transparency in predictive modeling domain expert do not tend to prefer black box predictive model model they would like to understand how prediction are made and possibly prefer model that emulate the way a human expert might make a decision with a few important variable and a clear convincing reason to make a particular prediction i will discus recent work on interpretable predictive modeling with decision list and sparse integer linear model i will describe several approach including an algorithm based on discrete optimization and an algorithm based on bayesian analysis i will show example of interpretable model for stroke prediction in medical patient and prediction of violent crime in young people raised in out of home care collaborator are ben letham berk ustun stefano traca siong thye goh tyler mccormick and david madigan 
team formation ha been long recognized a a natural way to acquire a diverse pool of useful skill by combining expert with complementary talent this allows organization to effectively complete beneficial project from different domain while also helping individual expert position themselves and succeed in highly competitive job market here we assume a collection of project ensuremath p where each project requires a certain set of skill and yield a different benefit upon completion we are further presented with a pool of expert ensuremath x where each expert ha his own skillset and compensation demand then we study the problem of hiring a cluster of expert t x so that the overall compensation cost doe not exceed a given budget b and the total benefit of the project that this team can collectively cover is maximized we refer to this a the clusterhire problem our work present a detailed analysis of the computational complexity and hardness of approximation of the problem a well a heuristic yet effective algorithm for solving it in practice we demonstrate the efficacy of our approach through experiment on real datasets of expert and demonstrate their advantage over intuitive baseline we also explore additional variant of the fundamental problem formulation in order to account for constraint and consideration that emerge in realistic cluster hiring scenario all variant considered in this paper have immediate application in the cluster hiring process a it emerges in the context of different organizational setting 
in many application classification label may not be associated with a single instance of record but may be associated with a data set of record the class behavior may not be possible to infer effectively from a single record but may be only be inferred by an aggregate set of record therefore in this problem the class label is associated with a set of instance both in the training and test data therefore the problem may be understood to be that of classifying a set of data set typically the classification behavior may only be inferred from the overall pattern of data distribution and very little information is embedded in any given record for classification purpose we refer to this problem a the setwise classification problem the problem can be extremely challenging in scenario where the data is received in the form of a stream and the record within any particular data set may not necessarily be received contiguously in this paper we present a first approach for real time and streaming classification of such data we present experimental result illustrating the effectiveness of the approach 
we study the problem of determining if an input matrix a rm x n can be well approximated by a low rank matrix specifically we study the problem of quickly estimating the rank or stable rank of a the latter often providing a more robust measure of the rank since we seek significantly sublinear time algorithm we cast these problem in the property testing framework in this framework a either ha low rank or stable rank or is far from having this property the algorithm should read only a small number of entry or row of a and decide which case a is in with high probability if neither case occurs the output is allowed to be arbitrary we consider two notion of being far a requires changing at least an fraction of it entry or a requires changing at least an fraction of it row we call the former the entry model and the latter the row model we show for testing if a matrix ha rank at most d in the entry model we improve the previous number of entry of a that need to be read from o d krauthgamer and sasson soda to o d our algorithm is the first to adaptively query the entry of a which for constant d we show is necessary to achieve o query for the important case of d we also give a new non adaptive algorithm improving the previous o query to o log for testing if a matrix ha rank at most d in the row model we prove an d lower bound on the number of row that need to be read even for adaptive algorithm our lower bound match a non adaptive upper bound of krauthgamer and sasson for testing if a matrix ha stable rank at most d in the row model or requires changing an d fraction of it row in order to have stable rank at most d we prove that reading d row is necessary and sufficient we also give an empirical evaluation of our rank and stable rank algorithm on real and synthetic datasets 
a tensor provide a natural representation for the higher order relation tensor factorization technique such a tucker decomposition and candecomp parafac decomposition have been applied to many field tucker decomposition ha strong capacity of expression but the time complexity is unpractical for the large scale real problem on the other hand candecomp parafac decomposition is linear in the feature dimensionality but the assumption is so strong that it abandon some important information besides both of td and cp decompose a tensor into several factor matrix however the factor matrix are not natural for the representation of the higher order relation to overcome these problem we propose a near linear tensor factorization approach which decompose a tensor into factor tensor in order to model the higher order relation without loss of important information in addition to reduce the time complexity and the number of the parameter we decompose each slice of the factor tensor into two smaller matrix we conduct experiment on both synthetic datasets and real datasets the experimental result on the synthetic datasets validate that our model ha strong capacity of expression the result on the real datasets show that our approach outperforms the state of the art tensor factorization method 
stochastic gradient descent sgd is a popular technique for large scale optimization problem in machine learning in order to parallelize sgd minibatch training need to be employed to reduce the communication cost however an increase in minibatch size typically decrease the rate of convergence this paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch we prove that the convergence rate doe not decrease with increasing minibatch size experiment demonstrate that with suitable implementation of approximate optimization the resulting algorithm can outperform standard sgd in many scenario 
point of interest poi recommendation ha become an important mean to help people discover attractive location however extreme sparsity of user poi matrix creates a severe challenge to cope with this challenge viewing mobility record on location based social network lbsns a implicit feedback for poi recommendation we first propose to exploit weighted matrix factorization for this task since it usually serf collaborative filtering with implicit feedback better besides researcher have recently discovered a spatial clustering phenomenon in human mobility behavior on the lbsns i e individual visiting location tend to cluster together and also demonstrated it effectiveness in poi recommendation thus we incorporate it into the factorization model particularly we augment user and poi latent factor in the factorization model with activity area vector of user and influence area vector of poi respectively based on such an augmented model we not only capture the spatial clustering phenomenon in term of two dimensional kernel density estimation but we also explain why the introduction of such a phenomenon into matrix factorization help to deal with the challenge from matrix sparsity we then evaluate the proposed algorithm on a large scale lbsn dataset the result indicate that weighted matrix factorization is superior to other form of factorization model and that incorporating the spatial clustering phenomenon into matrix factorization improves recommendation performance 
targeted online advertising is a prime source of revenue for many internet company it is a common industry practice to use a generalized second price auction mechanism to rank advertisement at every opportunity of an impression this greedy algorithm is suboptimal for both advertiser and publisher when advertiser have a finite budget in a greedy mechanism high performing advertiser tend to drop out of the auction marketplace fast and that adversely affect both the advertiser experience and the publisher revenue we describe a method for improving such ad serving system by including a budget pacing component that serf ad by being aware of global supply pattern such a system is beneficial for both advertiser and publisher we demonstrate the benefit of this component using experiment we conducted on advertising at linkedin 
user recommender system are a key component in any on line social networking platform they help the user growing their network faster thus driving engagement and loyalty in this paper we study link prediction with explanation for user recommendation in social network for this problem we propose wtfw who to follow and why a stochastic topic model for link prediction over directed and node attributed graph our model not only predicts link but for each predicted link it decides whether it is a topical or a social link and depending on this decision it produce a different type of explanation a topical link is recommended between a user interested in a topic and a user authoritative in that topic the explanation in this case is a set of binary feature describing the topic responsible of the link creation a social link is recommended between user which share a large social neighborhood in this case the explanation is the set of neighbor which are more likely to be responsible for the link creation our experimental assessment on real world data confirms the accuracy of wtfw in the link prediction and the quality of the associated explanation 
the automatic evaluation of computer program is a nascent area of research with a potential for large scale impact extant program assessment system score mostly based on the number of test case passed providing no insight into the competency of the programmer in this paper we present a system to grade computer program automatically in addition to grading a program on it programming practice and complexity the key kernel of the system is a machine learning based algorithm which determines closeness of the logic of the given program to a correct program this algorithm us a set of highly informative feature derived from the abstract representation of a given program that capture the program s functionality these feature are then used to learn a model to grade the program which are built against evaluation done by expert we show that the regression model provide much better grading than the ubiquitous test case pas based grading and rival the grading accuracy of other open response problem such a essay grading we also show that our novel feature add significant value over and above basic keyword expression count feature in addition to this we propose a novel way of posing computer program grading a a one class modeling problem and report encouraging preliminary result we show the value of the system through a case study in a real world industrial deployment to the best of the author knowledge this is the first time a system using machine learning ha been developed and used for grading program the work is timely with regard to the recent boom in massively online open courseware moocs which promise to produce a significant amount of hand graded digitized data 
deep societal benefit will spring from advance in data availability and in computational procedure for mining insight and inference from large data set i will describe effort to harness data for making prediction and guiding decision touching on work in transportation healthcare online service and interactive system i will start with effort to learn and field predictive model that forecast flow of traffic in greater city region moving from the ground to the air i will discus fusing data from aircraft to make inference about atmospheric condition and using these result to enhance air transport i will then focus on experience with building and fielding predictive model in clinical medicine i will show how inference about outcome and intervention can provide insight and guide decision making moving beyond data captured by hospital i will discus the promise of transforming anonymized behavioral data drawn from web service into large scale sensor network for public health including effort to identify adverse effect of medication and to understand illness in population i will conclude by describing how we can use machine learning to leverage the complementarity of human and machine intellect to solve challenging problem in science and society 
with the globalisation of the world s economy and ever evolving financial structure fraud ha become one of the main dissipaters of government wealth and perhaps even a major contributor in the slowing down of economy in general although corporate residence fraud is known to be a major factor data availability and high sensitivity have caused this domain to be largely untouched by academia the current belgian government ha pledged to tackle this issue at large by using a variety of in house approach and cooperation with institution such a academia the ultimate goal being a fair and efficient taxation system this is the first data mining application specifically aimed at finding corporate residence fraud where we show the predictive value of using both structured and fine grained invoicing data we further describe the problem involved in building such a fraud detection system which are mainly data related e g data asymmetry quality volume variety and velocity and deployment related e g the need for explanation of the prediction made 
correlation clustering is arguably the most natural formulation of clustering given a set of object and a pairwise similarity measure between them the goal is to cluster the object so that to the best possible extent similar object are put in the same cluster and dissimilar object are put in different cluster a it just need a definition of similarity it broad generality make it applicable to a wide range of problem in different context and in particular make it naturally suitable to clustering structured object for which feature vector can be difficult to obtain despite it simplicity generality and wide applicability correlation clustering ha so far received much more attention from the algorithmic theory community than from the data mining community the goal of this tutorial is to show how correlation clustering can be a powerful addition to the toolkit of the data mining researcher and practitioner and to encourage discussion and further research in the area in the tutorial we will survey the problem and it most common variant with an emphasis on the algorithmic technique and key idea developed to derive efficient solution we will motivate the problem and discus real world application the scalability issue that may arise and the existing approach to handle them 
how can we optimize the topology of a networked system to bring a flu under control propel a video to popularity or stifle a network malware in it infancy previous work on information diffusion ha focused on modeling the diffusion dynamic and selecting node to maximize minimize influence only a paucity of recent study have attempted to address the network modification problem where the goal is to either facilitate desirable spread or curtail undesirable one by adding or deleting a small subset of network node or edge in this paper we focus on the widely studied linear threshold diffusion model and prove for the first time that the network modification problem under this model have supermodular objective function this surprising property allows u to design efficient data structure and scalable algorithm with provable approximation guarantee despite the hardness of the problem in question both the time and space complexity of our algorithm are linear in the size of the network which allows u to experiment with million of node and edge we show that our algorithm outperform an array of heuristic in term of their effectiveness in controlling diffusion process often beating the next best by a significant margin 
in topic modelling various alternative prior have been developed for instance asymmetric and symmetric prior for the document topic and topic word matrix respectively the hierarchical dirichlet process prior for the document topic matrix and the hierarchical pitman yor process prior for the topic word matrix for information retrieval language model exhibiting word burstiness are important indeed this burstiness effect ha been show to help topic model a well and this requires additional word probability vector for each document here we show how to combine these idea to develop high performing non parametric topic model exhibiting burstiness based on standard gibbs sampling experiment are done to explore the behavior of the model under different condition and to compare the algorithm with previously published the full non parametric topic model with burstiness are only a small factor slower than standard gibbs sampling for lda and require double the memory making them very competitive we look at the comparative behaviour of different model and present some experimental insight 
pattern discovery is a core data mining activity initial approach were dominated by the frequent pattern discovery paradigm only pattern that occur frequently in the data were explored having been thoroughly researched and it limitation now well understood this paradigm is giving way to a new one which can be called statistically sound pattern discovery in this paradigm the main impetus is to discover statistically significant pattern which are unlikely to have occurred by chance and are likely to hold in future data thus the new paradigm provides a strict control over false discovery and overfitting this tutorial cover both classic and cutting edge research topic on pattern discovery combined to statistical significance testing we start with an advanced introduction to the relevant form of statistical significance testing including different school and alternative model their underlying assumption practical issue and limitation we then discus their application to data mining specific problem including evaluation of nested pattern the multiple testing problem algorithmic strategy and real world consideration we present the current state of the art solution and explore in detail how this approach to pattern discovery can deliver efficient and effective discovery of small set of interesting pattern 
counting the number of distinct element in a large dataset is a common task in web application and database this problem is difficult in limited memory setting where storing a large hash table table is intractable this paper advance the state of the art in probabilistic method for estimating the number of distinct element in a streaming setting new streaming algorithm are given that provably beat the optimal error for min count and hyperloglog while using the same sketch this paper also contributes to the understanding and theory of probabilistic cardinality estimation introducing the concept of an area cutting process and the martingale estimator these idea lead to theoretical analysis of both old and new sketch and estimator and show the new estimator are optimal for several streaming setting while also providing accurate error bound that match those obtained via simulation furthermore the area cutting process provides a geometric intuition behind all method for counting distinct element which are not affected by duplicate this intuition lead to a new sketch discrete max count and the analysis of a class of sketch self similar area cutting decomposition that have attractive property and unbiased estimator for both streaming and non streaming setting together these contribution lead to multi faceted advance in sketch construction cardinality and error estimation the theory and intuition for the problem of approximate counting of distinct element for both the streaming and non streaming case 
a online service have more and more popular incident diagnosis ha emerged a a critical task in minimizing the service downtime and ensuring high quality of the service provided for most online service incident diagnosis is mainly conducted by analyzing a large amount of telemetry data collected from the service at runtime time series data and event sequence data are two major type of telemetry data technique of correlation analysis are important tool that are widely used by engineer for data driven incident diagnosis despite their importance there ha been little previous work addressing the correlation between two type of heterogeneous data for incident diagnosis continuous time series data and temporal event data in this paper we propose an approach to evaluate the correlation between time series data and event data our approach is capable of discovering three important aspect of event timeseries correlation in the context of incident diagnosis existence of correlation temporal order and monotonic effect our experimental result on simulation data set and two real data set demonstrate the effectiveness of the algorithm 
data stream mining ha gained growing attention due to it wide emerging application such a target marketing email filtering and network intrusion detection in this paper we propose a prototype based classification model for evolving data stream called syncstream which dynamically model time changing concept and make prediction in a local fashion instead of learning a single model on a sliding window or ensemble learning syncstream capture evolving concept by dynamically maintaining a set of prototype in a new data structure called the p tree the prototype are obtained by error driven representativeness learning and synchronization inspired constrained clustering to identify abrupt concept drift in data stream pca and statistic based heuristic approach are employed syncstream ha several attractive benefit a it is capable of dynamically modeling evolving concept from even a small set of prototype and is robust against noisy example b owing to synchronization based constrained clustering and the p tree it support an efficient and effective data representation and maintenance c gradual and abrupt concept drift can be effectively detected empirical result show that our method achieves good predictive performance compared to state of the art algorithm and that it requires much le time than another instance based stream mining algorithm 
in many diverse setting aggregated opinion of others play an increasingly dominant role in shaping individual decision making one key prerequisite of harnessing the crowd wisdom is the independency of individual opinion yet in real setting collective opinion are rarely simple aggregation of independent mind recent experimental study document that disclosing prior collective opinion distorts individual decision making a well a their perception of quality and value highlighting a fundamental disconnect from current modeling effort how to model social influence and it impact on system that are constantly evolving in this paper we develop a mechanistic framework to model social influence of prior collective opinion e g online product rating on subsequent individual decision making we find our method successfully capture the dynamic of rating growth helping u separate social influence bias from inherent value using large scale longitudinal customer rating datasets we demonstrate that our model not only effectively ass social influence bias but also accurately predicts long term cumulative growth of rating solely based on early rating trajectory we believe our framework will play an increasingly important role a our understanding of social process deepens it promotes strategy to untangle manipulation and social bias and provides insight towards a more reliable and effective design of social platform 
inferring diffusion network from trace of cascade ha been extensively studied to better understand information diffusion in many domain a widely used assumption in previous work is that the diffusion network is homogenous and diffusion process of cascade follow the same pattern however in social medium user may have various interest and the connection among them are usually multi faceted in addition different cascade normally diffuse at different speed and spread to diverse scale and hence show various diffusion pattern it is challenging for traditional model to capture the heterogeneous user interaction and diverse pattern of cascade in social medium in this paper we investigate a novel problem of inferring multi aspect diffusion network with multi pattern cascade in particular we study the effect of various diffusion pattern on the information diffusion process by analyzing user retweeting behavior on a microblogging dataset by incorporating aspect level user interaction and various diffusion pattern a new model for inferring multi aspect transmission rate between user using multi pattern cascade mmrate is proposed we also provide an expectation maximization algorithm to effectively estimate the parameter experimental result on both synthetic and microblogging datasets demonstrate the superior performance of our approach over the state of the art method in inferring multi aspect diffusion network 
given two homogeneous rating matrix with some overlapped user item whose mapping are unknown this paper aim at answering two question first can we identify the unknown mapping between the user and or item second can we further utilize the identified mapping to improve the quality of recommendation in either domain our solution integrates a latent space matching procedure and a refining process based on the optimization of prediction to identify the matching then we further design a transfer based method to improve the recommendation performance using both synthetic and real data we have done extensive experiment given different real life scenario to verify the effectiveness of our model the code and other material are available at http www csie ntu edu tw r matching 
multi task feature learning ha been proposed to improve the generalization performance by learning the shared feature among multiple related task and it ha been successfully applied to many real world problem in machine learning data mining computer vision and bioinformatics most existing multi task feature learning model simply assume a common noise level for all task which may not be the case in real application recently a calibrated multivariate regression cmr model ha been proposed which calibrates different task with respect to their noise level and achieves superior prediction performance over the non calibrated one a major challenge is how to solve the cmr model efficiently a it is formulated a a composite optimization problem consisting of two non smooth term in this paper we propose a variant of the calibrated multi task feature learning formulation by including a squared norm regularizer we show that the dual problem of the proposed formulation is a smooth optimization problem with a piecewise sphere constraint the simplicity of the dual problem enables u to develop fast dual optimization algorithm with low per iteration cost we also provide a detailed convergence analysis for the proposed dual optimization algorithm empirical study demonstrate that the dual optimization algorithm quickly converges and it is much more efficient than the primal optimization algorithm moreover the calibrated multi task feature learning algorithm with and without the squared norm regularizer achieve similar prediction performance and both outperform the non calibrated one thus the proposed variant not only enables u to develop fast optimization algorithm but also keep the superior prediction performance of the calibrated multi task feature learning over the non calibrated one 
image are often used to convey many different concept or illustrate many different story we propose an algorithm to mine multiple diverse relevant and interesting text snippet for image on the web our algorithm scale to all image on the web for each image all webpage that contain it are considered the top k text snippet selection problem is posed a combinatorial subset selection with the goal of choosing an optimal set of snippet that maximizes a combination of relevancy interestingness and diversity the relevancy and interestingness are scored by machine learned model our algorithm is run at scale on the entire image index of a major search engine resulting in the construction of a database of image with their corresponding text snippet we validate the quality of the database through a large scale comparative study we showcase the utility of the database through two web scale application a augmentation of image on the web a webpage are browsed and b an image browsing experience similar in spirit to web browsing that is enabled by interconnecting semantically related image which may not be visually related through shared concept in their corresponding text snippet 
cardiac disease is the leading cause of death around the world with ischemic heart disease alone claiming million life in this burden can be attributed in part to the absence of biomarkers that can reliably identify high risk patient and match them to treatment that are appropriate for them in recent clinical study we have demonstrated the ability of computation to extract information with substantial prognostic utility that is typically disregarded in time series data collected from cardiac patient of particular interest are subtle variation in long term electrocardiographic ecg data that are usually overlooked a noise but provide a useful assessment of myocardial instability in multiple clinical cohort we have developed the pathophysiological basis for studying probabilistic variation in long term ecg and demonstrated the ability of this information to effectively risk stratify patient at risk of dying following heart attack in this paper we extend this work and focus on the question of how to reduce it computational complexity for scalable use in large datasets or energy constrained embedded device our basic approach to uncovering pathological structure within the ecg focus on characterizing beat to beat time warped shape deformation of the ecg using a modified dynamic time warping dtw and lomb scargle periodogram based algorithm a part of our effort to scale this work up we explore a novel approach to address the quadratic runtime of dtw we achieve this by developing the idea of adaptive downsampling to reduce the size of the input presented to dtw and describe change to the dynamic programming problem underlying dtw to exploit adaptively downsampled ecg signal when evaluated on data from patient in the disperse timi trial our result show that high morphologic variability is associated with an to fold increased risk of death within day of a heart attack moreover the use of adaptive downsampling with a modified dtw formulation achieves a to almost fold reduction in runtime relative to dtw without a significant change in biomarker discrimination 
with the rapid growth of web a variety of content sharing service such a flickr youtube blogger and tripadvisor etc have become extremely popular over the last decade on these website user have created and shared with each other various kind of resource such a photo video and travel blog the sheer amount of user generated content varies greatly in quality which call for a principled method to identify a set of authority who created high quality resource from a massive number of contributor of content since most previous study only infer global authoritativeness of a user there is no way to differentiate the authoritativeness in different aspect of life topic in this paper we propose a novel model of topic specific authority analysis taa which address the limitation of the previous approach to identify authority specific to given query topic s on a content sharing service this model jointly leverage the usage data collected from the sharing log and the favorite log the parameter in taa are learned from a constructed training dataset for which a novel logistic likelihood function is specifically designed to perform bayesian inference for taa with the new logistic likelihood we extend typical gibbs sampling by introducing auxiliary variable thorough experiment with two real world datasets demonstrate the effectiveness of taa in topic specific authority identification a well a the generalizability of the taa generative model 
unconditional cash transfer to the extreme poor via mobile telephony represent a radical new approach to giving givedirectly is a non governmental organization ngo at the vanguard of delivering this proven and effective approach to reducing poverty in this work we streamline an important step in the operation of the ngo by developing and deploying a data driven system for locating village with extreme poverty in kenya and uganda using the type of roof of a home thatched or metal a a proxy for poverty we develop a new remote sensing approach for selecting extremely poor village to target for cash transfer we develop an analytics algorithm that estimate housing quality and density in patch of publicly available satellite imagery by learning a predictive model with sieve of template matching result combined with color histogram a feature we develop and deploy a crowdsourcing interface to obtain labeled training data we deploy the predictive model to construct a fine scale heat map of poverty and integrate this discovered knowledge into the process of givedirectly s operation aggregating estimate at the village level we produce a ranked list from which top village are included in givedirectly s planned distribution of cash transfer the automated approach increase village selection efficiency significantly 
recommender system have become very important for many online activity such a watching movie shopping for product and connecting with friend on social network user behavioral analysis and user feedback both explicit and implicit modeling are crucial for the improvement of any online recommender system widely adopted recommender system at linkedin such a people you may know and endorsement are evolving by analyzing user behavior on impressed recommendation item in this paper we address modeling impression discounting of recommended item that is how to model user s no action feedback on impressed recommended item the main contribution of this paper include large scale analysis of impression data from linkedin and kdd cup novel anti noise regression technique and it application to learn four different impression discounting function including linear decay inverse decay exponential decay and quadratic decay applying these impression discounting function to linkedin s people you may know and endorsement recommender system 
the nystrom method is an efficient approach to enabling large scale kernel method the nystrom method generates a fast approximation to any large scale symmetric positive semidefinete spsd matrix using only a few column of the spsd matrix however since the nystrom approximation is low rank when the spectrum of the spsd matrix decay slowly the nystrom approximation is of low accuracy in this paper we propose a variant of the nystrom method called the modified nystrom by spectral shifting s nystrom the s nystrom method work well no matter whether the spectrum of spsd matrix decay fast or slow we prove that our s nystrom ha a much stronger error bound than the standard and modified nystrom method and that s nystrom can be even more accurate than the truncated svd of the same scale in some case we also devise an algorithm such that the s nystrom approximation can be computed nearly a efficient a the modified nystrom approximation finally our s nystrom method demonstrates significant improvement over the standard and modified nystrom method on several real world datasets 
with the rapid prevalence of smart mobile device the number of mobile apps available ha exploded over the past few year to facilitate the choice of mobile apps existing mobile app recommender system typically recommend popular mobile apps to mobile user however mobile apps are highly varied and often poorly understood particularly for their activity and function related to privacy and security therefore more and more mobile user are reluctant to adopt mobile apps due to the risk of privacy invasion and other security concern to fill this crucial void in this paper we propose to develop a mobile app recommender system with privacy and security awareness the design goal is to equip the recommender system with the functionality which allows to automatically detect and evaluate the security risk of mobile apps then the recommender system can provide app recommendation by considering both the apps popularity and the user security preference specifically a mobile app can lead to security risk because insecure data access permission have been implemented in this app therefore we first develop the technique to automatically detect the potential security risk for each mobile app by exploiting the requested permission then we propose a flexible approach based on modern portfolio theory for recommending apps by striking a balance between the apps popularity and the user security concern and build an app hash tree to efficiently recommend apps finally we evaluate our approach with extensive experiment on a large scale data set collected from google play the experimental result clearly validate the effectiveness of our approach 
community question answering cqa site have become valuable platform to create share and seek a massive volume of human knowledge how can we spot an insightful question that would inspire massive further discussion in cqa site how can we detect a valuable answer that benefit many user the long term impact e g the size of the population a post benefit of a question answer post is the key quantity to answer these question in this paper we aim to predict the long term impact of question answer shortly after they are posted in the cqa site in particular we propose a family of algorithm for the prediction problem by modeling three key aspect i e non linearity question answer coupling and dynamic we analyze our algorithm in term of optimality correctness and complexity we conduct extensive experimental evaluation on two real cqa data set to demonstrate the effectiveness and efficiency of our algorithm 
success of manufacturing company largely depends on reliability of their product scheduled maintenance is widely used to ensure that equipment is operating correctly so a to avoid unexpected breakdown such maintenance is often carried out separately for every component based on it usage or simply on some fixed schedule however scheduled maintenance is labor intensive and ineffective in identifying problem that develop between technician s visit unforeseen failure still frequently occur in contrast predictive maintenance technique help determine the condition of in service equipment in order to predict when and what repair should be performed the main goal of predictive maintenance is to enable pro active scheduling of corrective work and thus prevent unexpected equipment failure 
event detection in social medium is an important but challenging problem most existing approach are based on burst detection topic modeling or clustering technique which cannot naturally model the implicit heterogeneous network structure in social medium a a result only limited information such a term and geographic location can be used this paper present non parametric heterogeneous graph scan nphgs a new approach that considers the entire heterogeneous network for event detection we first model the network a a sensor network in which each node sens it neighborhood environment and report an empirical p value measuring it current level of anomalousness for each time interval e g hour or day then we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network cluster finally the event represented by each cluster is summarized with information such a type of event geographical location time and participant a a case study we consider two application using twitter data civil unrest event detection and rare disease outbreak detection and present empirical evaluation illustrating the effectiveness and efficiency of our proposed approach 
in performance display advertising a key metric of a campaign effectiveness is it conversion rate the proportion of user who take a predefined action on the advertiser website such a a purchase predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning one difficulty however is that the conversion can take place long after the impression up to a month and this delayed feedback hinders the conversion modeling we tackle this issue by introducing an additional model that capture the conversion delay intuitively this probabilistic model help determining whether a user that ha not converted should be treated a a negative sample when the elapsed time is larger than the predicted delay or should be discarded from the training set when it is too early to tell we provide experimental result on real traffic log that demonstrate the effectiveness of the proposed model 
in the competitive environment of the internet retaining and growing one s user base is of major concern to most web service furthermore the economic model of many web service is allowing free access to most content and generating revenue through advertising this unique model requires securing user time on a site rather than the purchase of good which make it crucially important to create new kind of metric and solution for growth and retention effort for web service in this work we address this problem by proposing a new retention metric for web service by concentrating on the rate of user return we further apply predictive analysis to the proposed retention metric on a service a a mean for characterizing lost customer finally we set up a simple yet effective framework to evaluate a multitude of factor that contribute to user return specifically we define the problem of return time prediction for free web service our solution is based on the cox s proportional hazard model from survival analysis the hazard based approach offer several benefit including the ability to work with censored data to model the dynamic in user return rate and to easily incorporate different type of covariates in the model we compare the performance of our hazard based model in predicting the user return time and in categorizing user into bucket based on their predicted return time against several baseline regression and classification method and find the hazard based approach to be superior 
early classification of time series is prevalent in many time sensitive application such a but not limited to early warning of disease outcome and early warning of crisis in stock market textcolor black for example early diagnosis allows physician to design appropriate therapeutic strategy at early stage of disease however practical adaptation of early classification of time series requires an easy to understand explanation interpretability and a measure of confidence of the prediction result uncertainty estimate these two aspect were not jointly addressed in previous time series early classification study such that a difficult choice of selecting one of these aspect is required in this study we propose a simple and yet effective method to provide uncertainty estimate for an interpretable early classification method the question we address here is how to provide estimate of uncertainty in regard to interpretable early prediction in our extensive evaluation on twenty time series datasets we showed that the proposed method ha several advantage over the state of the art method that provides reliability estimate in early classification namely the proposed method is more effective than the state of the art method is simple to implement and provides interpretable result 
the analysis of social sentiment expressed on the web is becoming increasingly relevant to a variety of application and it is important to understand the underlying mechanism which drive the evolution of sentiment in one way or another in order to be able to predict these change in the future in this paper we study the dynamic of news event and their relation to change of sentiment expressed on relevant topic we propose a novel framework which model the behavior of news and social medium in response to event a a convolution between event s importance and medium response function specific to medium and event type this framework is suitable for detecting time and duration of event a well a their impact and dynamic from time series of publication volume these data can greatly enhance event analysis for instance they can help distinguish important event from unimportant or predict sentiment and stock market shift a an example of such application we extracted news event for a variety of topic and then correlated this data with the corresponding sentiment time series revealing the connection between sentiment shift and event dynamic 
inference in topic model typically involves a sampling step to associate latent variable with observation unfortunately the generative model loses sparsity a the amount of data increase requiring o k operation per word for k topic in this paper we propose an algorithm which scale linearly with the number of actually instantiated topic kd in the document for large document collection and in structured hierarchical model kd ll k this yield an order of magnitude speedup our method applies to a wide variety of statistical model such a pdp and hdp at it core is the idea that dense slowly changing distribution can be approximated efficiently by the combination of a metropolis hastings step use of sparsity and amortized constant time sampling via walker s alias method 
this paper target the problem of computing meaningful clustering from uncertain data set existing method for clustering uncertain data compute a single clustering without any indication of it quality and reliability thus decision based on their result are questionable in this paper we describe a framework based on possible world semantics when applied on an uncertain dataset it computes a set of representative clustering each of which ha a probabilistic guarantee not to exceed some maximum distance to the ground truth clustering i e the clustering of the actual but unknown data our framework can be combined with any existing clustering algorithm and it is the first to provide quality guarantee about it result in addition our experimental evaluation show that our representative clustering have a much smaller deviation from the ground truth clustering than existing approach thus reducing the effect of uncertainty 
we present a direct multi class boosting dmcboost method for classification with the following property i instead of reducing the multi class classification task to a set of binary classification task dmcboost directly solves the multi class classification problem and only requires very weak base classifier ii dmcboost build an ensemble classifier by directly optimizing the non convex performance measure including the empirical classification error and margin function without resorting to any upper bound or approximation a a non convex optimization method dmcboost show competitive or better result than state of the art convex relaxation boosting method and it performs especially well on the noisy case 
performance monitor software for data center typically generates a great number of alert sequence these alert sequence indicate abnormal network event given a set of observed alert sequence it is important to identify the most critical alert that are potentially the cause of others while the need for mining critical alert over large scale alert sequence is evident most alert analysis technique stop at modeling and mining the causal relation among the alert this paper study the critical alert mining problem given a set of alert sequence we aim to find a set of k critical alert such that the number of alert potentially triggered by them is maximized we show that the problem is intractable therefore we resort to approximation and heuristic algorithm first we develop an approximation algorithm that obtains a near optimal alert set in quadratic time and propose pruning technique to improve it runtime performance moreover we show a faster approximation exists when the alert follow certain causal structure second we propose two fast heuristic algorithm based on tree sampling technique on real life data these algorithm identify a critical alert from up to mined causal relation in second meanwhile they preserve more than of solution quality and are up to time faster than their approximation counterpart 
most semi supervised learning model propagate the label over the laplacian graph where the graph should be built beforehand however the computational cost of constructing the laplacian graph matrix is very high on the other hand when we do classification data point lying around the decision boundary boundary point are noisy for learning the correct classifier and deteriorate the classification performance to address these two challenge in this paper we propose an adaptive semi supervised learning model different from previous semi supervised learning approach our new model needn t construct the graph laplacian matrix thus our method avoids the huge computational cost required by previous method and achieves a computational complexity linear to the number of data point therefore our method is scalable to large scale data moreover the proposed model adaptively suppresses the weight of boundary point such that our new model is robust to the boundary point an efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of the unlabeled data such that the induction of classifier and the transduction of label are adaptively unified into one framework extensive experimental result on six real world data set show that the proposed semi supervised learning model outperforms other related method in most case 
linear regression is a widely used tool in data mining and machine learning in many application fitting a regression model with only linear effect may not be sufficient for predictive or explanatory purpose one strategy which ha recently received increasing attention in statistic is to include feature interaction to capture the nonlinearity in the regression model such model ha been applied successfully in many biomedical application one major challenge in the use of such model is that the data dimensionality is significantly higher than the original data resulting in the small sample size large dimension problem recently weak hierarchical lasso a sparse interaction regression model is proposed that produce sparse and hierarchical structured estimator by exploiting the lasso penalty and a set of hierarchical constraint however the hierarchical constraint make it a non convex problem and the existing method find the solution of it convex relaxation which need additional condition to guarantee the hierarchical structure in this paper we propose to directly solve the non convex weak hierarchical lasso by making use of the gist general iterative shrinkage and thresholding optimization framework which ha been shown to be efficient for solving non convex sparse formulation the key step in gist is to compute a sequence of proximal operator one of our key technical contribution is to show that the proximal operator associated with the non convex weak hierarchical lasso admits a closed form solution however a naive approach for solving each subproblem of the proximal operator lead to a quadratic time complexity which is not desirable for large size problem to this end we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity we have conducted extensive experiment on both synthetic and real data set result show that our proposed algorithm is much more efficient and effective than it convex relaxation 
social medium such a twitter or weblogs are a popular source for live textual data much of this popularity is due to the fast rate at which this data arrives and there are a number of global event such a the arab spring where twitter is reported to have had a major influence however existing method for emerging topic detection are often only able to detect event of a global magnitude such a natural disaster or celebrity death and can monitor user selected keywords or operate on a curated set of hashtags only interesting emerging topic may however be of much smaller magnitude and may involve the combination of two or more word that themselves are not unusually hot at that time our contribution to the detection of emerging trend are three fold first of all we propose a significance measure that can be used to detect emerging topic early long before they become hot tag by drawing upon experience from outlier detection secondly by using hash table in a heavy hitter type algorithm for establishing a noise baseline we show how to track even all keyword pair using only a fixed amount of memory finally we aggregate the detected co trend into larger topic using clustering approach a often a a single event will cause multiple word combination to trend at the same time 
collaborative network are composed of expert who cooperate with each other to complete specific task such a resolving problem reported by customer a task is posted and subsequently routed in the network from an expert to another until being resolved when an expert cannot solve a task his routing decision i e where to transfer a task is critical since it can significantly affect the completion time of a task in this work we attempt to deduce the cognitive process of task routing and model the decision making of expert a a generative process where a routing decision is made based on mixed routing pattern in particular we observe an interesting phenomenon that an expert tends to transfer a task to someone whose knowledge is neither too similar to nor too different from his own based on this observation an expertise difference based routing pattern is developed we formalize multiple routing pattern by taking into account both rational and random analysis of task and present a generative model to combine them for a held out set of task our model not only explains their real routing sequence very well but also accurately predicts their completion time under three different quality measure our method significantly outperforms all the alternative with more than accuracy gain in practice with the help of our model hypothesis on how to improve a collaborative network can be tested quickly and reliably thereby significantly easing performance improvement of collaborative network 
poor academic performance in k is often a precursor to unsatisfactory educational outcome such a dropout which are associated with significant personal and social cost hence it is important to be able to predict student at risk of poor performance so that the right personalized intervention plan can be initiated in this paper we report on a large scale study to identify student at risk of not meeting acceptable level of performance in one state level and one national standardized assessment in grade of a major u school district an important highlight of our study is it scale both in term of the number of student included the number of year and the number of feature which provide a very solid grounding to the research we report on our experience with handling the scale and complexity of data and on the relative performance of various machine learning technique we used for building predictive model our result demonstrate that it is possible to predict student at risk of poor assessment performance with a high degree of accuracy and to do so well in advance these insight can be used to pro actively initiate personalized intervention program and improve the chance of student success 
the effective analysis of social network and graph structured data is often limited by the privacy concern of individual whose data make up these network differential privacy offer individual a rigorous and appealing guarantee of privacy but while differentially private algorithm for computing basic graph property have been proposed most graph modeling task common in the data mining community cannot yet be carried out privately in this work we propose algorithm for privately estimating the parameter of exponential random graph model ergms we break the estimation problem into two step computing private sufficient statistic then using them to estimate the model parameter we consider specific alternating statistic that are in common use for ergm model and describe a method for estimating them privately by adding noise proportional to a high confidence bound on their local sensitivity in addition we propose an estimation algorithm that considers the noise distribution of the private statistic and offer better accuracy than performing standard parameter estimation using the private statistic 
in this paper we propose a citywide and real time model for estimating the travel time of any path represented a a sequence of connected road segment in real time in a city based on the gps trajectory of vehicle received in current time slot and over a period of history a well a map data source though this is a strategically important task in many traffic monitoring and routing system the problem ha not been well solved yet given the following three challenge the first is the data sparsity problem i e many road segment may not be traveled by any gps equipped vehicle in present time slot in most case we cannot find a trajectory exactly traversing a query path either second for the fragment of a path with trajectory they are multiple way of using or combining the trajectory to estimate the corresponding travel time finding an optimal combination is a challenging problem subject to a tradeoff between the length of a path and the number of trajectory traversing the path i e support third we need to instantly answer user query which may occur in any part of a given city this call for an efficient scalable and effective solution that can enable a citywide and real time travel time estimation to address these challenge we model different driver travel time on different road segment in different time slot with a three dimension tensor combined with geospatial temporal and historical context learned from trajectory and map data we fill in the tensor s missing value through a context aware tensor decomposition approach we then devise and prove an object function to model the aforementioned tradeoff with which we find the most optimal concatenation of trajectory for an estimate through a dynamic programming solution in addition we propose using frequent trajectory pattern mined from historical trajectory to scale down the candidate of concatenation and a suffix tree based index to manage the trajectory received in the present time slot we evaluate our method based on extensive experiment using gps trajectory generated by more than taxi over a period of two month the result demonstrate the effectiveness efficiency and scalability of our method beyond baseline approach 
coverage function are an important class of discrete function that capture the law of diminishing return arising naturally from application in social network analysis machine learning and algorithmic game theory in this paper we propose a new problem of learning time varying coverage function and develop a novel parametrization of these function using random feature based on the connection between time varying coverage function and counting process we also propose an efficient parameter learning algorithm based on likelihood maximization and provide a sample complexity analysis we applied our algorithm to the influence function estimation problem in information diffusion in social network and show that with few assumption about the diffusion process our algorithm is able to estimate influence significantly more accurately than existing approach on both synthetic and real world data 
this paper study the following problem given sample from a high dimensional discrete distribution we want to estimate the leading mode of the underlying distribution a point is defined to be a mode if it is a local optimum of the density within a neighborhood under metric a we increase the amp quot scale amp quot parameter the neighborhood size increase and the total number of mode monotonically decrease the sequence of the mode reveal intrinsic topographical information of the underlying distribution though the mode finding problem is generally intractable in high dimension this paper unveils that if the distribution can be approximated well by a tree graphical model mode characterization is significantly easier an efficient algorithm with provable theoretical guarantee is proposed and is applied to application like data analysis and multiple prediction 
we consider regularized empirical risk minimization problem in particular we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function when the regularization function is block separable we can solve the minimization problem in a randomized block coordinate descent rbcd manner existing rbcd method usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinate in each iteration thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained however such a amp quot batch amp quot setting may be computationally expensive in practice in this paper we propose a mini batch randomized block coordinate descent mrbcd method which estimate the partial gradient of the selected block based on a mini batch of randomly sampled data in each iteration we further accelerate the mrbcd method by exploiting the semi stochastic optimization scheme which effectively reduces the variance of the partial gradient estimator theoretically we show that for strongly convex function the mrbcd method attains lower overall iteration complexity than existing rbcd method a an application we further trim the mrbcd method to solve the regularized sparse learning problem our numerical experiment show that the mrbcd method naturally exploit the sparsity structure and achieves better computational performance than existing method 
we provide statistical and computational analysis of sparse principal component analysis pca in high dimension the sparse pca problem is highly nonconvex in nature consequently though it global solution attains the optimal statistical rate of convergence such solution is computationally intractable to obtain meanwhile although it convex relaxation are tractable to compute they yield estimator with suboptimal statistical rate of convergence on the other hand existing nonconvex optimization procedure such a greedy method lack statistical guarantee in this paper we propose a two stage sparse pca procedure that attains the optimal principal subspace estimator in polynomial time the main stage employ a novel algorithm named sparse orthogonal iteration pursuit which iteratively solves the underlying nonconvex problem however our analysis show that this algorithm only ha desired computational and statistical guarantee within a restricted region namely the basin of attraction to obtain the desired initial estimator that fall into this region we solve a convex formulation of sparse pca with early stopping under an integrated analytic framework we simultaneously characterize the computational and statistical performance of this two stage procedure computationally our procedure converges at the rate of formula see text within the initialization stage and at a geometric rate within the main stage statistically the final principal subspace estimator achieves the minimax optimal statistical rate of convergence with respect to the sparsity level s dimension d and sample size n our procedure motivates a general paradigm of tackling nonconvex statistical learning problem with provable statistical guarantee 
in this paper we study the estimation of the k dimensional sparse principal subspace of covariance matrix in the high dimensional setting we aim to recover the oracle principal subspace solution i e the principal subspace estimator obtained assuming the true support is known a priori to this end we propose a family of estimator based on the semidefinite relaxation of sparse pca with novel regularization in particular under a weak assumption on the magnitude of the population projection matrix one estimator within this family exactly recovers the true support with high probability ha exact rank k and attains a formula see text statistical rate of convergence with s being the subspace sparsity level and n the sample size compared to existing support recovery result for sparse pca our approach doe not hinge on the spiked covariance model or the limited correlation condition a a complement to the first estimator that enjoys the oracle property we prove that another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse pca even when the previous assumption on the magnitude of the projection matrix is violated we validate the theoretical result by numerical experiment on synthetic datasets 
we propose a new method named calibrated multivariate regression cmr for fitting high dimensional multivariate regression model compared to existing method cmr calibrates the regularization for each regression task with respect to it noise level so that it is simultaneously tuning insensitive and achieves an improved finite sample performance computationally we develop an efficient smoothed proximal gradient algorithm which ha a worst case iteration complexity o where is a pre specified numerical accuracy theoretically we prove that cmr achieves the optimal rate of convergence in parameter estimation we illustrate the usefulness of cmr by thorough numerical simulation and show that cmr consistently outperforms other high dimensional multivariate regression method we also apply cmr on a brain activity prediction problem and find that cmr is a competitive a the handcrafted model created by human expert 
there are various parametric model for analyzing pairwise comparison data including the bradley terry luce btl and thurstone model but their reliance on strong parametric assumption is limiting in this paper we study a flexible model for pairwise comparison under which the probability of outcome are required only to satisfy a natural form of stochastic transitivity this class includes parametric model including the btl and thurstone model a special case but is considerably more general we provide various example of model in this broader stochastically transitive class for which classical parametric model provide poor fit despite this greater flexibility we show that the matrix of probability can be estimated at the same rate a in standard parametric model up to logarithmic term on the other hand unlike in the btl and thurstone model computing the minimax optimal estimator in the stochastically transitive model is non trivial and we explore various computationally tractable alternative we show that a simple singular value thresholding algorithm is statistically consistent but doe not achieve the minimax rate we then propose and study algorithm that achieve the minimax rate over interesting sub class of the full stochastically transitive class we complement our theoretical result with thorough numerical simulation 
in the graph inference problem one seek to recover the edge of an unknown graph from the observation of cascade propagating over this graph we approach this problem from the sparse recovery perspective we introduce a general model of cascade including the voter model and the independent cascade model for which we provide the first algorithm which recovers the graph s edge with high probability and o s log m measurement where s is the maximum degree of the graph and m is the number of node furthermore we show that our algorithm also recovers the edge weight the parameter of the diffusion process and is robust in the context of approximate sparsity finally we validate our approach empirically on synthetic graph 
