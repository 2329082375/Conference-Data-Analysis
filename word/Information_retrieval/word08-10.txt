privacy is an enormous problem in online social networking site while site such a facebook allow user fine grained control over who can see their profile it is difficult for average user to specify this kind of detailed policy in this paper we propose a template for the design of a social networking privacy wizard the intuition for the design come from the observation that real user conceive their privacy preference which friend should be able to see which information based on an implicit set of rule thus with a limited amount of user input it is usually possible to build a machine learning model that concisely describes a particular user s preference and then use this model to configure the user s privacy setting automatically a an instance of this general framework we have built a wizard based on an active learning paradigm called uncertainty sampling the wizard iteratively asks the user to assign privacy label to selected informative friend and it us this input to construct a classifier which can in turn be used to automatically assign privilege to the rest of the user s unlabeled friend to evaluate our approach we collected detailed privacy preference data from real facebook user our study revealed two important thing first real user tend to conceive their privacy preference in term of community which can easily be extracted from a social network graph using existing technique second our active learning wizard using community a feature is able to recommend high accuracy privacy setting using le user input than existing policy specification tool 
webgl leverage the power of opengl to present accelerated d graphic on a webpage the ability to put hardwareaccelerated d content in the browser will provide a mean for the creation of new web based application that were previously the exclusive domain of the desktop environment it will also allow the inclusion of feature that standalone d application do not have while webgl succeeds in bringing the power and lowlevel api of opengl to the browser it also expects a lot of web developer who are used to the dom and javascript library like jquery this paper will look at how mid level apis can help web developer create unique d content that is more than just duplicate of a standalone desktop application on a web page we will present one such web application named motionview built with c dl that provides a new mean for artist and motion capture studio to communicate with each other we will also highlight some upcoming project idea that make use of d browser technology in a way that would not have been possible in a desktop environment 
social medium sharing web site like flickr allow user to annotate image with free tag which significantly facilitate web image search and organization however the tag associated with an image generally are in a random order without any importance or relevance information which limit the effectiveness of these tag in search and other application in this paper we propose a tag ranking scheme aiming to automatically rank the tag associated with a given image according to their relevance to the image content we first estimate initial relevance score for the tag based on probability density estimation and then perform a random walk over a tag similarity graph to refine the relevance score experimental result on a flickr photo collection show that the proposed tag ranking method is both effective and efficient we also apply tag ranking into three application tag based image search tag recommendation and group recommendation which demonstrates that the proposed tag ranking approach really boost the performance of social tagging related application 
recommender system are an important component of many website two of the most popular approach are based on matrix factorization mf and markov chain mc mf method learn the general taste of a user by factorizing the matrix over observed user item preference on the other hand mc method model sequential behavior by learning a transition graph over item that is used to predict the next action based on the recent action of a user in this paper we present a method bringing both approach together our method is based on personalized transition graph over underlying markov chain that mean for each user an own transition matrix is learned thus in total the method us a transition cube a the observation for estimating the transition are usually very limited our method factorizes the transition cube with a pairwise interaction model which is a special case of the tucker decomposition we show that our factorized personalized mc fpmc model subsumes both a common markov chain and the normal matrix factorization model for learning the model parameter we introduce an adaption of the bayesian personalized ranking bpr framework for sequential basket data empirically we show that our fpmc model outperforms both the common matrix factorization and the unpersonalized mc model both learned with and without factorization 
privacy is an enormous problem in online social networking site while site such a facebook allow user fine grained control over who can see their profile it is difficult for average user to specify this kind of detailed policy in this paper we propose a template for the design of a social networking privacy wizard the intuition for the design come from the observation that real user conceive their privacy preference which friend should be able to see which information based on an implicit set of rule thus with a limited amount of user input it is usually possible to build a machine learning model that concisely describes a particular user s preference and then use this model to configure the user s privacy setting automatically a an instance of this general framework we have built a wizard based on an active learning paradigm called uncertainty sampling the wizard iteratively asks the user to assign privacy label to selected informative friend and it us this input to construct a classifier which can in turn be used to automatically assign privilege to the rest of the user s unlabeled friend to evaluate our approach we collected detailed privacy preference data from real facebook user our study revealed two important thing first real user tend to conceive their privacy preference in term of community which can easily be extracted from a social network graph using existing technique second our active learning wizard using community a feature is able to recommend high accuracy privacy setting using le user input than existing policy specification tool 
because it is an integral part of the internet routing apparatus and because it allows multiple instance of the same service to be naturally discovered ip anycast ha many attractive feature for any service that involve the replication of multiple instance across the internet while briefly considered a an enabler when content distribution network cdns first emerged the use of ip anycast wa deemed infeasible in that environment the main reason for this decision were the lack of load awareness of ip anycast and unwanted side effect of internet routing change on the ip anycast mechanism prompted by recent development in route control technology a well a a better understanding of the behavior of ip anycast in operational setting we revisit this decision and propose a load aware ip anycast cdn architecture that address these concern while benefiting from inherent ip anycast feature our architecture make use of route control mechanism to take server and network load into account to realize load aware anycast we show that the resulting redirection requirement can be formulated a a generalized assignment problem and present practical algorithm that address these requirement while at the same time limiting session disruption that plague regular ip anycast we evaluate our algorithm through trace based simulation using trace obtained from an operation cdn network 
web search engine use index to eciently retrieve page containing specified query term a well a page linking to specified page the problem of compressed index that permit such fast retrieval ha a long history we consider the problem assuming that the term in or link to a page are generated from a probability distribution how well compactly can we build such index that allow fast retrieval of particular interest is the case when the probability distribution is zipfian or a similar power law since these are the distribution that arise on the web we obtain sharp bound on the space requirement of boolean index for text document that follow zipf s law in the process we develop a general technique that applies to any probability distribution not necessarily a power law this is the first analysis of compression in index under arbitrary distribution our bound lead to quantitative version of rule of thumb that are folklore in indexing our experiment on several document collection show that the distribution of term appears to follow a double pareto law rather than zipf s law despite widely varying set of document the index size observed in the experiment conform well to our theoretical prediction 
address geocoding the process of finding the map location for a structured postal address is a relatively well studied problem in this paper we consider the more general problem of crosslingual location search where the query are not limited to postal address and the language and script used in the search query is different from the one in which the underlying data is stored to the best of our knowledge our system is the first crosslingual location search system that is able to geocode complex address we use a statistical machine transliteration system to convert location name from the script of the query to that of the stored data however we show that it is not sufficient to simply feed the resulting transliteration into a monolingual geocoding system a the ambiguity inherent in the conversion drastically expands the location search space and significantly lower the quality of result the strength of our approach lie in it integrated end to end nature we use abstraction and fuzzy search in the text domain to achieve maximum coverage despite transliteration ambiguity while applying spatial constraint in the geographic domain to focus only on viable interpretation of the query our experiment with structured and unstructured query in a set of diverse language and script arabic english hindi and japanese searching for location in different region of the world show full crosslingual location search accuracy at level comparable to that of commercial monolingual system we achieve these level of performance using technique that may be applied to crosslingual search in any language script and over arbitrary spatial data 
many have speculated that classifying web page can improve a search engine s ranking of result intuitively result should be more relevant when they match the class of a query we present a simple framework for classicationenhanced ranking that us click in combination with the classication of web page to derive a class distribution for the query we then go on to dene a variety of feature that capture the match between the class distribution of a web page and a query the ambiguity of a query and the coverage of a retrieved result relative to a query s set of class experimental result demonstrate that a ranker learned with these feature signicantly improves ranking over a competitive baseline furthermore our methodology is agnostic with respect to the classication space and can be used to derive query class for a variety of dierent taxonomy 
in this paper we describe technique for the discovery and construction of user profile leveraging from the emergent data web our system address the problem of sparseness of user profile information currently faced by both asserted and inferred profile system a profile mediator that dynamically build the most suitable user profile for a particular service or interaction in real time is employed in our prototype implementation 
recent research ha demonstrated beyond doubt the benefit of compressing natural language text using word based statistical semistatic compression not only it achieves extremely competitive compression rate but also direct search on the compressed text can be carried out faster than on the original text indexing based on inverted list benefit from compression a well such compression method assign a variable length codeword to each different text word some coding method plain huffman and restricted prefix byte code do not clearly mark codeword boundary and hence cannot be accessed at random position nor searched with the fastest text search algorithm other coding method tagged huffman end tagged dense code or s c dense code do mark codeword boundary achieving a self synchronization property that enables fast search and random access in exchange for some loss in compression effectiveness in this paper we show that by just performing a simple reordering of the target symbol in the compressed text more precisely reorganizing the byte into a wavelet treelike shape and using little additional space searching capability are greatly improved without a drastic impact in compression and decompression time with this approach all the code achieve synchronism and can be searched fast and accessed at arbitrary point moreover the reordered compressed text becomes an implicitly indexed representation of the text which can be searched for word in time independent of the text length that is we achieve not only fast sequential search time but indexed search time for almost no extra space cost we experiment with three well known word based compression technique with different characteristic plain huffman end tagged dense code and restricted prefix byte code and show the searching capability achieved by reordering the compressed representation on several corpus we show that the reordered version are not only much more efficient than their classical counterpart but also more efficient than explicit inverted index built on the collection when using the same amount of space 
we propose a novel method based on concept from expander graph to sample community in network we show that our sampling method unlike previous technique produce subgraphs representative of community structure in the original network these generated subgraphs may be viewed a stratified sample in that they consist of member from most or all community in the network using sample produced by our method we show that the problem of community detection may be recast into a case of statistical relational learning we empirically evaluate our approach against several real world datasets and demonstrate that our sampling method can effectively be used to infer and approximate community affiliation in the larger network 
in this paper we look at the social tag prediction problem given a set of object and a set of tag applied to those object by user can we predict whether a given tag could should be applied to a particular object we investigated this question using one of the largest crawl of the social bookmarking system del icio u gathered to date for url in del icio u we predicted tag based on page text anchor text surrounding host and other tag applied to the url we found an entropy based metric which capture the generality of a particular tag and informs an analysis of how well that tag can be predicted we also found that tag based association rule can produce very high precision prediction a well a giving deeper understanding into the relationship between tag our result have implication for both the study of tagging system a potential information retrieval tool and for the design of such system 
we consider the problem of optimal tagging for navigational purpose in one s own collection what is the best that a forgetful user can hope for in term of ease of retrieving a labeled object we prove that the number of tag ha to increase logarithmically in the collection size to maintain a manageable result set using flickr data we then show that user do indeed apply more and more tag a their collection grows and that this is not due to a global increase in tagging activity however a the additional term applied are not statistically independent user of large collection still have to deal with larger and larger result set even when more tag are used a search term we pose optimal tag suggestion for navigational purpose a an open problem 
this paper present a topic model that detects topic distribution over time our proposed model trend detection model tdm introduces a latent trend class variable into each document the trend class ha a probability distribution over topic and a continuous distribution over time experiment using our data set show that tdm is useful a a generative model in the analysis of the evolution of trend 
when search engine user have trouble finding information they may become frustrated possibly resulting in a bad experience even if they are ultimately successful in a user study in which participant were given difficult information seeking task half of all query submitted resulted in some degree of self reported frustration a third of all successful task involved at least one instance of frustration by modeling searcher frustration search engine can predict the current state of user frustration and decide when to intervene with alternative search strategy to prevent the user from becoming more frustrated giving up or switching to another search engine we present several model to predict frustration using feature extracted from query log and physical sensor we are able to predict frustration with a mean average precision of from the physical sensor and from the query log feature 
we describe a method for applying parsimonious language model to re estimate the term probability assigned by relevance model we apply our method to six topic set from test collection in five different genre our parsimonious relevance model i improve retrieval effectiveness in term of map on all collection ii significantly outperform their non parsimonious counterpart on most measure and iii have a precision enhancing effect unlike other blind relevance feedback method 
social tagging provides valuable and crucial information for large scale web image retrieval it is ontology free and easy to obtain however irrelevant tag frequently appear and user typically will not tag all semantic object in the image which is also called semantic loss to avoid noise and compensate for the semantic loss tag recommendation is proposed in literature however current recommendation simply rank the related tag based on the single modality of tag co occurrence on the whole dataset which ignores other modality such a visual correlation this paper proposes a multi modality recommendation based on both tag and visual correlation and formulates the tag recommendation a a learning problem each modality is used to generate a ranking feature and rankboost algorithm is applied to learn an optimal combination of these ranking feature from different modality experiment on flickr data demonstrate the effectiveness of this learning based multi modality recommendation strategy 
people search is an important search service with multiple application eg looking up a friend on facebook finding colleague in corporate email directory etc with the proportion of non english user on a steady rise people search service are being used by user from diverse language demographic user may issue name search query against these directory in language other than the language of the directory in which case the present monolingual name search approach will not work in this demo we present a multilingual people search system capable of performing fast name lookup on large user directory independent of the directory language our system ha application in area like social networking enterprise search and email address book search 
web service development and usage ha shifted from simple information processing service to high value business service that are crucial to productivity and success in order to deal with an increasing risk of unavailability or failure of mission critical web service we argue the need for advanced reservation of service in the form of derivative the contribution of this paper is twofold first we provide an abstract model of a market design that enables the trade of derivative for mission critical web service our model satisfies requirement that result from service characteristic such a intangibility and the impossibility to inventor service in order to meet fluctuating demand it comprehends principle from model of incomplete market such a the absence of a tradeable underlying and consistent arbitragefree derivative pricing furthermore we provide an architecture for a web service market that implement our model and describes the strategy space and interaction of market participant in the trading process of service derivative we compare the underlying pricing process to existing derivative model in energy exchange discus eventual shortcoming and propose wavelet a a preprocessing tool to analyze actual data and extract longand short term seasonalities 
wiki content templating enables reuse of content structure among wiki page in this paper we present a thorough study of this widespread feature showing how it two state of the art model functional and creational templating are sub optimal we then propose a third better model called lightly constrained lc templating and show it implementation in the moin wiki engine we also show how lc templating implementation are the appropriate technology to push forward semantically rich web page on the line of lowercase semantic web and microformats 
personalized web search ha emerged a one of the hottest topic for both the web industry and academic researcher however the majority of study on personalized search focused on a rather simple type of search which leaf an important research topic the personalization in exploratory search a an under studied area in this paper we present a study of personalization in task based information exploration using a system called tasksieve tasksieve is a web search system that utilizes a relevance feedback based profile called a task model for personalization it innovation include flexible and user controlled integration of query and task model task infused text snippet generation and on screen visualization of task model through an empirical study using human subject conducting task based exploration search we demonstrate that tasksieve push significantly more relevant document to the top of search result list a compared to a traditional search system tasksieve help user select significantly more accurate information for their task allows the user to do so with higher productivity and is viewed more favorably by subject under several usability related characteristic 
combating web spam ha become one of the top challenge for web search engine state of the art spam detection technique are usually designed for specific known type of web spam and are incapable and inefficient for recently appeared spam with user behavior analysis into web access log we propose a spam page detection algorithm based on bayes learning preliminary experiment on web access data collected by a commercial web site containing over billion user click in month show the effectiveness of the proposed detection framework and algorithm 
we introduce a multi label classification model and algorithm for labeling heterogeneous network where node belong to different type and different type have different set of classification label we present a graph based approach which model the mutual influence between node in the network a a random walk when viewing class label a color the random surfer is spraying different node type with different color palette hence the name graffiti we demonstrate the performance gain of our method by comparing it to three state of the art technique for graph based classification 
in opinion finding the retrieval system is tasked with retrieving not just relevant document but which also express an opinion towards the query target entity most opinion finding system are based on a two stage approach where initially the system aim to retrieve relevant document which are then re ranked according to the extent to which they are detected to be of an opinionated nature in this work we investigate how the underlying baseline retrieval system performance affect the overall opinion finding performance we apply two effective opinion finding technique to all the baseline run submitted to the trec blog track and draw new insight and conclusion 
several attempt have been made to analyze customer behavior on online e commerce site some study particularly emphasize the social network of customer user review and rating of a product exert effect on other consumer purchasing behavior whether a user refers to other user rating depends on the trust accorded by a user to the reviewer on the other hand the trust that is felt by a user for another user correlate with the similarity of two user rating this bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online community because it suggests clustering of similar user and the evolution of strong community this paper present a theoretical model along with analysis of an actual online e commerce site we analyzed a large community site in japan cosme the noteworthy characteristic of cosme are that user can bookmark their trusted user in addition they can post their own rating of product which facilitates our analysis of the rating bidirectional effect on trust and rating we describe an overview of the data in cosme analysis of effect from trust to rating and vice versa and our proposition of a measure of community gravity which measure how strongly a user might be attracted to a community our study is based on the cosme dataset in addition to the epinions dataset it elucidates important insight and proposes a potentially important measure for mining online social network 
the markov random walk model ha been recently exploited for multi document summarization by making use of the link relationship between sentence in the document set under the assumption that all the sentence are indistinguishable from each other however a given document set usually cover a few topic theme with each theme represented by a cluster of sentence the topic theme are usually not equally important and the sentence in an important theme cluster are deemed more salient than the sentence in a trivial theme cluster this paper proposes the cluster based conditional markov random walk model clustercmrw and the cluster based hit model clusterhits to fully leverage the cluster level information experimental result on the duc and duc datasets demonstrate the good effectiveness of our proposed summarization model the result also demonstrate that the clustercmrw model is more robust than the clusterhits model with respect to different cluster number 
it is crucial for a web crawler to distinguish between ephemeral and persistent content ephemeral content e g quote of the day is usually not worth crawling because by the time it reach the index it is no longer representative of the web page from which it wa acquired on the other hand content that persists across multiple page update e g recent blog posting may be worth acquiring because it match the page s true content for a sustained period of time in this paper we characterize the longevity of information found on the web via both empirical measurement and a generative model that coincides with these measurement we then develop new recrawl scheduling policy that take longevity into account a we show via experiment over real web data our policy obtain better freshness at lower cost compared with previous approach 
the vector space model ha been and to a great extent still is the de facto choice for profile representation in content based information filtering however user profile represented a weighted keyword vector have inherent dimensionality problem a the number of profile keywords increase the vector representation becomes ambiguous due to the exponential increase in the volume of the vector space and in the number of possible keyword combination we argue that the complexity and dynamic of information filtering require user profile representation which are resilient and resistant to this curse of dimensionality a user profile ha to be able to incorporate many feature and to adapt to a variety of interest change we propose an alternative network based profile representation that meet these challenging requirement experiment show that the network profile representation can more effectively capture additional information about a user s interest and thus achieve significant performance improvement over a vector based representation comprising the same weighted keywords 
we consider the problem of segmenting a webpage into visually and semantically cohesive piece our approach is based on formulating an appropriate optimization problem on weighted graph where the weight capture if two node in the dom tree should be placed together or apart in the segmentation we present a learning framework to learn these weight from manually labeled data in a principled manner our work is a significant departure from previous heuristic and rule based solution to the segmentation problem the result of our empirical analysis bring out interesting aspect of our framework including variant of the optimization problem and the role of learning 
the automatic content linking device monitor a conversation and us automatically recognized word to retrieve document that are of potential use to the participant the document set includes project related report or email transcribed snippet of past meeting and website retrieval result are displayed at regular interval 
we introduce a novel approach to expert finding based on multi step relevance propagation from document to related candidate relevance propagation is modeled with an absorbing random walk the evaluation on the two official enterprise trec data set demonstrates the advantage of our method over the state of the art method based on one step propagation 
music theme annotation would be really beneficial for supporting retrieval but are often neglected by user while annotating thus in order to support user in tagging and to fill the gap in the tag space in this paper we develop algorithm for recommending theme annotation our method exploit already existing user tag the lyric of music track a well a combination of both we compare the result for our recommended theme annotation against genre and style recommendation a much easier and already studied task we evaluate the quality of our recommended tag against an expert ground truth data set our result are promising and provide interesting insight into possible extension for music tagging system to support music search 
the emergence of semantic web technology and standard such a resource description framework rdf ha introduced novel data storage model such a the rdf graph model in this paper we present a research effort called r d which attempt to bridge the gap between rdf and rdbms concept by presenting a relational view of rdf data store thus r d is essentially a relational wrapper around rdf store that aim to make the variety of stable relational tool that are currently in the market available to rdf store without data duplication and synchronization issue 
hierarchical topic taxonomy have proliferated on the world wide web and exploiting the output space decomposition they induce in automated classification system is an active area of research in many domain classifier learned on a hierarchy of class have been shown to outperform those learned on a flat set of class in this paper we argue that the hierarchical arrangement of class lead to intuitive relationship between the corresponding classifier output score and that enforcing these relationship a a post processing step after classification can improve it accuracy we formulate the task of smoothing classifier output a a regularized isotonic tree regression problem and present a dynamic programming based method that solves it optimally this new problem generalizes the classic isotonic tree regression problem and both the new formulation and algorithm might be of independent interest in our empirical analysis of two real world text classification scenario we show that our approach to smoothing classifier output result in improved classification accuracy 
determining candidate view on important issue is critical in deciding whom to support and vote for but finding their statement and vote on an issue can be laborious in this paper we present psst political statement and support tracker a search engine to facilitate analysis of political statement and vote over time we show that prior tool for text analysis can be combined with minimal manual processing to provide a first step in the full automation of this process 
in this paper we study the community structure of endorsement network i e social network in which a directed edge u v is asserting an action of support from user u to user v example include scenario in which a user u is favoring a photo liking a post or following the microblog of user v starting from the hypothesis that the footprint of a community in an endorsement network is a bipartite directed clique from a set of follower to a set of leader we apply frequent itemset mining technique to discover such bicliques our analysis of real network discovers that an interesting phenomenon is taking place the leader of a community are endorsing each other forming a very dense nucleus 
social bookmarking ha emerged a a growing source of human generated content on the web in essence bookmarking involves url and tag on them in this paper we perform a large scale study of the usefulness of bookmarked url from the top social bookmarking site delicious instead of focusing on the dimension of tag which ha been covered in the previous work we explore social bookmarking from the dimension of url more specifically we investigate the delicious url and their content to quantify their value to a search engine for their value in leading to good content we show that the delicious url have higher quality content and more external outlinks for their value in satisfying user we show that the delicious url have more clicked url a well a get more click we suggest that based on their value the delicious url should be used a another source of seed url for crawler 
information retrieval model usually represent content only and not other consideration such a authority cost and recency how could multiple criterion be utilized in information retrieval and how would it effect the result in our experiment using multiple user centric criterion always produced better result than a single criterion 
indexing and retrieval of speech content in various form such a broadcast news customer care data and on line medium ha gained a lot of interest for a wide range of application from customer analytics to on line medium search for most retrieval application the speech content is typically first converted to a lexical or phonetic representation using automatic speech recognition asr the first step in searching through index built on these representation is the generation of pronunciation for named entity and foreign language query term this paper summarizes the result of the work conducted during the jhu summer workshop by the multilingual spoken term detection team on mining the web for pronunciation and analyzing their impact on spoken term detection we will first present method to use the vast amount of pronunciation information available on the web in the form of ipa and ad hoc transcription we describe technique for extracting candidate pronunciation from web page and associating them with orthographic word filtering out poorly extracted pronunciation normalizing ipa pronunciation to better conform to a common transcription standard and generating phonemic representation from ad hoc transcription we then present an analysis of the effectiveness of using these pronunciation to represent out of vocabulary oov query term on the performance of a spoken term detection std system we will provide comparison of web pronunciation against automated technique for pronunciation generation a well a pronunciation generated by human expert our result cover a range of speech index based on lattice confusion network and one best transcription at both word and word fragment level 
web site operator internet user and online service provider are besieged by a growing array of abuse and threat spam lead user to online scam and phishing web page which cyber criminal implant on innocent site to fool the unwary into revealing their financial data and password other criminal use web site to spread malware which can steal personal data or take over user computer into a botnet which can be used to send spam or mount cyber attack against web site and other internet service together these abuse undermine user trust hamper e commerce and cost the internet community huge loss in money service and support cost and time what should web site operator and online service provider do to protect themselves and their user what are internet company organization and law enforcement doing and not doing to combat these problem and how can the international internet community work together on these problem the panel brings together representative from the chain of organization that respond to internet abuse problem and promise a lively compelling and relevant discussion 
there ha been a lot of work on evaluating and improving the relevance of web search engine in this paper we suggest using human computation game to elicit data from player that can be used to improve search we describe page hunt a single player game the data elicited using page hunt ha several application including providing metadata for page providing query alteration for use in query refinement and identifying ranking issue we describe an experiment with over game player and highlight some interesting aspect of the data obtained 
in this paper we propose to model the blended search problem by assuming conditional dependency among query v and search result the probability distribution of this model are learned from search engine query log through unigram language model our experimental exploration show that a large number of query in generic web search have vertical search intention and our proposed algorithm can effectively blend vertical search result into generic web search which can improve the mean average precision map by a much a compared to traditional web search without blending 
it is notoriously difficult to program a solid web application besides addressing web interaction state maintenance and whimsical user navigation behavior programmer must also avoid a minefield of security vulnerability the problem is twofold first we lack a clear understanding of the new computation model underlying web application second we lack proper abstraction for hiding common and subtle coding detail that are orthogonal to the business functionality of specific web application this paper address both issue first we present a language bass for declarative server side scripting bass allows programmer to work in an ideal world using new abstraction to tackle common but problematic aspect of web programming the meta property of bass provide useful security guarantee second we present a language moss reflecting realistic web programming concept and scenario thus articulating the computation model behind web programming finally we present a translation from bass to moss demonstrating how the ideal programming model and security guarantee of bass can be implemented in practice 
although anchor text provides very useful information for web search a large portion of web page have few or no incoming hyperlink anchor which is known a the anchor text sparsity problem in this paper we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page s plausible missing anchor text from it similar web page in link anchor text we design experiment with two publicly available trec web corpus gov and clueweb to evaluate different approach for discovering missing anchor text experimental result show that our approach can effectively discover plausible missing anchor term we then use the web named page finding task in the trec terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval experimental result show that our approach can statistically significantly improve retrieval performance compared with several approach that only use anchor text aggregated over the web graph 
social medium such a web forum often have dense interaction between user and content where network model are often appropriate for analysis joint non negative matrix factorization model of participation and content data can be viewed a a bipartite graph model between user and medium and is proposed for analysis social medium the factorization allow simultaneous automatic discovery of leader and sub community in the web forum a well a the core latent topic in the forum result on topic detection of web forum and cluster analysis show that social feature are highly effective for forum analysis 
user of social networking service can connect with each other by forming community for online interaction yet a the number of community hosted by such website grows over time user have even greater need for effective community recommendation in order to meet more user in this paper we investigate two algorithm from very different domain and evaluate their effectiveness for personalized community recommendation first is association rule mining arm which discovers association between set of community that are shared across many user second is latent dirichlet allocation lda which model user community co occurrence using latent aspect in comparing lda with arm we are interested in discovering whether modeling low rank latent structure is more effective for recommendation than directly mining rule from the observed data we experiment on an orkut data set consisting of user and community our empirical comparison using the top k recommendation metric show that lda performs consistently better than arm for the community recommendation task when recommending a list of or more community however for recommendation list of up to community arm is still a bit better we analyze example of the latent information learned by lda to explain this finding to efficiently handle the large scale data set we parallelize lda on distributed computer and demonstrate our parallel implementation s scalability with varying number of machine 
portable reusable test collection are a vital part of research and development in information retrieval reusability is difficult to ass however the standard approach simulating judgment collection when group of system are held out then evaluating those held out system only work when there is a large set of relevance judgment to draw on during the simulation a test collection adapt to larger and larger corpus it becomes le and le likely that there will be sufficient judgment for such simulation experiment thus we propose a methodology for information retrieval experimentation that collect evidence for or against the reusability of a test collection while judgment are being made using this methodology along with the appropriate statistical analysis researcher will be able to estimate the reusability of their test collection while building them and implement course correction if the collection doe not seem to be achieving desired level of reusability we show the robustness of our design to inherent source of variance and provide a description of an actual implementation of the framework for creating a large test collection 
in this paper we propose a new phrase based ir model which integrates a measure of inseparability of phrase our experiment show it high potential to produce large improvement in retrieval effectiveness 
ranked retrieval ha a particular disadvantage in comparison with traditional boolean retrieval there is no clear cut off point where to stop consulting result this is a serious problem in some setup we investigate and further develop method to select the rank cut off value which optimizes a given effectiveness measure assuming no other input than a system s output for a query document score and their distribution the task is essentially a score distributional threshold optimization problem the recent trend in modeling score distribution is to use a normal exponential mixture normal for relevant and exponential for non relevant document score we discus the two main theoretical problem with the current model support incompatibility and non convexity and develop new model that address them the main contribution of the paper are two truncated normal exponential model varying in the way the out truncated score range are handled we conduct a range of experiment using the trec and legal track data and show that the truncated model lead to significantly better result 
it is generally believed that propagated anchor text is very important for effective web search a offered by the commercial search engine google bomb are a notable illustration of this however many year of trec web retrieval research failed to establish the effectiveness of link evidence for ad hoc retrieval on web collection the ultimate resolution to this dilemma wa that typical web search is very different from the traditional ad hoc methodology so far however no one ha established why link information like incoming link degree or anchor text doe not help ad hoc retrieval effectiveness several possible explanation were given including the collection being too small for anchor to be effective and the density of the link graph being too low the new trec web track collection is substantially larger than previous collection and ha a dense link graph our main finding is that propagated anchor text outperforms full text retrieval in term of early precision and in combination with it give an improvement in overall precision we then analyse the impact of link density and collection size by down sampling the number of link and the number of page respectively other finding are that contrary to expectation inter server link density ha little impact on effectiveness while the size of the collection ha a substantial impact on the quantity quality and effectiveness of anchor text we also compare the diversity of the search result of anchor text and full text approach which show that anchor text performs significantly better than full text search and confirm our finding for the ad hoc search task 
this paper address the issue of automatically extracting keyphrases from a document previously this problem wa formalized a classification and learning method for classification were utilized this paper point out that it is more essential to cast the problem a ranking and employ a learning to rank method to perform the task specifically it employ ranking svm a state of art method of learning to rank in keyphrase extraction experimental result on three datasets show that ranking svm significantly outperforms the baseline method of svm and naive bayes indicating that it is better to exploit learning to rank technique in keyphrase extraction 
query abandonment by search engine user is generally considered to be a negative signal in this paper we explore the concept of good abandonment we define a good abandonment a an abandoned query for which the user s information need wa successfully addressed by the search result page with no need to click on a result or refine the query we present an analysis of abandoned internet search query across two modality pc and mobile in three locale the goal is to approximate the prevalence of good abandonment and to identify type of information need that may lead to good abandonment across different locale and modality our study ha three key finding first query potentially indicating good abandonment make up a significant portion of all abandoned query second the good abandonment rate from mobile search is significantly higher than that from pc search across all locale tested third classified by type of information need the major class of good abandonment vary dramatically by both locale and modality our finding imply that it is a mistake to uniformly consider query abandonment a a negative signal further there is a potential opportunity for search engine to drive additional good abandonment especially for mobile search user by improving search feature and result snippet 
on line social network such a facebook are increasingly utilized by many user these network allow people to publish detail about themselves and connect to their friend some of the information revealed inside these network is private and it is possible that corporation could use learning algorithm on the released data to predict undisclosed private information in this paper we explore how to launch inference attack using released social networking data to predict undisclosed private information about individual we then explore the effectiveness of possible sanitization technique that can be used to combat such inference attack under different scenario 
search and recommendation system must include contextual information to effectively model user interest in this paper we present a systematic study of the effectiveness of five variant source of contextual information for user interest modeling post query navigation and general browsing behavior far outweigh direct search engine interaction a an information gathering activity therefore we conducted this study with a focus on website recommendation rather than search result the five contextual information source used are social historic task collection and user interaction we evaluate the utility of these source and overlap between them based on how effectively they predict user future interest our finding demonstrate that the source perform differently depending on the duration of the time window used for future prediction and that context overlap outperforms any isolated source designer of website suggestion system can use our finding to provide improved support for post query navigation and general browsing behavior 
with telecom market reaching saturation in many geography and revenue from voice call decreasing telecom operator are trying to identify new source of revenue for this purpose these operator can take advantage of their core functionality like location call control etc by exposing them a service to be composed by developer with third party offering available over the web to hide the complexity of underlying telecom protocol from application developer the operator are steadily adopting service oriented architecture soa and reference standard like parlay x and ims however a number of challenge still remain in rapid utilization of telecom functionality for creating new application existence of multiple protocol different class of developer and the need to coordinate and manage usage of these functionality in this paper we present sewnet a framework for creating application exploiting telecom functionality exposed over a converged ip network more specifically sewnet a provides an abstraction model for encapsulating invocation coordination and enrichment of the telecom functionality b render a service creation environment on top of this model and c caters to various different category of developer with the help of two use case scenario we demonstrate how sewnet can create service utilizing rich telecom functionality 
entity information management eim deal with organizing processing and delivering information about entity it emergence is a result of satisfying more sophisticated information need that go beyond document search in the recent year entity retrieval ha attracted much attention in the ir community inex ha started the xml entity ranking track since and trec ha launched the entity track since to investigate the problem of related entity finding some eim problem go beyond retrieval and ranking such a entity profiling which is about characterizing a specific entity and entity distillation which is about discovering the trend about an entity these problem have received le attention while they have many important application on the other hand the entity in the real world or in the web environment are usually not isolated they are connected or related with each other in one way or another for example the coauthorship make the author with similar research interest be connected the emergence of social medium such a facebook twitter and youtube ha further interweaved the related entity in a much larger scale million of user in these site can become friend fan or follower of others or tagger or commenters of different type of entity e g bookmark photo and video these network are complex in the sense that they are heterogeneous with multiple type of entity and of interaction they are large scale they are multi lingual and they are dynamic these feature of the complex network go beyond traditional social network analysis and require further research in this proposed research i investigate entity information management in the environment of complex network the main research question is how can the eim task be facilitated by modeling the content and structure of complex network the research is in the intersection of content based information retrieval and complex network analysis which deal with both unstructured text data and structured network the specific targeting eim task are entity retrieval entity profiling and entity distillation in addition to the main research question the following question are considered how can we accomplish a eim task involving diverse entity and interaction type how to model the evolution of entity profile a well a the underlying complex network how can the existing cross language ir work be leveraged to build entity profile with multi lingual evidence i propose to use probabilistic model and discriminative model in particular to address the above research question in my research i have developed discriminative model for expert search to integrate arbitrary document feature and to learn flexible combination strategy to rank expert in heterogeneous information source discriminative graphical model are proposed to jointly discover homepage by inference on the homepage dependence network the dependence of table element is exploited to collectively perform the entity retrieval task these work have shown the power of discriminative model for entity search and the benefit of utilizing the dependency among related entity what i would like to do next is to develop a unified probabilistic framework to investigate the research question raised in this proposal 
the question of how to publish an anonymized search log wa brought to the forefront by a well intentioned but privacy unaware aol search log release since then a series of ad hoc technique have been proposed in the literature though none are known to be provably private in this paper we take a major step towards a solution we show how query click and their associated perturbed count can be published in a manner that rigorously preserve privacy our algorithm is decidedly simple to state but non trivial to analyze on the opposite side of privacy is the question of whether the data we can safely publish is of any use our finding offer a glimmer of hope we demonstrate that a non negligible fraction of query and click can indeed be safely published via a collection of experiment on a real search log in addition we select an application keyword generation and show that the keyword suggestion generated from the perturbed data resemble those generated from the original data 
this paper explores the problem of computing pairwise similarity on document collection focusing on the application of more like this query in the life science domain three mapreduce algorithm are introduced one based on brute force a second where the problem is treated a large scale ad hoc retrieval and a third based on the cartesian product of posting list each algorithm support one or more approximation that trade eectiveness for eciency the characteristic of which are studied experimentally result show that the brute force algorithm is the most ecient of the three when exact similarity is desired however the other two algorithm support approximation that yield large efciency gain without signicant loss of eectiveness 
in this paper we give model and algorithm to describe and analyze the collaboration among author of wikipedia from a network analytical perspective the edit network encodes who interacts how with whom when editing an article it significantly extends previous network model that code author community in wikipedia several characteristic summarizing some aspect of the organization process and allowing the analyst to identify certain type of author can be obtained from the edit network moreover we propose several indicator characterizing the global network structure and method to visualize edit network it is shown that the structural network indicator are correlated with quality label of the associated wikipedia article 
we present a novel method for key term extraction from text document in our method document is modeled a a graph of semantic relationship between term of that document we exploit the following remarkable feature of the graph the term related to the main topic of the document tend to bunch up into densely interconnected subgraphs or community while non important term fall into weakly interconnected community or even become isolated vertex we apply graph community detection technique to partition the graph into thematically cohesive group of term we introduce a criterion function to select group that contain key term discarding group with unimportant term to weight term and determine semantic relatedness between them we exploit information extracted from wikipedia using such an approach give u the following two advantage first it allows effectively processing multi theme document second it is good at filtering out noise information in the document such a for example navigational bar or header in web page evaluation of the method show that it outperforms existing method producing key term with higher precision and recall additional experiment on web page prove that our method appears to be substantially more effective on noisy and multi theme document than existing method 
the rapid growth of the number of video in youtube provides enormous potential for user to find content of interest to them unfortunately given the difficulty of searching video the size of the video repository also make the discovery of new content a daunting task in this paper we present a novel method based upon the analysis of the entire user video graph to provide personalized video suggestion for user the resulting algorithm termed adsorption provides a simple method to efficiently propagate preference information through a variety of graph we extensively test the result of the recommendation on a three month snapshot of live data from youtube 
a lot of situation from daily life have found their counterpart in the internet buying or selling good discovering information or making social connection have been very successful in the virtual world and make the web the central working place until now the purpose of a web site ha been defined at the time of creation extension of the available functionality or transfer of the functionality onto another site ha only been possible with considerable additional effort with usekit we present a software platform that allows user to add individual selected functionality to any web site without installing software usekit offer a new approach towards a collaborative personalized and individually compiled view of the web usekit focus on personalized application and service that can be applied to any web site in analogy to file system permission this can be seen a an instance of the executable web or web if web wa read only and web wa read write user contributed code will morph online application into omnipresent functional platform with a single interface 
we consider the question of whether average precision a a measure of retrieval effectiveness can be regarded a deriving from a model of user searching behaviour it turn out that indeed it can be so regarded under a very simple stochastic model of user behaviour 
the proximity of query term in a document is a very important information to enable ranking model go beyond the bag of word assumption in information retrieval this paper study the integration of term proximity information into the unigram language modeling a new proximity language model plm is proposed which view query term proximity centrality a the dirichlet hyper parameter that weight the parameter of the unigram document language model several form of proximity measure are developed to be used in plm which could compute a query term s proximate centrality in a specific document in experiment the proximity language model is compared with the basic language model and previous work that combine the proximity information with language model using linear score combination the experiment result show that the proposed model performs better in both top precision and average precision 
traditional relation extraction method require pre specified relation and relation specific human tagged example bootstrapping system significantly reduce the number of training example but they usually apply heuristic based method to combine a set of strict hard rule which limit the ability to generalize and thus generate a low recall furthermore existing bootstrapping method do not perform open information extraction open ie which can identify various type of relation without requiring pre specification in this paper we propose a statistical extraction framework called statistical snowball statsnowball which is a bootstrapping system and can perform both traditional relation extraction and open ie statsnowball us the discriminative markov logic network mlns and softens hard rule by learning their weight in a maximum likelihood estimate sense mln is a general model and can be configured to perform different level of relation extraction in statsnwoball pattern selection is performed by solving an l norm penalized maximum likelihood estimation which enjoys well founded theory and efficient solver we extensively evaluate the performance of statsnowball in different configuration on both a small but fully labeled data set and large scale web data empirical result show that statsnowball can achieve a significantly higher recall without sacrificing the high precision during iteration with a small number of seed and the joint inference of mln can improve the performance finally statsnowball is efficient and we have developed a working entity relation search engine called renlifang based on it 
the web ha become an important medium for news delivery and consumption fresh content about a variety of topic and event is constantly being created and published on the web by many source a intuitively understood by reader and studied in journalism news article produced by different social group present different attitude towards and interpretation of the same news issue in this paper we propose a new paradigm for aggregating news article according to the news source related to the stakeholder of the news issue we implement this paradigm in a prototype system called localsavvy the system provides user the capability to aggregate and browse various local view about the news issue in which they are interested 
many overload control mechanism for web based application aim to prevent overload by setting limit on factor such a admitted load number of server thread buffer size for this they need online measurement of metric such a response time throughput and resource utilization this requires instrumentation of the server by modifying server code which may not be feasible or desirable an alternate approach is to use a proxy between the client and server we have developed a proxy based overload control platform called masth proxy multi class admission controlled self tuning http proxy it record detailed measurement support multiple request class manages queue of http request provides tunable parameter and enables easy implementation of dynamic overload control this give designer of overload control scheme a platform where they can concentrate on developing the core control logic without the need to modify upstream server code 
this paper present an end user oriented programming environment called mashroom major contribution herein include an end user programming model with an expressive data structure a well a a set of formally defined mashup operator the data structure take advantage of nested table and maintains the intuitiveness while allowing user to express complex data object the mashup operator are visualized with contextual menu and formula bar and can be directly applied on the data experiment and case study reveal that end user have little difficulty in effectively and efficiently using mashroom to build mashup application 
complex dialog with comprehensive underlying data model are gaining increasing importance in today s web application this in turn accelerates the need for highly dynamic dialog offering guidance to the user and thus reducing cognitive overload beyond that requirement from the field of aesthetic web accessibility platform independence and web service integration arise to this end we present an evolutionary extensible approach for the model driven construction of advanced dialog it is based on a domain specific language dsl focusing on simplicity and fostering collaboration with stakeholder 
a standard approach for determining a dirichlet smoothing parameter is to choose a value which maximizes a retrieval performance metric using training data consisting of query and relevance judgment there are however situation where training data doe not exist or the query and relevance judgment do not reflect typical user information need for the application we propose an unsupervised approach for estimating a dirichlet smoothing parameter based on collection statistic we show empirically that this approach can suggest a plausible dirichlet smoothing parameter value in case where relevance judgment cannot be used 
this paper evaluates undergraduate student knowledge interest and experience with topic from the trec robust track collection the goal is to characterize these topic along several dimension to help researcher make more informed decision about which topic are most appropriate to use in experimental iir evaluation with undergraduate student subject 
webgl leverage the power of opengl to present accelerated d graphic on a webpage the ability to put hardwareaccelerated d content in the browser will provide a mean for the creation of new web based application that were previously the exclusive domain of the desktop environment it will also allow the inclusion of feature that standalone d application do not have while webgl succeeds in bringing the power and lowlevel api of opengl to the browser it also expects a lot of web developer who are used to the dom and javascript library like jquery this paper will look at how mid level apis can help web developer create unique d content that is more than just duplicate of a standalone desktop application on a web page we will present one such web application named motionview built with c dl that provides a new mean for artist and motion capture studio to communicate with each other we will also highlight some upcoming project idea that make use of d browser technology in a way that would not have been possible in a desktop environment 
social medium sharing web site like flickr allow user to annotate image with free tag which significantly facilitate web image search and organization however the tag associated with an image generally are in a random order without any importance or relevance information which limit the effectiveness of these tag in search and other application in this paper we propose a tag ranking scheme aiming to automatically rank the tag associated with a given image according to their relevance to the image content we first estimate initial relevance score for the tag based on probability density estimation and then perform a random walk over a tag similarity graph to refine the relevance score experimental result on a flickr photo collection show that the proposed tag ranking method is both effective and efficient we also apply tag ranking into three application tag based image search tag recommendation and group recommendation which demonstrates that the proposed tag ranking approach really boost the performance of social tagging related application 
recommender system are an important component of many website two of the most popular approach are based on matrix factorization mf and markov chain mc mf method learn the general taste of a user by factorizing the matrix over observed user item preference on the other hand mc method model sequential behavior by learning a transition graph over item that is used to predict the next action based on the recent action of a user in this paper we present a method bringing both approach together our method is based on personalized transition graph over underlying markov chain that mean for each user an own transition matrix is learned thus in total the method us a transition cube a the observation for estimating the transition are usually very limited our method factorizes the transition cube with a pairwise interaction model which is a special case of the tucker decomposition we show that our factorized personalized mc fpmc model subsumes both a common markov chain and the normal matrix factorization model for learning the model parameter we introduce an adaption of the bayesian personalized ranking bpr framework for sequential basket data empirically we show that our fpmc model outperforms both the common matrix factorization and the unpersonalized mc model both learned with and without factorization 
privacy is an enormous problem in online social networking site while site such a facebook allow user fine grained control over who can see their profile it is difficult for average user to specify this kind of detailed policy in this paper we propose a template for the design of a social networking privacy wizard the intuition for the design come from the observation that real user conceive their privacy preference which friend should be able to see which information based on an implicit set of rule thus with a limited amount of user input it is usually possible to build a machine learning model that concisely describes a particular user s preference and then use this model to configure the user s privacy setting automatically a an instance of this general framework we have built a wizard based on an active learning paradigm called uncertainty sampling the wizard iteratively asks the user to assign privacy label to selected informative friend and it us this input to construct a classifier which can in turn be used to automatically assign privilege to the rest of the user s unlabeled friend to evaluate our approach we collected detailed privacy preference data from real facebook user our study revealed two important thing first real user tend to conceive their privacy preference in term of community which can easily be extracted from a social network graph using existing technique second our active learning wizard using community a feature is able to recommend high accuracy privacy setting using le user input than existing policy specification tool 
because it is an integral part of the internet routing apparatus and because it allows multiple instance of the same service to be naturally discovered ip anycast ha many attractive feature for any service that involve the replication of multiple instance across the internet while briefly considered a an enabler when content distribution network cdns first emerged the use of ip anycast wa deemed infeasible in that environment the main reason for this decision were the lack of load awareness of ip anycast and unwanted side effect of internet routing change on the ip anycast mechanism prompted by recent development in route control technology a well a a better understanding of the behavior of ip anycast in operational setting we revisit this decision and propose a load aware ip anycast cdn architecture that address these concern while benefiting from inherent ip anycast feature our architecture make use of route control mechanism to take server and network load into account to realize load aware anycast we show that the resulting redirection requirement can be formulated a a generalized assignment problem and present practical algorithm that address these requirement while at the same time limiting session disruption that plague regular ip anycast we evaluate our algorithm through trace based simulation using trace obtained from an operation cdn network 
web search engine use index to eciently retrieve page containing specified query term a well a page linking to specified page the problem of compressed index that permit such fast retrieval ha a long history we consider the problem assuming that the term in or link to a page are generated from a probability distribution how well compactly can we build such index that allow fast retrieval of particular interest is the case when the probability distribution is zipfian or a similar power law since these are the distribution that arise on the web we obtain sharp bound on the space requirement of boolean index for text document that follow zipf s law in the process we develop a general technique that applies to any probability distribution not necessarily a power law this is the first analysis of compression in index under arbitrary distribution our bound lead to quantitative version of rule of thumb that are folklore in indexing our experiment on several document collection show that the distribution of term appears to follow a double pareto law rather than zipf s law despite widely varying set of document the index size observed in the experiment conform well to our theoretical prediction 
address geocoding the process of finding the map location for a structured postal address is a relatively well studied problem in this paper we consider the more general problem of crosslingual location search where the query are not limited to postal address and the language and script used in the search query is different from the one in which the underlying data is stored to the best of our knowledge our system is the first crosslingual location search system that is able to geocode complex address we use a statistical machine transliteration system to convert location name from the script of the query to that of the stored data however we show that it is not sufficient to simply feed the resulting transliteration into a monolingual geocoding system a the ambiguity inherent in the conversion drastically expands the location search space and significantly lower the quality of result the strength of our approach lie in it integrated end to end nature we use abstraction and fuzzy search in the text domain to achieve maximum coverage despite transliteration ambiguity while applying spatial constraint in the geographic domain to focus only on viable interpretation of the query our experiment with structured and unstructured query in a set of diverse language and script arabic english hindi and japanese searching for location in different region of the world show full crosslingual location search accuracy at level comparable to that of commercial monolingual system we achieve these level of performance using technique that may be applied to crosslingual search in any language script and over arbitrary spatial data 
many have speculated that classifying web page can improve a search engine s ranking of result intuitively result should be more relevant when they match the class of a query we present a simple framework for classicationenhanced ranking that us click in combination with the classication of web page to derive a class distribution for the query we then go on to dene a variety of feature that capture the match between the class distribution of a web page and a query the ambiguity of a query and the coverage of a retrieved result relative to a query s set of class experimental result demonstrate that a ranker learned with these feature signicantly improves ranking over a competitive baseline furthermore our methodology is agnostic with respect to the classication space and can be used to derive query class for a variety of dierent taxonomy 
in this paper we describe technique for the discovery and construction of user profile leveraging from the emergent data web our system address the problem of sparseness of user profile information currently faced by both asserted and inferred profile system a profile mediator that dynamically build the most suitable user profile for a particular service or interaction in real time is employed in our prototype implementation 
recent research ha demonstrated beyond doubt the benefit of compressing natural language text using word based statistical semistatic compression not only it achieves extremely competitive compression rate but also direct search on the compressed text can be carried out faster than on the original text indexing based on inverted list benefit from compression a well such compression method assign a variable length codeword to each different text word some coding method plain huffman and restricted prefix byte code do not clearly mark codeword boundary and hence cannot be accessed at random position nor searched with the fastest text search algorithm other coding method tagged huffman end tagged dense code or s c dense code do mark codeword boundary achieving a self synchronization property that enables fast search and random access in exchange for some loss in compression effectiveness in this paper we show that by just performing a simple reordering of the target symbol in the compressed text more precisely reorganizing the byte into a wavelet treelike shape and using little additional space searching capability are greatly improved without a drastic impact in compression and decompression time with this approach all the code achieve synchronism and can be searched fast and accessed at arbitrary point moreover the reordered compressed text becomes an implicitly indexed representation of the text which can be searched for word in time independent of the text length that is we achieve not only fast sequential search time but indexed search time for almost no extra space cost we experiment with three well known word based compression technique with different characteristic plain huffman end tagged dense code and restricted prefix byte code and show the searching capability achieved by reordering the compressed representation on several corpus we show that the reordered version are not only much more efficient than their classical counterpart but also more efficient than explicit inverted index built on the collection when using the same amount of space 
we propose a novel method based on concept from expander graph to sample community in network we show that our sampling method unlike previous technique produce subgraphs representative of community structure in the original network these generated subgraphs may be viewed a stratified sample in that they consist of member from most or all community in the network using sample produced by our method we show that the problem of community detection may be recast into a case of statistical relational learning we empirically evaluate our approach against several real world datasets and demonstrate that our sampling method can effectively be used to infer and approximate community affiliation in the larger network 
in this paper we look at the social tag prediction problem given a set of object and a set of tag applied to those object by user can we predict whether a given tag could should be applied to a particular object we investigated this question using one of the largest crawl of the social bookmarking system del icio u gathered to date for url in del icio u we predicted tag based on page text anchor text surrounding host and other tag applied to the url we found an entropy based metric which capture the generality of a particular tag and informs an analysis of how well that tag can be predicted we also found that tag based association rule can produce very high precision prediction a well a giving deeper understanding into the relationship between tag our result have implication for both the study of tagging system a potential information retrieval tool and for the design of such system 
we consider the problem of optimal tagging for navigational purpose in one s own collection what is the best that a forgetful user can hope for in term of ease of retrieving a labeled object we prove that the number of tag ha to increase logarithmically in the collection size to maintain a manageable result set using flickr data we then show that user do indeed apply more and more tag a their collection grows and that this is not due to a global increase in tagging activity however a the additional term applied are not statistically independent user of large collection still have to deal with larger and larger result set even when more tag are used a search term we pose optimal tag suggestion for navigational purpose a an open problem 
this paper present a topic model that detects topic distribution over time our proposed model trend detection model tdm introduces a latent trend class variable into each document the trend class ha a probability distribution over topic and a continuous distribution over time experiment using our data set show that tdm is useful a a generative model in the analysis of the evolution of trend 
when search engine user have trouble finding information they may become frustrated possibly resulting in a bad experience even if they are ultimately successful in a user study in which participant were given difficult information seeking task half of all query submitted resulted in some degree of self reported frustration a third of all successful task involved at least one instance of frustration by modeling searcher frustration search engine can predict the current state of user frustration and decide when to intervene with alternative search strategy to prevent the user from becoming more frustrated giving up or switching to another search engine we present several model to predict frustration using feature extracted from query log and physical sensor we are able to predict frustration with a mean average precision of from the physical sensor and from the query log feature 
we describe a method for applying parsimonious language model to re estimate the term probability assigned by relevance model we apply our method to six topic set from test collection in five different genre our parsimonious relevance model i improve retrieval effectiveness in term of map on all collection ii significantly outperform their non parsimonious counterpart on most measure and iii have a precision enhancing effect unlike other blind relevance feedback method 
social tagging provides valuable and crucial information for large scale web image retrieval it is ontology free and easy to obtain however irrelevant tag frequently appear and user typically will not tag all semantic object in the image which is also called semantic loss to avoid noise and compensate for the semantic loss tag recommendation is proposed in literature however current recommendation simply rank the related tag based on the single modality of tag co occurrence on the whole dataset which ignores other modality such a visual correlation this paper proposes a multi modality recommendation based on both tag and visual correlation and formulates the tag recommendation a a learning problem each modality is used to generate a ranking feature and rankboost algorithm is applied to learn an optimal combination of these ranking feature from different modality experiment on flickr data demonstrate the effectiveness of this learning based multi modality recommendation strategy 
people search is an important search service with multiple application eg looking up a friend on facebook finding colleague in corporate email directory etc with the proportion of non english user on a steady rise people search service are being used by user from diverse language demographic user may issue name search query against these directory in language other than the language of the directory in which case the present monolingual name search approach will not work in this demo we present a multilingual people search system capable of performing fast name lookup on large user directory independent of the directory language our system ha application in area like social networking enterprise search and email address book search 
web service development and usage ha shifted from simple information processing service to high value business service that are crucial to productivity and success in order to deal with an increasing risk of unavailability or failure of mission critical web service we argue the need for advanced reservation of service in the form of derivative the contribution of this paper is twofold first we provide an abstract model of a market design that enables the trade of derivative for mission critical web service our model satisfies requirement that result from service characteristic such a intangibility and the impossibility to inventor service in order to meet fluctuating demand it comprehends principle from model of incomplete market such a the absence of a tradeable underlying and consistent arbitragefree derivative pricing furthermore we provide an architecture for a web service market that implement our model and describes the strategy space and interaction of market participant in the trading process of service derivative we compare the underlying pricing process to existing derivative model in energy exchange discus eventual shortcoming and propose wavelet a a preprocessing tool to analyze actual data and extract longand short term seasonalities 
wiki content templating enables reuse of content structure among wiki page in this paper we present a thorough study of this widespread feature showing how it two state of the art model functional and creational templating are sub optimal we then propose a third better model called lightly constrained lc templating and show it implementation in the moin wiki engine we also show how lc templating implementation are the appropriate technology to push forward semantically rich web page on the line of lowercase semantic web and microformats 
personalized web search ha emerged a one of the hottest topic for both the web industry and academic researcher however the majority of study on personalized search focused on a rather simple type of search which leaf an important research topic the personalization in exploratory search a an under studied area in this paper we present a study of personalization in task based information exploration using a system called tasksieve tasksieve is a web search system that utilizes a relevance feedback based profile called a task model for personalization it innovation include flexible and user controlled integration of query and task model task infused text snippet generation and on screen visualization of task model through an empirical study using human subject conducting task based exploration search we demonstrate that tasksieve push significantly more relevant document to the top of search result list a compared to a traditional search system tasksieve help user select significantly more accurate information for their task allows the user to do so with higher productivity and is viewed more favorably by subject under several usability related characteristic 
combating web spam ha become one of the top challenge for web search engine state of the art spam detection technique are usually designed for specific known type of web spam and are incapable and inefficient for recently appeared spam with user behavior analysis into web access log we propose a spam page detection algorithm based on bayes learning preliminary experiment on web access data collected by a commercial web site containing over billion user click in month show the effectiveness of the proposed detection framework and algorithm 
we introduce a multi label classification model and algorithm for labeling heterogeneous network where node belong to different type and different type have different set of classification label we present a graph based approach which model the mutual influence between node in the network a a random walk when viewing class label a color the random surfer is spraying different node type with different color palette hence the name graffiti we demonstrate the performance gain of our method by comparing it to three state of the art technique for graph based classification 
in opinion finding the retrieval system is tasked with retrieving not just relevant document but which also express an opinion towards the query target entity most opinion finding system are based on a two stage approach where initially the system aim to retrieve relevant document which are then re ranked according to the extent to which they are detected to be of an opinionated nature in this work we investigate how the underlying baseline retrieval system performance affect the overall opinion finding performance we apply two effective opinion finding technique to all the baseline run submitted to the trec blog track and draw new insight and conclusion 
several attempt have been made to analyze customer behavior on online e commerce site some study particularly emphasize the social network of customer user review and rating of a product exert effect on other consumer purchasing behavior whether a user refers to other user rating depends on the trust accorded by a user to the reviewer on the other hand the trust that is felt by a user for another user correlate with the similarity of two user rating this bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online community because it suggests clustering of similar user and the evolution of strong community this paper present a theoretical model along with analysis of an actual online e commerce site we analyzed a large community site in japan cosme the noteworthy characteristic of cosme are that user can bookmark their trusted user in addition they can post their own rating of product which facilitates our analysis of the rating bidirectional effect on trust and rating we describe an overview of the data in cosme analysis of effect from trust to rating and vice versa and our proposition of a measure of community gravity which measure how strongly a user might be attracted to a community our study is based on the cosme dataset in addition to the epinions dataset it elucidates important insight and proposes a potentially important measure for mining online social network 
the markov random walk model ha been recently exploited for multi document summarization by making use of the link relationship between sentence in the document set under the assumption that all the sentence are indistinguishable from each other however a given document set usually cover a few topic theme with each theme represented by a cluster of sentence the topic theme are usually not equally important and the sentence in an important theme cluster are deemed more salient than the sentence in a trivial theme cluster this paper proposes the cluster based conditional markov random walk model clustercmrw and the cluster based hit model clusterhits to fully leverage the cluster level information experimental result on the duc and duc datasets demonstrate the good effectiveness of our proposed summarization model the result also demonstrate that the clustercmrw model is more robust than the clusterhits model with respect to different cluster number 
it is crucial for a web crawler to distinguish between ephemeral and persistent content ephemeral content e g quote of the day is usually not worth crawling because by the time it reach the index it is no longer representative of the web page from which it wa acquired on the other hand content that persists across multiple page update e g recent blog posting may be worth acquiring because it match the page s true content for a sustained period of time in this paper we characterize the longevity of information found on the web via both empirical measurement and a generative model that coincides with these measurement we then develop new recrawl scheduling policy that take longevity into account a we show via experiment over real web data our policy obtain better freshness at lower cost compared with previous approach 
the vector space model ha been and to a great extent still is the de facto choice for profile representation in content based information filtering however user profile represented a weighted keyword vector have inherent dimensionality problem a the number of profile keywords increase the vector representation becomes ambiguous due to the exponential increase in the volume of the vector space and in the number of possible keyword combination we argue that the complexity and dynamic of information filtering require user profile representation which are resilient and resistant to this curse of dimensionality a user profile ha to be able to incorporate many feature and to adapt to a variety of interest change we propose an alternative network based profile representation that meet these challenging requirement experiment show that the network profile representation can more effectively capture additional information about a user s interest and thus achieve significant performance improvement over a vector based representation comprising the same weighted keywords 
we consider the problem of segmenting a webpage into visually and semantically cohesive piece our approach is based on formulating an appropriate optimization problem on weighted graph where the weight capture if two node in the dom tree should be placed together or apart in the segmentation we present a learning framework to learn these weight from manually labeled data in a principled manner our work is a significant departure from previous heuristic and rule based solution to the segmentation problem the result of our empirical analysis bring out interesting aspect of our framework including variant of the optimization problem and the role of learning 
the automatic content linking device monitor a conversation and us automatically recognized word to retrieve document that are of potential use to the participant the document set includes project related report or email transcribed snippet of past meeting and website retrieval result are displayed at regular interval 
we introduce a novel approach to expert finding based on multi step relevance propagation from document to related candidate relevance propagation is modeled with an absorbing random walk the evaluation on the two official enterprise trec data set demonstrates the advantage of our method over the state of the art method based on one step propagation 
music theme annotation would be really beneficial for supporting retrieval but are often neglected by user while annotating thus in order to support user in tagging and to fill the gap in the tag space in this paper we develop algorithm for recommending theme annotation our method exploit already existing user tag the lyric of music track a well a combination of both we compare the result for our recommended theme annotation against genre and style recommendation a much easier and already studied task we evaluate the quality of our recommended tag against an expert ground truth data set our result are promising and provide interesting insight into possible extension for music tagging system to support music search 
the emergence of semantic web technology and standard such a resource description framework rdf ha introduced novel data storage model such a the rdf graph model in this paper we present a research effort called r d which attempt to bridge the gap between rdf and rdbms concept by presenting a relational view of rdf data store thus r d is essentially a relational wrapper around rdf store that aim to make the variety of stable relational tool that are currently in the market available to rdf store without data duplication and synchronization issue 
hierarchical topic taxonomy have proliferated on the world wide web and exploiting the output space decomposition they induce in automated classification system is an active area of research in many domain classifier learned on a hierarchy of class have been shown to outperform those learned on a flat set of class in this paper we argue that the hierarchical arrangement of class lead to intuitive relationship between the corresponding classifier output score and that enforcing these relationship a a post processing step after classification can improve it accuracy we formulate the task of smoothing classifier output a a regularized isotonic tree regression problem and present a dynamic programming based method that solves it optimally this new problem generalizes the classic isotonic tree regression problem and both the new formulation and algorithm might be of independent interest in our empirical analysis of two real world text classification scenario we show that our approach to smoothing classifier output result in improved classification accuracy 
determining candidate view on important issue is critical in deciding whom to support and vote for but finding their statement and vote on an issue can be laborious in this paper we present psst political statement and support tracker a search engine to facilitate analysis of political statement and vote over time we show that prior tool for text analysis can be combined with minimal manual processing to provide a first step in the full automation of this process 
in this paper we study the community structure of endorsement network i e social network in which a directed edge u v is asserting an action of support from user u to user v example include scenario in which a user u is favoring a photo liking a post or following the microblog of user v starting from the hypothesis that the footprint of a community in an endorsement network is a bipartite directed clique from a set of follower to a set of leader we apply frequent itemset mining technique to discover such bicliques our analysis of real network discovers that an interesting phenomenon is taking place the leader of a community are endorsing each other forming a very dense nucleus 
social bookmarking ha emerged a a growing source of human generated content on the web in essence bookmarking involves url and tag on them in this paper we perform a large scale study of the usefulness of bookmarked url from the top social bookmarking site delicious instead of focusing on the dimension of tag which ha been covered in the previous work we explore social bookmarking from the dimension of url more specifically we investigate the delicious url and their content to quantify their value to a search engine for their value in leading to good content we show that the delicious url have higher quality content and more external outlinks for their value in satisfying user we show that the delicious url have more clicked url a well a get more click we suggest that based on their value the delicious url should be used a another source of seed url for crawler 
information retrieval model usually represent content only and not other consideration such a authority cost and recency how could multiple criterion be utilized in information retrieval and how would it effect the result in our experiment using multiple user centric criterion always produced better result than a single criterion 
indexing and retrieval of speech content in various form such a broadcast news customer care data and on line medium ha gained a lot of interest for a wide range of application from customer analytics to on line medium search for most retrieval application the speech content is typically first converted to a lexical or phonetic representation using automatic speech recognition asr the first step in searching through index built on these representation is the generation of pronunciation for named entity and foreign language query term this paper summarizes the result of the work conducted during the jhu summer workshop by the multilingual spoken term detection team on mining the web for pronunciation and analyzing their impact on spoken term detection we will first present method to use the vast amount of pronunciation information available on the web in the form of ipa and ad hoc transcription we describe technique for extracting candidate pronunciation from web page and associating them with orthographic word filtering out poorly extracted pronunciation normalizing ipa pronunciation to better conform to a common transcription standard and generating phonemic representation from ad hoc transcription we then present an analysis of the effectiveness of using these pronunciation to represent out of vocabulary oov query term on the performance of a spoken term detection std system we will provide comparison of web pronunciation against automated technique for pronunciation generation a well a pronunciation generated by human expert our result cover a range of speech index based on lattice confusion network and one best transcription at both word and word fragment level 
web site operator internet user and online service provider are besieged by a growing array of abuse and threat spam lead user to online scam and phishing web page which cyber criminal implant on innocent site to fool the unwary into revealing their financial data and password other criminal use web site to spread malware which can steal personal data or take over user computer into a botnet which can be used to send spam or mount cyber attack against web site and other internet service together these abuse undermine user trust hamper e commerce and cost the internet community huge loss in money service and support cost and time what should web site operator and online service provider do to protect themselves and their user what are internet company organization and law enforcement doing and not doing to combat these problem and how can the international internet community work together on these problem the panel brings together representative from the chain of organization that respond to internet abuse problem and promise a lively compelling and relevant discussion 
there ha been a lot of work on evaluating and improving the relevance of web search engine in this paper we suggest using human computation game to elicit data from player that can be used to improve search we describe page hunt a single player game the data elicited using page hunt ha several application including providing metadata for page providing query alteration for use in query refinement and identifying ranking issue we describe an experiment with over game player and highlight some interesting aspect of the data obtained 
in this paper we propose to model the blended search problem by assuming conditional dependency among query v and search result the probability distribution of this model are learned from search engine query log through unigram language model our experimental exploration show that a large number of query in generic web search have vertical search intention and our proposed algorithm can effectively blend vertical search result into generic web search which can improve the mean average precision map by a much a compared to traditional web search without blending 
it is notoriously difficult to program a solid web application besides addressing web interaction state maintenance and whimsical user navigation behavior programmer must also avoid a minefield of security vulnerability the problem is twofold first we lack a clear understanding of the new computation model underlying web application second we lack proper abstraction for hiding common and subtle coding detail that are orthogonal to the business functionality of specific web application this paper address both issue first we present a language bass for declarative server side scripting bass allows programmer to work in an ideal world using new abstraction to tackle common but problematic aspect of web programming the meta property of bass provide useful security guarantee second we present a language moss reflecting realistic web programming concept and scenario thus articulating the computation model behind web programming finally we present a translation from bass to moss demonstrating how the ideal programming model and security guarantee of bass can be implemented in practice 
although anchor text provides very useful information for web search a large portion of web page have few or no incoming hyperlink anchor which is known a the anchor text sparsity problem in this paper we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page s plausible missing anchor text from it similar web page in link anchor text we design experiment with two publicly available trec web corpus gov and clueweb to evaluate different approach for discovering missing anchor text experimental result show that our approach can effectively discover plausible missing anchor term we then use the web named page finding task in the trec terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval experimental result show that our approach can statistically significantly improve retrieval performance compared with several approach that only use anchor text aggregated over the web graph 
social medium such a web forum often have dense interaction between user and content where network model are often appropriate for analysis joint non negative matrix factorization model of participation and content data can be viewed a a bipartite graph model between user and medium and is proposed for analysis social medium the factorization allow simultaneous automatic discovery of leader and sub community in the web forum a well a the core latent topic in the forum result on topic detection of web forum and cluster analysis show that social feature are highly effective for forum analysis 
user of social networking service can connect with each other by forming community for online interaction yet a the number of community hosted by such website grows over time user have even greater need for effective community recommendation in order to meet more user in this paper we investigate two algorithm from very different domain and evaluate their effectiveness for personalized community recommendation first is association rule mining arm which discovers association between set of community that are shared across many user second is latent dirichlet allocation lda which model user community co occurrence using latent aspect in comparing lda with arm we are interested in discovering whether modeling low rank latent structure is more effective for recommendation than directly mining rule from the observed data we experiment on an orkut data set consisting of user and community our empirical comparison using the top k recommendation metric show that lda performs consistently better than arm for the community recommendation task when recommending a list of or more community however for recommendation list of up to community arm is still a bit better we analyze example of the latent information learned by lda to explain this finding to efficiently handle the large scale data set we parallelize lda on distributed computer and demonstrate our parallel implementation s scalability with varying number of machine 
portable reusable test collection are a vital part of research and development in information retrieval reusability is difficult to ass however the standard approach simulating judgment collection when group of system are held out then evaluating those held out system only work when there is a large set of relevance judgment to draw on during the simulation a test collection adapt to larger and larger corpus it becomes le and le likely that there will be sufficient judgment for such simulation experiment thus we propose a methodology for information retrieval experimentation that collect evidence for or against the reusability of a test collection while judgment are being made using this methodology along with the appropriate statistical analysis researcher will be able to estimate the reusability of their test collection while building them and implement course correction if the collection doe not seem to be achieving desired level of reusability we show the robustness of our design to inherent source of variance and provide a description of an actual implementation of the framework for creating a large test collection 
in this paper we propose a new phrase based ir model which integrates a measure of inseparability of phrase our experiment show it high potential to produce large improvement in retrieval effectiveness 
ranked retrieval ha a particular disadvantage in comparison with traditional boolean retrieval there is no clear cut off point where to stop consulting result this is a serious problem in some setup we investigate and further develop method to select the rank cut off value which optimizes a given effectiveness measure assuming no other input than a system s output for a query document score and their distribution the task is essentially a score distributional threshold optimization problem the recent trend in modeling score distribution is to use a normal exponential mixture normal for relevant and exponential for non relevant document score we discus the two main theoretical problem with the current model support incompatibility and non convexity and develop new model that address them the main contribution of the paper are two truncated normal exponential model varying in the way the out truncated score range are handled we conduct a range of experiment using the trec and legal track data and show that the truncated model lead to significantly better result 
it is generally believed that propagated anchor text is very important for effective web search a offered by the commercial search engine google bomb are a notable illustration of this however many year of trec web retrieval research failed to establish the effectiveness of link evidence for ad hoc retrieval on web collection the ultimate resolution to this dilemma wa that typical web search is very different from the traditional ad hoc methodology so far however no one ha established why link information like incoming link degree or anchor text doe not help ad hoc retrieval effectiveness several possible explanation were given including the collection being too small for anchor to be effective and the density of the link graph being too low the new trec web track collection is substantially larger than previous collection and ha a dense link graph our main finding is that propagated anchor text outperforms full text retrieval in term of early precision and in combination with it give an improvement in overall precision we then analyse the impact of link density and collection size by down sampling the number of link and the number of page respectively other finding are that contrary to expectation inter server link density ha little impact on effectiveness while the size of the collection ha a substantial impact on the quantity quality and effectiveness of anchor text we also compare the diversity of the search result of anchor text and full text approach which show that anchor text performs significantly better than full text search and confirm our finding for the ad hoc search task 
this paper address the issue of automatically extracting keyphrases from a document previously this problem wa formalized a classification and learning method for classification were utilized this paper point out that it is more essential to cast the problem a ranking and employ a learning to rank method to perform the task specifically it employ ranking svm a state of art method of learning to rank in keyphrase extraction experimental result on three datasets show that ranking svm significantly outperforms the baseline method of svm and naive bayes indicating that it is better to exploit learning to rank technique in keyphrase extraction 
query abandonment by search engine user is generally considered to be a negative signal in this paper we explore the concept of good abandonment we define a good abandonment a an abandoned query for which the user s information need wa successfully addressed by the search result page with no need to click on a result or refine the query we present an analysis of abandoned internet search query across two modality pc and mobile in three locale the goal is to approximate the prevalence of good abandonment and to identify type of information need that may lead to good abandonment across different locale and modality our study ha three key finding first query potentially indicating good abandonment make up a significant portion of all abandoned query second the good abandonment rate from mobile search is significantly higher than that from pc search across all locale tested third classified by type of information need the major class of good abandonment vary dramatically by both locale and modality our finding imply that it is a mistake to uniformly consider query abandonment a a negative signal further there is a potential opportunity for search engine to drive additional good abandonment especially for mobile search user by improving search feature and result snippet 
on line social network such a facebook are increasingly utilized by many user these network allow people to publish detail about themselves and connect to their friend some of the information revealed inside these network is private and it is possible that corporation could use learning algorithm on the released data to predict undisclosed private information in this paper we explore how to launch inference attack using released social networking data to predict undisclosed private information about individual we then explore the effectiveness of possible sanitization technique that can be used to combat such inference attack under different scenario 
search and recommendation system must include contextual information to effectively model user interest in this paper we present a systematic study of the effectiveness of five variant source of contextual information for user interest modeling post query navigation and general browsing behavior far outweigh direct search engine interaction a an information gathering activity therefore we conducted this study with a focus on website recommendation rather than search result the five contextual information source used are social historic task collection and user interaction we evaluate the utility of these source and overlap between them based on how effectively they predict user future interest our finding demonstrate that the source perform differently depending on the duration of the time window used for future prediction and that context overlap outperforms any isolated source designer of website suggestion system can use our finding to provide improved support for post query navigation and general browsing behavior 
with telecom market reaching saturation in many geography and revenue from voice call decreasing telecom operator are trying to identify new source of revenue for this purpose these operator can take advantage of their core functionality like location call control etc by exposing them a service to be composed by developer with third party offering available over the web to hide the complexity of underlying telecom protocol from application developer the operator are steadily adopting service oriented architecture soa and reference standard like parlay x and ims however a number of challenge still remain in rapid utilization of telecom functionality for creating new application existence of multiple protocol different class of developer and the need to coordinate and manage usage of these functionality in this paper we present sewnet a framework for creating application exploiting telecom functionality exposed over a converged ip network more specifically sewnet a provides an abstraction model for encapsulating invocation coordination and enrichment of the telecom functionality b render a service creation environment on top of this model and c caters to various different category of developer with the help of two use case scenario we demonstrate how sewnet can create service utilizing rich telecom functionality 
entity information management eim deal with organizing processing and delivering information about entity it emergence is a result of satisfying more sophisticated information need that go beyond document search in the recent year entity retrieval ha attracted much attention in the ir community inex ha started the xml entity ranking track since and trec ha launched the entity track since to investigate the problem of related entity finding some eim problem go beyond retrieval and ranking such a entity profiling which is about characterizing a specific entity and entity distillation which is about discovering the trend about an entity these problem have received le attention while they have many important application on the other hand the entity in the real world or in the web environment are usually not isolated they are connected or related with each other in one way or another for example the coauthorship make the author with similar research interest be connected the emergence of social medium such a facebook twitter and youtube ha further interweaved the related entity in a much larger scale million of user in these site can become friend fan or follower of others or tagger or commenters of different type of entity e g bookmark photo and video these network are complex in the sense that they are heterogeneous with multiple type of entity and of interaction they are large scale they are multi lingual and they are dynamic these feature of the complex network go beyond traditional social network analysis and require further research in this proposed research i investigate entity information management in the environment of complex network the main research question is how can the eim task be facilitated by modeling the content and structure of complex network the research is in the intersection of content based information retrieval and complex network analysis which deal with both unstructured text data and structured network the specific targeting eim task are entity retrieval entity profiling and entity distillation in addition to the main research question the following question are considered how can we accomplish a eim task involving diverse entity and interaction type how to model the evolution of entity profile a well a the underlying complex network how can the existing cross language ir work be leveraged to build entity profile with multi lingual evidence i propose to use probabilistic model and discriminative model in particular to address the above research question in my research i have developed discriminative model for expert search to integrate arbitrary document feature and to learn flexible combination strategy to rank expert in heterogeneous information source discriminative graphical model are proposed to jointly discover homepage by inference on the homepage dependence network the dependence of table element is exploited to collectively perform the entity retrieval task these work have shown the power of discriminative model for entity search and the benefit of utilizing the dependency among related entity what i would like to do next is to develop a unified probabilistic framework to investigate the research question raised in this proposal 
the question of how to publish an anonymized search log wa brought to the forefront by a well intentioned but privacy unaware aol search log release since then a series of ad hoc technique have been proposed in the literature though none are known to be provably private in this paper we take a major step towards a solution we show how query click and their associated perturbed count can be published in a manner that rigorously preserve privacy our algorithm is decidedly simple to state but non trivial to analyze on the opposite side of privacy is the question of whether the data we can safely publish is of any use our finding offer a glimmer of hope we demonstrate that a non negligible fraction of query and click can indeed be safely published via a collection of experiment on a real search log in addition we select an application keyword generation and show that the keyword suggestion generated from the perturbed data resemble those generated from the original data 
this paper explores the problem of computing pairwise similarity on document collection focusing on the application of more like this query in the life science domain three mapreduce algorithm are introduced one based on brute force a second where the problem is treated a large scale ad hoc retrieval and a third based on the cartesian product of posting list each algorithm support one or more approximation that trade eectiveness for eciency the characteristic of which are studied experimentally result show that the brute force algorithm is the most ecient of the three when exact similarity is desired however the other two algorithm support approximation that yield large efciency gain without signicant loss of eectiveness 
in this paper we give model and algorithm to describe and analyze the collaboration among author of wikipedia from a network analytical perspective the edit network encodes who interacts how with whom when editing an article it significantly extends previous network model that code author community in wikipedia several characteristic summarizing some aspect of the organization process and allowing the analyst to identify certain type of author can be obtained from the edit network moreover we propose several indicator characterizing the global network structure and method to visualize edit network it is shown that the structural network indicator are correlated with quality label of the associated wikipedia article 
we present a novel method for key term extraction from text document in our method document is modeled a a graph of semantic relationship between term of that document we exploit the following remarkable feature of the graph the term related to the main topic of the document tend to bunch up into densely interconnected subgraphs or community while non important term fall into weakly interconnected community or even become isolated vertex we apply graph community detection technique to partition the graph into thematically cohesive group of term we introduce a criterion function to select group that contain key term discarding group with unimportant term to weight term and determine semantic relatedness between them we exploit information extracted from wikipedia using such an approach give u the following two advantage first it allows effectively processing multi theme document second it is good at filtering out noise information in the document such a for example navigational bar or header in web page evaluation of the method show that it outperforms existing method producing key term with higher precision and recall additional experiment on web page prove that our method appears to be substantially more effective on noisy and multi theme document than existing method 
the rapid growth of the number of video in youtube provides enormous potential for user to find content of interest to them unfortunately given the difficulty of searching video the size of the video repository also make the discovery of new content a daunting task in this paper we present a novel method based upon the analysis of the entire user video graph to provide personalized video suggestion for user the resulting algorithm termed adsorption provides a simple method to efficiently propagate preference information through a variety of graph we extensively test the result of the recommendation on a three month snapshot of live data from youtube 
a lot of situation from daily life have found their counterpart in the internet buying or selling good discovering information or making social connection have been very successful in the virtual world and make the web the central working place until now the purpose of a web site ha been defined at the time of creation extension of the available functionality or transfer of the functionality onto another site ha only been possible with considerable additional effort with usekit we present a software platform that allows user to add individual selected functionality to any web site without installing software usekit offer a new approach towards a collaborative personalized and individually compiled view of the web usekit focus on personalized application and service that can be applied to any web site in analogy to file system permission this can be seen a an instance of the executable web or web if web wa read only and web wa read write user contributed code will morph online application into omnipresent functional platform with a single interface 
we consider the question of whether average precision a a measure of retrieval effectiveness can be regarded a deriving from a model of user searching behaviour it turn out that indeed it can be so regarded under a very simple stochastic model of user behaviour 
the proximity of query term in a document is a very important information to enable ranking model go beyond the bag of word assumption in information retrieval this paper study the integration of term proximity information into the unigram language modeling a new proximity language model plm is proposed which view query term proximity centrality a the dirichlet hyper parameter that weight the parameter of the unigram document language model several form of proximity measure are developed to be used in plm which could compute a query term s proximate centrality in a specific document in experiment the proximity language model is compared with the basic language model and previous work that combine the proximity information with language model using linear score combination the experiment result show that the proposed model performs better in both top precision and average precision 
traditional relation extraction method require pre specified relation and relation specific human tagged example bootstrapping system significantly reduce the number of training example but they usually apply heuristic based method to combine a set of strict hard rule which limit the ability to generalize and thus generate a low recall furthermore existing bootstrapping method do not perform open information extraction open ie which can identify various type of relation without requiring pre specification in this paper we propose a statistical extraction framework called statistical snowball statsnowball which is a bootstrapping system and can perform both traditional relation extraction and open ie statsnowball us the discriminative markov logic network mlns and softens hard rule by learning their weight in a maximum likelihood estimate sense mln is a general model and can be configured to perform different level of relation extraction in statsnwoball pattern selection is performed by solving an l norm penalized maximum likelihood estimation which enjoys well founded theory and efficient solver we extensively evaluate the performance of statsnowball in different configuration on both a small but fully labeled data set and large scale web data empirical result show that statsnowball can achieve a significantly higher recall without sacrificing the high precision during iteration with a small number of seed and the joint inference of mln can improve the performance finally statsnowball is efficient and we have developed a working entity relation search engine called renlifang based on it 
the web ha become an important medium for news delivery and consumption fresh content about a variety of topic and event is constantly being created and published on the web by many source a intuitively understood by reader and studied in journalism news article produced by different social group present different attitude towards and interpretation of the same news issue in this paper we propose a new paradigm for aggregating news article according to the news source related to the stakeholder of the news issue we implement this paradigm in a prototype system called localsavvy the system provides user the capability to aggregate and browse various local view about the news issue in which they are interested 
many overload control mechanism for web based application aim to prevent overload by setting limit on factor such a admitted load number of server thread buffer size for this they need online measurement of metric such a response time throughput and resource utilization this requires instrumentation of the server by modifying server code which may not be feasible or desirable an alternate approach is to use a proxy between the client and server we have developed a proxy based overload control platform called masth proxy multi class admission controlled self tuning http proxy it record detailed measurement support multiple request class manages queue of http request provides tunable parameter and enables easy implementation of dynamic overload control this give designer of overload control scheme a platform where they can concentrate on developing the core control logic without the need to modify upstream server code 
this paper present an end user oriented programming environment called mashroom major contribution herein include an end user programming model with an expressive data structure a well a a set of formally defined mashup operator the data structure take advantage of nested table and maintains the intuitiveness while allowing user to express complex data object the mashup operator are visualized with contextual menu and formula bar and can be directly applied on the data experiment and case study reveal that end user have little difficulty in effectively and efficiently using mashroom to build mashup application 
complex dialog with comprehensive underlying data model are gaining increasing importance in today s web application this in turn accelerates the need for highly dynamic dialog offering guidance to the user and thus reducing cognitive overload beyond that requirement from the field of aesthetic web accessibility platform independence and web service integration arise to this end we present an evolutionary extensible approach for the model driven construction of advanced dialog it is based on a domain specific language dsl focusing on simplicity and fostering collaboration with stakeholder 
a standard approach for determining a dirichlet smoothing parameter is to choose a value which maximizes a retrieval performance metric using training data consisting of query and relevance judgment there are however situation where training data doe not exist or the query and relevance judgment do not reflect typical user information need for the application we propose an unsupervised approach for estimating a dirichlet smoothing parameter based on collection statistic we show empirically that this approach can suggest a plausible dirichlet smoothing parameter value in case where relevance judgment cannot be used 
this paper evaluates undergraduate student knowledge interest and experience with topic from the trec robust track collection the goal is to characterize these topic along several dimension to help researcher make more informed decision about which topic are most appropriate to use in experimental iir evaluation with undergraduate student subject 
in this paper we propose a novel unsupervised approach to query segmentation an important task in web search we use a generative query model to recover a query s underlying concept that compose it original segmented form the model s parameter are estimated using an expectation maximization em algorithm optimizing the minimum description length objective function on a partial corpus that is specific to the query to augment this unsupervised learning we incorporate evidence from wikipedia experiment show that our approach dramatically improves performance over the traditional approach that is based on mutual information and produce comparable result with a supervised method in particular the basic generative language model contributes a improvement over the mutual information based method measured by segment f on the intersection test set em optimization further improves the performance by additional knowledge from wikipedia provides another improvement of adding up to a total of improvement from to 
large search engine process thousand of query per second on billion of page making query processing a major factor in their operating cost this ha led to a lot of research on how to improve query throughput using technique such a massive parallelism caching early termination and inverted index compression we focus on technique for compressing term position in web search engine index most previous work ha focused on compressing docid and frequency data or position information in other type of text collection compression of term position in web page is complicated by the fact that term occurrence tend to cluster within document but not across document boundary making it harder to exploit clustering effect also typical access pattern for position data are different from those for docid and frequency data we perform a detailed study of a number of existing and new technique for compressing position data in web index we also study how to efficiently access position data for ranking function that take proximity feature into account 
a growing trend in commercial search engine is the display of specialized content such a news product etc interleaved with web search result ideally this content should be displayed only when it is highly relevant to the search query a it competes for space with regular result and advertisement one measure of the relevance to the search query is the click through rate the specialized content achieves when displayed hence if we can predict this click through rate accurately we can use this a the basis for selecting when to show specialized content in this paper we consider the problem of estimating the click through rate for dedicated news search result for query for which news result have been displayed repeatedly before the click through rate can be tracked online however the key challenge for which previously unseen query to display news result remains in this paper we propose a supervised model that offer accurate prediction of news click through rate and satisfies the requirement of adapting quickly to emerging news event 
cross language information retrieval method are used to determine which segment of arabic language document match name based english query we investigate and contrast a word based translation model with a character based transliteration model in order to handle spelling variation and previously unseen name we measure performance by making a novel use of the training data from the ace entity translation 
in this paper we describe a buzz based recommender system based on a large source of query in an ecommerce application the system detects burst in query trend these burst are linked to external entity like news and inventory information to find the query currently in demand which we refer to a buzz query the system follows the paradigm of limited quantity merchandising in the sense that on a per day basis the system show recommendation around a single buzz query with the intent of increasing user curiosity and improving activity and stickiness on the site a semantic neighborhood of the chosen buzz query is selected and appropriate recommendation are made on product that relate to this neighborhood 
this paper establishes the theoretical framework of b bit minwise hashing the original minwise hashing method ha become a standard technique for estimating set similarity e g resemblance with application in information retrieval data management social network and computational advertising by only storing the lowest b bit of each minwise hashed value e g b or one can gain substantial advantage in term of computational efficiency and storage space we prove the basic theoretical result and provide an unbiased estimator of the resemblance for any b we demonstrate that even in the least favorable scenario using b may reduce the storage space at least by a factor of or compared to using b or b if one is interested in resemblance 
when you write paper how many time do you want to make some citation at a place but you are not sure which paper to cite do you wish to have a recommendation system which can recommend a small number of good candidate for every place that you want to make some citation in this paper we present our initiative of building a context aware citation recommendation system high quality citation recommendation is challenging not only should the citation recommended be relevant to the paper under composition but also should match the local context of the place citation are made moreover it is far from trivial to model how the topic of the whole paper and the context of the citation place should affect the selection and ranking of citation to tackle the problem we develop a context aware approach the core idea is to design a novel non parametric probabilistic model which can measure the context based relevance between a citation context and a document our approach can recommend citation for a context effectively moreover it can recommend a set of citation for a paper with high quality we implement a prototype system in citeseerx an extensive empirical evaluation in the citeseerx digital library against many baseline demonstrates the effectiveness and the scalability of our approach 
understanding user search intent expressed through their search query is crucial to web search and online advertisement web query classiflcation qc ha been widely studied for this purpose most previous qc algorithm classify individual query without considering their context information however a exemplifled by the well known example on query jaguar many web query are short and ambiguous whose real meaning are uncertain without the context information in this paper we incorporate context information into the problem of query classiflcation by using conditional random fleld crf model in our approach we use neighboring query and their corresponding clicked url web page in search session a the context information we perform extensive experiment on real world search log and validate the efiectiveness and e ciency of our approach we show that we can improve the f score by a compared to other state of the art baseline 
we demonstrate the merit of using document cluster that are created offline to improve the overall effectiveness and performance robustness of a state of the art pseudo feedbackbased query expansion method the relevance model 
the fast development of the web provides new way for effective distribution of network based digital good a digital marketplace provides a platform to enable web user to effectively acquire share market and distribute digital content however the success of the digital marketplace business model hinge on securely managing the digital right and usage of the digital content for example the digital content should be only consumable by paid user this paper describes a web based system that enables the secure exchange of digital content between web user and prevents user from illegally re sell of the digital content part of our solution is based on broadcast encryption technology category and subject descriptor k management of computing and information system security and protection 
advertising ha become an integral and inseparable part of the world wide web however neither public auditing nor monitoring mechanism still exist in this emerging area in this paper we present our initial effort on building a content level auditing service for web based ad network our content level measurement understanding the ad distribution mechanism and evaluating location based and behavioral targeting approach bring useful auditing information to all entity involved in the online advertising business we extensively evaluate google s aol s and adblade s ad network and demonstrate how their different design philosophy dominantly affect their performance at the content level 
whole page relevance defines how well the surface level representation of all element on a search result page and the corresponding holistic attribute of the presentation respond to user information need we introduce a method for evaluating the whole page relevance of web search engine result page our key contribution is that the method allows u to investigate aspect of component relevance that are difficult or impossible to judge in isolation such aspect include component level information redundancy and cross component coherence the method we describe complement traditional document relevance measurement affords comparative relevance assessment across multiple search engine and facilitates the study of important factor such a brand presentation effect and component level quality 
most model for online advertising assume that an advertiser s value from winning an ad auction which depends on the clickthrough rate or conversion rate of the advertisement is independent of other advertisement served alongside it in the same session this ignores an important externality eect a the advertising audience ha a limited attention span a high quality ad on a page can detract attention from other ad on the same page that is the utility to a winner in such an auction also depends on the set of other winner in this paper we introduce the problem of modeling externality in online advertising and study the winner determination problem in these model our model are based on choice model on the audience side we show that in the most general case the winner determination problem is hard even to approximate however we give an approximation algorithm for this problem with an approximation factor that is logarithmic in the ratio of the maximum to the minimum bid furthermore we show that there are some interesting special case such a the case where the audience preference are single peaked where the problem can be solved exactly in polynomial time for all these algorithm we prove that the winner determination algorithm can be combined with vcg style payment to yield truthful mechanism 
we present a tool for helping xml schema designer to obtain a high quality level for their specification the tool allows one to analyze relation between class of xml document and formally prove them for instance the tool can be used to check forward and backward compatibility of recommendation when such a relation doe not hold the tool allows one to identify the reason and report detailed counter example that exemplify the problem for this purpose the tool relies on recent advance in logic based automated theorem proving technique that allow for efficient reasoning on very large set of xml document we believe this tool can be of great value for standardization body that define specification using various xml type definition language such a w c specification and are concerned with quality assurance for their normative recommendation 
with the growing amount of information on user desktop and increasing scale and complexity of intranet enterprise and desktop search are becoming two increasingly important information retrieval application while the challenge arising there are not completely different from those that the web community ha faced for year advanced web search solution are often unable to address them properly in this tutorial we give a research prospective on distinctive feature of both enterprise and desktop search explain typical search scenario and review existing ranking technique and algorithm 
result diversity is a topic of great importance a more facet of query are discovered and user expect to find their desired facet in the first page of the result however the underlying question of how diversity interplay with quality and when preference should be given to one or both are not well understood in this work we model the problem a expectation maximization and study the challenge of estimating the model parameter and reaching an equilibrium one model parameter for example is correlation between page which we estimate using textual content of page and click data when available we conduct experiment on diversifying randomly selected query from a query log and the query chosen from the disambiguation topic of wikipedia our algorithm improves upon google in term of the diversity of random query retrieving to more aspect of query in top while maintaining a precision very close to google on a more selective set of query that are expected to benefit from diversification our algorithm improves upon google in term of precision and diversity of the result and significantly outperforms another baseline system for result diversification 
recent effort in test collection building have focused on scaling back the number of necessary relevance judgment and then scaling up the number of search topic since the largest source of variation in a cranfield style experiment come from the topic this is a reasonable approach however a topic set size grow and researcher look to crowdsourcing and amazon s mechanical turk to collect relevance judgment we are faced with issue of quality control this paper examines the robustness of the trec million query track method when some assessor make significant and systematic error we find that while average are robust assessor error can have a large effect on system ranking 
most current banner advertising is sold through negotiation thereby incurring large transaction cost and possibly suboptimal allocation we propose a new automated system for selling banner advertising in this system each advertiser specifies a collection of host webpage which are relevant to his product a desired total quantity of impression on these page and a maximum per impression price the system selects a subset of advertiser a winner and map each winner to a set of impression on page within his desired collection the distinguishing feature of our system a opposed to current combinatorial allocation mechanism is that mimicking the current negotiation system we guarantee that winner receive at least a many advertising opportunity a they requested or else receive ample compensation in the form of a monetary payment by the host such guarantee are essential in market like banner advertising where a major goal of the advertising campaign is developing brand recognition a we show the problem of selecting a feasible subset of advertiser with maximum total value is inapproximable we thus present two greedy heuristic and discus theoretical technique to measure their performance our first algorithm iteratively selects advertiser and corresponding set of impression which contribute maximum marginal per impression profit to the current solution we prove a bi criterion approximation for this algorithm showing that it generates approximately a much value a the optimum algorithm on a slightly harder problem however this algorithm might perform poorly on instance in which the value of the optimum solution is quite large a clearly undesirable failure mode hence we present an adaptive greedy algorithm which again iteratively selects advertiser with maximum marginal per impression profit but additionally reassigns impression at each iteration for this algorithm we prove a structural approximation result a newly defined framework for evaluating heuristic we thereby prove that this algorithm ha a better performance guarantee than the simple greedy algorithm 
personalization of web search result a a technique for improving user satisfaction ha received notable attention in the research community over the past decade much of this work focus on modeling and establishing a profile for each user to aid in personalization our work take a more query centric approach in this paper we present a method for efficient automatic identification of a class of query we define a localizable from a web search engine query log we determine a set of relevant feature and use conventional machine learning technique to classify query our experiment find that our technique is able to identify localizable query with accuracy 
this paper present a hierarchical topic model that simultaneously capture topic and author s interest our proposal the author interest topic model ait introduces a latent variable with a separate probability distribution over topic into each document experiment on a research paper corpus show that the ait is useful a a generative model 
the widespread deployment of recommender system ha lead to user feedback of varying quality while some user faithfully express their true opinion many provide noisy rating which can be detrimental to the quality of the generated recommendation the presence of noise can violate modeling assumption and may thus lead to instability in estimation and prediction even worse malicious user can deliberately insert attack profile in an attempt to bias the recommender system to their benefit while previous research ha attempted to study the robustness of various existing collaborative filtering cf approach this remains an unsolved problem approach such a neighbor selection algorithm association rule and robust matrix factorization have produced unsatisfactory result this work describes a new collaborative algorithm based on svd which is accurate a well a highly stable to shilling this algorithm exploit previously established svd based shilling detection algorithm and combine it with svd based cf experimental result show a much diminished effect of all kind of shilling attack this work also offer significant improvement over previous robust collaborative filtering framework 
modeling the response time of search engine is an important task for many application such a resource selection in federated text search limited research ha been conducted to address this task prior research calculated the search response time of all query in the same way either with the average response time of several sample query or with a single probability distribution which is irrelevant to the characteristic of query however the search response time may vary a lot for different type of query this paper proposes a novel query specific and source specific approach to model search response time some training data is acquired by measuring the search response time of some sample query from a search engine then a query specific model is estimated with the training data and their corresponding response time by utilizing ridge regression the obtained model can be used to predict search response time for new query a set of empirical study are conducted to show the effectiveness of the proposed method 
this paper proposes and evaluates a network aware forward caching approach for determining the optimal deployment strategy of forward cache to a network a key advantage of this approach is that we can reduce the network cost associated with forward caching to maximize the benefit obtained from their deployment we show in our simulation that a increase to net benefit could be achieved over the standard method of full cache deployment to cache all pop traffic in addition we show that this maximal point occurs when only of the total traffic is cached another contribution of this paper is the analysis we use to motivate and evaluate this problem we characterize the internet traffic of k subscriber of a u residential broadband provider we use both layer and layer analysis to investigate the traffic volume of the flow a well a study the general characteristic of the application used we show that http is a dominant protocol and account for of the total downstream traffic and that of that traffic is multimedia in addition we show that multimedia content using http exhibit a annualized growth rate and other http traffic ha a growth rate versus the over all annual growth rate of broadband traffic this show that http traffic will become ever more dominent and increase the potential caching opportunity furthermore we characterize the core backbone traffic of this broadband provider to measure the distance travelled by content and traffic we find that cdn traffic is much more efficient than p p content and that there is large skew in the air mile between pop in a typical network our finding show that there are many opportunties in broadband provider network to optimize how traffic is delivered and cached 
in a higher level task such a clustering of web result or word sense disambiguation knowledge of all possible distinct concept in which an ambiguous word can be expressed would be advantageous for instance in determining the number of cluster in case of clustering web search result we propose an algorithm to generate such a ranked list of distinct concept associated with an ambiguous word concept which are popular in term of usage are ranked higher we evaluate the coverage of the concept inferred from our algorithm on the result retrieved by querying the ambiguous word using a major search engine and show a coverage of for top document averaged over all keywords 
the research on image advertising is still in it infancy most previous approach suggest ad by directly matching an ad to a query image which lack the power to identify ad from adjacent market in this paper we tackle the problem by mining knowledge on adjacent market from ad video with a novel multi modal dirichlet process mixture set model which is a unified model of video frame clustering and ad ranking our approach is not only capable of discovering relevant ad e g car ad for a query car image but also suggesting ad from adjacent market e g tyre ad experimental result show that our proposed approach is fairly effective 
ontology population is prone to cause inconsistency because the populating process is imprecise or the populated data may conflict with the original data by assuming that the intensional part of the populated dl based ontology is fixed and each removable abox assertion is given a removal cost we repair the ontology by deleting a subset of removable abox assertion in which the sum of removal cost is minimum we call such subset a minimum cost diagnosis we show that unless p np the problem of finding a minimum cost diagnosis for a dl lite ontology is insolvable in ptime w r t data complexity in spite of that we present a feasible computational method for more general i e shiq ontology it transforms a shiq ontology to a set of disjoint propositional program thus reducing the original problem into a set of independent subproblems each such subproblem computes an optimal model and is solvable in logarithmic call to a sat solver experimental result show that the method can handle moderately complex ontology with over thousand of abox assertion where all abox assertion can be assumed removable 
web document are typically associated with many text stream including the body the title and the url that are determined by the author and the anchor text or search query used by others to refer to the document through a systematic large scale analysis on their cross entropy we show that these text stream appear to be composed in different language style and hence warrant respective language model to properly describe their property we propose a language modeling approach to web document retrieval in which each document is characterized by a mixture model with component corresponding to the various text stream associated with the document immediate issue for such a mixture model arise a all the text stream are not always present for the document and they do not share the same lexicon making it challenging to properly combine the statistic from the mixture component to address these issue we introduce an open vocabulary smoothing technique so that all the component language model have the same cardinality and their score can simply be linearly combined to ensure that the approach can cope with web scale application the model training algorithm is designed to require no labeled data and can be fully automated with few heuristic and no empirical parameter tuning the evaluation on web document ranking task show that the component language model indeed have varying degree of capability a predicted by the cross entropy analysis and the combined mixture model outperforms the state of the art bm f based system 
text reuse occurs in many different type of document and for many different reason one form of reuse duplicate or near duplicate document ha been a focus of researcher because of it importance in web search local text reuse occurs when sentence fact or passage rather than whole document are reused and modified detecting this type of reuse can be the basis of new tool for text analysis in this paper we introduce a new approach to detecting local text reuse and compare it to other approach this comparison involves a study of the amount and type of reuse that occurs in real document including trec newswire and blog collection 
combining evidence from multiple retrieval model ha been widely studied in the context of of distributed search metasearch and rank fusion much of the prior work ha focused on combining retrieval score or the ranking assigned by different retrieval model or ranking algorithm in this work we focus on the problem of choosing between retrieval model using performance estimation we propose modeling the difference in retrieval performance directly by using rank time feature feature that are available to the ranking algorithm and the retrieval score assigned by the ranking algorithm our experimental result show that when choosing between two ranker our approach yield significant improvement over the best individual ranker 
it ha been shown that learning to rank approach are capable of learning highly effective ranking function however these approach have mostly ignored the important issue of efficiency given that both efficiency and effectiveness are important for real search engine model that are optimized for effectiveness may not meet the strict efficiency requirement necessary to deploy in a production environment in this work we present a unified framework for jointly optimizing effectiveness and efficiency we propose new metric that capture the tradeoff between these two competing force and devise a strategy for automatically learning model that directly optimize the tradeoff metric experiment indicate that model learned in this way provide a good balance between retrieval effectiveness and efficiency with specific loss function learned model converge to familiar existing one which demonstrates the generality of our framework finally we show that our approach naturally lead to a reduction in the variance of query execution time which is important for query load balancing and user satisfaction 
recent work on language model for information retrieval ha shown that smoothing language model is crucial for achieving good retrieval performance many different effective smoothing method have been proposed which mostly implement various heuristic to exploit corpus structure in this paper we propose a general and unified optimization framework for smoothing language model on graph structure this framework not only provides a unified formulation of the existing smoothing heuristic but also serf a a road map for systematically exploring smoothing method for language model we follow this road map and derive several different instantiation of the framework some of the instantiation lead to novel smoothing method empirical result show that all such instantiation are effective with some outperforming the state of the art smoothing method 
we present the result of a community detection analysis of the wikipedia graph distinct community in wikipedia contain semantically closely related article the central topic of a community can be identified using pagerank extracted community can be organized hierarchically similar to manually created wikipedia category structure 
a the exponential growth of information generated on the world wide web social recommendation ha emerged a one of the hot research topic recently social recommendation form a specific type of information filtering technique that attempt to suggest information blog news music travel plan web page image tag etc that are likely to interest the user social recommendation involves the investigation of collective intelligence by using computational technique such a machine learning data mining natural language processing etc on social behavior data collected from blog wikis recommender system question answer community query log tag etc from area such a social network social search social medium social bookmark social news social knowledge sharing and social game in this tutorial we will introduce social recommendation and elaborate on how the various characteristic and aspect are involved in the social platform for collective intelligence moreover we will discus the challenging issue involved in social recommendation in the context of theory and model of social network method to improve recommender system using social contextual information way to deal with partial and incomplete information in the social context scalability and algorithmic issue with social computational technique 
the effectiveness of community driven annotation such a social bookmarking depends on user participation since the participation of many user is motivated by selfish reason an effective way to encourage participation is to create useful or entertaining application we demo two such tool a browser extension and a game 
spearman s footrule and kendall s tau are two well established distance between ranking they however fail to take into account concept crucial to evaluating a result set in information retrieval element relevance and positional information that is changing the rank of a highly relevant document should result in a higher penalty than changing the rank of an irrelevant document a similar logic hold for the top versus the bottom of the result ordering in this work we extend both of these metric to those with position and element weight and show that a variant of the diaconis graham inequality still hold the generalized two measure remain within a constant factor of each other for all permutation we continue by extending the element weight into a distance metric between element for example in search evaluation swapping the order of two nearly duplicate result should result in little penalty even if these two are highly relevant and appear at the top of the list we extend the distance measure to this more general case and show that they remain within a constant factor of each other we conclude by conducting simple experiment on web search data with the proposed measure our experiment show that the weighted generalization are more robust and consistent with each other than their unweighted counter part 
most of the faster community extraction algorithm are based on the clauset newman and moore cnm which is employed for network with size up to node the modification proposed by danon diaz and arena dda obtains better modularity among cnm and it variation but there is no improvement in speed a it author expressed in this paper we identify some inefficiency in the data structure employed by former algorithm we propose a new framework for the algorithm and a modification of the dda to make it applicable to large scale network for instance the community extraction of a network with million node and million edge wa performed in about minute in contrast to former cnm that required hour time the former cnm obtaining better modularity the scalability of our improvement is shown by applying it to network with size up to million node obtaining the best modularity and execution time compared to the former algorithm 
it is now widely recognized that user interaction with search result can provide substantial relevance information on the document displayed in the search result in this paper we focus on extracting relevance information from one source of user interaction i e user click data which record the sequence of document being clicked and not clicked in the result set during a user search session we formulate the problem a a global ranking problem emphasizing the importance of the sequential nature of user click with the goal to predict the relevance label of all the document in a search session this is distinct from conventional learning to rank method that usually design a ranking model defined on a single document in contrast in our model the relational information among the document a manifested by an aggregation of user click is exploited to rank all the document jointly in particular we adapt several sequential supervised learning algorithm including the conditional random field crf the sliding window method and the recurrent sliding window method to the global ranking problem experiment on the click data collected from a commercial search engine demonstrate that our method can outperform the baseline model for search result re ranking 
typical pseudo relevance feedback method assume the top retrieved document are relevant and use these pseudo relevant document to expand term the initial retrieval set can however contain a great deal of noise in this paper we present a cluster based resampling method to select better pseudo relevant document based on the relevance model the main idea is to use document cluster to find dominant document for the initial retrieval set and to repeatedly feed the document to emphasize the core topic of a query experimental result on large scale web trec collection show significant improvement over the relevance model for justification of the resampling approach we examine relevance density of feedback document a higher relevance density will result in greater retrieval accuracy ultimately approaching true relevance feedback the resampling approach show higher relevance density than the baseline relevance model on all collection resulting in better retrieval accuracy in pseudo relevance feedback this result indicates that the proposed method is effective for pseudo relevance feedback 
search auction have become a dominant source of revenue generation on the internet such auction have typically used per click bidding and pricing we propose the use of hybrid auction where an advertiser can make a per impression a well a a per click bid and the auctioneer then chooses one of the two a the pricing mechanism we assume that the advertiser and the auctioneer both have separate belief called prior on the click probability of an advertisement we first prove that the hybrid auction is truthful assuming that the advertiser are risk neutral we then show that this auction is superior to the existing per click auction in multiple way we show that risk seeking advertiser will choose only a per impression bid whereas risk averse advertiser will choose only a per click bid and argue that both kind of advertiser arise naturally hence the ability to bid in a hybrid fashion is important to account for the risk characteristic of the advertiser 
when a web user s underlying information need is not clearly specified from the initial query an effective approach is to diversify the result retrieved for this query in this paper we introduce a novel probabilistic framework for web search result diversification which explicitly account for the various aspect associated to an underspecified query in particular we diversify a document ranking by estimating how well a given document satisfies each uncovered aspect and the extent to which different aspect are satisfied by the ranking a a whole we thoroughly evaluate our framework in the context of the diversity task of the trec web track moreover we exploit query reformulations provided by three major web search engine w a a mean to uncover different query aspect the result attest the effectiveness of our framework when compared to state of the art diversification approach in the literature additionally by simulating an upper bound query reformulation mechanism from official trec data we draw useful insight regarding the effectiveness of the query reformulations generated by the different w in promoting diversity 
web based virtual research environment vres have been proposed a one way in which e science tool can be deployed to support and enhance the research process we are exploring the use of linked data in combination with the open provenance model opm and social web concept to facilitate interaction between people and data in the context of a vre in this demo we present the ourspaces vre and outline the technology used to link together provenance research artefact project geographical location and social data in the context of interdisciplinary research 
to realize service that provide serendipity this paper ass the surprise of each user when presented recommendation we propose a recommendation algorithm that focus on the search time that in the absence of any recommendation each user would need to find a desirable and novel item by himself following the hypothesis that the degree of user s surprise is proportional to the estimated search time we consider both innovator preference and trend for identifying item with long estimated search time to predict which item the target user is likely to purchase in the near future the candidate item this algorithm weight each item that innovator have purchased and that reflect one or more current trend it then list them in order of decreasing weight experiment demonstrate that this algorithm output recommendation that offer high user item coverage a low gini coefficient and long estimated search time and so offer a high degree of recommendation serendipitousness 
the world wide web consortium s rdf standard primarily consists of subject property object triple that specify the value that a given subject ha for a given property however it is frequently the case that even for a fixed subject and property the value varies with time a a consequence effort have been made to annotate rdf triple with valid time interval however to date no proposal exist for efficient indexing of such temporal rdf database it is clearly beneficial to store rdf data in a relational db however standard relational index are inadequately equipped to handle rdf s graph structure in this paper we propose the tgrin index structure that build a specialized index for temporal rdf that is physically stored in an rdbms past effort to store rdf in relational store include jena from hp sesame from openrdf org and store from the university of southampton we show that even when these effort are augmented with well known temporal index like r tree sr tree st index and map the tgrin index exhibit superior performance in term of index build time tgrin take two third or le of the time used by any other system and it us a comparable amount of memory and le disk space than jena sesame and store more importantly tgrin can answer query three to six time faster for average query graph pattern and five to ten time faster for complex query than these system 
the aim of the forum for information retrieval evaluation fire is to create a cranfield like evaluation framework in the spirit of trec clef and ntcir for indian language information retrieval for the first year six indian language have been selected bengali hindi marathi punjabi tamil and telugu this poster describes the task a well a the document and topic collection that are to be used at the fire workshop 
abstract search personalization ha been pursued in many way in order to provide better result ranking and better overall search experience to individual user however blindly applying personalization to all user query for example by a background model derived from the user s long term query and click history is not always appropriate for aiding the user in accomplishing her actual task user interest change over time a user sometimes work on very dierent category of task within a short timespan and historybased personalization may impede a user s desire of discovering new topic in this paper we propose a personalization framework that is selective in a twofold sense first it selectively employ personalization technique for query that are expected to benet from prior history information while refraining from undue action otherwise second we introduce the notion of task representing dierent granularity level of a user prole ranging from very specic search goal to broad topic and base our reasoning selectively on query relevant user task these consideration 
social annotation on a web document are highly generalized description of topic contained in that page their tagged frequency indicates the user attention with various degree this make annotation a good resource for summarizing multiple topic in a web page in this paper we present a tag oriented web document summarization approach by using both document content and the tag annotated on that document to improve summarization performance a new tag ranking algorithm named eigentag is proposed in this paper to reduce noise in tag meanwhile association mining technique is employed to expand tag set to tackle the sparsity problem experimental result show our tag oriented summarization ha a significant improvement over those not using tag 
although it is common practice to produce only a single clustering of a dataset in many case text document can be clustered along different dimension unfortunately not only do traditional text clustering algorithm fail to produce multiple clustering of a dataset the only clustering they produce may not be the one that the user desire in this paper we propose a simple active clustering algorithm that is capable of producing multiple clustering of the same data according to user interest in comparison to previous work on feedback oriented clustering the amount of user feedback required by our algorithm is minimal in fact the feedback turn out to be a simple a a cursory look at a list of word experimental result are very promising our system is able to generate clustering along the user specified dimension with reasonable accuracy on several challenging text classification task thus providing suggestive evidence that our approach is viable 
the cost a well a the power and reliability of a retrieval test collection are all proportional to the number of topic included in it test collection created through community evaluation such a trec generally use topic prior work estimated the reliability of topic set by extrapolating confidence level from those of smaller set and concluded that topic are sufficient to have high confidence in a comparison especially when the comparison is statistically significant using topic set that actually contain topic this paper show that statistically significant difference can be wrong even when statistical significance is accompanied by moderately large relative difference in score further using standardized evaluation score rather than raw evaluation score doe not increase the reliability of these paired comparison researcher should continue to be skeptical of conclusion demonstrated on only a single test collection 
in this paper we report on a large scale study of structural difference among the national web the study is based on a webscale crawl conducted in the summer more specifically we study two graph derived from this crawl the nation graph with node corresponding to nation and edge to link among nation and the host graph with node corresponding to host and edge to hyperlink among page on the host contrary to some of the previous work our result show that web of different nation are often very different from each other both in term of their internal structure and in term of their connectivity with other nation 
using data collection available on the internet ha for many people became the main medium for staying informed about the world many of these collection are in nature dynamic evolving a the subject they describe change the goal of different research area is to identify and highlight these change to better enable reader to track story in this work we restrict ourselves to news collection and investigate real life effectiveness and usability of temporal text mining ttm story tracking method we propose a new story tracking method and build a tool to support it additionally we investigate the effectiveness and usability of story tracking method and define a new framework for automatic and user oriented evaluation we built method and tool which allow for understanding discovery and search through user interaction although there are many ttm method developed there is a lack of common evaluation procedure therefore we propose an evaluation framework for measuring how different ttm method discover novel fact apart from the automatic evaluation we are interested in how can user interact with patten and learn about the underlying subject of the story they track for this purpose we propose a user testing environment that measure speed and accuracy in which user can use story tracking method to discover predefined set of ground truth sentence 
service description allow designer to document understand and use service creating new useful and complex service with aggregated business value unlike rpc based service rest characteristic require a different approach to service description we present the resource linking language rell that introduces the concept of medium type resource type and link type a first class citizen for a service description a proof of concept a crawler called restler that crawl restful service based on rell description is also presented 
this paper study the compressibility of rdf data set we show that big rdf data set are highly compressible due to the structure of rdf graph power law organization of uris and rdf syntax verbosity we present basic approach to compress rdf data and test them with three well known real world rdf data set 
the most common way of framing the search problem is a an exchange between a user and a database where the user issue query and the database reply with result that satisfy constraint imposed by the query but that also optimize some notion of relevance there are several variation to this basic model that augment the dialogue between human and machine through query refinement relevance feedback and other mechanism however rarely is this problem ever posed in a way in which the property of the client and server are fundamentally different and in a way in which exploiting the difference can be used to yield substantially different experience i propose a reframing of the basic search problem which presupposes that server are scalable on most dimension but suffer from low communication latency while client have lower scalability but support vastly richer user interaction because of lower communication latency framed in this manner there is clear utility in refactoring the search problem so that user interaction are processed fluidly by a client while the server is relegated to pre computing the property of a result set that cannot be efficiently left to the client i will demonstrate pivot an experimental client application that allows the user to visually interact with thousand of search result at once while using facetted based exploration in a zoomable interface i will argue that the evolving structure of the web will tend to push all ir based application in a similar direction which ha the algorithmic intelligence increasingly split between client and server put another way my claim is that future client will be neither thin nor dumb 
online community have become popular for publishing and searching content and also for connecting to other user user generated content includes for example personal blog bookmark and digital photo item can be annotated and rated by different user and user can connect to others that are usually friend and or share common interest we demonstrate a social recommendation system that take advantage of user connection and tagging behavior to compute recommendation of item in such community the advantage can be verified via comparison to a standard ir technique 
community question answering ha emerged a a popular and effective paradigm for a wide range of information need for example to find out an obscure piece of trivia it is now possible and even very effective to post a question on a popular community qa site such a yahoo answer and to rely on other user to provide answer often within minute the importance of such community qa site is magnified a they create archive of million of question and hundred of million of answer many of which are invaluable for the information need of other searcher however to make this immense body of knowledge accessible effective answer retrieval is required in particular a any user can contribute an answer to a question the majority of the content reflects personal often unsubstantiated opinion a ranking that combine both relevance and quality is required to make such archive usable for factual information retrieval this task is challenging a the structure and the content of community qa archive differ significantly from the web setting to address this problem we present a general ranking framework for factual information retrieval from social medium result of a large scale evaluation demonstrate that our method is highly effective at retrieving well formed factual answer to question a evaluated on a standard factoid qa benchmark we also show that our learning framework can be tuned with the minimum of manual labeling finally we provide result analysis to gain deeper understanding of which feature are significant for social medium search and retrieval our system can be used a a crucial building block for combining result from a variety of social medium content with general web search result and to better integrate social medium content for effective information access 
abstract in order to create more attractive tagclouds that get people interested in tagged content we propose a simple but novel tagcloud where font size is determined by tag s entropy value not the popularity to it content our method raise user emotional interest in the content by emphasizing more emotional tag our initial experiment show that emotional tagclouds attract more attention than normal tagclouds at first look thus they will enhance the role of tagcloud a a social signaller category and subject descriptor 
blog post opinion retrieval aim at finding blog post that are relevant and opinionated about a user s query in this paper we propose a simple probabilistic model for assigning relevant opinion score to document the key problem is how to capture opinion expression in the document that are related to the query topic current solution enrich general opinion lexicon by finding query specific opinion lexicon using pseudo relevance feedback on external corpus or the collection itself in this paper we use a general opinion lexicon and propose using proximity information in order to capture opinion term relatedness to the query we propose a proximity based opinion propagation method to calculate the opinion density at each point in a document the opinion density at the position of a query term in the document can then be considered a the probability of opinion about the query term at that position the effect of different kernel for capturing the proximity is also discussed experimental result on the blog dataset show that the proposed method provides significant improvement over standard trec baseline and achieves a increase in map over the best performing run in the trec blog track 
we examine two basic source for implicit relevance feedback on the segment level for search personalization eye tracking and display time a controlled study ha been conducted where participant had to view document in front of an eye tracker query a search engine and give explicit relevance rating for the result we examined the performance of the basic implicit feedback method with respect to improved ranking and compared their performance to a pseudo relevance feedback baseline on the segment level and the original ranking of a web search engine our result show that feedback based on display time on the segment level is much coarser than feedback from eye tracking but surprisingly for re ranking and query expansion it did work a well a eye tracking based feedback all behavior based method performed significantly better than our non behavior based baseline and especially improved poor initial ranking of the web search engine the study show that segment level display time yield comparable result a eye tracking based feedback thus it should be considered in future personalization system a an inexpensive but precise method for implicit feedback 
faceted search is becoming a popular method to allow user to interactively search and navigate complex information space a faceted search system present user with keyvalue metadata that is used for query refinement while popular in e commerce and digital library not much research ha been conducted on which metadata to present to a user in order to improve the search experience nor are there repeatable benchmark for evaluating a faceted search engine this paper proposes the use of collaborative filtering and personalization to customize the search interface to each user s behavior this paper also proposes a utility based framework to evaluate the faceted interface in order to demonstrate these idea and better understand personalized faceted search several faceted search algorithm are proposed and evaluated using the novel evaluation methodology 
in recent year there ha been a dramatic proliferation of research on information retrieval based on highly subjective concept such a emotion preference and aesthetic such retrieval method are fascinating but challenging since it is difficult to built a general retrieval model that performs equally well to everyone in this paper we propose two novel method bag of user model and residual modeling to accommodate the individual difference for emotion based music retrieval the proposed method are intuitive and generally applicable to other information retrieval task that involve subjective perception evaluation result show the effectiveness of the proposed method 
the semantic web is based on accessing and reusing rdf data from many different source which one may assign different level of authority and credibility existing semantic web query language like sparql have targeted the retrieval combination and reuse of fact but have so far ignored all aspect of meta knowledge such a origin authorship recency or certainty of data to name but a few in this paper we present an original generic formalized and implemented approach for managing many dimension of meta knowledge like source authorship certainty and others the approach re us existing rdf modeling possibility in order to represent meta knowledge then it extends sparql query processing in such a way that given a sparql query for data one may request meta knowledge without modifying the original query thus our approach achieves highly flexible and automatically coordinated querying for data and meta knowledge while completely separating the two area of concern 
researcher increasingly use electronic communication data to construct and study large social network effectively inferring unobserved tie e g i is connected to j from observed communication event e g i email j often overlooked however is the impact of tie definition on the corresponding network and in turn the relevance of the inferred network to the research question of interest here we study the problem of network inference and relevance for two email data set of different size and origin in each case we generate a family of network parameterized by a threshold condition on the frequency of email exchanged between pair of individual after demonstrating that different choice of the threshold correspond to dramatically different network structure we then formulate the relevance of these network in term of a series of prediction task that depend on various network feature in general we find a that prediction accuracy is maximized over a non trivial range of threshold corresponding to reciprocated email per year b that for any prediction task choosing the optimal value of the threshold yield a sizable boost in accuracy over naive choice and c that the optimal threshold value appears to be somewhat surprisingly consistent across data set and prediction task we emphasize the practical utility in defining tie via their relevance to the prediction task s at hand and discus implication of our empirical result 
this paper provides an overview on the synergy between social web and knowledge managemen topic program committee member a well a summary of accepted paper for the swkm workshop 
recently there ha been a surge in research that predicts retrieval relevance using historical click through data while a larger number of click between a query and a document provides a stronger confidence of relevance most model in the literature that learn from click are error prone a they do not take into account any confidence estimate sponsored search model are especially prone to this error a they are typically trained on search engine log in order to predict click through rate ctr the estimated ctr ultimately determines the rank at which an ad is shown and also impact the price cost per click for the advertiser in this paper we improve a model that applies collaborative filtering on click data by training a filter that ha been trained to predict pure relevance applying the filter to ad that have seen few click on live traffic result in improved ctr and click yield cy additionally in offline experiment we find that using feature based on the emph organic result improves the relevance based filter s performance 
since the invention of the web the browser ha become more and more powerful by now it is a programming and execution environment in itself the predominant language to program application in the browser today is javascript with browser becoming more powerful javascript ha been extended and new layer have been added e g dom support and xpath each browser vendor ha implemented these extension dierently and ha added it own extension today javascript is already becoming a victim of it own success and is being used way beyond it design space furthermore application and gui feature implemented in the browser have become increasingly complex for these reason programming client side web application with javascript is intricate the purpose of this paper is to reduce programming complexity by proposing xquery a a client side browser embedded programming language the intuition is that programming the browser involves mostly xml i e dom navigation and manipulation and the xquery family of w c standard were designed exactly for that purpose the paper proposes extension to xquery for the web browser and give a number of example that demonstrate the usefulness of xquery for the development of ajax style application furthermore the paper present the design of the implementation of an extension to the internet explorer and report on experience with the implementation of this extension finally the paper compare xquery with javascript and other approach e g flash gwt in order to develop complex browser embedded application 
reading and commenting online news is becoming a common user behavior in social medium discussion in the form of comment following news posting can be effectively facilitated if the service provider can recommend article based on not only the original news itself but also the thread of changing comment this turn the traditional news recommendation to a discussion moderator that can intelligently assist online forum in this work we present a framework to recommend relevant information in the forum based social medium using user comment when incorporating user comment we consider structural and semantic information carried by them experiment indicate that our proposed solution provide an effective recommendation service 
pagerank computes the importance of each node in a directed graph under a random surfer model governed by a teleportation parameter commonly denoted alpha this parameter model the probability of following an edge inside the graph or when the graph come from a network of web page and link clicking a link on a web page we empirically measure the teleportation parameter based on browser toolbar log and a click trail analysis for a particular user or machine such analysis produce a value of alpha we find that these value nicely fit a beta distribution with mean edge following probability between and depending on the site using these distribution we compute pagerank score where pagerank is computed with respect to a distribution a the teleportation parameter rather than a constant teleportation parameter these new metric are evaluated on the graph of page in wikipedia 
community question answering cqa ha emerged a a popular forum for user to pose question for other user to answer over the last few year cqa portal such a naver and yahoo answer have exploded in popularity and now provide a viable alternative to general purpose web search at the same time the answer to past question submitted in cqa site comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering unfortunately the quality of the submitted question and answer varies widely increasingly so that a large fraction of the content is not usable for answering query previous approach for retrieving relevant and high quality content have been proposed but they require large amount of manually labeled data which limit the applicability of the supervised approach to new site and domain in this paper we address this problem by developing a semi supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation that requires relatively few labeled example to initialize the training process result of a large scale evaluation demonstrate that our method are more effective than previous approach for finding high quality answer question and user more importantly our quality estimation significantly improves the accuracy of search over cqa archive over the state of the art method 
previous work on term dependency ha not taken into account semantic information underlying query phrase in this work we study the impact of utilizing phrase based concept for term dependency we use wikipedia to separate important and le important term dependency and treat them accordingly a feature in a linear feature based retrieval model we compare our method with a markov random field mrf model on four trec document collection our experimental result show that utilizing phrase based concept improves the retrieval effectiveness of term dependency and reduces the size of the feature set to large extent 
application increasingly make use of the distributed platform that the world wide web provides be it a a software a a service such a salesforce com an application infrastructure such a facebook com or a computing infrastructure such a a cloud a common characteristic of application of this kind is that they are deployed on infrastructure or make use of component that reside in different management domain current service management approach and system however often rely on a centrally managed configuration management database cmdb which is the basis for centrally orchestrated service management process in particular change management and incident management the distribution of management responsibility of www based application requires a decentralized approach to service management this paper proposes an approach of decentralized service management based on distributed configuration management and service process co ordination making use restful access to configuration information and atom based distribution of update a a novel foundation for service management process 
we leverage the ubiquity of bluetooth enabled device and propose a decentralized web based architecture that allows user to share their location by following each other in the style of twitter we demonstrate a prototype that operates in a large building which generates a dataset of detected bluetooth device at a rate of new device per day including the respective location where they were last detected user then query the dataset using their unique bluetooth id and share their current location with their follower by mean of unique uris that they control our separation between producer the building and consumer the user of bluetooth device location data allows u to create socially aware application that respect user s privacy while limiting the software necessary to run on mobile device to just a web browser 
semantic web ontology language such a owl have been widely used for knowledge representation through empirical analysis of real world ontology we discover that like many natural and social phenomenon the semantic web ontology is also scale free 
the open government directive is making u government data available via website such a data gov for public access in this paper we present a semantic web based approach that incrementally generates linked government data lgd for the u government in focusing on the trade off between high quality lgd generation requiring non trivial human expert input and massive lgd generation requiring low human processing cost our work is highlighted by the following feature i supporting low cost and extensible lgd publishing for massive government data ii using social semantic web web technology to incrementally enhance published lgd via crowdsourcing and iii facilitating mash ups by declaratively reusing cross dataset mapping which usually are hard coded in application 
pseudo relevance feedback prf which ha been widely applied in ir aim to derive a distribution from the top n pseudo relevant document d however these document are often a mixture of relevant and irrelevant document a a result the derived distribution is actually a mixture model which ha long been limiting the performance of prf this is particularly the case when we deal with difficult query where the truly relevant document in d are very sparse in this situation it is often easier to identify a small number of seed irrelevant document which can form a seed irrelevant distribution then a fundamental and challenging problem arises solely based on the mixed distribution and a seed irrelevance distribution how to automatically generate an optimal approximation of the true relevance distribution in this paper we propose a novel distribution separation model dsm to tackle this problem theoretical justification of the proposed algorithm are given evaluation result from our extensive simulated experiment on several large scale trec data set demonstrate the effectiveness of our method which outperforms a well respected prf model the relevance model rm a well a the use of rm on d with the seed negative document directly removed 
with the explosion of user generated web content in the form of blog wikis and discussion forum the internet ha rapidly become a massive dynamic repository of public opinion on an unbounded range of topic a key enabler of opinion extraction and summarization is sentiment classification the task of automatically identifying whether a given piece of text express positive or negative opinion towards a topic of interest building high quality sentiment classifier using standard text categorization method is challenging due to the lack of labeled data in a target domain in this paper we consider the problem of cross domain sentiment analysis can one for instance download rated movie review from rottentomatoes com or imbd discussion forum learn linguistic expression and sentiment laden term that generally characterize opinionated review and then successfully transfer this knowledge to the target domain thereby building high quality sentiment model without manual effort we outline a novel sentiment transfer mechanism based on constrained non negative matrix tri factorization of term document matrix in the source and target domain we report some preliminary result with this approach 
with the ever increasing growth of the internet numerous copy of document become serious problem for search engine opinion mining and many other web application since partial duplicate only contain a small piece of text taken from other source and most existing near duplicate detection approach focus on document level partial duplicate can not be dealt with well in this paper we propose a novel algorithm to realize the partial duplicate detection task besides the similarity between document our proposed algorithm can simultaneously locate the duplicated part the main idea is to divide the partial duplicate detection task into two subtasks sentence level near duplicate detection and sequence matching for evaluation we compare the proposed method with other approach on both english and chinese web collection experimental result appear to support that our proposed method is effectively and efficiently to detect both partial duplicate on large web collection 
user generated spoken audio remains a challenge for automatic speech recognition asr technology and content based audio surrogate derived from asr transcript must be error robust an investigation of the use of term cloud a surrogate for podcasts demonstrates that asr term cloud closely approximate term cloud derived from human generated transcript across a range of cloud size a user study confirms the conclusion that asr cloud are viable surrogate for depicting the content of podcasts 
analyzing three way data ha attracted a lot of attention recently due to the intrinsic rich structure in real world datasets the paratucker model ha been proposed to combine the axis capability of the parafac model and the structural generality of the tucker model however no algorithm have been developed for fitting the paratucker model in this paper we propose tanpt algorithm to solve the paratucker model we apply the algorithm for temporal relation co clustering on author topic evolution experiment on dblp datasets demonstrate it effectiveness 
recently a number of study have demonstrated that search engine logfiles are an important resource to determine the relevance relation between url and query term we hypothesized that the query associated with a url could also be presented a useful url metadata in a search engine result list e g for helping to determine the semantic category of a url we evaluated this hypothesis by a classification experiment based on the dmoz dataset our method can also annotate url that have no associated query 
various measure such a binary preference bpref inferred average precision infap and binary normalised discounted cumulative gain ndcg have been proposed a alternative to mean average precision map for being le sensitive to the relevance judgement completeness a the primary aim of any system building is to train the system to respond to user query in a more robust and stable manner in this paper we investigate the importance of the choice of the evaluation measure for training under different level of evaluation incompleteness we simulate evaluation incompleteness by sampling from the relevance assessment through large scale experiment on two standard trec test collection we examine retrieval sensitivity when training i e if a training process based on any of the four discussed measure ha an impact on the final retrieval performance experimental result show that training by bpref infap and ndcg provides significantly better retrieval performance than training by map when relevance judgement completeness is extremely low when relevance judgement completeness increase the measure behave more similarly 
we consider the problem of large scale retrieval evaluation recently two method based on random sampling were proposed a a solution to the extensive effort required to judge ten of thousand of document while the first method proposed by aslam et al is quite accurate and efficient it is overly complex making it difficult to be used by the community and while the second method proposed by yilmaz et al infap is relatively simple it is le efficient than the former since it employ uniform random sampling from the set of complete judgment further none of these method provide confidence interval on the estimated value the contribution of this paper is threefold we derive confidence interval for infap we extend infap to incorporate nonrandom relevance judgment by employing stratified random sampling hence combining the efficiency of stratification with the simplicity of random sampling we describe how this approach can be utilized to estimate ndcg from incomplete judgment we validate the proposed method using trec data and demonstrate that these new method can be used to incorporate nonrandom sample a were available in trec terabyte track 
social medium website promote diverse user interaction on medium object a well a user action with respect to other user the goal of this work is to discover community structure in rich medium social network and observe how it evolves over time through analysis of multi relational data the problem is important in the enterprise domain where extracting emergent community structure on enterprise social medium can help in forming new collaborative team aid in expertise discovery and guide long term enterprise reorganization our approach consists of three main part a relational hypergraph model for modeling various social context and interaction a novel hypergraph factorization method for community extraction on multi relational social data an on line method to handle temporal evolution through incremental hypergraph factorization extensive experiment on real world enterprise data suggest that our technique is scalable and can extract meaningful community to evaluate the quality of our mining result we use our method to predict user future interest our prediction outperforms baseline method frequency count plsa by on the average indicating the utility of leveraging multi relational social context by using our method 
this paper present a new query recommendation method that generates recommended query list by mining large scale user log starting from the user log of click through data we construct a bipartite network where the node on one side correspond to unique query on the other side to unique url inspired by the bipartite network based resource allocation method we try to extract the hidden information from the query url bipartite network the recommended query generated by the method are asymmetrical which mean two related query may have different strength to recommend each other to evaluate the method we use one week user log from chinese search engine sogou the method is not only content ignorant but also can be easily implemented in a paralleled manner which is feasible for commercial search engine to handle large scale user log 
fully automatic method that extract list of object from the web have been studied extensively record extraction the first step of this object extraction process identifies a set of web page segment each of which represents an individual object e g a product state of the art method suffice for simple search but they often fail to handle more complicated or noisy web page structure due to a key limitation their greedy manner of identifying a list of record through pairwise comparison i e similarity match of consecutive segment this paper introduces a new method for record extraction that capture a list of object in a more robust way based on a holistic analysis of a web page the method focus on how a distinct tag path appears repeatedly in the dom tree of the web document instead of comparing a pair of individual segment it compare a pair of tag path occurrence pattern called visual signal to estimate how likely these two tag path represent the same list of object the paper introduces a similarity measure that capture how closely the visual signal appear and interleave clustering of tag path is then performed based on this similarity measure and set of tag path that form the structure of data record are extracted experiment show that this method achieves higher accuracy than previous method 
this paper proposes a general framework of for a system with a semantic browsing and visualization interface called knowledge communication collaboration and creation browser kc browser integrates multimedia contest and web service on the grid network and make a semantic mash up called knowledge workspace k workspace with various visual gadget according to user s context e g their interest purpose and computational environment kc browser also achieves a link free browsing for seamless knowledge access by generating semantic link based on an arbitrary knowledge model such a ontology and vector space model it assist user to look down and to figure out various social and natural event from the web content we have implemented a prototype of kc browser and tested it to an international project on risk intelligence against natural disaster 
traditional document retrieval ha shown to be a competitive approach in xml element retrieval which is counter intuitive since the element retrieval task request all and only relevant document part to be retrieved this paper conduct a comparative analysis of document and element retrieval highlight the relative strength and weakness of both approach and explains the relative effectiveness of document retrieval approach at element retrieval task 
several relevance metric such a ndcg precision and pskip are proposed to measure search relevance where different metric try to characterize search relevance from different perspective yet we empirically find that the direct optimization of one metric cannot always achieve the optimal ranking of another metric in this paper we propose two novel relevance optimization approach which take different metric into a global consideration where the objective is to achieve an ideal tradeoff between different metric to achieve this objective we propose to co optimize multiple relevance metric and show their effectiveness 
we describe a new approach to information retrieval algorithmic mediation for intentional synchronous collaborative exploratory search using our system two or more user with a common information need search together simultaneously the collaborative system provides tool user interface and most importantly algorithmically mediated retrieval to focus enhance and augment the team s search and communication activity collaborative search outperformed post hoc merging of similarly instrumented single user run algorithmic mediation improved both collaborative search allowing a team of searcher to find relevant information more efficiently and effectively and exploratory search allowing the searcher to find relevant information that cannot be found while working individually 
content based multimedia information retrieval cbmir system which leverage multiple retrieval expert en often employ a weighting scheme when combining expert result through data fusion typically however a query will comprise multiple query image im leading to potentially n m weight to be assigned because of the large number of potential weight existing approach impose a hierarchy for data fusion such a uniformly combining query image result from a single retrieval expert into a single list and then weighting the result of each expert in this paper we will demonstrate that this approach is sub optimal and lead to the poor state of cbmir performance in benchmarking evaluation we utilize an optimization method known a coordinate ascent to discover the optimal set of weight en im which demonstrates a dramatic difference between known result and the theoretical maximum we find that imposing common combinatorial hierarchy for data fusion will half the optimal performance that can be achieved by examining the optimal weight set at the topic level we observe that approximately of the weight from set en im for any given query are assigned of the total weight mass for that topic furthermore we discover that the ideal distribution of weight follows a log normal distribution we find that we can achieve up to of the performance of fully optimized query using just these of the weight our investigation wa conducted on trecvid evaluation to inclusive and imageclefphoto totalling search topic optimized over a combined collection size of image and topic image 
predicting the performance of web query is useful for several application such a automatic query reformulation and automatic spell correction in the web environment accurate performance prediction is challenging because measure such a clarity that work well on homogeneous trec like collection are not a effective and are often expensive to compute we present rank time performance prediction rapp an effective and efficient approach for online performance prediction on the web rapp us retrieval score and aggregate of the rank time feature used by the documentranking algorithm to train regressors for query performance prediction on a set of over query sampled from the query log of a major search engine rapp achieves a linear correlation of with dcg and with ndcg analysis of prediction accuracy show that hard query are easier to identify while easy query are harder to identify 
in retrieval experiment an effectiveness metric is used to generate a score for each system topic pair being tested it is then usual to average the system topic score to obtain a system score which is used for the purpose of system comparison in this paper we explore the ramification of using the geometric mean gmap rather than the arithmetic mean map when computing an aggregate system score from a set of system topic score we find that gmap doe indeed handle variability in topic difficulty more consistently than doe the usual map aggregation method 
web service may be unable to interact with each other because of incompatibility between their interface in this paper we present an event driven approach which aim at adapting message exchanged during service interaction the proposed framework relies on the complex event processing cep technology which provides an environment for the development of application that need to continuously process analyse and respond to event stream our main contribution is a system that enables developer to design and implement cep based adapter these latter are deployed in a cep engine which is responsible for continuously receiving message and processing them according to rule implemented by the adapter resulting transformed message are thus forwarded to their original service recipient 
transcript of meeting are a document genre characterized by a complex narrative structure the essence is not only what is said but also by who and to whom this paper investigates whether we can use semantic annotation like the speaker in order to capture this debate structure a well a the related content of the debate the structure is visualized in a graph while the content is condensed into word cloud that are created using a parsimonious language model evaluation show that both tool adequately capture the structure and content of the debate at an aggregated level 
automated text categorization is an important technique for many web application such a document indexing document filtering and cataloging web resource many different approach have been proposed for the automated text categorization problem among them centroid based approach have the advantage of short training time and testing time due to it computational efficiency a a result centroid based classifier have been widely used in many web application however the accuracy of centroid based classifier is inferior to svm mainly because centroid found during construction are far from perfect location we design a fast class feature centroid cfc classifier for multi class single label text categorization in cfc a centroid is built from two important class distribution inter class term index and inner class term index cfc proposes a novel combination of these index and employ a denormalized cosine measure to calculate the similarity score between a text vector and a centroid experiment on the reuters corpus and newsgroup email collection show that cfc consistently outperforms the state of the art svm classifier on both micro f and macro f score particularly cfc is more effective and robust than svm when data is sparse 
we combine technique of xml mining and text mining for the benefit of information retrieval by manipulating the word sequence according to the xml structure of the marked up text we strengthen phrase boundary so that they are more obvious to the algorithm that extract multiword sequence from text consequently the quality of the indexed phrase improves which ha a positive effect on the average precision measured by the inex standard 
the goal of system evaluation in information retrieval ha always been to determine which of a set of system is superior on a given collection the tool used to determine system ordering is an evaluation metric such a average precision which computes relative collection specific score we argue that a broader goal is achievable in this paper we demonstrate that by use of standardization score can be substantially independent of a particular collection allowing system to be compared even when they have been tested on different collection compared to current method our technique provide richer information about system performance improved clarity in outcome reporting and greater simplicity in reviewing result from disparate source 
inferring an appropriate dtd or xml schema definition xsd for a given collection of xml document essentially reduces to learning deterministic regular expression from set of positive example word unfortunately there is no algorithm capable of learning the complete class of deterministic regular expression from positive example only a we will show the regular expression occurring in practical dtds and xsds however are such that every alphabet symbol occurs only a small number of time a such in practice it suffices to learn the subclass of deterministic regular expression in which each alphabet symbol occurs at most k time for some small k we refer to such expression a k occurrence regular expression k ore for short motivated by this observation we provide a probabilistic algorithm that learns k ore for increasing value of k and selects the deterministic one that best describes the sample based on a minimum description length argument the effectiveness of the method is empirically validated both on real world and synthetic data furthermore the method is shown to be conservative over the simpler class of expression considered in previous work 
one of the central task of r d strategy and portfolio management at large technology company and research institution refers to the identification of technological synergy throughout the organization these effort are geared towards saving resource by consolidating scattered expertise sharing best practice and reusing available technology across multiple product line in the past this task ha been done in a manual evaluation process by technical domain expert while feasible the major drawback of this approach is the enormous effort in term of availability and time for a structured and complete analysis every combination of any two technology ha to be rated explicitly we present a novel approach that recommends technological synergy in an automated fashion making use of abundant collective wisdom from the web both in pure textual form a well a classification ontology our method ha been deployed for practical support of the synergy evaluation process within our company we have also conducted empirical evaluation based on randomly selected technology pair so a to benchmark the accuracy of our approach a compared to a group of general computer science technologist a well a a control group of domain expert 
the selection of indexing term for representing document is a key decision that limit how effective subsequent retrieval can be often stemming algorithm are used to normalize surface form and thereby address the problem of not finding document that contain word related to query term through infectional or derivational morphology however rule based stemmer are not available for every language and it is unclear which method for coping with morphology are most effective in this paper we investigate an assortment of technique for representing text and compare these approach using data set in eighteen language and five different writing system we find character n gram tokenization to be highly effective in half of the language examined n gram outperform unnormalized word by more than in highly infective language relative improvement over are obtained in language with le morphological richness the choice of tokenization is not a critical and rule based stemming can be an attractive option if available we also conducted an experiment to uncover the source of n gram power and a causal relationship between the morphological complexity of a language and n gram effectiveness wa demonstrated 
this paper describes a series of user study on how people use the web via mobile device the data primarily come from contextual inquiry with participant between and and is complemented with a phone log analysis of panelist in we report four key contextual factor in using the web on mobile device and propose mobile web activity taxonomy the framework contains three user activity category identical to previous stationary web study information seeking communication and transaction and a new category personal space extension the new category refers to the practice that people put their content on the web for personal access therefore extending their personal information space 
text retrieval query frequently contain named entity the standard approach of term frequency weighting doe not work well when estimating the term frequency of a named entity since anaphoric expression like he she the movie etc are frequently used to refer to named entity in a document and the use of anaphoric expression cause the term frequency of named entity to be underestimated in this paper we propose a novel poisson model to estimate the frequency of anaphoric expression of a named entity without explicitly resolving the anaphoric expression our key assumption is that the frequency of anaphoric expression is distributed over named entity in a document according to the probability of whether the document is elite for the named entity this assumption lead u to formulate our proposed co referentially enhanced entity frequency ceef experimental result on the text collection of trec blog track show that ceef achieves significant and consistent improvement over state of the art retrieval method using standard term frequency estimation in particular we achieve a increase of map over the best performing run of trec blog track 
while large scale taxonomy especially for web page have been in existence for some time approach to automatically classify document into these taxonomy have met with limited success compared to the more general progress made in text classification we argue that this stem from three cause increasing sparsity of training data at deeper node in the taxonomy error propagation where a mistake made high in the hierarchy cannot be recovered and increasingly complex decision surface in higher node in the hierarchy while prior research ha focused on the first problem we introduce method that target the latter two problem first by biasing the training distribution to reduce error propagation and second by propagating up first guess expert information in a bottom up manner before making a refined top down choice finally we present an empirical study demonstrating that the suggested change lead to improvement in f score versus an accepted competitive baseline hierarchical svms 
in this poster we develop an evolutionary document summarization system for discovering the change and difference in each phase of a disaster evolution given a collection of document stream describing an event our system generates a short summary delivering the main development theme of the event by extracting the most representative and discriminative sentence at each phase experimental result on the collection of press release for hurricane wilma in demonstrate the efficacy of our proposal 
we explore a set of hypothesis on user behavior that are potentially at the origin of the mean average precision ap metric this allows u to propose a more realistic version of ap where user click non deterministically on relevant document and where the number of relevant document in the collection need not be known in advance we then depart from the assumption that a document is either relevant or irrelevant and we use instead relevance judgment similar to editorial label used for discounted cumulated gain dcg we assume that clicked document provide user with a certain level of utility and that a user end a search when she gathered enough utility based on the query log of a commercial search engine we show how to evaluate the utility associated with a label from the record of past user interaction with the search engine and we show how the two different user model can be evaluated based on their ability to predict accurately future click finally based on these user model we propose a measure that capture the relative quality of two ranking 
in order to artificially boost the rank of commercial page in search engine result search engine optimizers pay for link to these page on other website identifying paid link is important for a web search engine to produce highly relevant result in this paper we introduce a novel method of identifying such link we start with training a classifier of anchor text topic and analyzing web page for diversity of their outgoing commercial link then we use this information and analyze link graph of the russian web to find page that sell link and site that buy link and to identify the paid link testing on manually marked sample showed high efficiency of the algorithm 
the html specification introduces the audio and video medium element and with them the opportunity to change the way medium is integrated on the web the current html medium api provides way to play and get limited information about audio and video but no way to programatically access or create such medium in this paper we present an enhanced api for these medium element a well a detail about a mozilla firefox implementation created by the author which allows web developer to read and write raw audio data 
one of the several initiative to bridge the digital divide in developing country ha been the deployment of information kiosk or knowledge center in village in rural part of the country these kiosk provide service ranging from email chat and browsing to distance education program agricultural service and egovernance service a kiosk typically comprises of a computer with printer web cam multimedia system and internet connectivity and is owned by a local entrepreneur moving away from the pc based kiosk model we present an alternative platform to create and host such information kiosk in the telephony network we call these a voikiosks and they are accessible through voice interaction over an ordinary phone call 
in this paper we briefly describe the implementation of various open geospatial consortium web service interface standard in oracle spatial g we highlight how we utilize oracle s implementation of oasis web service security w to provide a robust security framework for these ogc web service we also discus our future direction in supporting ogc web service interface standard 
with the prosperity of tourism and web technology more and more people have willingness to share their travel experience on the web e g weblogs forum or web community these so called travelogue contain rich information particularly including location representative knowledge such a attraction e g golden gate bridge style e g beach history and activity e g diving surfing the location representative information in travelogue can greatly facilitate other tourist trip planning if it can be correctly extracted and summarized however since most travelogue are unstructured and contain much noise it is difficult for common user to utilize such knowledge effectively in this paper to mine location representative knowledge from a large collection of travelogue we propose a probabilistic topic model named a location topic model this model ha the advantage of differentiability between two kind of topic i e local topic which characterize location and global topic which represent other common theme shared by various location and representation of location in the local topic space to encode both location representative knowledge and similarity between location some novel application are developed based on the proposed model including destination recommendation for on flexible query characteristic summarization for a given destination with representative tag and snippet and identification of informative part of a travelogue and enriching such highlight with related image based on a large collection of travelogue the proposed framework is evaluated using both objective and subjective evaluation method and show promising result 
browser do not currently support the secure sharing of javascript object between principal we present this problem a the need for object view which are consistent and controllable version of object multiple view can be made for the same object and customized for the recipient we implement object view with a javascript library that wrap shared object and interposes on all access attempt the security challenge is to fully mediate access to object shared through a view and prevent privilege escalation we discus how object view can be deployed in two setting same origin sharing with rewriting based javascript isolation system like google caja and inter origin sharing between browser frame over a message passing channel to facilitate simple document sharing we build a policy system for declaratively defining policy for document object view notably our document policy system make it possible to hide element without breaking document structure invariant developer can control the fine grained behavior of object view with an aspect system that accepts programmatic policy 
search and recommendation system must effectively model user interest in order to provide personalized result the proliferation of social software make social network an increasingly important source for user interest modeling because of the social influence and correlation among friend however there are large variation in people s contribution of social content therefore it is impractical to accurately model interest for all user a a result application need to decide whether to utilize a user interest model based on it accuracy to address this challenge we present a study on the accuracy of user interest inferred from three type of social content social bookmarking file sharing and electronic communication in an organizational social network within a large scale enterprise first we demonstrate that combining different type of social content to infer user interest outperforms method that use only one type of social content second we present a technique to predict the inference accuracy based on easily observed network characteristic including user activeness network in degree out degree and betweenness centrality 
social networking site have been increasingly gaining popularity well known site such a facebook have been reporting growth rate a high a per week many social networking site have million of registered user who use these site to share photograph contact long lost friend establish new business contact and to keep in touch in this paper we investigate how easy it would be for a potential attacker to launch automated crawling and identity theft attack against a number of popular social networking site in order to gain access to a large volume of personal user information the first attack we present is the automated identity theft of existing user profile and sending of friend request to the contact of the cloned victim the hope from the attacker s point of view is that the contacted user simply trust and accept the friend request by establishing a friendship relationship with the contact of a victim the attacker is able to access the sensitive personal information provided by them in the second more advanced attack we present we show that it is effective and feasible to launch an automated cross site profile cloning attack in this attack we are able to automatically create a forged profile in a network where the victim is not registered yet and contact the victim s friend who are registered on both network our experimental result with real user show that the automated attack we present are effective and feasible in practice 
behavioral targeting bt is a recent trend of online advertising market however some classical bt solution which predefine the user segment for bt ad delivery are sometimes too large to numerous long tail advertiser who cannot afford to buy any large user segment due to budget consideration in this extend abstract we propose to rank user according to their probability of interest in an advertisement in a learning to rank framework we propose to extract three type of feature between user behavior such a search query ad click history etc and the ad content provided by advertiser through this way a long tail advertiser can select a certain number of top ranked user a needed from the user segment for ad delivery in the experiment we use a day ad click through log from a commercial search engine the result show that using our proposed feature under a learning to rank framework we can well rank user who potentially interest in an advertisement 
a wealth of knowledge is encoded in the form of table on the world wide web we propose a classification algorithm and a rich feature set for automatically recognizing layout table and attribute value table we report the frequency of these table type over a large analysis of the web and propose open challenge for extracting from attribute value table semantic triple knowledge we then describe a solution to a key problem in extracting semantic triple protagonist detection i e finding the subject of the table that often is not present in the table itself in of our web table our method find the correct protagonist in it top three returned candidate 
we investigate a representative case of sudden information need change of web user by analyzing search engine query log we show that the majority of query submitted by user after browsing document in the news domain are related to the most recently browsed document we investigate way of identifying whether a query is a good candidate for contextualization conditioned on the most recently browsed document by a user we build a successful classifier for this task which achieves precision at recall 
we study the cost per action or cost per acquisition cpa charging scheme in online advertising in this scheme instead of paying per click the advertiser pay only when a user take a specific action e g fill out a form or completes a transaction on their website we focus on designing efficient and incentive compatible mechanism that use this charging scheme we describe a mechanism based on a sampling based learning algorithm that under suitable assumption is asymptotically individually rational asymptotically bayesian incentive compatible and asymptotically ex ante efficient in particular we demonstrate our mechanism for the case where the utility function of the advertiser are independent and identically distributed random variable a well a the case where they evolve like independent reflected brownian motion 
we study online social network in which relationship can be either positive indicating relation such a friendship or negative indicating relation such a opposition or antagonism such a mix of positive and negative link arise in a variety of online setting we study datasets from epinions slashdot and wikipedia we find that the sign of link in the underlying social network can be predicted with high accuracy using model that generalize across this diverse range of site these model provide insight into some of the fundamental principle that drive the formation of signed link in network shedding light on theory of balance and status from social psychology they also suggest social computing application by which the attitude of one user toward another can be estimated from evidence provided by their relationship with other member of the surrounding social network 
web search is increasingly exploiting named entity like person place business address and date entity ranking is also of current interest at inex and trec numerical quantity are an important class of entity especially in query about price and feature related to product service and travel we introduce quantity consensus query qcqs where each answer is a tight quantity interval distilled from evidence of relevance in thousand of snippet entity search and factoid question answering have benefited from aggregating evidence from multiple promising snippet but these do not readily apply to quantity here we propose two new algorithm that learn to aggregate information from multiple snippet we show that typical signal used in entity ranking like rarity of query word and their lexical proximity to candidate quantity are very noisy our algorithm learn to score and rankquantity interval directly combining snippet quantity and snippet text information we report on experiment using hundred of qcqs with ground truth taken from trec qa wikipedia infoboxes and other source leading to ten of thousand of candidate snippet and quantity our algorithm yield about better map and ndcg compared to the best known collective ranker and are better than scoring snippet independent of each other 
semantic search refers to a loose set of concept challenge and technique having to do with harnessing the information of the growing web of data wod for web search here we propose a formal model of one specific semantic search task ad hoc object retrieval we show that this task provides a solid framework to study some of the semantic search problem currently tackled by commercial web search engine we connect this task to the traditional ad hoc document retrieval and discus appropriate evaluation metric finally we carry out a realistic evaluation of this task in the context of a web search application 
we present a simple and effective approach to complement search result for child s web query with child oriented multimedia result such a coloring page and music sheet our approach determines appropriate medium type for a query by searching google s database of frequent query for co occurrence of a query s term e g dinosaur with preselected multimedia term e g coloring page we show the effectiveness of this approach through an online user evaluation 
test collection are extensively used in the evaluation of information retrieval system crucial to their use is the degree to which result from them predict user effectiveness at first past study did not substantiate a relationship between system and user effectiveness more recently however correlation have begun to emerge the result of this paper strengthen and extend those finding we introduce a novel methodology for investigating the relationship which show great success in establishing a significant correlation between system and user effectiveness it is shown that user behave differently and discern difference between pair of system that have a very small absolute difference in test collection effectiveness our result strengthen the use of test collection in ir evaluation confirming that user effectiveness can be predicted successfully 
dynamic resource provisioning aim at maintaining the end to end response time of a web application within a pre defined sla although the topic ha been well studied for monolithic application provisioning resource for application composed of multiple service remains a challenge when the sla is violated one must decide which service s should be reprovisioned for optimal effect we propose to assign an sla only to the front end service other service are not given any particular response time objective service are autonomously responsible for their own provisioning operation and collaboratively negotiate performance objective with each other to decide the provisioning service s we demonstrate through extensive experiment that our system can add remove shift both server and cache within an entire multi service application under varying workload to meet the sla target and improve resource utilization 
blog have been expanded at an incredible speed in recent year plentiful personal information make blog a popular way mining user profile in this paper we propose a novel blogger interest modeling approach based on forgetting mechanism a new forgetting function is introduced to track interest drift based on that the short term interest model stim and long term interest model ltim are constructed to describe blogger short term and long term interest the experiment show that both model can identify blogger preference well respectively 
we present a static control flow analysis for javascript program running in a web browser our analysis tackle numerous challenge posed by modern web application including asynchronous communication framework and dynamic code generation we use our analysis to extract a model of expected client behavior a seen from the server and build an intrusion prevention proxy for the server the proxy intercept client request and disables those that do not meet the expected behavior we insert random asynchronous request to foil mimicry attack finally we evaluate our technique against several real application and show that it protects against an attack in a widely used web application 
in this paper a benchmark called wpbench is reported to evaluate the responsiveness of web browser for modern web application in wpbench variation of server and network are removed and the benchmark result is the closest to what web user would perceive to achieve these wpbench record user interaction with typical web application and then replay web navigation when benchmarking browser the replay mechanism can emulate the actual user interaction and the characteristic of the server and the network in a consistent way independent of browser so that any browser compliant to the standard can be benchmarked fairly in addition to describing the design and generation of wpbench we also report the wpbench comparison result on the responsiveness performance for three popular web browser internet explorer firefox and chrome 
a an indispensable technique in the field of information filtering recommender system ha been well studied and developed both in academia and in industry recently however most of current recommender system suffer the following problem the large scale and sparse data of the user item matrix seriously affect the recommendation quality a a result most of the recommender system cannot easily deal with user who have made very few rating the traditional recommender system assume that all the user are independent and identically distributed this assumption ignores the connection among user which is not consistent with the real world recommendation aiming at modeling recommender system more accurately and realistically we propose a novel probabilistic factor analysis framework which naturally fuse the user taste and their trusted friend favor together in this framework we coin the term social trust ensemble to represent the formulation of the social trust restriction on the recommender system the complexity analysis indicates that our approach can be applied to very large datasets since it scale linearly with the number of observation while the experimental result show that our method performs better than the state of the art approach 
ranking algorithm whose goal is to appropriately order a set of object document are an important component of information retrieval system previous work on ranking algorithm ha focused on case where only labeled data is available for training i e supervised learning in this paper we consider the question whether unlabeled test data can be exploited to improve ranking performance we present a framework for transductive learning of ranking function and show that the answer is affirmative our framework is based on generating better feature from the test data via kernelpca and incorporating such feature via boosting thus learning different ranking function adapted to the individual test query we evaluate this method on the letor trec ohsumed dataset and demonstrate significant improvement 
cross site scripting flaw have now surpassed buffer overflow a the world s most common publicly reported security vulnerability in recent year browser vendor and researcher have tried to develop client side filter to mitigate these attack we analyze the best existing filter and find them to be either unacceptably slow or easily circumvented worse some of these filter could introduce vulnerability into site that were previously bug free we propose a new filter design that achieves both high performance and high precision by blocking script after html parsing but before execution compared to previous approach our approach is faster protects against more vulnerability and is harder for attacker to abuse we have contributed an implementation of our filter design to the webkit open source rendering engine and the filter is now enabled by default in the google chrome browser 
this paper present an innovative style wise advertising platform for web page web page style mainly refers to visual effect such a color and layout unlike the most popular ad network such a google adsense which need publisher to change the original structure of their page and define the position and style of the embedded ad manually style wise page advertising aim to automatically deliver style consistent ad at proper position within the web page without breaking the layout of the original page our system is motivated from the fact that almost web page contain blank region without any content given a web page with some blank region style wise page advertising is able to detect the suitable blank area for advertising rank the ad according to the semantic relevance and web page style and embed relevant ad into these nonintrusive area style wise page advertising represents one of the first attempt towards contextual advertising which enables the publisher to save effort when applying online advertising service 
we study the recurrence dynamic of query in web search by analysing a large real world query log dataset we find that query frequency is more useful in predicting collective query recurrence whereas query recency is more useful in predicting individual query recurrence our finding provide valuable insight for understanding and improving web search 
we propose a novel iterative searching and refining prototype for tagged image this prototype named pivotbrowser capture semantically similar tag set in a structure called pivot by constructing a pivot for a textual query pivotbrowser first selects candidate image possibly relevant to the query the tag contained in these candidate image are then selected in term of their tag relevance to the pivot the shortlisted tag are clustered and one of the tag cluster is used to select the result from the candidate image ranking of the image in each partition is based on their relevance to the tag cluster with the guidance of the tag cluster presented a user is able to perform searching and iterative query refinement 
there are significant barrier to academic research into user web search preference academic researcher are unable to manipulate the result shown by a major search engine to user and would have no access to the interaction data collected by the engine our initial approach to overcoming this wa to ask participant to submit query to an experimental search engine rather than their usual search tool over several different experiment we found that initial user buy in wa high but that people quickly drifted back to their old habit and stopped contributing data here we report our investigation of possible reason why this occurs an alternative approach is exemplified by the lemur browser toolbar which allows local collection of user interaction data from search engine session but doe not allow result page to be modified we will demonstrate a new firefox toolbar that we have developed to support experiment in which search result may be arbitrarily manipulated using our toolbar academic can set up the experiment they want to conduct while collecting subject to human experimentation guideline query click and dwell time a well a optional explicit judgment 
a common practice in comparative evaluation of information retrieval ir system is to create a test collection comprising a set of topic query a document corpus and relevance judgment and to monitor the performance of retrieval system over such a collection a typical evaluation of a system involves computing a performance metric e g average precision ap for each topic and then using the average performance metric e g mean average precision map to express the overall system performance however average do not capture all the important aspect of system performance and used alone may not thoroughly express system effectiveness i e average of performance can mask large variance in individual topic effectiveness the author hypothesis is that in addition to the average of overall performance attention need to be paid to how a system performance varies across topic this variability can be measured by calculating the standard deviation sd of individual performance score we refer to this performance variation a volatility 
the analysis of search transaction log often characterizes a search session but rarely look at the end point when do user stop and what cue are present suggesting that stopping is eminent in this preliminary analysis of the log of search session conducted in a laboratory setting we identified the activity performed by participant a well a search transition that were invoked over the course of a search session the search transition per task on average contained a total of action we isolated the final transition in each search session for detailed analysis a hypothesized some behaviour are predictable and suggestive of stopping behavior with the potential for modeling 
a a good complement to page content anchor text have been extensively used and proven to be useful in commercial search engine however anchor text have been assumed to be independent whether they come from the same web site or not intuitively an anchor text from unrelated web site should be considered a stronger evidence than that from the same site this paper proposes two new method to take into account the possible relationship between anchor text we consider two relationship in this paper link from the same site and link from related site the importance assigned to the anchor text in these two situation is discounted experimental result show that these two new model outperform the baseline model which assumes independence between hyperlink 
name ambiguity is a big challenge in people information retrieval and ha received considerable attention especially with the increasing volume of web data in recent year in this demo we present a system grape which is capable of finding people related information over the web the salient feature of our system are people name disambiguation and people tag presentation which effectively distinguish different people entity sharing the same name and uniquely represent each namesake with a cluster of tag such a occupation birthdate and organization 
this paper proposes facetedpedia a faceted retrieval system for information discovery and exploration in wikipedia given the set of wikipedia article resulting from a keyword query facetedpedia generates a faceted interface for navigating the result article compared with other faceted retrieval system facetedpedia is fully automatic and dynamic in both facet generation and hierarchy construction and the facet are based on the rich semantic information from wikipedia the essence of our approach is to build upon the collaborative vocabulary in wikipedia more specifically the intensive internal structure hyperlink and folksonomy category system given the sheer size and complexity of this corpus the space of possible choice of faceted interface is prohibitively large we propose metric for ranking individual facet hierarchy by user s navigational cost and metric for ranking interface each with k facet by both their average pairwise similarity and average navigational cost we thus develop faceted interface discovery algorithm that optimize the ranking metric our experimental evaluation and user study verify the effectiveness of the system 
much of the information on the web is found in article from online news outlet magazine encyclopedia review collection and other source however extracting this content from the original html document is complicated by the large amount of le informative and typically unrelated material such a navigation menu form user comment and ad existing approach tend to be either brittle and demand significant expert knowledge and time manual or tool assisted generation of rule or code necessitate labeled example for every different page structure to be processed wrapper induction require relatively uniform layout template detection or a with visual page segmentation vip are computationally expensive we introduce maximum subsequence segmentation a method of global optimization over token level local classifier and apply it to the domain of news website training example are easy to obtain both learning and prediction are linear time and result are excellent our semi supervised algorithm yield an overall f score of surpassing even those produced by vip with a hypothetical perfect block selection heuristic we also evaluate against the recent cleaneval shared task with surprisingly good cross task performance cleaning general web page exceeding the top text only score based on levenshtein distance versus 
this paper proposes an efficient relevance feedback based interactive model for keyword generation in sponsored search advertising we formulate the ranking of relevant term a a supervised learning problem and suggest new term for the seed by leveraging user relevance feedback information active learning is employed to select the most informative sample from a set of candidate term for user labeling experiment show our approach improves the relevance of generated term significantly with little user effort required 
this paper present an extensive study about the evolution of textual content on the web which show how some new page are created from scratch while others are created using already existing content we show that a significant fraction of the web is a byproduct of the latter case we introduce the concept of web genealogical tree in which every page in a web snapshot is classified into a component we study in detail these component characterizing the copy and identifying the relation between a source of content and a search engine by comparing page relevance measure document returned by real query performed in the past and click through data we observe that source of copy are more frequently returned by query and more clicked than other document 
in this paper we investigate a novel and important problem in multi document summarization i e how to extract an easy to understand english summary for non native reader existing summarization system extract the same kind of english summary from english news document for both native and non native reader however the non native reader have different english reading skill because they have different english education and learning background an english summary which can be easily understood by native reader may be hardly understood by non native reader we propose to add the dimension of reading easiness or difficulty to multi document summarization and the proposed eusum system can produce easy to understand summary according to the english reading skill of the reader the sentence level reading easiness or difficulty is predicted by using the svm regression method and the reading easiness score of each sentence is then incorporated into the summarization process empirical evaluation and user study have been performed and the result demonstrate that the eusum system can produce more easy to understand summary for non native reader than existing summarization system with very little sacrifice of the summary s informativeness 
automatic topic discovery and tracking on web shared video can greatly benefit both web service provider and end user most of current solution of topic detection and tracking were done on news and cannot be directly applied on web video because the semantic information of web video is much le than that of news video in this paper we propose a bipartite graph model to address this issue the bipartite graph represents the correlation between web video and their keywords and automatic topic discovery is achieved through two step coarse topic filtering and fine topic re ranking first a weight updating co clustering algorithm is employed to filter out topic candidate at a coarse level then the video on each topic are re ranked by analyzing the link structure of the corresponding bipartite graph after the topic are discovered the interesting one can also be tracked over a period of time using the same bipartite graph model the key is to propagate the relevant score and keywords from the video of interest to other relevant one through the bipartite graph link experimental result on real web video from youku a youtube counterpart in china demonstrate the effectiveness of the proposed method we report very promising result 
the method of finding high quality answer ha significant impact on user satisfaction in community question answering system however due to the lexical gap between question and answer a well a spam typically existing in user generated content filtering and ranking answer is very challenging previous solution mainly focus on generating redundant feature or finding textual clue using machine learning technique none of them ever consider question and their answer a relational data but instead model them a independent information moreover they only consider the answer of the current question and ignore any previous knowledge that would be helpful to bridge the lexical and semantic gap we assume that answer are connected to their question with various type of latent link i e positive indicating high quality answer negative link indicating incorrect answer or user generated spam and propose an analogical reasoning based approach which measure the analogy between the new question answer linkage and those of relevant knowledge which contains only positive link the candidate answer which ha the most analogous link is assumed to be the best answer we conducted experiment based on million yahoo answer question answer thread and showed the effectiveness of our approach 
online advertising represents a growing part of the revenue of major internet service provider such a google and yahoo a commonly used strategy is to place advertisement ad on the search result page according to the user submitted query relevant ad are likely to be clicked by a user and to increase the revenue of both advertiser and publisher however bid phrase defined by ad owner are usually contained in limited number of ad directly matching user query with bid phrase often result in finding few appropriate ad to address this shortcoming query expansion is often used to increase the chance to match the ad nevertheless query expansion on top of the traditional inverted index face efficiency issue such a high time complexity and heavy i o cost moreover precision cannot always be improved sometimes even hurt due to the involvement of additional noise in this paper we propose an efficient ad search solution relying on a block based index able to tackle the issue associated with query expansion our index structure place cluster of similar bid phrase in corresponding block with their associated ad it reduces the number of merge operation significantly during query expansion and allows sequential scan rather than random access saving i o cost we adopt flexible block size according to the clustering result of bid phrase to further optimize the index structure for efficient ad search the pre computation of such cluster is achieved through an agglomerative iterative clustering algorithm finally we adapt the spreading activation mechanism to return the top k relevant ad improving search precision the experimental result of our prototype adsearch show that we can indeed return a larger number of relevant ad without sacrificing execution speed 
while query expansion technique have been shown to improve retrieval performance in a centralized setting they have not been well studied in a federated setting in this paper we consider how query expansion may be adapted to federated environment and propose several new method where focused expansion are used in a selective fashion to produce specific query for each source or a set of source on a number of different testbeds we show that focused query expansion can significantly outperform the previously proposed global expansion method and contrary to earlier work show that query expansion can improve performance over standard federated retrieval these finding motivate further research examining the different method for query expansion and other form of system and user interaction in order to continue improving the performance of interactive federated search system 
this paper present a new method of calculating the semantic similarity between two article based on wordnet to further improve the performance of the proposed method we build a new compact concept ontology cco from wordnet by combining the word with similar semantic meaning the experimental result show that our approach significantly outperforms a recent proposal of computing semantic similarity and demonstrate the superiority of the proposed cco method 
we investigate using topic prediction data a a summary of document content to compute measure of search result quality unlike existing quality measure such a query clarity that require the entire content of the top ranked result class based statistic can be computed efficiently online because class information is compact enough to precompute and store in the index in an empirical study we compare the performance of class based statistic to their language model counterpart for predicting two measure query difficulty and expansion risk our finding suggest that using class prediction can offer comparable performance to full language model while reducing computation overhead 
in this paper we present a system for cross lingual information retrieval which can handle ten of language and million of document functioning of the system is demonstrated on corpus of european legislation language more than document per language the system us an interactive web interface which can take advantage of a predefined thesaurus allowing the user to dynamically re rank the retrieval result based on the mapping onto a predefined thesaurus 
in this paper we highlight the use of multimedia technology in generating intrinsic summary of tourism related information the system utilizes an automated process to gather filter and classify information on various tourist spot on the web the end result present to the user is a personalized multimedia summary generated with respect to user query filled with text image video and real time news made retrievable for mobile device preliminary experiment demonstrate the superiority of our presentation scheme to traditional method 
web based social system enable new community based opportunity for participant to engage share and interact this community value and related service like search and advertising are threatened by spammer content polluter and malware disseminator in an effort to preserve community value and ensure longterm success we propose and evaluate a honeypot based approach for uncovering social spammer in online social system two of the key component of the proposed approach are the deployment of social honeypot for harvesting deceptive spam profile from social networking community and statistical analysis of the property of these spam profile for creating spam classifier to actively filter out existing and new spammer we describe the conceptual framework and design consideration of the proposed approach and we present concrete observation from the deployment of social honeypot in myspace and twitter we find that the deployed social honeypot identify social spammer with low false positive rate and that the harvested spam data contains signal that are strongly correlated with observable profile feature e g content friend information posting pattern etc based on these profile feature we develop machine learning based classifier for identifying previously unknown spammer with high precision and a low rate of false positive 
the study follows action science approach to the problem of nonprofit housing service after month of action science based activity such a organized participant observation in depth interview field work and focus group study the main finding are web suit nonprofit organization better than traditional web in term of maintenance cost and usability mapping tool make better gui with respect to web based housing service and context aware personalization can translate to better user experience a an rdfs based working prototype ha been built and tested a user survey show high level user satisfaction although the case study wa carried in a nonprofit housing organization the practice in action research approach can be applied to other npos a well 
we propose to mine structured query template from search log for enabling rich query interpretation that recognizes both query intent and associated attribute we formalize the notion of template a a sequence of keywords and domain attribute and our objective is to discover template with high precision and recall for matching query in a domain of interest our solution bootstrap from small seed input knowledge to discover relevant query template by harnessing the wealth of information available in search log we model this information in a tri partite quest network of query site and template we propose a probabilistic inferencing framework based on the dual metric of precision and recalland we show that the dual inferencing correspond respectively to the random walk in backward and forward direction we deployed and tested our algorithm over a real world search log of million query the algorithm achieved accuracy of a high a on f measure with little seed knowledge and even with incomplete domain schema 
a web search engine must update it index periodically to incorporate change to the web we argue in this paper that index update fundamentally impact the design of search engine result cache a performance critical component of modern search engine index update lead to the problem of cache invalidation invalidating cached entry of query whose result have changed naive approach such a flushing the entire cache upon every index update lead to poor performance and in fact render caching futile when the frequency of update is high solving the invalidation problem efficiently corresponds to predicting accurately which query will produce different result if re evaluated given the actual change to the index to obtain this property we propose a framework for developing invalidation predictor and define metric to evaluate invalidation scheme we describe concrete predictor using this framework and compare them against a baseline that us a cache invalidation scheme based on time to live ttl evaluation over wikipedia document using a query log from the yahoo search engine show that selective invalidation of cached search result can lower the number of unnecessary query evaluation by a much a compared to a baseline scheme while returning result of similar freshness in general our predictor enable fewer unnecessary invalidation and fewer stale result compared to a ttl only scheme for similar freshness of result 
dwell time on web page ha been extensively used for various information retrieval task however some basic yet important question have not been sufficiently addressed eg what distribution is appropriate to model the distribution of dwell time on a web page and furthermore what the distribution tell u about the underlying browsing behavior in this paper we draw an analogy between abandoning a page during web browsing and a system failure in reliability analysis and propose to model the dwell time using the weibull distribution using this distribution provides better goodness of fit to real world data and it uncovers some interesting pattern of user browsing behavior not previously reported for example our analysis reveals that web browsing in general exhibit a significant negative aging phenomenon which mean that some initial screening ha to be passed before a page is examined in detail giving rise to the browsing behavior that we call screen and glean in addition we demonstrate that dwell time distribution can be reasonably predicted purely based on low level page feature which broadens the possible application of this study to situation where log data may be unavailable 
detecting cluster or community in large real world graph such a large social or information network is a problem of considerable interest in practice one typically chooses an objective f unction that capture the intuition of a network cluster a set of nod e with better internal connectivity than external connectivity and then one applies approximation algorithm or heuristic to extract set of node that are related to the objective function and that lo ok like good community for the application of interest in this paper we explore a range of network community detection method in order to compare them and to understand their relative performance and the systematic bias in the cluster t hey identify we evaluate several common objective function that are used to formalize the notion of a network community and we examine several different class of approximation algorithm tha t aim to optimize such objective function in addition rather tha n simply fixing an objective and asking for an approximation to the be t cluster of any size we consider a size resolved version of the optimization problem considering community quality a a function of it size provides a much finer lens with which to examine communit y detection algorithm since objective function and approximation algorithm often have non obvious size dependent behavior 
we study in this paper the web forum crawling problem which is a very fundamental step in many web application such a search engine and web data mining a a typical user created content ucc web forum ha become an important resource on the web due to it rich information contributed by million of internet user every day however web forum crawling is not a trivial problem due to the in depth link structure the large amount of duplicate page a well a many invalid page caused by login failure issue in this paper we propose and build a prototype of an intelligent forum crawler irobot which ha intelligence to understand the content and the structure of a forum site and then decide how to choose traversal path among different kind of page to do this we first randomly sample download a few page from the target forum site and introduce the page content layout a the characteristic to group those pre sampled page and re construct the forum s sitemap after that we select an optimal crawling path which only traverse informative page and skip invalid and duplicate one the extensive experimental result on several forum show the performance of our system in the following aspect effectiveness compared to a generic crawler irobot significantly decrease the duplicate and invalid page efficiency with a small cost of pre sampling a few page for learning the necessary knowledge irobot save substantial network bandwidth and storage a it only fetch informative page from a forum site and long thread that are divided into multiple page can be re concatenated and archived a a whole thread which is of great help for further indexing and data mining 
with the explosive growth of digital camera and online medium it ha become crucial to design efficient method that help user browse and search large image collection the recent visualrank algorithm employ visual similarity to represent the link structure in a graph so that the classic pagerank algorithm can be applied to select the most relevant image however measuring visual similarity is difficult when there exist diversified semantics in the image collection and the result from visualrank cannot supply good visual summarization with diversity this paper proposes to rank the image in a structural fashion which aim to discover the diverse structure embedded in photo collection and rank the image according to their similarity among local neighborhood instead of across the entire photo collection we design a novel algorithm named rankcompete which generalizes the pagerank algorithm for the task of simultaneous ranking and clustering the experimental result show that rankcompete outperforms visualrank and provides an efficient but effective tool for organizing web photo 
client scripting permit end user to customize content layout or style of their favourite website but current scripting suffers from a tight coupling with the website if the page change all the scripting can fall apart the problem is that website are reckoned to evolve frequently and this can jeopardize all the scripting effort to avoid this situation this work enriches website with a modding interface in an attempt to decouple layman s script from website upgrade from the website viewpoint this interface ensures safe scripting i e script that do not break the page from a scripter perspective this interface limit tuning but increase change resilience the approach try to find a balance between openness scripter free inspection and modularity scripter isolation from website design decision that permit scripting to scale up a a mature software practice the approach is realized for greasemonkey script 
query difficulty prediction aim to identify in advance how well an information retrieval system will perform when faced with a particular search request the current standard evaluation methodology involves calculating a correlation coefficient to indicate how strongly the predicted query difficulty is related with an actual system performance measure usually average precision we run a series of experiment based on predictor that have been shown to perform well in the literature comparing these across different trec run our result demonstrate that the current evaluation methodology is severely limited although it can be used to demonstrate the performance of a predictor for a single system such performance is not consistent over a variety of retrieval system we conclude that published result in the query difficulty area are generally not comparable and recommend that prediction be evaluated against a spectrum of underlying search system 
long query form a difficult but increasingly important segment for web search engine query reduction a technique for dropping unnecessary query term from long query improves performance of ad hoc retrieval on trec collection also it ha great potential for improving long web query upto improvement in ndcg however query reduction on the web is hampered by the lack of accurate query performance predictor and the constraint imposed by search engine architecture and ranking algorithm in this paper we present query reduction technique for long web query that leverage effective and efficient query performance predictor we propose three learning formulation that combine these predictor to perform automatic query reduction these formulation enable trading of average improvement for the number of query impacted and enable easy integration into the search engine s architecture for rank time query reduction experiment on a large collection of long query issued to a commercial search engine show that the proposed technique significantly outperform baseline with more than improvement in ndcg in the impacted set of query extension to the formulation such a result interleaving further improves result we find that the proposed technique deliver consistent retrieval gain where it matter most poorly performing long web query 
this paper present result comparing user preference for search engine ranking with measure of effectiveness computed from a test collection it establishes that preference and evaluation measure correlate system measured a better on a test collection are preferred by user this correlation is established for both conventional web retrieval and for retrieval that emphasizes diverse result the ndcg measure is found to correlate best with user preference compared to a selection of other well known measure unlike previous study in this area this examination involved a large population of user gathered through crowd sourcing exposed to a wide range of retrieval system test collection and search task reason for user preference were also gathered and analyzed the work revealed a number of new result but also showed that there is much scope for future work refining effectiveness measure to better capture user preference 
audio podcasting is increasingly present in the educational field and is especially appreciated a an ubiquitous pervasive tool anywhere anytime at any pace for acquiring or expanding knowledge we designed and implemented a web based text to speech tt system for automatic generation of a set of structured audio podcasts from a single text document the system receives a document in input doc rtf or txt and in output provides a set of audio file that reflect the document s internal structure one mp file for each document section ready to be downloaded on portable mp player structured audio file are useful for everyone but are especially appreciated by blind user who must explore content audially fully accessible for the blind our system offer wai aria based web interface for easy navigation and interaction via screen reader and voice synthesizer and produce a set of accessible audio file for rockbox mp player mp and talk file allowing blind user to also listen to naturally spoken file name instead of their spelled out string in this demo we will show how the system work when a user interacts via screen reader and voice synthesizer showing the interaction with both our web based system and with an mp player 
evaluation measure act a objective function to be optimized by information retrieval system such objective function must accurately reflect user requirement particularly when tuning ir system and learning ranking function ambiguity in query and redundancy in retrieved document are poorly reflected by current evaluation measure in this paper we present a framework for evaluation that systematically reward novelty and diversity we develop this framework into a specific evaluation measure based on cumulative gain we demonstrate the feasibility of our approach using a test collection based on the trec question answering track 
information retrieval system are evaluated against test collection of topic document and assessment of which document are relevant to which topic document are chosen for relevance assessment by pooling run from a set of existing system new system can return unassessed document leading to an evaluation bias against them in this paper we propose to estimate the degree of bias against an unpooled system and to adjust the system s score accordingly bias estimation can be done via leave one out experiment on the existing pooled system but this requires the problematic assumption that the new system is similar to the existing one instead we propose that all system new and pooled be fully assessed against a common set of topic and the bias observed against the new system on the common topic be used to adjust score on the existing topic we demonstrate using resampling experiment on trec test set that our method lead to a marked reduction in error even with only a relatively small number of common topic and that the error decrease a the number of topic increase 
although many variant of language model have been proposed for information retrieval there are two related retrieval heuristic remaining external to the language modeling approach proximity heuristic which reward a document where the matched query term occur close to each other passage retrieval which score a document mainly based on the best matching passage existing study have only attempted to use a standard language model a a black box to implement these heuristic making it hard to optimize the combination parameter in this paper we propose a novel positional language model plm which implement both heuristic in a unified language model the key idea is to define a language model for each position of a document and score a document based on the score of it plms the plm is estimated based on propagated count of word within a document through a proximity based density function which both capture proximity heuristic and achieves an effect of soft passage retrieval we propose and study several representative density function and several different plm based document ranking strategy experiment result on standard trec test collection show that the plm is effective for passage retrieval and performs better than a state of the art proximity based retrieval model 
we introduce a generic engine for large scale similarity search and demonstrate it on a set of million flickr image 
we consider the problem of template independent news extraction the state of the art news extraction method is based on template level wrapper induction which ha two serious limitation it cannot correctly extract page belonging to an unseen template until the wrapper for that template ha been generated it is costly to maintain up to date wrapper for hundred of website because any change of a template may lead to the invalidation of the corresponding wrapper in this paper we formalize news extraction a a machine learning problem and learn a template independent wrapper using a very small number of labeled news page from a single site novel feature dedicated to news title and body are developed respectively correlation between the news title and the news body are exploited our template independent wrapper can extract news page from different site regardless of template in experiment a wrapper is learned from page from a single news site it achieved accuracy over news page from news site 
to manage the increasing amount of rdf data an rdf repository should provide not only necessary scalability and efficiency but also sufficient inference capability in this paper we propose a native rdf repository system to pursue a better tradeoff among the above requirement systemtakes the hypergraph representation for rdf a the data model for it persistent storage which effectively avoids the cost of data model transformation when accessing rdf data in addition a set of efficient semantic query processing technique are designed the result of performance evaluation on the lubm benchmark show that systemhas a better combined metric value than the other comparable system category and subject descriptor h information storage and retrieval general 
while question answering community have been gaining popularity for several year we wonder if the increased popularity actually improves or degrades the user experience in addition automatic qa system which utilize different source such a search engine and social medium are emerging rapidly qa community have already created abundant resource of million of question and hundred of million of answer the question whether they will continue to serve a an effective source is of information for web search and question answering is of vital importance in this poster we investigate the temporal evolution of a popular qa community yahoo answer with respect to it effectiveness in answering three basic type of question factoid opinion and complex question our experiment show that yahoo answer keep growing rapidly while it overall quality a an information source for factoid question answering degrades however instead of answering factoid question it might be more effective to answer opinion and complex question 
more and more structured information in the form of semantic data is nowadays available it offer a wide range of new possibility especially for semantic search and web data integration however their effective exploitation still brings about a number of challenge e g usability scalability and uncertainty in this paper we present dataplorer a solution designed to address these challenge we consider the usability through the use of hybrid query and faceted search while still preserving the scalability thanks to an extension of inverted index to support this type of query moreover dataplorer deal with uncertainty by mean of a powerful ranking scheme to find relevant result our experimental result show that our proposed approach is promising and it make u believe that it is possible to extend the current ir infrastructure to query and search the web of data 
internet source provide new way to acquire information about risk and to follow up the evolution of natural disaster in real time we will present first an architecture dedicated to unstructured information processing then we will show how the spatial and temporal representation extended with semantic property answer information ambiguity problem agate platform wa evaluated by a non governmental organization which filtered alert according to type of disaster and their location 
video summarization is a mechanism for generating short summary of the video to help people quickly make sense of the content of the video before downloading or seeking more detailed information to produce reliable automatic video summarization algorithm it is essential to first understand how human being create video summary with manual effort this paper focus on a corpus of instructional documentary video and seek to improve automatic video summary by understanding what feature in the video catch the eye and ear of human assessor and using these finding to inform automatic summarization algorithm the paper contributes a thorough and valuable methodology for performing automatic video summarization and the methodology can be extended to inform summarization of other video corpus 
while traditional question answering qa system tailored to the trec qa task work relatively well for simple question they do not suffice to answer real world question the community based qa system offer this service well a they contain large archive of such question where manually crafted answer are directly available however finding similar question in the qa archive is not trivial in this paper we propose a new retrieval framework based on syntactic tree structure to tackle the similar question matching problem we build a ground truth set from yahoo answer and experimental result show that our method outperforms traditional bag of word or tree kernel based method by in mean average precision it further achieves up to improvement by incorporating semantic feature a well a matching of potential answer our model doe not rely on training and it is demonstrated to be robust against grammatical error a well 
query processing is a major cost factor in operating large web search engine in this paper we study query result caching one of the main technique used to optimize query processing performance our first contribution is a study of result caching a a weighted caching problem most previous work ha focused on optimizing cache hit ratio but given that processing cost of query can vary very significantly we argue that total cost saving also need to be considered we describe and evaluate several algorithm for weighted result caching and study the impact of zipf based query distribution on result caching our second and main contribution is a new set of feature based cache eviction policy that achieve significant improvement over all previous method substantially narrowing the existing performance gap to the theoretically optimal clairvoyant method finally using the same approach we also obtain performance gain for the related problem of inverted list caching 
the web is increasingly understood a a global information space consisting not just of linked document but also of linked data more than just a vision the resulting web of data ha been brought into being by the maturing of the semantic web technology stack and by the publication of an increasing number of datasets according to the principle of linked data the linked data on the web ldow workshop brings together researcher and practitioner working on all aspect of linked data the workshop provides a forum to present the state of the art in the field and to discus ongoing and future research challenge in this workshop summary we will outline the technical context in which linked data is situated describe development in the past year through initiative such a the linking open data community project and look ahead to the workshop itself 
social annotation ha gained increasing popularity in many web based application leading to an emerging research area in text analysis and information retrieval this paper is concerned with developing probabilistic model and computational algorithm for social annotation we propose a unified framework to combine the modeling of social annotation with the language modeling based method for information retrieval the proposed approach consists of two step discovering topic in the content and annotation of document while categorizing the user by domain and enhancing document and query language model by incorporating user domain interest a well a topical background model in particular we propose a new general generative model for social annotation which is then simplified to a computationally tractable hierarchical bayesian network then we apply smoothing technique in a risk minimization framework to incorporate the topical information to language model experiment are carried out on a real world annotation data set sampled from del icio u our result demonstrate significant improvement over the traditional approach 
a weakly supervised extraction method identifies concept within conceptual hierarchy at the appropriate level of specificity e g bank v institution to which attribute e g routing number extracted from unstructured text best apply the extraction exploit labeled class of instance acquired from a combination of web document and query log and inserted into existing conceptual hierarchy the correct concept is identified within the top three position on average over gold standard attribute which corresponds to higher accuracy than in alternative experiment 
we focus on the task of target detection in automatic link generation with wikipedia i e given an n gram in a snippet of text find the relevant wikipedia concept that explain or provide background knowledge for it we formulate the task a a ranking problem and investigate the effectiveness of learning to rank approach and of the feature that we use to rank the target concept for a given n gram our experiment show that learning to rank approach outperform traditional binary classification approach also our proposed feature are effective both in binary classification and learning to rank setting 
since more and more business data are represented in xml format there is a compelling need of supporting analytical operation in xml query particularly the latest version of xquery proposed by w c xquery introduces a new construct to explicitly express grouping operation in flwor expression existing work in xml query processing mainly focus on physically matching query structure over xml document given the explicit grouping operation in a query how to efficiently compute grouping and aggregate function over xml document is not well studied yet in this paper we extend our previous xml query processing algorithm vert to efficiently perform grouping and aggregate function in query the main technique of our approach is introducing relational table to index value query pattern matching and aggregation computing are both conducted with table index we also propose two semantic optimization to further improve the query performance finally we present experimental result to validate the efficiency of our approach over other existing approach 
extracting semantic relation among entity is an important first step in various task in web mining and natural language processing such a information extraction relation detection and social network mining a relation can be expressed extensionally by stating all the instance of that relation or intensionally by defining all the paraphrase of that relation for example consider the acquisition relation between two company an extensional definition of acquisition contains all pair of company in which one company is acquired by another e g youtube google or powerset microsoft on the other hand we can intensionally define acquisition a the relation described by lexical pattern such a x is acquired by y or y purchased x where x and y denote two company we use this dual representation of semantic relation to propose a novel sequential co clustering algorithm that can extract numerous relation efficiently from unlabeled data we provide an efficient heuristic to find the parameter of the proposed coclustering algorithm using the cluster produced by the algorithm we train an l regularized logistic regression model to identify the representative pattern that describe the relation expressed by each cluster we evaluate the proposed method in three different task measuring relational similarity between entity pair open information extraction open ie and classifying relation in a social network system experiment conducted using a benchmark dataset show that the proposed method improves existing relational similarity measure moreover the proposed method significantly outperforms the current state of the art open ie system in term of both precision and recall the proposed method correctly classifies relation type in an online social network containing node and edge thereby demonstrating it efficacy in real world relation detection task 
the task in expert finding is to identify member of an organisation with relevant expertise on a given topic typically an expert search engine us evidence from the author of on topic document found in the organisation s intranet by search engine the search result click through behaviour of many intranet search engine user provides an additional source of evidence to identify topically relevant document and via document authorship expert in this poster we ass the usefulness of click through log data for expert finding we find that ranking author based solely on the click their document receive is reasonably effective at correctly identifying relevant expert moreover we show that this evidence can successfully be integrated with an existing expert search engine to increase it retrieval effectiveness 
we propose an expert finding method based on assumption of sequential dependence between a candidate expert and the query term in the scope of a document we assume that the strength of relation of a candidate to the document s content depends on it position in this document with respect to the position of the query term the experiment on the official enterprise trec data demonstrate the advantage of our method over the method based on independence of query term and person in a document 
question answering community such a naver and yahoo answer have emerged a popular and often effective mean of information seeking on the web by posting question for other participant to answer information seeker can obtain specific answer to their question user of popular portal such a yahoo answer already have submitted million of question and received hundred of million of answer from other participant however it may also take hour and sometime day until a satisfactory answer is posted in this paper we introduce the problem of predicting information seeker satisfaction in collaborative question answering community where we attempt to predict whether a question author will be satisfied with the answer submitted by the community participant we present a general prediction model and develop a variety of content structure and community focused feature for this task our experimental result obtained from a largescale evaluation over thousand of real question and user rating demonstrate the feasibility of modeling and predicting asker satisfaction we complement our result with a thorough investigation of the interaction and information seeking pattern in question answering community that correlate with information seeker satisfaction our model and prediction could be useful for a variety of application such a user intent inference answer ranking interface design and query suggestion and routing 
a research includes more and larger user study a significant problem lie in combining the many type of data file into a single table suitable for analysis by common statistical tool we have developed a data aggregation tool that combine user log expert scoring and task session attribute the tool also integrates the n gram derived from a given sequence of action in the user task the tool provides a gui for quick and easy configuration 
click through rate ctr is an important metric for ad system job portal recommendation system ctr impact publisher s revenue advertiser s bid amount in pay for performance business model we learn regression model using feature of the job optional click history of job feature of related job we show that our model predict ctr much better than predicting avg ctr for all job listing even in absence of the click history for the job listing 
we propose novel spatio temporal model to estimate click through rate in the context of content recommendation we track article ctr at a fixed location over time through a dynamic gamma poisson model and combine information from correlated location through dynamic linear regression significantly improving on per location model our model adjust for user fatigue through an exponential tilt to the first view ctr probability of click on first article exposure that is based only on user specific repeat exposure feature we illustrate our approach on data obtained from a module today module published regularly on yahoo front page and demonstrate significant improvement over commonly used baseline method large scale simulation experiment to study the performance of our model under different scenario provide encouraging result throughout all modeling assumption are validated via rigorous exploratory data analysis 
in this paper we study the problem of web forum crawling web forum ha now become an important data source of many web application while forum crawling is still a challenging task due to complex in site link structure and login control of most forum site without carefully selecting the traversal path a generic crawler usually downloads many duplicate and invalid page from forum and thus waste both the precious bandwidth and the limited storage space to crawl forum data more effectively and efficiently in this paper we propose an automatic approach to exploring an appropriate traversal strategy to direct the crawling of a given target forum in detail the traversal strategy consists of the identification of the skeleton link and the detection of the page flipping link the skeleton link instruct the crawler to only crawl valuable page and meanwhile avoid duplicate and uninformative one and the page flipping link tell the crawler how to completely download a long discussion thread which is usually shown in multiple page in web forum the extensive experimental result on several forum show encouraging performance of our approach following the discovered traversal strategy our forum crawler can archive more informative page in comparison with previous related work and a commercial generic crawler 
negative relevance feedback is a special case of relevance feedback where we do not have any positive example this often happens when the topic is difficult and the search result are poor although in principle any standard relevance feedback technique can be applied to negative relevance feedback it may not perform well due to the lack of positive example in this paper we conduct a systematic study of method for negative relevance feedback we compare a set of representative negative feedback method covering vector space model and language model a well a several special heuristic for negative feedback evaluating negative feedback method requires a test set with sufficient difficult topic but there are not many naturally difficult topic in the existing test collection we use two sampling strategy to adapt a test collection with easy topic to evaluate negative feedback experiment result on several trec collection show that language model based negative feedback method are generally more effective than those based on vector space model and using multiple negative model is an effective heuristic for negative feedback our result also show that it is feasible to adapt test collection with easy topic for evaluating negative feedback method through sampling 
we build a probabilistic model to identify implicit local intent query and leverage user s physical location to improve web search result for these query evaluation on commercial search engine show significant improvement on search relevance and user experience 
this paper establishes a connection between nmf and plsa on multi way data called ntf and t plsa respectively two type of t plsa model are proven to be equivalent to non negative parafac and non negative tucker this paper also show that by running ntf and t plsa alternatively they can jump out of each other s local minimum and achieve a better clustering solution 
with the booming development of the web popular chinese forum enable people to find experienced customer review for product in order to get an all around opinion about one product user need to go through plenty of web page which is time consuming and inefficient consequently automatic review mining and summarization ha become a hot research topic recently however previous approach are not applicable for mining chinese customer review in this paper we introduce soping a chinese customer review mining system that mine review from forum specifically we propose a novel search based approach to extract product feature and a feature oriented sentence orientation determination method our experimental result show that our proposed technique are highly effective 
giving suggestion to user of web based service is a common practice aimed at enhancing their navigation experience major web search engine usually provide suggestion under the form of query that are to some extent related to the current query typed by the user and the knowledge learned from the past usage of the system in this work we introduce search shortcut a successful query allowed in the past user to satisfy their information need differently from conventional suggestion technique our search shortcut allows to evaluate effectiveness by exploiting a simple train and test approach we have applied several collaborative filtering algorithm to this problem evaluating them on a real query log data we generate the shortcut from all user session belonging to the testing set and measure the quality of the shortcut suggested by considering the similarity between them and the navigational user behavior 
automatic summarization of jbig coded textual image is discussed compressed image are partially decompressed to compute relevant feature the feature extraction method is free from using any character recognition module summary sentence are ranked experiment considers document in indic script that lack in having any efficient ocr system script independent aspect of the approach is highlighted through use of two most popular indic script sentence selection efficiency of about is achieved when judged against man made summarization a nonparametric distribution free rank statistic show a correlation coefficient of a a measure of the minimum strength of the association between sentence ranking by machine and human 
much work ha attempted to model a user s click through behavior by mining the click log the task is not trivial due to the well known position bias problem some break throughs have been made two newly proposed click model dbn and ccm addressed this problem and improved document relevance estimation however to further improve the estimation we need a model that can capture more sophisticated user behavior in particular after clicking a search result a user s behavior such a the dwell time on the clicked document and whether there are further click on the clicked document can be highly indicative of the relevance of the document unfortunately such measure have not been incorporated in previous click model in this paper we introduce a novel click model called the post click click model pcc which provides an unbiased estimation of document relevance through leveraging both click behavior on the search page and post click behavior beyond the search page the pcc model is based on the bayesian approach and because of it incremental nature it is highly scalable to large scale and constantly growing log data extensive experimental result illustrate that the proposed method significantly outperforms the state of the art method merely relying on click log 
loose coupling is often quoted a a desirable property of system architecture one of the main goal of building system using web technology is to achieve loose coupling however given the lack of a widely accepted definition of this term it becomes hard to use coupling a a criterion to evaluate alternative web technology choice a all option may exhibit and claim to provide some kind of loose coupling effect this paper present a systematic study of the degree of coupling found in service oriented system based on a multi faceted approach thanks to the metric introduced in this paper coupling is no longer a one dimensional concept with loose coupling found somewhere in between tight coupling and no coupling the paper show how the metric can be applied to real world example in order to support and improve the design process of service oriented system 
several recent study have found only a weak relationship between the performance of a retrieval system and the success achievable by human searcher we hypothesize that searcher are successful precisely because they alter their behavior to explore the possible causal relation between system performance and search behavior we control system performance hoping to elicit adaptive search behavior subject each completed search using either a standard system or one of two degraded system using a general linear model we isolate the main effect of system performance by measuring and removing main effect due to searcher variation topic difficulty and the position of each search in the time series we find that searcher using our degraded system are a successful a those using the standard system but that in achieving this success they alter their behavior in way that could be measured in real time by a suitably instrumented system our finding suggest quite generally that some aspect of behavioral dynamic may provide unobtrusive indicator of system performance 
google entered china market a a late comer in late with no local employee an inadequate product line and small market share this talk will discus google china s effort to build up a team learn about local user need apply it global innovation model and won over user in the past year this talk will cover the result of our user study and our key finding about chinese user for searching and using the internet it will also discus how these finding were applied to our product and how these product gained traction in the market place it will also discus google s progress in chinese search relevance search user experience and key technology area where we innovated this talk will also discus the process of internationalization how google hired locally and applied it global project approach to encourage truly relevant local innovation it will discus several example of these innovation from product innovation like the weather map the input method editor sm greeting search to research innovation like parallel svm svd google china s progress dispelled the myth that multinational internet company cannot succeed in china the key ingredient like in any other success story are focus on the customer embrace the corporate culture empower local flexibility and of course innovate innovate innovate 
xml retrieval provides a focused access to the relevant content of document however in evaluation full document retrieval ha appeared competitive to focused xml retrieval we analyze the density of relevance in document and show that in sparsely relevant document focused retrieval performs better whereas in densely relevant document the performance of focused and document retrieval is equal 
we have developed an unsupervised framework for simultaneously extracting and normalizing attribute of product from multiple web page originated from different site our framework is designed based on a probabilistic graphical model that can model the page independent content information and the page dependent layout information of the text fragment in web page one characteristic of our framework is that previously unseen attribute can be discovered from the clue contained in the layout format of the text fragment our framework tackle both extraction and normalization task by jointly considering the relationship between the content and layout information dirichlet process prior is employed leading to another advantage that the number of discovered product attribute is unlimited an unsupervised inference algorithm based on variational method is presented the semantics of the normalized attribute can be visualized by examining the term weight in the model our framework can be applied to a wide range of web mining application such a product matching and retrieval we have conducted extensive experiment from four different domain consisting of over web page from over different web site demonstrating the robustness and effectiveness of our framework 
there ha recently been an upsurge of interest in the possibility of combining structured data and ad hoc information retrieval from traditional hypertext in this experiment we run query extracted from a query log of a major search engine against the semantic web to discover if the semantic web ha anything of interest to the average user we show that there is indeed much information on the semantic web that could be relevant for many query for people place and even abstract concept although they are overwhelmingly clustered around a semantic web enabled export of wikipedia known a dbpedia 
the impressionrank of a web page or more generally of a web site is the number of time user viewed the page while browsing search result impressionrank capture the visibility of page and site in search engine and is thus an important measure which is of interest to web site owner competitor market analyst and end user all previous approach to estimating the impressionrank of a page rely on privileged access to private data source like the search engine s query log in this paper we present the first external algorithm for estimating the impressionrank of a web page this algorithm relies on access to three public data source the search engine the query suggestion service of the search engine and the web in addition the algorithm is local and us modest resource it can therefore be used by almost any party to estimate the impressionrank of any page on any search engine en route to estimating the impressionrank of a page our algorithm solves a novel variant of the keyword extraction problem it find the most popular search keywords that drive impression of a page empirical analysis of the algorithm on the google and yahoo search engine indicates that it is accurate and provides interesting insight about site and search query 
in this paper we propose a collaborative knowledge semantic graph image search cksgis system it provides a novel way to conduct image search by utilizing the collaborative nature in wikipedia and by performing network analysis to form semantic graph for search term expansion the collaborative article editing process used by wikipedia s contributor is formalized a bipartite graph that are folded into network between term when a user type in a search term cksgis automatically retrieves an interactive semantic graph of related term that allow user to easily find related image not limited to a specific search term interactive semantic graph then serve a an interface to retrieve image through existing commercial search engine this method significantly save user time by avoiding multiple search keywords that are usually required in generic search engine it benefit both na ve user who do not posse a large vocabulary and professional who look for image on a regular basis in our experiment of the participant favored cksgis system rather than commercial search engine 
the primary business model behind web search is based on textual advertising where contextually relevant ad are displayed alongside search result we address the problem of selecting these ad so that they are both relevant to the query and profitable to the search engine showing that optimizing ad relevance and revenue is not equivalent selecting the best ad that satisfy these constraint also naturally incurs high computational cost and time constraint can lead to reduced relevance and profitability we propose a novel two stage approach which conduct most of the analysis ahead of time an offine preprocessing phase leverage additional knowledge that is impractical to use in real time and rewrite frequent query in a way that subsequently facilitates fast and accurate online matching empirical evaluation show that our method optimized for relevance match a state of the art method while improving expected revenue when optimizing for revenue we see even more substantial improvement in expected revenue 
in this paper we explore the use of parsimonious language model for web retrieval these model are smaller thus more efficient than the standard language model and are therefore well suited for large scale web retrieval we have conducted experiment on four trec topic set and found that the parsimonious language model result in improvement of retrieval effectiveness over the standard language model for all data set and measure in all case the improvement is significant and more substantial than in earlier experiment on newspaper newswire data 
this paper present a new video advertising system called adon which support intelligent overlay video ad unlike most current ad network such a youtube that overlay the ad at fixed position in the video e g on the bottom fifth of video second in adon is able to automatically detect a set of spatio temporal nonintrusive position and associate the contextually relevant ad with these position the overlay position are obtained on the basis of video structuring face and text detection a well a visual saliency analysis so that the intrusiveness to the user can be minimized the ad are selected according to content based multimodal relevance so that advertising relevance can be maximized adon represents one of the first attempt towards intelligent overlay video advertising by leveraging video content analysis technique 
a graph based semi supervised method for email spam filtering based on the local and global consistency method yield low error rate with very few labeled example the motivating application of this method is spam filter with access to very few labeled message for example during the initial deployment of a spam filter only a handful of labeled example are available but unlabeled example are plentiful we demonstrate the performance of our approach on trec and ceas email corpus our result compare favorably with the best known method using a few a just two labeled example one spam and one non spam 
web page like people are often known by others in a variety of context when those context are sufficiently distinct a page s importance may be better represented by multiple domain of authority rather than by one that indiscriminately mix reputation in this work we determine domain of authority by examining the context in which a page is cited however we find that it is not enough to determine separate domain of authority our model additionally determines the local flow of authority based upon the relative similarity of the source and target authority domain in this way we differentiate both incoming and outgoing hyperlink by topicality and importance rather than treating them indiscriminately we find that this approach compare favorably to other topical ranking method on two real world datasets and produce an approximately improvement in precision and quality of the top ten result over pagerank 
email spam is a much studied topic but even though current email spam detecting software ha been gaining a competitive edge against text based email spam new advance in spam generation have posed a new challenge image based spam image based spam is email which includes embedded image containing the spam message but in binary format in this paper we study the characteristic of image spam to propose two solution for detecting image based spam while drawing a comparison with the existing technique the first solution which us the visual feature for classification offer an accuracy of about i e an improvement of at least compared to existing solution svms support vector machine are used to train classifier using judiciously decided color texture and shape feature the second solution offer a novel approach for near duplication detection in image it involves clustering of image gmms gaussian mixture model based on the agglomerative information bottleneck aib principle using jensen shannon divergence j a the distance measure 
information filtering also referred to a publish subscribe complement one time searching since user are able to subscribe to information source and be notified whenever new document of interest are published in approximate information filtering only selected information source that are likely to publish document relevant to the user interest in the future are monitored to achieve this functionality a subscriber exploit statistical metadata to identify promising publisher and index it continuous query only in those publisher the statistic are maintained in a directory usually on a per keyword basis thus disregarding possible correlation among keywords using this coarse information poor publisher selection may lead to poor filtering performance and thus loss of interesting document based on the above observation this work extends query routing technique from the domain of distributed information retrieval in peer to peer p p network and provides new algorithm for exploiting the correlation among keywords in a filtering setting we develop and evaluate two algorithm based on single key and multi key statistic and utilize two different synopsis hash sketch and kmv synopsis to compactly represent publisher our experimental evaluation using two real life corpus with web and blog data demonstrates the filtering effectiveness of both approach and highlight the different tradeoff 
this paper present a novel approach to classifying library record by making use of what we call author profile a representation of an author s expertise along a library classification coupled with a string kernel classifier the idea is shown to bring a significant improvement over a baseline 
in this paper we present a semantic search technique considering the type of desired web resource and the semantic relationship between the resource and the query keywords in the ontology in order to effectively retrieve the most relevant top k resource we propose a novel ranking model to do this we devise a measure to determine the weight of the semantic relationship in addition we consider the number of meaningful semantic relationship between a resource and keywords the coverage of keywords and the distinguishability of keywords through experiment using real datasets we observe that our ranking model provides more accurate semantic search result compared to existing ranking model 
human computation is an eective way to channel human eort spent playing game to solving computational problem that are easy for human but dicult for computer to automate we propose thumb up a new game for human computation with the purpose of playing to rank search result our experience from user show that thumb up is not only fun to play but produce more relevant ranking than both a major search engine and optimal rank aggregation using the kemeny rule 
we bring forward a two phase semantic service discovery mechanism which support both the operation matchmaking and operation composition matchmaking a serial of experiment on a service management framework show that the mechanism gain better performance on both discovery recall rate and precision than a traditional matchmaker 
all information exchange on the internet whether through full text controlled vocabulary ontology or other mechanism ultimately requires that that an information provider and seeker use the same word or symbol in this paper we investigate what happens when both searcher and author are dynamically choosing term to match the other side with each side trying to anticipate the other doe a terminological convention ever emerge or do searcher and provider continue to miss potential partner through mi match of term we use a game theoretic setup to frame question and learning theory to make prediction about whether and which term will emerge a a convention 
when attempting to annotate music it is important to consider both acoustic content and social context this paper explores technique for collecting and combining multiple source of such information for the purpose of building a query by text music retrieval system we consider two representation of the acoustic content related to timbre and harmony and two social source social tag and web document we then compare three algorithm that combine these information source calibrated score averaging csa rankboost and kernel combination support vector machine kc svm we demonstrate empirically that each of these algorithm is superior to algorithm that use individual information source 
this paper proposes a new method for computing page importance referred to a browserank the conventional approach to compute page importance is to exploit the link graph of the web and to build a model based on that graph for instance pagerank is such an algorithm which employ a discrete time markov process a the model unfortunately the link graph might be incomplete and inaccurate with respect to data for determining page importance because link can be easily added and deleted by web content creator in this paper we propose computing page importance by using a user browsing graph created from user behavior data in this graph vertex represent page and directed edge represent transition between page in the user web browsing history furthermore the length of staying time spent on the page by user are also included the user browsing graph is more reliable than the link graph for inferring page importance this paper further proposes using the continuous time markov process on the user browsing graph a a model and computing the stationary probability distribution of the process a page importance an efficient algorithm for this computation ha also been devised in this way we can leverage hundred of million of user implicit voting on page importance experimental result show that browserank indeed outperforms the baseline method such a pagerank and trustrank in several task 
in the past some research suggested that engineer can use combined software reliability growth model srgms to obtain more accurate reliability prediction during testing in this paper three weighted combinational model namely equal linear and nonlinear weight are proposed for reliability estimation of web based software we further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed model preliminary result show that the linearly and nonlinearly weighted combinational model have better prediction capability than single srgm and equally weighted combinational model for web based software 
with the rapid increase in online video service video retrieval system are becoming increasingly important search tool to many user in many different field in this poster we present a novel video retrieval interface which support the creation of multiple search facet to aid user carrying out complex multi faceted search task the interface allows multiple search to be executed and viewed simultaneously and allows material to be reorganized between the facet an experiment is presented which compare the faceted interface to a tabbed interface similar to that on modern web browser and some preliminary result are given 
web search engine use highly optimized compression scheme to decrease inverted index size and improve query throughput and many index compression technique have been studied in the literature one approach taken by several recent study first performs a renumbering of the document id in the collection that group similar document together and then applies standard compression technique it is known that this can significantly improve index compression compared to a random document ordering we study index compression and query processing technique for such reordered index previous work ha focused on determining the best possible ordering of document in contrast we assume that such an ordering is already given and focus on how to optimize compression method and query processing for this case we perform an extensive study of compression technique for document id and present new optimization of existing technique which can achieve significant improvement in both compression and decompression performance we also propose and evaluate technique for compressing frequency value for this case finally we study the effect of this approach on query processing performance our experiment show very significant improvement in index size and query processing speed on the trec gov collection of million web page 
this paper provides a brief description of a research project on using ajax to enhance mobile web application for enterprise use the project known a mobileweaver ajax framework leverage enterprise soa service oriented architecture and the latest web technology on mobile device 
although pseudo relevance feedback prf technique improve average retrieval performance at the price of high variance not much is known about their optimality and the reason for their instability in this work we study more than topic from several test collection including the trec robust track and show that prf technique are highly suboptimal i e they do not make the fullest utilization of pseudo relevant document and under perform a careful selection of expansion term from the pseudo relevant document with the help of an oracle can actually improve retrieval performance dramatically by further we show that instability in prf technique is mainly due to wrong selection of expansion term from the pseudo relevant document our finding emphasize the need to revisit the problem of term selection to make a break through in prf 
many information retrieval ir system suffer from a radical variance in performance when responding to user query even for system that succeed very well on average the quality of result returned for some of the query is poor thus it is desirable that ir system will be able to identify difficult query in order to handle them properly understanding why some query are inherently more difficult than others is essential for ir and a good answer to this important question will help search engine to reduce the variance in performance hence better servicing their customer need the high variability in query performance ha driven a new research direction in the ir field on estimating the expected quality of the search result i e the query difficulty when no relevance feedback is given estimating the query difficulty is a significant challenge due to the numerous factor that impact retrieval performance many prediction method have been proposed recently however a many researcher observed the prediction quality of state of the art predictor is still too low to be widely used by ir application the low prediction quality is due to the complexity of the task which involves factor such a query ambiguity missing content and vocabulary mismatch the goal of this tutorial is to expose participant to the current research on query performance prediction also known a query difficulty estimation participant will become familiar with state of the art performance prediction method and with common evaluation methodology for prediction quality we will discus the reason that cause search engine to fail for some of the query and provide an overview of several approach for estimating query difficulty we then describe common methodology for evaluating the prediction quality of those estimator and some experiment conducted recently with their prediction quality a measured over several trec benchmark we will cover a few potential application that can utilize query difficulty estimator by handling each query individually and selectively based on it estimated difficulty finally we will summarize with a discussion on open issue and challenge in the field 
there are about of men and of woman suffering from colorblindness due to certain loss of color information the existing image search technique may not provide satisfactory result for these user in this demonstration we show an image search system that can accommodate colorblind user it can help these special user find and enjoy what they want by providing multiple service for them including search result reranking image recoloring and color indication 
user experience in social medium involves rich interaction with the medium content and other participant in the community in order to support such community it is important to understand the factor that drive the user engagement in this paper we show how to define statistical model of different complexity to describe pattern of song listening in an online music community first we adapt the lda model to capture music taste from listening activity across user and identify both the group of song associated with the specific taste and the group of listener who share the same taste second we define a graphical model that take into account listening session and capture the listening mood of user in the community our session model lead to group of song and group of listener with similar behavior across listening session and enables faster inference when compared to the lda model our experiment with the data from an online medium site demonstrate that the session model is better in term of the perplexity compared to two other model the lda based taste model that doe not incorporate cross session information and a baseline model that doe not use latent grouping of song 
in this paper we undertake a large scale study of online user behavior based on search and toolbar log we propose a new cc taxonomy of pageviews consisting of content news portal game vertical multimedia communication email social networking forum blog chat and search web search item search multimedia search we show that roughly half of all pageviews online are content one third are communication and the remaining one sixth are search we then give further breakdown to characterize the pageviews within each high level category we then study the extent to which page of certain type are revisited by the same user over time and the mechanism by which user move from page to page within and across host and within and across page type we consider robust scheme for assigning responsibility for a pageview to ancestor along the chain of referral we show that mail news and social networking pageviews are insular in nature appearing primarily in homogeneous session of one type search pageviews on the other hand appear on the path to a disproportionate number of pageviews but cannot be viewed a the principal mechanism by which those pageviews were reached finally we study the burstiness of pageviews associated with a url and show that by and large online browsing behavior is not significantly affected by breaking material with non uniform visit frequency 
with the sheer growth of online user data it becomes challenging to develop preference learning algorithm that are sufficiently flexible in modeling but also affordable in computation in this paper we develop nonparametric matrix factorization method by allowing the latent factor of two low rank matrix factorization method the singular value decomposition svd and probabilistic principal component analysis ppca to be data driven with the dimensionality increasing with data size we show that the formulation of the two nonparametric model are very similar and their optimization share similar procedure compared to traditional parametric low rank method nonparametric model are appealing for their flexibility in modeling complex data dependency however this modeling advantage come at a computational price it is highly challenging to scale them to large scale problem hampering their application to application such a collaborative filtering in this paper we introduce novel optimization algorithm which are simple to implement which allow learning both nonparametric matrix factorization model to be highly efficient on large scale problem our experiment on eachmovie and netflix the two largest public benchmark to date demonstrate that the nonparametric model make more accurate prediction of user rating and are computationally comparable or sometimes even faster in training in comparison with previous state of the art parametric matrix factorization model 
commercial web search engine have to process user query over huge web index under tight latency constraint in practice to achieve low latency large result cache are employed and a portion of the query traffic is served using previously computed result moreover search engine need to update their index frequently to incorporate change to the web after every index update however the content of cache entry may become stale thus decreasing the freshness of served result in this work we first argue that the real problem in today s caching for large scale search engine is not eviction policy but the ability to cope with change to the index i e cache freshness we then introduce a novel algorithm that us a time to live value to set cache entry to expire and selectively refreshes cached result by issuing refresh query to back end search cluster the algorithm prioritizes the entry to refresh according to a heuristic that combine the frequency of access with the age of an entry in the cache in addition for setting the rate at which refresh query are issued we present a mechanism that take into account idle cycle of back end server evaluation using a real workload show that our algorithm can achieve hit rate improvement a well a reduction in average hit age an implementation of this algorithm is currently in production use at yahoo 
world wide web provides plenty of multimedia resource for creating rich medium web application however the collected music and other medium resource always mismatch in the metric of time length existent music resizing approach suffer from perceptual artifact which degrade the performance of resized music in this paper a novel structure aware music resizing approach is proposed through lyric analysis our approach can compress different part of a music piece in variant compression rate experimental result show that the proposed method can effectively generate resized song with good quality 
search trail comprising query and web page view are created a searcher engage in information seeking activity online during known item search where the objective may be to locate a target web page searcher may waste valuable time repeatedly reformulating query a they attempt to locate an elusive page trail shortcut help user bypass unnecessary query and get them to their desired destination faster in this poster we present a comparative oracle study of technique to shortcut sub optimal search trail using label derived from social bookmarking anchor text query log and a human computation game we show that label can help user reach target page efficiently that the label source perform differently and that shortcut are potentially most useful when the target is challenging to find 
keyword query over structured database are notoriously ambiguous no single interpretation of a keyword query can satisfy all user and multiple interpretation may yield overlapping result this paper proposes a scheme to balance the relevance and novelty of keyword search result over structured database firstly we present a probabilistic model which effectively rank the possible interpretation of a keyword query over structured data then we introduce a scheme to diversify the search result by re ranking query interpretation taking into account redundancy of query result finally we propose ndcg w and w recall an adaptation of ndcg and s recall metric taking into account graded relevance of subtopics our evaluation on two real world datasets demonstrates that search result obtained using the proposed diversification algorithm better characterize possible answer available in the database than the result of the initial relevance ranking 
in web search ranking the expected result for some query could vary greatly depending upon location of the user we name such query regional sensitive query identifying regional sensitivity of query is important to meet user need the objective of this work is to identify whether a user expects only regional result for a query we present three novel feature generated from search log and build a meta query classifier to identify regional sensitive query experimental result show that the proposed method achieves high accuracy in identifying regional sensitive query 
this demonstration will show how graphical geospatial query specification can be used to obtain set of georeferenced data ranked by probability of relevance and displayed geographically and temporally in a geospatial browser with temporal support 
task recognizing named entity such a product people name or location from document have recently received significant attention in the literature many solution to these task assume the existence of reference entity table an important challenge that need to be addressed in the entity extraction task is that of ascertaining whether or not a candidate string approximately match with a named entity in a given reference table prior approach have relied on string based similarity which only compare a candidate string and an entity it match with in this paper we exploit web search engine in order to define new similarity function we then develop efficient technique to facilitate approximate matching in the context of our proposed similarity function in an extensive experimental evaluation we demonstrate the accuracy and efficiency of our technique 
topic form a crucial component of a test collection we show through visualization that the inex topic have shortcoming which question their validity for evaluating xml retrieval effectiveness 
how many iteration doe the ever more popular hit algorithm require to converge in score and perhaps more importantly in rank i e to get the node of a graph in the right order after pinning down the elusive notion of convergence in rank we provide the first non trivial bound on the convergence of hit a worst case example requiring a number of iteration superexponential in the size of the target graph to achieve even mild convergence suggests the need for greater caution in the experimental evaluation of the algorithm a recent result of poor performance e g v salsa might be due to insufficient iteration rather than to an intrinsic deficiency of hit an almost matching upper bound show that a long a one employ exponential acceleration e g through a squaring trick a polynomial running time practical in many application domain always provides strong convergence guarantee 
one goal of text mining is to provide reader with automatic method for quickly finding the key idea in individual document and whole corpus to this effect we propose a statistically well founded method for identifying the original idea that a document contributes to a corpus focusing on self referential diachronic corpus such a research publication blog email and news article our statistical model of passage impact defines interesting original content through a combination of impact and novelty and it can be used to identify the most original passage in a document unlike heuristic approach this statistical model is extensible and open to analysis we evaluate the approach on both synthetic and real data showing that the passage impact model outperforms a heuristic baseline method 
sequential pattern mining ha raised great interest in data mining research field in recent year however to our best knowledge no existing work study the problem of frequent sequence generator mining in this paper we present a novel algorithm feat abbr frequent sequence generator miner to perform this task experimental result show that feat is more efficient than traditional sequential pattern mining algorithm but generates more concise result set and is very effective for classifying web product review 
popular entity often have thousand of instance on the web in this paper we focus on the case where they are presented in table like format namely appearing with their attribute name it is observed that on one hand for the same entity different web page often incorporate different attribute on the other for the same attribute different web page often use different attribute name label therefore it is imaginably difficult to produce a global attribute schema for all the web entity of a given entity type based on their web instance although the global attribute schema is usually highly desired in web entity instance integration and web object extraction to this end we propose a novel framework of automatically learning a global attribute schema for all web entity of one specific entity type under this framework an iterative instance extraction procedure is first employed to extract sufficient web entity instance to discover enough attribute label next based on the label entity instance and related web page a maximum entropy based schema discovery approach is adopted to learn the global attribute schema for the target entity type experimental result on the chinese web achieve weighted average fscores of and on two global attribute schema for person type and movie type web entity respectively these result show that our framework is general efficient and effective 
in this paper we present fastrack a parameter free algorithm for dynamic resource provisioning that us simple statistic to promptly distill information about change in workload burstiness this information coupled with the application s end to end response time and system bottleneck characteristic guide resource allocation that show to be very effective under a broad variety of burstiness profile and bottleneck scenario 
the huge amount of knowledge in web community ha motivated the research interest in threaded discussion the dynamic nature of threaded discussion pose lot of challenging problem for computer scientist although technique such a semantic model and structural model have been shown to be useful in a number of area they are inefficient in understanding threaded discussion due to three reason i a most of user read existing message before posting post in a discussion thread are temporally dependent on the previous one it cause the semantics and structure to be coupled with each other in threaded discussion ii in online discussion thread there are a lot of junk post which are useless and may disturb content analysis and iii it is very hard to judge the quality of a post in this paper we propose a sparse coding based model named sm to simultaneously model semantics and structure of threaded discussion the model project each post into a topic space and approximates each post by a linear combination of previous post in the same discussion thread meanwhile the model also imposes two sparse constraint to force a sparse post reconstruction in the topic space and a sparse post approximation from previous post the sparse property effectively take into account the characteristic of threaded discussion towards the above three problem we demonstrate the competency of our model in three application reconstructing reply structure of threaded discussion identifying junk post and finding expert in a given board sub board in web community experimental result show encouraging performance of the proposed sm model in all these application 
javascript is a browser scripting language that allows developer to create sophisticated client side interface for web application however javascript code is also used to carry out attack against the user s browser and it extension these attack usually result in the download of additional malware that take complete control of the victim s platform and are therefore called drive by downloads unfortunately the dynamic nature of the javascript language and it tight integration with the browser make it difficult to detect and block malicious javascript code this paper present a novel approach to the detection and analysis of malicious javascript code our approach combine anomaly detection with emulation to automatically identify malicious javascript code and to support it analysis we developed a system that us a number of feature and machine learning technique to establish the characteristic of normal javascript code then during detection the system is able to identify anomalous javascript code by emulating it behavior and comparing it to the established profile in addition to identifying malicious code the system is able to support the analysis of obfuscated code and to generate detection signature for signature based system the system ha been made publicly available and ha been used by thousand of analyst 
we present a log based comparison of search pattern across three platform computer iphones and conventional mobile phone our goal is to understand how mobile search user differ from computer based search user and we focus heavily on the distribution and variability of task that user perform from each platform the result suggest that search usage is much more focused for the average mobile user than for the average computer based user however search behavior on high end phone resembles computer based search behavior more so than mobile search behavior a wide variety of implication follow from these finding first there is no single search interface which is suitable for all mobile phone we suggest that for the higher end phone a close integration with the standard computer based interface in term of personalization and available feature set would be beneficial for the user since these phone seem to be treated a an extension of the user computer for all other phone there is a huge opportunity for personalizing the search experience for the user s mobile need a these user are likely to repeatedly search for a single type of information need on their phone 
handling long query can involve either pruning the query to retain only the important term reduction or expanding the query to include related concept expansion while automatic technique to do so exist roughly performance improvement in term of map have been realized in past work through interactive variant we show that selectively reducing or expanding a query lead to an average improvement of in map over the baseline for standard trec test collection we demonstrate how user interaction can be used to achieve this improvement most interaction technique present user with a fixed number of option for all query we achieve improvement by interacting le with the user i e we present technique to identify the optimal number of option to present to user resulting in an interface with an average of fewer option to consider previous algorithm supporting interactive reduction and expansion are exponential in nature to extend their utility to operational environment we present technique to make the complexity of the algorithm polynomial we finally present an analysis of long query that continue to exhibit poor performance in spite of our new technique 
interpretation of tf idf are based on binary independence retrieval poisson information theory and language modelling this paper contributes a review of existing interpretation and then tf idf is systematically related to the probability p q d and p d q two approach are explored a space of independent and a space of disjoint term for independent term an extreme query non query term assumption uncovers tf idf and an analogy of p d q and the probabilistic odds o r d q mirror relevance feedback for disjoint term a relationship between probability theory and tf idf is established through the integral x dx log x this study uncovers component such a divergence from randomness and pivoted document length to be inherent part of a document query independence dqi measure and interestingly an integral of the dqi over the term occurrence probability lead to tf idf 
tagging ha emerged a a powerful mechanism that enables user to find organize and understand online entity recommender system similarly enable user to efficiently navigate vast collection of item algorithm combining tag with recommenders may deliver both the automation inherent in recommenders and the flexibility and conceptual comprehensibility inherent in tagging system in this paper we explore tagommenders recommender algorithm that predict user preference for item based on their inferred preference for tag we describe tag preference inference algorithm based on user interaction with tag and movie and evaluate these algorithm based on tag preference rating collected from movielens user we design and evaluate algorithm that predict user rating for movie based on their inferred tag preference our tag based algorithm generate better recommendation ranking than state of the art algorithm and they may lead to flexible recommender system that leverage the characteristic of item user find most important 
session fixation is a technique for obtaining the visitor s session identifier sid by forcing the visitor to use the sid supplied by the attacker the attacker who obtains the victim s sid can masquerade a the visitor in this paper we propose a technique to automatically detect session fixation vulnerability in web application our technique us attack simulator that executes a real session fixation attack and check whether it is successful or not in the experiment our system successfully detected vulnerability in our original test case and in a real world web application 
the semantics of rich multimedia presentation in the web such a smil svg and flash cannot or only to a very limited extend be understood by search engine today this hamper the retrieval of such presentation and make their archival and management a difficult task existing metadata model and metadata standard are either conceptually too narrow focus on a specific medium type only cannot be used and combined together or are not practically applicable for the semantic description of rich multimedia presentation in this paper we propose the multimedia metadata ontology m o for annotating rich structured multimedia presentation the m o provides a generic modeling framework for representing sophisticated multimedia metadata it allows for integrating the feature provided by the existing metadata model and metadata standard our approach base on semantic web technology and can be easily integrated with multimedia format such a the w c standard smil and svg with the m o we unlock the semantics of rich multimedia presentation in the web by making the semantics machine readable and machine understandable the m o is used with our semanticmm u framework for the multi channel generation of semantically rich multimedia presentation 
in this paper we present a prototype system that help user in early stage web research to create and reestablish context across fragmented work process without requiring them to explicitly collect and organize the material they visit the system cluster a user s web history and show it a research trail we present two user interaction model with the research trail the first interaction model is implemented a a standalone application which present a hierarchical view of research trail the second interaction model is integrated with the web browser it show the user s research trail a selectable and manipulable visual stream when they open a new tab thereby the newtab page serf a a springboard in the browser for a user resuming an ongoing task 
kiwi is a semantic wiki that combine the wiki philosophy of collaborative content creation with the method of the semantic web in order to enable effective knowledge management querying a wiki must be simple enough for beginning user yet powerful enough to accommodate experienced user to this end the keyword based kiwi query language kwql support query ranging from simple list of keywords to expressive rule for selecting and reshaping wiki meta data in this demo we showcase viskwql a visual interface for the kwql language aimed at supporting user in the query construction process viskwql and it editor are described and their functionality is illustrated using example query viskwql s editor provides guidance throughout the query construction process through hint warning and highlighting of syntactic error the editor enables round tripping between the twin language kwql and viskwql meaning that user can switch freely between the textual and visual form when constructing or editing a query it is implemented using html javascript and cs and can thus be used in almost any web browser without any additional software 
we present a novel approach to query reformulation which combine syntactic and semantic information by mean of generalized levenshtein distance algorithm where the substitution operation cost are based on probabilistic term rewrite function we investigate unsupervised compact and efficient model and provide empirical evidence of their effectiveness we further explore a generative model of query reformulation and supervised combination method providing improved performance at variable computational cost among other desirable property our similarity measure incorporate information theoretic interpretation of taxonomic relation such a specification and generalization 
we continue the line of research on graph compression started with webgraph but we move our focus to the compression of social network in a proper sense e g livejournal the approach that have been used for a long time to compress web graph rely on a specific ordering of the node lexicographical url ordering whose extension to general social network is not trivial in this paper we propose a solution that mix clustering and order and devise a new algorithm called layered label propagation that build on previous work on scalable clustering and can be used to reorder very large graph billion of node our implementation us overdecomposition to perform aggressively on multi core architecture making it possible to reorder graph of more than million node in a few hour experiment performed on a wide array of web graph and social network show that combining the order produced by the proposed algorithm with the webgraph compression framework provides a major increase in compression with respect to all currently known technique both on web graph and on social network these improvement make it possible to analyse in main memory significantly larger graph 
wikis have proven to be a valuable tool for collaboration and content generation on the web simple semantics and ease of use make wiki system well suited for meeting many emerging region need in the area of education collaboration and local content generation despite their usefulness current wiki software doe not work well in the network environment found in emerging region for example it is common to have long lasting network partition due to cost power and poor connectivity network partition make a traditional centralized wiki architecture unusable due to the unavailability of the central server existing solution towards addressing connectivity problem include web caching proxy and snapshot distribution while proxy and snapshot allow wiki data to be read while disconnected they prevent user from contributing update back to the wiki in this paper we detail the design and implementation of dtwiki a wiki system which explicitly address the problem of operating a wiki system in an intermittent environment the dtwiki system is able to cope with long lasting partition and bad connectivity while providing the functionality of popular wiki software such a mediawiki and twiki 
c sparql is an extension of sparql to support continuous query registered and continuously executed over rdf data stream considering window of such stream supporting stream in rdf format guarantee interoperability and open up important application in which reasoner can deal with knowledge that evolves over time we present c sparql by mean of example in urban computing 
mobile terminal such a cell phone are much more restricted in term of input output functionality and therefore some special technique must be incorporated to enable them to be easily used for web searching further searching for a location name is related to a dazzling variety of topic we relate these two factor to each other to yield a new search system for map and text information presenting search result a cluster is helpful for user especially in a mobile environment the system make mobile web searching easier and more efficient 
when trained and evaluated on accurately labeled datasets online email spam filter are remarkably effective achieving error rate an order of magnitude better than classifier in similar application but label acquired from user feedback or third party adjudication exhibit higher error rate than the best filter even filter trained using the same source of label it is appropriate to use naturally occuring label including error a training data in evaluating spam filter erroneous label are problematic however when used a ground truth to measure filter effectiveness any measurement of the filter s error rate will be augmented and perhaps masked by the label error rate using two natural source of label we demonstrate automatic and semi automatic method that reduce the influence of labeling error on evaluation yielding substantially more precise measurement of true filter error rate 
we present a programming model for building web application with security property that can be confidently verified during a security review in our model application are divided into isolated privilege separated component enabling rich security policy to be enforced in a way that can be checked by reviewer in our model the web framework enforces privilege separation and isolation of web application by requiring the use of an object capability language and providing interface that expose limited explicitly specified privilege to application component this approach restricts what each component of the application can do and quarantine buggy or compromised code it also provides a way to more safely integrate third party le trusted code into a web application we have implemented a prototype of this model based upon the java servlet framework and used it to build a webmail application our experience with this example suggests that the approach is viable and helpful at establishing reviewable application specific security property 
online social networking site are experiencing tremendous user growth with hundred of million of active user a a result there is a tremendous amount of user profile data online e g name birthdate etc protecting this data is a challenge the task of access policy composition is a tedious and confusing effort for the average user having hundred of friend we propose an approach that assist user in composing and managing their access control policy our approach is based on a supervised learning mechanism that leverage user provided example policy setting a training set to build classifier that are the basis for auto generated policy furthermore we provide mechanism to enable user to fuse policy decision that are provided by their friend or others in the social network these policy then regulate access to user profile object we implemented our approach and through extensive experimentation prove the accuracy of our proposed mechanism 
twitter offer an explicit mechanism to facilitate information diffusion and ha emerged a a new medium for communication many approach to find influentials have been proposed but they do not consider the temporal order of information adoption in this work we propose a novel method to find influentials by considering both the link structure and the temporal order of information adoption in twitter our method find distinct influentials who are not discovered by other method 
tag recommendation is a common way to enrich the textual annotation of multimedia content however state of the art recommendation method are built upon the pair wised tag relevance which hardly capture the context of the web video i e when who are doing what at where in this paper we propose the context oriented tag recommendation ctextr approach which expands tag for web video under the context consistent constraint given a web video ctextr first collect the multi form www resource describing the same event with the video which produce an informative and consistent context and then the tag recommendation is conducted based on the obtained context experiment on an web video collection show ctextr recommends various relevant tag to web video moreover the enriched tag improve the performance of web video categorization 
state of the art web search system enable aggregation of information from many source user are challenged to ass the reliability of information from different source we report on an empirical user study on the effect of displaying credibility rating of multiple cultural heritage source e g museum website art blog on user search performance and selection the result of our online interactive study n show that when explicitly presenting these rating people become significantly more confident in their selection of information from aggregated result 
understanding user intent is key to designing an effective ranking system in a search engine in the absence of any explicit knowledge of user intent search engine want to diversify result to improve user satisfaction in such a setting the probability ranking principle based approach of presenting the most relevant result on top can be sub optimal and hence the search engine would like to trade off relevance for diversity in the result in analogy to prior work on ranking and clustering system we use the axiomatic approach to characterize and design diversification system we develop a set of natural axiom that a diversification system is expected to satisfy and show that no diversification function can satisfy all the axiom simultaneously we illustrate the use of the axiomatic framework by providing three example diversification objective that satisfy different subset of the axiom we also uncover a rich link to the facility dispersion problem that result in algorithm for a number of diversification objective finally we propose an evaluation methodology to characterize the objective and the underlying axiom we conduct a large scale evaluation of our objective based on two data set a data set derived from the wikipedia disambiguation page and a product database 
we demonstrate sig ma both a service and an end user application to access the web of data a an integrated information space sig ma us an holistic approach in which large scale semantic web indexing logic reasoning data aggregation heuristic ad hoc ontology consolidation external service and responsive user interaction all play together to create rich entity description these consolidated entity description then form the base for embeddable data mashups machine oriented service a well a data browsing service finally we discus sig ma s peculiar characteristic and report on lessions learned and idea it inspires 
a major component of sense making is organizing grouping labeling and summarizing the data at hand in order to form a useful mental model a necessary precursor to identifying missing information and to reasoning about the data previous work ha shown the scatter gather model to be useful in exploratory activity that occur when user encounter unknown document collection however the topic structure communicated by scatter gather is closely tied to the behavior of the underlying clustering algorithm this structure may not reflect the mental model most applicable to the information need in this paper we describe the initial design of a mixed initiative information structuring tool that leverage aspect of the well studied scatter gather model but permit the user to impose their own desired structure when necessary 
we present a semantic approach to suggesting query completion which leverage entity and type information when compared to a frequency based approach we show that such information mostly help rare query 
freshness ha been increasingly realized by commercial search engine a an important criterion for measuring the quality of search result however most information retrieval method focus on the relevance of page content to given query without considering the recency issue in this work we mine page freshness from web user maintenance activity and incorporate this feature into web search we first quantify how fresh the web is over time from two distinct perspective the page itself and it in linked page and then exploit a temporal correlation between two type of freshness measure to quantify the confidence of page freshness result demonstrate page freshness can be better quantified when combining with temporal freshness correlation experiment on a real world archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness 
the inclusion of document length factor ha been a major topic in the development of retrieval model we believe that current model can be further improved by more refined estimation of the document s scope in this poster we present a new document length prior that us the size of the compressed document this new prior is introduced in the context of language modeling with dirichlet smoothing the evaluation performed on several collection show significant improvement in effectiveness 
this paper present sofie a system for automated ontology extension sofie can parse natural language document extract ontological fact from them and link the fact into an ontology sofie us logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate word to their most probable meaning to reason on the meaning of text pattern and to take into account world knowledge axiom this allows sofie to check the plausibility of hypothesis and to avoid inconsistency with the ontology the framework of sofie unites the paradigm of pattern matching word sense disambiguation and ontological reasoning in one unified model our experiment show that sofie delivers high quality output even from unstructured internet document 
the web offer rich relational data with different semantics in this paper we address the problem of document recommendation in a digital library where the document in question are networked by citation and are associated with other entity by various relation due to the sparsity of a single graph and noise in graph construction we propose a new method for combining multiple graph to measure document similarity where different factorization strategy are used based on the nature of different graph in particular the new method seek a single low dimensional embedding of document that capture their relative similarity in a latent space based on the obtained embedding a new recommendation framework is developed using semi supervised learning on graph in addition we address the scalability issue and propose an incremental algorithm the new incremental method significantly improves the efficiency by calculating the embedding for new incoming document only the new batch and incremental method are evaluated on two real world datasets prepared from citeseer experiment demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method 
we propose a new method to select relevant image to the given keywords from image gathered from theweb based on the probabilistic latent semantic analysis plsa model which is a probabilistic latent topic model originally proposed for text document analysis the experimental result show that the result by the proposed method is almost equivalent to or outperforms the result by existing method in addition it is proved that our method can select more various image compared to the existing svm based method 
social networking is one of the major technological phenomenon of the web with hundred of million of people participating social network enable a form of self expression for user and help them to socialize and share content with other user in spite of the fact that content sharing represents one of the prominent feature of existing social network site social network yet do not support any mechanism for collaborative management of privacy setting for shared content in this paper we model the problem of collaborative enforcement of privacy policy on shared data by using game theory in particular we propose a solution that offer automated way to share image based on an extended notion of content ownership building upon the clarke tax mechanism we describe a simple mechanism that promotes truthfulness and that reward user who promote co ownership we integrate our design with inference technique that free the user from the burden of manually selecting privacy preference for each picture to the best of our knowledge this is the first time such a protection mechanism for social networking ha been proposed in the paper we also show a proof of concept application which we implemented in the context of facebook one of today s most popular social network we show that supporting these type of solution is not also feasible but can be implemented through a minimal increase in overhead to end user 
in this work we propose a novel scheme for sentiment classification without labeled example which combine the strength of both learn based and lexicon based approach a follows we first use a lexicon based technique to label a portion of informative example from given task or domain then learn a new supervised classifier based on these labeled one finally apply this classifier to the task the experimental result indicate that proposed scheme could dramatically outperform learn based and lexicon based technique 
ranking search result is a fundamental problem in information retrieval in this paper we explore whether the use of proximity and phrase information can improve web retrieval accuracy we build on existing research by incorporating novel ranking feature based on flexible proximity term with recent state of the art machine learning ranking model we introduce a method of determining the goodness of a set of proximity term that take advantage of the structured nature of web document document metadata and phrasal information from search engine user query log we perform experiment on a large real world web data collection and show that using the goodness score of flexible proximity term can improve ranking accuracy over state of the art ranking method by a much a we also show that we can improve accuracy on the hardest query by a much a relative to state of the art approach 
we are in a phase of participatory web in which user add value to the information on the web by publishing tagging and sharing the participatory web ha enormous potential for an enterprise because unlike the user of the internet an enterprise is a community that share common goal assumption vocabulary and interest and ha reliable user identification and mutual trust along with a central governance and incentive to collaborate everyday the employee of an organization locate content relevant to their work on the web finding this information take time expertise and creativity which cost an organization money that is the web page employee find are knowledge asset owned by the enterprise this investment in web based knowledge asset is lost every time the enterprise fails to capture and reuse them icollaborate is tooled to capture user s web interaction persist and analyze it and feed that interaction back into the community the enterprise 
query term ranking approach are used to select effective term from a verbose query by ranking term feature used for query term ranking and selection in previous work do not consider grammatical relationship between term to address this issue we use syntactic feature extracted from dependency parsing result of verbose query we also modify the method for measuring the effectiveness of query term for query term ranking 
in this paper we propose a topical pagerank based algorithm for recommender system which aim to rank product by analyzing previous user item relationship and recommend top rank item to potentially interested user we evaluate our algorithm on movielens dataset and empirical experiment demonstrate that it outperforms other state of the art recommending algorithm 
this paper present a new collection based on dbpedia and inex for evaluating semantic search performance the proposed corpus is used to calculate the impact of considering document s structure on the retrieval performance of the lucene and bm ranking function result show that bm outperforms lucene in all the considered metric and that there is room for future improvement which may be obtained using a hybrid approach combining both semantic technology and information retrieval ranking function 
the last three year have seen a dramatic increase in both awareness and exploitation of web application vulnerability and saw dozen of high profile attack against website using cross site scripting x and cross site request forgery csrf for the purpose of information stealing website defacement malware planting clickjacking etc while an ideal solution may be to develop web application free from any exploitable vulnerability real world security is usually provided in layer we present content restriction and a content restriction enforcement scheme called content security policy csp which intends to be one such layer content restriction allow site designer or server administrator to specify how content interacts on their web site a security mechanism desperately needed by the untamed web these content restriction rule are activated and enforced by supporting web browser when a policy is provided for a site via http and we show how a system such a csp can be effective to lock down site and provide an early alert system for vulnerability on a web site our scheme is also easily deployed which is made evident by our prototype implementation in firefox and on the mozilla add ons web site 
the majority of people in rural developing region do not have access to the world wide web traditional network connectivity technology have proven to be prohibitively expensive in these area the emergence of new long range wireless technology provide hope for connecting these rural region to the internet however the network connectivity provided by these new solution are by nature intermittent due to high network usage rate frequent power cut and the use of delay tolerant link typical application especially interactive application like web search do not tolerate intermittent connectivity in this paper we present the design and implementation of ruralcafe a system intended to support efficient web search over intermittent network ruralcafe enables user to perform web search asynchronously and find what they are looking for in one round of intermittency a opposed to multiple round of search downloads ruralcafe doe this by providing an expanded search query interface which allows a user to specify additional query term to maximize the utility of the result returned by a search query given knowledge of the limited available network resource ruralcafe performs optimization to prefetch page to best satisfy a search query based on a user s search preference in addition ruralcafe doe not require modification to the web browser and can provide single round search result tailored to various type of network and economic constraint we have implemented and evaluated the effectiveness of ruralcafe using query from log made to a large search engine query made by user in an intermittent setting and live query from a small testbed deployment we have also deployed a prototype of ruralcafe in kerala india 
over the last few year flickr ha gained massive popularity and group in flickr are one of the main way for photo diffusion however the huge volume of group brings trouble for user to decide which group to choose in this paper we propose a tensor decomposition based group recommendation model to suggest group to user which can help tackle this problem the proposed model measure the latent association between user and group by considering both semantic tag and social relation experimental result show the usefulness of the proposed model 
general image retrieval is often carried out by a text based search engine such a google image search in this case natural language query are used a input to the search engine usually the user query are quite ambiguous and the returned result are not well organized a the ranking often done by the popularity of an image in order to address these problem we propose to use both textual and visual content of retrieved image to rerank web retrieved result in particular a machine learning technique a multi view clustering algorithm is proposed to reorganize the original result provided by the text based search engine preliminary result validate the effectiveness of the proposed framework 
we present aardvark a social search engine with aardvark user ask a question either by instant message email web input text message or voice aardvark then route the question to the person in the user s extended social network most likely to be able to answer that question a compared to a traditional web search engine where the challenge lie in finding the right document to satisfy a user s information need the challenge in a social search engine like aardvark lie in finding the right person to satisfy a user s information need further while trust in a traditional search engine is based on authority in a social search engine like aardvark trust is based on intimacy we describe how these consideration inform the architecture algorithm and user interface of aardvark and how they are reflected in the behavior of aardvark user 
in this paper we address the issue of learning to rank for document retrieval using thurstonian model based on sparse gaussian process thurstonian model represent each document for a given query a a probability distribution in a score space these distribution over score naturally give rise to distribution over document ranking however in general we do not have observed ranking with which to train the model instead each document in the training set is judged to have a particular relevance level for example bad fair good or excellent the performance of the model is then evaluated using information retrieval ir metric such a normalised discounted cumulative gain ndcg recently taylor et al presented a method called softrank which allows the direct gradient optimisation of a smoothed version of ndcg using a thurstonian model in this approach document score are represented by the output of a neural network and score distribution are created artificially by adding random noise to the score the softrank mechanism is a general one it can be applied to different ir metric and make use of different underlying model in this paper we extend the softrank framework to make use of the score uncertainty which are naturally provided by a gaussian process gp which is a probabilistic non linear regression model we further develop the model by using sparse gaussian process technique which give improved performance and efficiency and show competitive result against baseline method when tested on the publicly available letor ohsumed data set we also explore how the available uncertainty information can be used in prediction and how it affect model performance 
this demonstration present an xml ir system that allows user to give feedback of different granularity and type using dempster shafer theory of evidence to compute expanded and reweighted query 
the rapid growth of social networking site and web community have motivated web site to expose their apis to external developer who create mashups by assembling existing functionality current apis however aim toward developer with programming expertise they are not directly usable by wider class of user who do not have programming background but would nevertheless like to build their own mashups to address this need we propose a spreadsheet based web mashups development framework which enables user to develop mashups in the popular spreadsheet environment first we provide a mechanism that make structured data first class value of spreadsheet cell second we propose a new component model that can be used to develop fairly sophisticated mashups involving joining data source and keeping spreadsheet data up to date third to simplify mashup development we provide a collection of spreadsheet based mashup pattern that capture common web data access and spreadsheet presentation functionality user can reuse and customize these pattern to build spreadsheet based web mashups instead of developing them from scratch fourth we enable user to manipulate structured data presented on spreadsheet in a drag and drop fashion finally we have developed and tested a proof of concept prototype to demonstrate the utility of the proposed framework 
screen reader the dominant assistive technology used by visually impaired people to access the web function by speaking out the content of the screen serially using screen reader for conducting online transaction can cause considerable information overload because transaction such a shopping and paying bill typically involve a number of step spanning several web page one can combat this overload by using a transaction model for web accessibility that present only fragment of web page that are needed for doing transaction we can realize such a model by coupling a process automaton encoding state of a transaction with concept classifier that identify page fragment relevant to a particular state of the transaction in this paper we present a fully automated process that synergistically combine several technique for transforming unlabeled click stream data generated by transaction into a transactionmodel these technique include web content analysis to partition a web page into segment consisting of semantically related content contextual analysis of data surrounding clickable object in a page and machine learning method such a clustering of page segment based on contextual analysis statistical classification and automaton learning the use of unlabeled click stream in building transaction model ha important benefit i visually impaired user do not have to depend on sighted user for creating manually labeled training data to construct the model ii it is possible to mine personalized model from unlabeled transaction click stream associated with site that visually impaired user visit regularly iii since unlabeled data is relatively easy to obtain it is feasible to scale up the construction of domain specific transaction model e g separate model for shopping airline reservation bill payment etc iv adjusting the performance of deployed model over timtime with new training data is also doable we provide preliminary experimental evidence of the practical effectiveness of both domain specific a well a personalized accessibility transaction model built using our approach finally this approach is applicable for building transaction model for mobile device with limited size display a well a for creating wrapper for information extraction from web site 
in this poster we develop a novel method called hcc for hierarchical co clustering hcc brings together two interrelated but distinct theme from clustering hierarchical clustering and co clustering the goal of the former theme is to organize cluster into a hierarchy that facilitates browsing and navigation while the goal of the latter theme is to cluster different type of data simultaneously by making use of the relationship information our initial empirical result are promising and they demonstrate that simultaneously attempting both these goal in a single model lead to improvement over model that focus on a single goal 
data fusion is the combination of a number of independent search result relating to the same document collection into a single result to be presented to the user a number of probabilistic data fusion model have been shown to be effective in empirical study these typically attempt to estimate the probability that particular document will be relevant based on training data however little attempt ha been made to gauge how the accuracy of these estimation affect fusion performance the focus of this paper is twofold firstly that accurate estimation of the probability of relevance result in effective data fusion and secondly that an effective approximation of this probability can be made based on le training data that ha previously been employed this is based on the observation that the distribution of relevant document follows a similar pattern in most high quality result set curve fitting suggests that this can be modelled by a simple function that is le complex than other model that have been proposed the use of existing ir evaluation metric is proposed a a substitution for probability calculation mean average precision is used to demonstrate the effectiveness of this approach with evaluation result demonstrating competitive performance when compared with related algorithm with more onerous requirement for training data 
developing country face significant challenge in network access making even simple network task unpleasant many standard technique caching and predictive prefetching help somewhat but provide little or no assistance for personal data that is needed only by a single user sulula address this problem by leveraging the near ubiquity of cellular phone able to send and receive simple sm message rather than visit a kiosk and fetch data on demand a tiresome process at best user request a future visit if capacity exists the kiosk can schedule secure retrieval of that user s data saving time and more efficiently utilizing the kiosk s limited connectivity when the user arrives at a provisioned kiosk she need only obtain the session key on demand and thereafter ha instant access in addition sulula allows user to schedule data uploads experimental result show significant gain for the end user saving ten of minute of time for a typical email news reading session we also describe a small ongoing deployment in country for proof of concept lesson learned from that experience and provide a discussion on pricing and marketplace issue that remain to be addressed to make the system viable for developing world access 
opinion retrieval is a task of growing interest in social life and academic research which is to find relevant and opinionate document according to a user s query one of the key issue is how to combine a document s opinionate score the ranking score of to what extent it is subjective or objective and topic relevance score current solution to document ranking in opinion retrieval are generally ad hoc linear combination which is short of theoretical foundation and careful analysis in this paper we focus on lexicon based opinion retrieval a novel generation model that unifies topic relevance and opinion generation by a quadratic combination is proposed in this paper with this model the relevance based ranking serf a the weighting factor of the lexicon based sentiment ranking function which is essentially different from the popular heuristic linear combination approach the effect of different sentiment dictionary is also discussed experimental result on trec blog datasets show the significant effectiveness of the proposed unified model improvement of and have been obtained in term of map and p respectively the conclusion is not limited to blog environment besides the unified generation model another contribution is that our work demonstrates that in the opinion retrieval task a bayesian approach to combining multiple ranking function is superior to using a linear combination it is also applicable to other result re ranking application in similar scenario 
motivated by our work with political scientist who need to manually analyze large web archive of news site we present spotsigs a new algorithm for extracting and matching signature for near duplicate detection in large web crawl our spot signature are designed to favor natural language portion of web page over advertisement and navigational bar the contribution of spotsigs are twofold by combining stopword antecedent with short chain of adjacent content term we create robust document signature with a natural ability to filter out noisy component of web page that would otherwise distract pure n gram based approach such a shingling we provide an exact and efficient self tuning matching algorithm that exploit a novel combination of collection partitioning and inverted index pruning for high dimensional similarity search experiment confirm an increase in combined precision and recall of more than percent over state of the art approach such a shingling or i match and up to a factor of faster execution time than locality sensitive hashing lsh over a demonstrative gold set of manually assessed near duplicate news article a well a the trec wt g web collection 
motivated by several marketplace application on rapidly growing online social network we study the problem of efficient offline matching algorithm for online exchange market we consider two main model of one shot market and exchange market over time for one shot market we study three main variant of the problem one to one exchange market problem exchange market problem with short cycle and probabilistic exchange market problem we show that all the above problem are np hard and propose heuristic and approximation algorithm for these problem experiment show that the number of item exchanged will increase when exchange through cycle are allowed exploring algorithm for market over time is an interesting direction for future work 
javascript is an interpreted programming language most often used for enhancing webpage interactivity and functionality it ha powerful capability to interact with webpage document and browser window however it ha also opened the door for many browser based security attack insecure engineering practice of using javascript may not directly lead to security breach but they can create new attack vector and greatly increase the risk of browser based attack in this paper we present the first measurement study on insecure practice of using javascript on the web our focus is on the insecure practice of javascript inclusion and dynamic generation and we examine their severity and nature on unique website our measurement result reveal that insecure javascript practice are common at various website at least of the measured website manifest the insecure practice of including javascript file from external domain into the top level document of their webpage over of the measured website use the dangerous eval function to dynamically generate and execute javascript code on their webpage and in javascript dynamic generation using the document write method and the innerhtml property is much more popular than using the relatively secure technique of creating script element via dom method our analysis indicates that safe alternative to these insecure practice exist in common case and ought to be adopted by website developer and administrator for reducing potential security risk 
we introduce a client server toolkit called sync kit that demonstrates how client side database storage can improve the performance of data intensive website sync kit is designed to make use of the embedded relational database defined in the upcoming html standard to offload some data storage and processing from a web server onto the web browser to which it serf content our toolkit provides various strategy for synchronizing relational database table between the browser and the web server along with a client side template library so that portion web application may be executed client side unlike prior work in this area sync kit persists both template and data in the browser across web session increasing the number of concurrent connection a server can handle by up to a factor of four versus that of a traditional server only web stack and a factor of three versus a recent template caching approach 
to improve the precision at the very top rank of a document list presented in response to a query researcher suggested to exploit information induced from clustering of document highly ranked by some initial search we propose a novel model for ranking such query specific cluster by the presumed percentage of relevant document that they contain the model is based on i proposing a palette of witness cluster property that purportedly correlate with this percentage ii devising concrete quantitative measure for these property and iii ordering the cluster via aggregation of ranking induced by these individual measure empirical evaluation show that our model is consistently more effective than previously suggested method in detecting cluster containing a high relevant document percentage furthermore the precision at top rank performance of this model transcends that of standard document based retrieval and competes with that of a state of the art document based retrieval approach 
under different language context people choose different term or phrase to express their feeling and opinion when a user is writing a paper or chatting with a friend he she applies a specific language model corresponding to the underlying goal this paper present a log based study of analyzing the language model with specific goal we exhibit the statistical information of term and software program propose some method to estimate the divergence of language model with specific user goal and measure the discrimination of these model experimental result show that the language model with different user goal have large divergence and different discrimination these study conclusion can be applied to understand user need and improve human computer interaction hci 
huge amount of search and browse log data ha been accumulated in various search engine such massive search browse log data on the one hand provides great opportunity to mine the wisdom of crowd and improve web search a well a online advertisement on the other hand designing effective and efficient algorithm and tool to clean model and process large scale log data present great challenge in this tutorial we give a systematic survey on the application challenge fundamental principle and state of the art method of mining large scale search and browse log data we start with an introduction of search and browse log data and an overview of various log mining application then we focus on four popular area of log mining application namely query understanding document understanding query document matching and user understanding for each area we review the major task analyze the challenge and exemplify several representative solution finally we discus several new direction in search browse log mining the tutorial slide are available at the author homepage after the tutorial is presented 
we address the problem of clustering the refinement of a user search query the cluster computed by our proposed algorithm can be used to improve the selection and placement of the query suggestion proposed by a search engine and can also serve to summarize the different aspect of information relevant to the original user query our algorithm cluster refinement based on their likely underlying user intent by combining document click and session co occurrence information at it core our algorithm operates by performing multiple random walk on a markov graph that approximates user search behavior a user study performed on top search engine query show that our cluster are rated better than corresponding cluster computed using approach that use only document click or only session co occurrence information 
in this paper we address the problem of database selection for xml document collection that is given a set of collection and a user query how to rank the collection based on their goodness to the query goodness is determined by the relevance of the document in the collection to the query we consider keyword query and support lowest common ancestor lca semantics for defining query result where the relevance of each document to a query is determined by property of the lca of those node in the xml document that contain the query keywords to avoid evaluating query against each document in a collection we propose maintaining in a preprocessing phase information about the lcas of all pair of keywords in a document and use it to approximate the property of the lca based result of a query to improve storage and processing efficiency we use appropriate summary of the lca information based on bloom filter we address both a boolean and a weighted version of the database selection problem our experimental result show that our approach incurs low error in the estimation of the goodness of a collection and provides ranking that are very close to the actual one 
this paper focus on selectivity estimation for sparql graph pattern which is crucial to rdf query optimization the previous work take the join uniformity assumption which would lead to high inaccurate estimation in the case where property in sparql graph pattern are correlated we take into account the dependency among property in sparql graph pattern and propose a more accurate estimation model we first focus on two common sparql graph pattern star and chain pattern and propose to use bayesian network and chain histogram for estimating the selectivityof them then for an arbitrary composite sparql graph pattern we maximally combine the result of the star and chain pattern we have precomputed the experiment show that our method outperforms existing approach in accuracy 
selecting and presenting content culled from multiple heterogeneous and physically distributed source is a challenging task the exponential growth of the web data in modern time ha brought new requirement to such integration system data is not any more produced by content provider alone but also from regular user through the highly popular web social and semantic web application the plethora of the available web content increased it demand by regular user who could not any more wait the development of advanced integration tool they wanted to be able to build in a short time their own specialized integration application aggregator came to the risk of these user they allowed them not only to combine distributed content but also to process it in way that generate new service available for further consumption to cope with the heterogeneous data the linked data initiative aim at the creation and exploitation of correspondence across data value in this work although we share the linked data community vision we advocate that for the modern web linking at the data value level is not enough aggregator should base their integration task on the concept of an entity i e identifying whether different piece of information correspond to the same real world entity such a an event or a person we describe our theory system and experimental result that illustrate the approach s effectiveness 
using a clickstream sample of billion url from many thousand volunteer web user we wish to analyze typical usage of keyword search across the web in order to do this we need to be able to determine whether a given url represents a keyword search and if so which field contains the query although it is easy to recognize q a the query field in http www google com search hl en q music we must do this automatically for the long tail of diverse website this problem is the focus of this paper since the name type and number of field differ across site this doe not conform to traditional text classification or to multi class problem formulation the problem also exhibit highly non uniform importance across website since traffic follows a zipf distribution we developed a solution based on manually identifying the query field on the most popular site followed by an adaptation of machine learning for the rest it involves an interesting case instance structure labeling each website case usually involves selecting at most one of the field instance a positive based on seeing sample field value this problem structure and soft constraint which we believe ha broader applicability can be used to greatly reduce the manual labeling effort we employed active learning and judicious gui presentation to efficiently train a classifier with accuracy estimated at beating several baseline alternative 
resource selection is an important task in federated search to select a small number of most relevant information source current resource selection algorithm such a gloss cori redde geometric average and the recent classification based method focus on the evidence of individual information source to determine the relevance of available source current algorithm do not model the important relationship information among individual source for example an information source tends to be relevant to a user query if it is similar to another source with high probability of being relevant this paper proposes a joint probabilistic classification model for resource selection the model estimate the probability of relevance of information source in a joint manner by considering both the evidence of individual source and their relationship an extensive set of experiment have been conducted on several datasets to demonstrate the advantage of the proposed model 
we discus the use of social network in implementing viral marketing strategy in the first part of this tutorial we study influence maximization or how the structure of the social network affect the spread of behavior and technology in the second part we then consider how one might monopolize these natural process to generate revenue in a revenue maximization setting 
ranking document in response to user information need is a challenging task due in part to the dynamic nature of user interest with respect to a query i hypothesize that the interest of a given user are similar to the interest of the broader community of which he is a part and propose an innovative method that us social medium to characterize the interest of the community and use this characterization to improve future ranking by generating a community interest vector civ for a given query we use community interest to alter the ranking score of individual document retrieved by the query the civ is based on a continuously updated set of recent daily or past few hour user oriented text data the user oriented data can be user blog or user comment tagged news preliminary evaluation show that the new ranking method significantly improves ranking performance 
traditional information retrieval model assume that user express their information need via text query i e their talk in this poster we consider web browsing behavior outside of interaction with retrieval system i e user walk a an alternative source of signal describing user information need and compare it to the query expressed information need on a large dataset our finding demonstrate that information need expressed in different behavior modality are largely non overlapping and that past behavior in each modality is the most accurate predictor of future behavior in that modality result also show that browsing data provides a stronger source of signal than search query due to it greater volume which explains previous work that ha found implicit behavioral data to be a valuable source of information for user modeling and personalization 
in this paper we present a technique for ranking the most important type or category for a given query rather than trying to find the category of the query known a query categorization our approach seek to find the most important type related to the query result not necessarily the query category fall into this ranking of type and therefore our approach can be complementary 
in this paper we present a new web browsing system seamless browser for fast link traversal on a large screen like tv in navigating web user mainly suffer from cognitive overhead of determining whether or not to follow link this overhead can be reduced by providing preview information of the destination of link and also by providing semantic cue on the nearest location in relation to the anchor in order to reduce disorientation and annoyance from the preview information we propose that user will focus on the small area nearside around a pointer and a small number of hyperlink preview in that focused area will appear differently depending on the distance between the pointer and the hyperlink the nearer the distance is the richer the content of the information scent is we also propose that user can navigate the link path by controlling the pointer and the zooming interface so that user may go backward and forward seamlessly along several possible link path we found that combining the pointer and a zoom significantly improved performance for navigational task 
we address the problem of query segmentation given a keyword query submitted to a search engine the task is to group the keywords into phrase if possible previous approach to the problem achieve good segmentation performance on a gold standard but are fairly intricate our method is easy to implement and come with a comparable accuracy 
in this paper we examine user query with respect to diversity providing a mix of result across different interpretation using two query log analysis technique click entropy and reformulated query million query from the microsoft live search log were analysed we found that a broad range of query type may benefit from diversification additionally although there is a correlation between word ambiguity and the need for diversity the range of result user may wish to see for an ambiguous query stretch well beyond traditional notion of word sense 
previous scalability experiment found that early precision improves a collection size increase however that wa under the assumption that a collection s document are all sampled with uniform probability from the same population we contrast this to a large breadth first web crawl an important scenario in real world web search where the early document have quite different characteristic from the later document 
automatic compilation of lexicon is a dream of lexicon compiler a well a lexicon user this paper proposes a system that crawl english japanese person name transliteration from the web which work a back end collector for automatic compilation of bilingual person name lexicon our crawler collected k transliteration in five month from them an english japanese person name lexicon with k entry ha been compiled by an automatic post processing this lexicon is much larger than other similar resource including english japanese lexicon of heiner obtained from wikipedia 
an increasing amount of web information is in video format today s search technology allows video to be found using graphical feature and textual description however the information gleaned from video feature is coarse while textual description are often short and fail to capture the precise content of video we hypothesize that user comment contain supplemental information that effectively describes the content of a video this information once extracted can be applied to a search engine index to improve video search accuracy a complete comment based indexing system must encourage user to post comment and be able to analyze comment data to support research in the relevant area of interface design text analysis and information retrieval we present eduken a flexible web based tool that allows comment collection analysis and search to support the collection of real world comment data in an educational setting eduken includes an additional user interface designed for classroom use 
although most of existing research usually detects event by analyzing the content or structural information of web document a recent direction is to study the usage data in this paper we focus on detecting event from web click through data generated by web search engine we propose a novel approach which effectively detects event from click through data based on robust subspace analysis we first transform click through data to the d polar space next an algorithm based on generalized principal component analysis gpca is used to estimate subspace of transformed data such that each subspace contains query session of similar topic then we prune uninteresting subspace which do not contain query session corresponding to real event by considering both the semantic certainty and the temporal certainty of query session in each subspace finally various event are detected from interesting subspace by utilizing a nonparametric clustering technique compared with existing approach our experimental result based on real life click through data have shown that the proposed approach is more accurate in detecting real event and more effective in determining the number of event 
it is a kind of privacy infraction in personalized web service if the user profile submitted to one web site transferred to another site without user permission that can cause the second web site easily re identify to whom these personal data belong no matter whether the transfer is under control or by hacking this paper present a portable solution for user to bind their sensitive web data under the appointed domain such data including query log user account click stream etc could be used to identify the sensitive information of the particular user by our domain stretching de identification method if personal data leak from domain a to b the web user could still not be identified even though he logins to site under domain b using the same name and password in the experiment implemented by javascript we show the flexibility and efficiency of our de identification approach 
vacation planning is a frequent laborious task which requires skilled interaction with a multitude of resource this paper develops an end to end approach for constructing intra city travel itinerary automatically by tapping a latent source reflecting geo temporal breadcrumb left by million of tourist in particular the popular rich medium sharing site flickr allows photo to be stamped by the date and time of when they were taken and be mapped to point of interest poi by latitude longitude information a well a semantic metadata e g tag that describe them our extensive user study on a crowd sourcing marketplace amazon mechanical turk indicates that high quality itinerary can be automatically constructed from flickr data when compared against popular professionally generated bus tour 
relational autocorrelation is ubiquitous in relational domain this observed correlation between class label of linked instance in a network e g two friend are more likely to share political belief than two randomly selected people can be due to the effect of two different social process if social influence effect are present instance are likely to change their attribute to conform to their neighbor value if homophily effect are present instance are likely to link to other individual with similar attribute value both these effect will result in autocorrelated attribute value when analyzing static relational network it is impossible to determine how much of the observed correlation is due each of these factor however the recent surge of interest in social network ha increased the availability of dynamic network data in this paper we present a randomization technique for temporal network data where the attribute and link change over time given data from two time step we measure the gain in correlation and ass whether a significant portion of this gain is due to influence and or homophily we demonstrate the efficacy of our method on semi synthetic data and then apply the method to a real world social network dataset showing the impact of both influence and homophily effect 
web graph are approximate snapshot of the web created by search engine their creation is an error prone procedure that relies on the availability of internet node and the faultless operation of multiple software and hardware unit checking the validity of a web graph requires a notion of graph similarity web graph similarity help measure the amount and significance of change in consecutive web graph these measurement validate how well search engine acquire content from the web in this paper we study five similarity scheme three of them adapted from existing graph similarity measure and two adapted from well known document and vector similarity method we compare and evaluate all five scheme using a sequence of web graph for yahoo and study if the scheme can identify anomaly that may occur due to hardware or other problem 
aggregating many personal hierarchy into a common taxonomy also known a a folksonomy present several challenge due to it sparseness ambiguity noise and inconsistency we describe an approach to folksonomy learning based on relational clustering that address these challenge by exploiting structured metadata contained in personal hierarchy our approach cluster similar hierarchy using their structure and tag statistic then incrementally weave them into a deeper bushier tree we study folksonomy learning using social metadata extracted from the photo sharing site flickr we evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the open directory project our empirical result suggest that the proposed approach improves upon the state of the art folksonomy learning method 
web technology have enabled more and more people to freely comment on different kind of entity e g seller product service the large scale of information pose the need and challenge of automatic summarization in many case each of the user generated short comment come with an overall rating in this paper we study the problem of generating a rated aspect summary of short comment which is a decomposed view of the overall rating for the major aspect so that a user could gain different perspective towards the target entity we formally define the problem and decompose the solution into three step we demonstrate the effectiveness of our method by using ebay seller feedback comment we also quantitatively evaluate each step of our method and study how well human agree on such a summarization task the proposed method are quite general and can be used to generate rated aspect summary automatically given any collection of short comment each associated with an overall rating 
we present the first interdisciplinary work on transforming a popular problem in proteomics i e protein identification from tandem mass spectrum to an information retrieval ir problem we present an empirical comparison of popular ir approach such a those available from indri and lemur toolkits on benchmark datasets to representative popular baseline in the proteomics literature our experiment demonstrate statistically significant evidence that popular ir approach outperform representative baseline approach in proteomics 
this panel will debate various approach to improving video search and explore how professional cataloguing crowd sourced metadata and improvement in search algorithm will evolve over the next ten year panelist will explore the need of large scale video archive and compare these against the current capability of video search 
there are many online system where million of user post original content such a video review of item such a product service and business etc while there are general rule for good behavior or even formal term of service there are still user who post content that is not suitable increasingly online system rely on other user who view the posted content to provide feedback we study online system where user report negative feedback i e report abuse these system are quite distinct from much studied traditional reputation system that focus on eliciting popularity of content by various voting method the central problem that we study here is how to monitor the quality of negative feedback that is detect negative feedback which is incorrect or perhaps even malicious system address this problem by testing flag manually which is an expensive operation a a result there is a tradeoff between the number of manual test and the number of error defined a the number of incorrect flag the monitoring system miss our contribution are a follows we initiate a systematic study of negative feedback system our framework is general enough to be applicable for a variety of system in this framework the number of error the system admits is bounded over the worst case of adversarial user while simultaneously the system performs only small amount of manual testing for multitude of standard user who might still err while reporting our main contribution is a randomized monitoring algorithm that we call adaptive probabilistic testing apt that is simple to implement and ha guarantee on expected number of error even for adversarial user the total expected error is bounded by n over n flag for a given e finally we present empirical study of our algorithm that show it performance on both synthetic data and real data accumulated from a variety of negative feedback system at google our study indicates that the algorithm performs better than the analysis above show 
long query frequently contain many extraneous term that hinder retrieval of relevant document we present technique to reduce long query to more effective shorter one that lack those extraneous term our work is motivated by the observation that perfectly reducing long trec description query can lead to an average improvement of in mean average precision our approach involves transforming the reduction problem into a problem of learning to rank all sub set of the original query sub query based on their predicted quality and selecting the top sub query we use various measure of query quality described in the literature a feature to represent sub query and train a classifier replacing the original long query with the top ranked sub query chosen by the ranker result in a statistically significant average improvement of on our test set analysis of the result show that query reduction is well suited for moderately performing long query and a small set of query quality predictor are well suited for the task of ranking sub query 
the boom of product review website blog and forum on the web ha attracted many research effort on opinion mining recently there wa a growing interest in the finer grained opinion mining which detects opinion on different review feature a opposed to the whole review level the research on feature level opinion mining mainly rely on identifying the explicit relatedness between product feature word and opinion word in review however the sentiment relatedness between the two object is usually complicated for many case product feature word are implied by the opinion word in review the detection of such hidden sentiment association is still a big challenge in opinion mining especially it is an even harder task of feature level opinion mining on chinese review due to the nature of chinese language in this paper we propose a novel mutual reinforcement approach to deal with the feature level opinion mining problem more specially the approach cluster product feature and opinion word simultaneously and iteratively by fusing both their content information and sentiment link information under the same framework based on the product feature category and opinion word group we construct the sentiment association set between the two group of data object by identifying their strongest n sentiment link moreover knowledge from multi source is incorporated to enhance clustering in the procedure based on the pre constructed association set our approach can largely predict opinion relating to different product feature even for the case without the explicit appearance of product feature word in review thus it provides a more accurate opinion evaluation the experimental result demonstrate that our method outperforms the state of art algorithm 
sentence ranking is the issue of most concern in document summarization early researcher have presented the mutual reinforcement principle mr between sentence and term for simultaneous key phrase and salient sentence extraction in generic single document summarization in this work we extend the mr to the mutual reinforcement chain mrc of three different text granularity i e document sentence and term the aim is to provide a general reinforcement framework and a formal mathematical modeling for the mrc going one step further we incorporate the query influence into the mrc to cope with the need for query oriented multi document summarization while the previous summarization approach often calculate the similarity regardless of the query we develop a query sensitive similarity to measure the affinity between the pair of text when evaluated on the duc dataset the experimental result suggest that the proposed query sensitive mrc q mrc is a promising approach for summarization 
online collaboration and sharing is the central theme of many web based service that create the so called web phenomenon using the internet a a computing platform many web application set up mirror site to provide large scale availability and to achieve load balance however in the age of web where every user is also a writer and publisher the deployment of mirror site make consistency maintenance a web scale problem traditional concurrency control method e g two phase lock serialization etc are not up to the task for several reason first large network latency between mirror site will make two phase locking a throughput bottleneck second locking will block a large portion of concurrent operation which make it impossible to provide large scale availability on the other hand most web operation do not need strict serializability it is not the intention of a user who is correcting a typo in a shared document to block another who is adding a comment a long a consistency can still be achieved thus in order to enable maximal online collaboration and sharing we need a lock free mechanism that can maintain consistency among mirror site on the web in this paper we propose a flexible and efficient method to achieve consistency maintenance in the web world our experiment show it good performance improvement compared with existing method based on distributed lock 
the use of blog to track and comment on real world political news entertainment event is growing similarly a more individual start relying on the web a their primary information source and a more traditional medium outlet try reaching consumer through alternative venue the number of news site on the web is also continuously increasing content reuse whether in the form of extensive quotation or content borrowing across medium outlet is very common in blog and news entry outlet tracking the same real world event knowledge about which web entry re use content from which others can be an effective asset when organizing these entry for presentation on the other hand this knowledge is not cheap to acquire considering the size of the related space web entry it is essential that the technique developed for identifying re use are fast and scalable furthermore the dynamic nature of blog and news entry necessitates incremental processing for reuse detection in this paper we develop a novel qsign algorithm that efficiently and effectively analyze the blogosphere for quotation and reuse identification experiment result show that with qsign processing time gain from x to x are possible while maintaining reuse detection rate of upto furthermore processing time gain can be pushed multiple order of magnitude from x to x for recall 
we demonstrate a flight meta search engine that is based on the metamorph framework metamorph provides mechanism to model web form together with the interaction which are needed to fulfil a request and can generate interaction sequence that pose query using these web form and collect the result in this paper we discus an interesting new feature that make use of the form themselves a an information source we show how data can be extracted from web form rather than the data behind web form to generate a graph of flight connection between city the flight connection graph allows u to vastly reduce the number of query that the engine sends to airline website in the most interesting search scenario those that involve the controversial practice of creative ticketing in which agency attempt to find lower price fare by using more than one airline for a journey we describe a system which attains data from a number of website to identify promising route and prune the search tree heuristic that make use of geographical information and an estimation of cost based on historical data are employed the result are then made available to improve the quality of future search request 
the phenomenon of sponsored search advertising is gaining ground a the largest source of revenue for search engine firm across different industry have are beginning to adopt this a the primary form of online advertising this process work on an auction mechanism in which advertiser bid for different keywords and final rank for a given keyword is allocated by the search engine but how different are firm s actual bid from their optimal bid moreover what are other way in which firm can potentially benefit from sponsored search advertising based on the model and estimate from prior work we conduct a number of policy simulation in order to investigate to what extent an advertiser can benefit from bidding optimally for it keywords further we build a hierarchical bayesian modeling framework to explore the potential for cross selling or spillover effect from a given keyword advertisement across multiple product category and estimate the model using markov chain monte carlo mcmc method our analysis suggests that advertiser are not bidding optimally with respect to maximizing profit we conduct a detailed analysis with product level variable to explore the extent of cross selling opportunity across different category from a given keyword advertisement we find that there exists significant potential for cross selling through search keyword advertisement in that consumer often end up buying product from other category in addition to the product they were searching for latency the time it take for consumer to place a purchase order after clicking on the advertisement and the presence of a brand name in the keyword are associated with consumer spending on product category that are different from the one they were originally searching for on the internet 
previously we postulated the advantage of using entity extraction to implement a new peer to peer p p search framework for reducing network traffic and providing a trade off between precision and recall we now propose an entity ranking method designed for the short document characteristic of p p which significantly improves both precision and recall in top result p p search we construct a dynamic entity corpus using n gram statistic and metadata study it reliability and use it to identify correlation between user query term 
the world wide web consists not only of a huge number of unstructured text but also a vast amount of valuable structured data web table are a typical type of structured information that are pervasive on the web and web scale method that automatically extract web table have been studied extensively many powerful system e g octopus mesa use extracted web table a a fundamental component in the database vernacular a table is defined a a set of tuples which have the same attribute similarly a web table is defined a a set of row corresponding to database tuples which have the same column header corresponding to database attribute therefore to extract a web table is to extract a relation on the web in database table often contain foreign key which refer to other table therefore it follows that hyperlink inside a web table sometimes function a foreign key to other relation whose tuples are contained in the hyperlink s target page in this paper we explore this idea by asking can we discover new attribute for web table by exploring hyperlink inside web table this poster proposes a solution that take a web table a input frequent pattern are generated a new candidate relation by following hyperlink in the web table the confidence of candidate are evaluated and trustworthy candidate are selected to become new attribute for the table finally we show the usefulness of our method by performing experiment on a variety of web domain 
we discus the concept of relevance criterion in the context of e commerce search a vast body of research literature describes the beyond topical criterion used to determine the relevance of the document to the need we argue that in an e commerce scenario there are some difference and novel and different criterion can be used to determine relevance we experimentally validate this hypothesis by mean of amazon mechanical turk using a crowdsourcing approach 
kleio is an advanced information retrieval ir system developed at the uk national centre for text mining nactem the system offer textual and metadata search across medline and provides enhanced searching functionality by leveraging terminology management technology 
a number of online video social network out of which youtube is the most popular provides feature that allow user to post a video a a response to a discussion topic these feature open opportunity for user to introduce polluted content or simply pollution into the system for instance spammer may post an unrelated video a response to a popular one aiming at increasing the likelihood of the response being viewed by a larger number of user moreover opportunistic user promoter may try to gain visibility to a specific video by posting a large number of potentially unrelated response to boost the rank of the responded video making it appear in the top list maintained by the system content pollution may jeopardize the trust of user on the system thus compromising it success in promoting social interaction in spite of that the available literature is very limited in providing a deep understanding of this problem in this paper we go a step further by addressing the issue of detecting video spammer and promoter towards that end we manually build a test collection of real youtube user classifying them a spammer promoter and legitimates using our test collection we provide a characterization of social and content attribute that may help distinguish each user class we also investigate the feasibility of using a state of the art supervised classification algorithm to detect spammer and promoter and ass it effectiveness in our test collection we found that our approach is able to correctly identify the majority of the promoter misclassifying only a small percentage of legitimate user in contrast although we are able to detect a significant fraction of spammer they showed to be much harder to distinguish from legitimate user 
we study the problem of anonymizing user profile so that user privacy is sufficiently protected while the anonymized profile are still effective in enabling personalized web search we propose a bayes optimal privacy notion to bound the prior and posterior probability of associating a user with an individual term in the anonymized user profile set we also propose a novel bundling technique that cluster user profile into group by taking into account the semantic relationship between the term while satisfying the privacy constraint we evaluate our approach through a set of preliminary experiment using real data demonstrating it feasibility and effectiveness 
evaluation forum such a trec allow systematic measurement and comparison of information retrieval technique the goal is consistent improvement based on reliable comparison of the effectiveness of different approach and system in this paper we report experiment to determine whether this goal ha been achieved we ran five publicly available search system in a total of seventeen different configuration against nine trec adhoc style collection spanning to these runsets were then used a a benchmark for reassessing the relative effectiveness of the original trec run for those collection surprisingly there appears to have been no overall improvement in effectiveness for either median or top end trec submission even after allowing for several possible confounds we therefore question whether the effectiveness of adhoc information retrieval ha improved over the past decade and a half 
in this paper we propose a non greedy active learning method for text categorization using least square support vector machine lssvm our work is based on transductive experimental design ted an active learning formulation that effectively explores the information of unlabeled data despite it appealing property the optimization problem is however np hard and thus like most of other active learning method a greedy sequential strategy to select one data example after another wa suggested to find a suboptimum in this paper we formulate the problem into a continuous optimization problem and prove it convexity meaning that a set of data example can be selected with a guarantee of global optimum we also develop an iterative algorithm to efficiently solve the optimization problem which turn out to be very easy to implement our text categorization experiment on two text corpus empirically demonstrated that the new active learning algorithm outperforms the sequential greedy algorithm and is promising for active text categorization application 
user searching for information in a digital library or on the www can be modeled a individual moving through a semantic space by issuing query and clicking on hyperlink a they go they emit a stream of interaction data most of it is linguistic data lot of it is captured in log some of it is used to guess what the user is searching for but to most information retrieval system each user interaction is a stateless point in this space there is a timeline connecting each of these point but system seldom make use of this a sequence data in part because there is no clear way to systematically characterize the meaningful relation within a sequence of user activity it is a problem of pragmatic a much a it is of semantics the fact that a user clicked on a particular link or added a particular term to their query ha meaning primarily in relation to the preceding action a remaining challenge in ir is to extract feature of the user interaction data that will give meaning to those relation meanwhile from the user s perspective each of these point in time and semantic space are just part of a path of exploration to the user the exact term in a query or the specific word surrounding a hypertext link may be le important than the trajectory those term establish in relation to the user s path identifying the meaningful relation between query and page view within a sequence of activity increase our understanding of user and their information need formally we can model query and browsing behavior a surface form of a hidden process what is missing is a layer of abstraction for mapping sequence of interaction in a way that is both descriptive of user need and useful to automation the work i describe is an effort to identify feature of data in log of query and browsing activity that are highly predictive of certain type of behavior sequence of interaction data from individual user are modeled a sequence of expression statistical modeling technique that are effective for modeling sequence in natural language processing and bioinformatics are examined for their ability to model sequence of interaction between an information searcher and an information retrieval system query and click throughs in this stream of interaction can be tagged with feature such a semantic coordinate timing frequency of use type of action etc by analyzing large collection of interaction sequence it is possible to identify frequent pattern of user behavior from these pattern we can make prediction about future interaction for example certain pattern of link following in a digital library are highly predictive of user next step while other pattern are not general model of user interaction are useful for design and evaluation of search interface individual model of user interaction are useful for personalized search and customized content yet very little research ha been done to investigate which feature are optimal for modeling user query and browsing a interaction sequence an important first step is to identify informative feature and the relationship between feature i propose to construct model of user behavior based on user data in log of query and browsing activity and to identify feature that are highly predictive of certain type of user behavior i examine activity within search session on a digital library a a microcosm of larger system i expect to find feature that are useful in predictive model of user behavior both at an individual and aggregate level where possible i hope to identify meaningful relationship between those feature the work ha implication beyond the scope of digital library to larger system and broader search domain 
the sharing and control of information on the web need to be balanced to provide the web community with the best experience and outcome the policy aware web is an emerging direction that may hold the key in getting this balance right by allowing future policy language to maintain this harmonization this panel will discus highlight and debate the challenge in moving towards the policy aware web and the path it could lead the target audience includes all web stakeholder a most policy such a privacy and right are intrinsic to all web user information service and content 
peer service depend on one another to accomplish their task and their structure may evolve a service composition may be designed to replace it member service whenever the quality of the composite service fails to meet certain quality of service qos requirement finding service and service invocation endpoint having the greatest impact on the quality are important to guide subsequent service adaptation this paper proposes a technique that sample the qos of composite service and continually analyzes them to identify artifact for service adaptation the preliminary result show that our technique ha the potential to effectively find such artifact in service 
in this paper we present an approach of generating cascading style sheet document automatically if the desired effect on the content element is specified while a web user agent resolve the cs rule and computes their effect our approach handle the way back we argue that this can remarkably improve cs productivity since the process of cs authoring always involves this direction implicitly our approach claim a new and innovative way to reuse chunk of markup together with it presentation it furthermore bear potential for the optimization and reorganization of cs document we describe criterion for cs code quality we oriented on including a quantitative indicator for the abstractness of a cs presentation specification an evaluation and recomputation of the cs for html document show that concerning these criterion the automatically generated code come close to manually authored code 
proximity of query term in a document is an important criterion in ir however no investigation ha been made to determine the most useful term sequence for which proximity should be considered in this study we test the effectiveness of using proximity of partial term sequence n gram for web search we observe that the proximity of sequence of to term is most effective for long query while shorter or longer sequence appear le useful this suggests that combination of to term can best capture the intention in user query in addition we also experiment with weighing the importance of query sub sequence using query log frequency our preliminary test show promising empirical result 
in this paper we present a two step language independent spelling suggestion system in the first step candidate suggestion are generated using an information retrieval ir approach in step two candidate suggestion are re ranked using a new string similarity measure that us the length of the longest common substring occurring at the beginning and end of the word we obtained very impressive result by reranking candidate suggestion using the new similarity measure the accuracy of first suggestion is and for dutch danish and bulgarian language datasets respectively 
click data capture many user document preference for a query and ha been shown to help significantly improve search engine ranking however most click data is noisy and of low frequency with query associated to document via only one or a few click this severely limit the usefulness of click data a a ranking signal given potentially noisy click comprising result with at most one click for a query how do we extract high quality click that may be useful for ranking in this poster we introduce a technique based on query entropy for noise reduction in click data we study the effect of query entropy and a well a feature such a user engagement and the match between the query and the document based on query entropy plus other feature we can sample noisy data to of it overall size with query recall and an average increase of in precision for recalled query 
the approach of using passage level evidence for document retrieval ha shown mixed result when it is applied to a variety of test bed with different characteristic one main reason of the inconsistent performance is that there exists no unified framework to model the evidence of individual passage within a document this paper proposes two probabilistic model to formally model the evidence of a set of top ranked passage in a document the first probabilistic model follows the retrieval criterion that a document is relevant if any passage in the document is relevant and model each passage independently the second probabilistic model go a step further and incorporates the similarity correlation among the passage both model are trained in a discriminative manner furthermore we present a combination approach to combine the ranked list of document retrieval and passage based retrieval an extensive set of experiment have been conducted on four different trec test bed to show the effectiveness of the proposed discriminative probabilistic model for passage based retrieval the proposed algorithm are compared with a state of the art document retrieval algorithm and a language model approach for passage based retrieval furthermore our combined approach ha been shown to provide better result than both document retrieval and passage based retrieval approach 
this paper investigates whether web comment are of descriptive nature that is whether the combined text of a set of comment is similar in topic to the commented object if so comment may be used in place of the respective object in all kind of cross medium retrieval task our experiment reveal that comment on textual object are indeed descriptive comment suffice to expect a high similarity between the comment and the commented text comment suffice to replace the commented text in a ranking task and to measure the contribution of the commenters beyond the commented text 
in many case rather than a keyword search people intend to see what is going on through the internet then the integrated comprehensive information on news topic is necessary which we called news issue including the background history current progress different opinion and discussion etc traditionally news issue are manually generated by website editor it is quite a time consuming hard work and hence real time update is difficult to perform in this paper a three step automatic online algorithm for news issue construction is proposed the first step is a topic detection process in which newly appearing story are clustered into new topic candidate the second step is a topic tracking process where those candidate are compared with previous topic either merged into old one or generating a new one in the final step news issue are constructed by the combination of related topic and updated by the insertion of new topic an automatic online news issue construction process under practical web circumstance is simulated to perform news issue construction experiment f measure of the best result is either above topic detection or close to topic detection and tracking four news issue construction result are successfully generated in different time granularity one meet the need like what s new and the other three will answer question like what s hot or what s going on through the proposed algorithm news issue can be effectively and automatically constructed with real time update and lot of human effort will be released from tedious manual work 
the world wide web ha many document repository that can act a valuable source of additional data for various machine learning task in this paper we propose a method of improving text classification accuracy by using such an additional corpus that can easily be obtained from the web this additional corpus can be unlabeled and independent of the given classification task the method proposed here us topic modeling to extract a set of topic from the additional corpus those extracted topic then act a additional feature of the data of the given classification task an evaluation on the rcv dataset show significant improvement over a baseline method 
in this paper we begin to investigate how to automatically determine the subjectivity orientation of question posted by real user in community question answering cqa portal subjective question seek answer containing private state such a personal opinion and experience in contrast objective question request objective verifiable information often with support from reliable source knowing the question orientation would be helpful not only for evaluating answer provided by user but also for guiding the cqa engine to process question more intelligently our experiment on yahoo answer data show that our method exhibit promising performance 
traditional ad hoc retrieval model do not take into account the closeness or proximity of term document score in these model are primarily based on the occurrence or non occurrence of query term considered independently of each other intuitively document in which query term occur closer together should be ranked higher than document in which the query term appear far apart this paper outline several term term proximity measure and develops an intuitive framework in which they can be used to fully model the proximity of all query term for a particular topic a useful proximity function may be constructed from many proximity measure we use a learning approach to combine proximity measure to develop a useful proximity function in the framework an evaluation of the best proximity function show that there is a significant improvement over the baseline ad hoc retrieval model and over other more recent method that employ the use of single proximity measure 
in this paper we propose a bayesian learning approach to promoting diversity for information retrieval in biomedicine and a re ranking model to improve retrieval performance in the biomedical domain first the re ranking model computes the maximum posterior probability of the hidden property corresponding to each retrieved passage then it iteratively group the passage into subset according to their property finally these passage are re ranked from the subset a our output there is no need for our proposed method to use any external biomedical resource we evaluate our bayesian learning approach by conducting extensive experiment on the trec genomics data set the experimental result show the effectiveness of the proposed bayesian learning approach for promoting diversity in ranking for biomedical information retrieval on four year trec data set 
many reputation management system have been developed under the assumption that each entity in the system will use a variant of the same scoring function much of the previous work in reputation management ha focused on providing robustness and improving performance for a given reputation scheme in this paper we present a reputation based trust management framework that support the synthesis of trust related feedback from many different entity while also providing each entity with the flexibility to apply different scoring function over the same feedback data for customized trust evaluation we also propose a novel scheme to cache trust value based on recent client activity to evaluate our approach we implemented our trust management service and tested it on a realistic application scenario in both lan and wan distributed environment our result indicate that our trust management service can effectively support multiple scoring function with low overhead and high availability 
information retrieval effectiveness is usually evaluated using measure such a normalized discounted cumulative gain ndcg mean average precision map and precision at some cutoff precision k on a set of judged query recent research ha suggested an alternative evaluating information retrieval system based on user behavior particularly promising are experiment that interleave two ranking and track user click according to a recent study interleaving experiment can identify large difference in retrieval effectiveness with much better reliability than other click based method we study interleaving in more detail comparing it with traditional measure in term of reliability sensitivity and agreement to detect very small difference in retrieval effectiveness a reliable outcome with standard metric requires about judged query and this is about a reliable a interleaving with user impression amongst the traditional measure ndcg ha the strongest correlation with interleaving finally we present some new form of analysis including an approach to enhance interleaving sensitivity 
searching for prior art patent is an essential step for the patent examiner to validate or invalidate a patent application in this paper we consider the whole patent a the query which reduces the burden on the user and also make many more potential search feature available we explore how to automatically transform the query patent into an effective search query especially focusing on the effect of different patent field experiment show that the background summary of a patent is the most useful source of term for generating a query even though most previous work used the patent claim 
the standard layout model used by web browser is to lay text out in a vertical scroll using a single column the horizontal scroll layout model in which text is laid out in column whose height is set to that of the browser window and the viewer scroll horizontally seems well suited to multi column layout on electronic device we describe a study that examines how people read and in particular the strategy they use for scrolling with these two model when reading large textual document on a standard computer monitor we compare usability of the model and evaluate both user preference and the effect of the model on performance also interesting is the description of the browser and it user interface which we used for the study 
comment left by reader on web document contain valuable information that can be utilized in different information retrieval task including document search visualization and summarization in this paper we study the problem of comment oriented document summarization and aim to summarize a web document e g a blog post by considering not only it content but also the comment left by it reader we identify three relation namely topic quotation and mention by which comment can be linked to one another and model the relation in three graph the importance of each comment is then scored by i graph based method where the three graph are merged into a multi relation graph ii tensor based method where the three graph are used to construct a rd order tensor to generate a comment oriented summary we extract sentence from the given web document using either feature biased approach or uniform document approach the former score sentence to bias keywords derived from comment while the latter score sentence uniformly with comment in our experiment using a set of blog post with manually labeled sentence our proposed summarization method utilizing comment showed significant improvement over those not using comment the method using feature biased sentence extraction approach were observed to outperform that using uniform document approach 
realtime web search refers to the retrieval of very fresh content which is in high demand an effective portal web search engine must support a variety of search need including realtime web search however supporting realtime web search introduces two challenge not encountered in non realtime web search quickly crawling relevant content and ranking document with impoverished link and click information in this paper we advocate the use of realtime micro blogging data for addressing both of these problem we propose a method to use the micro blogging data stream to detect fresh url we also use micro blogging data to compute novel and effective feature for ranking fresh url we demonstrate these method improve effective of the portal web search engine for realtime web search 
this panel discus how polling in the httpd protocol affect how we are building the next generation of the web and it application a other technology html javascript etc move forward we ask should the web s protocol also evolve or is it sufficient for the web to continue through just get and post 
in this poster paper we propose a novel approach to improve web search relevancy by tokenizing a vietnamese query text prior submitting it to a search engine evaluation demonstrate it effectiveness and practical value 
a number of project are dedicated to creating digital library from scanned book such a google book udl digital library of india dli etc the ability to search in the content of document image is essential for the usability and popularity of these dl in this work we aim toward building a retrieval system over k document image coming from scanned book of telugu literature this is a challenge because i ocrs are not robust enough for indian language especially the telugu script ii the document image contain large number of degradation and artifact iii scalability to large collection is hard moreover user expect that the search system accept text query and retrieve relevant result in interactive time we propose a reverse annotation framework that label word image by their equivalent text label in the offline phase reverse annotation applies a retrieval based approach to recognition unlike traditional annotation recognition that identifies keywords for data reverse annotation identifies data that corresponds to a given keyword it first selects a set of keywords which are considered useful for labeling and retrieval such a those that repeat often and ignoring stopwords and rare word exemplar are obtained for each word from a crude ocr or human annotation the label are then propagated across the rest of the collection by matching word in the image feature space since such a matching is computationally expensive scalability is achieved using a fast approximate nearest neighbor technique based on hierarchical k mean once text label are assigned each document image is considered a bag of word over the labeled keywords a standard search engine is used to build a search index for quick online retrieval an example query and the retrieved result are shown in figure we are unaware of any conventional ocrs which can retrieve such image for the given query there are three major contribution of our work i recognizing the entire document collection together instead of one at a time this mean that the repetition of word in the test set is effectively used for improving accuracy ii speeding up recognition by clustering multiple instance of a given word iii recognising at the word level avoiding the pitfall of character segmentation and recognition other ocr technique that use word level context still rely on inaccurate component level classification using the technique developed from this work we were able to successfully build a retrieval system over our challenging dataset to the best of our knowledge this is the largest collection of document image that ha been made searchable for any indian language our algorithm is easily scalable to larger collection and directly applicable to document from other language script the first issue to discus is the fraction of word image that remain unrecognized at the end of the reverse annotation phase rare word noun etc are not labeled in the test set it is important to estimate the cost of not being able to answer such query if this cost is indeed high we need to explore method to label such infrequently occurring word in the collection needle to say such method should be computationally efficient without compromising on accuracy the other major issue to discus is the evaluation of retrieval result the true recall of the retrieval system cannot be computed since it is impossible to identify every occurrence of the given query in such large data question to be considered include whether precision alone is a sufficient indicator of retrieval performance whether there is some better document level effectiveness assessment possible and how best to estimate the relative satisfaction of the user s information need 
traditional naive bayes classifier performs miserably on web scale taxonomy in this paper we investigate the reason behind such bad performance we discover that the low performance are not completely caused by the intrinsic limitation of naive bayes but mainly come from two largely ignored problem contradiction pair problem and discriminative evidence cancelation problem we propose modification that can alleviate the two problem while preserving the advantage of naive bayes the experimental result show our modified naive bayes can significantly improve the performance on real web scale taxonomy 
recently an inductive approach to modelling term weighting function correctness ha provided a number of axiom constraint to which all good term weighting function should adhere these constraint have been shown to be theoretically and empirically sound in a number of work it ha been shown that when a term weighting function break one or more of the constraint it typically indicates sub optimality of that function this elegant inductive approach may more accurately model the human process of determining the relevance a document it is intuitive that a person s notion of relevance change a term that are either on or off topic are encountered in a given document ultimately it would be desirable to be able to mathematically determine the performance of term weighting function without the need for test collection many modern term weighting function do not satisfy the constraint in an unconditional manner however the degree to which these function violate the constraint ha not been investigated a comparison between weighting function from this perspective may shed light on the poor performance of certain function in certain setting moreover if a correlation exists between performance and the number of violation measuring the degree of violation could help more accurately predict how a certain scheme will perform on a given collection 
this paper address the problem of identifying redundant data in large scale service oriented information system specifically the paper put forward an automated method to pinpoint potentially redundant data attribute from a given collection of semantically annotated web service interface the key idea is to construct a service network to represent all input and output dependency between data attribute and operation captured in the service interface and to apply centrality measure from network theory in order to quantify the degree to which an attribute belongs to a given subsystem the proposed method wa tested on a federated governmental information system consisting of independently maintained information system providing altogether about service operation described in wsdl the accuracy of the method is evaluated in term of precision and recall 
this poster briefly describes a practical system named founderwise for harvesting and monitoring hot topic on the web founderwise consists of five component web crawler text classifier topic detector topic summarizer and topic analyzer in this poster we present two key component of topic detector and topic analyzer the system ha been successfully deployed in a few chinese major government department 
effective learning in multi label classification mlc requires an appropriate level of abstraction for representing the relationship between each instance and multiple category current mlc method have been focused on learning to map from instance to ranked list of category in a relatively high dimensional space the fine grained feature in such a space may not be sufficiently expressive for characterizing discriminative pattern and worse make the model complexity unnecessarily high this paper proposes an alternative approach by transforming conventional representation of instance and category into a relatively small set of link based meta level feature and leveraging successful learning to rank retrieval algorithm e g svm map over this reduced feature space controlled experiment on multiple benchmark datasets show strong empirical evidence for the strength of the proposed approach a it significantly outperformed several state of the art method including rank svm ml knn and iblr ml instance based logistic regression for multi label classification in most case 
in this paper we present a new document representation model based on implicit user feedback obtained from search engine query the main objective of this model is to achieve better result in non supervised task such a clustering and labeling through the incorporation of usage data obtained from search engine query this type of model allows u to discover the motivation of user when visiting a certain document the term used in query can provide a better choice of feature from the user s point of view for summarizing the web page that were clicked from these query in this work we extend and formalize a query model an existing but not very well known idea of query view for document representation furthermore we create a novel model based on frequent query pattern called the query set model our evaluation show that both query based model outperform the vector space model when used for clustering and labeling document in a website in our experiment the query set model reduces by more than the number of feature needed to represent a set of document and improves by over the quality of the result we believe that this can be explained because our model chooses better feature and provides more accurate label according to the user s expectation 
web search engine are facing formidable performance challenge due to data size and query load the major engine have to process ten of thousand of query per second over ten of billion of document to deal with this heavy workload such engine employ massively parallel system consisting of thousand of machine the significant cost of operating these system ha motivated a lot of recent research into more efficient query processing mechanism we investigate a new way to build such high performance ir system using graphical processing unit gpus gpus were originally designed to accelerate computer graphic application through massive on chip parallelism recently a number of researcher have studied how to use gpus for other problem domain such a database and scientific computing our contribution here is to design a basic system architecture for gpu based high performance ir to develop suitable algorithm for subtasks such a inverted list compression list intersection and top k scoring and to show how to achieve highly efficient query processing on gpu based system our experimental result for a prototype gpu based system on million web page indicate that significant gain in query processing performance can be obtained 
web search engine are facing formidable performance challenge due to data size and query load the major engine have to process ten of thousand of query per second over ten of billion of document to deal with this heavy workload such engine employ massively parallel system consisting of thousand of machine the significant cost of operating these system ha motivated a lot of recent research into more efficient query processing mechanism we investigate a new way to build such high performance ir system using graphical processing unit gpus gpus were originally designed to accelerate computer graphic application through massive on chip parallelism recently a number of researcher have studied how to use gpus for other problem domain such a database and scientific computing our contribution here is to design a basic system architecture for gpu based high performance ir to develop suitable algorithm for subtasks such a inverted list compression list intersection and top k scoring and to show how to achieve highly efficient query processing on gpubased system our experimental result for a prototype gpu based system on million web page show promising gain in query throughput 
over the year the notion of concept relatedness ha attracted considerable attention a variety of approach based on ontology structure information content association or context have been proposed to indicate the relatedness of abstract idea we propose a method based on the cross entropy reduction between language model of concept which are estimated based on document concept assignment the approach show improved or competitive result compared to state of the art method on two test set in the biomedical domain 
effective access to and navigation in information stored in deep web ontological repository or relational database ha yet to be realized due to issue with usability of user interface and the overall scope and complexity of information a well a the nature of exploratory user task we propose the integration and adaptation of novel navigation and visualization approach to faceted browsing such a visual depiction of facet and restriction visual navigation in cluster of search result and graph like exploration of individual search result property 
given a terabyte click log can we build an efficient and effective click model it is commonly believed that web search click log are a gold mine for search business because they reflect user preference over web document presented by the search engine click model provide a principled approach to inferring user perceived relevance of web document which can be leveraged in numerous application in search business due to the huge volume of click data scalability is a must we present the click chain model ccm which is based on a solid bayesian framework it is both scalable and incremental perfectly meeting the computational challenge imposed by the voluminous click log that constantly grow we conduct an extensive experimental study on a data set containing million query session obtained in july from a commercial search engine ccm consistently outperforms two state of the art competitor in a number of metric with over better log likelihood over better click perplexity and much more robust up to prediction of the first and the last clicked position 
twitter a microblogging service le than three year old command more than million user a of july and is growing fast twitter user tweet about any topic within the character limit and follow others to receive their tweet the goal of this paper is to study the topological characteristic of twitter and it power a a new medium of information sharing we have crawled the entire twitter site and obtained million user profile billion social relation trending topic and million tweet in it follower following topology analysis we have found a non power law follower distribution a short effective diameter and low reciprocity which all mark a deviation from known characteristic of human social network in order to identify influentials on twitter we have ranked user by the number of follower and by pagerank and found two ranking to be similar ranking by retweets differs from the previous two ranking indicating a gap in influence inferred from the number of follower and that from the popularity of one s tweet we have analyzed the tweet of top trending topic and reported on their temporal behavior and user participation we have classified the trending topic based on the active period and the tweet and show that the majority over of topic are headline news or persistent news in nature a closer look at retweets reveals that any retweeted tweet is to reach an average of user no matter what the number of follower is of the original tweet once retweeted a tweet get retweeted almost instantly on next hop signifying fast diffusion of information after the st retweet to the best of our knowledge this work is the first quantitative study on the entire twittersphere and information diffusion on it 
we propose a new multi view clustering method which us clustering result obtained on each view a a voting pattern in order to construct a new set of multi view cluster our experiment on a multilingual corpus of document show that performance increase significantly over simple concatenation and another multi view clustering technique 
in this paper we present a novel framework for extracting the ratable aspect of object from online user review extracting such aspect is an important challenge in automatically mining product opinion from the web and in generating opinion based summary of user review our model are based on extension to standard topic modeling method such a lda and plsa to induce multi grain topic we argue that multi grain model are more appropriate for our task since standard model tend to produce topic that correspond to global property of object e g the brand of a product type rather than the aspect of an object that tend to be rated by a user the model we present not only extract ratable aspect but also cluster them into coherent topic e g waitress and bartender are part of the same topic staff for restaurant this differentiates it from much of the previous work which extract aspect through term frequency analysis with minimal clustering we evaluate the multi grain model both qualitatively and quantitatively to show that they improve significantly upon standard topic model 
clustering hypertext document collection is an important task in information retrieval most clustering method are based on document content and do not take into account the hyper text link here we propose a novel pagerank based clustering prc algorithm which us the hypertext structure the prc algorithm produce graph partitioning with high modularity and coverage the comparison of the prc algorithm with two content based clustering algorithm show that there is a good match between prc clustering and content based clustering 
we present content extraction via tag ratio cetr a method to extract content text from diverse webpage by using the html document s tag ratio we describe how to compute tag ratio on a line by line basis and then cluster the resulting histogram into content and non content area initially we find that the tag ratio histogram is not easily clustered because of it one dimensionality therefore we extend the original approach in order to model the data in two dimension next we present a tailored clustering technique which operates on the two dimensional model and then evaluate our approach against a large set of alternative method using standard accuracy precision and recall metric on a large and varied web corpus finally we show that in most case cetr achieves better content extraction performance than existing method especially across varying web domain language and style 
e commerce is growing at an exponential rate in the last decade there ha been an explosion of online commercial activity enabled by world wide web www these day many consumer are le attracted to online auction preferring to buy merchandise quickly using fixed price negotiation sale at amazon com the leader in online sale of fixed price good rose in the first quarter of at ebay where auction make up of the site s sale revenue rose in brazil probably by cultural influence online auction are not been popular this work present a characterization and analysis of fixed price online negotiation using actual data from a brazilian marketplace we analyze seller practice considering seller profile and strategy we show that different seller adopt strategy according to their interest ability and experience moreover we confirm that choosing a selling strategy is not simple since it is important to consider the seller s characteristic to evaluate the applicability of a strategy the work also provides a comparative analysis of some selling practice in brazil with popular worldwide marketplace 
the goal of predicting query potential for personalization is to determine which query can benefit from personalization in this paper we investigate which kind of strategy is better for this task classification or regression we quantify the potential benefit of personalizing search result using two implicit click based measure click entropy and potential n meanwhile query are characterized by query feature and history feature then we build c svm classification model and epsilon svm regression model respectively according to these two measure the experimental result show that the classification model is a better choice for predicting query potential for personalization 
current web service security standard have inadequate support for end to end protection of data when some receiver of the data are unknown to the sender this paper present an approach to aid collaborative partner service in properly protecting each other s data our approach allows each partner to derive an adequate protection mechanism with minimum performance overhead for each message it sends based on those of the corresponding message it receives we modify the message handling mechanism of web service engine to dynamically gather protection requirement for a given outgoing message by aggregating requirement from original owner of message data 
today s web becomes a platform for service to be dynamically interconnected to produce a desired outcome it is important to formalize the semantics of the contextual element of web service in this paper we propose a novel technique called semantic genome propagation scheme sgps for measuring similarity between semantic concept we show how sgps is used to compute a multi dimensional similarity between two service we evaluate the sgps similarity measurement in term of the similarity performance and scalability 
high quality personalized recommendation are a key feature in many online system since these system often have explicit knowledge of social network structure the recommendation may incorporate this information this paper focus on network that represent trust and recommendation system that incorporate these trust relationship the goal of a trust based recommendation system is to generate personalized recommendation by aggregating the opinion of other user in the trust network in analogy to prior work on voting and ranking system we use the axiomatic approach from the theory of social choice we develop a set of five natural axiom that a trust based recommendation system might be expected to satisfy then we show that no system can simultaneously satisfy all the axiom however for any subset of four of the five axiom we exhibit a recommendation system that satisfies those axiom next we consider various way of weakening the axiom one of which lead to a unique recommendation system based on random walk we consider other recommendation system including system based on personalized pagerank majority of majority and minimum cut and search for alternative axiomatizations that uniquely characterize these system finally we determine which of these system are incentive compatible meaning that group of agent interested in manipulating recommendation can not induce others to share their opinion by lying about their vote or modifying their trust link this is an important property for system deployed in a monetized environment 
we describe dispute finder a browser extension that alert a user when information they read online is disputed by a source that they might trust dispute finder examines the text on the page that the user is browsing and highlight any phrase that resemble known disputed claim if a user click on a highlighted phrase then dispute finder show them a list of article that support other point of view dispute finder build a database of known disputed claim by crawling web site that already maintain list of disputed claim and by allowing user to enter claim that they believe are disputed dispute finder identifies snippet that make known disputed claim by running a simple textual entailment algorithm inside the browser extension referring to a cached local copy of the claim database in this paper we explain the design of dispute finder and the trade offs between the various design decision that we explored 
we analyze dependency in power law graph data web sample wikipedia sample and a preferential attachment graph using statistical inference for multivariate regular variation the well developed theory of regular variation is widely applied in extreme value theory telecommunication and mathematical finance and it provides a natural mathematical formalism for analyzing dependency between variable with power law however most of the proposed method have never been used in the web graph data mining the present work fill this gap the new insight this yield are striking the three above mentioned data set are shown to have a totally different dependence structure between different graph parameter such a in degree and pagerank 
the need for evaluating large amount of topic query make ir evaluation an uneasy task in this paper we study a topic selection problem for ir evaluation the selection criterion is based on the overall difficulty of the chosen set a well a the uncertainty of the final ir metric applied to the system our preliminary experiment demonstrate that our approach help to identify a set of topic that provides confident estimate of system performance while keeping the requirement of the query difficulty 
recently information retrieval researcher have witnessed the increasing interest in query substitution for ad search most previous work substitute search query via content based query similarity and few of them take the temporal characteristic of query into consideration in this extended abstract we propose a novel temporal similarity measurement for query substitution in ad search task we firstly extract temporal feature such a burst and periodicity from query frequency curve and then define the temporal query similarity by integrating these new feature with the temporal query frequency distribution compared to the traditional temporal similarity measurement such a correlation coefficient our proposed approach is more effective owing to the explicit extraction of high level semantic query temporal feature for similarity measure the experimental result demonstrate that the proposed similarity measure can make the ad more relevant to user search query compared to ad search without temporal feature 
a large number of web site publish page containing structured information about recognizable concept but these data are only partially used by current application although such information is spread across a myriad of source the web scale implies a relevant redundancy we present a domain independent system that exploit the redundancy of information to automatically extract and integrate data from the web our solution concentrate on source that provide structured data about multiple instance from the same conceptual domain e g financial data product information our proposal is based on an original approach that exploit the mutual dependency between the data extraction and the data integration task experiment confirmed the quality and the feasibility of the approach 
recently user generated data is growing rapidly and becoming one of the most important source of information in the web blogosphere the collection of blog on the web is one of the main source of information in this category in my work for my phd i mainly focussed on the blog distillation task which is given a user query find the blog that are most related to the query topic there are some property of blog that make blog analysis different from usual text analysis one of these property is related to the time stamp assigned to each post it is possible that the topic of a blog change over the time and this can affect blog relevance to the query also each post in a blog can have viewer generated comment that can change the relevance of the blog to the query if these are considered a part of the content of the blog another property is related to the meaning of the link between blog which are different than link between website finally blog distillation is different from traditional ad hoc search since the retrieval unit is a blog a collection of post instead of a single document with this view blog distillation is similar to the task of resource selection in federated search researcher have applied different method from similar problem to blog distillation like ad hoc search method expert search algorithm or method from resource selection in distributed information retrieval based on our preliminary experiment i decided to divide the blog distillation problem into two sub problem first of all i want to use mentioned property of blog to retrieve the most relevant post for a given query this part is very similar to the ad hoc retrieval after that i want to aggregate relevance of post in each blog and calculate relevance of the blog this part requires the development of a cross modal aggregation model that combine the different blog relevance clue found in the blogosphere we use structure based smoothing method for improving post retrieval the idea behind these smoothing method is to change the score of a document based on the score of it similar or related document we model the blogosphere a a single graph that represents relation between post and term the idea is that in accordance with the clustering hypothesis related document should have similar score for the same query to model the relatedness between post we define a new measure which take into account both content similarity and temporal distance in more recent work in the aggregation part of the problem we model each post a evidence about relevance of a blog to the query and use aggregation method like ordered weighted averaging operator to combine the evidence the ordered weighted averaging operator commonly called owa operator wa introduced by yager owa provides a parametrized class of mean type aggregation operator that can generate or operator max and operator min and any other aggregation operator between them for the next step i m thinking about capturing the temporal property of the blog blogger can change their interest over the time or write about different topic periodically capturing these change and using them in the retrieval is one the future wok that i m interested in also studying the relation between blog and news and their effect on each other is an interesting problem 
we propose a hybridization of collaborative filtering and content based recommendation system attribute used for content based recommendation are assigned weight depending on their importance to user the weight value are estimated from a set of linear regression equation obtained from a social network graph which capture human judgment about similarity of item 
how often do tag recur how hard is predicting tag recurrence what tag are likely to recur we try to answer these question by analysing the rsdc dataset in both individual and collective setting our finding provide useful insight for the development of tag suggestion technique etc 
in this technical demonstration we showcase the mindfinder system a novel image search engine different from existing interactive image search engine most of which only provide image level relevance feedback mindfinder enables user to sketch and tag query image at object level by considering the image database a a huge repository mindfinder is able to help user present and refine their initial thought in their mind and finally turn thought to a beautiful image s multiple action are enabled for user to flexibly design their query in a bilateral interactive manner by leveraging the whole image database including tagging refining query by dragging and dropping object from search result a well a editing object after each action the search result will be updated in real time to provide user up to date material to further formulate the query by the deliberate but easy design of the query mindfinder not only try to enable user to present on the query panel whatever they are imagining but also return to user the most similar image to the picture in user mind by scaling up the image database to million mindfinder ha the potential to reveal whatever in user mind that is where the name mindfinder come from 
this paper present a novel prototype hierarchy based clustering phc framework for the organization of web collection it solves simultaneously the problem of categorizing web collection and interpreting the clustering result for navigation by utilizing prototype hierarchy and the underlying topic structure of the collection phc is modeled a a multi criterion optimization problem based on minimizing the hierarchy evolution maximizing category cohesiveness and inter hierarchy structural and semantic resemblance the flexible design of metric enables phc to be a general framework for application in various domain in the experiment on categorizing collection of distinct domain phc achieves improvement in f over the state of the art technique further experiment provide insight on performance variation with abstract and concrete domain completeness of the prototype hierarchy and effect of different combination of optimization criterion 
to improve query performance and space efficiency an efficient random access blocked inverted index rabi is proposed rabi divide an inverted list into block and compress different part of each block with the corresponding encoding method to decrease space consumption rabi can provide fast addressing and random access function on the compressed blocked inverted index with the novel hybrid compression method which can provide both block level and inner block level skipping function and further enhance both space and time efficiency without inserting any additional auxiliary information experimental result show that rabi achieves both high space efficiency and search efficiency and outperforms the existing approach significantly 
in this paper we propose a novel approach for composing existing web service to satisfy the correctness constraint to the design including freeness of deadlock and unspecified reception and temporal constraint in computation tree logic formula an automated synthesis algorithm based on learning algorithm is introduced which guarantee that the composite service is the most general way of coordinating service so that the correctness is ensured we have implemented a prototype system evaluating the effectiveness and efficiency of our synthesis approach through an experimental study 
due to the lexical gap between question and answer automatically detecting right answer becomes very challenging for community question answering site in this paper we propose an analogical reasoning based method it treat question and answer a relational data and rank an answer by measuring the analogy of it link to a query with the link embedded in previous relevant knowledge the answer that link in the most analogous way to the new question is assumed to be the best answer we based our experiment on million yahoo answer question answer thread and showed the effectiveness of the approach 
people use weblogs to express thought present idea and share knowledge therefore weblogs are extraordinarily valuable resource amongs others for trend analysis trend are derived from the chronological sequence of blog post count per topic the comparison with a reference corpus allows qualitative statement over identified trend we propose a crosslanguage blog mining and trend visualisation system to analyse blog across language and topic the trend visualisation facilitates the identification of trend and the comparison with the reference news article corpus to prove the correctness of our system we computed the correlation between trend in blog and news article for a subset of blog and topic the evaluation corroborated our hypothesis of a high correlation coefficient for these subset and therefore the correctness of our system for different language and topic is proven 
efficiently querying rdf data is being an important factor in applying semantic web technology to real world application in this context many effort have been made to store and query rdf data in relational database using particular schema in this paper we propose a new scheme to store index and query rdf data in triple store graph feature of rdf data is taken into consideration which might help reduce the join cost on the vertical database structure we would partition rdf triple into overlapped group store them in a triple table with one more column of group identity and build up a signature tree to index them based on this infrastructure a complex rdf query is decomposed into multiple piece of sub query which could be easily filtered into some rdf group using signature tree index and finally is evaluated with a composed and optimized sql with specific constraint we compare the performance of our method with prior art on typical query over a large scaled lubm and uobm benchmark data more than million triple for some extreme case they can promote to order of magnitude 
with the explosive growth of online video automatic real time categorization of web video play a key role for organizing browsing and retrieving the huge amount of video on the web previous work show that in addition to text feature content feature of video are also useful for web video classification unfortunately extracting content feature is computationally prohibitive for real time video classification in this paper we propose a novel video classification framework that is able to exploit both content and text feature for video classification while avoiding the expensive computation of extracting content feature at classification time the main idea of our approach is to utilize the content feature extracted from training data to enrich the text based semantic kernel yielding content enriched semantic kernel the content enriched semantic kernel enable to utilize both content and text feature for classifying new video without extracting their content feature the experimental result show that our approach significantly outperforms the state of the art video classification method 
to deal with the problem of empty or too little answer returned from a web database in response to a user query this paper proposes a novel approach to provide relevant and ranked query result based on the user original query we speculate how much the user care about each specified attribute and assign a corresponding weight to it this original query is then rewritten a an approximate query by relaxing the query criterion range the relaxation order of all specified attribute and the relaxed degree on each specified attribute are varied with the attribute weight for the approximate query result we generate user contextual preference from database workload and use them to create a priori order of tuples in an off line preprocessing step only a few representative order are saved each corresponding to a set of context then these order and associated context are used at query time to expeditiously provide ranked answer result of a preliminary user study demonstrate that our query relaxation and result ranking method can capture the user s preference effectively the efficiency and effectiveness of our approach is also demonstrated by experimental result 
most existing approach to visual search reranking predominantly focus on mining information within the initial search result however the initial ranked list cannot provide enough cue for reranking by itself due to the typically unsatisfying visual search performance this paper present a new method for visual search reranking called crowdreranking which is characterized by mining relevant visual pattern from image search result of multiple search engine which are available on the internet observing that different search engine might have different data source for indexing and method for ranking it is reasonable to assume that there exist different search result yet certain common visual pattern relevant to a given query among those result we first construct a set of visual word based on the local image patch collected from multiple image search engine we then explicitly detect two kind of visual pattern i e salient and concurrent pattern among the visual word theoretically we formalize reranking a an optimization problem on the basis of the mined visual pattern and propose a close form solution empirically we conduct extensive experiment on several real world search engine and one benchmark dataset and show that the proposed crowdreranking is superior to the state of the art work 
in this paper we investigate generic method for placing photo uploaded to flickr on the world map a primary input for our method we use the textual annotation provided by the user to predict the single most probable location where the image wa taken central to our approach is a language model based entirely on the annotation provided by user we define extension to improve over the language model using tag based smoothing and cell based smoothing and leveraging spatial ambiguity further we demonstrate how to incorporate geonames footnote http www geonames org visited may a large external database of location for varying level of granularity we are able to place image on a map with at least twice the precision of the state of the art reported in the literature 
the social science strive to understand the political social and cultural world around u but have been impaired by limited access to the quantitative data source enjoyed by the hard science careful analysis of web document stream hold enormous potential to solve longstanding problem in a variety of social science discipline through massive data analysis this paper introduces the textmap access system which provides ready access to a wealth of interesting statistic on million of people place and thing across a number of interesting web corpus powered by a flexible and scalable distributed statistic computation framework using hadoop continually updated corpus include newspaper blog patent record legal document and scientific abstract well over a terabyte of raw text and growing daily the lydia textmap access system available through http www textmap com access provides instant access for student and scholar through a convenient web user interface we describe the architecture of the textmap access system and it impact on current research in political science sociology and business marketing 
the term web genre denotes the type of a given web resource in contrast to the topic of it content in this research we focus on recognizing the web genre blog wiki and forum we present a set of feature that exploit the hierarchical structure of the web page s html mark up and thus in contrast to related approach do not depend on a linguistic analysis of the page s content our result show that it is possible to achieve a very good accuracy for a fully language independent detection of structured web genre 
in this paper we present a web based framework for spatiotemporal screen real estate management of interactive public display the framework facilitates dynamic partitioning of the screen real estate into virtual screen assigned for multiple concurrent web application the framework is utilized in the implementation of so called ubi hotspot which provides various information service via different interaction modality including mobile the framework facilitates seamless integration of third party web application residing anywhere in the public internet into the ubi hotspot thus catering for a scalable and open architecture we report the deployment of a network of indoor and outdoor ubi hotspot at downtown oulu finland the quantitative data on the usage of the ubi hotspot implicitly speaks in favor of the practical applicability of the framework 
sensemaking task require user to perform complex research behavior to gather and comprehend information from many source such task are common and include for example researching vacation destination or deciding how to invest in this paper we present an algorithm and interface that provides context based page unit recommendation to assist in connection discovery during sensemaking task we exploit the natural note taking activity common to sensemaking behavior a the basis for a task specific context model each web page visited by a user is dynamically analyzed to determine the most relevant content fragment which are then recommended to the user our initial evaluation indicate that our approach improves user performance 
to evaluate the diversity of search result test collection have been developed that identify multiple intent for each query intent are the different meaning or facet that should be covered in a search result list this mean that topic development involves proposing a set of intent for each query we propose four measurable property of query to intent mapping allowing for more principled topic development for such test collection 
given a collection of document most of existing multidocument summarization method automatically generate a static summary for all the user however different user may have different opinion on the document thus there is a necessity for improving user interaction in the summarization process in this paper we propose an interactive document summarization system using information visualization technique 
existing question retrieval model work relatively well in finding similar question in community based question answering cqa service however they are designed for single sentence query or bag of word representation and are not sufficient to handle multi sentence question complemented with various context segmenting question into part that are topically related could assist the retrieval system to not only better understand the user s different information need but also fetch the most appropriate fragment of question and answer in cqa archive that are relevant to user s query in this paper we propose a graph based approach to segmenting multi sentence question the result from user study show that our segmentation model outperforms traditional system in question segmentation by over in user s satisfaction we incorporate the segmentation model into existing cqa question retrieval framework for more targeted question matching and the empirical evaluation result demonstrate that the segmentation boost the question retrieval performance by up to in mean average precision and in top one precision our model come with a comprehensive question detector equipped with both lexical and syntactic feature 
twitter a popular microblogging service ha received much attention recently an important characteristic of twitter is it real time nature for example when an earthquake occurs people make many twitter post tweet related to the earthquake which enables detection of earthquake occurrence promptly simply by observing the tweet a described in this paper we investigate the real time interaction of event such a earthquake in twitter and propose an algorithm to monitor tweet and to detect a target event to detect a target event we devise a classifier of tweet based on feature such a the keywords in a tweet the number of word and their context subsequently we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location we consider each twitter user a a sensor and apply kalman filtering and particle filtering which are widely used for location estimation in ubiquitous pervasive computing the particle filter work better than other comparable method for estimating the center of earthquake and the trajectory of typhoon a an application we construct an earthquake reporting system in japan because of the numerous earthquake and the large number of twitter user throughout the country we can detect an earthquake with high probability of earthquake of japan meteorological agency jma seismic intensity scale or more are detected merely by monitoring tweet our system detects earthquake promptly and sends e mail to registered user notification is delivered much faster than the announcement that are broadcast by the jma 
in this paper we propose a new paper ranking algorithm that give a high rank to paper which is credited by other authoritative paper or published in premier conference or journal also the proposed algorithm solves a problem that recent paper are rated poorly due to few citation 
anycast based content delivery network cdns have many property that make them ideal for the large scale distribution of content on the internet however because routing change can result in a change of the endpoint that terminates the tcp session tcp session disruption remains a concern for anycast cdns especially for large file downloads in this paper we demonstrate that this problem doe not require any complex solution in particular we present the design of a simple yet efficient mechanism to handle session disruption due to endpoint change with our mechanism a client can continue the download of the content from the point at which it wa before the endpoint change furthermore cdn server purge the tcp connection state quickly to handle frequent switching with low system overhead we demonstrate experimentally the effectiveness of our proposed mechanism and show that more complex mechanism are not required specifically we find that our mechanism maintains high download throughput even with a reasonably high rate of endpoint switching which is attractive for load balancing scenario moreover our result show that edge server can purge tcp connection state after a single timeout triggered retransmission without any tangible impact on ongoing connection besides improving server performance this behavior improves the resiliency of the cdn to certain denial of service attack 
because of the high volume and unpredictable arrival rate stream processing system may not always be able to keep up with the input data stream resulting in buffer overflow and uncontrolled loss of data load shedding the prevalent strategy for solving this overflow problem ha so far only been considered for relational stream processing but not for xml shedding applied to xml stream processing brings new opportunity and challenge due to complex nested nature of xml structure in this paper we tackle this unsolved xml shedding problem using a three pronged approach first we develop an xquery preference model that enables user to specify the relative importance of preserving different subpatterns in the xml result structure this transforms shedding into the problem of rewriting the user query into shed query that return approximate query answer with utility a measured by the given user preference model second we develop a cost model to compare the performance of alternate shed query third we develop two shedding algorithm optshed and fastshed optshed guarantee to find an optimal solution however at the cost of exponential complexity fastshed a confirmed by our experiment achieves a close to optimal result in a wide range of test case finally we describe the in automaton shedding mechanism for xquery stream engine the experiment show that our proposed utility driven shedding solution consistently achieve higher utility result compared to the existing relational shedding technique 
this paper investigates the strategic decision of online vendor for offering different mechanism such a sampling and online review of information product to increase their online sale focusing on measuring the effectiveness of electronic market design offering review sampling or both our study show that online market behavior a communication market and consumer learn product quality information both passively reading online review and actively but subjectively listening to music sampling using data from amazon first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect in general product with sampling option enjoy a higher conversion rate which lead to better sale than those without sampling because sampling decrease the uncertainty of consuming experience good second the impact of online review on sale conversion rate is lower for experience good with a sampling option than those without third when the uncertainty of the societal review is higher sampling play a more important role because it mitigates such uncertainty introduced by online review 
some application have to present their result in the form of ranked list this is the case of many information retrieval application in which document must be sorted according to their relevance to a given query this ha led the interest of the information retrieval community in method that automatically learn effective ranking function in this paper we propose a novel method which uncovers pattern or rule in the training data associating feature of the document with it relevance to the query and then us the discovered rule to rank document to address typical problem that are inherent to the utilization of association rule such a missing rule and rule explosion the proposed method generates rule on a demand driven basis at query time the result is an extremely fast and effective ranking method we conducted a systematic evaluation of the proposed method using the letor benchmark collection we show that generating rule on a demand driven basis can boost ranking performance providing gain ranging from to outperforming the state of the art method that learn to rank with no need of time consuming and laborious pre processing a a highlight we also show that additional information such a query term can make the generated rule more discriminative further improving ranking performance 
analyzing the large volume of online review would produce useful knowledge that could be of economic value to vendor and other interested party in particular the sentiment expressed in the online review have been shown to be strongly correlated with the sale performance of product in this paper we present an adaptive sentiment analysis model called s plsa which aim to capture the hidden sentiment factor in the review with the capability to be incrementally updated a more data become available we show how s plsa can be applied to sale performance prediction using an arsa model developed in previous literature a case study is conducted in the movie domain and result from preliminary experiment confirm the effectiveness of the proposed model 
the core task of sponsored search is to retrieve relevant ad for the user s query ad can be retrieved either by exact match when their bid term is identical to the query or by advanced match which index ad a document and is similar to standard information retrieval ir recently there ha been a great deal of research into developing advanced match ranking algorithm however no previous research ha addressed the ad indexing problem unlike most traditional search problem the ad corpus is defined hierarchically in term of advertiser account campaign and ad group which further consist of creatives and bid term this hierarchical structure make indexing highly non trivial a naively indexing all possible displayable ad lead to a prohibitively large and ineffective index we show that ad retrieval using such an index is not only slow but it precision is suboptimal a well we investigate various strategy for compact hierarchy aware indexing of sponsored search ad through adaptation of standard ir indexing technique we also propose a new ad retrieval method that yield more relevant ad by exploiting the structured nature of the ad corpus experiment carried out over a large ad test collection from a commercial search engine show that our proposed method are highly effective and efficient compared to more standard indexing and retrieval approach 
a the world wide web in china grows rapidly mining knowledge in chinese web page becomes more and more important mining web information usually relies on the machine learning technique which require a large amount of labeled data to train credible model although the number of chinese web page increase quite fast it still lack chinese labeled data however there are relatively sufficient english labeled web page these labeled data though in different linguistic representation share a substantial amount of semantic information with chinese one and can be utilized to help classify chinese web page in this paper we propose an information bottleneck based approach to address this cross language classification problem our algorithm first translates all the chinese web page to english then all the web page including chinese and english one are encoded through an information bottleneck which can allow only limited information to pas therefore in order to retain a much useful information a possible the common part between chinese and english web page is inclined to be encoded to the same code i e class label which make the cross language classification accurate we evaluated our approach using the web page collected from open directory project odp the experimental result show that our method significantly improves several existing supervised and semi supervised classifier 
we show how a number of novel email search feature can be implemented without any kind of natural language processing nlp or advanced data mining our approach inspects the email header of all message a user ha ever sent or received and it creates simple per contact summary including simple information about the message exchange history the domain of the sender or even the sender s gender with these summary advanced question task such a who do i still need to reply to or find fun message sent by friend become possible a a proof of concept we implemented a mozilla thunderbird extension adding powerful people search to the popular email client 
the paper proposes identifying relevant information source from the history of combined searching and browsing behavior of many web user while it ha been previously shown that user interaction with search engine can be employed to improve document ranking browsing behavior that occurs beyond search result page ha been largely overlooked in prior work the paper demonstrates that user post search browsing activity strongly reflects implicit endorsement of visited page which allows estimating topical relevance of web resource by mining large scale datasets of search trail we present heuristic and probabilistic algorithm that rely on such datasets for suggesting authoritative website for search query experimental evaluation show that exploiting complete post search browsing trail outperforms alternative in isolation e g clickthrough log and yield accuracy improvement when employed a a feature in learning to rank for web search 
the web of data ha emerged a a way of exposing structured linked data on the web it build on the central building block of the web uris http and benefit from it simplicity and wide spread adoption it doe however also inherit the unresolved issue such a the broken link problem broken link constitute a major challenge for actor consuming linked data a they require them to deal with reduced accessibility of data we believe that the broken link problem is a major threat to the whole web of data idea and that both linked data consumer and provider will require solution that deal with this problem since no general solution for fixing such link in the web of data have emerged we make three contribution into this direction first we provide a concise definition of the broken link problem and a comprehensive analysis of existing approach second we present dsnotify a generic framework able to assist human and machine actor in fixing broken link it us heuristic feature comparison and employ a time interval based blocking technique for the underlying instance matching problem third we derived benchmark datasets from knowledge base such a dbpedia and evaluated the effectiveness of our approach with respect to the broken link problem our result show the feasibility of a time interval based blocking approach for system that aim at detecting and fixing broken link in the web of data 
ontology mapping seek to find semantic correspondence between similar element of different ontology this paper proposes a neural network based approach to search for a global optimal solution that best satisfies ontology constraint experiment on oaei benchmark test show it dramatically improves the performance of preliminary mapping result 
by analyzing explicit implicit feedback information retrieval system can determine topical relevance and tailor search criterion to the user s need in this paper we investigate whether it is possible to infer what is relevant by observing user affective behaviour the sensory data employed range between facial expression and peripheral physiological signal we extract a set of feature from the signal and analyze the data using classification method such a svm and knn the result of our initial evaluation indicate that prediction of relevance is possible to a certain extent and implicit feedback model can benefit from taking into account user affective behavior 
web search result often integrate content from specialized corpus known a vertical given a query one important aspect of aggregated search is the selection of relevant vertical from a set of candidate vertical one drawback to previous approach to vertical selection is that method have not explicitly modeled user feedback however production search system often record a variety of feedback information in this paper we present algorithm for vertical selection which adapt to user feedback we evaluate algorithm using a novel simulator which model performance of a vertical selector situated in realistic query traffic 
in most previous work on personalized search algorithm the result for all query are personalized in the same manner however a we show in this paper there is a lot of variation across query in the benefit that can be achieved through personalization for some query everyone who issue the query is looking for the same thing for other query different people want very different result even though they express their need in the same way we examine variability in user intent using both explicit relevance judgment and large scale log analysis of user behavior pattern while variation in user behavior is correlated with variation in explicit relevance judgment the same query there are many other factor such a result entropy result quality and task that can also affect the variation in behavior we characterize query using a variety of feature of the query the result returned for the query and people s interaction history with the query using these feature we build predictive model to identify query that can benefit from personalization 
this paper present an eye tracking study that examines how people use the visual element of web page to complete certain task whilst these element are available to play their role in these task for sighted user it is not the case for visually disabled user this lack of access to some visual element of a page mean that visually disabled user are hindered in accomplishing these task our previous work ha introduced a framework that identifies these element and then reengineers web page such that these element can play their intended role in an audio a well a visual presentation to further improve our understanding of how these element are used and to validate our framework we track the eye movement of sighted user performing a number of different task the resulting gaze data show that there is a strong relationship between the aspect of a page that receive visual attention and the object identified by our framework the study also show some limitation a well a yielding information to address these short coming perhaps the most important result is the support provided for a particular kind of object called a way edge the visual construct used to group content into section there is a significant effect of way edge on the distribution of attention across task this is a result that not only provides strong evidence for the utility of re engineering but also ha consequence for our understanding of how people allocate attention to different part of a page we speculate that the phenomenon of banner blindness owes a much to way edge a it doe to colour and font size 
match making system refer to system where user want to meet other individual to satisfy some underlying need example of match making system include dating service resume job bulletin board community based question answering and consumer to consumer marketplace one fundamental component of a match making system is the retrieval and ranking of candidate match for a given user we present the first in depth study of information retrieval approach applied to match making system specifically we focus on retrieval for a dating service this domain offer several unique problem not found in traditional information retrieval task these include two sided relevance very subjective relevance extremely few relevant match and structured query we propose a machine learned ranking function that make use of feature extracted from the uniquely rich user profile that consist of both structured and unstructured attribute an extensive evaluation carried out using data gathered from a real online dating service show the benefit of our proposed methodology with respect to traditional match making baseline system our analysis also provides deep insight into the aspect of match making that are particularly important for producing highly relevant match 
in order to return relevant search result a search engine must keep it local repository synchronized to the web but it is usually impossible to attain perfect freshness hence it is vital for a production search engine continually to monitor and improve repository freshness most previous freshness metric formulated in the context of developing better synchronization policy focused on the web crawler while ignoring other part of a search engine but the freshness of document in a web crawler doe not necessarily translate directly into the freshness of search result a seen by user we propose metric for measuring freshness from a user s perspective which take into account the latency between when document are crawled and when they are viewed by user a well a the variation in user click and view frequency among different document we also describe a practical implementation of these metric that were used in a production search engine 
we analyze knowledge production in computer science by mean of coauthorship network for this we consider graduate program of different region of the world being program in brazil in north america in canada and in the united state and in europe in france in switzerland and in the united kingdom we use a dataset that consists of author and publication entry distributed among publication venue the result obtained for different metric of collaboration social network indicate the process of knowledge creation ha changed differently for each region research is increasingly done in team across different field of computer science the size of the giant component indicates the existence of isolated collaboration group in the european network contrasting to the degree of connectivity found in the brazilian and north american counterpart we also analyzed the temporal evolution of the social network representing the three region the number of author per paper experienced an increase in a time span of year we observe that the number of collaboration between author grows faster than the number of author benefiting from the existing network structure the temporal evolution show difference between well established field such a database and computer architecture and emerging field like bioinformatics and geoinformatics the pattern of collaboration analyzed in this paper contribute to an overall understanding of computer science research in different geographical region that could not be achieved without the use of complex network and a large publication database 
in this paper we propose the liquid query paradigm to support user in finding response to multi domain query through exploratory information seeking across structured information source web document deep web data and personal data repository wrapped by mean of a uniform notion of search service liquid query aim at filling the gap between general purpose search engine which are unable to find information spanning multiple topic and domain specific search system which cannot go beyond their domain limit the liquid query interface consists of interaction primitive that let user pose question and explore result spanning over multiple source incrementally thus getting closer and closer to the sought information we demonstrate our approach with a prototype built upon the yql yahoo query language framework 
consider an online ad campaign run by an advertiser the ad serving company that handle such campaign record user behavior that lead to impression of campaign ad a well a user response to such impression this is summarized and reported to the advertiser to help them evaluate the performance of their campaign and make better budget allocation decision the most popular reporting statistic are the click through rate and the conversion rate while these are indicative of the effectiveness of an ad campaign the advertiser often seek to understand more sophisticated long term effect of their ad on the brand awareness and the user behavior that lead to the conversion thus creating a need for the reporting measure that can capture both the duration and the frequency of the pathway to user conversion in this paper we propose an alternative data mining framework for analyzing user level advertising data in the aggregation step we compress individual user history into a graph structure called the adgraph representing local correlation between ad event for the reporting step we introduce several scoring rule called the adfactors af that can capture global role of ad and ad path in the adgraph in particular the structural correlation between an ad impression and the user conversion we present scalable local algorithm for computing the adfactors all algorithm were implemented using the mapreduce programming model and the pregel framework using an anonymous user level dataset of sponsored search campaign for eight different advertiser we evaluate our framework with different adgraphs and adfactors in term of their statistical fit to the data and show it value for mining the long term behavioral pattern in the advertising data 
one of the central issue in learning to rank for information retrieval is to develop algorithm that construct ranking model by directly optimizing evaluation measure used in information retrieval such a mean average precision map and normalized discounted cumulative gain ndcg several such algorithm including svmmap and adarank have been proposed and their effectiveness ha been verified however the relationship between the algorithm are not clear and furthermore no comparison have been conducted between them in this paper we conduct a study on the approach of directly optimizing evaluation measure in learning to rank for information retrieval ir we focus on the method that minimize loss function upper bounding the basic loss function defined on the ir measure we first provide a general framework for the study and analyze the existing algorithm of svmmap and adarank within the framework the framework is based on upper bound analysis and two type of upper bound are discussed moreover we show that we can derive new algorithm on the basis of this analysis and create one example algorithm called permurank we have also conducted comparison between svmmap adarank permurank and conventional method of ranking svm and rankboost using benchmark datasets experimental result show that the method based on direct optimization of evaluation measure can always outperform conventional method of ranking svm and rankboost however no significant difference exists among the performance of the direct optimization method themselves 
webpmi is a popular web based association measure to evaluate the semantic similarity between two query i e word or entity by leveraging search result returned by search engine this paper proposes a novel measure named cm pmi to evaluate query similarity at a finer granularity than webpmi under the assumption that a query is usually associated with more than one aspect and two query are deemed semantically related if their associated aspect set are highly consistent with each other cm pmi first extract contextual label from search result to represent the aspect of a query and then us the optimal matching method to ass the consistency between the aspect of two query experimental result on the benchmark miller charles dataset demonstrate the good effectiveness of the proposed cm pmi measure moreover we further fuse webpmi and cm pmi to obtain improved result 
fast technological advancement and little compliance with accessibility standard by web page author pose serious obstacle to the web experience of the blind user we propose a unified web document model that enables u to create a richer browsing experience and improved navigability for blind user the model provides an integrated view on all aspect of a web page and is leveraged to create a multi axial user interface 
the goal of the proposed tool multi searcher is to answer this research question can we expect people to be able to get information from text in language they can not read or understand the proposed tool multi searcher provides user with interactive contextual information that describes the translation in the user s own language so that the user ha a certain degree of confidence about the translation therefore the user is considered a an integral part of the retrieval process the tool provides possibility to interactively select relevant term from contextual information in order to improve the translation and thus improve the cross lingual information retrieval clir process 
proximity aware scoring function lead to significant effectiveness improvement for text retrieval for xml ir we can sometimes enhance the retrieval quality by exploiting knowledge about the document structure combined with established text ir method this paper introduces modified proximity score that take the document structure into account and demonstrates the effect for the inex benchmark 
in this paper we describe our experience in the jazz project beginning from a classical repositoryand java centric design and evolving towards an architecture which borrows heavily from the architecture of the web along the way we formed an open community to collaborate on adapting this architecture to various tool domain finally we discus our experience delivering the first generation of tool built and integrated using these technique 
image spam is a new obfuscating method which spammer invented to more effectively bypass conventional text based spam filter in this paper we extract local invariant feature of image and run a one class svm classifier which us the pyramid match kernel a the kernel function to detect image spam experimental result demonstrate that our algorithm is effective for fighting image spam 
with a wiki like search interface user can edit rank of search result and share the edits with the rest of the world this is an effective way of personalization a well a a practice of mass collaboration that allows user to vote for ranking and improve search performance currently there are several ongoing experimentation effort from the industry e g searchwiki by google and u rank by microsoft beyond that there is little published research on this new search paradigm in this paper we make an effort to establish a framework for rank editing and sharing in the context of web search where we identify fundamental issue and propose principled solution comparing to existing system for rank editing our framework allows user to specify not only relative but also absolute preference for edit sharing our framework provides enhanced flexibility allowing user to select arbitrarily aggregated view in addition edits can be shared among similar query we present a prototype system rant that implement the framework and provides search service through the google web search api 
collaborative filtering cf requires user rated training example for statistical inference about the preference of new user active learning strategy identify the most informative set of training example through minimum interaction with the user current active learning approach in cf make an implicit and unrealistic assumption that a user can provide rating for any queried item this paper introduces a new approach to the problem which doe not make such an assumption we personalize active learning for the user and query for only those item which the user can provide rating for we propose an extended form of bayesian active learning and use the aspect model for cf to illustrate and examine the idea a comparative evaluation of the new method and a well established baseline method on benchmark datasets show statistically significant improvement with our method over the performance of the baseline method that is representative for existing approach which do not take personalization into account 
this paper is concerned with the problem of imbalanced classification ic in web mining which often arises on the web due to the matthew effect a web ic application usually need to provide online service for user and deal with large volume of data classification speed emerges a an important issue to be addressed in face detection asymmetric cascade is used to speed up imbalanced classification by building a cascade structure of simple classifier but it often cause a loss of classification accuracy due to the iterative feature addition in it learning procedure in this paper we adopt the idea of cascade classifier in imbalanced web mining for fast classification and propose a novel asymmetric cascade learning method called floatcascade to improve the accuracy to the end floatcascade selects fewer yet more effective feature at each stage of the cascade classifier in addition a decision tree scheme is adopted to enhance feature diversity and discrimination capability for floatcascade learning we evaluate floatcascade through two typical ic application in web mining web page categorization and citation matching experimental result demonstrate the effectiveness and efficiency of floatcascade comparing to the state of the art ic method like asymmetric cascade asymmetric adaboost and weighted svm 
query forwarding is an important technique for preserving the result quality in distributed search engine where the index is geographically partitioned over multiple search site the key component in query forwarding is the thresholding algorithm by which the forwarding decision are given in this paper we propose a linear programming based thresholding algorithm that significantly outperforms the current state of the art in term of achieved search efficiency value moreover we evaluate a greedy heuristic for partial index replication and investigate the impact of result cache freshness on query forwarding performance finally we present some optimization that improve the performance further under certain condition we evaluate the proposed technique by simulation over a real life setting using a large query log and a document collection obtained from yahoo 
event are the fundamental abstraction to study the dynamic world we believe that the next generation of web i e event web will focus on interconnection between event a they occur across space and time in fact we argue that the real value of large volume of microblog data being created daily lie in it inherent spatio temporality and it correlation with the real world event in this context we studied the structural property of a corpus of twitter microblogs and found it to exhibit power law across space and time much like those exhibited by event in multiple domain the property studied over microblogs on different topic can be applied to study relationship between related event a well a data organization for event based real time and location aware application 
with the explosive growth of web and the recent development in digital medium technology the number of image on the web ha grown tremendously consequently web image clustering ha emerged a an important application some of the initial effort along this direction revolved around clustering web image based on the visual feature of image or textual feature by making use of the text surrounding the image however not much work ha been done in using multimodal information for clustering web image in this paper we propose a graph theoretical framework for simultaneously integrating visual and textual feature for efficient web image clustering specifically we model visual feature image and word from surrounding text using a tripartite graph partitioning this graph lead to clustering of the web image although graph partitioning approach ha been adopted before the main contribution of this work lie in a new algorithm that we propose consistent isoperimetric high order co clustering cihc for partitioning the tripartite graph computationally cihc is very quick a it requires a simple solution to a sparse system of linear equation our theoretical analysis and extensive experiment performed on real web image demonstrate the performance of cihc in term of the quality efficiency and scalability in partitioning the visual feature image word tripartite graph 
we explore new way of improving a search engine using data from web application such a blog and social bookmark this data contains entity such a document people and tag and relationship between them we propose a simple yet effective method based on faceted search that treat all entity in a unified manner returning all of them document people and tag on every search and allowing all of them to be used a search term we describe an implementation of such a social search engine on the intranet of a large enterprise and present large scale experiment which verify the validity of our approach category and subject descriptor h information storage and retrieval information search and retrieval 
we analyse the corpus of user relationship of the slashdot technology news site the data wa collected from the slashdot zoo feature where user of the website can tag other user a friend and foe providing positive and negative endorsement we adapt social network analysis technique to the problem of negative edge weight in particular we consider signed variant of global network characteristic such a the clustering coefficient node level characteristic such a centrality and popularity measure and link level characteristic such a distance and similarity measure we evaluate these measure on the task of identifying unpopular user a well a on the task of predicting the sign of link and show that the network exhibit multiplicative transitivity which allows algebraic method based on matrix multiplication to be used we compare our method to traditional method which are only suitable for positively weighted edge 
form mapping is the key problem that need to be solved in order to get access to the hidden web currently available solution for fully automatic mapping are not ready for commercial meta search engine which still have to rely on hand crafted code and are hard to maintain we believe that a thorough formal description of the problem with semantic web technology provides a promising perspective to develop a new class of vertical search engine that is more robust and easier to maintain than existing solution in this paper instead of trying to tackle the mapping problem we model the interaction necessary to fill out a web form first during a user assisted phase the connection from the visible element on the form to the domain concept is established then with help from background knowledge about the possible interaction step a plan for filling out the form is derived 
many research implementation of search engine are written in c c or java they are difficult to understand and modify because they are at least a few thousand line of code and contain many low level detail in this paper we show how to achieve a much shorter and higher level implementation one in about a few hundred line we accomplish this result through the use of a high level functional programming language f and some of it feature such a sequence pipe and structured input and output by using a search engine implementation a a case study we argue that functional programming fit the domain of information retrieval problem much better than imperative oo language like c and java functional programming language are ideal for rapid algorithm prototyping and data exploration in the field of information retrieval ir additionally our implementation can be used a case study in an ir course since it is a very high level but nevertheless executable specification of a search engine 
we tackle the problem of disambiguating entity on the web we propose a user driven scheme where graph of entity represented by globally identifiable declarative artifact self organize in a dynamic and probabilistic manner our solution ha the following two desirable property i it let end user freely define association between arbitrary entity and ii it probabilistically infers entity relationship based on uncertain link using constraint satisfaction mechanism we outline the interface between our scheme and the current data web and show how higher layer application can take advantage of our approach to enhance search and update of information relating to online entity we describe a decentralized infrastructure supporting efficient and scalable entity disambiguation and demonstrate the practicability of our approach in a deployment over several hundred of machine 
web hosting provider are increasingly looking into dynamic hosting to reduce cost and improve the performance of their platform instead of provisioning fixed resource to each customer dynamic hosting maintains a variable number of application instance to satisfy current demand while existing research in this area ha mostly focused on the algorithm that decide on the number and location of application instance we address the problem of efficient enactment of these decision once they are made we propose a new approach to application placement and experimentally show that it dramatically reduces the cost of application placement which in turn improves the end to end agility of the hosting platform in reacting to demand change 
the research objective of this work is to develop a general framework that incorporates collaborative social tagging with a novel ontology scheme conveying multiple perspective we propose a framework where multiple user tag the same object an image in our case and an ontology is extended based on these tag while being tolerant about different point of view we are not aware of any other work that attempted to devise such an environment and to study it dynamic the proposed framework characterizes the underlying process for controlled collaborative development of a multi perspective ontology and it application to improve image annotation searching and browsing our case study experiment with a set of selected annotated image indicates the soundness of the proposed ontological model 
many emerging semantic web application include ontology from one set of author and instance data from another often much larger set of author often ontology are reused and instance data is integrated in manner unanticipated by their author not surprisingly many instance data rich application encounter instance data that is not compatible with the expectation of the original ontology author s this line of work focus on issue related to semantic expectation mismatch in instance data our initial result include a customizable and extensible service oriented evaluation architecture and a domain implementation called pmlvalidator which check instance data using the corresponding ontology and additional style requirement 
algorithm defining similarity between object of an information network are important of many ir task simrank algorithm and it variation are popularly used in many application many fast algorithm are also developed in this note we first reformulate them a random walk on the network and express them using forward and backward transition probably in a matrix form second we show that p rank simrank is only the special case of p rank ha a unique solution of eet when decay factor c is equal to we also show that simfusion algorithm is a special case of p rank algorithm and prove that the similarity matrix of simfusion is the product of pagerank vector our experiment on the web datasets show that for p rank the decay factor c doesn t seriously affect the similarity accuracy and accuracy of p rank is also higher than simfusion and simrank 
we present an efficient method for approximate search in a combination of several metric space which are a generalization of low level image feature using an inverted index our approximation give very high recall with subsecond response time on a real data set of one million image extracted from flickr we further exploit the inverted index to improve efficiency of the query processing by combining our search in metric feature with search in associated textual metadata 
crowdsourcing is a new web phenomenon in which a firm take a function once performed in house and outsources it to a crowd usually in the form of an open contest designing efficient crowdsourcing mechanism is not possible without deep understanding of incentive and strategic choice of all participant this paper present an empirical analysis of determinant of individual performance in multiple simultaneous crowdsourcing contest using a unique dataset for the world s largest competitive software development portal topcoder com special attention is given to studying the effect of the reputation system currently used by topcoder com on behavior of contestant we find that individual specific trait together with the project payment and the number of project requirement are significant predictor of the final project quality furthermore we find significant evidence of strategic behavior of contestant high rated contestant face tougher competition from their opponent in the competition phase of the contest in order to soften the competition they move first in the registration phase of the contest signing up early for particular project although registration in topcoder contest is non binding it deters entry of opponent in the same contest our lower bound estimate show that this strategy generates significant surplus gain to high rated contestant we conjecture that the reputation cheap talk mechanism employed by topcoder ha a positive effect on allocative efficiency of simultaneous all pay contest and should be considered for adoption in other crowdsourcing platform 
today the major web search engine answer query by showing ten result snippet which need to be inspected by user for identifying relevant result in this paper we investigate how to extract structured information from the web in order to directly answer query by showing the content being searched for we treat user search trail i e post search browsing behavior a implicit label on the relevance between web content and user query based on such label we use information extraction approach to build wrapper and extract structured information an important observation is that many web site contain page for name entity of certain category e g aol music contains a page for each musician and these page have the same format this make it possible to build wrapper from a small amount of implicit label and use them to extract structured information from many web page for different name entity we propose struclick a fully automated system for extracting structured information for query containing name entity of certain category it can identify important web site from web search log build wrapper from user search trail filter out bad wrapper built from random user click and combine structured information from different web site for each query comparing with existing approach on information extraction struclick can assign semantics to extracted data without any human labeling or supervision we perform comprehensive experiment which show struclick achieves high accuracy and good scalability 
in an ideal world interface design is the art and science of helping user accomplish task in a timely efficient and pleasurable manner this paper study the inverse situation the vast emergence of deliberately constructed malicious interface that violate design best practice in order to accomplish goal counter to those of the user this ha become a commonplace occurrence both on and off the desktop particularly on the web a primary objective of this paper is to formally define this problem including construction of a taxonomy of malicious interface technique and a preliminary analysis of their impact on user finding are presented that gauge the self reported tolerance and expectation level of user with regard to malicious interface a well a the effectiveness and ease of use of existing countermeasure a second objective of this paper is to increase awareness dialogue and research in a domain that we consider largely unexplored but critical to future usability of the www our result were accomplished through significant compilation of malicious interface technique based on review of thousand of web site and by conducting three survey ultimately this paper concludes that malicious interface are a ubiquitous problem that demand intervention by the security and human computer interaction community in order to reduce the negative impact on the global user population 
search engine rely on searching multiple partitioned corpus to return result to user in a reasonable amount of time in this paper we analyze the standard two tier architecture for web search with the difference that the corpus to be searched for a given query is predicted in advance we show that any predictor better than random yield time saving but this decrease in the processing time yield an increase in the infrastructure cost we provide an analysis and investigate this trade off in the context of two different scenario on real world data we demonstrate that in general the decrease in answer time is justified by a small increase in infrastructure cost 
we propose the socialtrust framework for tamper resilient trust establishment in online social network two of the salient feature of socialtrust are it dynamic revision of trust by i distinguishing relationship quality from trust and ii incorporating a personalized feedback mechanism for adapting a the social network evolves 
with a growing number of work utilizing link information in enhancing document clustering it becomes necessary to make a comparative evaluation of the impact of different link type on document clustering various type of link between text document including explicit link such a citation link and hyperlink implicit link such a co authorship link and pseudo link such a content similarity link convey topic similarity or topic transferring pattern which is very useful for document clustering in this study we adopt a relaxation labeling rl based clustering algorithm which employ both content and linkage information to evaluate the effectiveness of the aforementioned type of link for document clustering on eight datasets the experimental result show that linkage is quite effective in improving content based document clustering furthermore a series of interesting finding regarding the impact of different link type on document clustering are discovered through our experiment 
this paper describes mediafaces a system that enables faceted exploration of medium collection the system process semi structured information source to extract object and facet e g the relationship between two object next we rank the facet based on a statistical analysis of image search query log and the tagging behaviour of user annotating photo in flickr for a given object of interest we can then retrieve the top k most relevant facet and present them to the user the system is currently deployed in production by yahoo s image search engine we present the system architecture it main component and the application of the system a part of the image search experience 
in japanese it is quite common for the same word to be written in several different way this is especially true for katakana word which are typically used for transliterating foreign language this ambiguity becomes critical for automatic processing such a information retrieval ir to tackle this problem we propose a simple but effective approach to generating katakana variant by considering phonemic representation of the original language for a given word the proposed approach is evaluated through an assessment of the variant it generates also the impact of the generated variant on ir is studied in comparison to an existing approach using katakana rewriting rule 
it is crucial to study basic principle that support adaptive and scalable retrieval function in large networked environment such a the web where information is distributed among dynamic system we conducted experiment on decentralized ir operation on various scale of information network and analyzed effectiveness efficiency and scalability of various search method result showed network structure i e how distributed system connect to one another is crucial for retrieval performance relying on partial index of distributed system some level of network clustering enabled very efficient and effective discovery of relevant information in large scale network for a given network clustering level search time wa well explained by a poly logarithmic relation to network size i e the number of distributed system indicating a high scalability potential for searching in a growing information space in addition network clustering only involved local self organization and required no global control clustering time remained roughly constant across the various scale of network 
multi document summarization aim to create a compressed summary while retaining the main characteristic of the original set of document many approach use statistic and machine learning technique to extract sentence from document in this paper we propose a new multi document summarization framework based on sentence level semantic analysis and symmetric non negative matrix factorization we first calculate sentence sentence similarity using semantic analysis and construct the similarity matrix then symmetric matrix factorization which ha been shown to be equivalent to normalized spectral clustering is used to group sentence into cluster finally the most informative sentence are selected from each group to form the summary experimental result on duc and duc data set demonstrate the improvement of our proposed framework over the implemented existing summarization system a further study on the factor that benefit the high performance is also conducted 
we study the problem of finding sentence that explain the relationship between a named entity and an ad hoc query which we refer to a entity support sentence this is an important sub problem of entity ranking which to the best of our knowledge ha not been addressed before in this paper we give the first formalization of the problem how it can be evaluated and present a full evaluation dataset we propose several method to rank these sentence namely retrieval based entity ranking based and position based we found that traditional bag of word model perform relatively well when there is a match between an entity and a query in a given sentence but they fail to find a support sentence for a substantial portion of entity this can be improved by incorporating small window of context sentence and ranking them appropriately 
web application often use html template to separate the webpage presentation from it underlying business logic and object this is now the de facto standard programming model for web application development this paper proposes a novel implementation for existing server side template engine flyingtemplate for a reduced bandwidth consumption in web application server and b off loading html generation task to web client instead of producing a fully generated html page the proposed template engine produce a skeletal script which includes only the dynamic value of the template parameter and the bootstrap code that run on a web browser at the client side it retrieves a client side template engine and the payload template separately with the goal of efficiency implementation transparency security and standard compliance in mind we developed flyingtemplate with two design principle effective browser cache usage and reasonable compromise which restrict the template usage pattern and relax the security policy slightly but in a controllable way this approach allows typical template based web application to run effectively with flyingtemplate a an experiment we tested the specweb banking application using flyingtemplate without any other modification and saw throughput improvement from x to x in it best mode in addition flyingtemplate can enforce compliance with a simple security policy thus addressing the security problem of client server partitioning in the web environment 
highly interconnected network with amazingly complex topology describe system a diverse a the world wide web our cell social system or the economy recent study indicate that these network are the result of self organizing process governed by simple but generic law resulting in architectural feature that make them much more similar to each other than one would have expected by chance i will discus the amazing order characterizing our interconnected world and it implication to network robustness and spreading process finally most of these network are driven by the temporal pattern characterizing human activity i will use communication and web browsing data to show that there is deep order in the temporal domain of human dynamic and discus the different way to understand and model the emerging pattern 
recent interest on xml semantic web and web ontology among other topic have sparked a renewed interest on graph structured database a fundamental query on graph is the reachability test of node recently hop labeling ha been proposed to index large collection of xml and or graph for efficient reachability test however there ha been few work on update of hop labeling this is compounded by the fact that web data change over time in response to these this paper study the incremental maintenance of hop labeling we identify the main reason for the inefficiency of update of existing hop label we propose two updatable hop labelings hybrid of hop labeling and their incremental maintenance algorithm the proposed hop labeling is derived from graph connectivity a opposed to set cover which is used by all previous work our experimental evaluation illustrates the space efficiency and update performance of various kind of hop labeling the main conclusion is that there is a natural way to spare some index size for update performance in hop labeling 
hypothesis generation is a crucial initial step for making scientific discovery this paper address the problem of automatically discovering interesting hypothesis from the web given a query containing one or two entity of interest our algorithm automatically generates a semantic profile describing the specified entity or provides the potential connection between two entity of interest we implemented a prototype on top of the google search engine and the experimental result demonstrate the effectiveness of our algorithm 
type ahead search is a new information access paradigm in which system can find answer to keyword query on the fly a a user type in a query it improves traditional autocomplete search by allowing query keywords to appear at different place in an answer in this paper we study the problem of automatic url completion and prediction using fuzzy type ahead search that is we interactively find relevant url that contain word matching query keywords even approximately a the user type in a query supporting fuzzy search is very important when the user ha limited knowledge about url we describe the design and implementation of our method and report the experimental result on firefox 
reciprocal rank fusion rrf a simple method for combining the document ranking from multiple ir system consistently yield better result than any individual system and better result than the standard method condorcet fuse this result is demonstrated by using rrf to combine the result of several trec experiment and to build a meta learner that rank the letor dataset better than any previously reported method 
research ha shown that little practical difference exists between the randomization student s paired t and bootstrap test of statistical significance for trec ad hoc retrieval experiment with topic we compared these three test on run with topic size down to topic we found that these test show increasing disagreement a the number of topic decrease at smaller number of topic the randomization test tended to produce smaller p value than the t test for p value le than the bootstrap exhibited a systematic bias towards p value strictly le than the t test with this bias increasing a the number of topic decreased we recommend the use of the randomization test although the t test appears to be suitable even when the number of topic is small 
this paper present a novel method for mining product review where it mine review by identifying product feature expression of opinion and relation between them by taking advantage of the fact that most of product feature are phrase a concept of shallow dependency parsing is introduced which extends traditional dependency parsing to phrase level this concept is then implemented for extracting relation between product feature and expression of opinion experimental evaluation show that the mining task can benefit from shallow dependency parsing 
blog news search engine are very important channel to reach information about the real time happening in this paper we study the popular query collected over one year period and compare their search result returned by a blog search engine i e technorati and a news search engine i e google news we observed that the number of hit returned by the two search engine for the same set of query were highly correlated suggesting that blog often provide commentary to current event reported in news a many popular query are related to some event we further observed a high cohesiveness among the returned search result for these query 
in information retrieval ir research aiming to reduce the cost of retrieval system evaluation ha been conducted along two line i the evaluation of ir system with reduced amount of manual relevance assessment and ii the fully automatic evaluation of ir system thus foregoing the need for manual assessment altogether the proposed method in both area are commonly evaluated by comparing their performance estimate for a set of system to a ground truth provided for instance by evaluating the set of system according to mean average precision in contrast in this poster we compare an automatic system evaluation approach directly to two evaluation based on incomplete manual relevance assessment for the particular case of trec s million query track we show that the automatic evaluation lead to result which are highly correlated to those achieved by approach relying on incomplete manual judgment 
we describe improvement to the use of semantic lexicon by a state of the art query interpretation system powering a major search engine we successfully compute concept label importance information for lexicon string lexicon augmentation with such information lead to a precision increase on affected query with no query coverage loss finally lexicon filtering based on label importance lead to a precision increase but at the expense of query coverage 
this paper identifies and explores the problem of seed selection in a web scale crawler we argue that seed selection is not a trivial but very important problem selecting proper seed can increase the number of page a crawler will discover and can result in a collection with more good and le bad page based on the analysis of the graph structure of the web we propose several seed selection algorithm effectiveness of these algorithm is proved by our experimental result on real web data 
most traditional text clustering method are based on bag of word bow representation based on frequency statistic in a set of document bow however ignores the important information on the semantic relationship between key term to overcome this problem several method have been proposed to enrich text representation with external resource in the past such a wordnet however many of these approach suffer from some limitation wordnet ha limited coverage and ha a lack of effective word sense disambiguation ability most of the text representation enrichment strategy which append or replace document term with their hypernym and synonym are overly simple in this paper to overcome these deficiency we first propose a way to build a concept thesaurus based on the semantic relation synonym hypernym and associative relation extracted from wikipedia then we develop a unified framework to leverage these semantic relation in order to enhance traditional content similarity measure for text clustering the experimental result on reuters and ohsumed datasets show that with the help of wikipedia thesaurus the clustering performance of our method is improved a compared to previous method in addition with the optimized weight for hypernym synonym and associative concept that are tuned with the help of a few labeled data user provided the clustering performance can be further improved 
verbose or long query are a small but significant part of the query stream in web search and are common in other application such a collaborative question answering cqa current search engine perform well with keyword query but are not in general effective for verbose query in this paper we examine query processing technique which can be applied to verbose query prior to submission to a search engine in order to improve the search engine s result we focus on verbose query that have sentence like structure but are not simple wh question and assume the search engine is a black box we evaluated the output of two search engine using query from a cqa service and our result show that among a broad range of technique the most effective approach is to simply reduce the length of the query this can be achieved effectively by removing stop structure instead of only stop word we show that the process of learning and removing stop structure from a query can be effectively automated 
in entity relation er graph v e node v represent typed entity and edge e represent typed relation for dynamic personalized pagerank query node are ranked by their steady state probability obtained using the standard random surfer model in this work we propose a framework to answer top k graph conductance query our top k ranking technique lead to a x speedup and overall our system executes query x faster than whole graph pagerank some query might contain hard predicate i e predicate that must be satisfied by the answer node e g we may seek authoritative paper on public key cryptography but only those written during we extend our system to handle hard predicate our system achieves these substantial query speedup while consuming only of the space taken by a regular text index 
we evaluate a framework for bm f based xml element retrieval the framework gather contextual information associated with each xml element into an associated field which we call a characteristic field the content of the element and the content of the characteristic field are then treated a distinct field for bm f weighting purpose evidence supporting this framework is drawn from both our own experiment and experiment reported in related work 
in this research capture recapture cr model are used to estimate the population of web robot based on web server access log from different website each robot is considered a an individual randomly surfing the web and each website is considered a a trap that record the visitation of robot we use maximum likelihood estimator to fit the observation data result show that there are identifiable robot user agent string and ip address being used by web robot around the world we also examine the origination of the named robot by their ip address the result suggest that over of web robot ip address are from united state and china 
in this paper we study the problem of entity retrieval for news application and the importance of the news trail history i e past related article to determine the relevant entity in current article we construct a novel entity labeled corpus with temporal information out of the trec novelty collection we develop and evaluate several feature and show that an article s history can be exploited to improve it summarization 
although higher order language model lm have shown benefit of capturing word dependency for information retrieval ir the tuning of the increased number of free parameter remains a formidable engineering challenge consequently in many real world retrieval system applying higher order lm is an exception rather than the rule in this study we address the parameter tuning problem using a framework based on a linear ranking model in which different component model are incorporated a feature using unigram and bigram lm with stage smoothing a example we show that our method lead to a bigram lm that outperforms significantly it unigram counterpart and the well tuned bm model 
information retrieval evaluation ha typically been performed over several dozen query each judged to near completeness there ha been a great deal of recent work on evaluation over much smaller judgment set how to select the best set of document to judge and how to estimate evaluation measure when few judgment are available in light of this it should be possible to evaluate over many more query without much more total judging effort the million query track at trec used two document selection algorithm to acquire relevance judgment for more than query we present result of the track along with deeper analysis investigating tradeoff between the number of query and number of judgment show that up to a point evaluation over more query with fewer judgment is more cost effective and a reliable a fewer query with more judgment total assessor effort can be reduced by with no appreciable increase in evaluation error 
in this paper we consider the problem of re ranking search result by incorporating user feedback we present a graph theoretic measure for discriminating irrelevant result from relevant result using a few labeled example provided by the user the key intuition is that node relatively closer in graph topology to the relevant node than the irrelevant node are more likely to be relevant we present a simple sampling algorithm to evaluate this measure at specific node of interest and an efficient branch and bound algorithm to compute the top k node from the entire graph under this measure on quantifiable prediction task the introduced measure outperforms other diffusion based proximity measure which take only the positive relevance feedback into account on the entity relation graph built from the author and paper of the entire dblp citation corpus million node and million edge our branch and bound algorithm take about second to retrieve the top node w r t this measure with labeled node 
we focus on the problem of query rewriting for sponsored search we base rewrite on a historical click graph that record the ad that have been clicked on in response to past user query given a query q we first consider simrank a a way to identify query similar to q i e query whose ad a user may be interested in we argue that simrank fails to properly identify query similarity in our application and we present two enhanced version of simrank one that exploit weight on click graph edge and another that exploit evidence we experimentally evaluate our new scheme against simrank using actual click graph and query form yahoo and using a variety of metric our result show that the enhanced method can yield more and better query rewrite 
there ha been recent interest in collecting user or assessor preference rather than absolute judgment of relevance for the evaluation or learning of ranking algorithm since measure like precision recall and dcg are defined over absolute judgment evaluation over preference will require new evaluation measure that explicitly model them we describe a class of such measure and compare absolute and preference measure over a large trec collection 
using a ground truth extracted from the wikipedia and a ground truth created through manual assessment we show that the apparent performance advantage seen in machine learning approach to link discovery are an artifact of trivial link that are actively rejected by manual assessor 
hypergraph partitioning ha been considered a a promising method to address the challenge of high dimensionality in document clustering with document modeled a vertex and the relationship among document captured by the hyperedges the goal of graph partitioning is to minimize the edge cut therefore the definition of hyperedges is vital to the clustering performance while several definition of hyperedges have been proposed a systematic understanding of desired characteristic of hyperedges is still missing to that end in this paper we first provide a unified clique perspective of the definition of hyperedges which serf a a guide to define hyperedges with this perspective based on the concept of hypercliques and shared reverse nearest neighbor we propose three new type of clique hyperedges and analyze their property regarding purity and size issue finally we present an extensive evaluation using real world document datasets the experimental result show that with shared reverse nearest neighbor based hyperedges the clustering performance can be improved significantly in term of various external validation measure without the need for fine tuning of parameter 
traditional information system return answer after a user submits a complete query user often feel left in the dark when they have limited knowledge about the underlying data and have to use a try and see approach for finding information a recent trend of supporting autocomplete in these system is a first step towards solving this problem in this paper we study a new information access paradigm called interactive fuzzy search in which the system search the underlying data on the fly a the user type in query keywords it extends autocomplete interface by allowing keywords to appear in multiple attribute in an arbitrary order of the underlying data and finding relevant record that have keywords matching query keywords approximately this framework allows user to explore data a they type even in the presence of minor error we study research challenge in this framework for large amount of data since each keystroke of the user could invoke a query on the backend we need efficient algorithm to process each query within millisecond we develop various incremental search algorithm using previously computed and cached result in order to achieve an interactive speed we have deployed several real prototype using these technique one of them ha been deployed to support interactive search on the uc irvine people directory which ha been used regularly and well received by user due to it friendly interface and high efficiency 
this paper investigates subject stopping behavior and estimate of recall in an interactive information retrieval iir experiment subject completed four recall oriented search task and were asked to estimate how many of the relevant document they believed they had found after each task subject also responded to an interview question probing their reason for stopping a search result show that most subject believed they found about of the relevant document and that this estimate wa correlated positively with number of document saved and actual recall even though subject recall estimate were inaccurate reason given for stopping search are also explored 
volunteer computing is a powerful way to harness distributed resource to perform large scale task similarly to other type of community based initiative volunteer computing is based on two pillar the first is computational allocating and managing large computing task the second is participative making large number of individual volunteer their computer resource to a project while the computational aspect of volunteer computing received much research attention the participative aspect remains largely unexplored in this study we aim to address this gap by drawing on social psychology and online community research we develop and test a three dimensional model of the factor determining volunteer computing user contribution we investigate one of the largest volunteer computing project seti home by linking survey data about contributor motivation to their activity log our finding highlight the difference between volunteer computing and other form of community based project and reveal the intricate relationship between individual motivation social affiliation tenure in the project and resource contribution implication for research and practice are discussed 
a typical desktop environment contains many document type email presentation web page pdfs etc each with different metadata predicting which type of document a user is looking for in the context of a given query is a crucial part of providing effective desktop search the problem is similar to selecting resource in distributed ir but there are some important difference in this paper we quantify the impact of type prediction in producing a merged ranking for desktop search and introduce a new prediction method that exploit type specific metadata in addition we show that type prediction performance and search effectiveness can be further enhanced by combining existing method of type prediction using discriminative learning model our experiment employ pseudo desktop collection and a human computation game for acquiring realistic and reusable query 
information retrieval on the www is important because it is hard to find what one is looking for there is a plethora of information available and searching relevant information is a challenge in the case of developing region we have the opposite problem information availability of global market is scarce most of the consumer and producer of information a well a good are relegated to local market in geographical vicinity in order to reach wider market it is important for this local information to reach wider audience local information for global consumption lig model at the same time locally relevant information such a delay in bus train timing mobile medical van schedule change electricity outage timing is not easily available either local information for local consumption lil model we introduce the term information uptrieval to address the reverse problem of acquiring assimilating aggregating and uploading global and local information that is relevant for developing region to a platform that improves the reach of the information while the www is an obvious example of one such platform given the low internet penetration in such region we need to explore effective alternative several innovative but disconnected approach have been attempted to address the information uptrieval problem ranging from the use of dvd esagu http www esagu in esagu through the use of wireless station on motorcycle first mile solution http www firstmilesolutions com many of these have met with reasonable success in their pilot deployment 
evaluation of sentiment analysis like large scale ir evaluation relies on the accuracy of human assessor to create judgment subjectivity in judgment is a problem for relevance assessment and even more so in the case of sentiment annotation in this study we examine the degree to which assessor agree upon sentence level sentiment annotation we show that inter assessor agreement is not contingent on document length or frequency of sentiment but correlate positively with automated opinion retrieval performance we also examine the individual annotation category to determine which category pose most difficulty for annotator 
federated search refers to the brokered retrieval of content from a set of auxiliary retrieval system instead of from a single centralized retrieval system federated search task occur in for example digital library where document from several retrieval system must be seamlessly merged or peer to peer information retrieval where document distributed across a network of local index must be retrieved in the context of web search aggregated search refers to the integration of non web content e g image video news article map tweet into a web search result page this is in contrast with classic web search where user are presented with a ranked list consisting exclusively of general web document a in other federated search situation the non web content is often retrieved from auxiliary retrieval system e g image or video database news index although aggregated search can be seen a an instance of federated search several aspect make aggregated search a unique and compelling research topic these include large source of evidence e g click log for deciding what non web item to return constrained interface e g mobile screen and a very heterogeneous set of available auxiliary resource e g image video map news article each of these aspect introduces problem and opportunity not addressed in the federated search literature aggregated search is an important future research direction for information retrieval all major search engine now provide aggregated search result a the number of available auxiliary resource grows deciding how to effectively surface content from each will become increasingly important the goal of this tutorial is to provide an overview of federated search and aggregated search technique for an intermediate information retrieval researcher at the same time the content will be valuable for practitioner in industry we will take the audience through the most influential work in these area and describe how they relate to real world aggregated search system we will also list some of the new challenge confronted in aggregated search and discus direction for future work 
in this paper we present formsys a web based system that service enables form document it offer two main service filling in form based on web service incoming soap message and invoking web service based on filled in form this can be applied to benefit individual to reduce the number of often repetitive form field they have to complete manually in many scenario it can also help organisation to remove the need for manual data entry by automatically triggering business process implementation based on incoming case data from filled in form while the concept applies to form of any type of document our implementation us adobe acroforms due to it universal applicability availability of a usable api and end user appeal in the demo we will show the two core function namely soap pdf and pdf soap along with use case application of the service developed from real world scenario essentially this work demonstrates how pdfs can be used a a channel for interacting with web service 
in many text retrieval task it is highly desirable to obtain a similarity profile of the document collection for a given query we propose sampling based technique to address this need using calibration technique to improve the accuracy experimental result confirm the effectiveness of the proposed approach 
an unsupervised clustering of the webpage on a website is a primary requirement for most wrapper induction and automated data extraction method since page content can vary drastically across page of one cluster e g all product page on amazon com traditional clustering method typically use some distance function between the dom tree representing a pair of webpage however without knowing which portion of the dom tree are important such distance function might discriminate between similar page based on trivial feature e g differing number of review on two product page or club together distinct type of page based on superficial feature present in the dom tree of both e g matching footer copyright leading to poor clustering performance we propose using search log to automatically find path in the dom tree that mark out important portion of page e g the product title in a product page such path are identified via a global analysis of the entire website whereby search data for popular page can be used to infer good path even for other page that receive little or no search traffic the webpage on the website are then clustered using these key path our algorithm only requires information on search query and the webpage clicked in response to them there is no need for human input and it doe not need to be told which portion of a webpage the user found interesting the resulting clustering achieve an adjusted rand score of over on half of the website a score of indicating a perfect clustering and better score on average than competing algorithm besides leading to refined clustering these key path can be useful in the wrapper induction process itself a shown by the high degree of match between the key path and the manually identified path used in existing wrapper for these site average precision 
the web browser is a cpu intensive program especially on mobile device webpage load too slowly expending significant time in processing a document s appearance due to power constraint most hardware driven speedup will come in the form of parallel architecture this is also true of mobile device such a phone and e book in this paper we introduce new algorithm for cs selector matching layout solving and font rendering which represent key component for a fast layout engine evaluation on popular site show speedup a high a x we also formulate the layout problem with attribute grammar enabling u to not only parallelize our algorithm but prove that it computes in o log time and without reflow 
algorithm are an integral part of computer science literature however none of the current search engine offer specialized algorithm search facility we describe a vertical search engine that identifies the algorithm present in document and extract and index the related metadata and textual description of the identified algorithm this algorithm specific information is then utilized for algorithm ranking in response to user query experimental result show the superiority of our system on other popular search engine 
with the proliferation of data apis it is not uncommon that user who have no clear idea about data apis will encounter difficulty to build mashups to satisfy their requirement in this paper we present a semantic based mashup navigation system smash that make mashup building easy by constructing and visualizing a real life data api network we build a sample network by gathering more than popular apis and find that the relationship between them are so complex that our system will play an important role in navigating user and give them inspiration to build interesting mashups easily the system is accessible at http www dart zju edu cn mashup 
rich medium data such a video imagery music and gaming do no longer play just a supporting role on the world wide web to text data thanks to web rich medium is the primary content on site such a flickr picasaweb youtube and qq because of massive user generated content the volume of rich medium being transmitted on the internet ha surpassed that of text it is vital to properly manage these data to ensure efficient bandwidth utilization to support effective indexing and search and to safeguard copyright just to name a few this panel invite both researcher and practitioner to discus the challenge of web scale medium data management in particular the panelist will address issue such a leveraging rich medium and web indexing search and scalability 
inferring the score distribution of relevant and non relevant document is an essential task for many ir application e g information filtering recall oriented ir meta search distributed ir modeling score distribution in an accurate manner is the basis of any inference thus numerous score distribution model have been proposed in the literature most of the model were proposed on the basis of empirical evidence and goodness of fit in this work we model score distribution in a rather different systematic manner we start with a basic assumption on the distribution of term in a document following the transformation applied on term frequency by two basic ranking function bm and language model we derive the distribution of the produced score for all document then we focus on the relevant document we detach our analysis from particular ranking function instead we consider a model for precision recall curve and given this model we present a general mathematical framework which given any score distribution for all retrieved document produce an analytical formula for the score distribution of relevant document that is consistent with the precision recall curve that follow the aforementioned model in particular assuming a gamma distribution for all retrieved document we show that the derived distribution for the relevant document resembles a gaussian distribution with a heavy right tail 
previous work on cluster based document retrieval ha used either static document cluster that are created offline or query specific dynamic document cluster that are created from top retrieved document we present the potential merit of integrating these two type of cluster 
we present a probabilistic model for generating personalised recommendation of item to user of a web service the matchbox system make use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user user and item are represented by feature vector which are mapped into a low dimensional trait space in which similarity is measured in term of inner product the model can be trained from different type of feedback in order to learn user item preference here we present three alternative direct observation of an absolute rating each user give to some item observation of a binary preference like don t like and observation of a set of ordinal rating on a user specific scale efficient inference is achieved by approximate message passing involving a combination of expectation propagation ep and variational message passing we also include a dynamic model which allows an item s popularity a user s taste or a user s personal rating scale to drift over time by using assumed density filtering adf for training the model requires only a single pas through the training data this is an on line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preference we evaluate the performance of the algorithm on the movielens and netflix data set consisting of approximately and rating respectively this demonstrates that training the model using the on line adf approach yield state of the art performance with the option of improving performance further if computational resource are available by performing multiple ep pass over the training data 
what make template content in the web so special that we need to remove it in this paper i present a large scale aggregate analysis of textual web content corroborating statistical law from the field of quantitative linguistics i analyze the idiosyncrasy of template content compared to regular full text content and derive a simple yet suitable quantitative model 
in this paper we present a folksonomy based approach for implicit user intent extraction during a web search process we present a number of result re ranking technique based on this representation that can be applied to any web search engine we perform a user experiment the result of which indicate that this type of representation is better at context extraction than using the actual textual content of the document 
we consider the problem of detecting epidemic tendency by mining search log we propose an algorithm based on click through information to select epidemic related query term we adopt linear regression to model epidemic occurrence and frequency of epidemic related term ert in search log the result show our algorithm is effective in finding ert which obtain a high correlation value with epidemic occurrence we also find the proposed method performs better when combining different ert than using single ert 
in this paper we present a novel image search system image search by concept map this system enables user to indicate not only what semantic concept are expected to appear but also how these concept are spatially distributed in the desired image to this end we propose a new image search interface to enable user to formulate a query called concept map by intuitively typing textual query in a blank canvas to indicate the desired spatial position of the concept in the ranking process by interpreting each textual concept a a set of representative visual instance the concept map query is translated into a visual instance map which is then used to evaluate the relevance of the image in the database experimental result demonstrate the effectiveness of the proposed system 
the research described in this paper form the backbone of a service that enables the faceted search experience of the yahoo search engine we introduce an approach for a machine learned ranking of entity facet based on user click feedback and feature extracted from three different ranking source the objective of the learned model is to predict the click through rate on an entity facet in an empirical evaluation we compare the performance of gradient boosted decision tree gbdt against a linear combination of feature on two different click feedback model using the raw click through rate ctr and click over expected click coec the result show a significant improvement in retrieval performance in term of discounted cumulated gain when ranking entity facet with gbdt trained on the coec model most notably this is true when evaluated against the ctr test set 
discovery the process under which party to legal case must reveal document relevant to the disputed issue is a core aspect of trial in the united state and a lesser but important factor in other country discovery on document stored in computerized system known variously a electronic discovery e discovery e disco edd and ed is increasingly the major factor in discovery and ha become a multi billion dollar industry i will discus the basic of e discovery the scale and diversity of the material involved and the economics of identifying and reviewing potentially responsive material i will then focus on three major ir area of interest search supervised machine learning including text classification and relevance feedback and interface support for manual relevance assessment for each i will discus technology currently used in e discovery the evaluation method applicable to measuring effectiveness and existing research result not yet seeing commercial practice i will also outline research direction that if successfully pursued would potentially be of great interest in e discovery application a particular focus will be on area where researcher can make progress without access to operational e discovery environment or realistic test collection connection will be drawn with the use of ir in related task such a enterprise search criminal investigation intelligence analysis historical research truth and reconciliation commission and freedom of information open record or sunshine law request 
in a traditional keyword search system over xml data a user composes a keyword query submits it to the system and retrieves relevant subtrees in the case where the user ha limited knowledge about the data often the user feel left in the dark when issuing query and ha to use a try and see approach for finding information in this paper we study a new information access paradigm for xml data called ink in which the system search on the underlying data on the fly a the user type in query keywords ink extends existing xml keyword search method by interactively answering query we propose effective index early termination technique and efficient search algorithm to achieve a high interactive speed we have implemented our algorithm and the experimental result show that our method achieves high search efficiency and result quality 
in batch evaluation of retrieval system performance is calculated based on predetermined relevance judgement applied to a list of document returned by the system for a query this evaluation paradigm however ignores the current standard operation of search system which require the user to view summary of document prior to reading the document themselves in this paper we modify the popular ir metric map and p to incorporate the summary reading step of the search process and study the effect on system ranking using trec data based on a user study we establish likely disagreement between relevance judgement of summary and of document and use these value to seed simulation of summary relevance in the trec data re evaluating the run submitted to the trec web track we find the average correlation between system ranking and the original trec ranking is kendall which is lower than commonly accepted for system ordering to be considered equivalent the system that ha the highest map in trec generally remains amongst the highest map system when summary are taken into account but other system become equivalent to the top ranked system depending on the simulated summary relevance given that system ordering alter when summary are taken into account the small amount of effort required to judge summary in addition to document second v second on average in our data should be undertaken when constructing test collection 
information retrieval ir evaluation score are generally designed to measure the effectiveness with which relevant document are identified and retrieved many score have been proposed for this purpose over the year these have primarily focused on aspect of precision and recall and while these are often discussed with equal importance in practice most attention ha been given to precision focused metric even for recall oriented ir task of growing importance such a patent retrieval these precision based score remain the primary evaluation measure our study examines different evaluation measure for a recall oriented patent retrieval task and demonstrates the limitation of the current score in comparing different ir system for this task we introduce pres a novel evaluation metric for this type of application taking account of recall and the user s search effort the behaviour of pres is demonstrated on run from the clef ip patent retrieval track a full analysis of the performance of pres show it suitability for measuring the retrieval effectiveness of system from a recall focused perspective taking into account the user s expected search effort 
we present a novel approach for filtering xml document using nondeterministic finite automaton and distributed hash table our approach differs architecturally from recent proposal that deal with distributed xml filtering they assume an xml broker architecture whereas our solution is built on top of distributed hash table the essence of our work is a distributed implementation of yfilter a state of the art automaton based xml filtering system on top of chord we experimentally evaluate our approach and demonstrate that our algorithm can scale to million of xpath query under various filtering scenario and also exhibit very good load balancing property 
most template detection method process web page in batch that a newly crawled page can not be processed until enough page have been collected this result in large storage consumption and a huge delay of data refreshing in this paper we present an incremental framework to detect template in which a page is processed a soon a it ha been crawled in this framework we don t need to cache any web page experiment show that our framework consumes le than storage than traditional method and also the speed of data refreshing is accelerated because of the incremental manner 
regression testing assures the quality of modified service oriented business application against unintended change however a typical regression test suite is large in size earlier execution of those test case that may detect failure is attractive many existing prioritization technique order test case according to their respective coverage of program statement in a previous version of the application on the other hand industrial service oriented business application are typically written in orchestration language such a w bpel and integrated with workflow step and web service via xpath and wsdl fault in these artifact may cause the application to extract wrong data from message leading to failure in service composition surprisingly current regression testing research hardly considers these artifact we propose a multilevel coverage model to capture the business process xpath and wsdl from the perspective of regression testing we develop a family of test case prioritization technique atop the model empirical result show that our technique can achieve significantly higher rate of fault detection than existing technique 
web search engine often federate many user query to relevant structured database for example a product related query might be federated to a product database containing their description and specification the relevant structured data item are then returned to the user along with web search result however each structured database is searched in isolation hence the search often produce empty or incomplete result a the database may not contain the required information to answer the query in this paper we propose a novel integrated search architecture we establish and exploit the relationship between web search result and the item in structured database to identify the relevant structured data item for a much wider range of query our architecture leverage existing search engine component to implement this functionality at very low overhead we demonstrate the quality and efficiency of our technique through an extensive experimental study 
mashup application mix and merge content data and code from multiple content provider in a user s browser to provide high value web application that can rival the user experience provided by desktop application current browser security model were not designed to support such application and they are therefore implemented with insecure workarounds in this paper we present a secure component model where component are provided by different trust domain and can interact using a communication abstraction that allows ease of specification of a security policy we have developed an implementation of this model that work currently in all major browser and address challenge of communication integrity and frame phishing an evaluation of the performance of our implementation show that this approach is not just feasible but also practical 
exploiting information induced from query specific c lustering of top retrieved document ha long been proposed a mean for improving precision at the very top rank of the returned result we present a novel language model approach to ranking query specific cluster by the presumed percentage of relevant document that they contain while most previous cluster ranking approach focus on the cluster a a whole our model also exploit information induced from document associated with the cluster our model substantially outperforms previous approach for identifying cluster containing a high relevant document percentage furthermore using the model to produce document ranking yield precision at top rank performance that is consistently better than that of the initial ranking upon which clustering is performed the performance also favorably compare with that of a state of the art pseudo feedback retrieval method 
in this poster paper we present a preliminary study on the predilection of web search engine towards various online news medium provider site using an access based measure 
the utility of query performance prediction qpp method is commonly evaluated by reporting correlation coefficient to denote how well the method perform at predicting the retrieval performance of a set of query however a quintessential question remains unexplored how strong doe the correlation need to be in order to realize an increase in retrieval performance in this work we address this question in the context of selective query expansion sqe and perform a large scale experiment the result show that to consistently and predictably improve retrieval effectiveness in the ideal sqe setting a kendall s tau correlation of tau is required a threshold which most existing query performance prediction method fail to reach 
along with the explosive growth of the world wide web an immense industry for the production and consumption of pornography ha grown though the censorship and legal restraint on pornography are discriminating in different historical cultural and national context selling pornography to minor is not allowed in most case detecting human skin tone is of utmost importance in pornography image filtering algorithm in this paper we propose two patch based skin color detection algorithm regular patch and irregular patch skin color detection algorithm on the basis of skin detection we extract dimensional feature from the input image and these feature are fed into a random forest classifier our algorithm ha been incorporated into an adult content filtering infrastructure and is now in active use for preventing minor from accessing pornographic image via mobile phone 
on the web of today the most prevalent solution for user to interact with data intensive application is the use of form based interface composed by several data input field such a text box radio button pull down list check box etc although these interface are popular and effective in many case free text interface are preferred over form based one in this paper we discus the proposal and the implementation of a novel ir based method for using data rich free text to interact with form based interface our solution take a free text a input extract implicitly data value from it and fill appropriate field using them for this task we rely on value of previous submission for each field which are freely obtained from the usage of form based interface 
the tag on social medium website such a flickr are frequently imprecise and incomplete thus there is still a gap between these tag and the actual content of the image this paper proposes a social image retagging scheme that aim at assigning image with better content descriptor the refining process is formulated a an optimization framework based on the consistency between visual similarity and semantic similarity in social image an effective iterative bound optimization algorithm is applied to learn the optimal tag assignment in addition a many tag are intrinsically not closely related to the visual content of the image we employ a knowledge based method to differentiate visual content related from unrelated tag and then constrain the tagging vocabulary of our automatic algorithm within the content related tag experimental result on a flickr image collection demonstrate the effectiveness of this approach 
combining multiple document to represent an information object is well known a an effective approach for many information retrieval task for example passage can be combined to represent a document for retrieval document cluster are represented using combination of the document they contain and feedback document can be combined to represent a query model various technique for combination have been introduced and among them representation technique based on concatenation and the arithmetic mean are frequently used some recent work ha shown the potential of a new representation technique using the geometric mean however these study lack a theoretical foundation explaining why the geometric mean should have advantage for representing multiple document in this paper we show that the arithmetic mean and the geometric mean are approximation to the center of mass in certain geometry and show empirically that the geometric mean is closer to the center through experiment with two ir task we show the potential benefit for geometric representation including a geometry based pseudo relevance feedback method that outperforms state of the art technique 
this poster explains the change introduced in the web content accessibility guideline wcag from wcag and proposes a checklist for adapting existing website finally it describes the most common criticism of the wcag and place them in the context of it origin and initial aim 
i have shown that the presence of difficult query aspect that are revealed only implicitly e g exploration opposition achievement cooperation risk can be improved by taking advantage of the known presence of other easier to verify query aspect the approach proceeds by mining a large external corpus and result in substantial improvement in re ranking the subset of the top retrieved document 
most retrieval model estimate the relevance of each document to a query and rank the document accordingly however such an approach ignores the uncertainty associated with the estimate of relevancy if a high estimate of relevancy also ha a high uncertainty then the document may be very relevant or not relevant at all another document may have a slightly lower estimate of relevancy but the corresponding uncertainty may be much le in such a circumstance should the retrieval engine risk ranking the first document highest or should it choose a more conservative safer strategy that give preference to the second document there is no definitive answer to this question a it depends on the risk preference of the user and the information retrieval system in this paper we present a general framework for modeling uncertainty and introduce an asymmetric loss function with a single parameter that can model the level of risk the system is willing to accept by adjusting the risk preference parameter our approach can effectively adapt to user different retrieval strategy we apply this asymmetric loss function to a language modeling framework and a practical risk aware document scoring function is obtained our experiment on several trec collection show that our risk averse approach significantly improves the jelinek mercer smoothing language model and a combination of our risk averse approach and the jelinek mercer smoothing method generally outperforms the dirichlet smoothing method experimental result also show that the risk averse approach even without smoothing from the collection statistic performs a well a three commonly adopted retrieval model namely the jelinek mercer and dirichlet smoothing method and bm model 
in the origin detection problem an algorithm is given a set s of document ordered by creation time and a query document d it need to output for every consecutive sequence of k alphanumeric term in d the earliest document in s in which the sequence appeared if such a document exists algorithm for the origin detection problem can for example be used to detect the origin of text segment in d and thus to detect novel content in d they can also find the document from which the author of d ha copied the most or show that d is mostly original we concentrate on solution that use only a fixed amount of memory we propose novel algorithm for this problem and evaluate them together with a large number of previously published algorithm our result show that detecting the origin of text segment efficiently can be done with very high accuracy even when the space used is le than of the size of the document in s the precision degrades smoothly with the amount of available space various estimation technique can be used to increase the performance of the algorithm 
web application like flickr youtube or del icio u are increasingly popular online community for creating editing and sharing content the growing size of these folksonomies pose new challenge in term of search and data mining in this paper we introduce a novel methodology for automatically ranking and classifying photo according to their attractiveness for folksonomy member to this end we exploit image feature known for having significant effect on the visual quality perceived by human e g sharpness and colorfulness a well a textual meta data in what is a multi modal approach using feedback and annotation available in the web photo sharing system flickr we assign relevance value to the photo and train classification and regression model based on these relevance assignment with the resulting machine learning model we categorize and rank photo according to their attractiveness application include enhanced ranking function for search and recommender method for attractive content large scale experiment on a collection of flickr photo demonstrate the viability of our approach 
logistic average misclassification percentage lam is a key measure for the spam filtering performance this paper demonstrates that a spam filter can achieve a perfect in lam the minimal value in theory by simply setting a biased threshold during the classifier modeling at the same time the overall classification performance reach only a low accuracy the result suggests that the role of lam for spam filtering evaluation should be re examined 
a sophisticated enterprise application move to the web some advanced user experience become difficult to migrate due to prohibitively high computation memory and bandwidth requirement state dependent visualization of large scale data set are particularly difficult since a change in the client s context necessitates a change in the displayed result this paper describes a web architecture where client are served a session specific image of the data with this image divided into tile dynamically generated by the server this set of tile is supplemented with a corpus of metadata describing the immediate vicinity of interest additional metadata is delivered a needed in a progressive fashion in support and anticipation of the user s action we discus how the design of this architecture wa motivated by the goal of delivering a highly responsive user experience a an example of a complete application built upon this architecture we present orgmaps an interactive system for navigating hierarchical data enabling fluid low latency navigation of tree of hundred of thousand of node on standard web browser using only html and javascript 
retrieval with logical imaging is derived from belief revision and provides a novel mechanism for estimating the relevance of a document through logical implication i e p q d in this poster we perform the first comprehensive evaluation of logical imaging li in information retrieval ir across several trec test collection when compared against standard baseline model we show that li fails to improve performance this failure can be attributed to a nuance within the model that mean non relevant document are promoted in the ranking while relevant document are demoted this is an important contribution because it not only contextualizes the effectiveness of li but crucially explains why it fails by addressing this nuance future li model could be significantly improved 
ranking is an essential part of information retrieval ir task such a web search nowadays there are hundred of feature for ranking so learning to rank ltr an interdisciplinary field of ir and machine learning ml ha attracted increasing attention those feature used in the ir are not always independent from each other hence the feature selection an important issue in ml should be paid attention to for ltr however the state of the art ltr approach merely analyze the connection among the feature from the aspect of feature selection in this paper we propose a hierarchical feature selection strategy containing phase for ranking and learn ranking function the experimental result show that ranking function based on the selected feature subset significantly outperform the one based on all feature 
we propose a method for improving classification performance in a one class setting by combining classifier of different modality we apply the method to the problem of distinguishing responsive document in a corpus of e mail like enron corpus we extract the social network of actor which is implicit in a large body of electronic communication and turn it into valuable feature for classifying the exchanged document working in a one class setting we folow a semi supervised approach based on the mapping convergence framework we propose an alternative interpretation that allows for broader applicability when positive and negative item are not naturally separable we propose an extension to the one class evaluation framework in truly one case case when only some positive training example are available we extent the one class setting to the co training principle that enables u to take advantage of multiple view on the data we report evaluation result of this extension on three different corpus including enron corpus 
with the development of semantic web in recent year an increasing amount of semantic data ha been created in form of resource description framework rdf current visualization technique help user quickly understand the underlying rdf data by displaying it structure in an overview however detailed information can only be accessed by further navigation an alternative approach is to display the global context a well a the local detail simultaneously in a unified view this view support the visualization and navigation on rdf data in an integrated way in this demonstration we present zoomrdf a framework that i adapts a space optimized visualization algorithm for rdf which allows more resource to be displayed thus maximizes the utilization of display space ii combine the visualization with a fisheye zooming concept which assigns more space to some individual node while still preserving the overview structure of the data iii considers both the importance of resource and the user interaction on them which offer more display space to those element the user may be interested in we implement the framework based on the gene ontology and demonstrate that it facilitates task like rdf data exploration and editing 
child face several challenge when using information access system these include formulating query judging the relevance of document and focusing attention on interface cue such a query suggestion while typing query it ha also been shown that child want a personalised web experience and prefer content presented to them that match their long term entertainment and education need to this end we have developed an interaction based information filtering system to address these challenge 
recently portfolio theory pt ha been proposed for information retrieval however under non trivial condition pt violates the original probability ranking principle prp in this poster we shall explore whether pt upholds a different ranking principle based on quantum theory i e the quantum probability ranking principle qprp and examine the relationship between this new model and the new ranking principle we make a significant contribution to the theoretical development of pt and show that under certain circumstance pt upholds the qprp and thus guarantee an optimal ranking according to the qprp a practical implication of this finding is that the parameter of pt can be automatically estimated via the qprp instead of resorting to extensive parameter tuning 
the aim of an opinion finding system is not just to retrieve relevant document but to also retrieve document that express an opinion towards the query target entity in this work we propose a way to use and integrate an opinion identification toolkit opinionfinder into the retrieval process of an information retrieval ir system such that opinionated relevant document are retrieved in response to a query in our experiment we vary the number of top ranked document that must be parsed in response to a query and investigate the effect on opinion retrieval performance and required parsing time we find that opinion finding retrieval performance is improved by integrating opinionfinder into the retrieval system and that retrieval performance grows a more post are parsed by opinionfinder however the benefit eventually tail off at a deep rank suggesting that an optimal setting for the system ha been achieved 
a key to success to contextual in video advertising is finding advertising keywords on video content effectively but there ha been little literature in the area so far this paper present some preliminary result of our learning based system that find relevant advertising keywords on particular scene of video content using their script the system is trained with not only feature proven useful in earlier study but novel feature that reflect the situation of a targeted scene experimental result show that the new feature are potentially helpful for enhancing the accuracy of keyword extraction for contextual in video advertising 
in this paper client site web mashups are studied from component oriented perspective and compoweb a componentoriented web architecture is proposed in compoweb a web application is decomposed into web component called gadget a gadget is an abstraction of functional or logical web component it is isolated from other gadget for security and reliability contract based channel are the only way to interact with each other an abstraction of contract based channel supported or required by a gadget is also presented it enables binding of gadget at deployment and promotes interchangeable gadget unlike the model of a normal function call where the function logic is executed in caller s context compoweb ensures that the function logic is executed in callee s context so that both the caller and callee are protected implementation of a prototype compoweb system and it performance are also presented 
current blog opinion retrieval approach cannot be applied if the topic relevance and opinion score distribution by rank are dissimilar this problem severely limit the feasibility of these approach we propose to tackle this problem by fitting the distribution of opinion score which replaces the original topic relevance score distribution with the simulated one our proposed score distribution fitting method markedly enhances the feasibility of a state of the art dictionary based opinion retrieval approach evaluation on a standard trec blog test collection show significant improvement over high quality topic relevance baseline 
we propose three heuristic to determine the country of origin of a person or institution via text based ie from the web we evaluate all method on a collection of music artist and band and show that some heuristic outperform earlier work on the topic by term of coverage while retaining similar precision level we further investigate an extension using country specific synonym list 
it is well known that anchor text play a critical role in a variety of search task performed over hypertextual domain including enterprise search wiki search and web search it is common practice to enrich a document s standard textual representation with all of the anchor text associated with it incoming hyperlink however this approach doe not help match relevant page with very few inlinks in this paper we propose a method for overcoming anchor text sparsity by enriching document representation with anchor text that ha been aggregated across the hyperlink graph this aggregation mechanism act to smooth or diffuse anchor text within a domain we rigorously evaluate our proposed approach on a large web search test collection our result show the approach significantly improves retrieval effectiveness especially for longer more difficult query 
this paper address the blog distillation problem that is given a user query find the blog most related to the query topic we model the blogosphere a a single graph that includes extra information besides the content of the post by performing a random walk on this graph we extract most relevant blog for each query our experiment on the trec data set show improvement in map and improvement in precision over the language modeling baseline 
a an alternative to previous study on extracting class attribute from unstructured text which consider either web document or query log a the source of textual data a bootstrapped method extract class attribute simultaneously from both source using a small set of seed attribute the method improves extraction precision and also improves attribute relevance across test class 
existing keyword suggestion tool from various search engine company could automatically suggest keywords related to the advertiser product or service counting in simple statistic of the keywords such a search volume cost per click cpc etc however the nature of the generalized second price auction suggests that better understanding the competitor keyword selection and bidding strategy better help to win the auction other than only relying on general search statistic in this paper we propose a novel keyword suggestion strategy called competitive analysis to explore the keyword based competition relationship among advertiser and eventually help advertiser to build campaign with better performance the experimental result demonstrate that the proposed competitive analysis can both help advertiser to promote their product selling and generate more revenue to the search engine company 
this demo will present hearsay a multi modal non visual web browser which aim to bridge the growing web accessibility divide between individual with visual impairment and their sighted counterpart and to facilitate full participation of blind individual in the growing web based society 
several study have found that the cranfield approach to evaluation can report significant performance difference between retrieval system for which little to no performance difference is found for human completing task with these system we revisit the relationship between precision and performance by measuring human performance on tightly controlled search task and with user interface offering limited interaction we find that human performance and retrieval precision are strongly related we also find that user change their relevance judging behavior based on the precision of the result this change in behavior coupled with the well known lack of perfect inter assessor agreement can reduce the measured performance gain predicted by increased precision 
like traditional supervised and semi supervised algorithm learning to rank for information retrieval requires document annotation provided by domain expert it is costly to annotate training data for different search domain and task we propose to exploit training data annotated for a related domain to learn to rank retrieved document in the target domain in which no labeled data is available we present a simple yet effective approach based on instance weighting scheme our method first estimate the importance of each related domain document relative to the target domain then heuristic are studied to transform the importance of individual document to the pairwise weight of document pair which can be directly incorporated into the popular ranking algorithm due to importance weighting ranking model trained on related domain is highly adaptable to the data of target domain ranking adaptation experiment on letor dataset demonstrate that with a fair amount of related domain training data our method significantly outperforms the baseline without weighting and most of time is not significantly worse than an ideal model directly trained on target domain 
we describe probabilistic model that leverage individual blog post evidence to improve blog seed retrieval performance our model offer a intuitive and principled method to combine multiple post in scoring a whole blog site by treating individual post a hidden variable when applied to the seed retrieval task our model yield state of the art result on the trec blog distillation task dataset 
recent technology trend in the web service w domain indicate that a solution eliminating the presumed complexity of the w standard may be in sight advocate of representational state transfer rest have come to believe that their idea explaining why the world wide web work are just a applicable to solve enterprise application integration problem and to simplify the plumbing required to build service oriented architecture in this paper we objectify the w v rest debate by giving a quantitative technical comparison based on architectural principle and decision we show that the two approach differ in the number of architectural decision that must be made and in the number of available alternative this discrepancy between freedom from choice and freedom of choice explains the complexity difference perceived however we also show that there are significant difference in the consequence of certain decision in term of resulting development and maintenance cost our comparison help technical decision maker to ass the two integration style and technology more objectively and select the one that best fit their need rest is well suited for basic ad hoc integration scenario w is more flexible and address advanced quality of service requirement commonly occurring in enterprise computing 
it ha been a constant aim of computer scientist programming language designer and practitioner to raise the level of programming abstraction and bring them a close to the user s natural context a possible the effort started right from our transition from machine code programming to assembly language programming from there to high level procedural language followed by object oriented programming nowadays service oriented software development and composition are the norm there have also been notable effort such a alice system from cmu to simplify the programming experience through the use of d virtual world the holy grail ha been to enable non technical user such a kid or non technical people to be able to understand and pick up programming and software development easily we present a novel approach to software development that let people use their voice to program or create new software through composition we demonstrate some basic programming task achieved by simply talking to a system over an ordinary phone such program constructed by talking can be created in user s local language and do not require it literacy or even literacy a a prerequisite we believe this approach will have a deep impact on software development especially development of web based software in the very near future 
the context of a search query often provides a search engine meaningful hint for answering the current query better previous study on context aware search were either focused on the development of context model or limited to a relatively small scale investigation under a controlled laboratory setting particularly about context aware ranking for web search the following two critical problem are largely remained unsolved first how can we take advantage of different type of context in ranking second how can we integrate context information into a ranking model in this paper we tackle the above two essential problem analytically and empirically we develop different ranking principle for different type of context moreover we adopt a learning to rank approach and integrate the ranking principle into a state of the art ranking model by encoding the context information a feature of the model we empirically test our approach using a large search log data set obtained from a major commercial search engine our evaluation us both human judgment and implicit user click data the experimental result clearly show that our context aware ranking approach improves the ranking of a commercial search engine which ignores context information furthermore our method outperforms a baseline method which considers context information in ranking 
incorporating feature extracted from clickthrough data called clickthrough feature ha been demonstrated to significantly improve the performance of ranking model for web search application such benefit however are severely limited by the data sparseness problem i e many query and document have no or very few click the ranker thus cannot rely strongly on clickthrough feature for document ranking this paper present two smoothing method to expand clickthrough data query clustering via random walk on click graph and a discounting method inspired by the good turing estimator both method are evaluated on real world data in three web search domain experimental result show that the ranking model trained on smoothed clickthrough feature consistently outperform those trained on unsmoothed feature this study demonstrates both the importance and the benefit of dealing with the sparseness problem in clickthrough data 
rich medium social network promote not only creation and consumption of medium but also communication about the posted medium item what cause a conversation to be interesting that prompt a user to participate in the discussion on a posted video we conjecture that people participate in conversation when they find the conversation theme interesting see comment by people whom they are familiar with or observe an engaging dialogue between two or more people absorbing back and forth exchange of comment importantly a conversation that is interesting must be consequential i e it must impact the social network itself our framework ha three part characterizing theme characterizing participant for determining interestingness and measure of consequence of a conversation deemed to be interesting first we detect conversational theme using a mixture model approach second we determine interestingness of participant and interestingness of conversation based on a random walk model third we measure the consequence of a conversation by measuring how interestingness affect the following three variable participation in related theme participant cohesiveness and theme diffusion we have conducted extensive experiment using dataset from the popular video sharing site youtube our result show that our method of interestingness maximizes the mutual information and is significantly better twice a large than three other baseline method number of comment number of new participant and pagerank based assessment 
service discovery employ matching technique to select service by comparing their description against user constraint semantic based matching approach achieve higher recall than syntactic based one a they employ ontological reasoning mechanism to match syntactically heterogeneous description however semantic based approach still have problem e g lack of scalability a an exhaustive search is often performed to located service conforming to constraint this paper proposes two approach that deal with the problem of scalability performance for composite service location first service are indexed based on the value they assign to their restricted attribute the attribute restricted by a given constraint then service that assign conforming value to those attribute are combined to form composite service the first proposed approach extends a local optimisation technique to perform the latter since identifying such value is np hard however this approach return false negative since the local optimisation technique doe not consider all the value hence a second approach that derives conforming value using domain rule is defined the used rule are returned with each composite service so that a user can understand the context in which it is retrieved result obtained from the experiment that varied the number of available service demonstrate that the performance of the local optimisation based approach is better than existing semantic based approach and recall is higher than syntactic based approach 
much research in learning to rank ha been placed on developing sophisticated learning method treating the training set a a given however the number of judgment in the training set directly aff ect the quality of the learned system given the expense of obtaining relevance judgment for constructing training data one often ha a limited budget in term of how many judgment he can get the major problem then is how to distribute this judgment e ffort across diff erent query in this paper we investigate the tradeo ff between the number of query and the number of judgment per query when training set are constructed in particular we show that up to a limit training set with more query but shallow le judgment per query are more cost effective than training set with le query but deep more judgment per query 
in web based service of dynamic content such a news article recommender system face the difficulty of timely identifying new item of high quality and providing recommendation for new user we propose a feature based machine learning approach to personalized recommendation that is capable of handling the cold start issue effectively we maintain profile of content of interest in which temporal characteristic of the content e g popularity and freshness are updated in real time manner we also maintain profile of user including demographic information and a summary of user activity within yahoo property based on all feature in user and content profile we develop predictive bilinear regression model to provide accurate personalized recommendation of new item for both existing and new user this approach result in an offline model with light computational overhead compared with other recommender system that require online re training the proposed framework is general and flexible for other personalized task the superior performance of our approach is verified on a large scale data set collected from the today module on yahoo front page with comparison against six competitive approach 
abstract this paper share our experience in designing a web crawler that can download billion of page using a singleserver implementation and model it performance we show that with the quadratically increasing complexity of verifying url uniqueness bfs crawl order and fixed per host ratelimiting current crawling algorithm cannot effectively cope with the sheer volume of url generated in large crawl highly branching spam legitimate multi million page blog site and infinite loop created by server side script we offer a set of technique for dealing with these issue and test their performance in an implementation we call irlbot in our recent experiment that lasted day irlbot running on a single server successfully crawled billion valid html page billion connection request and sustained an average download rate of mb s page s unlike our prior experiment with algorithm proposed in related work this version of irlbot did not experience any bottleneck and successfully handled content from over million host parsed out billion link and discovered a subset of the web graph with billion unique node 
audio identification via fingerprint ha been an active research field with wide application for year many technical paper were published and commercial software system were also employed however most of these previously reported method work on the raw audio format in spite of the fact that nowadays compressed format audio especially mp music ha grown into the dominant way to store on personal computer and transmit on the internet it would be interesting if a compressed unknown audio fragment is able to be directly recognized from the database without the fussy and time consuming decompression identification recompression procedure so far very few algorithm run directly in the compressed domain for music information retrieval and most of them take advantage of mdct coefficient or derived energy type of feature a a first attempt we propose in this paper utilizing compressed domain spectral entropy a the audio feature to implement a novel audio fingerprinting algorithm the compressed song stored in a music database and the possibly distorted compressed query excerpt are first partially decompressed to obtain the mdct coefficient a the intermediate result then by grouping granule into longer block remapping the mdct coefficient into new frequency line to unify the frequency distribution of long and short window and defining new subbands which cover the main frequency bandwidth of popular song in accordance with the scale factor band of short window we calculate the spectral entropy of all consecutive block and come to the final fingerprint sequence by mean of magnitude relationship modeling experiment show that such fingerprint exhibit strong robustness against various audio signal distortion like recompression noise interference echo addition equalization band pas filtering pitch shifting and slight time scale modification etc for s long query example which might be severely degraded an average top five retrieval precision rate of more than can be obtained in our test data set composed of popular song 
typical approach for querying structured web data collect crawl and pre process index large amount of data in a central data repository before allowing for query answering however this time consuming pre processing phase however leverage the benefit of linked data where structured data is accessible live and up to date at distributed web resource that may change constantly only to a limited degree a query result can never be current an ideal query answering system for linked data should return current answer in a reasonable amount of time even on corpus a large a the web query processor evaluating query directly on the live source require knowledge of the content of data source in this paper we develop and evaluate an approximate index structure summarising graph structured content of source adhering to linked data principle provide an algorithm for answering conjunctive query over linked data on theweb exploiting the source summary and evaluate the system using synthetically generated query the experimental result show that our lightweight index structure enables complete and up to date query result over linked data while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost 
both search engine click through log and social annotation have been utilized a user feedback for search result re ranking however to our best knowledge no previous study ha explored the correlation between these two factor for the task of search result ranking in this paper we show that the gap between search query and social tag of the same web page can well reflect it user preference score motivated by this observation we propose a novel algorithm called query tag gap qtg to re rank search result for better user satisfaction intuitively on one hand the search user intention are generally described by their query before they read the search result on the other hand the web annotator semantically tag web page after they read the content of the page the difference between user recognition of the same page before and after they read it is a good reflection of user satisfaction in this extended abstract we formally define the query set and tag set of the same page a user preand postknowledge respectively we empirically show the strong correlation between user satisfaction and user s knowledge gap before and after reading the page based on this gap experiment have shown outstanding performance of our proposed qtg algorithm in search result re ranking 
query translation is an important task in cross language information retrieval clir aiming to translate query into language used in document the purpose of this paper is to investigate the necessity of translating query term which might differ from one term to another some untranslated term cause irreparable performance drop while others do not we propose an approach to estimate the translation probability of a query term which help decide if it should be translated or not the approach learns regression and classification model based on a rich set of linguistic and statistical property of the term experiment on ntcir and ntcir english chinese clir task demonstrate that the proposed approach can significantly improve clir performance an in depth analysis is also provided for discussing the impact of untranslated out of vocabulary oov query term and translation quality of non oov query term on clir performance 
we present the conceptual framework of the social honeypot project for uncovering social spammer who target online community and initial empirical result from twitter and myspace two of the key component of the social honeypot project are the deployment of social honeypot for harvesting deceptive spam profile from social networking community and statistical analysis of the property of these spam profile for creating spam classifier to actively filter out existing and new spammer 
advertiser use sponsored search to drive traffic to their site at a conversion rate and cost per conversion that provides value to them however very often advertiser bid at a constant price on a bundle of keywords either for lack of enough data to fully optimize their bid at a keyword level or indirectly by opting into advanced matching am that allows an advertiser to reach a large number of query while explicitly bidding only on a limited number then this single bid price reflects the return the advertiser get from the full bundle under these condition the advertiser is competing too aggressively for some keyword auction and with too low bid for others in this paper we propose a solution to improve the fairness of each keyword s bid price within an am bundle adjusting the am keyword bid by the ratio of it conversion rate to the conversion rate it would have reached had it been an exact match em first we describe how we measure advertiser conversion rate despite the opt in nature of conversion tracking and illustrate the need for bid adjustment in the context of am then we present our approach to predict conversion rate in a robust manner our model us a number of feature capturing the quality of the match between the ad and the query then we describe how we adjust keyword bid price to reflect their value to the advertiser thereby improving the auction through fewer incorrectly high bid in the auction advertiser return through more auction won by high value keywords and le by low value keywords and user satisfaction through higher conversion rate finally we present experimental result from our live system 
detection of web attack is an important issue in current defense in depth security framework in this paper we propose a novel general framework for adaptive and online detection of web attack the general framework can be based on any online clustering method a detection model based on the framework is able to learn online and deal with concept drift in web audit data stream str dbscan that we extended dbscan to streaming data a well a strap are both used to validate the framework the detection model based on the framework automatically label the web audit data and adapts to normal behavior change while identifies attack through dynamical clustering of the streaming data a very large size of real http log data collected in our institute is used to validate the framework and the model the preliminary testing result demonstrated it effectiveness 
a novel and efficient learning algorithm is proposed for the binary linear classification problem the algorithm is trained using the rocchio s relevance feedback technique and build a classifier by the intermediate hyperplane of two common tangent hyperplanes for the given category and it complement experimental result presented are very encouraging and justify the need for further research 
motivated by the emergence of auction based marketplace for display ad such a the right medium exchange we study the design of a bidding agent that implement a display advertising campaign by bidding in such a marketplace the bidding agent must acquire a given number of impression with a given target spend when the highest external bid in the marketplace is drawn from an unknown distribution p the quantity and spend constraint arise from the fact that display ad are usually sold on a cpm basis we consider both the full information setting where the winning price in each auction is announced publicly and the partially observable setting where only the winner obtains information about the distribution these differ in the penalty incurred by the agent while attempting to learn the distribution we provide algorithm for both setting and prove performance guarantee using bound on uniform closeness from statistic and technique from online learning we experimentally evaluate these algorithm both algorithm perform very well with respect to both target quantity and spend further our algorithm for the partially observable case performs nearly a well a that for the fully observable setting despite the higher penalty incurred during learning 
traditionally web application have required an internet connection in order to work with data browser have lacked any mechanism to allow web application to operate offline with a set of data to provide constant access to application recently through browser plug in such a google gear browser have gained the ability to persist data for offline use however until now it s been difficult for a web developer using these plug in to manage persisting data both locally for offline use and in the internet cloud due to synchronization requirement managing throughput and latency to the cloud and making it work within the confines of a standard compliant web browser historically in non browser environment programming language environment have offered automated object persistence to shield the developer from these complexity in our research we have created a framework which introduces automated persistence of data object for javascript utilizing the internet unlike traditional object persistence solution ours relies only on existing or forthcoming internet standard and doe not rely upon specific runtime mechanism such a o or interpreter compiler support a new design wa required in order to be suitable to the internet s unique characteristic of varying connection quality and a browser s specific restriction we validate our approach using benchmark which show that our framework can handle thousand of data object automatically reducing the amount of work needed by developer to support offline web application 
it cost about m dollar to just build a search system that scale to the web web service which open up web search result to the public allow academic developer and entrepreneur to achieve instant web search parity for free and enable them to focus on building their additional secret sauce on top to create even grander relevant service for example a social network could leverage open search and their data about user to personalize web search additionally one of the best way to gather data about web search behavior is to build your own search system proto type ir and web search system based on open search can be used to gather user interaction data and test the applicability of research idea open source tool like lucene and nutch and open search service like from major search engine can greatly help developer implement these type of system quickly allowing for fast production and evaluation we will give detailed overview of the popular open search tool showcasing example of search and ir algorithm and system implemented using them a well a discussing how evaluation can be performed 
capturing the context of a user s query from the previous query and click in the same session may help understand the user s information need a context aware approach to document re ranking query suggestion and url recommendation may improve user search experience substantially in this paper we propose a general approach to context aware search to capture context of query we learn a variable length hidden markov model vlhmm from search session extracted from log data although the mathematical model is intuitive how to learn a large vlhmm with million of state from hundred of million of search session pose a grand challenge we develop a strategy for parameter initialization in vlhmm learning which can greatly reduce the number of parameter to be estimated in practice we also devise a method for distributed vlhmm learning under the map reduce model we test our approach on a real data set consisting of billion query billion click and million search session and evaluate the effectiveness of the vlhmm learned from the real data on three search application document re ranking query suggestion and url recommendation the experimental result show that our approach is both effective and efficient 
this work aim to provide a novel site specific web page segmentation and section importance detection algorithm which leverage structural content and visual information the structural and content information is leveraged via template a generalized regular expression learnt over set of page the template along with visual information result into high sectioning accuracy the experimental result demonstrate the effectiveness of the approach 
enterprise search e is different from traditional ir due to a number of reason among which the high level of ambiguity of term in query and document and existence of graph structured enterprise data ontology that describe the concept of interest and their relationship to each other are the most important one our method identifies concept from the enterprise ontology in the query and corpus we propose a ranking scheme for ontology sub graph on top of approximately matched token q gram the ranking leverage the graph structure of the ontology to incorporate not explicitly mentioned concept it improves previous solution by using a fine grained ranking function that is specifically designed to cope with high level of ambiguity this method is able to capture much more of the semantics of query and document than previous technique we prove this claim by an evaluation of our method in three real life scenario from two different domain and found it to consistently be superior both in term of precision and recall 
generative model such a statistical language modeling have been widely studied in the task of expert search to model the relationship between expert and their expertise indicated in supporting document on the other hand discriminative model have received little attention in expert search research although they have been shown to outperform generative model in many other information retrieval and machine learning application in this paper we propose a principled relevance based discriminative learning framework for expert search and derive specific discriminative model from the framework compared with the state of the art language model for expert search the proposed research can naturally integrate various document evidence and document candidate association into a single model without extra modeling assumption or effort an extensive set of experiment have been conducted on two trec enterprise track corpus i e w c and cerc to demonstrate the effectiveness and robustness of the proposed framework 
we present finding from a log based study designed to track the adoption of feature of a new real time query refinement interface deployed on the yahoo search engine several trend from the first four month are noted and discussed 
the collective contribution of billion of user across the globe each day result in an ever changing web in vertical like news and real time search recency is an obvious significant factor for ranking however traditional link based web ranking algorithm typically run on a single web snapshot without concern for user activity associated with the dynamic of web page and link therefore a stale page popular many year ago may still achieve a high authority score due to it accumulated in link to remedy this situation we propose a temporal web link based ranking scheme which incorporates feature from historical author activity we quantify web page freshness over time from page and in link activity and design a web surfer model that incorporates web freshness based on a temporal web graph composed of multiple web snapshot at different time point it includes authority propagation among snapshot enabling link structure at distinct time point to influence each other when estimating web page authority experiment on a real world archival web corpus show our approach improves upon pagerank in both relevance and freshness of the search result 
blog feed search pose different and interesting challenge from traditional ad hoc document retrieval the unit of retrieval the blog are collection of document the blog post in this work we adapt a state of the art federated search model to the feed retrieval task showing a significant improvement over algorithm based on the best performing submission in the trec blog distillation task we also show that typical query expansion technique such a pseudo relevance feedback using the blog corpus do not provide any significant performance improvement and in many case dramatically hurt performance we perform an in depth analysis of the behavior of pseudo relevance feedback for this task and develop a novel query expansion technique using the link structure in wikipedia this query expansion technique provides significant and consistent performance improvement for this task yielding a and improvement in map over the unexpanded query for our baseline and federated algorithm respectively 
we analyse query length and fit power law and poisson distribution to four different query set we provide a practical model for query length based on the truncation of a poisson distribution for short query and a power law distribution for longer query that better fit real query length distribution than earlier proposal 
it ha been observed that many query submitted to search engine are location sensitive traditional search technique fail to interpret the significance of such geographical clue and a such are unable to return highly relevant search result although there have been effort in the literature to support location aware information retrieval critical challenge still remain in term of search result quality and data scalability in this paper we propose an innovative probabilistic ranking framework for domain information retrieval where user are interested in a set of location sensitive topic our proposed method recognizes the geographical distribution of topic influence in the process of ranking document and model it accurately using probabilistic gaussian process classifier additionally we demonstrate the effectiveness of the proposed ranking framework by implementing it in a web search service for nba news extensive performance evaluation is performed on real web document collection which confirms that our proposed mechanism work significantly better around averagely using dcg measure than other popular location aware information retrieval technique in ranking quality 
pseudo relevance feedback prf via query expansion ha been proven to be e ective in many information retrieval ir task in most existing work the top ranked document from an initial search are assumed to be relevant and used for prf one problem with this approach is that one or more of the top retrieved document may be non relevant which can introduce noise into the feedback process besides existing method generally do not take into account the significantly different type of query that are often entered into an ir system intuitively wikipedia can be seen a a large manually edited document collection which could be exploited to improve document retrieval effectiveness within prf it is not obvious how we might best utilize information from wikipedia in prf and to date the potential of wikipedia for this task ha been largely unexplored in our work we present a systematic exploration of the utilization of wikipedia in prf for query dependent expansion specifically we classify trec topic into three category based on wikipedia entity query ambiguous query and broader query we propose and study the effectiveness of three method for expansion term selection each modeling the wikipedia based pseudo relevance information from a different perspective we incorporate the expansion term into the original query and use language modeling ir to evaluate these method experiment on four trec test collection including the large web collection gov show that retrieval performance of each type of query can be improved in addition we demonstrate that the proposed method out performs the baseline relevance model in term of precision and robustness 
most classification algorithm are best at categorizing the web document into a few category such a the top two level in the open directory project such a classification method doe not give very detailed topic related class information for the user because the first two level are often too coarse however classification on a large scale hierarchy is known to be intractable for many target category with cross link relationship among them in this paper we propose a novel deep classification approach to categorize web document into category in a large scale taxonomy the approach consists of two stage a search stage and a classification stage in the first stage a category search algorithm is used to acquire the category candidate for a given document based on the category candidate we prune the large scale hierarchy to focus our classification effort on a small subset of the original hierarchy a a result the classification model is trained on the small subset before being applied to assign the category for a new document since the category candidate are sufficiently close to each other in the hierarchy a statistical language model based classifier using n gram feature is exploited furthermore the structure of the taxonomy can be utilized in this stage to improve the performance of classification we demonstrate the performance of our proposed algorithm on the open directory project with over category experimental result show that our proposed approach can reach on the measure of mi f at the th level which is improvement over top down based svm classification algorithm 
processing large volume of information generally requires massive amount of computational power which consumes a significant amount of energy an emerging challenge is the development of environmentally friendly system that are not only efficient in term of time but also energy efficient in this poster we outline our initial effort at developing greener filtering system by employing field programmable gate array fpga to perform the core information processing task fpgas enable code to be executed in parallel at a chip level while consuming only a fraction of the power of a standard von neuman style processor on a number of test collection we demonstrate that the fpga filtering system performs time faster than the itanium based implementation resulting in considerable energy saving 
opinion retrieval involves the measuring of opinion score of a document about the given topic we propose a new method namely sentiment relevance flow that naturally unifies the topic relevance and the opinionated nature of a document experiment conducted over a large scaled web corpus show that the proposed approach improves performance of opinion retrieval in term of precision at top rank 
finding the core member of a virtual community is an important problem in community analysis here we presented an simulated annealing algorithm to solve this problem by optimizing the user interest concentration ratio in user group a an example we test this algorithm on a virtual community site and evaluate it result using human gold standard method 
through the investigation of email document structure this paper proposes a multi field learning mfl framework which break the multi field document text classification tc problem into several sub document tc problem and make the final category prediction by weighted linear combination of several sub document tc result many previous statistical tc algorithm can be easily rebuilt within the mfl framework via turning binary result to spamminess score which is a real number and reflects the likelihood that the classified email is spam the experimental result in the trec spam track show that the performance of many tc algorithm can be improved within the mfl framework 
recent work in supervised learning of term based retrieval model ha shown significantly improved accuracy can often be achieved via better model estimation in this paper we show retrieval accuracy with metzler and croft s markov random field mrf approach can be similarly improved via supervised learning while the original mrf method estimate a parameter for each of it three feature class from data parameter within each class are set via a uniform weighting scheme adopted from the standard unigram we conjecture greater mrf retrieval accuracy should be possible by better estimating within class parameter particularly for verbose query employing natural language term retrieval experiment with these query on three trec document collection show our improved mrf consistently out performs both the original mrf and supervised unigram baseline additional experiment using blind feedback and evaluation with optimal weighting demonstrate both the immediate value and further potential of our method 
a lot of recent research ha focused on the content based dissemination of xml data however due to the heterogeneous data schema used by different data publisher even for data in the same domain an important challenge is how to efficiently and effectively disseminate relevant data to subscriber whose subscription might be specified based on schema that are different from those used by the data publisher this paper examines the option to resolve this schema heterogeneity problem in xml data dissemination and proposes a novel paradigm that is based on data rewriting our experimental result demonstrate the effectiveness of the data rewriting paradigm and identifies the tradeoff of the various approach 
in this paper we study a novel problem of staring people discovery from social network which is concerned with finding people who are not only authoritative but also sociable in the social network we formalize this problem a an optimization programming problem taking the co author network a a case study we define three objective function and propose two method to combine these objective function a genetic algorithm based method is further presented to solve this problem experimental result show that the proposed solution can effectively find the staring people from social network 
a pressing task during the unification process is to identify a user s vertical search intention based on the user s query in this paper we propose a novel method to propagate social annotation which includes user supplied tag data to both query and v for semantically bridging them our proposed algorithm consists of three key step query annotation vertical annotation and query intention identification our algorithm referred to a tagqv verifies that the social tagging can be propagated to represent web object such a query and v besides web page experiment on real web search query demonstrate the effectiveness of tagqv in query intention identification 
current web technology ha enabled the distribution of informative content through dynamic medium platform in addition the availability of the same content in the form of digital multimedia data ha dramatically increased content based cross medium retrieval application are needed to efficiently access desired information from this variety of data source this paper present a novel approach for cross medium information aggregation and describes a prototype system implementing this approach the prototype adopts online newspaper article and tv newscast a information source to deliver a service made up of item including both contribution extensive experiment prove the effectiveness of the proposed approach in a real world business context 
a the number of telephony voice application grow there will be a need for a browser to surf the web of interconnected voice application called a voicesites these voicesites are accessed through a telephone over an audio channel we present the concept and architecture of t web browser a world wide telecom web browser that enables browsing the web of voice application through an ordinary phone this browser will support rich browsing feature such a history and bookmarking 
this paper present the motivation resource and result for the first web people search task which wa organized a part of the semeval evaluation exercise also we will describe a survey and proposal for a new task attribute extraction which is planned for inclusion in the second evaluation planned for autumn 
this paper proposes new concept of query free web search for daily living we ordinarily benefit from additional information about our daily activity that we are currently engaged in when washing a coffee maker for example we receive the benefit if we obtain such information a cleaning a coffee maker with vinegar remove it stain well our proposed method automatically search for a web page including such information relates to an activity of daily living when the activity is performed we assume that wireless sensor node are attached to daily object to detect object use our method make a query from the name of object which are used then the method retrieves a web page relates to the activity of daily living by using the query 
in this paper we propose a method for predicting the ranking position of a web page assuming a set of successive past top k ranking we study the evolution of web page in term of ranking trend sequence used for markov model training which are in turn used to predict future ranking the prediction are highly accurate for all experimental setup and similarity measure 
in opinion finding the retrieval system is tasked with retrieving not just relevant document but those that also express an opinion towards the query target entity this task ha been studied in the context of the blogosphere by group participating in the trec blog track spam blog splogs are thought to be a problem on the blogosphere in this paper we investigate the extent to which spam ha affected the participating group retrieval system over the three year of the trec blog track opinion finding task our result show that spam can be an issue with most system retrieving some spam for every topic however removing spam from the ranking doe not markedly change the relative performance of opinion finding approach 
in this paper we measured and analyzed the workload on yahoo video the nd largest u s video sharing site to understand it nature and the impact on online video data center design we discovered interesting statistical property on both static and temporal dimension of the workload including file duration and popularity distribution arrival rate dynamic and predictability and workload stationarity and burstiness complemented with queueing theoretic technique we further extended our understanding on the measurement data with a virtual design on the workload and capacity management component of a data center assuming the same workload a measured which reveals key result regarding the impact of service level agreement slas and workload scheduling scheme on the design and operation of such large scale video distribution system 
huge amount of search log data have been accumulated in various search engine currently a commercial search engine receives billion of query and collect tera byte of log data on any single day other than search log data browse log can be collected by client side browser plug in which record the browse information if user permission are granted such massive amount of search browse log data on the one hand provide great opportunity to mine the wisdom of crowd and improve search result a well a online advertisement on the other hand designing effective and efficient method to clean model and process large scale log data also present great challenge in this tutorial we focus on mining search and browse log data for web information retrieval we consider a web information retrieval system consisting of four component namely query understanding document understanding query document matching and user understanding accordingly we organize the tutorial material along these four aspect for each aspect we will survey the major task challenge fundamental principle and state of the art method the goal of this tutorial is to provide a systematic survey on large scale search browse log mining to the ir community it will help ir researcher to get familiar with the core challenge and promising direction in log mining at the same time this tutorial may also serve the developer of web information retrieval system a a comprehensive and in depth reference to the advanced log mining technique 
this work investigates cluster labeling enhancement by utilizing wikipedia the free on line encyclopedia we describe a general framework for cluster labeling that extract candidate label from wikipedia in addition to important term that are extracted directly from the text the labeling quality of each candidate is then evaluated by several independent judge and the top evaluated candidate are recommended for labeling our experimental result reveal that the wikipedia label agree with manual label associated by human to a cluster much more than with significant term that are extracted directly from the text we show that in most case even when human s associated label appears in the text pure statistical method have difficulty in identifying them a good descriptor furthermore our experiment show that for more than of the cluster in our test collection the manual label or an inflection or a synonym of it appears in the top five label recommended by our system 
this paper present a novel entropy descriptor in the sense of geometric manifold with this descriptor entropy cycle can be easily designed for image classification minimizing this entropy lead to an optimal entropy cycle where image are connected in the semantic order during classification the training step is to find an optimal entropy cycle in each class in the test step an unknown image is grouped into a class if the entropy increase a the result of inserting the image into the cycle of this class is relatively least the proposed approach can generalize well on difficult image classification problem where image with same object are taken in multiple view experimental result show that this entropy descriptor performs well in image classification and ha potential in the image based modeling retrieval 
in this paper we describe a fixed threshold sequential minimal optimization fsmo for a joint constraint learning algorithm of structural classification svm problem because fsmo us the fact that the joint constraint formulation of structural svm ha b fsmo break down the quadratic programming qp problem of structural svm into a series of smallest qp problem each involving only one variable by using only one variable fsmo is advantageous in that each qp sub problem doe not need subset selection 
a a social service in web folksonomy provides the user the ability to save and organize their bookmark online with social annotation or tag social annotation are high quality descriptor of the web page topic a well a good indicator of web user interest we propose a personalized search framework to utilize folksonomy for personalized search specifically three property of folksonomy namely the categorization keyword and structure property are explored in the framework the rank of a web page is decided not only by the term matching between the query and the web page s content but also by the topic matching between the user s interest and the web page s topic in the evaluation we propose an automatic evaluation framework based on folksonomy data which is able to help lighten the common high cost in personalized search evaluation a series of experiment are conducted using two heterogeneous data set one crawled from del icio u and the other from dogear extensive experimental result show that our personalized search approach can significantly improve the search quality 
collaborative browsing or co browsing is the co navigation of the web with other people at a distance supported by software that take care of synchronizing the browser current state of the art solution are able to do co browsing of static web page and do not support the synchronization of javascript interaction however currently many web page use javascript and ajax technique to create highly dynamic and interactive web application in this paper we describe two approach for co browsing that both support the synchronization of the javascript and ajax interaction of dynamic web page one approach is based on synchronizing the output of the javascript engine by sending over the change made on the dom tree the other approach is based on synchronizing the input of the javascript engine by synchronizing ui event and incoming data since the latter solution offer a better user experience and is more scalable it is elaborated in more detail an important aspect of both approach is that they operate at the dom level therefore the client side can be implemented in javascript and no browser extension are required to the best of the author knowledge this is the first dom level co browsing solution that also enables co browsing of the dynamic interaction part of web page the presented co browsing solution ha been implemented in a research demonstrator which allows user to do co browsing of web application on browser based networked television 
in hir application we intent to characterize this behavior through the application of questionnaire to a sample of health professional interview to a smaller set of professional and if log are available through the analysis of search made on general and specialized search engine the next step is to propose an information retrieval framework that defines what how and where will the previously identified contextual feature be used in the ir system another necessary step involves the development of automatic method to collect contextual feature e g to identify the searcher s expertise or a search s scenario this may done through the analysis of the searcher s or group s characteristic past interaction process document characteristic or even by exploiting temporal property like frequency and length of search this data may be in log file and it can be acquired from other system e g search history user s bookmark email or o ce application this framework will be used to build a prototype based on standard ir model that integrates method for the collection of contextual feature 
we have conducted a user study to evaluate several generalist and health specific search engine on health information retrieval user evaluated the relevance of the top document of search engine in two different health information need we introduce the concept of local and global precision and analyze how they affect the evaluation result show that google surpasses the precision of all other engine including the health specific one and that precision differs with the type of clinical question and it medical specialty 
web is being extensively used for personal expression which includes rating review recommendation blog this user created content e g book review on amazon com becomes the property of the website and the user often doe not have easy access to it in some case user s feedback may get averaged with feedback from other user e g rating of a video we argue that the creator of such content need to be able to retain a link to her created content we introduce the concept of meb which is a user controlled store of such retained link a meb allows a user to access share all the review she ha given on different website with this capability user can allow their friend to search through their feedback searching through one s social network allows harnessing the power of social network where known relationship provide the context trust necessary to interpret feedback 
we report on the construction of the pan wikipedia vandalism corpus pan wvc using amazon s mechanical turk the corpus compiles edits on wikipedia article among which vandalism edits have been identified human annotator cast a total of vote on the edits so that each edit wa reviewed by at least annotator whereas the achieved level of agreement wa analyzed in order to label an edit a regular or vandalism the corpus is available free of charge 
social bookmarking system are becoming increasingly important data source for bootstrapping and maintaining semantic web application their emergent information structure have become known a folksonomies a key question for harvesting semantics from these system is how to extend and adapt traditional notion of similarity to folksonomies and which measure are best suited for application such a community detection navigation support semantic search user profiling and ontology learning here we build an evaluation framework to compare various general folksonomy based similarity measure which are derived from several established information theoretic statistical and practical measure our framework deal generally and symmetrically with user tag and resource for evaluation purpose we focus on similarity between tag and between resource and consider different method to aggregate annotation across user after comparing the ability of several tag similarity measure to predict user created tag relation we provide an external grounding by user validated semantic proxy based on wordnet and the open directory project we also investigate the issue of scalability we find that mutual information with distributional micro aggregation across user yield the highest accuracy but is not scalable per user projection with collaborative aggregation provides the best scalable approach via incremental computation the result are consistent across resource and tag similarity 
the ancestorrank algorithm calculates an authority score by using just one characteristic of the web graph the number of ancestor per node for scalability we estimate the number of ancestor by using a probabilistic counting algorithm we also consider the case in which ancestor which are closer to the node have more influence than those farther from the node thus we further apply a decay factor delta on the contribution from successively earlier ancestor the resulting authority score is used in combination with a content based ranking algorithm our experiment show that a long a delta is in the range of ancestorrank can greatly improve bm performance and in our experiment is often better than pagerank 
many real life datasets have skewed distribution of event when the probability of observing few event far exceeds the others in this paper we observed that in skewed datasets the state of the art collaborative filtering method perform worse than a simple probabilistic model our test bench includes a real ad click stream dataset which is naturally skewed the same conclusion is obtained even from the popular movie rating dataset when we pose a binary prediction problem of whether a user will give maximum rating to a movie or not 
how best to present query search result is an important problem in search engine and information retrieval system when a single query retrieves many result simply showing them a a long list will provide user with poor overview nowadays ranking and clustering query search result have been two useful separate post processing technique to organize retrieved document in this paper we proposed a spectral analysis method based on the content similarity network to integrate the clustering and ranking technique for improving literature search the new approach organizes all these search result into category intelligently and simultaneously rank the result in each category a variety of theoretical and empirical study have demonstrated that the presented method performs well in real application especially in biomedical literature retrieval moreover any free text information can be analyzed with the new method i e the proposed approach can be applied to various information system such a web search engine and literature search service 
in most ir clustering problem we directly cluster the document working in the document space using cosine similarity between document a the similarity measure in many real world application however we usually have knowledge on the word side and wish to transform this knowledge to the document concept side in this paper we provide a mechanism for this knowledge transformation to the best of our knowledge this is the first model for such type of knowledge transformation this model us a nonnegative matrix factorization model x fsgt where x is the word document semantic matrix f is the posterior probability of a word belonging to a word cluster and represents knowledge in the word space g is the posterior probability of a document belonging to a document cluster and represents knowledge in the document space and s is a scaled matrix factor which provides a condensed view of x we show how knowledge on word can improve document clustering i e knowledge in the word space is transformed into the document space we perform extensive experiment to validate our approach 
microblog service let user broadcast brief textual message to people who follow their activity often these post contain term called hashtags marker of a post s meaning audience etc this poster treat the following problem given a user s stated topical interest retrieve useful hashtags from microblog post our premise is that a user interested in topic x might like to find hashtags that are often applied to post about x this poster proposes a language modeling approach to hashtag retrieval the main contribution is a novel method of relevance feedback based on hashtags the approach is tested on a corpus of data harvested from twitter com 
we conduct the first systematical adoption of the semantic web solution in the integration management and utilization of tcm information and knowledge resource a the result the largest tcm semantic web ontology is engineered a the uniform knowledge representation mechanism the ontology based query and search engine is deployed mapping legacy and heterogeneous relational database to the semantic web layer for query and search across database boundary the first global herb drug interaction network is mapped through semantic integration and the semantic graph mining methodology is implemented for discovering and interpreting interesting pattern from this network the platform and underlying methodology are proved effective in tcm related drug usage discovery and safety analysis 
online advertising is a rapidly growing multi billion dollar industry it ha become a significant element of the web browsing experience online advertising provider use sophisticated ad targeting and ranking algorithm with the dual aim of maximizing revenue while providing a superior user experience a a result advertising optimization is a very complex research problem since it combine relevance with user interaction model advertiser valuation and commercial constraint online advertising integrates a number of core research area machine learning data mining search auction theory and user modeling this workshop is intended to serve a an open forum for discussion of new idea and current research in the field of online advertising we expect that the workshop will promote a community of researcher interested in this area and yield future collaboration and exchange the research included should address problem faced by advertiser end user advertising platform and the market 
a document or web page in isolation may appear completely reasonable but may represent a biased perspective on the topic being discussed given the topic of a document we propose new metric provocativeness and balance that suggest when the topic could be controversial we explore the use of these metric to characterize the subjectivity of the topic in the trec blog track 
we define and study the process of context transfer in search advertising which is the transition of a user from the context of web search to the context of the landing page that follows an ad click we conclude that in the vast majority of case the user is shown one of three type of page which can be accurately distinguished using automatic text classification 
this poster introduces a novel concept based video indexing approach it is developed based on a rich set of base concept of which the model are available then for a given concept with several labeled sample we combine the base concept to fit it and it model can thus be obtained accordingly empirical result demonstrate that this method can achieve great performance even with very limited labeled data we have compared different representation approach including both sparse and non sparse method our conclusion is that the sparse method will lead to much better performance 
given a controversial political topic our aim is to classify document debating the topic into pro or con our approach extract topic related term pro con related term and pair of topic related and pro con related term and us them a the basis for constructing a pro query and a con query following standard lm technique a document is classified a pro or con depending on which of the query likelihood is higher for the document our experiment show that our approach is promising 
we introduce a statistical model for abbreviation disambiguation in web search based on analysis of web data resource including anchor text click log and query log by combining evidence from multiple source we are able to accurately disambiguate the abbreviation in query experiment on real web search query show promising result 
online social medium such a delicious and digg are represented a tripartite network whose vertex are user tag and resource detecting community from such tripartite network is practically important modularity is often used a the criterion for evaluating the goodness of network division into community for tripartite network neubauer defines a tripartite modularity which extends murata s bipartite modularity however neubauer s tripartite modularity still us projection and it will lose information that original tripartite network have this paper proposes new tripartite modularity for tripartite network that do not use projection experimental result show that better community structure can be detected by optimizing our tripartite modularity 
collaborative tagging used in online social content system is naturally characterized by many synonym causing low precision retrieval we propose a mechanism based on user preference profile to identify synonym that can be used to retrieve more relevant document by expanding the user s query using a popular online book catalog we discus the effectiveness of our method over usual similarity based expansion method 
geographical context is very important for image million of image on the web have been already assigned latitude and longitude information due to the rapid proliferation of such image with geographical context it is still difficult to effectively search and browse them since we do not have way to decide their relevance in this paper we focus on the geographical relevance of image which is defined a to what extent the main object in an image match landmark at the location where the image wa taken recently researcher have proposed to use game based approach to label large scale data such a web image however previous work have not shown the quality of collected game log in detail and how the log can improve existing application to answer these question we design and implement a web based and multi player game to collect human knowledge while people are enjoying the game then we thoroughly analyze the game log obtained during a three week study with participant and propose method to determine the image geographical relevance in addition we conduct an experiment to compare our method with a commercial search engine experimental result show that our method dramatically improve image search relevance furthermore we show that we can derive geographically relevant object and their salient portion in image which is valuable for a number of application such a image location recognition 
we consider the problem of deep web source selection and argue that existing source selection method are inadequate a they are based on local similarity assessment specically they fail to account for the fact that source can vary in trustworthiness and individual result can vary in importance in response we formulate a global measure to calculate relevance and trustworthiness of a source based on agreement between the answer provided by different source agreement is modeled a a graph with source at the vertex on this agreement graph source quality score namely sourcerank are calculated a the stationary visit probability of a weighted random walk our experiment on online database and book source from google base show that sourcerank improves relevance of the result by over existing method and google base ranking sourcerank also reduces linearly with the corruption level of the source 
in this paper we present a novel near duplicate document detection method that can easily be tuned for a particular domain our method represents each document a a real valued sparse k gram vector where the weight are learned to optimize for a specified similarity function such a the cosine similarity or the jaccard coefficient near duplicate document can be reliably detected through this improved similarity measure in addition these vector can be mapped to a small number of hash value a document signature through the locality sensitive hashing scheme for efficient similarity computation we demonstrate our approach in two target domain web news article and email message our method is not only more accurate than the commonly used method such a shingle and i match but also show consistent improvement across the domain which is a desired property lacked by existing method 
we present a novel web image semantic analysis wisa system which explores the problem of adaptively modeling the distribution of the semantic label of the web image on it surrounding text to deal with this problem we employ a new piecewise penalty weighted regression model to learn the weight of the contribution of the different part of the surrounding text to the semantic label of image experimental result on a real web image data set show that it can improve the performance of web image semantic annotation significantly 
in this demo we present a novel interactive image search system image search by d semantic map this system enables user to indicate what semantic concept are expected to appear and even how these concept are spatially distributed in the desired image to this end we design an intuitive interface for user to formulate a query in the form of d semantic map called concept map by typing textual query in a blank canvas in the ranking process by interpreting each textual concept a a set of representative visual instance the concept map query is translated into a visual instance map which is then used for comparison with the image in the database besides in this demo we also show an image search system with a simplest semantic map a d color map where the concept are limited from the color 
online forum contain a huge amount of valuable user generated content in this paper we address the problem of extracting question answer pair from forum question answer pair extracted from forum can be used to help question answering service e g yahoo answer among other application we propose a sequential pattern based classification method to detect question in a forum thread and a graph based propagation method to detect answer for question in the same thread experimental result show that our technique are very promising 
focused crawling is a critical technique for topical resource discovery on the web we propose a new frontier prioritizing algorithm namely the otie on line topical importance estimation algorithm which efficiently and effectively combine link based and content based analysis to evaluate the priority of an uncrawled url in the frontier we then demonstrate otie s advantage over traditional prioritizing algorithm by real crawling experiment 
this paper study the problem of unified ranked retrieval of heterogeneous xml document and web data we propose an effective search engine called sailer to adaptively and versatilely answer keyword query over the heterogenous data we model the web page and xml document a graph we propose the concept of pivotal tree to effectively answer keyword query and present an effective method to identify the top k pivotal tree with the highest rank from the graph moreover we propose effective index to facilitate the effective unified ranked retrieval we have conducted an extensive experimental study using real datasets and the experimental result show that sailer achieves both high search efficiency and accuracy and outperforms the existing approach significantly 
in this paper we propose feature subset non negative matrix factorization nmf which is an unsupervised approach to simultaneously cluster data point and select important feature we apply our proposed approach to various document understanding task including document clustering summarization and visualization experimental result demonstrate the effectiveness of our approach for these task 
almost all text application use the well known vector space model for text representation and analysis while the vector space model ha proven itself to be an effective and efficient representation for mining purpose it doe not preserve information about the ordering of the word in the representation in this paper we will introduce the concept of distance graph representation of text data such representation preserve distance and ordering information between the word and provide a much richer representation of the underlying text this approach enables knowledge discovery from text which is not possible with the use of a pure vector space representation because it loses much le information about the ordering of the underlying word furthermore this representation doe not require the development of new mining and management technique this is because the technique can also be converted into a structural version of the vector space representation which allows the use of all existing tool for text in addition existing technique for graph and xml data can be directly leveraged with this new representation thus a much wider spectrum of algorithm is available for processing this representation 
the retrieval of sentence is a core task within information retrieval in this poster we employ a language model that incorporates a prior which encodes the importance of sentence within the retrieval model then in a set of comprehensive experiment using the trec novelty track we show that including this prior substantially improves retrieval effectiveness and significantly outperforms the current state of the art in sentence retrieval 
the transition of personal information management pim tool off the desktop to the web present an opportunity to augment these tool with capability provided by the wealth of real time information readily available in this paper we describe a next generation personal information assistance engine that let end user delegate to it various simple contextand activity reactive task and reminder our system atomate treat r atom feed from social networking and life tracking site a sensor stream integrating information from such feed into a simple unified rdf world model representing people place and thing and their timevarying state and activity combined with other information source on the web including the user s online calendar web based e mail client news feed and messaging service atomate can be made to automatically carry out a variety of simple task for the user ranging from context aware filtering and messaging to sharing and social coordination action atomate s open architecture and world model easily accommodate new information source and action via the addition of feed and web service to make routine use of the system easy for non programmer atomate provides a constrained input natural language interface cnli for behavior specification and a direct manipulation interface for inspecting and updating it world model 
we analyse the query log of a click oriented japanese search engine that utilises the link structure of wikipedia for encouraging the user to change his information need and to perform repeated serendipitous exploratory search our result show that user tend to make transition within the same query type from person name to person name from place name to place name and so on 
we present a system for personalized tag suggestion for flickr while the user is entering selecting new tag for a particular picture the system is suggesting related tag to her based on the tag that she or other people have used in the past along with some of the tag already entered the suggested tag are dynamically updated with every additional tag entered selected we describe three algorithm which can be applied to this problem in experiment our best performing method yield an improvement in precision of over a baseline method very similar to the system currently used by flickr our system is accessible at http ltaa epfl ch flickr tag to the best of our knowledge this is the first study on tag suggestion in a setting where i no full text information is available such a for blog ii no item ha been tagged by more than one person such a for social bookmarking site and iii suggestion are dynamically updated requiring efficient yet effective algorithm 
this paper study document ranking under uncertainty it is tackled in a general situation where the relevance prediction of individual document have uncertainty and are dependent between each other inspired by the modern portfolio theory an economic theory dealing with investment in financial market we argue that ranking under uncertainty is not just about picking individual relevant document but about choosing the right combination of relevant document this motivates u to quantify a ranked list of document on the basis of it expected overall relevance mean and it variance the latter serf a a measure of risk which wa rarely studied for document ranking in the past through the analysis of the mean and variance we show that an optimal rank order is the one that balancing the overall relevance mean of the ranked list against it risk level variance based on this principle we then derive an efficient document ranking algorithm it generalizes the well known probability ranking principle prp by considering both the uncertainty of relevance prediction and correlation between retrieved document moreover the benefit of diversification is mathematically quantified we show that diversifying document is an effective way to reduce the risk of document ranking experimental result in text retrieval confirm performance 
given only the url of a web page can we identify it topic this is the question that we examine in this paper usually web page are classified using their content but a url only classifier is preferable i when speed is crucial ii to enable content filtering before an objection able web page is downloaded iii when a page s content is hidden in image iv to annotate hyperlink in a personalized web browser without fetching the target page and v when a focused crawler want to infer the topic of a target page before devoting bandwidth to download it we apply a machine learning approach to the topic identification task and evaluate it performance in extensive experiment on categorized web page from the open directory project odp when training separate binary classifier for each topic we achieve typical f measure value between and and a typical precision of around we also ran experiment on a small data set of university web page for the task of classifying these page into faculty student course and project page our method improve over previous approach by point of f measure 
query expansion is an effective method to improve the usability of multimedia search most existing multimedia search engine are able to automatically expand a list of textual query term based on text search technique which can be called textual query expansion tqe however the annotation title and tag around web video are generally noisier for text only query expansion and search matching in this paper we propose a novel multi modal query expansion mmqe framework for web video search to solve the issue compared with traditional method mmqe provides a more intuitive query suggestion by transforming tex tual query to visual presentation based on visual clustering paral lel to this mmqe can enhance the process of search matching with strong pertinence of intent specific query by joining textual visual and social cue from both metadata and content of video experimental result on real web video from youtube demon strate the effectiveness of the proposed method 
web forum have become an important data resource for many web application but extracting structured data from unstructured web forum page is still a challenging task due to both complex page layout design and unrestricted user created post in this paper we study the problem of structured data extraction from various web forum site our target is to find a solution a general a possible to extract structured data such a post title post author post time and post content from any forum site in contrast to most existing information extraction method which only leverage the knowledge inside an individual page we incorporate both page level and site level knowledge and employ markov logic network mlns to effectively integrate all useful evidence by learning their importance automatically site level knowledge includes the linkage among different object page such a list page and post page and the interrelationship of page belonging to the same object the experimental result on forum show a very encouraging information extraction performance and demonstrate the ability of the proposed approach on various forum we also show that the performance is limited if only page level knowledge is used while when incorporating the site level knowledge both precision and recall can be significantly improved 
this paper present privacygrid a framework for supporting anonymous location based query in mobile information delivery system the privacygrid framework offer three unique capability first it provides a location privacy protection preference profile model called location p p which allows mobile user to explicitly define their preferred location privacy requirement in term of both location hiding measure e g location k anonymity and location l diversity and location service quality measure e g maximum spatial resolution and maximum temporal resolution second it provides fast and effective location cloaking algorithm for location k anonymity and location l diversity in a mobile environment we develop dynamic bottom up and top down grid cloaking algorithm with the goal of achieving high anonymization success rate and efficiency in term of both time complexity and maintenance cost a hybrid approach that carefully combine the strength of both bottom up and top down cloaking approach to further reduce the average anonymization time is also developed last but not the least privacygrid incorporates temporal cloaking into the location cloaking process to further increase the success rate of location anonymization we also discus privacygrid mechanism for supporting anonymous location query experimental evaluation show that the privacygrid approach can provide close to optimal location k anonymity a defined by per user location p p without introducing significant performance penalty 
a of today the amount of data on the semantic web ha grown considerably the service for searching and browsing entity on the semantic web are in demand to provide such service we developed the falcon system in this poster we present the feature of the falcon system 
the web abounds with dyadic data that keep increasing by every single second previous work ha repeatedly shown the usefulness of extracting the interaction structure inside dyadic data a commonly used tool in extracting the underlying structure is the matrix factorization whose fame wa further boosted in the netflix challenge when we were trying to replicate the same success on real world web dyadic data we were seriously challenged by the scalability of available tool we therefore in this paper report our effort on scaling up the nonnegative matrix factorization nmf technique we show that by carefully partitioning the data and arranging the computation to maximize data locality and parallelism factorizing a ten of million by hundred of million matrix with billion of nonzero cell can be accomplished within ten of hour this result effectively assures practitioner of the scalability of nmf on web scale dyadic data 
we present a framework for querying and reusing graph based business process model the framework is based on a new visual query language for business process called bpmn q the language address process definition and extends the standard bpmn visual notation for modeling business process for it concrete syntax bpmn q is used to query process model by matching a process model graph to a query graph moreover the reusing framework is enhanced with a semantic query expander component this component provides the user with the flexibility to get not only the perfectly matched process model to their query but also the model with high similarity the query engine of the framework is built on top of traditional rdbms a novel decomposition based and selectivity aware relational processing mechanism is employed to achieve an efficient and scalable performance for graph based bpmn q query 
research article typically introduce new result or finding and relate them to knowledge entity of immediate relevance however a large body of context knowledge related to the result is often not explicitly mentioned in the article to overcome this limitation the state of the art information retrieval approach rely on the latent semantic analysis in which term in article are projected to a lower dimensional latent space and best possible match in this space are identified however this approach may not perform well enough if the number of explicit knowledge entity in the article is too small compared to the amount of knowledge in the domain we address the problem by exploiting a domain knowledge layer a rich network of relation among knowledge entity in the domain extracted from a large corpus of document the knowledge layer supply the context knowledge that let u relate different knowledge entity and hence improve the information retrieval performance we develop and study a new framework for i learning and aggregating the relation in the knowledge layer from the literature corpus ii and for exploiting these relation to improve the information retrieval of relevant document 
we explore the use of the landing page content in sponsored search ad selection specifically we compare the use of the ad s intrinsic content to augmenting the ad with the whole or part of the landing page we explore two type of extractive summarization technique to select useful region from the landing page out of context and in context method out of context method select salient region from the landing page by analyzing the content alone without taking into account the ad associated with the landing page in context method use the ad context including it title creative and bid phrase to help identify region of the landing page that should be used by the ad selection engine in addition we introduce a simple yet effective unsupervised algorithm to enrich the ad context to further improve the ad selection experimental evaluation confirms that the use of landing page can significantly improve the quality of ad selection we also find that our extractive summarization technique reduce the size of landing page substantially while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page 
duplicate url have brought serious trouble to the whole pipeline of a search engine from crawling indexing to result serving url normalization is to transform duplicate url to a canonical form using a set of rewrite rule nowadays url normalization ha attracted significant attention a it is lightweight and can be flexibly integrated into both the online e g crawling and the offline e g index compression part of a search engine to deal with a large scale of website automatic approach are highly desired to learn rewrite rule for various kind of duplicate url in this paper we rethink the problem of url normalization from a global perspective and propose a pattern tree based approach which is remarkably different from existing approach most current approach learn rewrite rule by iteratively inducing local duplicate pair to more general form and inevitably suffer from noisy training data and are practically inefficient given a training set of url partitioned into duplicate cluster for a targeted website we develop a simple yet efficient algorithm to automatically construct a url pattern tree with the pattern tree the statistical information from all the training sample is leveraged to make the learning process more robust and reliable the learning process is also accelerated a rule are directly summarized based on pattern tree node in addition from an engineering perspective the pattern tree help select deployable rule by removing conflict and redundancy an evaluation on more than million duplicate url from website showed that the proposed approach achieves very promising performance in term of both de duping effectiveness and computational efficiency 
identifying intent boundary in search query log is important for learning user behavior and applying their experience time based query based and cluster based approach are proposed experiment show that the integration of intent cluster and dynamic time model performs the best 
we introduce and explore the concept of an individual s relevance threshold a a way of reconciling difference in outcome between batch and user experiment 
tag are user generated label for entity existing research on tag recommendation either focus on improving it accuracy or on automating the process while ignoring the efficiency issue we propose a highly automated novel framework for real time tag recommendation the tagged training document are treated a triplet of word doc tag and represented in two bipartite graph which are partitioned into cluster by spectral recursive embedding sre tag in each topical cluster are ranked by our novel ranking algorithm a two way poisson mixture model pmm is proposed to model the document distribution into mixture component within each cluster and aggregate word into word cluster simultaneously a new document is classified by the mixture model based on it posterior probability so that tag are recommended according to their rank experiment on large scale tagging datasets of scientific document citeulike and web page del icio u indicate that our framework is capable of making tag recommendation efficiently and effectively the average tagging time for testing a document is around second with over test document correctly labeled with the top nine tag we suggested 
to overcome the training data insufficiency problem for dedicated model in topical ranking this paper proposes to utilize click through data to improve learning the efficacy of click through data is explored under the framework of preference learning the empirical experiment on a commercial search engine show that the model trained with the dedicated labeled data combined with skip next preference could beat the baseline model and the generic model in ndcg for and respectively 
document prior feature such a pagerank and url depth can improve the retrieval effectiveness of web information retrieval ir system however not all query equally benefit from the application of a document prior feature this paper aim to investigate whether the retrieval performance can be further enhanced by selecting the best document prior feature on a per query basis we present a novel method for selecting the best document prior feature on a per query basis we evaluate our technique on the trec gov web test collection and it associated trec web search task our experiment demonstrate the effectiveness and robustness of our proposed selection method 
nowadays information is primarily searched on the www from a user perspective the readability is an important criterion for measuring the accessibility and thereby the quality of an information we show that modern content extraction algorithm help to estimate the readability of a web document quite accurate 
a topic is defined a a seminal event or activity along with all directly related event and activity it is represented a a chronological sequence of document by different author published on the internet in this paper we define a task called topic anatomy which summarizes and associate core part of a topic graphically so that reader can understand the content easily the proposed topic anatomy model called tscan derives the major theme of a topic from the eigenvectors of a temporal block association matrix then the significant event of the theme and their summary are extracted by examining the constitution of the eigenvectors finally the extracted event are associated through their temporal closeness and context similarity to form the evolution graph of the topic experiment based on the official tdt corpus demonstrate that the generated evolution graph comprehensibly describe the storyline of topic moreover in term of content coverage and consistency the produced summary are superior to those of other summarization method based on human composed reference summary 
we examine the behavioral pattern of email usage in a large scale enterprise over a three month period in particular we focus on two main question q what do reply depend on and q what is the gain of augmenting contact through the friend of friend from the email social graph for q we identify and evaluate the significance of several factor that affect the reply probability and the email response time we find that all factor of our considered set are significant provide their relative ordering and identify the recipient list size and the intensity of email communication between the correspondent a the dominant factor we highlight various novel threshold behavior and provide support for existing hypothesis such a that of the least effort reply for q we find that the number of new contact extracted from the friend of friend relationship amount to a large number but which is still a limited portion of the total enterprise size we believe that our result provide significant insight towards informed design of advanced email feature including those of social networking type 
in contextual advertising advertiser show ad to user so that they will click on them and eventually purchase a product optimizing this action sequence called the conversion funnel is the ultimate goal of advertising advertiser however often have very different sub goal for their ad such a purchase request for a quote or simply a site visit often an improvement for one advertiser s goal come at the expense of others a single ranking function must balance these different goal in order to make an efficient system for all advertiser we propose a ranking method that globally balance the goal of all advertiser while simultaneously improving overall performance our method ha been shown to improve significantly over the baseline in online traffic at a major ad network 
we consider experiment to measure the quality of a web search algorithm based on how much total time user take to complete assigned search task using that algorithm we first analyze our data to verify that there is in fact a negative relationship between a user s total search time and a user s satisfaction for the type of task under consideration secondly we fit a model with the user s total search time a the response to compare two different search algorithm finally we propose an alternative experimental design which we demonstrate to be a substantial improvement over our current design in term of variance reduction and efficiency 
million of user retrieve information from the internet using search engine mining these user session can provide valuable information about the quality of user experience and the perceived quality of search result often search engine rely on accurate estimate of click through rate ctr to evaluate the quality of user experience the vast heterogeneity in the user population and presence of automated software program bot can result in high variance in the estimate of ctr to improve the estimation accuracy of user experience metric like ctr we argue that it is important to identify typical and atypical user session in clickstreams our approach to identify these session is based on detecting outlier using mahalanobis distance in the user session space our user session model incorporates several key clickstream characteristic including a novel conformance score obtained by markov chain analysis editorial result show that our approach of identifying typical and atypical session ha a precision of about filtering out these atypical session reduces the uncertainty confidence interval of the mean ctr by about these result demonstrate that our approach of identifying typical and atypical user session is extremely valuable for cleaning noisy user session data for increased accuracy in evaluating user experience 
in this paper we present a novel hybrid recommender system called relationalcf which integrate content and demographic information into a collaborative filtering framework by using relational distance computation approach without the effort of form transformation and feature construction our experiment suggest that the effective combination of various kind of information based on relational distance approach provides improved accurate recommendation than other approach 
the explosion of online content ha made the management of such content non trivial web related task such a web page categorization news filtering query categorization tag recommendation etc often involve the construction of multi label categorization system on a large scale existing multi label classification method either do not scale or have unsatisfactory performance in this work we propose metalabeler to automatically determine the relevant set of label for each instance without intensive human involvement or expensive cross validation extensive experiment conducted on benchmark data show that the metalabeler tends to outperform existing method moreover metalabeler scale to million of multi labeled instance and can be deployed easily this enables u to apply the metalabeler to a large scale query categorization problem in yahoo yielding a significant improvement in performance 
the memose medium emotion search system is a specialized search engine for fundamental emotion in all kind of emotional laden document we apply a controlled vocabulary for basic emotion a slide control to adjust the intensity of the emotion and the approach of broad folksonomies the paper describes the indexing and the retrieval tool of memose and result from it evaluation 
we have developed an approach for analyzing online job advertisement in different domain industry from different region worldwide our approach is able to extract precise information from the text content supporting useful employment market analysis locally and globally a major component in our approach is an information extraction framework which is composed of two challenging task the first task is to detect unformatted text block automatically based on an unsupervised learning model identifying these useful text block through this learning model allows the generation of highly effective feature for the next task which is text fragment extraction learning the task of text fragment extraction learning is formulated a a domain adaptation model for text fragment classification one advantage of our approach is that it can easily adapt to a large number of online job advertisement in different and new domain extensive experiment have been conducted to demonstrate the effectiveness and flexibility of our approach 
many web application such a ad matching system vertical search engine and page categorization system require the identification of a particular type or class of page on the web the sheer number and diversity of the page on the web however make the problem of obtaining a good sample of the class of interest hard in this paper we describe a successfully deployed end to end system that start from a biased training sample and make use of several state of the art machine learning algorithm working in tandem including a powerful active learning component in order to achieve a good classification system the system is evaluated on traffic from a real world ad matching platform and is shown to achieve high categorization effectiveness with a significant reduction in editorial effort and labeling time 
bag of visual word bow ha been popular for visual classification in recent year in this paper we propose a novel bow expansion method to alleviate the effect of visual word correlation problem we achieve this by diffusing the weight of visual word in bow based on visual word relatedness which is rigorously defined within a visual ontology the proposed method is tested in video indexing experiment on trecvid video retrieval benchmark and an improvement of over the traditional bow is reported 
alhambra is a browser based system designed to enforce and test web browser security policy at the core of alhambra is a policy enhanced browser supporting fine grain security policy that restrict web page content and execution alhambra requires no server side modification or addition to the web application policy can restrict the construction of the document a well a the execution of javascript using access control rule and a taint tracking engine using the alhambra browser we present two security policy that we have built using our architecture both designed to prevent cross site scripting the first policy us a taint tracking engine to prevent cross site scripting attack that exploit bug in the client side of the web application the second one us browsing history to create policy that restrict the content of document and prevent the inclusion of malicious content using alhambra we analyze the impact of policy on the compatibility of web page to test compatibility alhambra support revisiting user generated browsing session and comparing multiple security policy in parallel to quickly and automatically evaluate security policy to compare security policy for identical page we have also developed useful comparison metric that quantify difference between identical page executed with different security policy not only do we show that our policy are effective with minimal compatibility cost we also demonstrate that alhambra can enforce strong security policy and provide quantitative evaluation of the difference introduced by security policy 
this paper present a boosting based algorithm for learning a bipartite ranking function brf with partially labeled data until now different attempt had been made to build a brf in a transductive setting in which the test point are given to the method in advance a unlabeled data the proposed approach is a semi supervised inductive ranking algorithm which a opposed to transductive algorithm is able to infer an ordering on new example that were not used for it training we evaluate our approach using the trec ohsumed and the reuters data collection comparing against two semi supervised classification algorithm for rocarea auc uninterpolated average precision aup mean precision tp and precision recall pr curve in the most interesting case where there are an unbalanced number of irrelevant example over relevant one we show our method to produce statistically significant improvement with respect to these ranking measure 
recently along with the rapid growth of the web the preservation effort have also increased a a consequence large amount of past web data are stored in web archive this historical data can be used for better understanding of long term page topic and characteristic in this paper we propose an interactive visualization system called page history explorer for exploring page history it allows for roughly portraying evolution of page and summarizing their content over time we use a temporal term cloud a a structure for visualizing prevailing and active term appearing on page in the past 
pseudo feedback based automatic query expansion yield effective retrieval performance on average but result in performance inferior to that of using the original query for many information need we address an important cause of this robustness issue namely the query drift problem by fusing the result retrieved in response to the original query and to it expanded form our approach post performance that is significantly better than that of retrieval based only on the original query and more robust than that of retrieval using the expanded query 
although there are many paper examining ambiguity in information retrieval this paper show that there is a whole class of ambiguous word that past research ha barely explored it is shown that the class is more ambiguous than other word type and is commonly used in query the lack of test collection containing ambiguous query is highlighted and a method for creating collection from existing resource is described test using the new collection show the impact of query ambiguity on an ir system it is shown that conventional system are incapable of dealing effectively with such query and that current assumption about how to improve search effectiveness do not hold when searching on this common query type 
retrieval in a question and answer archive involves finding good answer for a user s question in contrast to typical document retrieval a retrieval model for this task can exploit question similarity a well a ranking the associated answer in this paper we propose a retrieval model that combine a translation based language model for the question part with a query likelihood approach for the answer part the proposed model incorporates word to word translation probability learned through exploiting different source of information experiment show that the proposed translation based language model for the question part outperforms baseline method significantly by combining with the query likelihood language model for the answer part substantial additional effectiveness improvement are obtained 
searching for medical information on the web ha become highly popular but it remains a challenging task because searcher are often uncertain about their exact medical situation and unfamiliar with medical terminology to address this challenge we have built an intelligent medical web search engine called imed which us medical knowledge and an interactive questionnaire to help searcher form query this paper focus on imed s iterative search advisor which integrates medical and linguistic knowledge to help searcher improve search result iteratively such an iterative process is common for general web search and especially crucial for medical web search because searcher often miss desired search result due to their limited medical knowledge and the task s inherent difficulty imed s iterative search advisor help the searcher in several way first relevant symptom and sign are automatically suggested based on the searcher s description of his situation second instead of taking for granted the searcher s answer to the question imed rank and recommends alternative answer according to their likelihood of being the correct answer third related mesh medical phrase are suggested to help the searcher refine his situation description we demonstrate the effectiveness of imed s iterative search advisor by evaluating it using real medical case record and usmle medical exam question 
while there are many way to develop search expertise i maintain that most member of the general public do so in an inefficient manner one reason is that with current tool is difficult to observe expert a a mean of acquiring search expertise in a scalable fashion this call for a redesign of computer mediated communication tool to make individual search strategy visible to other user i present a research agenda to investigate this claim which draw upon theory of social learning i use design based research to build novel system that enable imitation based learning of search expertise 
by analogy with merging document ranking the output from multiple search result clustering algorithm can be combined into a single output in this paper we study the feasibility of meta search result clustering which ha unique feature compared to the general meta clustering problem after showing that the combination of multiple search result clustering is empirically justified we cast meta clustering a an optimization problem of an objective function measuring the probabilistic concordance between the clustering combination and the single clustering we then show using an easily computable upper bound on such a function that a simple stochastic optimization algorithm delivers reasonable approximation of the optimal value very efficiently and we also provide a method for labeling the generated cluster with the most agreed upon cluster label optimal meta clustering with meta labeling is applied to three description centric state of the art search result clustering algorithm the performance improvement is demonstrated through a range of evaluation technique i e internal classification oriented and information retrieval oriented using suitable test collection of search result with document level relevance judgment per subtopic 
in commercial search engine a ranking function is selected for deployment mainly by comparing the relevance measurement over candidate in this paper we suggest to select web ranking function according to both their relevance and robustness to the change that may lead to relevance degradation over time we argue that the ranking robustness can be effectively measured by taking into account the ranking score distribution across web page we then improve ndcg with two new metric and show their superiority in term of stability to ranking score turbulence and stability in function selection 
automatic categorization of user query is an important component of general purpose web search engine particularly for triggering rich query specific content and sponsored link we propose an unsupervised learning scheme that reduces dramatically the cost of setting up and maintaining such a categorizer while retaining good categorization power the model is stored a a graph of concept where graph edge represent the cross reference between the concept concept and relation are extracted from query log by an offline web mining process which us a search engine a a powerful summarizer for building a concept graph empirical evaluation indicates that the system compare favorably on publicly available data set such a kdd cup a well a on portion of the current query stream of yahoo search where it is already changing the experience of million of web search user 
suppose you buy a new laptop and simply because you like it so much you recommend it to friend encouraging them to purchase it a well what would be an adequate price for the vendor of the laptop to pay for your recommendation personal recommendation like this are of considerable commercial interest but unlike in sponsored search auction there can be no truthful price despite this lack of truthfulness the vendor of the product might still decide to pay you for recommendation e g because she want to i provide you with an additional incentive to actually recommend her or to ii increase your satisfaction and or brand loyalty this lead u to investigate a pricing scheme based on the shapley value that satisfies certain axiom of fairness we find that it is vulnerable to manipulation and show how to overcome these difficulty using the anonymity proof shapley value of 
we consider the problem of identifying the consensus ranking for the result of a query given preference among those result from a set of individual user once consensus ranking are identified for a set of query these ranking can serve for both evaluation and training of retrieval and learning system we present a novel approach to collecting the individual user preference over image search result we use a collaborative game in which player are rewarded for agreeing on which image result is best for a query our approach is distinct from other labeling game because we are able to elicit directly the preference of interest with respect to image query extracted from query log a a source of relevance judgment this data provides a useful complement to click data furthermore the data is free of positional bias and is collected by the game without the risk of frustrating user with non relevant result this risk is prevalent in standard mechanism for debiasing click we describe data collected over day from a deployed version of this game that amount to about million expressed preference between pair finally we present several approach to modeling this data in order to extract the consensus ranking from the preference and better sort the search result for targeted query 
automatic folksonomy construction from tag ha attracted much attention recently however inferring hierarchical relation between concept from tag ha a drawback in that it is difficult to distinguish between more popular and more general concept instead of tag we propose to use userspecified relation for learning folksonomy we explore two statistical framework for aggregating many shallow individual hierarchy expressed through the collection set relation on the social photosharing site flickr into a common deeper folksonomy that reflects how a community organizes knowledge our approach address a number of challenge that arise while aggregating information from diverse user namely noisy vocabulary and variation in the granularity level of the concept expressed our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference taxonomy e g the web directory created by the open directory project our empirical result suggest that user specified relation are a good source of evidence for learning folksonomies 
the paper is concerned with the problem of question recommendation specifically given a question a query we are to retrieve and rank other question according to their likelihood of being good recommendation of the queried question a good recommendation provides alternative aspect around user interest we tackle the problem of question recommendation in two step first represent question a graph of topic term and then rank recommendation on the basis of the graph we formalize both step a the tree cutting problem and then employ the mdl minimum description length for selecting the best cut experiment have been conducted with the real question posted at yahoo answer the question are about two domain travel and computer internet experimental result indicate that the use of the mdl based tree cut model can significantly outperform the baseline method of word based vsm or phrase based vsm the result also show that the use of the mdl based tree cut model is essential to our approach 
we discus the use of social network in implementing viral marketing strategy while influence maximization ha been studied in this context see chapter of we study revenue maximization arguably a more natural objective in our model a buyer s decision to buy an item is influenced by the set of other buyer that own the item and the price at which the item is offered we focus on algorithmic question of finding revenue maximizing marketing strategy when the buyer are completely symmetric we can find the optimal marketing strategy in polynomial time in the general case motivated by hardness result we investigate approximation algorithm for this problem we identify a family of strategy called influence and exploit strategy that are based on the following idea initially influence the population by giving the item for free to carefully a chosen set of buyer then extract revenue from the remaining buyer using a greedy pricing strategy we first argue why such strategy are reasonable and then show how to use recently developed set function maximization technique to find the right set of buyer to influence 
entity resolution er is a problem that arises in many area in most of case it represents a task that multiple entity from different source require to be identified if they refer to the same or different object because there are not unique identifier associated with them in this paper we propose a model using web page identification to identify entity and merge those entity refer to one object together we use a classical name disambiguation problem a case study and examine our model on a subset of digital library record a the first stage of our work the favorable result indicated that our proposed approach is highly effective 
information retrieval is one of the most popular information access method for overcoming the information overload problem of the web however it interaction model is still utilizing the old text based ranked list and static interaction algorithm in this paper we introduce our adaptive visualization approach for searching the web which we call adaptive vibe it is an extended version of a reference point based spatial visualization algorithm and is designed to serve a a user interaction module for a personalized search system personalized search can incorporate dynamic user interest and different context improving search result when it is combined with adaptive visualization it can encourage user to become involved in the search process more actively by exploring the information space and learning new fact for effective searching in this paper we introduce the rationale and function of our adaptive visualization approach and discus the approach potential to create a better search environment for the web 
information retrieval system are compared using evaluation metric with researcher commonly reporting result for simple metric such a precision at or reciprocal rank together wi th more complex one such a average precision or discounted cumulative gain in this paper we demonstrate that complex metric are a good a or better than simple metric at predicting the performance of the simple metric on other topic therefore reporting of result from simple metric alongside complex one is redundant 
anonymization of social network before they are published or shared ha become an important research question recent work on anonymizing social network ha looked at privacy preserving technique for publishing a single instance of the network however social network evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis we study the problem of repeatedly publishing social network data a the network evolves while preserving privacy of user publishing multiple instance of the same network independently ha privacy risk since stitching the information together may allow an adversary to identify user in the network we propose method to anonymize a dynamic network such that the privacy of user is preserved when new node and edge are added to the published network these method make use of link prediction algorithm to model the evolution of the social network using this predicted graph to perform group based anonymization the loss in privacy caused by new edge can be reduced we evaluate the privacy loss on publishing multiple social network instance using our method 
in this paper we cast the image ranking problem into the task of identifying authority node on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of image through an iterative procedure based on the pagerank computation a numerical weight is assigned to each image this measure it relative importance to the other image being considered the incorporation of visual signal in this process differs from the majority of large scale commercial search engine in use today commercial search engine often solely rely on the text clue of the page in which image are embedded to rank image and often entirely ignore the content of the image themselves a a ranking signal to quantify the performance of our approach in a real world system we conducted a series of experiment based on the task of retrieving image for of the most popular product query our experimental result show significant improvement in term of user satisfaction and relevancy in comparison to the most recent google image search result 
diversity ha been heavily motivated in the information retrieval literature a an objective criterion for result set in search and recommender system perhaps one of the most well known and most used algorithm for result set diversification is that of maximal marginal relevance mmr in this paper we show that while mmr is somewhat ad hoc and motivated from a purely pragmatic perspective we can derive a more principled variant via probabilistic inference in a latent variable graphical model this novel derivation present a formal probabilistic latent view of mmr plmmr that a remove the need to manually balance relevance and diversity parameter b show that specific definition of relevance and diversity metric appropriate to mmr emerge naturally and c formally derives variant of latent semantic indexing lsi similarity metric for use in plmmr empirically plmmr outperforms mmr with standard term frequency based similarity and diversity metric since plmmr maximizes latent diversity in the result 
we investigate the problem of learning document classifier in a multilingual setting from collection where label are only partially available we address this problem in the framework of multiview learning where different language correspond to different view of the same document combined with semi supervised learning in order to benefit from unlabeled document we rely on two technique coregularization and consensus based self training that combine multiview and semi supervised learning in different way our approach train different monolingual classifier on each of the view such that the classifier decision over a set of unlabeled example are in agreement a much a possible and iteratively label new example from another unlabeled training set based on a consensus across language specific classifier we derive a boosting based training algorithm for this task and analyze the impact of the number of view on the semi supervised learning result on a multilingual extension of the reuters rcv rcv corpus using five different language our experiment show that coregularization and consensus based self training are complementary and that their combination is especially effective in the interesting and very common situation where there are few view language and few labeled document available 
in this paper we report our system that disambiguates person name in web search result the system us named entity compound key word and url a feature for document similarity calculation which typically show high precision but low recall clustering result we propose to use a two stage clustering algorithm by bootstrapping to improve the low recall value in which clustering result of the first stage are used to extract feature used in the second stage clustering experimental result revealed that our algorithm yield better score than the best system at the latest weps workshop 
searcher have a choice about which web search engine they use when looking for information online if they are unsuccessful on one engine user may switch to a different engine to continue their search by predicting when switch are likely to occur the search experience can be modified to retain searcher or ensure a quality experience for incoming searcher in this poster we present research on a technique for predicting search engine switch our finding show that prediction is possible at a reasonable level of accuracy particularly when personalization or user grouping is employed these finding have implication for the design of application to support more effective online searching 
we study personalized item recommendation within an enterprise social medium application suite that includes blog bookmark community wikis and shared file recommendation are based on two of the core element of social medium people and tag relationship information among people tag and item is collected and aggregated across different source within the enterprise based on these aggregated relationship the system recommends item related to people and tag that are related to the user each recommended item is accompanied by an explanation that includes the people and tag that led to it recommendation a well a their relationship with the user and the item we evaluated our recommender system through an extensive user study result show a significantly better interest ratio for the tag based recommender than for the people based recommender and an even better performance for a combined recommender tag applied on the user by other people are found to be highly effective in representing that user s topic of interest 
current search mechanism of dht based p p system can well handle a single keyword search problem other than single keyword search multi keyword search is quite popular and useful in many real application simply using the solution for single keyword search will require distributed intersection union operation in wide area network leading to unacceptable traffic cost a it is well known that bloom filter bf is effective in reducing traffic we would like to use bf encoding to handle multi keyword search applying bf is not difficult but how to get optimal result is not trivial in this study we show through mathematical proof that the optimal setting of bf in term of traffic cost is determined by the global statistical information of keywords not the minimized false positive rate a claimed by previous method through extensive experiment we demonstrate how to obtain optimal setting we further argue that the intersection order between set is important for multi keyword search thus we design optimal order strategy based on bf for both and and or query to better evaluate the performance of this design we conduct extensive simulation on trec wt g test collection and the query log of a commercial search engine result show that our design significantly reduces the search traffic of existing approach by 
in this paper we propose a new approach to measure similarity among academic paper based on their reference our similarity measure us both in link and out link by transforming in link and out link into undirected link 
we introduce a grocery retrieval system that map shopping list written in natural language into actual product in a grocery store we have developed the system using nine month of shopping basket data from a large finnish supermarket to evaluate the system we used real shopping list gathered from customer of the supermarket our system achieves over precision for product at rank one and the precision is around for product at rank 
we introduce the posterior probabilistic clustering ppc which provides a rigorous posterior probability interpretation for nonnegative matrix factorization nmf and remove the uncertainty in clustering assignment furthermore ppc is closely related to probabilistic latent semantic indexing plsi 
in the last year blog search ha been a new exciting task in information retrieval the presence of user generated information with valuable opinion make this field of huge interest in this poster we use part of this information the reader comment to improve the quality of post snippet with the objective of enhancing the user access to the relevant post in a result list we propose a simple method for snippet generation based on sentence selection using the comment to guide the selection process we evaluated our approach with standard trec methodology in the blog collection showing significant improvement up to in term of map over the baseline 
online auction have become a pervasive transaction mechanism for e commerce a the largest online marketplace in the world ebay is an attractive case study that enables the study of online auction utilizing data involving real people and transaction in this paper we present a detailed investigation and analysis of multiple online auction property including consumer surplus sniping bidding strategy and their cross relationship our goal is to evaluate the theoretical foundation of online auction and discover pattern and behavior hidden due to the lack of real and extensive transaction data among our finding we uncover an important correlation among sniping and high surplus ratio which implies the uncertainty of true value in a competitive environment the key issue is the wrong assumption that bidder valuation are independent from each other which lead to inefficient auction in order to address the inefficiency of current online format we introduce a declining price auction model customized for online transaction conceptually this model ought to deal with the complexity of competition in an online environment while maximizing social welfare 
cell phone are increasingly becoming attractive target of various worm which cause the leakage of user privacy extra service charge and depletion of battery power in this work we study propagation of cell phone worm which exploit multimedia messaging service mm and or bluetooth for spreading we then propose a systematic countermeasure against the worm at the terminal level we adopt graphic turing test and identity based signature to block unauthorized message from leaving compromised phone at the network level we propose a push based automated patching scheme for cleansing compromised phone through experiment on phone device and a wide variety of network we show that cellular system taking advantage of our defense can achieve a low infection rate e g le than within hour even under severe attack 
geographic information ha spawned many novel web application where global positioning system gps play important role in bridging the application and end user learning knowledge from user raw gps data can provide rich context information for both geographic and mobile application however so far raw gps data are still used directly without much understanding in this paper an approach based on supervised learning is proposed to automatically infer transportation mode from raw gps data the transportation mode such a walking driving etc implied in a user s gps data can provide u valuable knowledge to understand the user it also enables context aware computing based on user s present transportation mode and design of an innovative user interface for web user our approach consists of three part a change point based segmentation method an inference model and a post processing algorithm based on conditional probability the change point based segmentation method wa compared with two baseline including uniform duration based and uniform length based method meanwhile four different inference model including decision tree bayesian net support vector machine svm and conditional random field crf are studied in the experiment we evaluated the approach using the gps data collected by user over six month period a a result beyond other two segmentation method the change point based method achieved a higher degree of accuracy in predicting transportation mode and detecting transition between them decision tree outperformed other inference model over the change point based segmentation method 
speech retrieval system utilize automatic speech recognition asr to generate textual data for indexing however automatic transcription include error either because of out of vocabulary oov word or due to asr inaccuracy in this work we address spoken information retrieval in turkish a morphologically rich language where oov rate are high we apply several technique such a using subword unit and indexing alternative hypothesis to cope with the oov problem and asr inaccuracy experiment are performed on our turkish broadcast news bn corpus which also incorporates a spoken ir collection result indicate that word segmentation is quite useful but the efficiency of indexing alternative hypothesis depends on retrieval type 
publishing personal content on the web is gaining increased popularity with dramatic growth in social networking website and availability of cheap personal domain name and hosting service although the internet enables easy publishing of any content intended to be generally accessible restricting personal content to a selected group of contact is more difficult social networking website partially enable user to restrict access to a selected group of user of the same network by explicitly creating a friend list while this limited restriction support user privacy on those few selected website personal website must still largely be protected manually by sharing password or obscure link our focus is the general problem of privacy enabled web content sharing from any user chosen web server by leveraging the existing circle of trust in popular instant messaging im network we propose a scheme called im based privacy enhanced content sharing impecs for personal web content sharing impecs enables a publishing user s personal data to be accessible only to her im contact a user can put her personal web page on any web server she want v being restricted to a specific social networking website and maintain privacy of her content without requiring site specific password our prototype of impecs required only minor modification to an im server and php script on a web server the general idea behind impecs extends beyond im and im circle of trust any equivalent scheme ideally containing pre arranged group could similarly be leveraged 
sponsored search is one of the major source of revenue for search engine on the world wide web it ha been observed that while showing ad for every query maximizes short term revenue irrelevant ad lead to poor user experience and le revenue in the long term hence it is in search engine interest to place ad only for query that are likely to attract ad click many algorithm for estimating query advertisability exist in literature but most of these method have been proposed for and tested on the frequent or head query since query frequency on search engine are known to be distributed a a power law this leaf a huge fraction of the query uncovered in this paper we focus on the more challenging problem of estimating query advertisability for infrequent or tail query these require fundamentally different method than head query for e g tail query are almost all unique and require the estimation method to be online and inexpensive we show that previously proposed method do not apply to tail query and when modified for our scenario they do not work well further we give a simple yet effective approach which estimate query advertisability using only the word present in the query we evaluate our approach on a real world dataset consisting of search engine query and user click our result show that our simple approach outperforms a more complex one based on regularized regression 
the np hard max k cover problem requires selecting k set from a collection so a to maximize the size of the union this classic problem occurs commonly in many setting in web search and advertising for moderately sized instance a greedy algorithm give an approximation of e however the greedy algorithm requires updating score of arbitrary element after each step and hence becomes intractable for large datasets we give the first max cover algorithm designed for today s large scale commodity cluster our algorithm ha provably almost the same approximation a greedy but run much faster furthermore it can be easily expressed in the mapreduce programming paradigm and requires only polylogarithmically many pass over the data our experiment on five large problem instance show that our algorithm is practical and can achieve good speedup compared to the sequential greedy algorithm 
middleware for web service composition such a bpel engine provides the execution environment for service a well a additional functionality such a monitoring and self tuning given it role in service provisioning it is very important to ass the performance of middleware in the context of a soa this paper present soabench a framework for the automatic generation and execution of testbeds for benchmarking middleware for composite web service and for assessing the performance of existing soa infrastructure soabench defines a testbed model characterized by the composite service to execute the workload to generate the deployment configuration to use the performance metric to gather the data analysis to perform on them and the report to produce we have validated soabench by benchmarking the performance of different bpel engine 
we track a large set of rapidly changing web page and examine the assumption that the arrival of content change follows a poisson process on a microscale we demonstrate that there are significant difference in the behavior of page that can be exploited to maintain freshness in a web corpus 
breakpoints are perhaps the quintessential feature of a de bugger they allow a developer to stop time and study the program state breakpoints are typically specified by selecting a line of source code for large complex web page with multiple developer the relevant source line for a given user interface problem may not be known to the developer in this paper we describe the implementation of breakpoints in dynamically created source and on error message network event dommutation domobject property change and cs style rule update adding these domain specific breakpoints to a general purpose debugger for javascript allows the developer to initiate the debugging process via web page abstraction rather than lower level source code view the breakpoints are implemented in the open source fire bug project version for the firefox web browser 
social network system like last fm play a significant role in web containing large amount of multimedia enriched data that are enhanced both by explicit user provided annotation and implicit aggregated feedback describing the personal preference of each user it is also a common tendency for these system to encourage the creation of virtual network among their user by allowing them to establish bond of friendship and thus provide a novel and direct medium for the exchange of data we investigate the role of these additional relationship in developing a track recommendation system taking into account both the social annotation and friendship inherent in the social graph established among user item and tag we created a collaborative recommendation system that effectively adapts to the personal information need of each user we adopt the generic framework of random walk with restarts in order to provide with a more natural and efficient way to represent social network in this work we collected a representative enough portion of the music social network last fm capturing explicitly expressed bond of friendship of the user a well a social tag we performed a series of comparison experiment between the random walk with restarts model and a user based collaborative filtering method using the pearson correlation similarity the result show that the graph model system benefit from the additional information embedded in social knowledge in addition the graph model outperforms the standard collaborative filtering method 
in this paper we propose a new application of bayesian language model based on pitman yor process for information retrieval this model is a generalization of the dirichlet distribution the pitman yor process creates a power law distribution which is one of the statistical property of word frequency in natural language our experiment on robust indicate that this model improves the document retrieval performance compared to the commonly used dirichlet prior and absolute discounting smoothing technique 
designing effective ranking function is a core problem for information retrieval and web search since the ranking function directly impact the relevance of the search result the problem ha been the focus of much of the research at the intersection of web search and machine learning and learning ranking function from preference data in particular ha recently attracted much interest the objective of this paper is to empirically examine several objective function that can be used for learning ranking function from preference data specifically we investigate the role of tie in the learning process by tie we mean preference judgment that two document have equal degree of relevance with respect to a query this type of data ha largely been ignored or not properly modeled in the past in this paper we analyze the property of tie and develop novel learning framework which combine tie and preference data using statistical paired comparison model to improve the performance of learned ranking function the resulting optimization problem explicitly incorporating tie and preference data are solved using gradient boosting method experimental study are conducted using three publicly available data set which demonstrate the effectiveness of the proposed new method 
indexing is an important information retrieval ir operation which must be parallelised to support large scale document corpus we propose a novel adaptation of the state of the art single pas indexing algorithm in term of the mapreduce programming model we then experiment with this adaptation in the context of the hadoop mapreduce implementation in particular we explore the scale of improvement that can be achieved when using firstly more processing hardware and secondly larger corpus our result show that indexing speed increase in a close to linear fashion when scaling corpus size or number of processing machine this suggests that the proposed indexing implementation is viable to support upcoming large scale corpus 
query reformulation technique based on query log have recently proven to be effective for web query however when initial query have reasonably good quality these technique are often not reliable enough to identify the helpful reformulations among the suggested query in this paper we show that we can use a few a two feature to rerank a list of reformulated query or expanded query to be specific generated by a log based query reformulation technique our result across five trec collection suggest that there are consistently more useful reformulations in the first two position in the new ranked list than there were initially which lead to statistically significant improvement in retrieval effectiveness 
query suggestion ha been an effective approach to help user narrow down to the information they need however most of existing study focused on only popular head query since rare query posse much le information e g click than popular query in the query log it is much more difficult to efficiently suggest relevant query to a rare query in this paper we propose an optimal rare query suggestion framework by leveraging implicit feedback from user in the query log our model resembles the principle of pseudo relevance feedback which assumes that top returned result by search engine are relevant however we argue that the clicked url and skipped url contain different level of information and thus should be treated differently hence our framework optimally combine both the click and skip information from user and us a random walk model to optimize the query correlation our model specifically optimizes two parameter the restarting jumping rate of random walk and the combination ratio of click and skip information unlike the rocchio algorithm our learning process doe not involve the content of the url but simply leverage the click and skip count in the query url bipartite graph consequently our model is capable of scaling up to the need of commercial search engine experimental result on one month query log from a large commercial search engine with over million rare query demonstrate the superiority of our framework with statistical significance over the traditional random walk model and pseudo relevance feedback model 
we investigate the task of finding link from wikipedia page to external web page such external link significantly extend the information in wikipedia with information from the web at large while retaining the encyclopedic organization of wikipedia we use a language modeling approach to create a full text and anchor text run and experiment with different document prior in addition we explore whether social bookmarking site delicious can be exploited to further improve our performance we have constructed a test collection of topic which are wikipedia page on different entity our finding are that the anchor text index is a very effective method to retrieve home page url class and anchor text length prior and their combination lead to the best result using delicious on it own doe not lead to very good result but it doe contain valuable information combining the best anchor text run and the delicious run lead to further improvement 
modern day marco polo young influencers in emerging market who have more access to technology than their peer often act a the gateway to new website and technology in their respective country however a they influence their peer through account gifting they are often met with barrier such a language 
micro blogging service provide platform for user to share their feeling and idea on the go desiging to produce information stream in almost micro blogging service although are capable of recording rich and diverse sens still suffer from a drawback of not being able to provide deeper and summarized view in this paper we present a novel framework pusic to musicalize micro blogging message for term or user pusic can be used to summarize user message into certain expression of emotion explore the emotion and sens and transform them into music and serve a a presentation of crowd net art we generate the music from two aspect emotion and harmony the former is tackled by emotion detection from message while the latter is estabilished by rule based harmonic heuristic according to the detected emotion pusic ha been announced online for people s experience and further investigation 
several type of query are widely used on the world wide web and the expected retrieval method can vary depending on the query type we propose a method for classifying query into informational and navigational type because term in navigational query often appear in anchor text for link to other page we analyze the distribution of query term in anchor text on the web for query classification purpose while content based retrieval is effective for informational query anchor based retrieval is effective for navigational query our retrieval system combine the result obtained with the content based and anchor based retrieval method in which the weight for each retrieval result is determined automatically depending on the result of the query classification we also propose a method for improving anchor based retrieval our retrieval method which computes the probability that a document is retrieved in response to the given query identifies synonym of query term in the anchor text on the web and us these synonym for smoothing purpose in the probability estimation we use the ntcir test collection and show the effectiveness of individual method and the entire web retrieval system experimentally 
leveraging clickthrough data ha become a popular approach for evaluating and optimizing information retrieval system although data is plentiful one must take care when interpreting click since user behavior can be affected by various source of presentation bias while the issue of position bias in clickthrough data ha been the topic of much study other presentation bias effect have received comparatively little attention for instance since user must decide whether to click on a result based on it summary e g the title url and abstract one might expect click to favor more attractive result in this paper we examine result summary attractiveness a a potential source of presentation bias this study distinguishes itself from prior work by aiming to detect systematic bias in click behavior due to attractive summary inflating perceived relevance our experiment conducted on the google web search engine show substantial evidence of presentation bias in click towards result with more attractive title 
in many web application such a blog classification and new sgroup classification labeled data are in short supply it often happens that obtaining labeled data in a new domain is expensive and time consuming while there may be plenty of labeled data in a related but different domain traditional text classification ap proaches are not able to cope well with learning across different domain in this paper we propose a novel cross domain text classification algorithm which extends the traditional probabilistic latent semantic analysis plsa algorithm to integrate labeled and unlabeled data which come from different but related domain into a unified probabilistic model we call this new model topic bridged plsa or tplsa by exploiting the common topic between two domain we transfer knowledge across different domain through a topic bridge to help the text classification in the target domain a unique advantage of our method is it ability to maximally mine knowledge that can be transferred between domain resulting in superior performance when compared to other state of the art text classification approach experimental eval uation on different kind of datasets show that our proposed algorithm can improve the performance of cross domain text classification significantly 
a machine learning approach to learning to rank train a model to optimize a target evaluation measure with repect to training data currently existing information retrieval measure are impossible to optimize directly except for model with a very small number of parameter the ir community thus face a major challenge how to optimize ir measure of interest directly in this paper we present a solution specifically we show that lambdarank which smoothly approximates the gradient of the target measure can be adapted to work with four popular ir target evaluation measure using the same underlying gradient construction it is likely therefore that this construction is extendable to other evaluation measure we empirically show that lambdarank find a locally optimal solution for mean ndcg mean ndcg map and mrr with a confidence rate we also show that the amount of effective training data varies with ir measure and that with a sufficiently large training set size matching the training optimization measure to the target evaluation measure yield the best accuracy 
comprehensive coverage of the public web is crucial to web search engine search engine use crawler to retrieve page and then discover new one by extracting the page outgoing link however the set of page reachable from the publicly linked web is estimated to be significantly smaller than the invisible web the set of document that have no incoming link and can only be retrieved through web application and web form the sitemaps protocol is a fast growing web protocol supported jointly by major search engine to help content creator and search engine unlock this hidden data by making it available to search engine in this paper we perform a detailed study of how classic discovery crawling compare with sitemaps in key measure such a coverage and freshness over key representative website a well a over billion of url seen at google we observe that sitemaps and discovery crawling complement each other very well and offer different tradeoff 
this poster present an overview of the characteristic of a one button information retrieval interface with closed caption from tv watching activity which is intended to lighten the burden of remembering and entering query term while watching tv we investigated this interface with an experimental system named video bookmarking search which estimate query term from closed caption with named entity recognition and sentence labeling technique according to an empirical evaluation for search query from bookmark using seven actual tv show on city life travel health and cuisine we found wider query and search result are acceptable through the query input free interface despite the fact that the number of query and search result that are directly relevant to the user original intention is not high the main reason is a watching user s interest is wider than what is expressed with query term 
most of the approach for dealing with uncertainty in the semantic web rely on the principle that this uncertainty is already asserted in this paper we propose a new approach to learn and reason about uncertainty in the semantic web using instance data we learn the uncertainty of an owl ontology and use that information to perform probabilistic reasoning on it for this purpose we use markov logic a new representation formalism that combine logic with probabilistic graphical model 
we investigate how people interact with web search engine result page using eye tracking while previous research ha focused on the visual attention devoted to the organic search result this paper examines other component of contemporary search engine such a ad and related search we systematically varied the type of task informational or navigational the quality of the ad relevant or irrelevant to the query and the sequence in which ad of different quality were presented we measured the effect of these variable on the distribution of visual attention and on task performance our result show significant effect of each variable the amount of visual attention that people devote to organic result depends on both task type and ad quality the amount of visual attention that people devote to ad depends on their quality but not the type of task interestingly the sequence and predictability of ad quality is also an important factor in determining how much people attend to ad when the quality of ad varied randomly from task to task people paid little attention to the ad even when they were good these result further our understanding of how attention devoted to search result is influenced by other page element and how previous search experience influence how people attend to the current page 
well tuned large vocabulary continuous speech recognition lvcsr ha been shown to generally be more effective than vocabulary independent technique for ranked retrieval of spoken content when one or the other approach is used alone tuning lvcsr system to a topic domain can be costly however and the experiment in this paper show that out of vocabulary oov query term can significantly reduce retrieval effectiveness when that tuning is not performed further experiment demonstrate however that retrieval effectiveness for query with oov term can be substantially improved by combining evidence from lvcsr with additional evidence from vocabulary independent ranked utterance retrieval rur the combination is performed by using relevance judgment from held out topic to learn generic i e topic independent smooth non decreasing transformation from lvcsr and rur system score to probability of topical relevance evaluated using a clef collection that includes topic spontaneous conversational speech audio and relevance judgment the system recovers of the mean uninterpolated average precision that could have been obtained through lvcsr domain tuning for very short query or for longer query 
in this paper we report finding on how user behavior vary in task with different difficulty level a well a of different type two behavioral signal document dwell time and number of content page viewed per query were found to be able to help the system detect when user are working with difficult task 
this tutorial focus on the task of automated information linking in text and multimedia source in any task where information is fused from different source this linking is a necessary step to solve the problem we borrow method from computational linguistics computer vision and data mining although the main focus is on finding equivalence relation in the source the tutorial open view on the recognition of other relation type 
this paper proposes an effective approach to provide relevant search term for conceptual web search semantic term suggestion function ha been included so that user can find the most appropriate query term to what they really need conventional approach for term suggestion involve extracting frequently occurring key term from retrieved document they must deal with term extraction difficulty and interference from irrelevant document in this paper we propose a semantic term suggestion function called collective intelligence based term suggestion cits cits provides a novel social network based framework for relevant term suggestion with a semantic graph of the search term without limiting to the specific query term a visualization of semantic graph is presented to the user to help browsing search result from related term in the semantic graph the search result are ranked each time according to their relevance to the related term in the entire query session comparing to two popular commercial search engine a user study of user on search term showed better user satisfaction and indicated the potential usefulness of proposed method in real world search application 
we propose a keyword auction protocol called the generalized second price with an exclusive right gsp exr in existing keyword auction the number of displayed advertisement is determined in advance thus we consider adjusting the number of advertisement dynamically based on bid in the gsp exr the number of slot can be either or k when k slot are displayed the protocol is identical to the gsp if the value per click of the highest ranked bidder is large enough then this bidder can exclusively display her advertisement by paying a premium thus this pricing scheme is relatively simple and seller revenue is at least a good a the gsp also in the gsp exr the highest ranked bidder ha no incentive to change the number of slot by over under bidding a long a she retains the top position 
in this paper we define a new query expansion method that relies on term similarity metric derived from the electric resistance network this proposed metric let u measure the mutual relevancy in between term and between their group this paper show how to define this metric automatically from the document collection and then apply it in query expansion for document retrieval task the experiment show this method can be used to find good expansion term of search query and improve document retrieval performance on two trec genomic track datasets 
we proposed and implemented a novel clustering algorithm called lair which ha constant running time average for on the fly scatter gather browsing our experiment showed that when running on a single processor the lair on line clustering algorithm wa several hundred time faster than a parallel buckshot algorithm running on multiple processor this paper report on a study that examined the effectiveness of the lair algorithm in term of clustering quality and it impact on retrieval performance we conducted a user study on subject to evaluate on the fly lair clustering in scatter gather search task by comparing it performance to the buckshot algorithm a classic method for scatter gather browsing result showed significant difference in term of subjective perception of clustering quality subject perceived that the lair algorithm produced significantly better quality cluster than the buckshot method did subject felt that it took le effort to complete the task with the lair system which wa more effective in helping them in the task interesting pattern also emerged from subject comment in the final open ended questionnaire we discus implication and future research 
with the advent of xml a a standard for representation and exchange of structured document a growing amount of xml document are being stored in peer to peer p p network cur rent research on p p search engine proposes the use of informa tion retrieval ir technique to perform content based search but doe not take into account structural feature of document p p system typically have no central index thus avoiding single point of failure but distribute all information among participating peer accordingly a querying peer ha only limited access to the index information and should select carefully which peer can help answering a given query by contributing resource such a local index information or cpu time for ranking computation bandwidth consumption is a major issue to guarantee scalability p p system have to reduce the number of peer involved in the retrieval process a a result the retrieval quality in term of recall and precision may suffer substantially in the proposed thesis document structure is considered a an extra source of information to improve the retrieval quality of xml document in a p p environment the thesis centre on the following question how can structural information help to improve the retrieval of xml document in term of result quality such a precision recall and specificity can xml structure support the routing of query in distributed environment especially the selection of promising peer how can xml ir technique be used in a p p network while minimizing bandwidth consumption and considering performance aspect to answer these question and to analyze possible achievement a search engine is proposed that exploit structural hint expressed explicitly by the user or implicitly by the self describing structure of xml document additionally more focused and specific result are obtained by providing ranked retrieval unit that can be either xml document a a whole or the most relevant passage of thesis document xml information retrieval technique are applied in two way to select those peer participating in the retrieval process and to compute the relevance of document the indexing approach includes both content and structural information of document to support efficient execution of multi term query index key consist of rare combination of content structure tuples performance is increased by using only fixedsized posting list frequent index key are combined with each other iteratively until the new combination is rare with a posting list size under a pre set threshold all posting list are sorted by taking into account classical ir measure such a term frequency and inverted term frequency a well a weight for potential retrieval unit of a document with a slight bias towards document on peer with good collection regarding the current index key and with good peer characteristic such a online time available bandwidth and latency when extracting the posting list for a specific query a re ordering on the posting list is performed that take into account the structural similarity between key and query according to this preranking peer are selected that are expected to hold information about potentially relevant document and retrieval unit the final ranking is computed in parallel on those selected peer the computation is based on an extension of the vector space model and distinguishes between weight for different structure of the same content this allows weighting xml element with respect to their discriminative power e g a title will be weighted much higher than a footnote additionally relevance is computed a a mixture of content relevance and structural similarity between a given query and a potential retrieval unit currently a first prototype for p p information retrieval of xml document called spirix is being implemented experiment to evaluate the proposed technique and use of structural hint will be performed on a distributed version of the inex wikipedia collection 
this paper address the problem of named entity recognition in query nerq which involves detection of the named entity in a given query and classification of the named entity into predefined class nerq is potentially useful in many application in web search the paper proposes taking a probabilistic approach to the task using query log data and latent dirichlet allocation we consider context of a named entity i e the remainder of the named entity in query a word of a document and class of the named entity a topic the topic model is constructed by a novel and general learning method referred to a w lda weakly supervised latent dirichlet allocation which employ weakly supervised learning rather than unsupervised learning using partially labeled seed entity experimental result show that the proposed method based on w lda can accurately perform nerq and outperform the baseline method 
effective search session segmentation grouping query according to common task or intent can be useful for improving relevance search evaluation and query suggestion previous work ha largely attempted to segment search session off line after the fact in contrast we present preliminary investigation of predicting in real time whether a user is about to switch interest that is whether the user is about to finish the current search and switch to another search task or stop searching altogether we explore an approach for this task using client side user behavior such a click scroll and mouse movement contextualized by the content of the search result page and previous search our experiment over thousand of real search show that we can identify context and user behavior pattern that indicate that a user is about to switch to a new search task these preliminary result can be helpful for more effective query suggestion and personalization 
a the web grows in size interface interaction across website diverge for differentiation and arguably for a better user experience however this size diversity is also a cognitive load for the user who ha to learn a new user interface for every new website she visit several study have confirmed the importance of well designed website in this paper we propose a method for quantitative evaluation of the navigational complexity of user interaction on the web our approach of quantifying interaction complexity exploit the modeling of the web a a graph and us the information theoretic definition of complexity it enables u to measure the navigational complexity of web interaction in bit our approach is structural in nature and can be applied to both traditional paradigm of web interaction browsing and to emerging paradigm of web interaction like web widget 
language modeling approach have been effectively dealing with the dependency among query term based on n gram such a bigram or trigram model however bigram language model suffer from adjacency sparseness problem which mean that dependent term are not always adjacent in document but can be far from each other sometimes with distance of a few sentence in a document to resolve the adjacency sparseness problem this paper proposes a new type of bigram language model by explicitly incorporating the proximity feature between two adjacent term in a query experimental result on three test collection show that the proposed bigram language model significantly improves previous bigram model a well a tao s approach the state of art method for proximity based method 
blog wikis tagging podcasts and social networking website such a myspace facebook flickr and youtube have radically changed user interaction on the world wide web from a static one way consumption model to a dynamic multi way participation model broad user power and flexibility have changed how people engage in and experience their interconnection interest and collaboration online social interaction will evolve in the next decade to address the growing need of it user community and make entry into many aspect of our life this evolution may very well be among the most exciting one of our time where the individual and collective power of people to contribute and share content experience idea expertise etc may be even more enhanced than it is today the enhancement may be shaped through a better understanding of user need and behavior and it may be enabled through the seamless convergence of multi modal technology new application new domain data mining and better navigational and search capability some of these change will also permeate into the workplace and change the way we work this panel will discus how online social interaction may evolve in the next decade and what impact it may have on diverse dimension in our world 
a beautiful and well laid out web page greatly facilitates user accessing and enhances browsing experience we use visual quality vq to denote the aesthetic of web page in this paper a computational aesthetic approach is proposed to learn the evaluation model for the visual quality of web page first a web page layout extraction algorithm v lbe is introduced to partition a web page into major layout block then regarding a web page a a semi structured image feature known to significantly affect the visual quality of a web page are extracted to construct a feature vector the experimental result show the initial success of our approach potential application include web search and web design 
phishing is a significant security threat to the internet which cause tremendous economic loss every year in this paper we proposed a novel hybrid phish detection method based on information extraction ie and information retrieval ir technique the identity based component of our method detects phishing webpage by directly discovering the inconsistency between their identity and the identity they are imitating the keywords retrieval component utilizes ir algorithm exploiting the power of search engine to identify phish our method requires no training data no prior knowledge of phishing signature and specific implementation and thus is able to adapt quickly to constantly appearing new phishing pattern comprehensive experiment over a diverse spectrum of data source with page show that both component have a low false positive rate and the stacked approach achieves a true positive rate of with a false positive rate of 
the objective of this research work is to study the effect of toponym place name ambiguity in the geographical information retrieval gir task our experience with gir system show that toponym ambiguity may be an important factor in the inability of these system to take advantage from geographical knowledge previous study over ambiguity and information retrieval ir suggested that disambiguation may be useful in some specific ir scenario we suppose that gir may constitute such a scenario this preliminary study wa carried out over the wordnet based manually disambiguated collection developed for the clir wsd task using the geoclef collection of geographically related topic the employed gir system wa based on the geoworse system that participated in geoclef the experiment were carried out considering the manual disambiguation and comparing this result with those obtained by randomly disambiguating the document collection and those obtained by using always the most common referent the obtained result show no significant difference in the overall result although the work gave an insight into some error that are produced by toponym ambiguity and how they may affect the result these preliminary result also suggest that wordnet is not a suitable resource for the planned research 
we model budget constrained keyword bidding in sponsored search auction a a stochastic multiple choice knapsack problem s mckp and design an algorithm to solve s mckp and the corresponding bidding optimization problem our algorithm selects item online based on a threshold function which can be built updated using historical data our algorithm achieved about performance compared to the offline optimum when applied to a real bidding dataset with synthetic dataset and iid item set it performance ratio against the offline optimum converges to one empirically with increasing number of period 
efficient similarity search in high dimensional space is important to content based retrieval system recent study have shown that sketch can effectively approximate l distance in high dimensional space and that filtering with sketch can speed up similarity search by an order of magnitude it is a challenge to further reduce the size of sketch which are already compact without compromising accuracy of distance estimation this paper present an efficient sketch algorithm for similarity search with l distance and a novel asymmetric distance estimation technique our new asymmetric estimator take advantage of the original feature vector of the query to boost the distance estimation accuracy we also apply this asymmetric method to existing sketch for cosine similarity and l distance evaluation with datasets extracted from image and telephone record show that our l sketch outperforms existing method and the asymmetric estimator consistently improve the accuracy of different sketch method to achieve the same search quality asymmetric estimator can reduce the sketch size by to 
web technology ha enabled more and more people to freely express their opinion on the web making the web an extremely valuable source for mining user opinion about all kind of topic in this paper we study how to automatically integrate opinion expressed in a well written expert review with lot of opinion scattering in various source such a blogspaces and forum we formally define this new integration problem and propose to use semi supervised topic model to solve the problem in a principled way experiment on integrating opinion about two quite different topic a product and a political figure show that the proposed method is effective for both topic and can generate useful aligned integrated opinion summary the proposed method is quite general it can be used to integrate a well written review with opinion in an arbitrary text collection about any topic to potentially support many interesting application in multiple domain 
web crawler are highly automated and seldom regulated manually the diversity of crawler activity often lead to ethical problem such a spam and service attack in this research quantitative model are proposed to measure the web crawler ethic based on their behavior on web server we investigate and define rule to measure crawler ethic referring to the extent to which web crawler respect the regulation set forth in robot txt configuration file we propose a vector space model to represent crawler behavior and measure the ethic of web crawler based on the behavior vector the result show that ethicality score vary significantly among crawler most commercial web crawler behavior are ethical however many commercial crawler still consistently violate or misinterpret certain robot txt rule we also measure the ethic of big search engine crawler in term of return on investment the result show that google ha a higher score than other search engine for a u website but ha a lower score than baidu for chinese website 
we investigate how to organize a large collection of geotagged photo working with a dataset of about million image collected from flickr our approach combine content analysis based on text tag and image data with structural analysis based on geospatial data we use the spatial distribution of where people take photo to define a relational structure between the photo that are taken at popular place we then study the interplay between this structure and the content using classification method for predicting such location from visual textual and temporal feature of the photo we find that visual and temporal feature improve the ability to estimate the location of a photo compared to using just textual feature we illustrate using these technique to organize a large photo collection while also revealing various interesting property about popular city and landmark at a global scale 
behavioral targeting bt is a technique used by online advertiser to increase the effectiveness of their campaign and is playing an increasingly important role in the online advertising market however it is underexplored in academia when looking at how much bt can truly help online advertising in commercial search engine to answer this question in this paper we provide an empirical study on the click through log of advertisement collected from a commercial search engine from the comprehensively experiment result on the sponsored search log of the commercial search engine over a period of seven day we can draw three important conclusion user who clicked the same ad will truly have similar behavior on the web click through rate ctr of an ad can be averagely improved a high a by properly segmenting user for behavioral targeted advertising in a sponsored search using the short term user behavior to represent user is more effective than using the long term user behavior for bt the statistical t test verifies that all conclusion drawn in the paper are statistically significant to the best of our knowledge this work is the first empirical study for bt on the click through log of real world ad 
any given web search engine may provide higher quality result than others for certain query therefore it is in user best interest to utilize multiple search engine in this paper we propose and evaluate a framework that maximizes user search effective ness by directing them to the engine that yield the best result for the current query in contrast to prior work on meta search we do not advocate for replacement of multiple engine with an aggregate one but rather facilitate simultaneous use of individual engine we describe a machine learning approach to supporting switching between search engine and demonstrate it viability at tolerable interruption level our finding have implication for fluid competition between search engine 
automatic document classification adc is still one of the major information retrieval problem it usually employ a supervised learning strategy where we first build a classification model using pre classified document and then use this model to classify unseen document the majority of supervised algorithm consider that all document provide equally important information however in practice a document may be considered more or le important to build the classification model according to several factor such a it timeliness the venue where it wa published in it author among others in this paper we are particularly concerned with the impact that temporal effect may have on adc and how to minimize such impact in order to deal with these effect we introduce a temporal weighting function twf and propose a methodology to determine it for document collection we applied the proposed methodology to acm dl and medline and found that the twf of both follows a lognormal we then extend three adc algorithm namely knn rocchio and na ve bayes to incorporate the twf experiment showed that the temporally aware classifier achieved significant gain outperforming or at least matching state of the art algorithm 
the semantic web community until now ha used traditional database system for the storage and querying of rdf data the sparql query language also closely follows sql syntax a a natural consequence most of the sparql query processing technique are based on database query processing and optimization technique for sparql join query optimization previous work like rdf x and hexastore have proposed to use way index on the rdf data although these index speed up merge join by order of magnitude for complex join query generating large intermediate join result the scalability of the query processor still remains a challenge in this paper we introduce i bitmat a compressed bit matrix structure for storing huge rdf graph and ii a novel light weight sparql join query processing method that employ an initial pruning technique followed by a variable binding matching algorithm on bitmats to produce the final result our query processing method doe not build intermediate join table and work directly on the compressed data we have demonstrated our method against rdf graph of upto billion triple the largest among result published until now single node non parallel system and have compared our method with the state of the art rdf store rdf x and monetdb our result show that the competing method are most effective with highly selective query on the other hand bitmat can deliver order of magnitude better performance on complex low selectivity query over massive data 
in this paper we try to leverage a large scale and multilingual knowledge base wikipedia to help effectively analyze and organize web information written in different language based on the observation that one wikipedia concept may be described by article in different language we adapt existing topic modeling algorithm for mining multilingual topic from this knowledge base the extracted universal topic have multiple type of representation with each type corresponding to one language accordingly new document of different language can be represented in a space using a group of universal topic which make various multilingual web application feasible 
the quantum probability ranking principle qprp ha been recently proposed and account for interdependent document relevance when ranking however to be instantiated the qprp requires a method to approximate the interference between two document in this poster we empirically evaluate a number of different method of approximation on two trec test collection for subtopic retrieval it is shown that these approximation can lead to significantly better retrieval performance over the state of the art 
in this paper we discus challenge and provide solution for capturing and maintaining accurate model of user profile using semantic web technology by aggregating and sharing distributed fragment of user profile information spread over multiple service our framework for profile management allows for evolvable extensible and expressive user profile we have implemented a prototype targeting the retail domain on the hp lab retail store assistant 
many researcher have noted that web search query are often ambiguous or unclear we present an approach for identifying the popular meaning of query using web search log and user click behavior we show our approach to produce more complete and user centric intent than expert judge by evaluating on trec query this approach wa also used by the trec web track judge to obtain more representative topic description from real query 
online social networking site like myspace facebook and flickr have become a popular way to share and disseminate content their massive popularity ha led to viral marketing technique that attempt to spread content product and idea on these site however there is little data publicly available on viral propagation in the real world and few study have characterized how information spread over current online social network in this paper we collect and analyze large scale trace of information dissemination in the flickr social network our analysis based on crawl of the favorite marking of million user on million photo aim at answering three key question a how widely doe information propagate in the social network b how quickly doe information propagate and c what is the role of word of mouth exchange between friend in the overall propagation of information in the network contrary to viral marketing intuition we find that a even popular photo do not spread widely throughout the network b even popular photo spread slowly through the network and c information exchanged between friend is likely to account for over of all favorite marking but with a significant delay at each hop 
we investigate tag and query log to see if the term people use to annotate website are similar to the one they use to query for them over a set of url we compare the distribution of tag used to annotate each url with the distribution of query term for click on the same url understanding the relationship between the distribution is important to determine how useful tag data may be for improving search result and conversely query data for improving tag prediction in our study we compare both term frequency distribution using vocabulary overlap and relative entropy we also test statistically whether the term count come from the same underlying distribution our result indicate that the vocabulary used for tagging and searching for content are similar but not identical we further investigate the content of the website to see which of the two distribution tag or query is most similar to the content of the annotated searched url finally we analyze the similarity for different category of url in our sample to see if the similarity between distribution is dependent on the topic of the website or the popularity of the url 
query on major web search engine produce complex result page primarily composed of two type of information organic result that is short description and link to relevant web page and sponsored search result the small textual advertisement often displayed above or to the right of the organic result strategy for optimizing each type of result in isolation and the consequent user reaction have been extensively studied however the interplay between these two complementary source of information ha been ignored a situation we aim to change our finding indicate that their perceived relative usefulness a evidenced by user click depends on the nature of the query specifically we found that when both source focus on the same intent for navigational query there is a clear competition between ad and organic result while for non navigational query this competition turn into synergy we also investigate the relationship between the perceived usefulness of the ad and their textual similarity to the organic result and propose a model that formalizes this relationship to this end we introduce the notion of responsive ad which directly address the user s information need and incidental ad which are only tangentially related to that need our finding support the hypothesis that in the case of navigational query which are usually fully satisfied by the top organic result incidental ad are perceived a more valuable than responsive ad which are likely to be duplicative on the other hand in the case of non navigational query incidental ad are perceived a le beneficial possibly because they diverge too far from the actual user need we hope that our finding and further research in this area will allow search engine to tune ad selection for an increased synergy between organic and sponsored result leading to both higher user satisfaction and better monetization 
in this paper we describe an experimental search engine on our chinese web archive since the original data set contains nearly billion chinese web page crawled from past year from the collection million article like page are selected and then partitioned into million set of similar page the title and publication date are determined for the page an index is built when searching the system return related page in a chronological order this way if a user is interested in news report or commentary for certain previously happened event he she will be able to find a quite rich set of highly related page in a convenient way 
in this paper we propose a novel recrawling method based on navigation pattern called selective recrawling the goal of selective recrawling is to automatically select page collection that have large coverage and little redundancy to a pre defined vertical domain it only requires several seed object and can select a set of url pattern to cover most object the selected set can be used to recrawl the web page for quite a period of time and renewed periodically experiment on local event data show that our method can greatly reduce the downloading of web page while keep the comparative object coverage 
many technique have been proposed to scale web application however the data interdependency between the database query and transaction issued by the application limit their efficiency we claim that major scalability improvement can be gained by restructuring the web application data into multiple independent data service with exclusive access to their private data store while this restructuring doe not provide performance gain by itself the implied simplification of each database workload allows a much more efficient use of classical technique we illustrate the data denormalization process on three benchmark application tpc w rubis and rubbos we deploy the resulting service oriented implementation of tpc w across an node cluster and show that restructuring it data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to master slave database replication while preserving strong consistency and transactional property 
web spam detection ha become one of the top challenge for the internet search industry instead of using some heuristic rule we propose a feature re extraction strategy to optimize the detection result based on the predicted spamicity obtained by the preliminary detection through the host level web graph three type of feature are extracted experiment on webspam uk benchmark show that with this strategy the performance of web spam detection can be improved evidently 
the use of rdf data published on the web for application is still a cumbersome and resource intensive task due to the limited software support and the lack of standard programming paradigm to deal with everyday problem such a combination of rdf data from dierent source object identifier consolidation ontology alignment and mediation or plain querying and filtering task in this paper we present a framework semantic web pipe that support fast implementation of semantic data mash ups while preserving desirable property such a abstraction encapsulation component orientation code re usability and maintainability which are common and well supported in other application area 
with a suitable algorithm for ranking the expertise of a user in a collaborative tagging system we will be able to identify expert and discover useful and relevant resource through them we propose that the level of expertise of a user with respect to a particular topic is mainly determined by two factor firstly an expert should posse a high quality collection of resource while the quality of a web resource depends on the expertise of the user who have assigned tag to it secondly an expert should be one who tends to identify interesting or useful resource before other user do we propose a graph based algorithm spear spamming resistant expertise analysis and ranking which implement these idea for ranking user in a folksonomy we evaluate our method with experiment on data set collected from delicious com comprising over web document million user and million shared bookmark we also show that the algorithm is more resistant to spammer than other method such a the original hit algorithm and simple statistical measure 
will be the mobile eureka moment with the olympics just round the corner and the mobile market experiencing a phenomenal growth we believe that www will truly be the climax and focal point in the midst of this mobile revolution mobile web initiative spearheaded by w c is making a strong stand on how to realize the vision of pervasive mobile computing three screen service tv pc phone are demanding new architecture to be developed furthermore the need to go beyond technology demand an embrace of the human centric aspect of mobile computing the objective of this workshop is to provide a single forum for researcher sociologist and technologist to discus the state of the art present their contribution and set future direction in personalized application for mobile user with a focus in rich social medium the transition from the wired internet to the mobile web ha generated a wide array of novel service bringing in billion of dollar in revenue in the recent year so there are a few natural question that one may ask what is the next big thing in the mobile web a far a application and service are concerned how will operator provide these service efficiently with this next wave of application and what are the emerging business model that drive the next generation of service in a nutshell to answer some of the question above we believe that in general there is a convergence in the fixed and mobile world by mean of standard like ip multimedia subsystem ims this convergence allows enterprise and consumer application to be blended via a unified infrastructure this unification result in a number of advantage like a application availability from any access method b enhanced user experience for combined blended service presence messaging address book etc c centralized user profile shared between application and common provisioning d flexible charging and billing of multimedia service in particular e scalable deployment with guaranteed quality of service and last but not least f secure solution through built in identity management authentication and authorization procedure there are a variety of novel application that are starting to emerge a general shift from static content to a more dynamic nature ha been observed in recent year in particular multimedia and social networking are the foundation of emerging application one such example is the delivery of iptv to the third screen mobile pda social networking messaging application like twitter and peer to peer application are also on the verge of breakthrough in the mobile landscape interestingly a lot of the momentum ha been reinvigorated with the introduction of smart phone like apple s iphone device are becoming more powerful and standard are being written to take advantage of these smart feature finally we believe that the mobile web is at the forefront of technology and this workshop will discus many of these technical and business challenge that are on the wish list of everyday practitioner 
w bpel defines a standard for executable process executable process are business process which can be automated through an it infrastructure the w bpel specification also introduces the concept of abstract process in contrast to their executable sibling abstract process are not executable and can have part where business logic is disguised nevertheless the w bpel specification introduces a notion of compatibility between such an under specified abstract process and a fully specified executable one basically this compatibility notion defines a set of syntactical rule that can be augmented or restricted by profile so far there exist two of such profile the abstract process profile for observable behavior and the abstract process profile for template none of these profile defines a concept of behavioral equivalence therefore both profile are too strict with respect to the rule they impose when deciding whether an executable process is compatible to an abstract one in this paper we propose a novel profile that extends the existing abstract process profile for observable behavior by defining a behavioral relationship we also show that our novel profile allows for more flexibility when deciding whether an executable and an abstract process are compatible 
pseudo relevance feedback is an effective technique for improving retrieval result traditional feedback algorithm use a whole feedback document a a unit to extract word for query expansion which is not optimal a a document may cover several different topic and thus contain much irrelevant information in this paper we study how to effectively select from feedback document those word that are focused on the query topic based on position of term in feedback document we propose a positional relevance model prm to address this problem in a unified probabilistic way the proposed prm is an extension of the relevance model to exploit term position and proximity so a to assign more weight to word closer to query word based on the intuition that word closer to query word are more likely to be related to the query topic we develop two method to estimate prm based on different sampling process experiment result on two large retrieval datasets show that the proposed prm is effective and robust for pseudo relevance feedback significantly outperforming the relevance model in both document based feedback and passage based feedback 
due to the reliance on the textual information associated with an image image search engine on the web lack the discriminative power to deliver visually diverse search result the textual description are key to retrieve relevant result for a given user query but at the same time provide little information about the rich image content in this paper we investigate three method for visual diversification of image search result the method deploy lightweight clustering technique in combination with a dynamic weighting function of the visual feature to best capture the discriminative aspect of the resulting set of image that is retrieved a representative image is selected from each cluster which together form a diverse result set based on a performance evaluation we find that the outcome of the method closely resembles human perception of diversity which wa established in an extensive clustering experiment carried out by human assessor 
novelty detection is a difficult task particularly at sentence level most of the approach proposed in the past consist of re ordering all sentence following their novelty score however this re ordering ha usually little value in fact a naive baseline with no novelty detection capability yield often better performance than any state of the art novelty detection mechanism we argue here that this is because current method initiate too early the novelty detection process when few sentence have been seen it is unlikely that the user is negatively affected by redundancy therefore re ordering the first sentence may be harmful in term of performance we propose here a query dependent method based on cluster analysis to determine where we must start filtering redundancy 
web service composition enables seamless and dynamic integration of business application on the web the performance of the composed application is determined by the performance of the involved web service therefore non functional quality of service aspect are crucial for selecting the web service to take part in the composition identifying the best candidate web service from a set of functionally equivalent service is a multi criterion decision making problem the selected service should optimize the overall qos of the composed application while satisfying all the constraint specified by the client on individual qos parameter in this paper we propose an approach based on the notion of skyline to effectively and efficiently select service for composition reducing the number of candidate service to be considered we also discus how a provider can improve it service to become more competitive and increase it potential of being included in composite application we evaluate our approach experimentally using both real and synthetically generated datasets 
the choice of indexing term used to represent document crucially determines how e ective subsequent retrieval will be ir system commonly use rule based stemmer to normalize surface word form to combat the problem of not finding document that contain word related to query term by inflectional or derivational morphology but such stemmer are not available in all language in this paper we explore the effectiveness of unsupervised morphological segmentation a an alternative to stemming using test set in thirteen european language we find that unsupervised segmentation is significantly better than unnormalized word in several case by more than however rule based stemming if available is better in low complexity language we also compare these method to the use of character n gram finding that on average n gram yield the best performance 
bug locating usually involves intensive search activity and incurs unpredictable cost of labor and time an issue of information retrieval on bug location is particularly addressed to facilitate identifying bug from software code in this paper a novel bug retrieval approach with co location shrinkage c is proposed the proposed approach ha been implemented in open source software project collected from real world repository and consistently improves the retrieval accuracy of a state of the art support vector machine svm model 
many ranking model have been proposed in information retrieval and recently machine learning technique have also been applied to ranking model construction most of the existing method do not take into consideration the fact that significant difference exist between query and only resort to a single function in ranking of document in this paper we argue that it is necessary to employ different ranking model for different query and onduct what we call query dependent ranking a the first such attempt we propose a k nearest neighbor knn method for query dependent ranking we first consider an online method which creates a ranking model for a given query by using the labeled neighbor of the query in the query feature space and then rank the document with respect to the query using the created model next we give two offline approximation of the method which create the ranking model in advance to enhance the efficiency of ranking and we prove a theory which indicates that the approximation are accurate in term of difference in loss of prediction if the learning algorithm used is stable with respect to minor change in training example our experimental result show that the proposed online and offline method both outperform the baseline method of using a single ranking function 
in this paper we describe our mining system which automatically mine tag from feedback text in an ecommerce scenario it render these tag in a visually appealing manner further emoticon are attached to mined tag to add sentiment to the visual aspect 
a the largest online marketplace ebay strives to promote it inventory throughout the web via different type of online advertisement contextually relevant link to ebay asset on third party site is one example of such advertisement avenue keyword extraction is the task at the core of any contextual advertisement system in this paper we explore a machine learning approach to this problem the proposed solution us linear and logistic regression model learnt from human labeled data combined with document text and ebay specific feature in addition we propose a solution to identify the prevalent category of ebay item in order to solve the problem of keyword ambiguity 
today a huge amount of text is being generated for social purpose on social networking service on the web unlike traditional document such text is usually extremely short and tends to be informal analysis of such text benefit many application such a advertising search and content filtering in this work we study one traditional text mining task on such new form of text that is extraction of meaningful keywords we propose several intuitive yet useful feature and experiment with various classification model evaluation is conducted on facebook data performance of various feature and model are reported and compared 
interleaving experiment are an attractive methodology for evaluating retrieval function through implicit feedback designed a a blind and unbiased test for eliciting a preference between two retrieval function an interleaved ranking of the result of two retrieval function is presented to the user it is then observed whether the user click more on result from one retrieval function or the other while it wa shown that such interleaving experiment reliably identify the better of the two retrieval function the naive approach of counting all click equally lead to a suboptimal test we present new method for learning how to score different type of click so that the resulting test statistic optimizes the statistical power of the experiment this can lead to substantial saving in the amount of data required for reaching a target confidence level our method are evaluated on an operational search engine over a collection of scientific article 
in order to address privacy concern many social medium website allow user to hide their personal profile from the public in this work we show how an adversary can exploit an online social network with a mixture of public and private user profile to predict the private attribute of user we map this problem to a relational classification problem and we propose practical model that use friendship and group membership information which is often not hidden to infer sensitive attribute the key novel idea is that in addition to friendship link group can be carrier of significant information we show that on several well known social medium site we can easily and accurately recover the information of private profile user to the best of our knowledge this is the first work that us link based and group based classification to study privacy implication in social network with mixed public and private user profile 
we investigate the effectiveness of both the standard evaluation measure and the opinion component for topical opinion retrieval we analyze how relevance is affected by opinion by perturbing relevance ranking by the outcome of opinion only classifier built by monte carlo sampling topical opinion ranking are obtained by either re ranking or filtering the document of a first pas retrieval of topic relevance the proposed approach establishes the correlation between the accuracy and the precision of the classifier and the performance of the topical opinion retrieval among other result it is possible to ass the effectiveness of the opinion component by comparing the effectiveness of the relevance baseline with the topical opinion ranking 
we investigate to what extent people making relevance judgement for a reusable ir test collection are exchangeable we consider three class of judge gold standard judge who are topic originator and are expert in a particular information seeking task silver standard judge who are task expert but did not create topic and bronze standard judge who are those who did not define topic and are not expert in the task analysis show low level of agreement in relevance judgement between these three group we report on experiment to determine if this is sufficient to invalidate the use of a test collection for measuring system performance when relevance assessment have been created by silver standard or bronze standard judge we find that both system score and system ranking are subject to consistent but small difference across the three assessment set it appears that test collection are not completely robust to change of judge when these judge vary widely in task and topic expertise bronze standard judge may not be able to substitute for topic and task expert due to change in the relative performance of assessed system and gold standard judge are preferred 
we describe a system for synchronization and organization of user contributed content from live music event we start with a set of short video clip taken at a single event by multiple contributor who were using a varied set of capture device using audio fingerprint we synchronize these clip such that overlapping clip can be displayed simultaneously furthermore we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content including identifying key moment of interest and descriptive text for important captured segment of the show we also identify the preferred audio track when multiple clip overlap we thus create a much improved representation of the event that build on the automatic content match our work demonstrates important principle in the use of content analysis technique for social medium content on the web and applies those principle in the domain of live music capture 
this paper address the problem of finding new location of moved web page we discus why the content based approach ha a limitation in solving the problem and why it is important to exploit the knowledge on where to search for the page 
expert search system often employ a document search component to identify on topic document which are then used to identify people likely to have relevant expertise this work investigates the impact of the retrieval effectiveness of the underlying document search component it ha been previously shown that applying technique to the underlying document search component that normally improve the effectiveness of a document search engine also have a positive impact on the retrieval effectiveness of the expert search engine in this work we experiment with fictitious perfect document ranking to attempt to identify an upper bound in expert search system performance our surprising result infer that non relevant document can bring useful expertise evidence and that removing these doe not lead to an upper bound in retrieval performance 
for many user with a physical or motor disability using a computer mouse or other pointing device to navigate the web is cumbersome or impossible due to problem with pointing accuracy at the same time web accessibility using a keyboard in major browser is rudimentary requiring many key press to select link or other element we introduce keysurf a character controlled web navigation system which address this situation by presenting an interface which allows a user to activate any web page element with only two or three keystroke through an implementation of a user centric incremental search algorithm element are matched according to user expectation a character are entered into the interface we show how our interface can be integrated with a speech recognition input a well a with specialized on screen keyboard for people with disability using the user s browsing history we improve the efficiency of the selection process and find potentially interesting page link for the user within the current web page we present the result from a pilot study evaluating the performance of various component of our system 
a half day single track workshop is designed to gather academic researcher and industrial practitioner at to share idea and knowledge of know how and to discus all relevant issue including the business model enabling technology and killer application of web based question answering qa especially the user interactive qa service and application the workshop program consists of two session one for academic paper and the other for industrial practice paper each session consists of a leading talk followed by three short presentation sufficient time is allocated for brainstorming discussion in each session 
one reason for semi supervised clustering fail to deliver satisfactory performance in document clustering is that the transformed optimization problem could have many candidate solution but existing method provide no mechanism to select a suitable one from all those candidate this paper alleviates this problem by posing the same task a a soft constrained optimization problem and introduces the salient degree measure a an information guide to control the searching of an optimal solution experimental result show the effectiveness of the proposed method in the improvement of the performance especially when the amount of priori domain knowledge is limited 
web crawler are increasingly used for focused task such a the extraction of data from wikipedia or the analysis of social network like last fm in these case page are far more uniformly structured than in the general web and thus crawler can use the structure of web page for more precise data extraction and more expressive analysis in this demonstration we present a focused structure based crawler generator the not so creepy crawler nc what set nc apart is that all analysis and decision task of the crawling process are delegated to an arbitrary xml query engine such a xquery or xcerpt customizing crawler just mean writing declarative xml query that can access the currently crawled document a well a the metadata of the crawl process we identify four type of query that together sufice to realize a wide variety of focused crawler we demonstrate nc with two application the first extract data about city from wikipedia with a customizable set of attribute for selecting and reporting these city it illustrates the power of nc where data extraction from wiki style fairly homogeneous knowledge site is required in contrast the second use case demonstrates how easy nc make even complex analysis task on social networking site here exemplified by last fm 
robust statistical learning based web spam detection system often requires large amount of labeled training data however labeled sample are more difficult expensive and time consuming to obtain than unlabeled one this paper proposed link based semi supervised learning algorithm to boost the performance of a classifier which integrates the traditional self training with the topological dependency based link learning the experiment with a few labeled sample on standard webspam uk benchmark showed that the algorithm are effective 
the semantic web initiative ha led to an upsurge of the interest in rule a a general and powerful way of processing combining and analyzing semantic information since several of the technology underlying rule based system are already quite mature it is important to understand how such system might perform on the web scale openrulebench is a suite of benchmark for analyzing the performance and scalability of different rule engine currently the study span five different technology and eleven system but openrulebench is an open community resource and contribution from the community are welcome in this paper we describe the tested system and technology the methodology used in testing and analyze the result 
a recommender system must be able to suggest item that are likely to be preferred by the user in most system the degree of preference is represented by a rating score given a database of user past rating on a set of item traditional collaborative filtering algorithm are based on predicting the potential rating that a user would assign to the unrated item so that they can be ranked by the predicted rating to produce a list of recommended item in this paper we propose a collaborative filtering approach that address the item ranking problem directly by modeling user preference derived from the rating we measure the similarity between user based on the correlation between their ranking of the item rather than the rating value and propose new collaborative filtering algorithm for ranking item based on the preference of similar user experimental result on real world movie rating data set show that the proposed approach outperforms traditional collaborative filtering algorithm significantly on the ndcg measure for evaluating ranked result 
addressing user s information need ha been one of the main goal of web search engine since their early day in some case user cannot see their need immediately answered by search result simply because these need are too complex and involve multiple aspect that are not covered by a single web or search result page this typically happens when user investigate a certain topic in domain such a education travel or health which often require collecting fact and information from many page we refer to this type of activity a research mission these research mission account for of user session and more than of all query volume a verified by a manual analysis that wa conducted by yahoo editor we demonstrate in this paper that such mission can be automatically identified on the fly a the user interacts with the search engine through careful runtime analysis of query flow and query session the on the fly automatic identification of research mission ha been implemented in search pad a novel yahoo application that wa launched in and that we present in this paper search pad help user keeping trace of result they have consulted it novelty however is that unlike previous note taking product it is automatically triggered only when the system decides with a fair level of confidence that the user is undertaking a research mission and thus is in the right context for gathering note beyond the search pad specific application we believe that changing the level of granularity of query modeling from an isolated query to a list of query pertaining to the same research mission so a to better reflect a certain type of information need can be beneficial in a number of other web search application session awareness is growing and it is likely to play in the near future a fundamental role in many on line task this paper present a first step on this path 
adversarial ir in general and search engine spam in particular are engaging research topic with a real world impact for web user advertiser and publisher the airweb workshop will bring researcher and practitioner in these area together to present and discus state of the art technique a well a real world experience given the continued growth in search engine spam creation and detection effort we expect interest in this airweb to surpass that of the previous three edition of the workshop held jointly with www sigir and www respectively 
typically the evaluation of information retrieval ir system is focused upon two main system attribute efficiency and effectiveness however it ha been argued that it is also important to consider accessibility i e the extent to which the ir system make information easily accessible but it is unclear how accessibility relates to typical ir evaluation and specifically whether there is a trade off between accessibility and effectiveness in this poster we empirically explore the relationship between effectiveness and accessibility to determine whether the two objective i e maximizing effectiveness and maximizing accessibility are compatible or not to this aim we empirically examine this relationship using two popular ir model and explore the trade off between access and performance a these model are tuned 
relational database hold a vast quantity of information and making them accessible to the web is an big challenge there is a need to make these database accessible with a little difficulty a possible opening them up to the power and serendipity of the web our work present a series of pattern that bridge the relational database model with the architecture of the web along with an implementation of some of them the aim is for relational database to be made accessible with no intermediate step and no extra metadata required this approach can vastly increase the data available on the web therefore making the web itself all the more powerful while enabling it user to seamlessly perform task that previously required bridging multiple domain and paradigm or were not possible 
this paper proposes a new method for displaying large scale tag cloud we use a topographical image that help user to grasp the relationship among tag intuitively a a background to the tag cloud we apply this interface to a blog navigation system and show that the proposed method enables user to find the desired tag easily even if the tag cloud are very large and above tag our approach is also effective for understanding the overall structure of a large amount of tagged document 
we present the design of plurality an interactive tagging system plurality s modular architecture allows user to automatically generate high quality tag over web content a well a over archival and personal content typically beyond the reach of existing web social tagging system three of the salient feature of plurality are i it self learning and feedback sensitive capability based on a user s personalized tagging style ii it leveraging of the collective intelligence of existing social tagging service and iii it context awareness for optimizing tag suggestion e g based on spatial or temporal feature 
measuring the similarity between semantic relation that hold among entity is an important and necessary step in various web related task such a relation extraction information retrieval and analogy detection for example consider the case in which a person know a pair of entity e g google youtube between which a particular relation hold e g acquisition the person is interested in retrieving other such pair with similar relation e g microsoft powerset existing keyword based search engine cannot be applied directly in this case because in keyword based search the goal is to retrieve document that are relevant to the word used in a query not necessarily to the relation implied by a pair of word we propose a relational similarity measure using a web search engine to compute the similarity between semantic relation implied by two pair of word our method ha three component representing the various semantic relation that exist between a pair of word using automatically extracted lexical pattern clustering the extracted lexical pattern to identify the different pattern that express a particular semantic relation and measuring the similarity between semantic relation using a metric learning approach we evaluate the proposed method in two task classifying semantic relation between named entity and solving word analogy question the proposed method outperforms all baseline in a relation classification task with a statistically significant average precision score of moreover it reduces the time taken by latent relational analysis to process word analogy question from day to le than hour with an sat score of 
query formulation is one of the most difficult and important aspect of information seeking and retrieval two technique term relevance feedback and query suggestion provide method to help user formulate query but each is limited in different way in this research we combine these two technique by automatically creating query suggestion using term relevance feedback technique to evaluate our approach we conducted an interactive information retrieval study with subject and topic each subject completed four topic half with a term suggestion system and half with a query suggestion system we also investigated the source of the suggestion approximately half of all subject were provided with system generated suggestion while half were provided with user generated suggestion result show that subject used more query suggestion than term suggestion and saved more document with these suggestion even though there were no significant difference in performance subject preferred the query suggestion system and rated it higher along a number of dimension including it ability to help them think of new approach to searching qualitative data provided insight into subject usage and rating and indicated that subject often used the suggestion even when they did not click on them 
we propose a cross language retrieval model that is solely based on wikipedia a a training corpus the main contribution of our work are a translation model based on linked text in wikipedia and a term weighting method associated with it a combination scheme to interpolate the link translation model with retrieval based on latent dirichlet allocation on the clef data we achieve improvement with respect to the best german english system at the bilingual track non significant and improvement against a baseline based on machine translation significant 
search engine are important resource for finding information on the web they are also important for publisher and advertiser to present their content to user thus user satisfaction is key and must be quantified in this tutorial we give a practical review of web search metric from a user satisfaction point of view we cover metric for relevance comprehensiveness coverage diversity discovery freshness content freshness and presentation we will also describe how these metric can be mapped to proxy metric for the stage of a generic search engine pipeline the practitioner can apply these metric readily and the researcher can get motivation for new problem to work on especially in formalizing and refining metric 
collaborative tagging is a powerful method to create folksonomies that can be used to grasp filter user preference or enhance web search recent research ha shown that depending on the number of user and the quality of user provided tag powerful community driven semantics or ontology can emerge a it wa evident analyzing user data from social web application such a del icio u or flickr unfortunately most web page do not contain tag and thus no vocabulary that describes the information provided a common problem in web page annotation is to motivate user for constant participation i e tagging in this paper we describe our approach of a binary verification game that embeds collaborative tagging into on line game in order to produce domain specific folksonomies 
a set of word is often insufficient to express a user s information need in order to account for various information need associated with a query diversification seems to be a reasonable strategy by diversifying the result set we increase the probability of result being relevant to the user s information need when the given query is ambiguous a diverse result set must contain a set of document that cover various subtopics for a given query we propose a graph based method which exploit the link structure of the web to return a ranked list that provides complete coverage for a query our method not only provides diversity to the result set but also avoids excessive redundancy moreover the probability of relevance of a document is conditioned on the document that appear before it in the result list we show the effectiveness of our method by comparing it with a query likelihood model a the baseline 
in this paper we propose a new approach to automatically compose data providing web service our approach exploit existing mature work done in data integration system specifically data providing service are modeled a rdf parameterized view over mediated ontology then an rdf oriented algorithm to compose service based on query rewriting technique wa devised we apply also an optimization algorithm on the generated composition to speed up it execution the result of our experiment show that the algorithm scale up very well up to a large number of service covering thus most realistic application 
we present a probabilistic model of a user s search history and a target query reformulation we derive a simple transitive similarity algorithm for disambiguating query and improving history based query reformulation accuracy we compare the merit of this approach to other method and present result on both example assessed by human editor and on automatically labeled click data 
passage can be hidden within a text to circumvent their disallowed transfer such release of compartmentalized information is of concern to all corporate and governmental organization we explore the methodology to detect such hidden passage within a document a document is divided into passage using various document splitting technique and a text classifier is used to categorize such passage we present a novel document splitting technique called dynamic windowing which significantly improves precision recall and f measure 
we propose a novel method to detect cultural difference over the world automatically by using a large amount of geotagged image on the photo sharingweb site such a flickr we employ the state of the art object recognition technique developed in the research community of computer vision to mine representative photo of the given concept for representative local region from a large scale unorganized collection of consumer generated geotagged photo the result help u understand how object scene or event corresponding to the same given concept are visually different depending on local region over the world 
accurate web page classification often depends crucially on information gained from neighboring page in the local web graph prior work ha exploited the class label of nearby page to improve performance in contrast in this work we utilize a weighted combination of the content of neighbor to generate a better virtual document for classification in addition we break page into field finding that a weighted combination of text from the target and field of neighboring page is able to reduce classification error by more than a third we demonstrate performance on a large dataset of page from the open directory project and validate the approach using page from a crawl from the stanford webbase interestingly we find no value in anchor text and unexpected value in page title and especially title of parent page in the virtual document 
with the rapid growth of wireless technology and mobile device there is a great demand for personalized service in m commerce collaborative filtering cf is one of successful technique to produce personalized recommendation for user this paper proposes a novel approach to improve cf algorithm where the contextual information of a user and the multicriteria rating of an item are considered besides the typical information on user and item the multilinear singular value decomposition msvd technique is utilized to explore both explicit relation and implicit relation among user item and criterion we implement the approach in an existing m commerce platform and encouraging experimental result demonstrate it effectiveness 
searching post effectively ha become an important issue in large scale online community especially if search user have different inclination when they search post they have different kind of post in their mind to address this problem in this paper we propose a scheme of ranking post based on search user inclination user ranking score is employed to capture post that are relevant to a specific user inclination specifically we present a scheme to rank post in term of user expertise and popularity experimental result show that different user inclination can produce quite different search result and the proposed scheme achieves about accuracy 
the classical probabilistic model attempt to capture the ad hoc information retrieval problem within a rigorous probabilistic framework it ha long been recognized that the primary obstacle to effective performance of the probabilistic model is the need to estimate a relevance model the dirichlet compound multinomial dcm distribution which relies on hierarchical bayesian modeling technique or the polya urn scheme is a more appropriate generative model than the traditional multinomial distribution for text document we explore a new probabilistic model based on the dcm distribution which enables efficient retrieval and accurate ranking because the dcm distribution capture the dependency of repetitive word occurrence the new probabilistic model is able to model the concavity of the score function more effectively to avoid the empirical tuning of retrieval parameter we design several parameter estimation algorithm to automatically set model parameter additionally we propose a pseudo relevance feedback algorithm based on the latent mixture modeling of the dirichlet compound multinomial distribution to further improve retrieval accuracy finally our experiment show that both the baseline probabilistic retrieval algorithm based on the dcm distribution and the corresponding pseudo relevance feedback algorithm outperform the existing language modeling system on several trec retrieval task 
modeling the beyond topical aspect of relevance are currently gaining popularity in ir evaluation for example the discounted cumulated gain dcg measure implicitly model some aspect of higher order relevance via diminishing the value of relevant document seen later during retrieval e g due to information cumulated redundancy and effort in this paper we focus on the concept of negative higher order relevance nhor made explicit via negative gain value in ir evaluation we extend the computation of dcg to allow negative gain value perform an experiment in a laboratory setting and demonstrate the characteristic of nhor in evaluation the approach lead to intuitively reasonable performance curve emphasizing from the user s point of view the progression of retrieval towards success or failure we discus normalization issue when both positive and negative gain value are allowed and conclude by discussing the usage of nhor to characterize test collection 
we present a new efficient algorithm for top n match retrieval of sequential pattern our approach is based on an incremental approximation of the string edit distance using index information and a stack based search our approach produce hypothesis with average edit error of about edits from the optimal sed result while using only about of the cpu computation 
question answering qa help one go beyond traditional keywords based querying and retrieve information in more precise form than given by a document or a list of document several community based qa cqa service have emerged allowing information seeker pose their information need a question and receive answer from their fellow user a question may receive multiple answer from multiple user and the asker or the community can choose the best answer while the asker can thus indicate if he wa satisfied with the information he received there is no clear way of evaluating the quality of that information we present a study to evaluate and predict the quality of an answer in a cqa setting we chose yahoo answer a such cqa service and selected a small set of question each with at least five answer we asked amazon mechanical turk worker to rate the quality of each answer for a given question based on different criterion each answer wa rated by five different worker we then matched their assessment with the actual asker s rating of a given answer we show that the quality criterion we used faithfully match with asker s perception of a quality answer we furthered our investigation by extracting various feature from question answer and the user who posted them and training a number of classifier to select the best answer using those feature we demonstrate a high predictability of our trained model along with the relative merit of each of the feature for such prediction these model support our argument that in case of cqa contextual information such a a user s profile can be critical in evaluating and predicting content quality 
when search is against structured document it is beneficial to extract information from user query in a format that is consistent with the backend data structure a one step toward this goal we study the problem of query tagging which is to assign each query term to a pre defined category our problem could be approached by learning a conditional random field crf model or other statistical model in a supervised fashion but this would require substantial human annotation effort in this work we focus on a semi supervised learning method for crfs that utilizes two data source a small amount of manually labeled query and a large amount of query in which some word token have derived label i e label information automatically obtained from additional resource we present two principled way of encoding derived label information in a crf model such information is viewed a hard evidence in one setting and a soft evidence in the other in addition to the general methodology of how to use derived label in semi supervised crfs we also present a practical method on how to obtain them by leveraging user click data and an in domain database that contains structured document evaluation on product search query show the effectiveness of our approach in improving tagging accuracy 
we introduce in this paper the family of information based model for ad hoc information retrieval these model draw their inspiration from a long standing hypothesis in ir namely the fact that the difference in the behavior of a word at the document and collection level brings information on the significance of the word for the document this hypothesis ha been exploited in the poisson mixture model in the notion of eliteness in bm and more recently in dfr model we show here that combined with notion related to burstiness it can lead to simpler and better model 
with the continuing advance in data storage and communication technology there ha been an explosive growth of music information from different application domain a an effective technique for organizing browsing and searching large data collection music information retrieval is attracting more and more attention how to measure and model the similarity between different music item is one of the most fundamental yet challenging research problem in this paper we introduce a novel framework based on a multimodal and adaptive similarity measure for various application distinguished from previous approach our system can effectively combine music property from different aspect into a compact signature via supervised learning in addition an incremental locality sensitive hashing algorithm ha been developed to support efficient retrieval process with different kind of query experimental result based on two large music collection reveal various advantage of the proposed framework including effectiveness efficiency adaptiveness and scalability 
we present a novel framework for the query performance prediction task that is estimating the effectiveness of a search performed in response to a query in lack of relevance judgment our approach is based on using statistical decision theory for estimating the utility that a document ranking provides with respect to an information need expressed by the query to address the uncertainty in inferring the information need we estimate utility by the expected similarity between the given ranking and those induced by relevance model the impact of a relevance model is based on it presumed representativeness of the information need specific query performance predictor instantiated from the framework substantially outperform state of the art predictor over five trec corpus 
we present two modification to the popular k mean clustering algorithm to address the extreme requirement for latency scalability and sparsity encountered in user facing web application first we propose the use of mini batch optimization for k mean clustering this reduces computation cost by order of magnitude compared to the classic batch algorithm while yielding significantly better solution than online stochastic gradient descent second we achieve sparsity with projected gradient descent and give a fast accurate projection onto the l ball source code is freely available http code google com p sofia ml 
the new concept proposed in this paper is a query free web search that automatically retrieves a web page including information related to the daily activity that we are currently engaged in for automatically displaying the page on internet connected domestic appliance around u such a television when we are washing a coffee maker for example a web page is retrieved that includes tip such a cleaning a coffee maker with vinegar remove stain well a method designed on the basis of this concept automatically search for a web page by using a query constructed from the use of ordinary household object that is detected by sensor attached to the object an in situ experiment test a variety of ir technique and the experiment confirmed that our daily activity can produce related web page with high accuracy 
clustering and retrieval of web page dominantly relies on analyzing either the content of individual web page or the link structure between them some literature also suggests to use the structure of web page notably the structure of it dom tree however little work considers the visual structure of web page for clustering in this paper i we motivate visual structure based web page clustering and retrieval for a number of application ii we formalize a visual box model based representation of web page that support new metric of visual similarity and iii we report on our current work on evaluating human perception of visual similarity of web page and applying the learned visual similarity feature to web page clustering and retrieval 
mobile social network is a typical social network where one or more individual of similar interest or commonality conversing and connecting with one another using the mobile phone our work in this paper focus on the experimental study for this kind of social network with the support of large scale real mobile call data the main contribution can be summarized a three fold firstly a large scale real mobile phone call log of one city ha been extracted from a mobile phone carrier in china to construct mobile social network secondly common feature of traditional social network such a power law distribution and small diameter etc have been experimented with which we confirm that the mobile social network is a typical scale free network and ha small world phenomenon lastly different from traditional analytical method important property of the actor such a gender and age have been introduced into our experiment with some interesting finding about human behavior for example the middle age people are more active than the young and old people and the female is unusual more active than the male while in the old age 
this paper present a general framework for building classifier that deal with short and sparse text web segment by making the most of hidden topic discovered from large scale data collection the main motivation of this work is that many classification task working with short segment of text web such a search snippet forum chat message blog news feed product review and book movie summary fail to achieve high accuracy due to the data sparseness we therefore come up with an idea of gaining external knowledge to make the data more related a well a expand the coverage of classifier to handle future data better the underlying idea of the framework is that for each classification task we collect a large scale external data collection called universal dataset and then build a classifier on both a small set of labeled training data and a rich set of hidden topic discovered from that data collection the framework is general enough to be applied to different data domain and genre ranging from web search result to medical text we did a careful evaluation on several hundred megabyte of wikipedia m word and medline m word with two task web search domain disambiguation and disease categorization for medical text and achieved significant quality enhancement 
in recent year personal lifelogs pls have become an emerging field of research pls are collection of digital data taken from an individual s life experience gathered from both digital and physical world pls collection can potentially be collected over period of year and thus can be very large in our research group four researcher have been engaged in collection of individual one year long pls data set these collection include log of computer and mobile phone activity digital photo in particular passively captured microsoft sensecam image geo location via gps surrounding people or object via bluetooth and various biometric data the complex data type and heterogeneous structure of this corpus brings great challenge to traditional content based information retrieval ir yet the rich connection integral to personal experience offer exciting potential opportunity to leverage feature from human memory and associated model to support retrieval my phd project aim to develop an interface to assist ir from pls in doing this i plan to exploit feature in human memory in particular the mechanism in associative memory model previous study in personal information re finding have explored the use of generally well remembered attribute or metadata of the search target such a date item type format author of document there have also been system which utilize associated computer item or real life event e g to assist re finding task however few of them looked into exactly what type of associated item event people tend to recall i plan to explore association among pl item a well a their attribute regarding their role in an individual s memory since i believe that some association and type of metadata which are available and feasible for use may have been omitted in existing system due to the method used in previous research where the user behaviour may have been guided by the searching or management tool available to them a indicated by some information seeking study e g different search context search motivation or personal difference such a habit may lead to varied recall of content and information seeking behaviour for this reason i will also investigate the influence on personal information re finding behaviour of context lifestyle and difference in prior personal experience of ir tool result from these study will be used to explore personalisation in search e g to dynamically increase the importance of geo location in scoring of search result for subject who travel frequently a indicated by people tend to make small step to approach the target they are looking for rather than trying to do this in a single search with a perfect query comprising all of the relevant detail partially because of their trouble in recalling them to relieve user from the heavy cognitive burden of recalling the exact target and on the other hand to reduce the rate of inaccurate query caused by false recall my proposed interface will be based on browsing and recognizing instead of traditional recalling and searching for example a user will be able to browse and narrow result by recognizing landmark and estimating the target activity time range from the user s digital or physical life an important issue in my work will be to consider the challenge of evaluating my work with only a very limited number of pl datasets to partially address this issue i am currently in engaged in a number of smaller scale diary study of searching experience for larger number of subject 
with the phenomenal success of networking site e g facebook twitter and linkedin social network have drawn substantial attention on online social networking site link recommendation is a critical task that not only help improve user experience but also play an essential role in network growth in this paper we propose several link recommendation criterion based on both user attribute and graph structure to discover the candidate that satisfy these criterion link relevance is estimated using a random walk algorithm on an augmented social graph with both attribute and structure information the global and local influence of the attribute is leveraged in the framework a well besides link recommendation our framework can also rank attribute in a social network experiment on dblp and imdb data set demonstrate that our method outperforms state of the art method based on network structure and node attribute information for link recommendation 
the ability of fast similarity search at large scale is of great importance to many information retrieval ir application a promising way to accelerate similarity search is semantic hashing which design compact binary code for a large number of document so that semantically similar document are mapped to similar code within a short hamming distance although some recently proposed technique are able to generate high quality code for document known in advance obtaining the code for previously unseen document remains to be a very challenging problem in this paper we emphasise this issue and propose a novel self taught hashing sth approach to semantic hashing we first find the optimal l bit binary code for all document in the given corpus via unsupervised learning and then train l classifier via supervised learning to predict the l bit code for any query document unseen before our experiment on three real world text datasets show that the proposed approach using binarised laplacian eigenmap lapeig and linear support vector machine svm outperforms state of the art technique significantly 
this paper address several key issue in extraction and mining of an academic social network extraction of a researcher social network from the existing web integration of the publication from existing digital library expertise search on a given topic and association search between researcher we developed a social network system called arnetminer based on proposed method to the above problem in total researcher profile and publication were extracted integrated after the system having been in operation for two year the paper describes the architecture and main feature of the system it also briefly present the experimental result of the proposed method 
it ha been widely observed that search query are composed in a very different style from that of the body or the title of a document many technique explicitly accounting for this language style discrepancy have shown promising result for information retrieval yet a large scale analysis on the extent of the language difference ha been lacking in this paper we present an extensive study on this issue by examining the language model property of search query and the three text stream associated with each web document the body the title and the anchor text our information theoretical analysis show that query seem to be composed in a way most similar to how author summarize document in anchor text or title offering a quantitative explanation to the observation in past work we apply these web scale n gram language model to three search query processing sqp task query spelling correction query bracketing and long query segmentation by controlling the size and the order of different language model we find that the perplexity metric to be a good accuracy indicator for these query processing task we show that using smoothed language model yield significant accuracy gain for query bracketing for instance compared to using web count a in the literature we also demonstrate that applying web scale language model can have marked accuracy advantage over smaller one 
many ranking algorithm applying machine learning technique have been proposed in informational retrieval and web search however most of existing approach do not explicitly take into account the fact that query vary significantly in term of ranking and entail different treatment regarding the ranking model in this paper we apply a divide and conquer framework for ranking specialization i e learning multiple ranking model by addressing query difference we first generate query representation by aggregating ranking feature through pseudo feedback and employ unsupervised clustering method to identify a set of ranking sensitive query topic based on training query to learn multiple ranking model for respective ranking sensitive query topic we define a global loss function by combining the ranking risk of all query topic and we propose a unified svm based learning process to minimize the global loss moreover we employ an ensemble approach to generate the ranking result for each test query by applying a set of ranking model of the most appropriate query topic we conduct experiment using a benchmark dataset for learning ranking function a well a a dataset from a commercial search engine experimental result show that our proposed approach can significantly improve the ranking performance over existing single model approach a well a straightforward local ranking approach and the automatically identified ranking sensitive topic are more useful for enhancing ranking performance than pre defined query categorization 
in this poster we present the p p xml data localization layer of xlive an xquery mediation system developed at university of versailles a major challenge in the evaluation of xquery over a p p network in the context of multiple xml source is to abstract from structural heterogeneity most existing approach have mainly exploited varied type of semantic mapping between peer schema and or ontology complex query algorithm are required to exploit these semantic link between peer in contrast our approach focus on a simple semantic layer it is built on a chord dht for indexing semantic description of mediated data source a mapping process transforms an xml view of a mediator each peer is equipped with a mediator to a semantic form that is published into the p p network and stored in the dht for a given user query a search in the dht retrieves all relevant data source able to contribute to the result elaboration then every peer that contains relevant data is directly queried to collect data for elaborating the final answer 
in this paper we first observe and analyze the temporal behavior of user in a social rating network on expressing rating and creating social relation then we model the temporal dynamic of a srn based on our observation and using bidirectional effect of rating and social relationship while existing model for other type of social network have captured some of the factor our model is the first one to represent all four factor i e social relation on rating social influence social relation on social relation transitivity rating on social relation selection and rating on rating correlational influence we also model the strength of each effect throughout the evolution of a srn using our model we develop a generative model for srns such a model can serve a basis for several purpose in particular link prediction rating prediction and prediction of future community structure given the sensitive nature of social network data there are only very few public social rating network datasets this motivates the development of generative model to create such synthetic datasets for research purpose our experimental study on the epinions dataset demonstrates that the proposed model produce social rating network that agree with real world data on a comprehensive set of evaluation criterion much better than existing model 
personalized web service strive to adapt their service advertisement news article etc to individual user by making use of both content and user information despite a few recent advance this problem remains challenging for at least two reason first web service is featured with dynamically changing pool of content rendering traditional collaborative filtering method inapplicable second the scale of most web service of practical interest call for solution that are both fast in learning and computation in this work we model personalized recommendation of news article a a contextual bandit problem a principled approach in which a learning algorithm sequentially selects article to serve user based on contextual information about the user and article while simultaneously adapting it article selection strategy based on user click feedback to maximize total user click the contribution of this work are three fold first we propose a new general contextual bandit algorithm that is computationally efficient and well motivated from learning theory second we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic finally using this offline evaluation method we successfully applied our new algorithm to a yahoo front page today module dataset containing over million event result showed a click lift compared to a standard context free bandit algorithm and the advantage becomes even greater when data get more scarce 
quicklinks for a website are navigational shortcut displayed below the website homepage on a search result page and that let the user directly jump to selected point inside the website since the real estate on a search result page is constrained and valuable picking the best set of quicklinks to maximize the benefit for a majority of the user becomes an important problem for search engine using user browsing trail obtained from browser toolbars and a simple probabilistic model we formulate the quicklink selection problem a a combinatorial optimizaton problem we first demonstrate the hardness of the objective and then propose an algorithm that is provably within a factor of e of the optimal we also propose a different algorithm that work on tree and that can find the optimal solution unlike the previous algorithm this algorithm can incorporate natural constraint on the set of chosen quicklinks the efficacy of our method is demonstrated via empirical result on both a manually labeled set of website and a set for which quicklink click through rate for several webpage were obtained from a real world search engine 
this write up provides a summary of the international workshop on context enabled source and service selection integration and adaptation csssia organized in conjunction with www at beijing china on april nd we outline the motivation for organizing the workshop briefly describe the organizational detail and program of the workshop and summarize each of the paper accepted by the workshop more information about the workshop can be found at http www c adelaide edu au csssia 
pseudo relevance feedback assumes that most frequent term in the pseudo feedback document are useful for the retrieval in this study we re examine this assumption and show that it doe not hold in reality many expansion term identified in traditional approach are indeed unrelated to the query and harmful to the retrieval we also show that good expansion term cannot be distinguished from bad one merely on their distribution in the feedback document and in the whole collection we then propose to integrate a term classification process to predict the usefulness of expansion term multiple additional feature can be integrated in this process our experiment on three trec collection show that retrieval effectiveness can be much improved when term classification is used in addition we also demonstrate that good term should be identified directly according to their possible impact on the retrieval effectiveness i e using supervised learning instead of unsupervised learning 
this paper investigates whether web comment can be exploited for cross medium retrieval comparing web item such a text image video music product or personal profile can be done at various level of detail our focus is on topic similarity we propose to compare user supplied comment on web item in lieu of the commented item themselves if this approach is feasible the task of extracting and mapping feature between arbitrary pair of item type can be circumvented and well known text retrieval model can be applied instead given that comment are available we report on result of a preliminary but nonetheless large scale experiment which show that if comment on textual item are compared with comment on video item topically similar pair achieve a sufficiently high cross medium similarity 
personalization of information retrieval tailor search towards individual user to meet their particular information need personalization system obtain additional information about user and their context beyond the query they submit to the system and use this information to bring the desired document to top rank the additional information can come from various source user preference user behavior context etc to avoid user taking extra effort in providing explicit preference most personalization approach have adopted an implicit strategy to obtain user interest from their behavior and or context such a query history browsing history and so on task topic knowledge and desktop information have been used a evidence for personalization tailoring display time threshold based on task information wa found to improve implicit relevance feedback performance user s familiarity with search topic wa found to be positively correlated with reading time but negatively correlated with search efficacy this indicated the possibility of inferring topic familiarity from searching behavior desktop information wa also found to be a good source for personalization and personalization using only those file relevant to user query are more effective than using the entire desktop data since search often happens in a work task environment we examine how user generated product and retained document can help improve search performance to these end this study look at how the following factor can help personalize search feature of user s work task including task stage and task type user s familiarity with work task topic user s saving and using behavior and task product s that the user generated for the work task work task are designed to include multiple sub task each being a stage two type of sub task interdependence are considered parallel where the sub task do not depend upon each other or dependent where one sub task depends upon the accomplishment of other sub task s the study examines the interaction effect of these factor dwell time and document usefulness it also look at a personalization technique that extract term for query expansion from work task product s and user behavior there are three research question rq doe the stage of the user s task help predict document usefulness from dwell time in the parallel and the dependent task respectively rq doe the user s familiarity with work task topic help predict document usefulness from dwell time in the parallel and the dependent task respectively rq do user s task product s and saving and using behavior help with query disambiguation twenty four participant are recruited each coming three time a three experiment session to a usability laboratory working on three sub task in a general task either a parallel or a dependent take the parallel task a an example it asks the participant to write a three section article on hybrid car and each section is finished in one session the three section focus on honda civic sedan hybrid nissan altima sedan hybrid and toyota camry sedan hybrid respectively when searching for information half of the participant use a query expansion condition where the system recommends search term based on their work in previous session and the other half use a non query expansion system condition data are collected by three major mean logging software that record user system interaction an eye tracker that record eye movement and questionnaire that elicit user background information and their perception on a number of aspect the result will provide new evidence on personalizing search by taking account of the examined contextual factor 
user feedback is considered to be a critical element in the information seeking process an important aspect of the feedback cycle is relevance assessment that ha progressively become a popular practice in web searching activity and interactive information retrieval ir the value of relevance assessment lie in the disambiguation of the user s information need which is achieved by applying various feedback technique such technique vary from explicit to implicit and help determine the relevance of the retrieved document the former type of feedback is usually obtained through the explicit and intended indication of document a relevant positive feedback or irrelevant negative feedback explicit feedback is a robust method for improving a system s overall retrieval performance and producing better query reformulations at the expense of user cognitive resource on the other hand implicit feedback technique tend to collect information on search behavior in a more intelligent and unobtrusive manner by doing so they disengage the user from the cognitive burden of document rating and relevance judgment information seeking activity such a reading time saving printing selecting and referencing have been all treated a indicator of relevance despite the lack of sufficient evidence to support their effectiveness besides their apparent difference both category of feedback technique determine document relevance with respect to the cognitive and situational level of the interactive dialogue that occurs between the user and the retrieval system however this approach doe not account for the dynamic interplay and adaptation that take place between the different dialogue level but most importantly it doe not consider the affective dimension of interaction user interact with intention motivation and feeling apart from real life problem and information object which are all critical aspect of cognition and decision making by evaluating user affective response towards an information object e g a document prior and post to their exposure to it a more accurate understanding of the object s property and degree of relevance to the current information need may be facilitated furthermore system that can detect and respond accordingly to user emotion could potentially improve the naturalness of human computer interaction and progressively optimize their retrieval strategy the current study investigates the role of emotion in the information seeking process a the latter are communicated through multi modal interaction and reconsiders relevance feedback with respect to what occurs on the affective level of interaction a well 
user feedback is considered to be a critical element in the information seeking process especially in relation to relevance assessment current feedback technique determine content relevance with respect to the cognitive and situational level of interaction that occurs between the user and the retrieval system however apart from real life problem and information object user interact with intention motivation and feeling which can be seen a critical aspect of cognition and decision making the study presented in this paper serf a a starting point to the exploration of the role of emotion in the information seeking process result show that the latter not only interweave with different physiological psychological and cognitive process but also form distinctive pattern according to specific task and according to specific user 
we describe preliminary result of experiment with an unsupervised framework for query segmentation transforming keyword query into structured query the resulting query can be used to more accurately search product database and potentially improve result presentation and query suggestion the key to developing an accurate and scalable system for this task is to train a query segmentation or attribute detection system over labeled data which can be acquired automatically from query and click through log the main contribution of our work is a new method to automatically acquire such training data resulting in significantly higher segmentation performance compared to previously reported method 
personalized search is a promising way to better serve different user information need search history is one of the major information source for search personalization we investigated the impact of history length on the effectiveness of personalized ranking we carried out task based user study for web search and obtained ranked relevance judgment for all query query context derived from previous query in the same task are used to re rank result for the current query experimental result show that the performance of personalization generally improves a more query are accumulated but most of the benefit come from a few immediately preceding query 
collaborative filtering aim to predict user taste by minimising the mean error produced when predicting hidden user rating the aim of a deployed recommender system is to iteratively predict user preference over a dynamic growing dataset and system administrator are confronted with the problem of having to continuously tune the parameter calibrating their cf algorithm in this work we formalise cf a a time dependent iterative prediction problem we then perform a temporal analysis of the netflix dataset and evaluate the temporal performance of two cf algorithm we show that due to the dynamic nature of the data certain prediction method that improve prediction accuracy on the netflix probe set do not show similar improvement over a set of iterative train test experiment with growing data we then address the problem of parameter selection and update and propose a method to automatically assign and update per user neighbourhood size that on the temporal scale outperforms setting global parameter 
an improved understanding of the relationship between search intent result quality and searcher behavior is crucial for improving the effectiveness of web search while recent progress in user behavior mining ha been largely focused on aggregate server side click log we present a new search behavior model that incorporates fine grained user interaction with the search result we show that mining these interaction such a mouse movement and scrolling can enable more effective detection of the user s search intent potential application include automatic search evaluation improving search ranking result presentation and search advertising a a case study we report result on distinguishing between research and purchase variant of commercial intent that show our method to be more effective than the current state of the art 
we examine the effect of incorporating gaze based attention feedback from the user on personalizing the search process employing eye tracking data we keep track of document part the user read in some way we use this information on the subdocument level a implicit feedback for query expansion and reranking we evaluated three different variant incorporating gaze data on the subdocument level and compared them against a baseline based on context on the document level our result show that considering reading behavior a feedback yield powerful improvement of the search result accuracy of ca in the general case however the extent of the improvement varies depending on the internal structure of the viewed document and the type of the current information need 
semantic web data exhibit very skewed frequency distribution among term efficient large scale distributed reasoning method should maintain load balance in the face of such highly skewed distribution of input data we show that term based partitioning used by most distributed reasoning approach ha limited scalability due to load balancing problem we address this problem with a method for data distribution based on clustering in elastic region instead of assigning data to fixed peer data flow semi randomly in the network data item speed date while being temporarily collocated in the same peer we introduce a bias in the routing to allow semantically clustered neighborhood to emerge our approach is self organising efficient and doe not require any central coordination we have implemented this method on the marvin platform and have performed experiment on large real world datasets using a cluster of up to node we compute the rdfs closure over different datasets and show that our clustering algorithm drastically reduces computation time calculating the rdfs closure of million triple in minute 
in this paper we address a relatively new and interesting text categorization problem classify a political blog a either liberal or conservative based on it political leaning our subjectivity analysis based method is twofold we identify subjective sentence that contain at least two strong subjective clue based on the general inquirer dictionary from subjective sentence identified we extract opinion expression and other feature to build political leaning classifier experimental result with a political blog corpus we built show that by using feature from subjective sentence can significantly improve the classification performance in addition by extracting opinion expression from subjective sentence we are able to reveal opinion that are characteristic of a specific political leaning to some extent 
most existing automatic taxonomy induction system exploit one or more feature to induce a taxonomy nevertheless there is no systematic study examining which are the best feature for the task under various condition this paper study the impact of using different feature on taxonomy induction for different type of relation and for term at different abstraction level the evaluation show that different condition need different technology or different combination of the technology in particular co occurrence and lexico syntactic pattern are good feature for is a sibling and part of relation contextual co occurrence pattern and syntactic feature work well for concrete term co occurrence work well for abstract term 
previous study on search engine click modeling have identified two presentation factor that affect user behavior position bias the same result will get a different number of click when displayed in different position and externality the same result might get more click when displayed with result of relatively lower quality than when shown with higher quality result in this paper we focus on analyzing the sequence of user action to model user click behavior on sponsored listing shown on the search result page we first show that temporal click sequence are good indicator of externality in the advertising domain we then describe the positional rationality hypothesis to explain both the position bias and the externality and based on this hypothesis we further propose the temporal click model tcm a bayesian framework that is scalable and computationally efficient to the best of our knowledge this is the first attempt in the literature to estimate positional bias externality and unbiased user perceived ad quality from user click log in a combined model we finally evaluate the proposed model on two real datasets each containing over million ad impression obtained from a commercial search engine the experimental result show that tcm outperforms two other competitive method at click prediction 
this work present the use of click graph in improving query intent classifier which are critical if vertical search and general purpose search service are to be offered in a unified user interface previous work on query classification have primarily focused on improving feature representation of query e g by augmenting query with search engine result in this work we investigate a completely orthogonal approach instead of enriching feature representation we aim at drastically increasing the amount of training data by semi supervised learning with click graph specifically we infer class membership of unlabeled query from those of labeled one according to their proximity in a click graph moreover we regularize the learning with click graph by content based classification to avoid propagating erroneous label we demonstrate the effectiveness of our algorithm in two different application product intent and job intent classification in both case we expand the training data with automatically labeled query by over two order of magnitude leading to significant improvement in classification performance an additional finding is that with a large amount of training data obtained in this fashion classifier using only query word phrase a feature can work remarkably well 
guanxi is a type of dyadic social interaction based on feeling qing and trust xin long studied by scholar of chinese origin it ha recently drawn the attention of researcher outside of china we define the concept of guanxi a applied to the interaction between web site we explore method to identify guanxi in the chinese web show the unique characteristic of the chinese web which result from it and introduce a mechanism for simulating guanxi in a web graph model 
having a mechanism to validate the opinion and to identify expert in a forum could help people to favor one opinion against another to achieve this some solution have already been introduced including social network analysis technique and reputation modeling however neither of these solution considers the user knowledge to identify an expert in this paper a novel method is proposed which estimate user knowledge based on the forum itself and identifies the possible area of expertise associated with each user 
the amount of information available on the web ha increased rapidly reaching level that few would ever have imagined possible we live in what could be called the information explosion era and this situation pose new problem for computer scientist user demand useful and reliable information from the web in the shortest time possible but the obstacle to fulfilling this demand are many including language barrier and the so called long tail even worse user may provide only vague specification of the information that they actually want so that a more concrete specification must somehow be inferred by web access tool natural language processing nlp is one of the key technology for solving the above web usability problem almost all the web page provide with the essential information in the form of natural language text and the amount of these text information is huge in order to offer solution to these problem we must perform searching and extracting information from the web text using nlp technology the aim of this workshop nlp challenge in the information explosion era nlpix is to bring researcher and practitioner together in order to discus our most pressing need with respect to accessing information on the web and to discus new idea in nlp technology that might offer viable solution for those issue 
currently most link related application treat all link in the same web page to be identical one link related application usually requires one certain property of hyperlink but actually not all link have this property or they have this property on different level based on a study of how human user judge the link the idea of the link function classification lfc is introduced in this paper the link function reflect the purpose that link are created by web page designer and the way they are used by viewer link in a certain function class imply one certain relationship between the adjacent page and thus they can be assumed to have similar property an algorithm is proposed to analyze the link function based on both vision and structure feature which simulates the reaction on the link of human user current application can be enhanced by lfc with a more accurate modeling of the web graph new mining method can be also developed by making more and stronger assumption on link within each function class due to the purer property set they share 
there are many proposed method for using clickthrough data for common query to improve the quality of search result returned for that query in this study we examine the search behaviour of user in a close knit community on such query we argue that the benefit of using aggregated clickthrough data varies from task to task it may improve document ranking for navigational or specific informational query but is le likely to be of value to user issuing a broad informational query 
we propose a method of classifying xml document and extracting xml schema from xml by inductive inference based on constraint logic programming the goal of this work is to type a large collection of xml approximately but efficiently this can also process xml code written in a different schema or even code which is schema le our approach is intended to achieve identification based on the syntax and semantics of the xml document by information extraction using ontology and to support retrieval and data management our approach ha three step the first step is xml to predicate the second step is to compare predicate and classifies structure which represent similar meaning in different structure and the last step is predicate to rule by using ontology and to maintain xml schema we evaluate similarity of data type and data range by using an ontology dictionary and xml schema is made from result of second and last step 
this paper explores topic aspect i e subtopic or facet classification for english and chinese collection the evaluation model assumes a bilingual user who ha found document on a topic and identified a few passage in each language on aspect of that topic additional passage are then automatically labeled using a k nearest neighbor classifier and local i e result set latent semantic analysis experiment show that when few training example are available in either language classification using training example from both language can often achieve higher effectiveness than using training example from just one language when the total number of training example is held constant classification effectiveness correlate positively with the fraction of same language training example in the training set these result suggest that supervised classification can benefit from hand annotating a few same language example and that when performing classification in bilingual collection it is useful to label some example in each language 
with the increasing importance of search system on the web there is a continuing push to design interface which are a better match with the kind of real world task in which user are engaged in this paper we consider how broad complex search task may be supported via the search interface in particular we consider search task which may be composed of multiple aspect or multiple related subtasks for example in decision making task the user may investigate multiple possible solution before settling on a single final solution while other task such a report writing may involve searching on multiple interrelated topic a search interface is presented which is designed to support such broad search task allowing a user to create search aspect each of which model an independent subtask of some larger task the interface is built on the intuition that user should be able to structure their searching environment when engaged on complex search task where the act of structuring and organization may aid the user in understanding his or her task a user study wa carried out which compared our aspectual interface to a standard web search interface the result suggest that an aspectual interface can aid user when engaged in broad search task where the search aspect must be identified during searching for a task where search aspect were pre defined no advantage over the baseline wa found result for a decision making task were le clear cut but show some evidence for improved task performance 
characterizing the relationship that exists between a person s social group and his her personal behavior ha been a long standing goal of social network analyst in this paper we apply data mining technique to study this relationship for a population of over million people by turning to online source of data the analysis reveals that people who chat with each other using instant messaging are more likely to share interest their web search are the same or topically similar the more time they spend talking the stronger this relationship is people who chat with each other are also more likely to share other personal characteristic such a their age and location and they are likely to be of opposite gender similar finding hold for people who do not necessarily talk to each other but do have a friend in common our analysis is based on a well defined mathematical formulation of the problem and is the largest such study we are aware of 
searching for web service access point is no longer attached to service registry a web search engine have become a new major source for discovering web service in this work we conduct a thorough analytical investigation on the plurality of web service interface that exist on the web today using our web service crawler engine wsce we collect metadata service information on retrieved interface through accessible ubrs service portal and search engine we use this data to determine web service statistic and distribution based on object size type of technology employed and the number of functioning service this statistical data can be used to help determine the current status of web service we determine an intriguing result that of the available web service on the web are considered to be active we further use our finding to provide insight on improving the service retrieval process 
a the mobile internet continues to grow there is an increasing need to provide user with effective search and information access service in order to build more effective mobile search service we must first understand the impact that various interface choice have on mobile user for example the majority of mobile location based search service are built on top of a map visualization but is this intuitive design decision the optimal interface choice from a human centric perspective in order to tackle this fundamental design question we have developed two proactive mobile search interface one map based and the other text based that utilize key mobile context to improve the search and information discovery experience of mobile user in this paper we present the result of an exploratory field study of these two interface involving user over a month period where we focus in particular on the impact that the type of user interface e g map v text ha on the search and information discovery experience of mobile user we highlight the main usage result including that map are not the interface of choice for certain information access task and outline key implication for the design of next generation mobile search service 
well acceptance of relevance feedback and collaborative system ha given the user to express their preference in term of multiple query example the technology devised to utilize these user preference is expected to mine the semantic knowledge embedded within these query example in this paper we propose a video mining framework based on dynamic learning from query using a statistical model for topic prerogative feature selection the proposed method is specifically designed for multiple query example scenario the effectiveness of the proposed framework ha been established with an extensive experimentation on trecvid data collection the result reveal that our approach achieves a performance that is in par with the best result for this corpus without the requirement of any textual data 
we consider the problem of approximating the pagerank of a target node using only local information provided by a link server this problem wa originally studied by chen gan and suel cikm who presented an algorithm for tackling it we prove that local approximation of pagerank even to within modest approximation factor is infeasible in the worst case a it requires probing the link server for n node where n is the size of the graph the di culty emanates from node of high in degree and or from slow convergence of the pagerank random walk we show that when the graph ha bounded in degree and admits fast pagerank convergence then local pagerank approximation can be done using a small number of query unfortunately natural graph such a the web graph are abundant with high in degree node making this algorithm or any other local approximation algorithm too costly on the other hand reverse natural graph tend to have low in degree while maintaining fast pagerank convergence it follows that calculating reverse pagerank locally is frequently more feasible than computing pagerank locally we demonstrate that reverse pagerank is useful for several application including computation of hub score for web page flnding in uencers in social network obtaining good seed for crawling and measurement of semantic relatedness between concept in a taxonomy 
we describe an optimize and dispatch approach for delivering pay per impression advertisement in online advertising the platform provider for an advertising network commits to showing advertiser banner ad while capping the number of advertising message shown to a unique user a the user transition through the network the traditional approach for enforcing frequency cap ha been to use cross site cooky to track user however cross site cooky and other tracking mechanism can infringe on the user privacy in this paper we propose a novel linear programming approach that decides when to show an ad to the user based solely on the page currently viewed by the user we show that the frequency cap are fulfilled in expectation we show the efficacy of that approach using simulation result 
finding the occurrence of structural pattern in xml data is a key operation in xml query processing existing algorithm for this operation focus almost exclusively on path pattern or tree pattern requirement in flexible querying of xml data have motivated recently the introduction of query language that allow a partial specification of path pattern in a query in this paper we focus on the efficient evaluation of partial path query a generalization of path pattern query our approach explicitly deal with repeated label that is multiple occurrence of the same label in a query we show that partial path query can be represented a rooted dag for which a topological ordering of the node exists we present three algorithm for the efficient evaluation of these query under the indexed streaming evaluation model the first one exploit a structural summary of data to generate a set of path pattern that together are equivalent to a partial path query to evaluate these path pattern we extend pathstack so that it can work on path pattern with repeated label the second one extract a spanning tree from the query dag us a stack based algorithm to find the match of the root to leaf path in the tree and merge join the match to compute the answer finally the third one exploit multiple pointer of stack entry and a topological ordering of the query dag to apply a stack based holistic technique an analysis of the algorithm and extensive experimental evaluation show that the holistic algorithm outperforms the other one 
recently the problem of named entity recognition in query nerq is attracting increasingly attention in the field of information retrieval however the lack of context information in short query make some classical named entity recognition ner algorithm fail in this paper we propose to utilize the search session information before a query a it context to address this limitation we propose to improve two classical ner solution by utilizing the search session context which are known a conditional random field crf based solution and topic model based solution respectively in both approach the relationship between current focused query and previous query in the same session are used to extract novel context aware feature experimental result on real user search session data show that the nerq algorithm using search session context performs significantly better than the algorithm using only information of the short query 
with the increasing amount of data and the need to integrate data from multiple data source a challenging issue is to find near duplicate record efficiently in this paper we focus on efficient algorithm to find pair of record such that their similarity are above a given threshold several existing algorithm rely on the prefix filtering principle to avoid computing similarity value for all possible pair of record we propose new filtering technique by exploiting the ordering information they are integrated into the existing method and drastically reduce the candidate size and hence improve the efficiency experimental result show that our proposed algorithm can achieve up to x x speed up over previous algorithm on several real datasets and provide alternative solution to the near duplicate web page detection problem 
visual summarization is a attractive new scheme to summarize web page which can help achieve a more friendly user experience in search and re finding task by allowing user quickly get the idea of what the web page is about and helping user recall the visited web page in this paper we perform a careful study on the recently proposed visual summarization approach including the thumbnail of the web page snapshot the internal image in the web page which is representative of the content in the page and the visual snippet which is a synthesized image based on the internal image the title and the logo found in the web page moreover since the internal image based summarization approach hardly work when the representative internal image are unavailable we propose a new strategy which retrieves the representative image from the external to summarize the web page the experimental result suggest that the various summarization approach have respective advantage on different type of web page while internal image and thumbnail can provide a reliable summarization on web page with dominant image and web page with simple structure respectively the external image are regarded a a useful information to complement the internal image and are demonstrated very useful in helping user understanding new web page the visual snippet performs well on the re finding task since it incorporates the title and logo which are advantageous on identifying the visited web page 
in this paper we present a framework for online discovery of semantic link from relational data our framework is based on declarative specification of the linkage requirement by the user that allows matching data item in many real world scenario these requirement are translated to query that can run over the relational data source potentially using the semantic knowledge to enhance the accuracy of link discovery our framework let data publisher to easily find and publish high quality link to other data source and therefore could significantly enhance the value of the data in the next generation of web 
in this poster we present a novel p p peer to peer based distributed service network dsn which is a next generation operable and manageable distributed core network architecture and functional structure proposed by china mobile for telecommunication service and wireless internet our preliminary implementation of p p voip voice over internet protocol system over dsn platform demonstrate it effectiveness and promising future 
this paper present a novel work on the task of extracting data from web forum million of user contribute rich information to web forum everyday which ha become an important resource for manyweb application such a product opinion retrieval social network analysis and so on the novelty of the proposed algorithm is that it can not only extract the pure text but also distinguish between the original post and reply experimental result on a large number of real web forum indicate that the proposed algorithm can correctly ex 
in a text retrieval community many researcher have shown a good quality of searching a current snapshot of the web however only a small number have demonstrated a good quality of searching a long term archival domain where document are preserved for a long time i e ten year or more in such a domain a search application is not only applicable for archivist or historian but also in a context of national library and enterprise search searching document repository email etc in the rest of this paper we will explain three problem of searching document archive and propose possible approach to solve these problem our main research question is how to improve the quality of search in a document archive using temporal information 
current search engine do not in general perform well with longer more verbose query one of the main issue in processing these query is identifying the key concept that will have the most impact on effectiveness in this paper we develop and evaluate a technique that us query dependent corpus dependent and corpus independent feature for automatic extraction of key concept from verbose query we show that our method achieves higher accuracy in the identification of key concept than standard weighting method such a inverse document frequency finally we propose a probabilistic model for integrating the weighted key concept identified by our method into a query and demonstrate that this integration significantly improves retrieval effectiveness for a large set of natural language description query derived from trec topic on several newswire and web collection 
in this paper we introduce a new web mining and search technique topic initiator detection tid on the web given a topic query on the internet and the resulting collection of time stamped web document which contain the query keywords the task of tid is to automatically return which web document or it author initiated the topic or wa the first to discus about the topic to deal with the tid problem we design a system framework and propose algorithm initrank initiator ranking to rank the web document by their possibility to be the topic initiator we first extract feature from the web document and design several topic initiator indicator then we propose a tcl graph which integrates the time content and link information and design an optimization framework over the graph to compute initrank experiment show that compared with baseline method such a direct time sorting well known link based ranking algorithm pagerank and hit initrank achieves the best overall performance with high effectiveness and robustness in case study we successfully detected the first web document related to a famous rumor of an australia product banned in usa and the pre release of ibm and google cloud computing collaboration before the official announcement 
nearest neighbor collaborative filtering provides a successful mean of generating recommendation for web user however this approach suffers from several shortcoming including data sparsity and noise the cold start problem and scalability in this work we present a novel method for recommending item to user based on expert opinion our method is a variation of traditional collaborative filtering rather than applying a nearest neighbor algorithm to the user rating data prediction are computed using a set of expert neighbor from an independent dataset whose opinion are weighted according to their similarity to the user this method promise to address some of the weakness in traditional collaborative filtering while maintaining comparable accuracy we validate our approach by predicting a subset of the netflix data set we use rating crawled from a web portal of expert review measuring result both in term of prediction accuracy and recommendation list precision finally we explore the ability of our method to generate useful recommendation by reporting the result of a user study where user prefer the recommendation generated by our approach 
we investigated the efficacy of visual and textual web page preview in predicting the helpfulness of web page related to a specific topic we ran two study in the usability lab and collected data through an online survey participant total of were asked to rate the expected helpfulness of a web page based on a preview four different thumbnail variation a textual web page summary a thumbnail title url combination a title url combination in the lab study the same participant also rated the helpfulness of the actual web page themselves in the online study the web page rating were collected from a separate group of participant our result show that thumbnail add information about the relevance of web page that is not available in the textual summary of web page title snippet url however showing only thumbnail with no textual information result in poorer performance than showing only textual summary the prediction inaccuracy caused by textual v visual preview wa different textual preview tended to make user overestimate the helpfulness of web page whereas thumbnail made user underestimate the helpfulness of web page in most case in our study the best performance wa obtained by combining sufficiently large thumbnail at least x pixel with page title and url and it wa better to make user focus primarily on the thumbnail by placing the title and url below the thumbnail our study highlighted four key aspect that affect the performance of preview the visual textual mode of the preview the zoom level and size of the thumbnail a well a the positioning of key information element 
sponsored search system are tasked with matching query to relevant advertisement the current state of the art matching algorithm expand the user s query using a variety of external resource such a web search result while these expansion based algorithm are highly effective they are largely inefficient and cannot be applied in real time in practice such algorithm are applied offline to popular query with the result of the expensive operation cached for fast access at query time in this paper we describe an efficient and effective approach for matching ad against rare query that were not processed offline the approach build an expanded query representation by leveraging offline processing done for related popular query our experimental result show that our approach significantly improves the effectiveness of advertising on rare query with only a negligible increase in computational cost 
in this paper we present a novel approach to pseudo relevance feedback prf called multilingual prf multiprf the key idea is to harness multilinguality given a query in a language we take the help of another language to ameliorate the well known problem of prf viz a the expansion term from prf are primarily based on co occurrence relationship with query term and thus other term which are lexically and semantically related such a morphological variant and synonym are not explicitly captured and b prf is quite sensitive to the quality of the initially retrieved top k document and is thus not robust in multiprf given a query in language l it is translated into language l and prf is performed on a collection in language l and the resultant feedback model is translated from l back into l the final feedback model is obtained by combining the translated model with the original feedback model of the query in l experiment were performed on standard clef collection in language with widely differing characteristic viz french german finnish and hungarian with english a the assisting language we observe that multiprf outperforms prf and is more robust with consistent and significant improvement in the above widely differing language a thorough analysis of the result reveal that the second language help in obtaining both co occurrence based conceptual term a well a lexically and semantically related term additionally the use of the second language collection reduces the sensitivity to performance of initial retrieval thereby making it more robust 
algorithm in distributed information retrieval often rely on accurate knowledge of the size of a collection the multiple capture recapture method of shokouhi et al is one of the more reliable algorithm for determining collection size but it relies on sample with a uniform number of document such uniform sample are often hard to obtain in a working system a simple generalisation of multiple capture recapture doe not rely on uniform sample size simulation show it is a accurate a the original method even when sample size vary considerably making it a useful technique in real tool 
wikisearch is a search engine customized for the wikipedia corpus but with design feature that may be generalized to other search system it feature enhance basic functionality and enable more fluid interactivity while supporting both workflow in the search process and the experimental process used in lab testing 
photo sharing service have attracted million of people and helped construct massive social network on the web a popular trend is that user share their image collection within social group which greatly promotes the interaction between user and expands their social network existing system have difficulty in generating satisfactory social group suggestion because the image are classified independently and their relationship in a collection is ignored in this work we intend to produce suggestion of suitable photo sharing group from a user s personal photo collection by mining image on the web and leveraging the collection context both visual content and textual annotation are integrated to generate initial prediction of the event or topic depicted in the image a user collection based label propagation method is proposed to improve the group suggestion by modeling the relationship of image in the same collection a a sparse weighted graph experiment on real user image and comparison with the state of the art technique demonstrate the effectiveness of the proposed approach 
publication time p time for short of web page is often required in many application area in this paper we address the issue of p time detection and it application for page rank we first propose an approach to extract p time for a page with explicit p time displayed on it body we then present a method to infer p time for a page without p time we further introduce a temporal sensitive page rank model using p time experiment demonstrate that our method outperform the baseline method significantly 
in this paper we present triplify a simplistic but effective approach to publish linked data from relational database triplify is based on mapping http uri request onto relational database query triplify transforms the resulting relation into rdf statement and publishes the data on the web in various rdf serialization in particular a linked data the rationale for developing triplify is that the largest part of information on the web is already stored in structured form often a data contained in relational database but usually published by web application only a html mixing structure layout and content in order to reveal the pure structured information behind the current web we have implemented triplify a a light weight software component which can be easily integrated into and deployed by the numerous widely installed web application our approach includes a method for publishing update log to enable incremental crawling of linked data source triplify is complemented by a library of configuration for common relational schema and a rest enabled data source registry triplify configuration containing mapping are provided for many popular web application including oscommerce wordpress drupal gallery and phpbb we will show that despite it light weight architecture triplify is usable to publish very large datasets such a gb of geo data from the openstreetmap project 
we consider blog feed search identifying relevant blog for a given topic an individual s search behavior often involves a combination of exploratory behavior triggered by salient feature of the information object being examined plus goal directed in depth information seeking behavior we present a two stage blog feed search model that directly build on this insight we first rank blog post for a given topic and use their parent blog a selection of blog that we rank using a blog based model 
in this paper we discus our work in progress towards a scalable hierarchical classification system for book using the library of congress subject hierarchy we examine the characteristic of this domain which make the problem very challenging and we look at several appropriate performance measurement we show that both hieron and hierarchical support vector machine perform moderately well 
social tagging is becoming increasingly popular in many web application where user can annotate resource e g web page with arbitrary keywords i e tag a tag recommendation module can assist user in tagging process by suggesting relevant tag to them it can also be directly used to expand the set of tag annotating a resource the benefit are twofold improving user experience and enriching the index of resource however the former one is not emphasized in previous study though a lot of work ha reported that different user may describe the same concept in different way we address the problem of personalized tag recommendation for text document in particular we model personalized tag recommendation a a query and ranking problem and propose a novel graph based ranking algorithm for interrelated multi type object when a user issue a tagging request both the document and the user are treated a a part of the query tag are then ranked by our graph based ranking algorithm which take into consideration both relevance to the document and preference of the user finally the top ranked tag are presented to the user a suggestion experiment on a large scale tagging data set collected from del icio u have demonstrated that our proposed algorithm significantly outperforms algorithm which fail to consider the diversity of different user interest 
mobile voice search is a fast growing business it provides user an easier way to search for information using voice from mobile device in this paper we describe a statistical approach to query parsing to assure search effectiveness the task is to segment speech recognition asr output including asr best and asr word lattice into segment and associate each segment with needed concept in the application we train the model including concept prior probability query segment generation probability and query subject probability from application data such a query log and source database we apply the learned model on a mobile business search application and demonstrate the robustness of query parsing to asr error 
in this paper we propose a novel framework called smart miner for web usage mining problem which us link information for producing accurate user session and frequent navigation pattern unlike the simple session concept in the time and navigation based approach where session are sequence of web page requested from the server or viewed in the browser smart miner session are set of path traversed in the web graph that corresponds to user navigation among web page we have modeled session construction a a new graph problem and utilized a new algorithm smart sra to solve this problem efficiently for the pattern discovery phase we have developed an efficient version of the apriori all technique which us the structure of web graph to increase the performance from the experiment that we have performed on both real and simulated data we have observed that smart miner produce at least more accurate web usage pattern than other approach including previous session construction method we have also studied the effect of having the referrer information in the web server log to show that different version of smart sra produce similar result our another contribution is that we have implemented distributed version of the smart miner framework by employing map reduce paradigm we conclude that we can efficiently process terabyte of web server log belonging to multiple web site by our scalable framework 
discovering user specific and implicit geographic intention in web search can greatly help satisfy user information need we build a geo intent analysis system that us minimal supervision to learn a model from large amount of web search log for this discovery we build a city language model which is a probabilistic representation of the language surrounding the mention of a city in web query we use several feature derived from these language model to identify user implicit geo intent and pinpoint the city corresponding to this intent determine whether the geo intent is localized around the user current geographic location predict city for query that have a mention of an entity that is located in a specific place experimental result demonstrate the effectiveness of using feature derived from the city language model we find that the system ha over precision and more than accuracy for the task of detecting user implicit city level geo intent the system achieves more than accuracy in determining whether implicit geo query are local geo query neighbor region geo query or none of these the city language model can effectively retrieve city in location specific query with high precision and recall human evaluation show that the language model predicts city label for location specific query with high accuracy 
we present a model of tabbed browsing that represents a hybrid between a markov process capturing the graph of hyperlink and a branching process capturing the birth and death of tab we present a mathematical criterion to characterize whether the process ha a steady state independent of initial condition and we show how to characterize the limiting behavior in both case we perform a series of experiment to compare our tabbed browsing model with pagerank and show that tabbed browsing is able to explain of the deviation between actual measured browsing behavior and the behavior predicted by the simple pagerank model we find this to be a surprising result a the tabbed browsing model doe not make use of any notion of site popularity but simply capture deviation in user likelihood to open and close tab from a particular node in the graph 
recent technology trend in web service indicate that a solution eliminating the perceived complexity of the w standard technology stack may be in sight advocate of representational state transfer rest have come to believe that their idea explaining why the world wide web work are just a applicable to solve enterprise application integration problem and to radically simplify the plumbing required to implement a service oriented architecture soa in this tutorial we give an introduction to the rest architectural style a the foundation for restful web service the tutorial start from the basic design principle of rest and how they are applied to service oriented computing service orientation concentrate on identifying self contained unit of functionality which should then be exposed a easily reusable and repurposable service this tutorial focus not on the identification of those unit but on how to design the service representing them we explain how decision on the soa level already shape the architectural style that will be used for the eventual it architecture and how the soa process itself ha to be controlled to yield service which can then be implemented restfully we do not claim that rest is the only architectural style that can be used for soa design but we do argue that it doe have distinct advantage for loosely coupled service and massive scale and that any soa approach already ha to be specifically restful on the business level to yield meaningful input for it architecture design 
nowadays the increasing amount of semantic data available on the web lead to a new stage in the potential of semantic web application however it also introduces new issue due to the heterogeneity of the available semantic resource one of the most remarkable is redundancy that is the excess of different semantic description coming from different source to describe the same intended meaning in this paper we propose a technique to perform a large scale integration of sens expressed a ontology term in order to cluster the most similar one when indexing large amount of online semantic information it can dramatically reduce the redundancy problem on the current semantic web in order to make this objective feasible we have studied the adaptability and scalability of our previous work on sense integration to be translated to the much larger scenario of the semantic web our evaluation show a good behaviour of these technique when used in large scale experiment then making feasible the proposed approach 
domain expertise can have an important influence on how people search in this poster we present finding from a log based study into how medical domain expert search the web for information related to their expertise a compared with non expert we find difference in site visited query vocabulary and search behavior the finding have implication for the automatic identification of domain expert from interaction log and the use of domain knowledge in application such a query suggestion or page recommendation to support non expert 
in this poster we propose a novel document summarization approach named ontology enriched multi document summarization oms for utilizing background knowledge to improve summarization result oms first map the sentence of input document onto an ontology then link the given query to a specific node in the ontology and finally extract the summary from the sentence in the subtree rooted at the query node by using the domain related ontology oms can better capture the semantic relevance between the query and the sentence and thus lead to better summarization result a a byproduct the final summary generated by oms can be represented a a tree showing the hierarchical relationship of the extracted sentence evaluation result on the collection of press release by miami dade county department of emergency management during hurricane wilma in demonstrate the efficacy of oms 
this paper present smart caching scheme for web browser for modern web application the style formatting and layout calculation often account for substantial amount of the local computation in order to render a web page in this paper we propose two caching scheme to reduce the computation of style formatting and layout calculation named smart style caching and layout caching respectively the stable style data and layout data for dom document object model element are recorded to construct the cache when a web page is browsed the cached data is checked in the granularity of dom element and applied directly if the identified dom element is not changed in the sequent visit to the same page we have implemented a prototype of the proposed caching scheme based on the webkit layout engine the experimental result with web page from the top web site show that with the smart style caching scheme enabled the time consumed for style formatting is reduced by on average with both the smart style caching scheme and layout caching scheme enabled the time consumed for layout calculation are reduced by on average and the overall performance improvement is about 
the world wide web allows user to create and publish a variety of resource including multimedia one most of the contemporary best practice for designing web interface however do not take into account the d technique in this paper we present a novel approach for designing interactive web application layer interface paradigm lip the background layer of the lip type user interface is a d scene which a user cannot directly interact with the foreground layer is html content only taking an action on this content e g pressing a hyperlink scrolling a page can affect the d scene we introduce a reference implementation of lip copernicus the virtual d encyclopedia which show one of the potential path of the evolution of wikipedia towards web based on the evaluation of copernicus we prove that designing web interface according to lip provides user a better browsing experience without harming the interaction 
with the rapid growth in the number of online web service the problem of service adaptation ha received significant attention in matching and adaptation the functional description of service including interface and data a well a behavioral description are important existing work on matching and adaptation focus only on one aspect in this paper we present a semi automated matching approach that considers both service description we introduce two protocol aware service interface matching algorithm i e depth based interface matching and iterative reference based interface matching these algorithm refine the result of interface matching by incorporating the ordering constraint imposed by business protocol definition on service operation we have implemented a prototype and performed experiment using the specification of synthetic and real world web service experiment show that the proposed approach lead to a significant improvement in the quality of matching between service 
automatically assessing the quality and helpfulness of consumer review is more and more desirable with the evolutionary development of online review system existing helpfulness assessment methodology make use of the positive vote fraction a a benchmark and heuristically find a best guess to estimate the helpfulness of review document this benchmarking methodology ignores the voter population size and treat the the same positive vote fraction a the same helpfulness value we propose a review recommendation approach that make use of the probability density of the review helpfulness a the benchmark and exploit graphical model and expectation maximization em algorithm for the inference of review helpfulness the experimental result demonstrate that the proposed approach is superior to existing approach 
in this paper we propose a graph based approach to constructing a multilingual association dictionary from wikipedia in which we exploit two kind of link in wikipedia article to associate multilingual word and concept together in a graph the mined association dictionary is applied in cross language information retrieval clir to verify it quality we evaluate our approach on four clir data set and the experimental result show that it is possible to mine a good multilingual association dictionary from wikipedia article 
query result caching is an important mechanism for search engine efficiency in this study we first review several query feature that are used to determine the content of a static result cache next we introduce a new feature that more accurately represents the popularity of a query by measuring the stability of query frequency over a set of time interval experimental result show that this new feature achieves hit ratio better than those of the previously proposed feature 
we use wikipedia article to semantically inform the generation of query model to this end we apply supervised machine learning to automatically link query to wikipedia article and sample term from the linked article to re estimate the query model on a recent large web corpus we observe substantial gain in term of both traditional metric and diversity measure 
given a collection of document various multi document summarization method have been proposed to generate a short summary however few study have been reported on aggregating different summarization method to possibly generate better summarization result we propose a weighted consensus summarization method to combine the result from single summarization system experimental result on duc data set demonstrate the performance improvement by aggregating multiple summarization system and our proposed weighted consensus summarization method outperforms other combination method 
an improved understanding of the relationship between search intent result quality and searcher behavior is crucial for improving the effectiveness of web search while recent progress in user behavior mining ha been largely focused on aggregate server side click log we present a new class of search behavior model that also exploit fine grained user interaction with the search result we show that mining these interaction such a mouse movement and scrolling can enable more effective detection of the user s search goal potential application include automatic search evaluation improving search ranking result presentation and search advertising we describe extensive experimental evaluation over both controlled user study and log of interaction data collected from hundred of real user the result show that our method is more effective than the current state of the art technique both for detection of searcher goal and for an important practical application of predicting ad click for a given search session 
google scholar allows researcher to search through a free and extensive source of information on scientific publication in this paper we show that within the limited context of sigir proceeding the ranking created by google scholar are both significantly different and very negatively correlated with those of domain expert 
we propose a novel approach to find alias of a given name from the web we exploit a set of known name and their alias a training data and extract lexical pattern that convey information related to alias of name from text snippet returned by a web search engine the pattern are then used to find candidate alias of a given name we use anchor text and hyperlink to design a word co occurrence model and define numerous ranking score to evaluate the association between a name and it candidate alias the proposed method outperforms numerous baseline and previous work on alias extraction on a dataset of personal name achieving a statistically significant mean reciprocal rank of moreover the alias extracted using the proposed method improve recall by in a relation detection task 
typically information retrieval evaluation focus on measuring the performance of the system s ability at retrieving relevant information and not the query s ability however the effectiveness of a retrieval system is strongly influenced by the quality of the query submitted in this paper the effectiveness and effort of querying is empirically examined in the context of the principle of least effort zipf s law and the law of diminishing return this query focused investigation lead to a number of novel finding which should prove useful in the development of future retrieval method and evaluation technique while also motivating further research into query side evaluation 
recently behavioral targeting bt is attracting much attention from both industry and academia due to it rapid growth in online advertising market though a basic assumption of bt which is the user who share similar web browsing behavior will have similar preference over ad ha been empirically verified we argue that the user ad click preference and web browsing behavior are not reflecting the same user intent though they are correlated in this paper we propose to formulate bt a a transfer learning problem we treat the user preference over ad and web browsing behavior a two different user behavioral domain and propose to utilize transfer learning strategy across these two user behavioral domain to segment user for bt ad delivery we show that some classical bt solution could be formulated in transfer learning view a an example we propose to leverage translated learning which is a recent proposed transfer learning algorithm to benefit the bt ad delivery experimental result on real ad click data show that bt user segmentation by the approach of transfer learning can outperform the classical user segmentation strategy for larger than in term of smoothed ad click through rate ctr 
in this paper we formally define the problem of topic modeling with network structure tmn we propose a novel solution to this problem which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data the proposed method bridge topic modeling and social network analysis which leverage the power of both statistical topic model and discrete regularization the output of this model well summarizes topic in text map a topic on the network and discovers topical community with concrete selection of a topic model and a graph based regularizer our model can be applied to text mining problem such a author topic analysis community discovery and spatial text mining empirical experiment on two different genre of data show that our approach is effective which improves text oriented method a well a network oriented method the proposed model is general it can be applied to any text collection with a mixture of topic and an associated network structure 
yahoo answer ya is a large and diverse question answer forum acting not only a a medium for sharing technical knowledge but a a place where one can seek advice gather opinion and satisfy one s curiosity about a countless number of thing in this paper we seek to understand ya s knowledge sharing and activity we analyze the forum category and cluster them according to content characteristic and pattern of interaction among the user while interaction in some category resemble expertise sharing forum others incorporate discussion everyday advice and support with such a diversity of category in which one can participate we find that some user focus narrowly on specific topic while others participate across category this not only allows u to map related category but to characterize the entropy of the user interest we find that lower entropy correlate with receiving higher answer rating but only for category where factual expertise is primarily sought after we combine both user attribute and answer characteristic to predict within a given category whether a particular answer will be chosen a the best answer by the asker 
web search is generally motivated by an information need since asking well formulated question is the fastest and the most natural way to obtain information for human being almost all query posed to search engine correspond to some underlying question which reflect the user s information need accurate determination of these question may substantially improve the quality of search result and usability of search interface in this paper we propose a new framework for question guided search in which a retrieval system would automatically generate potentially interesting question to user based on the search result of a query since the answer to such question are known to exist in the search result these question can potentially guide user directly to the answer that they are looking for eliminating the need to scan the document in the result list moreover in case of imprecise or ambiguous query automatically generated question can naturally engage user into a feedback cycle to refine their information need and guide them towards their search goal implementation of the proposed strategy raise new challenge in content indexing question generation ranking and feedback we propose new method to address these challenge and evaluated them with a prototype system on a subset of wikipedia evaluation result show the promise of this new question guided search strategy 
our aim is to investigate if and how the performance of distributed information retrieval dir system can be improved through personalization toward this aim we are building a testbed of document collection and corresponding personalized relevance judgment in this paper we discus our intended approach for personalizing the three different phase of the dir process we also describe the test collection we are building and discus our methodology for evaluating personalized dir using relevance information taken from social bookmarking data 
associating label with online product can be a labor intensive task we study the extent to which a standard bag of visual word image classifier can be used to tag product with useful information such a whether a sneaker ha lace or velcro strap using scale invariant feature transform sift image descriptor at random keypoints a hierarchical visual vocabulary and a variant of nearest neighbor classification we achieve accuracy between and on and class classification task using several dozen training example we also increase accuracy by combining information from multiple view of the same product 
wikipedia the free encyclopedia now contains over two million english article and is widely regarded a a high quality authoritative encyclopedia some wikipedia article however are of questionable quality and it is not always apparent to the visitor which article are good and which are bad we propose a simple metric word count for measuring article quality in spite of it striking simplicity we show that this metric significantly outperforms the more complex method described in related work 
the success and popularity of social network system such a del icio u facebook myspace and youtube have generated many interesting and challenging problem to the research community among others discovering social interest shared by group of user is very important because it help to connect people with common interest and encourages people to contribute and share more content the main challenge to solving this problem come from the difficulty of detecting and representing the interest of the user the existing approach are all based on the online connection of user and so unable to identify the common interest of user who have no online connection in this paper we propose a novel social interest discovery approach based on user generated tag our approach is motivated by the key observation that in a social network human user tend to use descriptive tag to annotate the content that they are interested in our analysis on a large amount of real world trace reveals that in general user generated tag are consistent with the web content they are attached to while more concise and closer to the understanding and judgment of human user about the content thus pattern of frequent co occurrence of user tag can be used to characterize and capture topic of user interest we have developed an internet social interest discovery system isid to discover the common user interest and cluster user and their saved url by different interest topic our evaluation show that isid can effectively cluster similar document by interest topic and discover user community with common interest no matter if they have any online connection 
in this work we investigate the contribution of query term and their corresponding ad click rate on commercial intent of query a probabilistic model is proposed following the hypothesis that a query is likely to receive ad click based on contribution from it individual term 
how doe the web search behavior of rich and poor people differ do men and woman tend to click on difffferent result for the same query what are some query almost exclusively issued by african american these are some of the question we address in this study our research combine three data source the query log of a major u based web search engine profile information provided by million of it user birth year gender and zip code and u census information including detailed demographic information aggregated at the level of zip code through this combination we can annotate each query with e g the average per caput income in the zip code it originated from though conceptually simple this combination immediately creates a powerful user modeling tool the main contribution of this work are the following first we provide a demographic description of a large sample of search engine user in the u and show that it agrees well with the distribution of the u population second we describe how different segment of the population differ in their search behavior e g with respect to the query they formulate or the url they click third we explore application of our methodology to improve web search relevance and to provide better query suggestion these result enable a wide range of application including improving web search and advertising where for instance targeted advertisement for family vacation could be adapted to the expected income 
collaborative filtering cf algorithm used to build web based recommender system are often evaluated in term of how accurately they predict user rating however current evaluation technique disregard the fact that user continue to rate item over time the temporal characteristic of the system s top n recommendation are not investigated in particular there is no mean of measuring the extent that the same item are being recommended to user over and over again in this work we show that temporal diversity is an important facet of recommender system by showing how cf data change over time and performing a user survey we then evaluate three cf algorithm from the point of view of the diversity in the sequence of recommendation list they produce over time we examine how a number of characteristic of user rating pattern including profile size and time between rating affect diversity we then propose and evaluate set method that maximise temporal recommendation diversity without extensively penalising accuracy 
among the retrieval model that have been proposed in the last year the esa model of gabrilovich and markovitch received much attention the author report on a significant improvement in the retrieval performance which is explained with the semantic concept introduced by the document collection underlying esa their explanation appears plausible but our analysis show that the connection are more involved and that the concept hypothesis doe not hold in our contribution we analyze several property that in fact affect the retrieval performance moreover we introduce a formalization of esa which reveals it close connection to existing retrieval model 
this paper present a transductive approach to learn ranking function for extractive multi document summarization at the first stage the proposed approach identifies topic theme within a document collection which help to identify two set of relevant and irrelevant sentence to a question it then iteratively train a ranking function over these two set of sentence by optimizing a ranking loss and fitting a prior model built on keywords the output of the function is used to find further relevant and irrelevant sentence this process is repeated until a desired stopping criterion is met 
query log analysis ha received substantial attention in recent year in which the click graph is an important technique for describing the relationship between query and url state of the art approach based on the raw click frequency for modeling the click graph however are not noise eliminated nor do they handle heterogeneous query url pair well in this paper we investigate and develop a novel entropy biased framework for modeling click graph the intuition behind this model is that various query url pair should be treated differently i e common click on le frequent but more specific url are of greater value than common click on frequent and general url based on this intuition we utilize the entropy information of the url and introduce a new concept namely the inverse query frequency iqf to weigh the importance discriminative ability of a click on a certain url the iqf weighting scheme is never explicitly explored or statistically examined for any bipartite graph in the information retrieval literature we not only formally define and quantify this scheme but also incorporate it with the click frequency and user frequency information on the click graph for an effective query representation to illustrate our methodology we conduct experiment with the aol query log data for query similarity analysis and query suggestion task experimental result demonstrate that considerable improvement in performance are obtained with our entropy biased model moreover our method can also be applied to other bipartite graph 
in online advertising pervasive in commercial search engine advertiser typically bid on few term and the scarcity of data make ad matching difficult suggesting additional bidterms can significantly improve ad clickability and conversion rate in this paper we present a large scale bidterm suggestion system that model an advertiser s intent and find new bidterms consistent with that intent preliminary experiment show that our system significantly increase the coverage of a state of the art production system used at yahoo while maintaining comparable precision 
traditional retrieval evaluation us explicit relevance judgment which are expensive to collect relevance assessment inferred from implicit feedback such a click through data can be collected inexpensively but may be le reliable we compare assessment derived from click through data to another source of implicit feedback that we assume to be highly indicative of relevance purchase decision evaluating retrieval run based on a log of an audio visual archive we find agreement between system ranking and purchase decision to be surprisingly high 
most traditional ranking model roughly score the relevance of a given document by observing simple term statistic such a the occurrence of query term within the document or within the collection intuitively the relative importance of query term with regard to other individual non query term in a document can also be exploited to promote the rank of document in which the query is dedicated a the main topic in this paper we introduce a simple technique named intra document term ranking which involves ranking all the term in a document according to their relative importance within that particular document we demonstrate that the information regarding the rank position of given query term within the intra document term ranking can be useful for enhancing the precision of top retrieved result by traditional ranking model experiment are conducted on three standard trec test collection 
in this paper we present adheat a social ad model considering user influence in addition to relevance for matching ad traditionally ad placement employ the relevance model such a model match ad with web page content user interest or both we have observed however on social network that the relevance model suffers from two shortcoming first influential user user who contribute opinion seldom click ad that are highly relevant to their expertise second because influential user content and activity are attractive to other user hint word summarizing their expertise and activity may be widely preferred therefore we propose adheat which diffuses hint word of influential user to others and then match ad for each user with aggregated hint we performed experiment on a large online q a community with half a million user the experimental result show that adheat outperforms the relevance model on ctr click through rate by significant margin 
in this paper we incorporate concrete domain and action theory into a very expressive description logic dl called alcqo notably this extension can significantly augment the expressive power for modeling and reasoning about dynamic aspect of service contracting meanwhile the original nature and advantage of classical dl are also preserved to the extent possible 
in my research i propose a coherence measure with the goal of discovering and using topic structure within and between document of which i explore it extension and application in information retrieval 
supporting fast access to large rdf store ha been one of key challenge for enabling use of the semantic web in real life application more so in sensor based system where large amount of historic data need to be stored we propose a semantics based temporal view mechanism that enables faster access to time varying data by caching into memory only the required subset of rdf triple we describe our experience of implementing such a framework in the context of a wide area network monitoring system our preliminary result show that our solution significantly improves client access time and scale well for moderate data set 
online forum host a rich information exchange often with contribution from many subject matter expert in this work we evaluate algorithm for thread retrieval in a large and active online forum community we compare method that utilize thread structure to a na ve method that treat a thread a a single document we find that thread structure help and additionally selective method of thread scoring which only use evidence from a small number of message in the thread significantly and consistently outperform inclusive method which use all the message in the thread 
we report the result of the analysis of website traffic log and argue that both unique ip address and cooky vastly overestimate unique visitor e g by factor of in our study google analytics absolute unique visitor measure is shown to produce similar x overestimation to address the problem we present a new model for relating unique visitor to ip address or cooky 
we address the task of separating personal from non personal blog and report on a set of baseline experiment where we compare the performance on a small set of feature across a set of five classifier we show that with a limited set of feature a performance of up to can be obtained 
we present a novel passage based approach to re ranking document in an initially retrieved list so a to improve precision at top rank while most work on passage based document retrieval rank a document based on the query similarity of it constituent passage our approach leverage information about the centrality of the document passage with respect to the initial document list passage centrality is induced over a bipartite document passage graph wherein edge weight represent document passage similarity empirical evaluation show that our approach yield effective re ranking performance furthermore the performance is superior to that of previously proposed passage based document ranking method 
we introduce a new dissimilarity function for ranked list the expected weighted hoeffding distance that ha several advantage over current dissimilarity measure for ranked search result first it is easily customized for user who pay varying degree of attention to website at different rank second unlike existing measure such a generalized kendall s tau it is based on a true metric preserving meaningful embeddings when visualization technique like multi dimensional scaling are applied third our measure can effectively handle partial or missing rank information while retaining a probabilistic interpretation finally the measure can be made computationally tractable and we give a highly efficient algorithm for computing it we then apply our new metric with multi dimensional scaling to visualize and explore relationship between the result set from different search engine showing how the weighted hoeffding distance can distinguish important difference in search engine behavior that are not apparent with other rank distance metric such visualization are highly effective at summarizing and analyzing insight on which search engine to use what search strategy user can employ and how search result evolve over time we demonstrate our technique using a collection of popular search engine a representative set of query and frequently used query manipulation method 
in this demo we present a system called irin designed for performing image retrieval in image rich information network we first introduce mok simrank to significantly improve the speed of simrank one of the most popular algorithm for computing node similarity in information network next we propose an algorithm called simlearn to extend mok simrank to heterogeneous image rich information network and account for both link based and content based similarity by seamlessly integrating reinforcement learning with feature learning 
a opposed to representing a document a a bag of word in most information retrieval application we propose a model of representing a web page a set of named entity of multiple type specifically four type of named entity are extracted namely person geographic location organization and time moreover the relation among these entity are also extracted weighted classified and marked by label on top of this model some interesting application are demonstrated in particular we introduce a notion of person activity which contains four different element person location time and activity with this notion and based on a reasonably large set of web page we are able to show how one person s activity can be attributed by time and location which give a good idea of the mobility of the person under question 
with the enormous and still growing amount of data and user interaction on the web it becomes more and more necessary for data consumer to be able to ass the trustworthiness of data on the web in this paper we present a framework for trust establishment and assessment on the web of data different from many approach that build trust metric within network of people we propose a model to represent the trust in concrete piece of web data for a specific consumer in which also the context is considered further more we provide three strategy for trust assessment based on the principle of linked data to overcome the shortcoming of the conventional web of document i e the lack of semantics and interlinking 
question answering system increasingly need to deal with complex information need that require more than simple factoid answer the evaluation of such system is usually carried out using precisionor recall based system performance metric previous work ha demonstrated that when user are shown two search result list side by side they can reliably differentiate between the quality of the list we investigate the consistency between this user based approach and system oriented metric in the question answering environment our initial result indicate that the two methodology show a high level of disagreement 
the ever growing volume of web data make it increasingly challenging to accurately find relevant information about a specific person on the web to address the challenge caused by name ambiguity in web people search this paper explores a novel graph based framework to both disambiguate and tag people entity in web search result experimental result demonstrate the effectiveness of the proposed framework in tag discovery and name disambiguation 
evaluation metric play a critical role both in the context of comparative evaluation of the performance of retrieval system and in the context of learning to rank ltr a objective function to be optimized many different evaluation metric have been proposed in the ir literature with average precision ap being the dominant one due a number of desirable property it posse however most of these measure including average precision do not incorporate graded relevance in this work we propose a new measure of retrieval effectiveness the graded average precision gap gap generalizes average precision to the case of multi graded relevance and inherits all the desirable characteristic of ap it ha a nice probabilistic interpretation it approximates the area under a graded precision recall curve and it can be justified in term of a simple but moderately plausible user model we then evaluate gap in term of it informativeness and discriminative power finally we show that gap can reliably be used a an objective metric in learning to rank by illustrating that optimizing for gap using softrank and lambdarank lead to better performing ranking function than the one constructed by algorithm tuned to optimize for ap or ndcg even when using ap or ndcg a the test metric 
we explore an alternative information retrieval paradigm called query by multiple example qbme where the information need is described not by a set of term but by a set of document intuitive idea for qbme include using the centroid of these document or the well known rocchio algorithm to construct the query vector we consider this problem from the perspective of text classification and find that a better query vector can be obtained through learning with support vector machine svms for online query we show how svms can be learned from one class example in linear time for offline query we show how svms can be learned from positive and unlabeled example together in linear or polynomial time the effectiveness and efficiency of the proposed approach have been confirmed by our experiment on four real world datasets 
this paper proposes a method of crawling web server connected to the internet without imposing a high processing load we are using the crawler for a field survey of the digital divide including the ability to connect to the network rather than employing normal web page crawling algorithm which usually collect all page found on the target server we have developed server crawling algorithm which collect only minimum page from the same server and achieved low load and high speed crawling of server 
back in the heady day of and www toronto we held a panel titled finding anything in the billion page web are algorithm the key in retrospect the answer to this question seems laughably obvious the search industry ha burgeoned on a foundation of algorithm cloud computing and machine learning a we move into the second decade of this millennium we are confronted with a dizzying array of new paradigm for finding content including social network and location based search and advertising this panel pull together senior expert from academia and the major search principal to debate whether search will continue to look anything like the keywords give blue link paradigm that google ha popularized what do emerging approach and paradigm natural language search social search location based search mean for the future of search in general 
we present a document expansion approach that us conditional random field crf segmentation to automatically extract salient phrase from ad title we then supplement the ad document with query segment that are probable translation of the document phrase a learned from a large commercial search engine s click log our approach provides a significant improvement in dcg and interpolated precision and recall on a large set of human labeled query ad pair 
we organized a machine translation mt task at the seventh ntcir workshop participating group were requested to machine translate sentence in patent document and also search topic for retrieving patent document across language we analyzed the relationship between the accuracy of mt and it effect on the retrieval accuracy 
user interactive question answering qa community such a yahoo answer are growing in popularity however a these qa site always have thousand of new question posted daily it is difficult for user to find the question that are of interest to them consequently this may delay the answering of the new question this give rise to question recommendation technique that help user locate interesting question in this paper we adopt the probabilistic latent semantic analysis plsa model for question recommendation and propose a novel metric to evaluate the performance of our approach the experimental result show our recommendation approach is effective 
this paper present a near real time multilingual news monitoring and analysis system that form the backbone of our research work the system integrates technology to address the problem related to information extraction and analysis of open source intelligence on the world wide web by chaining together different technique in text mining automated machine learning and statistical analysis we can automatically determine who where and to a certain extent what is being reported in news article 
in this paper we propose a probabilistic survival model derived from the survival analysis theory for measuring aspect novelty the retrieved document query relevance and novelty are combined at the aspect level for re ranking experiment conducted on the trec and genomics collection demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval 
the desktop search tool provide powerful query capability and result presentation technique however they do not take the user context into account we propose to exploit collected information about user activity with desktop file and application for activity based desktop search when i prepare for a project review and type in a search box the name of a colleague i expect to find her last deliverable draft but not her email with a paper review or our joint conference presentation ideally the desktop search system should be able to infer my current task from the log of my previous activity and present task specific search result 
search trail mined from browser or toolbar log comprise query and the post query page that user visit implicit endorsement from many trail can be useful for search result ranking where the presence of a page on a trail increase it query relevance follow ing a search trail requires user effort yet little is known about the benefit that user obtain from this activity versus say sticking with the clicked search result or jumping directly to the destination page at the end of the trail in this paper we present a log based study estimating the user value of trail following we compare the relevance topic coverage topic diversity novelty and utility of full trail over that provided by sub trail trail origin landing page and trail destination page where trail end our finding demonstrate significant value to user in following trail especially for certain query type the finding have implication for the design of search system including trail recommendation system that display trail on search result page 
in this paper we describe the fully dynamic semantic portal we implemented integrating semantic web technology and service oriented architecture soa the goal of the portal are twofold first it help administrator to easily propose new feature in the portal using semantics to ease the orchestration process secondly it automatically generates a customized user interface for these scenario this user interface take into account different device and assist end user in the use of the portal taking benefit of context awareness all the added value of this portal is based on a core semantics defined by an ontology we present here the main feature of this portal and how it wa implemented using state of the art technology and framework 
a with any application of machine learning web search ranking requires labeled data the label usually come in the form of relevance assessment made by editor click log can also provide an important source of implicit feedback and can be used a a cheap proxy for editorial label the main difficulty however come from the so called position bias url appearing in lower position are le likely to be clicked even if they are relevant in this paper we propose a dynamic bayesian network which aim at providing u with unbiased estimation of the relevance from the click log experiment show that the proposed click model outperforms other existing click model in predicting both click through rate and relevance 
contextual advertising also called content match refers to the placement of small textual ad within the content of a generic web page it ha become a significant source of revenue for publisher ranging from individual blogger to major newspaper at the same time it is an important way for advertiser to reach their intended audience this reach depends on the total number of exposure of the ad impression and it click through rate ctr that can be viewed a the probability of an end user clicking on the ad when shown these two orthogonal critical factor are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaign in this paper we address the problem of forecasting the number of impression for new or changed ad in the system producing such forecast even within large margin of error is quite challenging ad selection in contextual advertising is a complicated process based on ten or even hundred of page and ad feature the publisher content and traffic vary over time and the scale of the problem is daunting over a course of a week it involves billion of impression hundred of million of distinct page hundred of million of ad and varying bid of other competing advertiser we tackle these complexity by simulating the presence of a given ad with it associated bid over week of historical data we obtain an impression estimate by counting how many time the ad would have been displayed if it were in the system over that period of time we estimate this count by an efficient two level search algorithm over the distinct page in the data set experimental result show that our approach can accurately forecast the expected number of impression of contextual ad in real time we also show how this method can be used in tool for bid selection and ad evaluation 
we describe a mixed modality method to index and search software documentation in three way plain text ocr text of embedded figure and visual feature of these figure using a corpus of computer book with a total of page and figure we empirically demonstrate that our method achieves better precision recall than do alternative based on single modality 
search engine return ranked list of web page in response to query these page are starting point for post query navigation but may be insufficient for search task involving multiple step search trail mined from toolbar log start with a query and contain page visited by one user during post query navigation implicit endorsement from many trail can enhance result ranking rather than using trail solely to improve ranking it may also be worth providing trail information directly to user in this paper we quantify the benefit that user currently obtain from trail following and compare different method for finding the best trail for a given query and each top ranked result we compare the relevance topic coverage topic diversity and utility of trail selected using different method and break out finding by factor such a query type and origin relevance our finding demonstrate value in trail highlight interesting difference in the performance of trailfinding algorithm and show we can find best trail for a query that outperform the trail most user follow finding have implication for enhancing web information seeking using trail 
we demonstrate that regularization can improve feedback in a language modeling framework 
this paper introduces a novel method for composing web service in the presence of external volatile information our approach which we call the informed presumptive is compared to previous state of the art approach for web service composition in volatile environment we show empirically that the informed presumptive strategy produce composition in significantly le time than the other strategy with lesser backtracks 
most of today s state of the art retrieval model including bm and language modeling are grounded in probabilistic principle having a working understanding of these principle can help researcher understand existing retrieval model better and also provide industrial practitioner with an understanding of how such model can be applied to real world problem this half day tutorial will cover the fundamental of two dominant probabilistic framework for information retrieval the classical probabilistic model and the language modeling approach the element of the classical framework will include the probability ranking principle the binary independence model the poisson model and the widely used bm model within language modeling framework we will discus various distributional assumption and smoothing technique special attention will be devoted to the event space and independence assumption underlying each approach the tutorial will outline several technique for modeling term dependence and addressing vocabulary mismatch we will also survey application of probabilistic model in the domain of cross language and multimedia retrieval the tutorial will conclude by suggesting a set of open problem in probabilistic model of ir attendee should have a basic familiarity with probability and statistic a brief refresher of basic concept including random variable event space conditional probability and independence will be given at the beginning of the tutorial in addition to slide some hand on exercise and example will be used throughout the tutorial 
in this paper we present a novel approach to semantic theme based video retrieval that considers entire video a retrieval unit and exploit automatically detected visual concept to improve the result of retrieval based on spoken content we deploy a query prediction method that make use of a coherence indicator calculated on top returned document and taking into account the information about visual concept presence in video to make a choice between query expansion method the main contribution of our approach is in it ability to exploit noisy shot level concept detection to improve semantic theme based video retrieval strikingly improvement is possible using an extremely limited set of concept in the experiment performed on trecvid and datasets our approach show an interesting performance improvement compared to the best performing baseline 
we study entity ranking on the inex entity track and propose a simple graph based ranking approach that enables to combine score on document and paragraph level the combined approach improves the retrieval result not only on the inex testset but similarly on trec s expert finding task 
sentiment classification aim to automatically predict sentiment polarity e g positive or negative of user publishing sentiment data e g review blog although traditional classification algorithm can be used to train sentiment classifier from manually labeled text data the labeling work can be time consuming and expensive meanwhile user often use some different word when they express sentiment in different domain if we directly apply a classifier trained in one domain to other domain the performance will be very low due to the difference between these domain in this work we develop a general solution to sentiment classification when we do not have any label in a target domain but have some labeled data in a different domain regarded a source domain in this cross domain sentiment classification setting to bridge the gap between the domain we propose a spectral feature alignment sfa algorithm to align domain specific word from different domain into unified cluster with the help of domain independent word a a bridge in this way the cluster can be used to reduce the gap between domain specific word of the two domain which can be used to train sentiment classifier in the target domain accurately compared to previous approach sfa can discover a robust representation for cross domain data by fully exploiting the relationship between the domain specific and domain independent word via simultaneously co clustering them in a common latent space we perform extensive experiment on two real world datasets and demonstrate that sfa significantly outperforms previous approach to cross domain sentiment classification 
to improve the search result for socially connect user we propose a ranking framework social network document rank sndocrank this framework considers both document content and the similarity between a searcher and document owner in a social network and us a multi level actor similarity ma algorithm to efficiently calculate user similarity in a social network our experiment result based on youtube data show that compared with the tf idf algorithm the sndocrank method return more relevant document of interest our finding suggest that in this framework a searcher can improve search by joining larger social network having more friend and connecting larger local community in a social network 
document in many corpus such a digital library and webpage contain both content and link information to explicitly consider the document relation represented by link in this paper we propose a citation topic ct model which assumes a probabilistic generative process for corpus in the ct model a given document is modeled a a mixture of a set of topic distribution each of which is borrowed cited from a document that is related to the given document moreover the ct model contains a random process for selecting the related document according to the structure of the generative model determined by link and therefore the transitivity of the relation among document is captured we apply the ct model on the document clustering task and the experimental comparison against several state of the art approach demonstrate very promising performance 
email spam filter are commonly trained on a sample of spam and ham non spam message we investigate the effect on filter performance of using sample of spam and ham message sent month before those to be filtered our result show that filter performance deteriorates with the overall age of spam and ham sample but at different rate spam and ham sample of different age may be mixed to advantage provided temporal cue are elided 
recent effort on the task of spoken document retrieval sdr have made use of speech lattice speech lattice contain information about alternative speech transcription hypothesis other than the best transcript and this information can improve retrieval accuracy by overcoming recognition error present in the best transcription in this paper we look at using lattice for the query by example spoken document retrieval task retrieving document from a speech corpus where the query are themselves in the form of complete spoken document query exemplar we extend a previously proposed method for sdr with short query to the query by example task specifically we use a retrieval method based on statistical modeling we compute expected word count from document and query lattice estimate statistical model from these count and compute relevance score a divergence between these model experimental result on a speech corpus of conversational english show that the use of statistic from lattice for both document and query exemplar result in better retrieval accuracy than using only best transcript for either document or query or both in addition we investigate the effect of stop word removal which further improves retrieval accuracy to our knowledge our work is the first to have used a lattice based approach to query by example spoken document retrieval 
advance in digital capture and storage technology mean that it is now possible to capture and store one s entire life experience in personal digital archive these vast personal archive or human digital memory hdms pose new challenge and opportunity for the research community not the least of which is developing effective mean of retrieval from hdms personal archive retrieval research is still in it infancy and there is much scope for novel research my phd proposes to develop effective hdm retrieval algorithm by combining rich source of context associated with item such a location and people present data with information obtained by linking hdm item in novel way 
in this paper we describe two example of implementation of the medium fragment uri specification which is currently being developed by the w c medium fragment working group the group s mission is to create standard addressing scheme for medium fragment on the web using uniform resource identifier uris we describe two scenario to illustrate the implementation more specifically we show how user agent ua will either be able to resolve medium fragment uris without help from the server or will make use of a medium fragment aware server finally we present some ongoing discussion and issue regarding the implementation of the medium fragment specification 
in this paper we formalize the problem of basic graph pattern bgp optimization for sparql query and main memory graph implementation of rdf data we define and analyze the characteristic of heuristic for selectivity based static bgp optimization the heuristic range from simple triple pattern variable counting to more sophisticated selectivity estimation technique customized summary statistic for rdf data enable the selectivity estimation of joined triple pattern and the development of efficient heuristic using the lehigh university benchmark lubm we evaluate the performance of the heuristic for the query provided by the lubm and discus some of them in more detail 
in the sponsored search model search engine are paid by business that are interested in displaying ad for their site alongside the search result business bid for keywords and their ad is displayed when the keyword is queried to the search engine an important problem in this process is keyword generation given a business that is interested in launching a campaign suggest keywords that are related to that campaign we address this problem by making use of the query log of the search engine we identify query related to a campaign by exploiting the association between query and url a they are captured by the user s click these query form good keyword suggestion since they capture the wisdom of the crowd a to what is related to a site we formulate the problem a a semi supervised learning problem and propose algorithm within the markov random field model we perform experiment with real query log and we demonstrate that our algorithm scale to large query log and produce meaningful result 
in recent year active learning method based on experimental design achieve state of the art performance in text classification application although these method can exploit the distribution of unlabeled data and support batch selection they cannot make use of labeled data which often carry useful information for active learning in this paper we propose a novel active learning method for text classification called supervised experimental design sed which seamlessly incorporates label information into experimental design experimental result show that sed outperforms it counterpart which either discard the label information even when it is available or fail to exploit the distribution of unlabeled data 
a website can regulate search engine crawler access to it content using the robot exclusion protocol specified in it robot txt file the rule in the protocol enable the site to allow or disallow part or all of it content to certain crawler resulting in a favorable or unfavorable bias towards some of them a survey on the robot txt usage of about site found some evidence of such bias the news of which led to widespread discussion on the web in this paper we report on our survey of about million site our survey try to correct the shortcoming of the previous survey and show the lack of any significant preference towards any particular search engine category and subject descriptor h informa 
in the field of information retrieval one is often faced with the problem of computing the correlation between two ranked list the most commonly used statistic that quantifies this correlation is kendall s often time in the information retrieval community discrepancy among those item having high ranking are more important than those among item having low ranking the kendall s statistic however doe not make such distinction and equally penalizes error both at high and low ranking in this paper we propose a new rank correlation coefficient ap correlation ap that is based on average precision and ha a probabilistic interpretation we show that the proposed statistic give more weight to the error at high ranking and ha nice mathematical property which make it easy to interpret we further validate the applicability of the statistic using experimental data 
this paper describes a method to automatically acquire query translation pair by mining web click through data the extraction requires no crawling or chinese word segmentation and can capture popular translation experimental result on a real click through data show that only of the extracted query are in the dictionary and our method can achieve in top to in top precision in translating web query moreover the extracted translation are semantically relevant to the source query which is particularly useful for cross lingual information retrieval clir 
social tagging describes a community of user labeling web content with tag it is a simple activity that enriches our knowledge about resource on the web for a computer to help user search the tagged repository it must know when tag are good or bad we describe tagscore a scoring function that rate the goodness of tag the tag and their rating give u a succinct synopsis for a page we find similar page in del icio u by comparing synopsis our approach give good correlation to the full cosine similarity but is hundred of time faster 
based on our field study and consultation with field expert we identified three main problem that are of key importance to online web personalization and customer relationship management detecting change in individual behaviour reporting on user action that may need special care and detecting change in visitation frequency we propose solution to these problem and experiment on real world data from an investment bank collected over year of web traffic these solution can be applied on any domain where individual tend to revisit the website and can be identified accurately 
a significant portion of web search query are name entity query the major search engine have been exploring various way to provide better user experience for name entity query such a showing search task bing search and showing direct answer yahoo kosmix in order to provide the search task or direct answer that can satisfy most popular user intent we need to capture these intent together with relationship between them in this paper we propose an approach for building a hierarchical taxonomy of the generic search intent for a class of name entity e g musician or city the proposed approach can find phrase representing generic intent from user query and organize these phrase into a tree so that phrase indicating equivalent or similar meaning are on the same node and the parent child relationship of tree node represent the relationship between search intent and their sub intent three different method are proposed for tree building which are based on directed maximum spanning tree hierarchical agglomerative clustering and pachinko allocation model our approach are purely based on search log and do not utilize any existing taxonomy such a wikipedia with the evaluation by human judge via mechanical turk it is shown that our approach can build tree of phrase that capture the relationship between important search intent 
displaying sponsored ad alongside the search result is a key monetization strategy for search engine company since user are more likely to click ad that are relevant to their query it is crucial for search engine to deliver the right ad for the query and the order in which they are displayed there are several work investigating on how to learn a ranking function to maximize the number of ad click in this paper we address a new revenue optimization problem and aim to answer the question how to construct a ranking model that can deliver high quality ad to the user a well a maximize search engine revenue we introduce two novel method from di fferent machine learning perspective and both of them take the revenue component into careful consideration the algorithm are built upon the click through log data with real ad click and impression the extensively experimental result verify the proposed algorithm that can produce more revenue than other method a well a avoid losing relevance accuracy to provide deep insight into the importance of each feature to search engine revenue we extract twelve basic feature from four category the experimental study provides a feature ranking list according to the revenue benefit of each feature 
over the last few year blog web log have gained massive popularity and have become one of the most influential web social medium in our time every blog post in the blogosphere ha a well defined timestamp which is not taken into account by search engine by conducting research regarding this feature of the blogosphere we can attempt to discover bursty term and correlation between them during a time interval we apply kleinberg s automaton on extracted title of blog post to discover bursty term we introduce a novel representation of a term s burstiness evolution called state series and we employ a euclidean based distance metric to discover potential correlation between term without taking into account their context we evaluate the result trying to match them with real life event finally we propose some idea for further evaluation technique and future research in the field 
this paper introduces a captcha based on upright orientation of line drawing rendered from d model the model are selected from a large database and image are rendered from random viewpoint affording many different drawing from a single d model the captcha present the user with a set of image and the user must choose an upright orientation for each image this task generally requires understanding of the semantic content of the image which is believed to be difficult for automatic algorithm we describe a process called covert filtering whereby the image database can be continually refreshed with drawing that are known to have a high success rate for human by inserting randomly into the captcha new image to be evaluated our analysis show that covert filtering can ensure that captchas are likely to be solvable by human while deterring attacker who wish to learn a portion of the database we performed several user study that evaluate how effectively people can solve the captcha comparing these result to an attack based on machine learning we find that human posse a substantial performance advantage over computer 
service computing is emerging a a new discipline the acceptance of web service technology stem from the fact that service enable easy integration and interoperation of enterprise level distributed system however currently software developer are forced to translate business level service requirement and encode them into program using low level abstraction such a object we propose to introduce language construct for service oriented programming that would enable raising programming abstraction from object to service 
in this paper we demonstrate a novel landmark photo search and browsing system agate which rank landmark image search result considering their relevance diversity and quality agate learns from community photo the most interested aspect and related activity of a landmark and generates adaptively a table of content toc a a summary of the attraction to facilitate the user browsing image search result are thus re ranked with the toc so a to ensure a quick overview of the attraction of the landmark a novel non parametric toc generation and set based ranking algorithm mom dpm set is proposed a the key technology of agate experimental result based on human evaluation show the effectiveness of our model and user preference for agate 
in this paper we analyze query and session intended to satisfy child s information need using a large scale query log the aim of this analysis is twofold i to identify difference between such query and session and general query and session ii to enhance the query log by including annotation of query session and action for future research on information retrieval for child we found statistically significant difference between the set of general purpose and query seeking for content intended for child we show that our finding are consistent with previous study on the physical behavior of child using web search engine 
peer to peer p p application continue to grow in popularity and have reportedly overtaken web application a the single largest contributor to internet traffic using trace collected from a large edge network we conduct an extensive analysis of p p traffic compare p p traffic with web traffic and discus the implication of increased p p traffic in addition to studying the aggregate p p traffic we also analyze and compare the two main constituent of p p traffic in our data namely bittorrent and gnutella the result presented in the paper may be used for generating synthetic workload gaining insight into the functioning of p p application and developing network management strategy for example our result suggest that new model are necessary for internet traffic a a first step we present flow level distributional model for web and p p traffic that may be used in network simulation and emulation experiment 
retrieval system for polyphonic music rely on the automatic estimation of similarity between two musical piece in the case of symbolic music existing system either consider a monophonic reduction based on melody or propose algorithm with high complexity in this paper we propose a new approach musical piece are represented a a sequence of chord which are estimated from group of note sounding at the same time a root and a mode are associated to each chord local alignment is then applied for estimating a similarity score between these sequence experiment performed on midi file collected on the internet show that the system proposed allows the retrieval of different version of the same song 
a wireless network proliferate web browser operate in an increasingly hostile network environment the http protocol ha the potential to protect web user from network attacker but real world deployment must cope with misconfigured server causing imperfect web site and user to compromise browsing session inadvertently forcehttps is a simple browser security mechanism that web site or user can use to opt in to stricter error processing improving the security of http by preventing network attack that leverage the browser s lax error processing by augmenting the browser with a database of custom url rewrite rule forcehttps allows sophisticated user to transparently retrofit security onto some insecure site that support http we provide a prototype implementation of forcehttps a a firefox browser extension 
crawl selection policy ha a direct influence on web search effectiveness because a useful page that is not selected for crawling will also be absent from search result yet there ha been little or no work on measuring this effect we introduce an evaluation framework based on relevance judgment pooled from multiple search engine measuring the maximum potential ndcg that is achievable using a particular crawl this allows u to evaluate different crawl policy and investigate important scenario like selection stability over multiple iteration we conduct two set of crawling experiment at the scale of billion and million page respectively these show that crawl selection based on pagerank indegree and trans domain indegree all allow better retrieval effectiveness than a simple breadth first crawl of the same size pagerank is the most reliable and effective method trans domain indegree can outperform pagerank but over multiple crawl iteration it is le effective and more unstable finally we experiment with combination of crawl selection method and per domain page limit which yield crawl with greater potential ndcg than pagerank 
discussion board and online forum are important platform for people to share information user post question or problem onto discussion board and rely on others to provide possible solution and such question related content sometimes even dominates the whole discussion board however to retrieve this kind of information automatically and effectively is still a non trivial task in addition the existence of other type of information e g announcement plan elaboration etc make it difficult to assume that every thread in a discussion board is about a question we consider the problem of identifying question related thread and their potential answer a classification task experimental result across multiple datasets demonstrate that our method can significantly improve the performance in both question detection and answer finding subtasks we also do a careful comparison of how different type of feature contribute to the final result and show that non content feature play a key role in improving overall performance finally we show that a ranking scheme based on our classification approach can yield much better performance than prior published method 
natural language processing technique are believed to hold a tremendous potential to supplement the purely quantitative method of text information retrieval this ha led to the emergence of a large number of nlp based ir research project over the last few year even though the empirical evidence to support this ha often been inadequate most contribution of nlp to ir mainly concentrate on document representation and compound term matching strategy researcher have noted that the simple term based representation of document content such a vector representation is usually inadequate for accurate discrimination the bag of word representation doe not invoke linguistic consideration and allow modelling of relationship between subset of word however even though a variety of content indicator such a syntactic phrase have been tried and investigated for representing document rather than single term in ir system the matching strategy over those representation still cannot go beyond traditional statistical technique that measure term co occurrence characteristic and proximity in analyzing text structure in this paper we propose a novel ir strategy sir with nlp technique involved at the syntactic level within sir document and query representation are built on the basis of a syntactic data structure of the natural language text the dependency tree in which syntactic relationship between word are identified and structured in the form of a tree in order to capture the syntactic relation between word in their hierarchical structural representation the matching strategy in sir upgrade from the traditional statistical technique by introducing a similarity measure method executing on the graph representation level a the key determiner a basic ir experiment is designed and implemented on the trec data to evaluate if this novel ir model is feasible experimental result indicate that this approach ha the potential to outperform the standard bag of word ir model especially in response to syntactical structured query 
event tracking is the task of discovering temporal pattern of popular event from text stream existing approach for event tracking have two limitation scalability and inability to rule out non relevant portion in text stream in this study we propose a novel approach to tackle these limitation to demonstrate the approach we track news event across a collection of weblogs spanning a two month time period 
learning to rank is a new statistical learning technology on creating a ranking model for sorting object the technology ha been successfully applied to web search and is becoming one of the key machinery for building search engine existing approach to learning to rank however did not consider the case in which there exists relationship between the object to be ranked despite of the fact that such situation are very common in practice for example in web search given a query certain relationship usually exist among the the retrieved document e g url hierarchy similarity etc and sometimes it is necessary to utilize the information in ranking of the document this paper address the issue and formulates it a a novel learning problem referred to a learning to rank relational object in the new learning task the ranking model is defined a a function of not only the content feature of object but also the relation between object the paper further focus on one setting of the learning problem in which the way of using relation information is predetermined it formalizes the learning task a an optimization problem in the setting the paper then proposes a new method to perform the optimization task particularly an implementation based on svm experimental result show that the proposed method outperforms the baseline method for two ranking task pseudo relevance feedback and topic distillation in web search indicating that the proposed method can indeed make effective use of relation information and content information in ranking 
human have always desired to guess the future in order to adapt their behavior and maximize chance of success in this paper we conduct exploratory analysis of future related information on the web we focus on the future related information which is grounded in time that is the information on forthcoming event whose expected occurrence date are already known we collect data by crawling search engine index and analyze collective view of future time referenced event discussed on the web 
search corpus are growing larger and larger over the last year the ir research community ha moved from the several hundred thousand document on the trec disk to the ten of million of u s government web page of gov to the one billion general interest web page in the new clueweb collection but traditional mean of acquiring relevance judgment and evaluating e g pooling document to calculate average precision do not seem to scale well to these new large collection they require substantially more cost in human assessment for the same reliability in evaluation if the additional cost go over the assessing budget error in evaluation are inevitable some alternative to pooling that support low cost and reliable evaluation have recently been proposed a number of them have already been used in trec and other evaluation forum trec million query legal chemical web relevance feedback track clef patent ir inex evaluation via implicit user feedback e g click and crowdsourcing have also recently gained attention in the community thus it is important that the methodology the analysis they support and their strength and weakness are well understood by the ir community furthermore these approach can support small research group attempting to start investigating new task on new corpus with relatively low cost even group that do not participate in trec clef or other evaluation conference can benefit from understanding how these method work how to use them and what they mean a they build test collection for task they are interested in the goal of this tutorial is to provide attendee with a comprehensive overview of technique to perform low cost in term of judgment effort evaluation a number of topic will be covered including alternative to pooling evaluation measure robust to incomplete judgment evaluating with no relevance judgment statistical inference of evaluation metric inference of relevance judgment query selection technique to test the reliability of the evaluation and reusability of the constructed collection the tutorial should be of interest to a wide range of attendee those new to the field will come away with a solid understanding of how low cost evaluation method can be applied to construct inexpensive test collection and evaluate new ir technology while those with intermediate knowledge will gain deeper insight and further understand the risk and gain of low cost evaluation attendee should have a basic knowledge of the traditional evaluation framework cranfield and metric such a average precision and ndcg along with some basic knowledge on probability theory and statistic more advanced concept will be explained during the tutorial 
computational advertising is an emerging scientific sub discipline at the intersection of large scale search and text analysis information retrieval statistical modeling machine learning classification optimization and microeconomics the central challenge of computational advertising is to find the best match between a given user in a given context and a suitable advertisement the aim of this tutorial is to present the state of the art in computational advertising in particular in it ir related aspect and to expose the participant to the current research challenge in this field the tutorial doe not assume any prior knowledge of web advertising and will begin with a comprehensive background survey going deeper our focus will be on using a textual representation of the user context to retrieve relevant ad at first approximation this process can be reduced to a conventional setup by constructing a query that describes the user context and executing the query against a large inverted index of ad we show how to augment this approach using query expansion and text classification technique tuned for the ad retrieval problem in particular we show how to use the web a a repository of query specific knowledge and use the web search result retrieved by the query a a form of a relevance feedback and query expansion we also present solution that go beyond the conventional bag of word indexing by constructing additional feature using a large external taxonomy and a lexicon of named entity obtained by analyzing the entire web a a corpus the last part of the tutorial will be devoted to a potpourri of recent research result and open problem inspired by computational advertising challenge in text summarization natural language generation named entity recognition computer human interaction and other sigir relevant area 
we propose a method to predict a user s favourite location in a city based on his flickr geotags in other city we define a similarity between the geotag distribution of two user based on a gaussian kernel convolution the geotags of the most similar user are then combined to rerank the popular location in the target city personalised for this user we show that this method can give personalised travel recommendation for user with a clear preference for a specific type of landmark 
motivated by the commonly used faceted search interface in e commerce this paper investigates interactive relevance feedback mechanism based on faceted document metadata in this mechanism the system recommends a group of document facet value pair and let user select relevant one to restrict the returned document we propose four facet value pair recommendation approach and two retrieval model that incorporate user feedback on document facet evaluated based on user feedback collected through amazon mechanical turk our experimental result show that the boolean filtering approach which is widely used in faceted search in e commerce doesn t work well for text document retrieval due to the incompleteness low recall of metadata assignment in semi structured text document instead a soft model performs more effectively the faceted feedback mechanism can also be combined with document based relevance feedback and pseudo relevance feedback to further improve the retrieval performance 
the term web is used to describe application that distinguish themselves from previous generation of software by a number of principle existing work show that web application can be successfully exploited for technology enhance learning however in depth analysis of the relationship between web technology on the one hand and teaching and learning on the other hand are still rare in this article we will analyze the technological principle of the web and describe their pedagogical implication on learning we will furthermore show that web is not only well suited for learning but also for research on learning the wealth of service that is available and their openness regarding api and data allow to assemble prototype of technology supported learning application in amazingly small amount of time these prototype can be used to evaluate research hypothesis quickly we will present two example prototype and discus the lesson we learned from building and using these prototype 
uploading tourist photo is a popular activity on photo sharing platform these photograph and their associated metadata tag geo tag and temporal information should be useful for mining information about the site visited however user supplied metadata are often noisy and efficient filtering method are needed before extracting useful knowledge we focus here on exploiting temporal information associated with tourist site that appear in flickr from automatically filtered set of geo tagged photo we deduce answer to question like how long doe it take to visit a tourist attraction or what can i visit in one day in this city our method is evaluated and validated by comparing the automatically obtained visit duration time to manual estimation 
searching for people on the web is one of the most common query type to the web search engine today however when a person name is queried the returned webpage often contain document related to several distinct namesake who have the queried name the task of disambiguating and finding the webpage related to the specific person of interest is left to the user many web people search weps approach have been developed recently that attempt to automate this disambiguation process nevertheless the disambiguation quality of these technique leaf a major room for improvement this paper present a new server side weps approach it is based on collecting co occurrence information from theweb and thus it us theweb a an external data source a skyline based classification technique is developed for classifying the collected co occurrence information in order to make clustering decision the clustering technique is specifically designed to a handle the dominance that exists in data and b to adapt to a given clustering quality measure these property allow the framework to get a major advantage in term of result quality over all the latest weps technique we are aware of including all the method covered in the recent weps competition 
numerous study have examined the ability of query performance prediction method to estimate a query s quality for system effectiveness measure such a average precision however little work ha explored the relationship between these method and user rating of query quality in this poster we report the finding from an empirical study conducted on the trec clueweb corpus where we compared and contrasted user rating of query quality against a range of query performance prediction method given a set of query it is shown that user rating of query quality correlate to both system effectiveness measure and a number of pre retrieval predictor 
web search engine typically index and retrieve at the page level in this study we investigate a dynamic pruning strategy that allows the query processor to first determine the most promising website and then proceed with the similarity computation for those page only within these site 
defining a measure of similarity between query is an interesting and difficult problem a reliable query similarity measure can be used in a variety of application such a query recommendation query expansion and advertising in this paper we exploit the information present in query log in order to develop a measure of semantic similarity between query our approach relies on the concept of the query flow graph the query flow graph aggregate query reformulations from many user node in the graph represent query and two query are connected if they are likely to appear a part of the same search goal our query similarity measure is obtained by projecting the graph or appropriate subgraphs of it on a low dimensional euclidean space our experiment show that the measure we obtain capture a notion of semantic similarity between query and it is useful for diversifying query recommendation 
access to realistic complex graph datasets is critical to research on social networking system and application simulation on graph data provide critical evaluation of new system and application ranging from community detection to spam filtering and social web search due to the high time and resource cost of gathering real graph datasets through direct measurement researcher are anonymizing and sharing a small number of valuable datasets with the community however performing experiment using shared real datasets face three key disadvantage concern that graph can be de anonymized to reveal private information increasing cost of distributing large datasets and that a small number of available social graph limit the statistical confidence in the result the use of measurement calibrated graph model is an attractive alternative to sharing datasets researcher can fit a graph model to a real social graph extract a set of model parameter and use them to generate multiple synthetic graph statistically similar to the original graph while numerous graph model have been proposed it is unclear if they can produce synthetic graph that accurately match the property of the original graph in this paper we explore the feasibility of measurement calibrated synthetic graph using six popular graph model and a variety of real social graph gathered from the facebook social network ranging from to million edge we find that two model consistently produce synthetic graph with common graph metric value similar to those of the original graph however only one produce high fidelity result in our application level benchmark while this show that graph model can produce realistic synthetic graph it also highlight the fact that current graph metric remain incomplete and some application expose graph property that do not map to existing metric 
this paper present r u in a social networking application that leverage web and ims based converged network technology to create a rich next generation service r u in allows a user to search in real time and solicit participation of like minded partner for an activity of mutual interest e g a rock concert a soccer game or a movie it is an example of a situational mashup application that exploit content and capability of a telecom operator blended with web technology to provide an enhanced value added service experience 
in this paper we study the characteristic of search query submitted from mobile device using various yahoo one search application during a month period in the second half of and report the query pattern derived from million english sample query submitted by user in u canada europe and asia we examine the query distribution and topical category the query belong to in order to find new trend we compare and contrast the search pattern between u v international query and between query from various search interface xhtml wap java widget and sm we also compare our result with previous study wherever possible either to confirm previous finding or to find interesting difference in the query distribution and pattern 
this paper proposes a system called rerankeverything which enables user to rerank search result in any search service such a a web search engine an e commerce site a hotel reservation site and so on in conventional search service interaction between user and service are quite limited and complicated in addition search function and interaction to refine search result differ depending on the service by using rerankeverything user can interactively explore search result in accordance with their interest by reranking search result from various viewpoint 
we developed the h bitmap index for efficient information retrieval the h bitmap index is a hierarchical document term matrix the original document term matrix is called the leaf matrix and an upper matrix is the summary of it lower matrix our experiment result show the h bitmap index performs better than the inverted index with a minor space overhead 
in this paper we show how a user profile can be enhanced when a more detailed description of the product is included two main assumption have been considered the first implies that the set of feature used to describe an item can be organized into a well defined set of component or category and the second is that the user s rating for a given item is obtained by combining user opinion of the relevance of each component 
relevance feedback which traditionally us the term in the relevant document to enrich the user s initial query is an effective method for improving retrieval performance the traditional relevance feedback algorithm lead to overfitting because of the limited amount of training data and large term space this paper introduces an online bayesian logistic regression algorithm to incorporate relevance feedback information the new approach address the overfitting problem by projecting the original feature space onto a more compact set which retains the necessary information the new set of feature consist of the original retrieval score the distance to the relevant document and the distance to non relevant document to reduce the human evaluation effort in ascertaining relevance we introduce a new active learning algorithm based on variance reduction to actively select document for user evaluation the new active learning algorithm aim to select feedback document to reduce the model variance the variance reduction approach lead to capturing relevance diversity and uncertainty of the unlabeled document in a principled manner these are the critical factor of active learning indicated in previous literature experiment with several trec datasets demonstrate the effectiveness of the proposed approach 
this paper study quality of human label used to train search engine ranker our specific focus is performance improvement obtained by using overlapping relevance label which is by collecting multiple human judgment for each training sample the paper explores whether when and for which sample one should obtain overlapping training label a well a how many label per sample are needed the proposed selective labeling scheme collect additional label only for a subset of training sample specifically for those that are labeled relevant by a judge our experiment show that this labeling scheme improves the ndcg of two web search ranker on several real world test set with a low labeling overhead of around label per sample this labeling scheme also outperforms several method of using overlapping label such a simple k overlap majority vote the highest label etc finally the paper present a study of how many overlapping label are needed to get the best improvement in retrieval accuracy 
easy reuse and integration of declaratively described information in a distributed setting is one of the main motivation for building the semantic web despite of this claim reuse and recombination of rdf data today is mostly done using data replication and procedural code a simple declarative mechanism for reusing and combining rdf data would help user to generate content for the semantic web having such a mechanism the semantic web could better benefit from user generated content a it is broadly present in the so called web but also from better linkage of existing content we propose networked graph which allow user to define rdf graph both by extensionally listing content but also by using view on other graph these view can be used to include part of other graph to transform data before including it and to denote rule the relationship between graph are described declaratively using sparql query and an extension of the sparql semantics networked graph are easily exchangeable between and interpretable on different computer using existing protocol networked graph can be evaluated in a distributed setting 
a soft error redirection is a url redirection to a page that return the http status code ok but ha actually no relevant content to the client request since such redirections degrade the performance of web search engine in many way it is highly desirable to remove a many of them a possible we propose a novel approach to detect soft error redirections by analyzing redirection log collected during crawling operation experimental result on huge crawl data show that our measure can classify soft error redirections effectively 
rank correlation statistic are useful for determining whether a there is a correspondence between two measurement particularly when the measure themselves are of le interest than their relative ordering kendall s in particular ha found use in information retrieval a a meta evaluation measure it ha been used to compare evaluation measure evaluate system ranking and evaluate predicted performance in the meta evaluation domain however correlation between system confound relationship between measurement practically guaranteeing a positive and significant estimate of regardless of any actual correlation between the measurement we introduce an alternative measure of distance between ranking that corrects this by explicitly accounting for correlation between system over a sample of topic and moreover ha a probabilistic interpretation for use in a test of statistical significance we validate our measure with theory simulated data and experiment 
there are many on line setting in which user publicly express opinion a number of these offer mechanism for other user to evaluate these opinion a canonical example is amazon com where review come with annotation like of people found the following review helpful opinion evaluation appears in many off line setting a well including market research and political campaign reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself rather than asking what did y think of x we are asking what did z think of y s opinion of x here we develop a framework for analyzing and modeling opinion evaluation using a large scale collection of amazon book review a a dataset we find that the perceived helpfulness of a review depends not just on it content but also but also in subtle way on how the expressed evaluation relates to other evaluation of the same product a part of our approach we develop novel method that take advantage of the phenomenon of review plagiarism to control for the effect of text in opinion evaluation and we provide a simple and natural mathematical model consistent with our finding our analysis also allows u to distinguish among the prediction of competing theory from sociology and social psychology and to discover unexpected difference in the collective opinion evaluation behavior of user population from different country 
online community have become popular for publishing and searching content a well a for finding and connecting to other user user generated content includes for example personal blog bookmark and digital photo these item can be annotated and rated by different user and these social tag and derived user specific score can be leveraged for searching relevant content and discovering subjectively interesting item moreover the relationship among user can also be taken into consideration for ranking search result the intuition being that you trust the recommendation of your close friend more than those of your casual acquaintance query for tag or keyword combination that compute and rank the top k result thus face a large variety of option that complicate the query processing and pose efficiency challenge this paper address these issue by developing an incremental top k algorithm with two dimensional expansion social expansion considers the strength of relation among user and semantic expansion considers the relatedness of different tag it present a new algorithm based on principle of threshold algorithm by folding friend and related tag into the search space in an incremental on demand manner the excellent performance of the method is demonstrated by an experimental evaluation on three real world datasets crawled from deli cio u flickr and librarything 
the vector space model vsm is a popular and widely applied model in information retrieval ir vsm creates vector space whose dimensionality is usually high e g ten of thousand of term this may cause various problem such a susceptibility to noise and difficulty in capturing the underlying semantic structure which are commonly recognized a different aspect of the curse of dimensionality in this paper we investigate a novel aspect of the dimensionality curse which is referred to a hubness and manifested by the tendency of some document called hub to be included in unexpectedly many search result list hubness may impact vsm considerably since hub can become obstinate result irrelevant to a large number of query thus harming the performance of an ir system and the experience of it user we analyze the origin of hubness showing it is primarily a consequence of high intrinsic dimensionality of data and not a result of other factor such a sparsity and skewness of the distribution of term frequency we describe the mechanism through which hubness emerges by exploring the behavior of similarity measure in high dimensional vector space our consideration begin with the classical vsm tf idf term weighting and cosine similarity but the conclusion generalize to more advanced variation such a okapi bm moreover we explain why hubness may not be easily mitigated by dimensionality reduction and propose a similarity adjustment scheme that take into account the existence of hub experimental result over real data indicate that significant improvement can be obtained through consideration of hubness 
modeling term dependence ha been shown to have a significant positive impact on retrieval current model however use sequential term dependency leading to an increased query latency especially for long query in this paper we examine two query segmentation model that reduce the number of dependency we find that two stage segmentation based on both query syntactic structure and external information source such a query log attains retrieval performance comparable to the sequential dependence model while achieving a reduction in query latency 
this paper proposes a learning approach for the merging process in multilingual information retrieval mlir to conduct the learning approach we also present a large number of feature that may influence the mlir merging process these feature are mainly extracted from three level query document and translation after the feature extraction we then use the frank ranking algorithm to construct a merge model to our knowledge this practice is the first attempt to use a learning based ranking algorithm to construct a merge model for mlir merging in our experiment three test collection for the task of crosslingual information retrieval clir in ntcir and are employed to ass the performance of our proposed method moreover several merging method are also carried out for a comparison including traditional merging method the step merging strategy and the merging method based on logistic regression the experimental result show that our method can significantly improve merging quality on two different type of datasets in addition to the effectiveness through the merge model generated by frank our method can further identify key factor that influence the merging process this information might provide u more insight and understanding into mlir merging 
rogue access point rap pose serious security threat to local network an analytic model of prior probability distribution of segmental tcp jitter stj is deduced from the mechanism of ieee mac distributed coordinated function dcf and used to differentiate the type of wire and wlan connection which is the crucial step for rap detecting stj a the detecting metric can reflect more the characteristic of mac than ack pair since it can eliminate the delay caused by packet transmission the experiment on an operated network show the average detection ratio of the algorithm with stj is more than and the average detection time is le than s with improvement of and over the detecting approach of ack pair respectively farther more no wlan training trace is needed in the detecting algorithm 
we address the task of blog feed distillation to find blog that are principally devoted to a given topic the task may be viewed a an association finding task between topic and blogger under this view it resembles the expert finding task for which a range of model have been proposed we adopt two language modeling based approach to expert finding and determine their effectiveness a feed distillation strategy the two model capture the idea that a human will often search for key blog by spotting highly relevant post the posting model or by taking global aspect of the blog into account the blogger model result show the blogger model outperforms the posting model and delivers state of the art performance out of the box 
a number of work have shown that the aggregation of several information retrieval ir system work better than each system working individually nevertheless early investigation in the context of clef robust wsd task in which semantics is involved showed that aggregation strategy achieve only slight improvement this paper proposes a re ranking approach which relies on inter document similarity the novelty of our idea is twofold the output of a semantic based ir system is exploited to re weigh document and a new strategy based on semantic vector is used to compute inter document similarity 
the goal of interactive cross lingual information retrieval system is to support user in formulating effective query and selecting the document which satisfy their information need regardless of the language of these document this dissertation aim at harnessing user system interaction extracting the added value and integrating it back into the system to improve cross lingual information retrieval for successive user to achieve this user input at different interaction point will be evaluated this will among others include interaction during user assisted query translation implicit and explicit relevance feedback and social tag to leverage this input explorative study need to be conducted to determine beneficial user input and the method of extracting it 
information retrieval system face a number of challenge originating mainly from the semantic gap problem implicit feedback technique have been employed in the past to address many of these issue although this wa a step towards the right direction a need to personalise and tailor the search experience to the user specific need ha become evident in this study we examine way of personalising affective model trained on facial expression data using personalised data we adapt these model to individual user and compare their performance to a general model the main goal is to determine whether the behavioural difference of user have an impact on the model ability to determine topical relevance and if by personalising them we can improve their accuracy for modelling relevance we extract a set of feature from the facial expression data and classify them using support vector machine our initial evaluation indicates that accounting for individual difference and applying personalisation introduces in most case a noticeable improvement in the model performance 
the increasing availability of gps enabled device is changing the way people interact with the web and brings u a large amount of gps trajectory representing people s location history in this paper based on multiple user gps trajectory we aim to mine interesting location and classical travel sequence in a given geospatial region here interesting location mean the culturally important place such a tiananmen square in beijing and frequented public area like shopping mall and restaurant etc such information can help user understand surrounding location and would enable travel recommendation in this work we first model multiple individual location history with a tree based hierarchical graph tbhg second based on the tbhg we propose a hit hypertext induced topic search based inference model which regard an individual s access on a location a a directed link from the user to that location this model infers the interest of a location by taking into account the following three factor the interest of a location depends on not only the number of user visiting this location but also these user travel experience user travel experience and location interest have a mutual reinforcement relationship the interest of a location and the travel experience of a user are relative value and are region related third we mine the classical travel sequence among location considering the interest of these location and user travel experience we evaluated our system using a large gps dataset collected by user over a period of one year in the real world a a result our hit based inference model outperformed baseline approach like rank by count and rank by frequency meanwhile when considering the user travel experience and location interest we achieved a better performance beyond baseline such a rank by count and rank by interest etc 
we present a new captcha which is based on identifying an image s upright orientation this task requires analysis of the often complex content of an image a task which human usually perform well and machine generally do not given a large repository of image such a those from a web search result we use a suite of automated orientation detector to prune those image that can be automatically set upright easily we then apply a social feedback mechanism to verify that the remaining image have a human recognizable upright orientation the main advantage of our captcha technique over the traditional text recognition technique are that it is language independent doe not require text entry e g for a mobile device and employ another domain for captcha generation beyond character obfuscation this captcha lends itself to rapid implementation and ha an almost limitless supply of image we conducted extensive experiment to measure the viability of this technique 
a online collaborative question answering cqa servicessuch a yahoo answer and baidu know are attracting user question and answer at an explosive rate the truly urgent and important question are increasingly getting lost in the crowd that is question that require immediate response are pushed out of the way by the trivial but more recently arriving question unlike other question in collaborative question answering cqa for which user might be willing to wait until good answer appear urgent question are likely to be of interest to the asker only if answered in the next few minute or hour for such question late response are either not useful or are simply not applicable unfortunately current collaborative question answering system do not distinguish urgent question from the rest and could thus be ineffective for urgent information need we explore textand datamining method for automatically identifying urgent question in the cqa setting our result indicate that modeling the question context i e the particular forum category where the question wa posted can increase classification accuracy compared to the text of the question alone 
this paper proposes several vectorial operator for processing xml twig query which are easy to be performed and inherently efficient for both ancestor descendant a d and parent child p c relationship we develop optimization on the vectorial operator to improve the efficiency of answering twig query in holistic we propose an algorithm to answer gtp query based on our vectorial operator 
in this paper we propose a reranking model to improve the aspect level performance in the biomedical domain this model iteratively computes the maximum hidden aspect for every retrieved passage and then reranks these passage from aspect subset the experimental result show the improvement of the aspect level performance up to for genomics topic and for genomics topic 
we analyze the social network emerging from the user comment activity on the website slashdot the network present common feature of traditional social network such a a giant component small average path length and high clustering but differs from them showing moderate reciprocity and neutral assortativity by degree using kolmogorov smirnov statistical test we show that the degree distribution are better explained by log normal instead of power law distribution we also study the structure of discussion thread using an intuitive radial tree representation thread show strong heterogeneity and self similarity throughout the different nesting level of a conversation we use these result to propose a simple measure to evaluate the degree of controversy provoked by a post 
current web search engine return result page containing mostly text summary even though the matched web page may contain informative picture a text excerpt i e snippet is generated by selecting keywords around the matched query term for each returned page to provide context for user s relevance judgment however in many scenario we found that the picture in web page if selected properly could be added into search result page and provide richer contextual description because a picture is worth a thousand word such new summary is named a image excerpt by well designed user study we demonstrate image excerpt can help user make much quicker relevance judgment of search result for a wide range of query type to implement this idea we propose a practicable approach to automatically generate image excerpt in the result page by considering the dominance of each picture in each web page and the relevance of the picture to the query we also outline an efficient way to incorporate image excerpt in web search engine web search engine can adopt our approach by slightly modifying their index and inserting a few low cost operation in their workflow our experiment on a large web dataset indicate the performance of the proposed approach is very promising 
this paper present a method for increasing the quality of automatically extracted instance attribute by exploiting weakly supervised and unsupervised instance relatedness data this data consists of a class label for instance and b distributional similarity score the method organizes the text derived data into a graph and automatically propagates attribute among related instance through random walk over the graph experiment on various graph topology illustrate the advantage of the method over both the original attribute list and a per class attribute extractor both in term of the number of attribute extracted per instance and the accuracy of the top ranked attribute 
recently increasing attention ha been focused on directly optimizing ranking measure and inducing sparsity in learning model however few attempt have been made to relate them together in approaching the problem of learning to rank in this paper we consider the sparse algorithm to directly optimize the normalized discounted cumulative gain ndcg which is a widely used ranking measure we begin by establishing a reduction framework under which we reduce ranking a measured by ndcg to the importance weighted pairwise classification furthermore we provide a sound theoretical guarantee for this reduction bounding the realized ndcg regret in term of a properly weighted pairwise classification regret which implies that good performance can be robustly transferred from pairwise classification to ranking based on the converted pairwise loss function it is conceivable to take into account sparsity in ranking model and to come up with a gradient possessing certain performance guarantee for the sake of achieving sparsity a novel algorithm named rsrank ha also been devised which performs l regularization using truncated gradient descent finally experimental result on benchmark collection confirm the significant advantage of rsrank in comparison with several baseline method 
we propose a novel cost efficient approach to threshold selection for binary web page classification problem with imbalanced class distribution in many binary classification task the distribution of class is highly skewed in such problem using uniform random sampling in constructing sample set for threshold setting requires large sample size in order to include a statistically sufficient number of example of the minority class on the other hand manually labeling example is expensive and budgetary consideration require that the size of sample set be limited these conflicting requirement make threshold selection a challenging problem our method of sample set construction is a novel approach based on stratified sampling in which manually labeled example are expanded to reflect the true class distribution of the web page population our experimental result show that using false positive rate a the criterion for threshold setting result in lower variance threshold estimate than using other widely used accuracy measure such a f and precision 
social tagging service allow user to annotate various online resource with freely chosen keywords tag they not only facilitate the user in finding and organizing online resource but also provide meaningful collaborative semantic data which can potentially be exploited by recommender system traditional study on recommender system focused on user rating data while recently social tagging data is becoming more and more prevalent how to perform resource recommendation based on tagging data is an emerging research topic in this paper we consider the problem of document e g web page research paper recommendation using purely tagging data that is we only have data containing user tag document and the relationship among them we propose a novel graph based representation learning algorithm for this purpose the user tag and document are represented in the same semantic space in which two related object are close to each other for a given user we recommend those document that are sufficiently close to him her experimental result on two data set crawled from del icio u and citeulike show that our algorithm can generate promising recommendation and outperforms traditional recommendation algorithm 
in this paper we study how the performance and usability of web dialogue system could be enhanced by using an appropriate representation of the different type of knowledge involved in communication general dialogue mechanism specific domain restricted linguistic and conceptual knowledge and information on how well the communication process is doing we describe the experiment carried out to analyze how to improve this knowledge representation in the web dialogue system we developed 
online review in which user publish detailed commentary about their experience and opinion with product service or event are extremely valuable to user who rely on them to make informed decision however review vary greatly in quality and are constantly increasing in number therefore automatic assessment of review helpfulness is of growing importance previous work ha addressed the problem by treating a review a a stand alone document extracting feature from the review text and learning a function based on these feature for predicting the review quality in this work we exploit contextual information about author identity and social network for improving review quality prediction we propose a generic framework for incorporating social context information by adding regularization constraint to the text based predictor our approach can effectively use the social context information available for large quantity of unlabeled review it also ha the advantage that the resulting predictor is usable even when social context is unavailable we validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse 
we developed a new web authentication protocol with passwordbased mutual authentication which prevents various kind of phishing attack this protocol provides a protection of user s password against any phishers even if a dictionary attack is employed and prevents phishers from imitating a false sense of successful authentication to user the protocol is designed considering interoperability with many recent web application which requires many feature which current http authentication doe not provide the protocol is proposed a an internet draft submitted to ietf and implemented in both server side a an apache extension and client side a a mozilla based browser and an ie based one category and subject descriptor k management of computing and information system security and protection authentication 
geography and social relationship are inextricably intertwined the people we interact with on a daily basis almost always live near u a people spend more time online data regarding these two dimension geography and social relationship are becoming increasingly precise allowing u to build reliable model to describe their interaction these model have important implication in the design of location based service security intrusion detection and social medium supporting local community using user supplied address data and the network of association between member of the facebook social network we can directly observe and measure the relationship between geography and friendship using these measurement we introduce an algorithm that predicts the location of an individual from a sparse set of located user with performance that exceeds ip based geolocation this algorithm is efficient and scalable and could be run on a network containing hundred of million of user 
in this paper we propose a semi supervised learning approach for classifying program bot generated web search traffic from that of genuine human user the work is motivated by the challenge that the enormous amount of search data pose to traditional approach that rely on fully annotated training sample we propose a semi supervised framework that address the problem in multiple front first we use the captcha technique and simple heuristic to extract from the data log a large set of training sample with initial label though directly using these training data is problematic because the data thus sampled are biased to tackle this problem we further develop a semi supervised learning algorithm to take advantage of the unlabeled data to improve the classification performance these two proposed algorithm can be seamlessly combined and very cost efficient to scale the training process in our experiment the proposed approach showed significant i e improvement compared to the traditional supervised approach 
most of the search engine optimization technique attempt to predict user interest by learning from the past information collected from different source but a user s current interest often depends on many factor which are not captured in the past information in this paper we attempt to identify user s current interest in real time from the information provided by the user in the current query session by identifying user s interest in real time the engine could adapt differently to different user in real time experimental verification indicates that our approach is encouraging for short query 
ad auction in sponsored search support broad match that allows an advertiser to target a large number of query while bidding only on a limited number while giving more expressiveness to advertiser this feature make it challenging to optimize bid to maximize their return choosing to bid on a query a a broad match because it provides high profit result in one bidding for related query which may yield low or even negative profit we abstract and study the complexity of the em bid optimization problem which is to determine an advertiser s bid on a subset of keywords possibly using broad match so that her profit is maximized in the query language model when the advertiser is allowed to bid on all query a broad match we present a linear programming lp based polynomial time algorithm that get the optimal profit in the model in which an advertiser can only bid on keywords ie a subset of keywords a an exact or broad match we show that this problem is not approximable within any reasonable approximation factor unless p np to deal with this hardness result we present a constant factor approximation when the optimal profit significantly exceeds the cost this algorithm is based on rounding a natural lp formulation of the problem finally we study a budgeted variant of the problem and show that in the query language model one can find two budget constrained ad campaign in polynomial time that implement the optimal bidding strategy our result are the first to address bid optimization under the broad match feature which is common in ad auction 
a large body of work ha been devoted to identifying community structure in network a community is often though of a a set of node that ha more connection between it member than to the remainder of the network in this paper we characterize a a function of size the statistical and structural property of such set of node we define the network community profile plot which characterizes the best possible community according to the conductance measure over a wide range of size scale and we study over large sparse real world network taken from a wide range of application domain our result suggest a significantly more refined picture of community structure in large real world network than ha been appreciated previously our most striking finding is that in nearly every network dataset we examined we observe tight but almost trivial community at very small scale and at larger size scale the best possible community gradually blend in with the rest of the network and thus become le community like this behavior is not explained even at a qualitative level by any of the commonly used network generation model moreover this behavior is exactly the opposite of what one would expect based on experience with and intuition from expander graph from graph that are well embeddable in a low dimensional structure and from small social network that have served a testbeds of community detection algorithm we have found however that a generative model in which new edge are added via an iterative forest fire burning process is able to produce graph exhibiting a network community structure similar to our observation 
popularity of content in social medium is unequally distributed with some item receiving a disproportionate share of attention from user predicting which newly submitted item will become popular is critically important for both company that host social medium site and their user accurate and timely prediction would enable the company to maximize revenue through differential pricing for access to content or ad placement prediction would also give consumer an important tool for filtering the ever growing amount of content predicting popularity of content in social medium however is challenging due to the complex interaction among content quality how the social medium site chooses to highlight content and influence among user while these factor make it difficult to predict popularity emph a priori we show that stochastic model of user behavior on these site allows predicting popularity based on early user reaction to new content by incorporating aspect of the web site design such model improve on prediction based on simply extrapolating from the early vote we validate this claim on the social news portal digg using a previously developed model of social voting based on the digg user interface 
with the increasing popularity of location based service such a tour guide and location based social network we now have accumulated many location data on the web in this paper we show that by using the location data based on gps and user comment at various location we can discover interesting location and possible activity that can be performed there for recommendation our research is highlighted in the following location related query in our daily life if we want to do something such a sightseeing or food hunting in a large city such a beijing where should we go if we have already visited some place such a the bird s nest building in beijing s olympic park what else can we do there by using our system for the first question we can recommend her to visit a list of interesting location such a tiananmen square bird s nest etc for the second question if the user visit bird s nest we can recommend her to not only do sightseeing but also to experience it outdoor exercise facility or try some nice food nearby to achieve this goal we first model the user location and activity history that we take a input we then mine knowledge such a the location feature and activity activity correlation from the geographical database and the web to gather additional input finally we apply a collective matrix factorization method to mine interesting location and activity and use them to recommend to the user where they can visit if they want to perform some specific activity and what they can do if they visit some specific place we empirically evaluated our system using a large gps dataset collected by user over a period of year in the real world we extensively evaluated our system and showed that our system can outperform several state of the art baseline 
over the last few year both availability and accessibility of current news story on the web have dramatically improved in particular user can now access news from a variety of source hosted on the web from newswire presence such a the new york time to integrated news search within web search engine however of central interest is the emerging impact that user generated content ugc is having on this online news landscape indeed the emergence of web ha turned a static news consumer base into a dynamic news machine where news story are summarised and commented upon in summary value is being added to each news story in term of additional content importantly however while there ha been movement in commercial circle to exploit this extra value to enrich online news there ha been little research from the academic community on how can be achieved indeed the main purpose of this thesis is to research practical technique for the integration of ugc to improve the news search component of the most ubiquitous of web tool i e the web search engine 
this paper proposes a novel method to translate tag attached to multimedia content for cross language retrieval the main issue in this problem is the sense disambiguation of tag given with few textual context in order to solve this problem the proposed method represents both tag and it translation candidate a network of co occurring tag since a network allows richer expression of context than other expression such a co occurrence vector the method translates a tag by selecting the optimal one from possible candidate based on a network similarity even when neither the textual context nor sophisticated language resource are available the experiment on the mir flickr test set show that the proposed method achieves accuracy in translating tag from english into german which is significantly higher than the baseline method of a frequency based translation and a co occurrence based translation 
falconer is a semantic web search engine enhanced sioc semantically interlinked online community application which is designed to demonstrate the ability of accelerating the creation and reuse process of semantic web data with easy to use user interface in this process semantic web search engine feed existing semantic data into the sioc framework where new semantic data are composed by the community and indexed again by those search engine compared to existing social semantic web application falconer inherently conforms to sioc specification it provides semantic search engine based user registration suggestion friend auto discovery and semantic annotation for forum post content another distinctive feature is that it enables user to subscribe any resource having a uri a the topic they are interested in the relationship among user topic and post are further visualized for analyzing the topic trend in the community a all semantic data are formatted in rdf and rdfa they can be queried with sparql query language 
we apply the dynamic markov compression model to detect spam edits in the wikipedia the method appears to outperform previous effort based on compression model providing performance comparable to method based on manually constructed rule 
the world wide web ha become the world s largest networked information resource but reference to geographical location remain unstructured and typically implicit in nature this lack of explicit spatial knowledge within the web make it difficult to service user need for location specific information at present spatial knowledge is hidden in many small information fragment such a address on web page annotated photo with gps coordinate geographic mapping application and geotags in user generated content several emerging format that primarily or secondarily include location metadata like georss kml and microformats aim to improve this state of affair however the question remains how to extract index mine find view mashup and exploit web content using it location semantics this workshop brings together researcher from academia and industry lab to discus and present the latest result and trend in all facet of the relationship between location concept and web information 
document summarization play an increasingly important role with the exponential growth of document on the web many supervised and unsupervised approach have been proposed to generate summary from document however these approach seldom simultaneously consider summary diversity coverage and balance issue which to a large extent determine the quality of summary in this paper we consider extract based summarization emphasizing the following three requirement diversity in summarization which seek to reduce redundancy among sentence in the summary sufficient coverage which focus on avoiding the loss of the document s main information when generating the summary and balance which demand that different aspect of the document need to have about the same relative importance in the summary we formulate the extract based summarization problem a learning a mapping from a set of sentence of a given document to a subset of the sentence that satisfies the above three requirement the mapping is learned by incorporating several constraint in a structure learning framework and we explore the graph structure of the output variable and employ structural svm for solving the resulted optimization problem experiment on the duc data set demonstrate significant performance improvement in term of f and rouge metric 
microblogging a introduced by twitter is becoming a source of tracking real time news although identifying the highest quality or most useful post or tweet from twitter for breaking news is still an open problem major web search engine seem convinced of the value of such post and have already started allocating part of their search result page to them in this paper we study a different aspect of the problem for a search engine instead of the value of the post we study the value of the shortened url referenced in these post our result indicate that unlike frequently bookmarked url which are generally of high quality frequently tweeted url tend to fall in two opposite category they are either high in quality or they are spam identifying the quality category of a url is not trivial but the combination of characteristic can reveal some trend 
an infosuasive web application is mainly intended to be at the same time informative and persuasive i e it aim at supporting knowledge need and it ha also the declared or not declared goal of influencing user s opinion attitude and behavior most web application in fact are infosuasive except those whose aim is mainly operational in this paper we investigate the complex set of element that informs the very early design of infosuasive web application we propose a conceptual framework aimed at supporting the actor involved in this process to integrate their different viewpoint to organize the variety of issue that need to be analyzed to find a direction in the numerous design option and to represent the result of this activity in an effective way our approach is value driven since it is centered around the concept of communication value regarded a a vehicle to fulfill communication goal on specific communication target we place the analysis of these aspect in the wider context of web requirement analysis highlighting their relationship with business value analysis and user need analysis we pinpoint how value and communication goal impact on various design dimension of infosuasive web application content information architecture interaction operation and lay out our approach is multidisciplinary and wa inspired to goal based and value based requirement engineering often used in web engineering to brand design often used in marketing and to value centered design framework a proposed by the hci community a case study exemplifies our methodological proposal discussing a large project in which we are currently involved 
in sponsored search a number of advertising slot is available on a search result page and have to be allocated among a set of advertiser competing to display an ad on the page this give rise to a bipartite matching market that is typically cleared by the way of an automated auction several auction mechanism have been proposed with variant of the generalized second price gsp being widely used in practice there is a rich body of work on bipartite matching market that build upon the stable marriage model of gale and shapley and the assignment model of shapley and shubik this line of research offer deep insight into the structure of stable outcome in such market and their incentive property in this paper we model advertising auction in term of an assignment model with linear utility extended with bidder and item specific maximum and minimum price auction mechanism like the commonly used gsp or the well known vickrey clarke grove vcg can be interpreted a simply computing a bidderoptimal stable matchingin this model for a suitably defined set of bidder preference s but our model includes much richer bidder and preference we prove that in our model the existence of a stable matching is guaranteed and under a non degeneracy assumption a bidder optimal stable matching exists a well we give a fast algorithm to find such matching in polynomial time and use it to design truthful mechanism that generalizes gsp is truthful for profit maximizing bidder correctly i mplements feature like bidder specific minimum price and position specific bid and work for rich mixtur e of bidder and preference our main technical contribution are the existence of bidder optimal matchings and group strategyproofness of the resulting mechanism and are proved by induction on the progress of the matching algorithm 
we present theoretical bound and empirical robustness of score regularization given change in the similarity measure 
we consider the problem of ranking refinement i e to improve the accuracy of an existing ranking function with a small set of labeled instance we are particularly interested in learning a better ranking function using two complementary source of information ranking information given by the existing ranking function i e a base ranker and that obtained from user feedback this problem is very important in information retrieval where the feedback is gradually collected the key challenge in combining the two source of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from user feedback tends to be noisy we present a novel boosting framework for ranking refinement that can effectively leverage the us of the two source of information our empirical study show that the proposed algorithm is effective for ranking refinement and furthermore significantly outperforms the baseline algorithm that incorporate the output from the base ranker a an additional feature 
in this paper we devise a novel robust music identification algorithm utilizing compressed domain audio zernike moment adapted from image processing technique a the pivotal feature audio fingerprint derived from this feature exhibit strong robustness against various audio signal distortion including the challenging pitch shifting and time scale modification experiment show that in our test dataset composed of popular song a s music query example which might have been severely corrupted is still sufficient to identify it original near duplicate copy with more than top five precision rate 
previous work analyzing social network ha mainly focused on binary friendship relation however in online social network the low cost of link formation can lead to network with heterogeneous relationship strength e g acquaintance and best friend mixed together in this case the binary friendship indicator provides only a coarse representation of relationship information in this work we develop an unsupervised model to estimate relationship strength from interaction activity e g communication tagging and user similarity more specifically we formulate a link based latent variable model along with a coordinate ascent optimization procedure for the inference we evaluate our approach on real world data from facebook and linkedin showing that the estimated link weight result in higher autocorrelation and lead to improved classification accuracy 
this paper report the estimated number of spam blog in order to ass their current state in the blogosphere to extract spam blog i developed a traversal method among co citation cluster of blog from a spam seed spam seed were collected in term of high out degree and spam keyword according to the experiment a mixed seed set composed of high out degree and spam keyword seed is more effective than individual seed set in term of f measure in conclusion mixed seed from different method are effective in improving the f measure result of spam extraction with co citation cluster 
motivated by contextual advertising system and other web application involving efficiency accuracy tradeoff we study similarity caching here a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item we study two objective that dictate the efficiency accuracy tradeoff and provide our caching policy for these objective by conducting extensive experiment on real data we show similarity caching can significantly improve the efficiency of contextual advertising system with minimal impact on accuracy inspired by the above we propose a simple generative model that embodies two fundamental characteristic of page request arriving to advertising system namely long range dependence and similarity we provide theoretical bound on the gain of similarity caching in this model and demonstrate these gain empirically by fitting the actual data to the model 
in this paper we present a description based mashup framework for lightweight integration of web content our implementation show that we can integrate not only the web service but also the web application easily even the web content dynamically generated by client side script 
link analysis method have been used successfully for knowledge discovery from the link structure of mutually linking entity existing link analysis method have been inherently designed based on the fact that the entire link structure of the target graph is observable such a public web document however link information in graph in the real world such a human relationship or economic activity is rarely open to public if link analysis can be performed using graph with private link in a privacy preserving way it enables u to rank entity connected with private tie such a people organization or business transaction in this paper we present a secure link analysis for graph with private link by mean of cryptographic protocol our solution are designed a privacy preserving expansion of well known link analysis method pagerank and hit the outcome of our protocol are completely equivalent to those of pagerank and hit furthermore our protocol theoretically guarantee that the private link information possessed by each node is not revealed to other node we demonstrate the efficiency of our solution by experimental study comparing with existing solution such a secure function evaluation decentralized spectral analysis and privacy preserving link analysis 
a bipartite query url graph where an edge indicates that a document wa clicked for a query is a useful construct for finding group of related query and url here we use this behavior graph for classification we choose a click graph sampled from two week of image search activity and the task of adult filtering identifying content in the graph that is inappropriate for minor we show how to perform classification using random walk on this graph and two method for estimating classifier parameter 
the performance of server side application is becoming increasingly important a more application exploit the web application model extensive work ha been done to improve the performance of individual software component such a web server and programming language runtimes this paper describes a novel approach to boost web application performance by improving inter process communication between a programming language runtime and web server runtime the approach reduces redundant processing for memory copying and the context switch overhead between user space and kernel space by exploiting the zero copy data transfer methodology such a the sendfile system call in order to transparently utilize this optimization feature with existing web application we propose enhancement of the php runtime fastcgi protocol and web server our proposed approach achieves a performance improvement with micro benchmark and a performance improvement for a standard web benchmark specweb 
in this poster a tool named bucefalo is presented this tool is specially designed to improve the information retrieval task in web based personal health record phr this tool implement semantic and multilingual query expansion technique and information filtering algorithm in order to help user find the most valuable information about a specific clinical case the filtering model is based on fuzzy prototype based filtering data quality measure user profile and healthcare ontology the first experimental result illustrate the feasibility of this tool 
micro blogging is a new form of social communication that encourages user to share information about anything they are seeing or doing the motivation facilitated by the ability to post brief text message through a variety of device twitter the most popular micro blogging tool is exhibiting rapid growth up to of online american are using twitter by december compared to in may due to it nature micro blogosphere ha unique feature i it is a source of extremely up to date information about what is happening in the world ii it capture the wisdom of million of people and cover a broad range of domain these feature make micro blogosphere more than a popular medium of social communication we believe that it ha additionally become a valuable source of extremely up to date news on virtually any subject of user interest making use of micro blogosphere in this new role we meet the following challenge a since any given subject is generally mentioned in the micro blogging stream on the continuous basis a method is needed for locating period of news on this subject b additionally even for such period stream filtering is required for removing noise and for extracting message that best describe the news to address these challenge we make and exploit the following observation a for an arbitrary subject event that catch user interest gain distinguishably more attention than the average mentioning of the subject resulting in message activity burst for it b most of the message in an activity burst describe common event in close variation either rephrased or retweeted between the user we demonstrate tweetsieve a system that allows obtaining news on any given subject by sifting the twitter stream our work is related to frequecy based analysis applied to blog but higher latency and lower coverage in blog make the analysis le effective than in case of micro blog in tweetsieve demo the user is able to express the subject of her interest by an arbitrary search string the system show the period of event occuring for the subject and output tweet that best describe each of the event figure show a screenshot of the system for semantic search a a sample subject the underlying process consists of two step identifying activity burst counting the message matching the search string in the stream over time the frequency curve is constructed activity burst in the curve are identified by taking the period of frequency exceeding the standard deviation from the average selecting message that best describe news event for the set of all message matching the search string in an activity burst we apply the message granular variation of our keyphrase extraction algorithm that is specifically suited to efficiently filtering noisy data the algorithm cluster message with respect to their similarity to each other and chooses central message from the most dense cluster a the similarity measure we use jaccard coefficient for the bag of word representation of message the demonstration illustrates the potential of our approach in bringing news acquisition to a new level of promptness and coverage range 
wikipedia provides an information quality assessment model with criterion for human peer reviewer to identify featured article for this classification task is an article featured or not we present a machine learning approach that exploit an article s character trigram distribution our approach differs from existing research in that it aim to writing style rather than evaluating meta feature like the edit history the approach is robust straightforward to implement and outperforms existing solution we underpin these claim by an experiment design where among others the domain transferability is analyzed the achieved performance in term of the f measure for featured article are within a single wikipedia domain and in a domain transfer situation 
learning to understand user search intent from their online behavior is crucial for both web search and online advertising however it is a challenging task to collect and label a sufficient amount of high quality training data for various user intent such a compare product plan a travel etc motivated by this bottleneck we start with some user common sense i e a set of rule to generate training data for learning to predict user intent the rule generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noise in this paper we introduce a co learning framework clf to tackle the problem of learning from biased and noisy rule generated training data clf firstly generates multiple set of possibly biased and noisy training data using different rule and then train the individual user search intent classifier over different training datasets independently the intermediate classifier are then used to categorize the training data themselves a well a the unlabeled data the confidently classified data by one classifier are added to other training datasets and the incorrectly classified one are instead filtered out from the training datasets the algorithmic performance of this iterative learning procedure is theoretically guaranteed 
this paper report a word shape coding method to facilitate retrieval of camera based document image without ocr due to perspective distortion many reported word shape coding method fail on camera based image in this paper the problem is addressed by approximating the perspective transformation with an affine transformation and employing an affine invariant namely length ratio to represent the connected component component in a document image are classified into a few cluster each of which is assigned with a representative symbol retrieval are based on word comprising of symbol the experiment result showed that the proposed method achieved an average retrieval precision of and recall of 
in this work a novel mobile browser for geo referenced picture is introduced and described we use the term browser to denote a system aimed at browsing picture selected from a large set like internet photo sharing service the criterion to filter a subset of picture to browse are three the user s actual position the user s actual heading and the user s preference in this work we only focus on the first two criterion leaving the integration of user s preference for future development 
the shape of the web in term of it graphical structure ha been a widely interested topic two graph bow tie and daisy have stood out from previous research in this work we take a different approach by viewing the web a a hierarchy of three level namely page level host level and domain level such structure are analyzed and compared with a snapshot of chinese web in early involving million page million host and million domain some interesting result have emerged for example the chinese web appears more like a teapot with a large size of scc a medium size of in and a small size of out at page level than the classic bow tie or daisy shape some challenging phenomenon are also observed for example the in become much smaller than out at host and domain level future work will tackle these puzzle 
suggesting topic that are related to user s goal or interest is very important in web search however search engine today focus on suggesting mainly reformulations and lexical variant of the query mined from query log in this demonstration we show a system that can suggest related topic for a query based on the top search result for the query it can help user in exploring the topic related to their information need the topic suggestion system can be integrated with any search engine or it can be easily installed on the client machine a a browser plugin 
hierarchical clustering is often used to cluster person name referring to the same entity since the correct number of cluster for a given person name is not known a priori some way of deciding where to cut the resulting dendrogram to balance risk of overor under clustering is needed this paper report on experiment in which outcome specific and result set measure are used to learn a global similarity threshold result on the web people search weps task indicate that approximately of the optimal f measure can be achieved on held out data 
in the past two year the amount of data published in rdf and following the linked data principle ha increased dramatically everyday people are publishing datasets a linked data however application that consume linked data are not mainstream yet to overcome this issue we present a beginner tutorial on consuming linked data we will discus existing technique how user can currently consume linked data and use it in their current application 
the analysis of query log from blog search engine show that news related query occupy a significant portion of the log this raise a interesting research question on whether the blogosphere can be used to identify important news story in this paper we present novel approach to identify important news story headline from the blogosphere for a given day the proposed system consists of two component based on the language model framework the query likelihood and the news headline prior for the query likelihood we propose several approach to estimate the query language model and the news headline language model we also suggest several criterion to evaluate the news headline prior that is the prior belief about the importance or newsworthiness of the news headline for a given day experimental result show that our system significantly outperforms a baseline system specifically the proposed approach give and further increase in map and p over the best performing result of the trec top story identification task 
in community type content such a blog and sn we call the user s unawareness of information a a content hole and the search for this information a a content hole search a content hole search differs from similarity searching and ha a variety of type in this paper we propose different type of content hole and define each type we also propose an analysis of dialogue related to community type content and introduce content hole search by using wikipedia a an example 
plagiarism the unacknowledged reuse of text ha increased in recent year due to the large amount of text readily available for instance recent study claim that nowadays a high rate of student report include plagiarism making manual plagiarism detection practically infeasible automatic plagiarism detection tool assist expert to analyse document for plagiarism nevertheless the lack of standard collection with case of plagiarism ha prevented accurate comparing model making difference hard to appreciate seminal effort on the detection of text reuse have fostered the composition of standard resource for the accurate evaluation and comparison of method the aim of this phd thesis is to address three of the main problem in the development of better model for automatic plagiarism detection i the adequate identification of good potential source for a given suspicious text ii the detection of plagiarism despite modification such a word substitution and paraphrasing special stress is given to cross language plagiarism and iii the generation of standard collection of case of plagiarism and text reuse in order to provide a framework for accurate comparison of model regarding difficulty i and ii we have carried out preliminary experiment over the meter corpus given a suspicious document dq and a collection of potential source document d the process is divided in two step first a small subset of potential source document d in d is retrieved the document d in d are the most related to dq and therefore the most likely to include the source of the plagiarised fragment in it we performed this stage on the basis of the kullback leibler distance over a subsample of document s vocabulary afterwards a detailed analysis is carried out comparing dq to every d in d in order to identify potential case of plagiarism and their source this comparison wa made on the basis of word n gram by considering n these n gram level are flexible enough to properly retrieve plagiarised fragment and their source despite modification the result is offered to the user to take the final decision further experiment were done in both stage in order to compare other similarity measure such a the cosine measure the jaccard coefficient and diverse fingerprinting and probabilistic model one of the main weakness of currently available model is that they are unable to detect cross language plagiarism approaching the detection of this kind of plagiarism is of high relevance a the most of the information published is written in english and author in other language may find it attractive to make use of direct translation our experiment carried out over parallel and a comparable corpus show that model of standard cross language information retrieval are not enough in fact if the analysed source and target language are related in some way common linguistic ancestor or technical vocabulary a simple comparison based on character n gram seems to be the option however in those case where the relation between the implied language is weaker other model such a those based on statistical machine translation are necessary we plan to perform further experiment mainly to approach the detection of cross language plagiarism in order to do that we will use the corpus developed under the framework of the pan competition on plagiarism detection cf pan clef http pan webis de model that consider cross language thesaurus and comparison of cognate will also be applied 
user query to search engine are observed to predominantly contain inflected content word but lack stopwords and capitalization thus they often resemble natural language query after case folding and stopword removal query recovery aim to generate a linguistically well formed query from a given user query a input to provide natural language processing task and cross language information retrieval clir the evaluation of query translation show that translation score nist and bleu decrease after case folding stopword removal and stemming a baseline method for query recovery reconstructs capitalization and stopwords which considerably increase translation score and significantly increase mean average precision for a standard clir task 
the combined effort of human volunteer have recently extracted numerous fact from wikipedia storing them a machine harvestable object attribute value triple in wikipedia infoboxes machine learning system such a kylin use these infoboxes a training data accurately extracting even more semantic knowledge from natural language text but in order to realize the full power of this information it must be situated in a cleanly structured ontology this paper introduces kog an autonomous system for refining wikipedia s infobox class ontology towards this end we cast the problem of ontology refinement a a machine learning problem and solve it using both svms and a more powerful joint inference approach expressed in markov logic network we present experiment demonstrating the superiority of the joint inference approach and evaluating other aspect of our system using these technique we build a rich ontology integrating wikipedia s infobox class schema with wordnet we demonstrate how the resulting ontology may be used to enhance wikipedia with improved query processing and other feature 
when online ad are shown together they compete for user attention and conversion imposing negative externality on each other while the competition for user attention in sponsored search can be captured via model of clickthrough rate the post click competition for conversion cannot since the value per click of an advertiser is proportional to the conversion probability conditional on a click which depends on the other ad displayed the private value of an advertiser is no longer one dimensional and the gsp mechanism is not adequately expressive we study the design of expressive gsp like mechanism for the simplest form that an advertiser s private value can have in the presence of such externalitiesan advertiser s value depends on exclusivity i e whether her ad is shown exclusively or along with other ad our auction take a input two dimensional per click bid for exclusive and nonexclusive display and have two type of outcome either a single ad is displayed exclusively or multiple ad are simultaneously shown we design two expressive auction that are both extension of gspthe first auction gsp d is designed with the property that the allocation and pricing are identical to gsp when multiple ad are shown the second auction np d is designed to be a next price auction we show that both auction have high efficiency and revenue in all reasonable equilibrium further the np d auction is guaranteed to always have an equilibrium with revenue at least a much a the current gsp mechanism however we find that unlike with one dimensional valuation the gsp like auction for these richer valuation do not always preserve efficiency and revenue with respect to the vcg mechanism 
this work present a flexible xml selection language flexpath which allows the formulation of flexible constraint on both structure and content of xml document some experimental result obtained with a preliminary prototype are described in order to show that the idea promise good result 
in microblogging service such a twitter the user may become overwhelmed by the raw data one solution to this problem is the classification of short text message a short text do not provide sufficient word occurrence traditional classification method such a bag of word have limitation to address this problem we propose to use a small set of domain specific feature extracted from the author s profile and text the proposed approach effectively classifies the text to a predefined set of generic class such a news event opinion deal and private message 
cluster label quality is crucial for browsing topic hierarchy obtained via document clustering intuitively the hierarchical structure should influence the labeling accuracy however most labeling algorithm ignore such structural property and therefore the impact of hierarchical structure on the labeling accuracy is yet unclear in our work we integrate hierarchical information i e sibling and parent child relation in the cluster labeling process we adapt standard labeling approach namely maximum term frequency jensen shannon divergence chi square test and information gain to take use of those relationship and evaluate their impact on different datasets namely the open directory project wikipedia trec ohsumed and the clef ip european patent dataset we show that hierarchical relationship can be exploited to increase labeling accuracy especially on high level node 
fuzzy ontology are envisioned to be useful in the semantic web existing fuzzy ontology reasoner are not scalable enough to handle the scale of data that the web provides in this paper we propose a framework of fuzzy query language for fuzzy ontology and present query answering algorithm for these query language over fuzzy dl lite ontology moreover this paper report on implementation of our approach in the fuzzy dl lite query engine in the ontosearch system and preliminary but encouraging benchmarking result to the best of our knowledge this is the first ever scalable query engine for fuzzy ontology 
learning to rank ha attracted great attention in the ir community much thought and research ha been placed on query document feature extraction and development of sophisticated learning to rank algorithm however relatively little research ha been conducted on selecting document for learning to rank data set nor on the effect of these choice on the efficiency and effectiveness of learning to rank algorithm in this paper we employ a number of document selection methodology widely used in the context of evaluation depth k pooling sampling infap statap active learning mtc and on line heuristic hedge certain methodology e g sampling and active learning have been shown to lead to efficient and effective evaluation we investigate whether they can also enable efficient and effective learning to rank we compare them with the document selection methodology used to create the letor datasets further all of the utilized methodology are different in nature and thus they construct training data set with different property such a the proportion of relevant document in the data or the similarity among them we study how such property affect the efficiency effectiveness and robustness of learning to rank collection 
in this paper we study on the reliability of search engine user using click through data we proposed a graph based approach to evaluate user reliability according to how user click on search result list we tried to incorporate this measure of reliability into relevance feedback for improving ranking performance experimental result indicate that the proposed approach is both effective and applicable 
a the largest online marketplace in the world ebay ha a huge inventory where there are plenty of great rare item with potentially large even rapturous buyer these item are obscured in long tail of ebay item listing and hard to find through existing searching or browsing method it is observed that there are great rarity demand from user according to ebay query log to keep up with the demand the paper proposes a method to automatically detect rare item in ebay online listing a large set of feature relevant to the task are investigated to filter item and further measure item rareness the experiment on the most rarity demand intensitive domain show that the method may effectively detect rare item precision 
question classification play an important role in most question answering system in this paper we exploit semantic feature in support vector machine svms for question classification we propose a semantic tree kernel to incorporate semantic similarity information a diverse set of semantic feature is evaluated experimental result show that svms with semantic feature especially semantic class can significantly outperform the state of the art system 
this paper describes a simple clustering approach to person name disambiguation of retrieved document the method are based on standard ir concept and do not require any task specific feature we compare different term weighting and indexing method and evaluate their performance against the web people search task weps despite their simplicity these approach achieve very competitive performance 
sponsored search is one of the enabling technology for today s web search engine it corresponds to matching and showing ad related to the user query on the search engine result page user are likely to click on topically related ad and the advertiser pay only when a user click on their ad hence it is important to be able to predict if an ad is likely to be clicked and maximize the number of click we investigate the sponsored search problem from a machine learning perspective with respect to three main sub problem how to use click data for training and evaluation which learning framework is more suitable for the task and which feature are useful for existing model we perform a large scale evaluation based on data from a commercial web search engine result show that it is possible to learn and evaluate directly and exclusively on click data encoding pairwise preference following simple and conservative assumption we find that online multilayer perceptron learning based on a small set of feature representing content similarity of different kind significantly outperforms an information retrieval baseline and other learning model providing a suitable framework for the sponsored search task 
in this paper we describe a problem of discovering query cluster from a click through graph of web search log the graph consists of a set of web search query a set of page selected for the query and a set of directed edge that connects a query node and a page node clicked by a user for the query the proposed method extract all maximal bipartite clique bicliques from a click through graph and compute an equivalence set of query i e a query cluster from the maximal bicliques a cluster of query is formed from the query in a biclique we present a scalable algorithm that enumerates all maximal bicliques from the click through graph we have conducted experiment on yahoo web search query and the result is promising 
a traditional medium and information device integrate with the web they must suddenly support a vastly larger database of relevant item many device use remote control with on screen keyboard which are not well suited for text entry but are difficult to displace we introduce a text entry method which significantly improves text entry speed for on screen keyboard using the same simple up down left right enter interface common to remote control and gaming device used to enter text the paper describes quicksuggest s novel adaptive user interface demonstrates quantitative improvement from simulation result on million of user query and show ease of use and efficiency with no learning curve in user experiment 
the context of the user query preceding a given query is utilized to improve the effectiveness of query classification earlier effort utilize fixed number of preceding query to derive such context information we propose and evaluate an approach dqw that identifies a set of unambiguous preceding query in a dynamically determined window to utilize in classifying an ambiguous query furthermore utilizing a relationship net r net that represents relationship among known category we improve the classification effectiveness for those ambiguous query whose predicted category in this relationship net is related to the category of a query within the window our result indicate that the hybrid approach dqw r net statistically significantly improves the conditional random field crf query classification approach when static query windowing and hierarchical taxonomy are used sqw tax in term of precision recall and f measure 
the behavioral description based web service composition wsc problem aim at the automatic construction of a coordinator web service that control a set of web service to reach a goal state however solving the wsc problem exactly with a realistic model is doubly exponential in the number of variable in web service description in this paper we propose a novel efficient approximation based algorithm using automatic abstraction and refinement to dramatically reduce the number of variable needed to solve the problem 
the migration from desktop application to web based service is scattering personal data across a myriad of web site such a google flickr youtube and amazon s this dispersal pose new challenge for user making it more difficult for them to organize search and archive their data much of which is now hosted by web site create heterogeneous multi web service object collection and share them in a protected way and manipulate their data with standard application or script in this paper we show that a web service interface supporting standardized naming protection and object access service can solve these problem and can greatly simplify the creation of a new generation of object management service for the web we describe the implementation of menagerie a proof of concept prototype that provides these service for web based application at a high level menagerie creates an integrated file and object system from heterogeneous personal web service object dispersed across the internet we present several object management application we developed on menagerie to show the practicality and benefit of our approach 
named entity recognition study the problem of locating and classifying part of free text into a set of predefined category although extensive research ha focused on the detection of person location and organization entity there are many other entity of interest including phone number date time and currency to name a few example we refer to these type of entity a semi structured named entity since they usually follow certain syntactic format according to some convention although their structure is typically not well defined regular expression solution require significant amount of manual effort and supervised machine learning approach rely on large set of labeled training data therefore these approach do not scale when we need to support many semi structured entity type in many language and region in this paper we study this problem and propose a novel three level bootstrapping framework for the detection of semi structured entity we describe the proposed technique for phone date and time entity and perform extensive evaluation on english german polish swedish and turkish document despite the minimal input from the user our approach can achieve precision and recall for phone entity and precision and recall for date and time entity on average we also discus implementation detail and report run time performance result which show significant improvement over regular expression based solution 
this paper present a technique for visualizing large spatial data set in web mapping system wms the technique creates a hierarchical clustering tree which is subsequently used to extract cluster that can be displayed at a given scale without cluttering the map voronoi polygon are used a aggregation symbol to represent the cluster this technique retains hierarchical relationship between data item at different scale in addition aggregation symbol do not overlap and their size and the number of point that they cover is controlled by the same parameter a prototype ha been implemented and tested showing the effectiveness of the method for visualizing large data set in wms 
the run time binding of web service ha been recently put forward in order to support rapid and dynamic web service composition with the growing number of alternative web service that provide the same functionality but differ in quality parameter the service composition becomes a decision problem on which component service should be selected such that user s end to end qos requirement e g availability response time and preference e g price are satisfied although very efficient local selection strategy fails short in handling global qos requirement solution based on global optimization on the other hand can handle global constraint but their poor performance render them inappropriate for application with dynamic and real time requirement in this paper we address this problem and propose a solution that combine global optimization with local selection technique to benefit from the advantage of both world the proposed solution consists of two step first we use mixed integer programming mip to find the optimal decomposition of global qos constraint into local constraint second we use distributed local selection to find the best web service that satisfy these local constraint the result of experimental evaluation indicate that our approach significantly outperforms existing solution in term of computation time while achieving close to optimal result 
we present an approach for answering entity retrieval query using click through information in query log data from a commercial web search engine we compare result using click graph and session graph and present an evaluation test set making use of wikipedia list of page 
nowadays web portal contain large amount of information that is meant for various visitor or group of visitor to effectively navigate within the content the website need to know it user in order to provide personalized content to them we propose a method for automatic estimation of the user s interest in a web page he visit this estimation is used for the recommendation of web portal page through presenting adaptive link that the user might like we conducted series of experiment in the domain of our faculty web portal to evaluate proposed approach 
the state of the art in named entity recognition relies on a combination of local feature of the text and global knowledge to determine the type of the recognized entity this is problematic in some case resulting in entity being classified a belonging to the wrong type we show that using global information about the corpus improves the accuracy of type identification we explore the notion of a global domain frequency that relates relation identifying term with pair of entity type which are used in that relation we use this to identify entity whose type are not compatible with the term they co occur in the text our result on a large corpus of social medium content allows the identification of mistyped entity with accuracy 
we present hamlet a suite of principle scoring model and algorithm to automatically propagate metadata along edge in a document neighborhood a a showcase scenario we consider tag prediction in community based web tagging application experiment using real world data demonstrate the viability of our approach in large scale environment where tag are scarce to the best of our knowledge hamlet is the first system to promote an efficient and precise reuse of shared metadata in highly dynamic large scale web tagging system 
researcher have been studying and developing teaching material for information retrieval ir such a toolkits also have been built that provide hand on experience to student for example ir toolbox is an effort to close the gap between the student understanding of ir concept and real life indexing and search system such tool might be good for helping student in non technical area such a in the library and information science field to develop their conceptual model of search engine however they do not cover emerging topic and skill such a content based image retrieval cbir and fusion search although there is open source software such a those in http www searchtools com tool tool opensource html that can be used to teach basic and advanced ir topic they require a student to have high level technical knowledge and to spend a long time to gain a practical understanding of these topic we present a new and rapid approach to teach basic and advanced ir topic such a text retrieval web based ir cbir and fusion search to computer science c graduate student we designed project that would help student grasp the abovementioned ir topic student working in team were given a practical application to start with the superimposed application for image description and retrieval saidr earlier sierra allows user to associate part of image with multimedia information such a text annotation also user may retrieve information in one of two way perform text based retrieval on annotation perform cbir on image and part of image that look like a query image or part of a query image each team wa asked to build an enhancement for this application involving text retrieval and or cbir in three week time the sub project are described in table the outcome of this activity wa that student learned about ir concept while being able to relate their applicability to a real world problem figure detail of these project may be found at http collab dlib vt edu runwiki wiki pl tabletpcimageretrievalsuperimposedinformation we will demonstrate the tool developed along with the ir concept they illustrate table we believe these tool may aid others to learn about basic and advanced topic in ir 
can we leverage the community contributed collection of rich medium on the web to automatically generate representative and diverse view of the world s landmark we use a combination of contextand content based tool to generate representative set of image for location driven feature and landmark a common search task to do that we using location and other metadata a well a tag associated with image and the image visual feature we present an approach to extracting tag that represent landmark we show how to use unsupervised method to extract representative view and image for each landmark this approach can potentially scale to provide better search and representation for landmark worldwide we evaluate the system in the context of image search using a real life dataset of image from the san francisco area 
the analysis of the leading social video sharing platform youtube reveals a high amount of redundancy in the form of video with overlapping or duplicated content in this paper we show that this redundancy can provide useful information about connection between video we reveal these link using robust content based video analysis technique and exploit them for generating new tag assignment to this end we propose different tag propagation method for automatically obtaining richer video annotation our technique provide the user with additional information about video and lead to enhanced feature representation for application such a automatic data organization and search experiment on video clustering and classification a well a a user evaluation demonstrate the viability of our approach 
blog facilitate online debate and discussion for million of people around the world identifying the most popular and prevailing topic discussed in the blogosphere is a crucial task this poster describes our novel approach to the quantification of the level of topic propagation in the blogosphere our model us graph theoretic representation of the blogosphere s link structure that allows it to deduce the percolation threshold which is then used in the quantification and definition of a global topic the result of our experiment on a blog collection show that our model is able to quantify the propagation of topic moreover our model is successful in identifying specific topic that propagate throughout the blogosphere and classifies them a global 
due to the rapid growth in the size of the web web search engine are facing enormous performance challenge the larger engine in particular have to be able to process ten of thousand of query per second on ten of billion of document making query throughput a critical issue to satisfy this heavy workload search engine use a variety of performance optimization including index compression caching and early termination we focus on two technique inverted index compression and index caching which play a crucial rule in web search engine a well a other high performance information retrieval system we perform a comparison and evaluation of several inverted list compression algorithm including new variant of existing algorithm that have not been studied before we then evaluate different inverted list caching policy on large query trace and finally study the possible performance benefit of combining compression and caching the overall goal of this paper is to provide an updated discussion and evaluation of these two technique and to show how to select the best set of approach and setting depending on parameter such a disk speed and main memory cache size 
vertical aggregation is the task of incorporating result from specialized search engine or vertical e g image video news into web search result vertical selection is the subtask of deciding given a query which vertical if any are relevant state of the art approach use machine learned model to predict which vertical are relevant to a query when trained using a large set of labeled data a machine learned vertical selection model outperforms baseline which require no training data unfortunately whenever a new vertical is introduced a costly new set of editorial data must be gathered in this paper we propose method for reusing training data from a set of existing source vertical to learn a predictive model for a new target vertical we study method for learning robust portable and adaptive cross vertical model experiment show the need to focus on different type of feature when maximizing portability the ability for a single model to make accurate prediction across multiple vertical than when maximizing adaptability the ability for a single model to make accurate prediction for a specific vertical we demonstrate the efficacy of our method through extensive experimentation for vertical 
keyword based retrieval match search term and document via term co occurrence such an approach doe not allow matching based on the specific plant characteristic description that are often used in botanical text retrieval this study applies information extraction technique to automatically extract plant characteristic information from text and allows user to search using such information in combination with keywords an evaluation experiment wa conducted using actual user the result indicate that this approach enhances task based retrieval performance 
the web ha dramatically enhanced people s ability to communicate idea knowledge and opinion but the authoring tool that most people understand blog and wikis primarily guide user toward authoring text in this work we show that substantial gain in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualization we then describe several extension we have created for blog and wikis that enable user to publish share and aggregate such structured information using the same workflow they apply to text in particular we aim to preserve those attribute that make blog and wikis so effective one click access to the information one click publishing of content natural authoring interface and the ability to easily copy and paste information and visualization from other source 
we address a specific enterprise document search scenario where the information need is expressed in an elaborate manner in our scenario information need are expressed using a short query of a few keywords together with example of key reference page given this setup we investigate how the example can be utilized to improve the end to end performance on the document retrieval task our approach is based on a language modeling framework where the query model is modified to resemble the example page we compare several method for sampling expansion term from the example page to support query dependent and query independent query expansion the latter is motivated by the wish to increase aspect recall and attempt to uncover aspect of the information need not captured by the query for evaluation purpose we use the csiro data set created for the trec enterprise track the best performance is achieved by query model based on query independent sampling of expansion term from the example document 
their patient record from paper to computer enormous amount of electronic medical record emr have become available for medical research some of the emr data are well structured for which traditional database management system can provide effective retrieval and management function however most of the emr data such a progress note and consultation letter are in free text format how to effectively and efficiently retrieve and discover useful information from the vast amount of such semi structured data is a challenge faced by medical professional without proper tool the rich information and knowledge buried in the medical health record are unavailable for clinical research and decision making the objective of our research is to develop text analytics tool that are capable of parsing clinical medical data so that predefined search subject that correspond to a list of medical diagnosis can be extracted in addition to this particular core functionality it is also desired that several important asset should be present within the text analytics tool in order to improve it overall ability to be used a recommendation tool in this research we work with research scientist at the institute for clinical evaluative science ice in toronto and examine a number of technique for structuring and processing free text document in order to effectively and efficiently search and analyze vast amount of medical record we implement several powerful medical text analytics tool for clinical data searching and classification for data classification our tool sort through a great amount of patientrecords to identify the likelihood of a patient having myocardial infarction mi or hypertension htn and classify the patient accordingly our tool can also identify the likelihood of a patient being a smoker previous smoker or non smoker based on the text data of medical record 
this paper present a new way of thinking for ir metric optimization it is argued that the optimal ranking problem should be factorized into two distinct yet interrelated stage the relevance prediction stage and ranking decision stage during retrieval the relevance of document is not known a priori and the joint probability of relevance is used to measure the uncertainty of document relevance in the collection a a whole the resulting optimization objective function in the latter stage is thus the expected value of the ir metric with respect to this probability measure of relevance through statistically analyzing the expected value of ir metric under such uncertainty we discover and explain some interesting property of ir metric that have not been known before our analysis and optimization framework do not assume a particular relevance retrieval model and metric making it applicable to many existing ir model and metric the experiment on one of resulting application have demonstrated it significance in adapting to various ir metric 
growing interest in online collection of digital book and video content motivates the development and optimization of adequate retrieval system however traditional method for collecting relevance assessment to tune system performance are challenged by the nature of digital item in such collection where assessor are faced with a considerable effort to review and ass content by extensive reading browsing and within document searching the extra strain is caused by the length and cohesion of the digital item and the dispersion of topic within it we propose a method for the collective gathering of relevance assessment using a social game model to instigate participant engagement the game provides incentive for assessor to follow a predefined review procedure and make provision for the quality control of the collected relevance judgment we discus the approach in detail and present the result of a pilot study conducted on a book corpus to validate the approach our analysis reveals intricate relationship between the affordances of the system the incentive of the social game and the behavior of the assessor we show that the proposed game design achieves two designated goal the incentive structure motivates endurance in assessor and the review process encourages truthful assessment 
music information retrieval mir hold great promise a a technology for managing large music archive one of the key component of mir that ha been actively researched into is music tagging while significant progress ha been achieved most of the existing system still adopt a simple classification approach and apply machine learning classifier directly on low level acoustic feature consequently they suffer the shortcoming of poor accuracy lack of comprehensive evaluation result and the associated analysis based on large scale datasets and incomplete content representation arising from the lack of multimodal and temporal information integration in this paper we introduce a novel system called mmtagger that effectively integrates both multimodal and temporal information in the representation of music signal the carefully designed multilayer architecture of the proposed classification framework seamlessly combine multiple gaussian mixture model gmms and support vector machine svm into a single framework the structure preserve more discriminative information leading to more accurate and robust tagging experiment result obtained with two large music collection highlight the various advantage of our multilayer framework over state of the art technique 
people often use computer other than their own to access web content but blind user are restricted to using computer equipped with expensive special purpose screen reading program that they use to access the web webanywhere is a web based self voicing web application that enables blind web user to access the web from almost any computer that can produce sound without installing new software webanywhere could serve a a convenient low cost solution for blind user on the go for blind user unable to afford another screen reader and for web developer targeting accessible design this paper describes the implementation of webanywhere overview an evaluation of it by blind web user and summarizes a survey of public terminal that show it can run on most public computer 
this paper investigates the agreement of relevance assessment between official trec judgment and those generated from an interactive ir experiment result show that of document judged relevant by our user matched official trec judgment several factor contributed to difference in the agreement the number of retrieved relevant document the number of relevant document judged system effectiveness per topic and the ranking of relevant document 
contextual advertising support much of the web s ecosystem today user experience and revenue shared by the site publisher and the ad network depend on the relevance of the displayed ad to the page content a with other document retrieval system relevance is provided by scoring the match between individual ad document and the content of the page where the ad are shown query in this paper we show how this match can be improved significantly by augmenting the ad page scoring function with extra parameter from a logistic regression model on the word in the page and ad a key property of the proposed model is that it can be mapped to standard cosine similarity matching and is suitable for efficient and scalable implementation over inverted index the model parameter value are learnt from log containing ad impression and click with shrinkage estimator being used to combat sparsity to scale our computation to train on an extremely large training corpus consisting of several gigabyte of data we parallelize our fitting algorithm in a hadoop framework experimental evaluation is provided showing improved click prediction over a holdout set of impression and click event from a large scale real world ad placement engine our best model achieves a lift in precision relative to a traditional information retrieval model which is based on cosine similarity for recalling of the click in our test data 
since it introduction social medium a group of internet based application that allow the creation and exchange of user generated content ha attracted more and more user over the year many platform have arisen that allow user to publish information communicate with others connect to like minded and share anything a user want to share text centric example are mailing list forum blog community question answering collaborative knowledge source social network and microblogs with new platform starting all the time given the volume of information available in social medium way of accessing this information intelligently are needed this is the scope of my research why should we care about information in social medium here are three example that motivate my interest a viewpoint research someone want to take note of the viewpoint on a particular issue b answer to problem many problem have been encountered before and people have shared solution c product development gaining insight into how people use a product and what feature they wish for eas the development of new product looking at these example of information need in social medium we observe that they revolve not just around relevance in the traditional sense i e object relevant to a given topic but also around criterion like credibility authority viewpoint expertise and experience however these additional aspect are typically conditioned on the topical relevance of information object in social medium information object come in several type but many are utterance created by people blog post email question answer tweet people and their utterance offer two natural entry point to information contained in social medium utterance that are relevant and people that are of interest i focus on three task in which the interaction between the two is key 
knowledge automatically extracted from text capture instance class of instance and relation among them in particular the acquisition of class attribute e g top speed body style and number of cylinder for the class of sport car from text is a particularly appealing task and ha received much attention recently given it natural fit a a building block towards the far reaching goal of constructing knowledge base from text this tutorial provides an overview of extraction method developed in the area of web based information extraction with the purpose of acquiring attribute of open domain class the attribute are extracted for class organized either a a flat set or hierarchically the extraction method operate over unstructured or semi structured text available within collection of web document or over relatively more intriguing data source consisting of anonymized search query the method take advantage of weak supervision provided in the form of seed example or small amount of annotated data or draw upon knowledge already encoded within human compiled resource e g wikipedia the more ambitious method aiming at acquiring a many accurate attribute from text a possible for hundred or thousand of class covering a wide range of domain of interest need to be designed to scale to web collection this restriction ha significant consequence on the overall complexity and choice of underlying tool in order for the extracted attribute to ultimately aid information retrieval in general and web search in particular by producing relevant attribute for open domain class along with other type of relation among instance or among class 
in numerous everyday domain it ha been demonstrated that increasing the number of option beyond a handful can lead to paralysis and poor choice and decrease satisfaction with the choice were this so called paradox of choice to hold in search engine use it would mean that increasing recall can actually work counter to user satisfaction if it implies choice from a more extensive set of result item the existence of this effect wa demonstrated in an experiment where user n were shown a search scenario and a query and were required to choose the best result item within second having to choose from six result yielded both higher subjective satisfaction with the choice and greater confidence in it correctness than when there were item on the result page we discus this finding in the wider context of choice architecture that is how result presentation affect choice and satisfaction 
name ambiguity problem ha been a challenging issue for a long history in this paper we intend to make a thorough investigation of the whole problem specifically we formalize the name disambiguation problem in a unified framework the framework can incorporate both attribute and relationship into a probabilistic model we explore a dynamic approach for automatically estimating the person number k and employ an adaptive distance measure to estimate the distance between object experimental result show that our proposed framework can significantly outperform the baseline method 
the abundant knowledge in web community ha motivated the research interest in discussion thread the dynamic nature of discussion thread pose interesting and challenging problem for computer scientist although technique such a semantic model or structural model have been shown to be useful in a number of area they are inefficient in understanding discussion thread due to the temporal dependence among post in a discussion thread such dependence cause that semantics and structure coupled with each other in discussion thread in this paper we propose a sparse coding based model named sm to simultaneously model semantic and structure of discussion thread 
we discover community from social network data and analyze the community evolution these community are inherent characteristic of human interaction in online social network a well a paper citation network also community may evolve over time due to change to individual role and social status in the network a well a change to individual research interest we present an innovative algorithm that deviate from the traditional two step approach to analyze community evolution in the traditional approach community are first detected for each time slice and then compared to determine correspondence we argue that this approach is inappropriate in application with noisy data in this paper we propose facetnet for analyzing community and their evolution through a robust unified process in this novel framework community not only generate evolution they also are regularized by the temporal smoothness of evolution a a result this framework will discover community that jointly maximize the fit to the observed data and the temporal evolution our approach relies on formulating the problem in term of non negative matrix factorization where community and their evolution are factorized in a unified way then we develop an iterative algorithm with proven low time complexity which is guaranteed to converge to an optimal solution we perform extensive experimental study on both synthetic datasets and real datasets to demonstrate that our method discovers meaningful community and provides additional insight not directly obtainable from traditional method 
an analysis of the social video sharing platform youtube reveals a high amount of community feedback through comment for published video a well a through meta rating for these comment in this paper we present an in depth study of commenting and comment rating behavior on a sample of more than million comment on youtube video for which we analyzed dependency between comment view comment rating and topic category in addition we studied the influence of sentiment expressed in comment on the rating for these comment using the sentiwordnet thesaurus a lexical wordnet based resource containing sentiment annotation finally to predict community acceptance for comment not yet rated we built different classifier for the estimation of rating for these comment the result of our large scale evaluation are promising and indicate that community feedback on already rated comment can help to filter new unrated comment or suggest particularly useful but still unrated comment 
search engine click log provide an invaluable source of relevance information but this information is biased because we ignore which document from the result list the user have actually seen before and after they clicked otherwise we could estimate document relevance by simple counting in this paper we propose a set of assumption on user browsing behavior that allows the estimation of the probability that a document is seen thereby providing an unbiased estimate of document relevance to train test and compare our model to the best alternative described in the literature we gather a large set of real data and proceed to an extensive cross validation experiment our solution outperforms very significantly all previous model a a side effect we gain insight into the browsing behavior of user and we can compare it to the conclusion of an eye tracking experiment by joachim et al in particular our finding confirm that a user almost always see the document directly after a clicked document they also explain why document situated just after a very relevant document are clicked more often 
web search provider often include search service for domain specific subcollections called vertical such a news image video job posting company summary and artist profile we address the problem of vertical selection predicting relevant vertical if any for query issued to the search engine s main web search page in contrast to prior query classification and resource selection task vertical selection is associated with unique resource that can inform the classification decision we focus on three source of evidence the query string from which feature are derived independent of external resource log of query previously issued directly to the vertical and corpus representative of vertical content we focus on different vertical which differ in term of semantics medium type size and level of query traffic we compare our method to prior work in federated search and retrieval effectiveness prediction an in depth error analysis reveals unique challenge across different vertical and provides insight into vertical selection for future work 
a useful approach for enabling computer to automatically create new content is utilizing the text medium and information already present on the world wide web the newly created content is known a machine generated content for example a machine generated content system may create a multimedia news show with two animated anchor presenting a news story one anchor read the news story with text taken from an existing news article and the other anchor regularly interrupt with his or her own opinion about the story in this paper we present such a system and describe in detail it strategy for autonomously extracting and selecting the opinion given by the second anchor 
application of semantic technology often require the representation of and reasoning with structured object that is object composed of part connected in complex way although owl is a general and powerful language it class description and axiom cannot be used to describe arbitrarily connected structure an owl representation of structured object can thus be underconstrained which reduces the inference that can be drawn and cause performance problem in reasoning to address these problem we extend owl with description graph which allow for the description of structured object in a simple and precise way to represent conditional aspect of the domain we also allow for swrl like rule over description graph based on an observation about the nature of structured object we ensure decidability of our formalism we also present a hypertableau based decision procedure which we implemented in the hermit reasoner to evaluate it performance we have extracted description graph from the galen and fma ontology classified them successfully and even detected a modeling error in galen 
large volume of spatio temporal thematic data being created using site like twitter and jaiku can potentially be combined to detect event and understand various situation a they are evolving at different spatio temporal granularity across the world taking inspiration from traditional image pixel which represent aggregation of photon energy at a location we consider aggregation of user interest level at different geo location a social pixel combining such pixel spatio temporally allows for creation of social image and video here we describe how the use of relevant medium processing inspired situation detection operator upon such image and domain based rule can be used to decide relevant control action the idea are showcased using a swine flu monitoring application which us twitter data 
the semantic web for health care and life science workshop will be held in beijing china on april the goal of the workshop is to foster the development and advancement in the use of semantic web technology to facilitate collaboration research and development and innovation adoption in the domain of health care and life science we also encourage the participation of all research community in this event with enhanced participation from asia due to the location of the event the workshop consists of two invited keynote talk eight peer reviewed presentation and one panel discussion 
with the advent of web technology website have evolved from static page to dynamic interactive web based application with the ability to replicate common desktop functionality however for blind and visually impaired individual who rely upon screen reader web application force them to adapt to an inaccessible use model many technology including wai aria ajax and improved screen reader support are rapidly evolving to improve this situation however simply combining them doe not solve the problem of screen reader user the main contribution of this paper are two model of interaction for screen reader user for both traditional website and web application further contribution are a discussion of accessibility difficulty screen reader user encounter when interacting with web application a user workflow design model for improving web accessibility and a set of design requirement for developer to ease the user s burden and increase accessibility these model accessibility difficulty and design implication are based directly on response and lesson learned from usability research focusing on web usage and screen reader user without the conscious effort of web engineer and designer most blind and visually impaired user will shy away from using new web technology in favor of desktop based application 
key value store kv are the most prevalent storage system for large scale web service a they do not have the structural complexity of rdbmss they are more efficient in this paper we introduce the problem of keyword search over kv and analyze it difference with keyword search over document and relational database we propose a novel method called keyword search over key value store kstore to solve the problem our user study using two real life data set show that kstore provides better ranking than the extended version of document and relational database keyword search proposal and pose reasonable overhead 
for the last few year we have been studying the diffusion of private information for user a they visit various web site triggering data gathering aggregation by third party this paper report on our longitudinal study consisting of multiple snapshot of our examination of such diffusion over four year we examine the various technical way by which third party aggregator acquire data and the depth of user related information acquired we study technique for protecting privacy diffusion a well a limitation of such technique we introduce the concept of secondary privacy damage our result show increasing aggregation of user related data by a steadily decreasing number of entity a handful of company are able to track user movement across almost all of the popular web site virtually all the protection technique have significant limitation highlighting the seriousness of the problem and the need for alternate solution 
in this poster based on our previous work in building a lightweight ddos distributed denial of service attack detection mechanism for web server using tcm knn transductive confidence machine for k nearest neighbor and genetic algorithm based instance selection method we further propose a more efficient and effective instance selection method named e fcm extend fuzzy c mean by using this method we can obtain much cheaper training time for tcm knn while ensuring high detection performance therefore the optimized mechanism is more suitable for lightweight ddos attack detection in real network environment 
this paper proposes multiclass visualrank a method that expands the idea of visualrank into more than one category of image multiclass visualrank divide image retrieved from search engine into several category based on distinctive pattern of visual feature and give ranking within the category experimental result show that our method can extract several different image category relevant to given keyword and give good ranking score to retrieved image 
this paper formulates a collaborative system for modeling business application the system us a semantic wiki to enable collaboration between the various stakeholder involved in the design of the system and translates the captured intelligence into business model which are used for designing a business system 
clickthrough on search result have been successfully used to infer user interest and preference but are often noisy and potentially ambiguous we explore the potential of a complementary more sensitive signal mouse movementsin providing insight into the intent behind a web search query we report preliminary result of studying user mouse movement on search result page with the goal of inferring user intent in particular to explore whether we can automatically distinguish the different query class such a navigational v informational our preliminary exploration confirms the value of studying mouse movement for user intent inference and suggests interesting avenue for future exploration 
modern technique for distributed information retrieval use a set of document sampled from each server but these sample have been underutilised in server selection we describe a new server selection algorithm sushi which unlike earlier algorithm can make full use of the text of each sampled document and which doe not need training data sushi can directly optimise for many common case including high precision retrieval and by including a simple stopping condition can do so while reducing network traffic our experiment compare sushi with alternative and show it achieves the same effectiveness a the best current method while being substantially more efficient selecting a few a a many server 
in this paper we explore the use of category information for ad hoc retrieval in wikipedia we show that technique for entity ranking exploiting this category information can also be applied to ad hoc topic and lead to significant improvement automatically assigned target category are good surrogate for manually assigned category which perform only slightly better 
this paper target on enhancing latent semantic indexing lsi by exploiting category label specifically in the term document matrix the vector for each term either appearing in label or semantically close to label is scaled before performing singular value decomposition svd to boost it impact on the generated left singular vector a a result the similarity among document in the same category are increased furthermore an adaptive scaling strategy is designed to better utilize the hierarchical structure of category experimental result show that the proposed approach is able to significantly improve the performance of hierarchical text categorization 
multi view clustering is an important problem in information retrieval due to the abundance of data offering many perspective and generating multi view representation we investigate in this short note a late fusion approach for multi view clustering based on the latent modeling of cluster cluster relationship we derive a probabilistic multi view clustering model outperforming an early fusion approach based on multi view feature correlation analysis 
we used eye tracking equipment to observe participant a they performed three search task using three graphically enhanced web search interface kartoo searchme and viewzi in this poster we describe finding of the study focusing on how the presentation of serp result influence how the user scan and attends to the result and the user satisfaction with these search engine 
in the context of document retrieval in the biomedical domain this paper explores the complex relationship between the quality of initial query result and the overall utility of an interactive retrieval system we demonstrate that a content similarity browsing tool can compensate for poor retrieval result and that the relationship between retrieval performance and overall utility is non linear argument are advanced with user simulation which characterize the relevance of document that a user might encounter with different browsing strategy with broader implication to ir this work provides a case study of how user simulation can be exploited a a formative tool for automatic utility evaluation simulation based study provide researcher with an additional evaluation tool to complement interactive and cranfield style experiment 
generally web service are provided with different qos value so they can be selected dynamically in service composition process however the conventional context free composition qos model doe not consider the changeability of qos value and the context sensitive constraint during composition process in this paper we propose a rule based context sensitive qos model to support the changeability of qos value and the context sensitive constraint by considering context in the qos model web service composition can be used widely and flexibly in the real world business 
the exploitation of fundamental invariant is among the most elegant solution to many computational problem in a wide variety of domain one of the more powerful approach to exploit invariant is the principle of guilt by association in particular the principle of guilt by association is the foundation of remote homolog detection protein function prediction disease subtype diagnosis treatment plan prognosis and other challenge in computational biology the principle suggests that two entity are in a specific relationship if they exhibit invariant property underlying that relationship for example a protein is predicted to have a particular biological function if it exhibit the underlying invariant property of that functional group viz guilty by association to other member of that functional group through the shared invariant property in my talk i plan to present several facet of guilt by association in the computational prediction of protein function and draw parallel of these facet in information retrieval specifically i plan to touch on the following facet a the issue of chance association b novel generalizable form of association c fusion of multiple heterogeneous source of evidence d the dichotomy of knowing to a high degree of reliability that two entity are in some relationship and yet not knowing what that relationship is i hope this talk will be for the informational retrieval community a window to the opportunity in computational biology that may benefit from the depth and variety of solution information retrieval ha to offer 
log of user search on web health topic can exhibit sign of escalation of medical concern where initial query about common symptom are followed by query about serious rare illness we present an effort to predict such escalation based on the structure and content of page encountered during medical search session we construct and then characterize the performance of classifier that predict whether an escalation will occur after the access of a page our finding have implication for ranking algorithm and the design of search interface 
it is difficult to apply machine learning to new domain because often we lack labeled problem instance in this paper we provide a solution to this problem that leverage domain knowledge in the form of affinity between input feature and class for example in a baseball v hockey text classification problem even without any labeled data we know that the presence of the word puck is a strong indicator of hockey we refer to this type of domain knowledge a a labeled feature in this paper we propose a method for training discriminative probabilistic model with labeled feature and unlabeled instance unlike previous approach that use labeled feature to create labeled pseudo instance we use labeled feature directly to constrain the model s prediction on unlabeled instance we express these soft constraint using generalized expectation ge criterion term in a parameter estimation objective function that express preference on value of a model expectation in this paper we train multinomial logistic regression model using ge criterion but the method we develop is applicable to other discriminative probabilistic model the complete objective function also includes a gaussian prior on parameter which encourages generalization by spreading parameter weight to unlabeled feature experimental result on text classification data set show that this method outperforms heuristic approach to training classifier with labeled feature experiment with human annotator show that it is more beneficial to spend limited annotation time labeling feature rather than labeling instance for example after only one minute of labeling feature we can achieve accuracy on the ibm v mac text classification problem using ge fl whereas ten minute labeling document result in an accuracy of only 
various sector in developing country are typically dominated by the presence of a large number of small and micro business that operate in an informal unorganized manner many of these are single person run micro business and cannot afford to buy and maintain their own it infrastructure for others easy availability of cheap labour provides a convenient alternative even though it result in inefficiency a little or no record are maintained and only manual paper based process are followed this result in high response time for customer no formal accountability and higher charge for the business this translates to lower earnings and loss due to inefficiency in this paper we look at few such micro business segment and explore their current model of operation while identifying existing inefficiency and pain point we build upon the finding and propose an approach for delivering benefit of it solution to such micro business segment finally we present technology that realizes the proposed approach in the specific context of two such segment 
in this paper we study the problem of keyword proximity search over xml document and leverage the efficiency and effectiveness we take the disjunctive semantics among input keywords into consideration and identify meaningful compact connected tree a the answer of keyword proximity query we introduce the notion of compact lowest common ancestor clca and maximal clca mclca and propose compact connected tree cctrees and maximal cctrees mcctrees to efficiently and effectively answer keyword query we propose a novel ranking mechanism race to rank compact connected tree by taking into consideration both the structural similarity and the textual similarity our extensive experimental study show that our method achieves both high search efficiency and effectiveness and outperforms existing approach significantly 
we propose still a signature free remote exploit binary code injection attack blocker to protect web server and web application still is robust to almost all anti signature anti static analysis and anti emulation obfuscation 
web search engine discover indexable document by recursively crawling from a seed url their ranking take into account link popularity while this work well it introduces bias towards older document older document are more likely to be the target of link while new document with few or no incoming link are unlikely to rank highly in search result we describe a novel system for new web search based on link retrieved from the twitter micro blogging service the twitter service allows individual organisation and government to rapidly disseminate very short message to a wide variety of interested party when a twitter message contains a url we use the twitter message a a description of the url s target a twitter is frequently used for discussion of current event these message offer useful upto date annotation and instantaneous popularity reading for a small but timely portion of the web our working system is simple and fast and we believe may offer a significant advantage in revealing new information on the web that would otherwise be hidden from searcher beyond the basic system we anticipate the twitter message may add supplementary term for a url or add weight to existing term and that the reputation or authority of each message sender may serve to weight both annotation and query independent popularity 
the result of the ecml pkdd discovery challenge suggest that semi supervised learning method work well for spam filtering when the source of available labeled example differs from those to be classified we have attempted to reproduce these result using data from the and trec spam track and have found the opposite effect method like self training and transductive support vector machine yield inferior classifier to those constructed using supervised learning on the labeled data alone we investigate difference between the ecml pkdd and trec data set and methodology that may account for the opposite result 
current search engine do not support user search for chemical entity chemical name and formula beyond simple keyword search usually a chemical molecule can be represented in multiple textual way a simple keyword search would retrieve only the exact match and not the others we show how to build a search engine that enables search for chemical entity and demonstrate empirically that it improves the relevance of returned document our search engine first extract chemical entity from text performs novel indexing suitable for chemical name and formula and support different query model that a scientist may require we propose a model of hierarchical conditional random field for chemical formula tagging that considers long term dependency at the sentence level to substring search of chemical name a search engine must index substring of chemical name indexing all possible sub sequence is not feasible in practice we propose an algorithm for independent frequent subsequence mining to discover sub term of chemical name with their probability we then propose an unsupervised hierarchical text segmentation hts method to represent a sequence with a tree structure based on discovered independent frequent subsequence so that sub term on the hts tree should be indexed query model with corresponding ranking function are introduced for chemical name search experiment show that our approach to chemical entity tagging perform well furthermore we show that index pruning can reduce the index size and query time without changing the returned ranked result significantly finally experiment show that our approach out perform traditional method for document search with ambiguous chemical term 
distributed search engine based on geographical partitioning of a central web index emerge a a feasible solution to the immense growth of the web user base and query traffic however there is still lack of research in quantifying the performance and quality gain that can be achieved by such architecture in this paper we develop various cost model to evaluate the performance benefit of a geographically distributed search engine architecture based on partial index replication and query forwarding specifically we focus on possible performance gain due to the distributed nature of query processing and web crawling process we show that any response time gain achieved by distributed query processing can be utilized to improve search relevance a the use of complex but more accurate algorithm can now be enabled for document ranking we also show that distributed web crawling lead to better web coverage and try to see if this improves the search quality we verify the validity of our claim over large real life datasets via simulation 
local aspect of web search associating web content and query with geography is a topic of growing interest however the underlying question of how spatial variation is manifested in search query is still not well understood here we develop a probabilistic framework for quantifying such spatial variation on complete yahoo query log we find that our model is able to localize large class of query to within a few mile of their natural center based only on the distribution of activity for the query our model provides not only an estimate of a query s geographic center but also a measure of it spatial dispersion indicating whether it ha highly local interest or broader regional or national appeal we also show how variation on our model can track geographically shifting topic over time annotate a map with each location s distinctive query and delineate the sphere of influence for competing query in the same general domain 
we study how to automatically extract tourist trip from large volume of geo tagged photograph working with more than million of these photograph that are publicly available via photosharing community such a flickr and panoramio our goal is to satisfy the need of a tourist who specifies a starting location typically a hotel together with a bounded travel distance and demand a tour that visit the popular site along the way our system named antourage solves this intractable problem using a novel adaptation of the max min ant system mmas meta heuristic experiment using gps metadata crawled from flickr show that antourage can generate high quality tour 
learning to rank ha become a popular approach to build a ranking model for web search recently based on our observation the constitution of the training set will greatly influence the performance of the learned ranking model meanwhile the number of query in web search is nearly infinite and the human labeling cost is expensive hence a subset of query need to be carefully selected for training in this paper we develop a greedy algorithm to sample the query by simultaneously taking the query density difficulty and diversity into consideration the experimental result on a collected web search dataset comprising query show that the proposed method can lead to a more informative training set for building an effective model 
in this paper targeting del icio u tag data we propose a method folksoviz for deriving subsumption relationship between tag by using wikipedia text and visualizing a folksonomy to fulfill this method we propose a statistical model for deriving subsumption relationship based on the frequency of each tag on the wikipedia text a well a the tsd tag sense disambiguation method for mapping each tag to a corresponding wikipedia text the derived subsumption pair are visualized effectively on the screen the experiment show that the folksoviz manages to find the correct subsumption pair with high accuracy 
we present a method for projecting retrieval score across two corpus with a shared parallel corpus 
this paper considers the problem of identifying on the web compound document cdocs group of web page that in aggregate constitute semantically coherent information entity example of cdocs are a news article consisting of several html page or a set of page describing specification price and review of a digital camera being able to identify cdocs would be useful in many application including web and intranet search user navigation automated collection generation and information extraction in the past several heuristic approach have been proposed to identify cdocs however heuristic fail to capture the variety of type style and goal of information on the web and do not account for the fact that the definition of a cdoc often depends on the context this paper present an experimental evaluation of three machine learning based algorithm for cdoc discovery these algorithm are responsive to the varying structure of cdocs and adaptive to their application specific nature based on our previous work this paper proposes a different scenario for discovering cdocs and compare in this new setting the local machine learned clustering algorithm from to a global purely graph based approach and a conditional markov network approach previously applied to noun coreference task the result show that the approach of outperforms the other algorithm suggesting that global relational characteristic of web site are too noisy for cdoc identification purpose 
we study in this paper the problem of bridging the semantic gap between low level image feature and high level semantic concept which is the key hindrance in content based image retrieval piloted by the rich textual information of web image the proposed framework try to learn a new distance measure in the visual space which can be used to retrieve more semantically relevant image for any unseen query image the framework differentiates with traditional distance metric learning method in the following way a ranking based distance metric learning method is proposed for image retrieval problem by optimizing the leave one out retrieval performance on the training data to be scalable million of image together with rich textual information have been crawled from the web to learn the similarity measure and the learning framework particularly considers the indexing problem to ensure the retrieval efficiency to alleviate the noise in the unbalanced label of image and fully utilize the textual information a latent dirichlet allocation based topic level text model is introduced to define pairwise semantic similarity between any two image the learnt distance measure can be directly applied to application such a content based image retrieval and search based image annotation experimental result on the two application in a two million web image database show both the effectiveness and efficiency of the proposed framework 
this paper address the issue of query refinement which involves reformulating ill formed search query in order to enhance relevance of search result query refinement typically includes a number of task such a spelling error correction word splitting word merging phrase segmentation word stemming and acronym expansion in previous research such task were addressed separately or through employing generative model this paper proposes employing a unified and discriminative model for query refinement specifically it proposes a conditional random field crf model suitable for the problem referred to a conditional random field for query refinement crf qr given a sequence of query word crf qr predicts a sequence of refined query word a well a corresponding refinement operation in that sense crf qr differs greatly from conventional crf model two type of crf qr model namely a basic model and an extended model are introduced one merit of employing crf qr is that different refinement task can be performed simultaneously and thus the accuracy of refinement can be enhanced furthermore the advantage of discriminative model over generative model can be fully leveraged experimental result demonstrate that crf qr can significantly outperform baseline method furthermore when crf qr is used in web search a significant improvement of relevance can be obtained 
in this paper we propose a novel approach for measuring similarity between web object our similarity measure is defined based on the representation of a web object a a collection of tag precisely we first construct a vector space in which multiple term are mapped into a single dimension by using information available from open directory project and delicious com then we position web object in the vector space and apply the traditional cosine measure for similarity computation we demonstrate that the proposed similarity computation method is able to overcome the limitation of traditional vector space approach while at the same time require le computational cost compare to lsi latent semantic indexing 
relocation in personal desktop resource is an interesting and promising research topic this demonstration illustrates a new perspective in exploring desktop resource to help user re find expected data resource more effectively different from existing work our prototype orientspace ha two feature automatically extract and maintain user task to support task based exploration and support vague search by exploiting association between desktop resource 
it ha been observed that precision increase with collection size one explanation could be that the redundancy of information increase making it easier to find multiple document conveying the same information arguably a user ha no interest in reading the same information over and over but would prefer a set of diverse search result covering multiple aspect of the search topic in this paper we look at the impact of the collection size on the relevance and diversity of retrieval result by down sampling the collection our main finding is that we can we can improve diversity by randomly removing the majority of the result this will significantly reduce the redundancy and only marginally affect the subtopic coverage 
with the rise of digital video consumption contextual video advertising demand have been increasing in recent year this paper present a novel video advertising system that selects relevant text ad for a given video scene by automatically identifying the situation of the scene the situation information of video scene is inferred from available video script experimental result show that the use of the situation information enhances the accuracy of ad retrieval for video scene the proposed system represents one of the pioneer video advertising system using contextual information obtained from video script 
transferring knowledge from one domain to another is challenging due to a number of reason since both conditional and marginal distribution of the training data and test data are non identical model trained in one domain when directly applied to a different domain is usually low in accuracy for many application with large feature set such a text document sequence data medical data image data of different resolution etc two domain usually do not contain exactly the same feature thus introducing large number of missing value when considered over the union of feature from both domain in other word it marginal distribution are at most overlapping in the same time these problem are usually high dimensional such a several thousand of feature thus the combination of high dimensionality and missing value make the relationship in conditional probability between two domain hard to measure and model to address these challenge we propose a framework that first brings the marginal distribution of two domain closer by filling up those missing value of disjoint feature afterwards it look for those comparable sub structure in the latent space a mapped from the expanded feature vector where both marginal and conditional distribution are similar with these sub structure in latent space the proposed approach then find common concept that are transferable across domain with high probability during prediction unlabeled instance are treated a query the mostly related labeled instance from out domain are retrieved and the classification is made by weighted voting using retrieved out domain example we formally show that importing feature value across domain and latent semantic index can jointly make the distribution of two related domain easier to measure than in original feature space the nearest neighbor method employed to retrieve related out domain example is bounded in error when predicting in domain example software and datasets are available for download 
in this paper we describe some preliminary work on using monolingual projection of document collection for performing cross language information retrieval task the proposed methodology us multidimensional scaling for projecting the vector space representation of a given multilingual document collection into space of lower dimensionality an independent projection is computed for each different language and the structural similarity of the resulting projection are exploited for information retrieval task 
with the emergence of yahoo pipe and several similar service data mashup tool have started to gain interest of business user making these tool simple and accessible ton user with no or little programming experience ha become a pressing issue in this paper we introduce mario mashup automation with runtime orchestration and invocation a new tool that radically simplifies data mashup composition we have developed an intelligent automatic composition engine in mario together with a simple user interface using an intuitive wishful search abstraction it thus allows user to explore the space of potentially composable data mashups and preview composition result a they iteratively refine their wish i e mashup composition goal it also let user discover and make use of system capability without having to understand the capability of individual component and instantly reflects change made to the component by presenting an aggregate view of changed capability of the entire system we describe our experience with using mario to compose flow of yahoo pipe component 
online photo service such a flickr and zooomr allow user to share their photo with family friend and the online community at large an important facet of these service is that user manually annotate their photo using so called tag which describe the content of the photo or provide additional contextual and semantical information in this paper we investigate how we can assist user in the tagging phase the contribution of our research is twofold we analyse a representative snapshot of flickr and present the result by mean of a tag characterisation focussing on how user tag photo and what information is contained in the tagging based on this analysis we present and evaluate tag recommendation strategy to support the user in the photo annotation task by recommending a set of tag that can be added to the photo the result of the empirical evaluation show that we can effectively recommend relevant tag for a variety of photo with different level of exhaustiveness of original tagging 
recent research provides evidence for the presence of emergent semantics in collaborative tagging system while several method have been proposed little is known about the factor that influence the evolution of semantic structure in these system a natural hypothesis is that the quality of the emergent semantics depends on the pragmatic of tagging user with certain usage pattern might contribute more to the resulting semantics than others in this work we propose several measure which enable a pragmatic differentiation of tagger by their degree of contribution to emerging semantic structure we distinguish between categorizers who typically use a small set of tag a a replacement for hierarchical classification scheme and describers who are annotating resource with a wealth of freely associated descriptive keywords to study our hypothesis we apply semantic similarity measure to different partition of a real world and large scale folksonomy containing different ratio of categorizers and describers our result not only show that verbose tagger are most useful for the emergence of tag semantics but also that a subset containing only of the most verbose tagger can produce result that match and even outperform the semantic precision obtained from the whole dataset moreover the result suggest that there exists a causal link between the pragmatic of tagging and resulting emergent semantics this work is relevant for designer and analyst of tagging system interested i in fostering the semantic development of their platform ii in identifying user introducing semantic noise and iii in learning ontology 
web search engine depend on the full text inverted index data structure because the query processing performance is so dependent on the size of the inverted index a plethora of research ha focused on fast end effective technique for compressing this structure recently several author have proposed technique for improving index compression by optimizing the assignment of document identifier to the document in the collection leading to significant reduction in overall index size in this paper we propose improved technique for document identifier assignment previous work includes simple and fast heuristic such a sorting by url a well a more involved approach based on the traveling salesman problem or on graph partitioning these technique achieve good compression but do not scale to larger document collection we propose a new framework based on performing a traveling salesman computation on a reduced sparse graph obtained through locality sensitive hashing this technique achieves improved compression while scaling to ten of million of document based on this framework we describe a number of new algorithm and perform a detailed evaluation on three large data set showing improvement in index size 
improving the precision of information retrieval ha been a challenging issue on chinese web a exemplified by chinese recipe on the web it is not easy natural for people to use keywords e g recipe name to search recipe since the name can be literally so abstract that they do not bear much if any information on the underlying ingredient or cooking method in this paper we investigate the underlying feature of chinese recipe and based on workflow like cooking procedure we model recipe a graph we further propose a novel similarity measurement based on the frequent pattern and devise an effective filtering algorithm to prune unrelated data so a to support efficient on line searching benefiting from the characteristic of graph frequent common pattern can be mined from a cooking graph database so in our prototype system called recipeview we extend the subgraph mining algorithm fsg to cooking graph and combine it with our proposed similarity measurement resulting in an approach that well caters for specific user need our initial experimental study show that the filtering algorithm can efficiently prune unrelated cooking graph without affecting the retrieval performance and the similarity measurement get a relatively higher precision recall against it counterpart 
cross language information retrieval clir between language of the same origin is an interesting topic of research the similarity of the writing system used for these language can be used effectively to not only improve clir but to overcome the problem of textual variation textual error and even the lack of linguistic resource like stemmer to an extent we have conducted clir experiment between three language which use writing system script of brahmi origin namely hindi bengali and marathi we found significant improvement for all the six language pair using a method for fuzzy text search based on surface similarity in this paper we report these result and compare them with a baseline clir system and a clir system that us scaled edit distance sed for fuzzy string matching 
web service process currently lack monitoring and dynamic runtime adaptation mechanism in highly dynamic process service frequently need to be exchanged due to a variety of reason in this paper we present viedame a system which allows monitoring of bpel process according to quality of service qos attribute and replacement of existing partner service based on various pluggable replacement strategy the chosen replacement service can be syntactically or semantically equivalent to the bpel interface service can be automatically replaced during runtime without any downtime of the overall system we implemented our solution with an aspect oriented approach by intercepting soap message and allow service to be exchanged during runtime with little performance penalty cost a shown in our experiment thereby making our approach suitable for high availability bpel environment 
patent search task are difficult and challenging often requiring expert patent analyst to spend hour even day sourcing relevant information to aid them in this process analyst use information retrieval system and tool to cope with their retrieval task with the growing interest in patent search it is important to determine their requirement and expectation of the tool and system that they employ in this poster we report a subset of the finding of a survey of patent analyst conducted to elicit their search requirement 
ad on the search engine se are generally ranked based on their click through rate ctr hence accurately predicting the ctr of an ad is of paramount importance for maximizing the se s revenue we present a model that inherits the click information of rare new ad from other semantically related ad the semantic feature are derived from the query ad click through graph and advertiser account information we show that the model learned using these feature give a very good prediction for the ctr value 
emergence of online content voting network allows user to share and rate content including social news photo and video the basic idea behind online content voting network is that aggregate user activity e g submitting and rating content make high quality content thrive through the unprecedented scale high dynamic and divergent quality of user generated content ugc to better understand the nature and impact of online content voting network we have analyzed digg a popular online social news aggregator and rating website based on a large amount of data collected we provide an in depth study of digg in particular we study structural property of digg social network impact of social network property on user digging activity and vice versa distribution of user diggs content promotion and information filtering we also provide insight into design of content promotion algorithm and recommendation assisted content discovery overall we believe that the result presented in this paper are crucial in understanding online content rating network 
dwell time a a user behavior ha been found in previous study to be an unreliable predictor of document usefulness with contextual factor such a the user s task needing to be considered in it interpretation task stage ha been shown to influence search behavior including usefulness judgment a ha task type this paper report on an investigation of how task stage and task type may help predict usefulness from the time that user spend on retrieved document over the course of several information seeking episode a stage controlled experiment wa conducted with participant each coming time to work on sub task of a general task couched either a parallel or dependent task type the full task wa to write a report on the general topic with interim document produced for each sub task result show that task stage can help in inferring document usefulness from decision time especially in the parallel task the finding can be used to increase accuracy in predicting document usefulness and accordingly in personalizing search for multi session task 
community question answering cqa ha emerged a a popular type of service where user ask and answer question and access historical question answer pair cqa archive contain very large volume of question organized into a hierarchy of category a an essential function of cqa service question retrieval in a cqa archive aim to retrieve historical question answer pair that are relevant to a query question in this paper we present a new approach to exploiting category information of question for improving the performance of question retrieval and we apply the approach to existing question retrieval model including a state of the art question retrieval model experiment conducted on real cqa data demonstrate that the proposed technique are capable of outperforming a variety of baseline method significantly 
widget container are used everywhere on the web for instance a customizable start page to web desktop in this poster we describe the extension of a widget container with an inter widget communication layer a well a the subsequent application programming interface apis added to the widget object to support this feature we present the benefit of a drag and drop facility within widget and conclude by a call for standardization of inter widget communication on the web 
our study address the problem of large scale contradiction detection and management from data extracted from the web we describe the first systematic solution to the problem based on a novel statistical measure for contradiction which exploit firstand second order moment of sentiment our approach enables the interactive analysis and online identification of contradiction under multiple level of time granularity the proposed algorithm can be used to analyze and track opinion evolution over time and to identify interesting trend and pattern it us an incrementally updatable data structure to achieve computational efficiency and scalability experiment with real datasets show promising time performance and accuracy 
the algorithmic small world hypothesis state that not only are pair of individual in a large social network connected by short path but that ordinary individual can find these path although theoretically plausible empirical evidence for the hypothesis is limited a most chain in small world experiment fail to complete thereby biasing estimate of true chain length using data from two recent small world experiment comprising a total of message chain and directed at one of target spread across country we model heterogeneity in chain attrition rate a a function of individual attribute we then introduce a rigorous way of estimating true chain length that is provably unbiased and can account for empirically observed variation in attrition rate our finding provide mixed support for the algorithmic hypothesis on the one hand it appears that roughly half of all chain can be completed in step thus supporting the six degree of separation assertion but on the other hand estimate of the mean are much longer suggesting that for at least some of the population the world is not small in the algorithmic sense we conclude that search distance in social network are fundamentally different from topological distance for which the mean and median of the shortest path length between node tend to be similar 
a a principled approach to capturing semantic relation of word in information retrieval statistical translation model have been shown to outperform simple document language model which rely on exact matching of word in the query and document a main challenge in applying translation model to ad hoc information retrieval is to estimate a translation model without training data existing work ha relied on training on synthetic query generated based on a document collection however this method is computationally expensive and doe not have a good coverage of query word in this paper we propose an alternative way to estimate a translation model based on normalized mutual information between word which is le computationally expensive and ha better coverage of query word than the synthetic query method of estimation we also propose to regularize estimated translation probability to ensure sufficient probability mass for self translation experiment result show that the proposed mutual information based estimation method is not only more efficient but also more effective than the synthetic query based method and it can be combined with pseudo relevance feedback to further improve retrieval accuracy the result also show that the proposed regularization strategy is effective and can improve retrieval accuracy for both synthetic query based estimation and mutual information based estimation 
despite general awareness of the importance of keeping one s system secure and widespread availability of consumer security technology actual investment in security remains highly variable across the internet population allowing attack such a distributed denial of service ddos and spam distribution to continue unabated by modeling security investment decision making in established e g weakest link best shot and novel game e g weakest target and allowing expenditure in self protection versus self insurance technology we can examine how incentive may shift between investment in a public good protection and a private good insurance subject to factor such a network size type of attack loss probability loss magnitude and cost of technology we can also characterize nash equilibrium and social optimum for different class of attack and defense in the weakest target game an interesting result is that for almost all parameter setting more effort is exerted at nash equilibrium than at the social optimum we may attribute this to the strategic uncertainty of player seeking to self protect at just slightly above the lowest protection level 
typical web session can be hijacked easily by a network eavesdropper in attack that have come to be designated sidejacking the rise of ubiquitous wireless network often unprotected at the transport layer ha significantly aggravated this problem while ssl can protect against eavesdropping it usability disadvantage often make it unsuitable when the data is not considered highly confidential most web based email service for example use ssl only on their login page and are thus vulnerable to sidejacking we propose sessionlock a simple approach to securing web session against eavesdropping without extending the use of ssl sessionlock is easily implemented by web developer using only javascript and simple server side logic it performance impact is negligible and all major web browser are supported interestingly it is particularly easy to implement on single page ajax web application e g gmail or yahoo mail with approximately line of javascript and line of server side verification code 
we introduce smoothing of retrieval effectiveness score which balance result from prior incomplete query set against limited additional complete information in order to obtain more refined system ordering than would be possible on the new query alone 
in this paper a language model adapted to graph based representation of image content is proposed and assessed the full indexing and retrieval process are evaluated on two different image corpus we show that using the spatial relationship with graph model ha a positive impact on the result of standard language model lm and outperforms the baseline built upon the current state of the art support vector machine svm classification method 
writing and publishing review online ha become an increasingly popular way for people to express opinion and sentiment analyzing the large volume of online review available can produce useful knowledge that are of interest to vendor and other party prior study in the literature have shown that online review have a significant correlation with the sale of product and therefore mining the review could help predict the sale performance of relevant product however those study fail to consider one important factor that may significantly affect the accuracy of the prediction i e the quality of the review in this paper we propose a regression model that explicitly take into account the quality factor and discus how this quality information can be predicted when it is not readily available experimental result on a movie review dataset confirm the effectiveness of the proposed model 
