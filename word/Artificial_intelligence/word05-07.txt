although there ha been much discussion of belief change e g gardenfors spohn goal change ha not received much attention in this paper we propose a method for goal change in the framework of reiter s theory of action in the situation calculus mccarthy and hayes levesque et al and investigate it property we extend the framework developed in shapiro et al and shapiro and lesp erance where goal and goal expansion were modelled but goal contraction wa not 
we present a framework for the debugging of logically contradicting terminology which is based on traditional modelbased diagnosis to study the feasibility of this highly general approach we prototypically implemented the hitting set algorithm presented in reiter and applied it in three different scenario first we use a description logic reasoning system a a black box to determine necessarily maximal conict set then we use our own non optimized dl reasoning engine to produce small and a specialized algorithm to determine minimal conict set in a number of experiment we show that the rst method already fails for relatively small terminology however based on small or minimal conict set we can often calculate diagnosis in reasonable time 
compilability is a measure of how effectively compilation or preprocessing can be applied to knowledge base specified in a particular knowledge representation formalism the aim of compilation is to allow for efficient on line query processing a theory of compilability ha been established for organizing knowledge representation formalism according to a scheme of compilability class and bear strong analogy to the classical theory of complexity which permit the organization of computational problem according to complexity class we develop a novel theory of compilability called parameterized compilability which incorporates the notion of parameterization a used in parameterized complexity and permit for refined analysis of compilability 
smoothing approach to the simultaneous localization and mapping slam problem in robotics are superior to the more common filtering approach in being exact better equipped to deal with non linearity and computing the entire robot trajectory however while filtering algorithm that perform map update in constant time exist no analogous smoothing method is available we aim to rectify this situation by presenting a smoothingbased solution to slam using loopy belief propagation lbp that can perform the trajectory and map update in constant time except when a loop is closed in the environment the slam problem is represented a a gaussian markov random field gmrf over which lbp is performed we prove that lbp in this case is equivalent to gauss seidel relaxation of a linear system the inability to compute marginal covariance efficiently in a smoothing algorithm ha previously been a stumbling block to their widespread use lbp enables the efficient recovery of the marginal covariance albeit approximately of landmark and pose while the final covariance are overconfident the one obtained from a spanning tree of the gmrf are conservative making them useful for data association experiment in simulation and using real data are presented 
we introduce and analyze q potential game and qcongestion game where q is a positive integer a potential congestion game is a potential congestion game we show that a game is a q potential game if and only if it is up to an isomorphism a q congestion game a a corollary we derive the result that every game in strategic form is a qcongestion game for some q i t is further shown that every q congestion game is isomorphic to a qnetwork game where the network environment is defined by a directed graph with one origin and one destination finally we discus our main agenda the issue of representing q congestion game with non negative cost function by congestion model with non negative and monotonic facility cost function we provide some initial result in this regard 
the question of how machine can be endowed with the ability to acquire and robustly manipulate commonsense knowledge is a fundamental scientic problem here we formulate an approach to this problem that we call knowledge infusion we argue that robust logic offer an appropriate semantics for this endeavor because it support provably efcient algorithm for a basic set of necessary learning and reasoning task we observe that multiple concept can be learned simultaneously from a common data set in a data efcient manner we also point out that the preparation of appropriate teaching material for training system constructed according to these principle raise new challenge 
in the field of heuristic search it is well known that improving the quality of an admissible heuristic can significantly decrease the search effort required to find an optimal solution existing literature often assumes that admissible heuristic are consistent implying that consistency is a desirable attribute to the contrary this paper show that an inconsistent heuristic can be preferable to a consistent heuristic theoretical and empirical result show that in many case inconsistency can be used to achieve large performance improvement the conventional wisdom about inconsistent heuristic is wrong 
recently the problem of dimensionality reduction ha received a lot of interest in many field of information processing we consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional euclidean space the most popular manifold learning algorithm include locally linear embedding isomap and laplacian eigenmap however these algorithm are nonlinear and only provide the embedding result of training sample in this paper we propose a novel linear dimensionality reduction algorithm called isometric projection isometric projection construct a weighted data graph where the weight are discrete approximation of the geodesic distance on the data manifold a linear subspace is then obtained by preserving the pairwise distance in this way isometric projection can be defined everywhere comparing to principal component analysis pca which is widely used in data processing our algorithm is more capable of discovering the intrinsic geometrical structure specially pca is optimal only when the data space is linear while our algorithm ha no such assumption and therefore can handle more complex data space experimental result on two real life data set illustrate the effectiveness of the proposed method 
extracting a map from a stream of experience is a key problem in robotics and artificial intelligence in general we propose a technique called subjective mapping that seek to learn a fully specified predictive model or map without the need for expert provided model of the robot s motion and sensor apparatus we briefly overview the recent advancement presented elsewhere icml ijcai and isrr that make this possible examine it significance in relationship to other development in the field and outline open issue that remain to be addressed we propose an approach to mapping which requires no a priori knowledge of model this short paper will outline this approach and the recently developed technique that make it possible a well a examine it significance for the broader artificial intelligence endeavor we first introduce the basic principle of subjective mapping we then briefly discus the recently developed algorithm called action respecting embedding bowling ghodsi wilkinson and it extension wilkinson bowling ghodsi bowling et al which form the crux of the subjective mapping approaching we then discus how subjective mapping relates to other recent advance in artificial intelligence we finally outline what open issue remain to be addressed 
ai ha had notable success in building highperformance game playing program to compete against the best human player however the availability of fast and plentiful machine with large memory and disk creates the possibility of a game this ha been done before for simple or relatively small game in this paper we present new idea and algorithm for solving the game of checker checker is a popular game of skill with a search space of possible position this paper report on our first result one of the most challenging checker opening ha been solved the white doctor opening is a draw solving roughly more opening will result in the game theoretic value of checker being determined 
we introduce a general study of routing mediator a routing mediator can act in a given multi agent encounter on behalf of the agent that give it the right of play routing mediator differ from one another according to the information they may have our study concentrate on the use of routing mediator in order to reach correlated strong equilibrium a multi agent behavior which is stable against deviation by coalition we study the relationship between the power of different routing mediator in establishing correlated strong equilibrium surprisingly our main result show a natural class of routing mediator that allow to implement fair and efficient outcome a a correlated super strong equilibrium in a very wide class of game 
arc consistency play such a key role in constraint programming for solving real life problem that it is almost the only algorithm used for reducing domain there are a few specific problem for which a stronger form of propagation often called shaving is more efficient nevertheless in many case shaving at each node of the search tree is not worth doing arc consistency filtering is much faster and the additional domain reduction inferred by shaving do not pay off in this paper we propose a new kind of shaving called quickshaving which is guided by the search a quickshaving may infer some additional domain reduction compared with arc consistency it can improve the search for a solution by an exponential ratio moreover the advantage of quick shaving is that in practice unlike a standard form of shaving the additional domain reduction deduced by quickshaving come at a very low overhead compared with arc consistency 
online mechanism design considers the problem of sequential decision making in a multi agent system with self interested agent the agent population is dynamic and each agent ha private information about it value for a sequence of decision we introduce a method ironing to transform an algorithm for online stochastic optimization into one that is incentive compatible ironing achieves this by canceling decision that violate a form of monotonicity the approach is applied to the consensus algorithm and experimental result in a resource allocation domain show that not many decision need to be canceled and that the overhead of ironing is manageable 
in many sensing application including environmental monitoring measurement system must cover a large space with only limited sensing resource one approach to achieve required sensing coverage is to use robot to convey sensor within this space planning the motion of these robot coordinating their path in order to maximize the amount of information collected while placing bound on their resource e g path length or energy capacity is anp hard problem in this paper we present an efficient path planning algorithm that coordinate multiple robot each having a resource constraint to maximize the informativeness of their visited location in particular we use a gaussian process to model the underlying phenomenon and use the mutual information between the visited location and remainder of the space to characterize the amount of information collected we provide strong theoretical approximation guarantee for our algorithm by exploiting the submodularity property of mutual information in addition we improve the efficiency of our approach by extending the algorithm using branch and bound and a region based decomposition of the space we provide an extensive empirical analysis of our algorithm comparing with existing heuristic on datasets from several real world sensing application 
a density based clustering algorithm called outclust is presented the algorithm exploit a notion of local density in order to find homogeneous group of object a opposite to object mostly deviating from the overall population the proposed algorithm try to simultaneously consider several feature of real data set namely finding cluster of different shape and density in high dimensional data in presence of noise it is shown that the method is able to identify very meaningful cluster and experimental comparison with partitioning hierarchial and density based clustering algorithm is presented pointing out that the algorithm achieves good clustering quality 
abstract although many paper about belief update have been written it precise scope still remains unclear in this paper we aim at identifying this scope and we show that belief update is a specific case of feedback free action progression this strong connection with the field of reasoning about action lead u to reconsider belief update and investigate new issue especially reverse update which is to regression what update is to progression 
symmetry is an important factor in solving many constraint satisfaction problem one common type of symmetry is when we have symmetric value in a recent series of paper we have studied method to break value symmetry walsh a walsh our result identify computational limit on eliminating value symmetry for instance we prove that pruning all symmetric value is np hard in general nevertheless experiment show that much value symmetry can be broken in practice these result may be useful to researcher in planning scheduling and other area a value symmetry occurs in many different domain 
imitation learning also called learning by watching or programming by demonstration ha emerged a a mean of accelerating many reinforcement learning task previous work ha shown the value of imitation in domain where a single mentor demonstrates execution of a known optimal policy for the benefit of a learning agent we consider the more general scenario of learning from mentor who are themselves agent seeking to maximize their own reward we propose a new algorithm based on the concept of transferable utility for ensuring that an observer agent can learn efficiently in the context of a selfish not necessarily helpful mentor we also address the question of when an imitative agent should request help from a mentor and when the mentor can be expected to acknowledge a request for help in analogy with other type of active learning we call the proposed approach active imitation learning 
most previous study on active learning focused on the problem of model selection i e how to identify the optimal classification model from a family of predefined model using a small carefully selected training set in this paper we address the problem of active algorithm selection the goal of this problem is to efficiently identify the optimal learning algorithm for a given dataset from a set of algorithm using a small training set in this study we present a general framework for active algorithm selection by extending the idea of the hedge algorithm it employ the worst case analysis to identify the example that can effectively increase the weighted loss function defined in the hedge algorithm we further extend the framework by incorporating the correlation information among unlabeled example to accurately estimate the change in the weighted loss function and maximum entropy discrimination to automatically determine the combination weight used by the hedge algorithm our empirical study with the datasets of wcci performance prediction challenge show promising performance of the proposed framework for active algorithm selection 
time series data abounds in real world problem measuring the similarity of time series is a key to solving these problem one state of the art measure is the longest common subsequence t his measure advocate using the length of the longest common subsequence a an indication of similarity between sequence but ignores information contained in the second third longest subsequence in order to capture the common information in sequence maximally we propose a novel measure of sequence similarity the number of all common subsequence we show that this measure satisfies the common property of similarity function calculating this measure is not trivial a a brute force approach is exponential in time we present a novel dynamic programming algorithm to calculate this number in polynomial time we also suggest a different way of extending a class of such measure to multidimensional real valued time series in the spirit of probabilistic metric space we conducted an experimental study on the new similarity measure and the extension method for classification it wa found that both the new similarity and the extension method are consistently competitive 
we study the problem of evaluating the goodness of a kernel matrix for a classification task a kernel matrix evaluation is usually used in other expensive procedure like feature and model selection the goodness measure must be calculated efficiently most previous approach are not efficient except for kernel target alignment kta that can be calculated in o n time complexity although kta is widely used we show that it ha some serious drawback we propose an efficient surrogate measure to evaluate the goodness of a kernel matrix based on the data distribution of class in the feature space the measure not only overcomes the limitation of kta but also posse other property like invariance efficiency and error bound guarantee comparative experiment show that the measure is a good indication of the goodness of a kernel matrix 
in this paper we focus on extending the expressive power of constraint based temporal reasoning formalism we begin with the well known simple temporal problem with uncertainty and incorporate three extension prior observability in which the value of uncontrollable event become known prior to their actual occurrence partial shrinkage in which an observation event trigger the reduction of a contingent temporal interval and a generalization of partial shrinkage to requirement link making it possible to express certain type of uncertainty that may arise even when the time point in a problem are themselves fully controllable we describe level of controllability in the resulting formalism the generalized stpu and relate this formalism to related development in disjunctive temporal reasoning throughout we motivate our approach with simple real world example that illustrate the limitation of existing formalism and the flexibility of our proposed extension 
in this paper we present a general scheme to create mechanism that approximate the social welfare in the presence of selfish but rational behavior of agent the usual approach is to design truthful mechanism in which an agent can only lose by impersonating a another agent in contrast our approach is to allow an agent to impersonate several different agent we design the mechanism such that only a limited set of impersonation are reasonable to rational agent our mechanism make sure that for any choice of such impersonation by the agent an approximation to the social welfare is achieved we demonstrate our result on the well studied domain of combinatorial auction ca our mechanism are algorithmic implementation a notion recently suggested in babaioff lavi pavlov 
successful negotiator look beyond a purely utilitarian view we propose a new agent architecture that integrates the utilitarian information and semantic view allowing the definition of strategy that take these three dimension into account information based agency value the information in dialogue in the context of a communication language based on a structured ontology and on the notion of commitment this abstraction unifies measure such a trust reputation and reliability in a single framework 
we study the effect of feature selection and human feedback on feature in active learning setting our experiment on a variety of text categorization task indicate that there is significant potential in improving classifier performance by feature reweighting beyond that achieved via selective sampling alone standard active learning if we have access to an oracle that can point to the important most predictive feature consistent with previous finding we find that feature selection based on the labeled training set ha little effect but our experiment on human subject indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant feature furthermore these experiment show that feature labeling take much le about th time than document labeling we propose an algorithm that interleaf labeling feature and document which significantly accelerates active learning 
mechanism design ha traditionally focused almost exclusively on the design of truthful mechanism there are several drawback to this in certain setting e g voting setting no desirable strategyproof mechanism exist truthful mechanism are unable to take advantage of the fact that computationally bounded agent may not be able to find the best manipulation and when designing mechanism automatically this approach lead to constrained optimization problem for which current technique do not scale to very large instance in this paper we suggest an entirely different approach we start with a na ive manipulable mechanism and incrementally make it more strategyproof over a sequence of iteration we give example of mechanism that variant of our approach generate including the vcg mechanism in general setting with payment and the plurality with runoff voting rule we also provide several basic algorithm for automatically executing our approach in general setting finally we discus how computationally hard it is for agent to find any remaining beneficial manipulation 
we identify some weak point of the lrta k algorithm in the propagation of heuristic change to solve them we present a new algorithm lrta l k that is based on the selection and updating of the interior state of a local space around the current state it keep the good theoretical property of lrta k while improving substantially it performance it is related with a lookahead depth greater than we provide experimental evidence of the benefit of the new algorithm on real time benchmark with respect to existing approach 
possibilistic network and possibilistic logic are two standard framework of interest for representing uncertain piece of knowledge possibilistic network exhibit relationship between variable while possibilistic logic rank logical formula according to their level of certainty for multiply connected network it is well known that the inference process is a hard problem this paper study a new representation of possibilistic network called hybrid possibilistic network it result from combining the two semantically equivalent type of standard representation we first present a propagation algorithm through hybrid possibilistic network this inference algorithm on hybrid network is strictly more efficient and confirmed by experimental study than the one of standard propagation algorithm 
the conditional independence assumption of naive bayes essentially ignores attribute dependency and is often violated on the other hand although a bayesian network can represent arbitrary attribute dependency learning an optimal bayesian network from data is intractable the main reason is that learning the optimal structure of a bayesian network is extremely time consuming thus a bayesian model without structure learning is desirable in this paper we propose a novel model called hidden naive bayes hnb in an hnb a hidden parent is created for each attribute which combine the influence from all other attribute we present an approach to creating hidden parent using the average of weighted one dependence estimator hnb inherits the structural simplicity of naive bayes and can be easily learned without structure learning we propose an algorithm for learning hnb based on conditional mutual information we experimentally test hnb in term of classification accuracy using the uci data set recommended by weka witten frank and compare it to naive bayes langley iba thomas c quinlan sbc langley sage nbtree kohavi cl tan friedman geiger goldszmidt and aode webb boughton wang the experimental result show that hnb outperforms naive bayes c sbc nbtree and cl tan and is competitive with aode 
in this paper we argue that it may be possible to help searcher to better understand the relevance of search result by generating explanation that highlight how other user have interacted with such result under similar search condition in the past we propose the use of the search history of a community of online user a a source of these explanation we describe the result of a recent study to examine the use of such explanation based technique to help web searcher better appreciate the relevancy of search result we highlight shortcoming of this approach in it current form and offer suggestion a to how it may be improved in future work 
there are numerous application where we need to ensure that multiple moving object are sufficiently far apart furthermore in many moving object domain there is positional indeterminacy we are not sure exactly when a given moving object will be at a given location yaman et al provided a logic of motion but did not provide algorithm to ensure that moving object are kept sufficiently far apart in this paper we extend their logic to include a far predicate we develop the checkfar algorithm that check if any given two object will always be sufficiently far apart at during a time interval we have run a set of experiment showing that our checkfar algorithm scale very well 
this paper present a self supervised algorithm for learning perceptual structure based upon correlation in different sensory modality the brain and cognitive science have gathered an enormous body of neurological and phenomenological evidence in the past half century that demonstrates the extraordinary degree of interaction between sensory modality during the course of ordinary perception this paper present a new framework for creating artificial perceptual system inspired by these finding where the primary architectural motif is the cross modal transmission of perceptual information to enhance each sensory channel individually the basic hypothesis underlying this approach is that the world ha regularity natural law tend to correlate physical property and biological perceptory system have evolved to take advantage of this they share information continually and opportunistically across seemingly disparate perceptual channel not epiphenomenologically but rather a a fundamental component of normal perception it is therefore essential that their artificial counterpart be able to share information synergistically within their perceptual channel if they are to approach degree of biological sophistication this paper is a preliminary step in that direction 
various task in decision making and decision support require selecting a preferred subset of item from a given set of item recent work in this area considered method for specifying such preference based on the attribute value of individual element within the set of these the approach of brafman et al appears to be the most general in this paper we consider the problem of computing an optimal subset given such a specification the problem is shown to be np hard in the general case necessitating heuristic search method we consider two algorithm class for this problem direct set construction and implicit enumeration a solution to appropriate csps new algorithm are presented in each class and compared empirically against previous result 
in this paper we introduce a multi context variant of reiter s default logic the logic provides a syntactical counterpart of roelofsen and serafini s information chain approach ijcai yet ha several advantage it is closer to standard way of representing nonmonotonic inference and a number of result from that area come for free it is closer to implementation in particular the restriction to logic programming give u a computationally attractive framework and it allows u to handle a problem with the information chain approach related to skeptical reasoning 
markov model have been a keystone in artificial intelligence for many decade however they remain unsatisfactory when the environment modelled is partially observable there are pathological example where no history of fixed length is sufficient for accurate prediction or decision making on the other hand working with a hidden state like in hidden markov model or partially observable markov decision process ha a high computational cost in order to circumvent this problem we suggest the use of a context based model our approach replaces strict transition probability by influence on transition the method proposed provides a trade off between a fully and partially observable model we also discus the capacity of our framework to model hierarchical knowledge and abstraction simple example are given in order to show the advantage of the algorithm 
a general n ary constraint is usually represented explicitly a a set of it solution tuples which may need exponential space in this paper we introduce a new representation for general n ary constraint called constrained decision diagram cdd cdd generalizes bdd style representation and the main feature is that it combine constraint reasoning consistency technique with a compact data structure we present an application of cdd for recording all solution of a conjunction of constraint instead of an explicit representation we can implicitly encode the solution by mean of constraint propagation our experiment confirm the scalability and demonstrate that cdds can drastically reduce the space needed over explicit and zbdd representation 
consistency are property of constraint network cns that can be exploited in order to make inference when a signicant amount of such inference can be performed cns are much easier to solve in this paper we interest ourselves in relation ltering consistency for binary constraint i e consistency that allow to identify inconsistent pair of value we propose a new consistency called dual consistency dc and relate it to path consistency pc we show that conservative dc cdc i e dc with only relation associated with the constraint of the network considered is more powerful in term of ltering than conservative pc cpc following the approach of mac gregor we introduce an algorithm to establish strong cdc with a very low worst case space complexity even if the relative efcienc y of the algorithm introduced to establish strong cdc partly depends on the density of the constraint graph the experiment we have conducted show that on many series of csp instance cdc is largely faster than cpc up to more than one order of magnitude besides we have observed that enforcing cdc in a preprocessing stage can signicantly speed up the resolution of hard structured instance 
animated text is an appealing field of creative graphical design manually designed text animation is largely employed in advertising movie title and web page in this paper we propose to link through state of the art nlp technique the affective content detection of a piece of text to the animation of the word in the text itself this methodology allows u to automatically generate affective text animation and open some new perspective for advertising internet application and intelligent interface an actor read a script he read those word with the intention of transforming cold print into living speech vocal inflection tone of voice gesture and facial expression are all part of the actor s contribution to the play with the body s subtle vibration and frequency he express the hidden emotional meaning we can say that through his interpretation he brings the script to life in this paper we will show that through automatic detection of the affective meaning of text using state of the art nlp technique we can consequently animate the word that compose them in automated text animation the text itself is capable of augmenting it expressivity and of moving in an 
deictic representation is a representational paradigm based on selective attention and pointer that allows an agent to learn and reason about rich complex environment in this article we present a hierarchical reinforcement learning framework that employ aspect of deictic representation we also present a bayesian algorithm for learning the correct representation for a given sub problem and empirically validate it on a complex game environment 
this paper is concerned with the interaction between word sense disambiguation and the interpretation of noun compound nc in english we develop technique for disambiguating word sense specifically in nc and then investigate whether word sense information can aid in the semantic relation interpretation of nc to disambiguate word sense we combine the one sense per collocation heuristic with the grammatical role of polysemous noun and analysis of word sense combinatorics we built supervised and unsupervised classifier for the task and demonstrate that the supervised method are superior to a number of baseline and also a benchmark state of the art wsd system finally we show that wsd can significantly improve the accuracy of nc interpretation 
in this paper we propose the directed graph embedding dge method that embeds vertex on a directed graph into a vector space by considering the link structure of graph the basic idea is to preserve the locality property of vertex on a directed graph in the embedded space we use the transition probability together with the stationary distribution of markov random walk to measure such locality property it turn out that by exploring the directed link of the graph using random walk we can get an optimal embedding on the vector space that preserve the local affinity which is inherent in the directed graph experiment on both synthetic data and real world web page data are considered the application of our method to web page classification problem get a significant improvement comparing with state of art method 
automatically discovering semantic relation between ontology is an important task with respect to overcoming semantic heterogeneity on the semantic web existing ontology matching system however often produce erroneous mapping in this paper we address the problem of error in mapping by proposing a completely automatic debugging method for ontology mapping the method us logical reasoning to discover and repair logical inconsistency caused by erroneous mapping we describe the debugging method and report experiment on mapping submitted to the ontology alignment evaluation challenge that show that the proposed method actually improves mapping created by different matching system without any human intervention 
supervised machine learning technique developed in the probably approximately correct maximum a posteriori and structural risk minimiziation framework typically make the assumption that the test data a learner is applied to is drawn from the same distribution a the training data in various prominent application of learning technique from robotics to medical diagnosisto processcontrol this assumptionis violated we consider a novel framework where a learner may influence the test distribution in a bounded way from this framework we derive an efficient algorithm that act a a wrapper around a broad class of existing supervised learning algorithm while guarranteeing more robustbehavior under change inthe input distribution 
a formula in first order logic can be viewed a a tree with a logical connective at each node and a knowledge base can be viewed a a tree whose root is a conjunction markov logic richardson and domingo make this conjunction probabilistic a well a the universal quantifier directly under it but the rest of the tree remains purely logical this cause an asymmetry in the treatment of conjunction and disjunction and of universal and existential quantifier we propose to overcome this by allowing the feature of markov logic network mlns to be nested mlns we call this representation recursive random field rrfs rrfs can represent many first order distribution exponentially more compactly than mlns we perform inference in rrfs using mcmc and icm and weight learning using a form of backpropagation weight learning in rrfs is more powerful than structure learning in mlns applied to first order knowledge base it provides a very flexible form of theory revision we evaluate rrfs on the problem of probabilistic integrity constraint in database and obtain promising result for example an mln with the formula r x s x can treat world that violate both r x and s x a le probable than world that only violate one since an mln act a a soft conjunction the grounding of r x and s x simply appear a distinct formula mlns convert the knowledge base to cnf before performing learning or inference this is not possible for the disjunction r x s x no distinction is made between satisfying both r x and s x and satisfying just one since a universally quantified formula is effectively a conjunction over all it grounding while an existentially quantified formula is a disjunction over them this lead to the two quantifier being handled differently this asymmetry can be avoided by softening disjunction and existential quantification in the same way that markov logic softens conjunction and universal quantification the result is a representation where mlns can have nested mlns a feature we call these recursive markov logic network or recursive random field rrfs for short 
caching symmetry and search with decomposition are powerful technique for pruning the search space of constraint problem in this paper we present an innovative way of efficiently combining these technique with branch and bound for solving certain type of constraint optimization problem cop our new method significantly reduces the overhead of performing decomposition during search when dynamic variable ordering are employed in addition it support the exploitation of dynamic symmetry that appear only during search symmetry have not previously been combined with decomposition finally we achieve a superior integration of decomposition and caching with branch and bound than previous approach we test our method on the maximum density still life problem and show that each of our idea yield a significant gain in search performance 
this paper present a model sharedactivity for collaborative agent acting in a group the model suggests mental state for agent with different level of cooperation and permit the formation of group in which member increase individual benefit unlike previous model the model cover group member behavior where group member do not have a joint goal but act collaboratively the model defines key component of a collaborative activity and provides a platform for supporting such activity we studied the behavior of the model in a simulation environment result show how the benefit attained by cooperation is influenced by the complexity of the environment the number of group member and the social dependency between the member the result demonstrate that the model cover social behavior both in setting previously addressed a well a in novel setting 
a situated conversational agent sca is an agent that engages in dialog about the context within which it is embedded situated dialog is characterized by it deep connection to the embedding context and the precise cross timing of linguistic and non linguistic action this paper describes initial research into the construction of an sca that engages in dialog about collaborative physical task in which agent engage in dialog with the joint goal of manipulating the physical context in some manner constructing an sca that can interact naturally in such task requires an agent with the ability to interleave planning action and observation while operating in a partially observable environment consequently i propose to model an sca a a partially observable markov decision process pomdp 
stability against potential deviation by set of agent is a most desired property in the design and analysis of multi agent system however unfortunately this property is typically not satisfied in game theoretic term a strong equilibrium which is a strategy profile immune to deviation by coalition rarely exists this paper suggests the use of mediator in order to enrich the set of situation where we can obtain stability against deviation by coalition a mediator is defined to be a reliable entity which can ask the agent for the right to play on their behalf and is guaranteed to behave in a pre specified way based on message received from the agent however a mediator cannot enforce behavior that is agent can play in the game directly without the mediator s help a mediator generates a new game for the player the mediated game we prove some general result about mediator and mainly concentrate on the notion of strong mediated equilibrium which is just a strong equilibrium at the mediated game we show that desired behavior which are stable against deviation by coalition can be obtained using mediator in several class of setting 
memoization is a fundamental technique in computer science providing the basis for dynamic programming this paper explores using memoization to improve the performance of rejection sampling algorithm it is shown that reusing value produced in previous sample and stored in a cache is beneficial the paper go on to explore the idea of recursive memoization in which value are aggressively reused from the cache even in the process of computing a value to store in the cache this lead to the value in the cache becoming dependent on each other and therefore produce a biased sampler however in practice this seems to be quite beneficial furthermore we show that the error in the cache tends to zero in the long run we demonstrate the usefulness of memoized sampling in a duplicate bridge simulation and in experiment with probabilistic grammar 
symmetry breaking ha been shown to be an important method to speed up the search in constraint satisfaction problem that contain symmetry when breaking symmetry by dominance detection a computationally efficient symmetry breaking scheme can be achieved if we can solve the dominance detection problem in polynomial time we study the complexity of dominance detection when value and variable symmetry appear simultaneously in constraint satisfaction problem csps with single valued variable and set csps we devise an efficient dominance detection algorithm for csps with single valued variable that yield symmetry free search tree and that is based on the abstraction to the actual intuitive structure of a symmetric csp 
we describe a new sequential learning scheme called stacked sequential learning stacked sequential learning is a meta learning algorithm in which an arbitrary base learner is augmented so a make it aware of the label of nearby example we evaluate the method on several sequential partitioning problem which are characterized by long run of identical label we demonstrate that on these problem sequential stacking consistently improves the performance of non sequential base learner that sequential stacking often improves performance of learner such a crfs that are designed specifically for sequential task and that a sequentially stacked maximum entropy learner generally outperforms crfs 
we introduce a flexible framework to specify problem solution outcome and preference among them the proposal combine idea from answer set programming asp answer set optimization aso and cp net the problem domain is structured into component asp technique are used to specify value of component a well a global intercomponent constraint among these value aso method are used to describe preference among the value of a component and cp net technique to represent inter component dependency and corresponding preference 
my thesis will contribute to the field of constraint processing and multiagent system by proposing novel scheme for achieving privacy in constraint optimization a well a new way of analyzing and understanding the privacy property of constraint optimization algorithm motivation it is my belief that there are many domain within constraint processing that motivate the need for more private algorithm and that this work will be broadly applicable however i would like to present two particular salient example in the following paragraph one domain that naturally motivates privacy preserving constraint optimization algorithm is scheduling in this domain a number of agent including say sally in human resource wish to schedule event subject to their individual constraint they wish to keep these constraint private sally s calendar for one contains the intersection of multiple professional endeavor a well a item relevant to her personal life in this case the agent involved including sally want a solution that will optimize the global social welfare but that doesn t mean they want everyone to know how sally s preference for attending cancer support group meeting led to that result 
there are numerous case where we need to reason about vehicle whose intention and itinerary are not known in advance to u for example coast guard agent tracking boat don t always know where they are headed likewise in drug enforcement application it is not always clear where drug carrying airplane which do often show up on radar are headed and how legitimate plane with an approved flight manifest can avoid them likewise traffic planner may want to understand how many vehicle will be on a given road at a given time past work on reasoning about vehicle such a the logic of motion by yaman et al yaman et al only deal with vehicle whose plan are known in advance and don t capture such situation in this paper we develop a formal probabilistic extension of their work and show that it capture both vehicle whose itinerary are known and those whose itinerary are not known we show how to correctly answer certain query against a set of statement about such vehicle a prototype implementation show our system to work efficiently in practice 
research on preference elicitation and reasoning typically focus on preference over single object of interest however in a number of application the outcome of interest are set of such atomic object for instance when creating the program for a film festival editing a newspaper or putting together a team we need to select a set of film resp article member that is optimal with respect to quality diversity cohesiveness etc this paper describes an intuitive approach for specifying preference over set of object an algorithm for computing an optimal subset given a set of candidate object and a preference specification is developed and evaluated 
a modal logic is any logic for handling modality concept like possibility necessity and knowledge artificial int elligence us modal logic most heavily to represent and reason about knowledge of agent about others knowledge this type of reasoning occurs in dialog collaboration and competition in many application it is also important to be able t o reason about the probability of belief and event in this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge we also present exact and approximate algorithm for reasoning about the truth value of query that are encoded a probabilistic modal logic formula we provide an exact algorithm which take a probabilistic kripke structure and answer probabilistic modal query in polynomial time in the size of the model then we introduce an approximate method for application in which we have very many or infinitely many state exact method are impractical in these application and we show that our method return a close estimate efficiently over multinomial distribution blei ng jordan but are not granular enough for multiple level of complex belief about belief furthermore reasoning with these r epresentations is computationally hard because they mix structure continuous variable and discrete variable neces sary for distribution over distribution in this paper we address the need for granular representation of probabilistic belief about probabilistic belief we develop and present a representation and reasoning algorithm for nested probabilistic modality we describe syntax semantics and tractable algorithm for reasoning wit h our representation our reasoning algorithm evaluate the value of a query on a state given a model on the way to these contribution we provide a theory for probabilistic modal logic we introduce a framework for modeling probabilistic knowledge that is based on modal logic and possible world semantics with a probability distribution over the accessibility relation we introduce t wo exact method top down and bottom up that we can choose from based on the property of our application we show that when the number of nested modal function is small our top down method work faster whereas when we have complex nesting the bottom up approach which is polynomial time in the number of state is faster for very large even infinite model neither of these method is tractable therefore we use sampling to estimate the truth value of the query in our approximation method we reduce our problem to inference in bayesian network so all the exact and variational method applicable to bayesian network can be used most previous related work are limited to combining probability with a special case of modal logic in which accessibility relation are equivalence relation we call t his special case probabilistic knowledge among those fagin halpern heifetz mongin are mainly concerned with providing a sound and complete axiomatization for the logic of knowledge and probability in contrast we focus on providing tractable reasoning method to answer query on one model for the general probabilistic modal logic a well a the special case of probabilistic knowledg e another work related to ours is milch koller in which probabilistic epistemic logic is used to reason about the mental state of an agent this logic is a special case of probabilistic knowledge with the additional assumption of 
in this work we define a new framework in order to improve the knowledge representation power of answer set programming paradigm our proposal is to use notion from possibility theory to extend the stable model semantics by taking into account a certainty level expressed in term of necessity measure on each rule of a normal logic program first of all we introduce possibilistic definite logic program and show how to compute the conclusion of such program both in syntactic and semantic way the syntactic handling is done by help of a fix point operator the semantic part relies on a possibility distribution on all set of atom and we show that the two approach are equivalent in a second part we define what is a possibilistic stable model for a normal logic program with default negation again we define a possibility distribution allowing to determine the stable model 
auction are a class of multi party negotiation protocol classical auction try to maximize social welfare by select ing the highest bidder a the winner if bidder are rational this ensures that the sum of profit for all bidder and the seller is maximized in all such auction however only the winner and the seller make any profit we believe that so cial welfare distribution is a desired goal of any multi party protocol in the context of auction this goal translates into a rather radical proposal of profit sharing between all bidder and the seller we propose a profit sharing auction psa where a part of the selling price paid by the winner is paid back to the bidder the obvious criticism of this mecha nism is the incentive for the seller to share it profit with non winning bidder we claim that this loss can be compensated by attracting more bidder to such an auction resulting in an associated increase in selling price we run several set of experiment where equivalent item are concurrently sold at a first price sealed bid a vickrey and a psa auction a population of learning bidder repeatedly choose to go to one of these auction based on their valuation for the good being auctioned and their learned estimate of profit from these auction result show that seller make more or equivalent profit by using psa a compared to the classical auction additionally psa always attracts more bidder which might create auxiliary revenue stream and a desirable lower vari ability in selling price interestingly then a rational seller ha the incentive to share profit and offer an auction like psa which maximizes and distributes social welfare 
unlike the case for sequential and conditional planning much of the work on iterative planning planning where loop may be needed lean heavily on theorem proving this paper doe the following it proposes a different approach where generating plan is decoupled from verifying them describes the implementation of an iterative planner based on the situation calculus present a few example illustrating the sort of plan that can be generated show some of the strength and weakness of the approach and finally sketch the beginning of a theory where validation of plan is done offline 
we add a limited but useful form of quantification to coalition logic a popular formalism for reasoning about cooperation in game like multi agent system the basic construct of quantified coalition logic qcl allow u to express property a there exists a coalition c satisfying property p such that c can achieve we give an axiomatization of qcl and show that while it is no more expressive than coalition logic it is exponentially more succinct the time complexity of qcl model checking for symbolic and explicit state representation is shown to be no worse than that of coalition logic we illustrate the formalism by showing how to succinctly specify such social choice mechanism a majority voting which in coalition logic require specification that are exponentially long in the number of agent 
a new algorithm neighborhood minmax projection nmmp is proposed for supervised dimensionality reduction in this paper the algorithm aim at learning a linear transformation and focus only on the pairwise point where the two point are neighbor of each other after the transformation the considered pairwise point within the same class are a close a possible while those between different class are a far a possible we formulate this problem a a constrained optimization problem in which the global optimum can be effectively and efficiently obtained compared with the popular supervised method linear discriminant analysis lda our method ha three significant advantage first it is able to extract more discriminative feature second it can deal with the case where the class distribution are more complex than gaussian third the singularity problem existing in lda doe not occur naturally the performance on several data set demonstrates the effectiveness of the proposed method 
we present a new approach to ensemble classification that requires learning only a single base classifier the idea is to learn a classifier that simultaneously predicts pair of test label a opposed to learning multiple predictor for single test label then coordinating the assignment of individual label by propagating belief on a graph over the data we argue that the approach is statistically well motivated even for independent identically distributed iid data in fact we present experimental result that show improvement in classification accuracy over single example classifier across a range of iid data set and over a set of base classifier like boosting the technique increase representational capacity while controlling variance through a principled form of classifier combination 
logical filtering is the problem of tracking the possible state of a world belief state after a sequence of action and observation it is fundamental to application in partially observable dynamic domain this paper present the first exact logical filtering algorithm that is tractable for all deterministic domain our tractability result is interesting because it contrast sharply with intractability result for structured stochastic domain the key to this advance lie in using logical circuit to represent belief state we prove that both filtering time and representation size are linear in the sequence length and the input size they are independent of the domain size if the action have compact representation the number of variable in the resulting formula is at most the number of state feature we also report on a reasoning algorithm answering propositional question for our circuit which can handle question about past time step smoothing we evaluate our algorithm extensively on aiplanning domain our method outperforms competing method sometimes by order of magnitude 
restart strategy are commonly used for minimizing the computational cost of randomized algorithm but require prior knowledge of the run time distribution in order to be effective we propose a portfolio of two strategy one fixed with a provable bound on performance the other based on a model of run time distribution updated a the two strategy are run on a sequence of problem computational resource are allocated probabilistically to the two strategy based on their performance using a well known k armed bandit problem solver we present bound on the performance of the resulting technique and experiment with a satisfiability problem solver showing rapid convergence to a near optimal execution time 
in this paper we address the problem of deriving sensible information from a collection of argumentation system coming from different agent a general framework for merging argumentation system from dung s theory of argumentation is presented each argumentation system give both a set of argument and the way they interact i e attack or non attack according to the corresponding agent the aim is to define the argument system or the set of argument system that best represents the group our framework is general enough to handle the case when agent do not share the same set of argument merging argumentation system is shown a a valuable approach for defining set of argument acceptable by the group 
binary voting tree provide a succinct representation for a large and prominent class of voting rule in this paper we investigate the pac learnability of this class of rule we show that while in general a learning algorithm would require an exponential number of sample if the number of leaf is polynomial in the size of the set of alternative then a polynomial training set suffices we apply these result in an emerging theory automated design of voting rule by learning 
agent must form and update mental model about each other in a wide range of domain team coordination plan recognition social simulation user modeling game of incomplete information etc existing research typically treat the problem of forming belief about other agent a an isolated subproblem where the modeling agent start from an initial set of possible model for another agent and then maintains a belief about which of those model applies this initial set of model is typically a full specification of possible agent type although such a rich space give the modeling agent high accuracy in it belief it will also incur high cost in maintaining those belief in this paper we demonstrate that by taking this modeling problem out of it isolation and placing it back within the overall decision making context the modeling agent can drastically reduce this rich model space without sacrificing any performance our approach comprises three method the first method cluster model that lead to the same behavior in the modeling agent s decision making context the second method cluster model that may produce different behavior but produce equally preferred outcome with respect to the utility of the modeling agent the third technique sacrifice a fixed amount of accuracy by clustering model that lead to performance loss that are below a certain threshold we illustrate our framework using a social simulation domain and demonstrate it value by showing the minimal mental model space that it generates 
this paper describes work on the detection of anomalous material in text we show several variant of an automatic technique for identifying an unusual segment within a document and consider text which are unusual because of author genre biber topic or emotional tone we evaluate the technique using many experiment over large document collection created to contain randomly inserted anomalous segment in order to successfully identify anomaly in text we define more than stylistic feature to characterize writing some of which are well established stylistic determiner but many of which are novel using these feature with each of our method we examine the effect of segment size on our ability to detect anomaly allowing segment of size word word and word we show substantial improvement over a baseline in all case for all method and identify the method variant which performs consistently better than others 
knowledge worker spend the majority of their working hour processing and manipulating information these user face continual cost a they switch between task to retrieve and create information the tasktracer project at oregon state university investigates the possibility of a desktop software system that will record in detail how knowledge worker complete task and intelligently leverage that information to increase efficiency and productivity our approach assigns each observed user interface action to a task for which it is likely being performed in this demonstration we show how we have applied machine learning in this environment 
temporal reasoning is widely used within both computer science and a i however the underlying complexity of temporal proof in discrete temporal logic ha led to the use of simplified formalism and technique such a temporal interval algebra or model checking in this paper we show that tractable sub class of propositional linear temporal logic can be developed based on the use of xor fragment of the logic we not only show that such fragment can be decided tractably via clausal temporal resolution but also show the benefit of combining multiple xor fragment for such combination we establish completeness and complexity of the resolution method and also describe how such a temporal language might be used in application area for example the verification of multi agent system this new approach to temporal reasoning provides a framework in which tractable temporal logic can be engineered by intelligently combining appropriate xor fragment 
an ontology consists of a set of concept a set of constraint imposing on instance of concept and the subsumption relation it is assumed that an ontology is a tree under the subsumption relation between concept to preserve structural property of ontology the ontology revision is not only contracting ontology by discarding statement inconsistent with a revising statement but also extracting statement consistent with the revising statement and adding some other statement in the ontology revision the consistency of a revising statement with the theory of the logical closure of the ontology under the closed world assumption is discussed the basic postulate of the ontology revision are proposed and a concrete ontology revision is given based on the consistence or inconsistence of an ontology and a revising statement 
over the past few year some embedding method have been proposed for feature extraction and dimensionality reduction in various machine learning and pattern classification task among the method proposed are neighborhood preserving embedding npe locality preserving projection lpp and local discriminant embedding lde which have been used in such application a face recognition and image video retrieval however although the data in these application are more naturally represented a higher order tensor the embedding method can only work with vectorized data representation which may not capture well some useful information in the original data moreover highdimensional vectorized representation also suffer from the curse of dimensionality and the high computational demand in this paper we propose some novel tensor embedding method which unlike previous method take data directly in the form of tensor of arbitrary order a input these method allow the relationship between dimension of a tensor representation to be efficiently characterized moreover they also allow the intrinsic local geometric and topological property of the manifold embedded in a tensor space to be naturally estimated furthermore they do not suffer from the curse of dimensionality and the high computational demand we demonstrate the effectiveness of the proposed tensor embedding method on a face recognition application and compare them with some previous method extensive experiment show that our method are not only more effective but also more efficient 
in the research to date the performance of recommender system ha been extensively evaluated across various dimension increasingly the issue of robustness against malicious attack is receiving attention from the research community in previous work we have shown that knowledge of certain domain statistic is sufficient to allow successful attack to be mounted against recommender system in this paper we examine the extent of domain knowledge that is actually required and find that even when little such knowledge is known it remains possible to mount successful attack 
representing and reasoning about knowledge is critical in artificial intelligence there is a distinction between factual and ontological knowledge factual knowledge represents a set of fact about individual object that are known or believed whereas ontological background knowledge represents implicit concept and relationship that are assumed to exist in the world ontological knowledge is often represented a a hierarchy of concept because splitting thing of the real world into category and sub category is a natural way of human thinking one example of conceptual hierarchy in ai is ontology that are widely used in such area a natural language processing semantic web etc representation of both type of knowledge becomes difficult when the knowledge is imprecise one example of imprecision is granularity i e inability to distinguish between the individual object in this case knowledge cannot be represented precisely but can be approximated with respect to the granularity of the domain approximation of factual knowledge ha been extensively researched and often employ rough set theory pawlak for dealing with indiscernibility of object similar approach ha been applied to ontology to approximate concept in the hierarchy doherty et al the open problem is the approximation of hierarchical relationship such a is a part of between concept this paper address this issue using rough mereology polkowski skowron complemented with interval analysis moore the principal contribution is to provide rough mereological function that can be used for representation and reasoning with formal ontology in approximation space specifically approximate concept membership and approximate concept subsumption function will be provided it can be demonstrated that the interval based function are free of the shortcoming of the previously suggested definition cao sui zhang klinov mazlack 
web search is a challenging task previous research mainly exploit text on the web page or link information between the page while multimedia information is largely ignored this paper proposes a new framework for web search which exploit image content to help improve the search performance in this framework candidate image are retrieved at first by considering their associated text information then image related to the query are identified by analyzing the density of the visual feature space after that an image based rank of the web page is generated which is combined with the traditional keyword based search result to produce the final search result experiment demonstrate the promise of the proposed framework 
to facilitate interactive design the solution to configuration problem can be compiled into a decision diagram we develop three heuristic for reducing the time and space required to do this these heuristic are based on the distinctive clustered and hierarchical structure of the constraint graph of configuration problem the first heuristic attempt to limit the growth in the size of the decision diagram by providing an order in which constraint are added to the decision diagram the second heuristic provides an initial order for the variable within the decision diagram finally the third heuristic group variable together so that they can be reordered by a dynamic variable reordering procedure used during the construction of the decision diagram these heuristic provide one to two order magnitude improvement in the time to compile a wide range of configuration 
this paper describes a simple complete search for cumulative scheduling based on the detection and resolution of minimal critical set mc the heuristic for selecting mc relies on an estimation of the related reduction of the search space an extension of the search procedure using self adapting shaving is proposed the approach wa implemented on top of classical constraint propagation algorithm and tested on resource constrained project scheduling problem rcpsp we were able to close more than of the previously open problem of the psplib kolisch and sprecher and improve more than of the best known lower bound on those heavily studied problem other new result on open shop and cumulative job shop scheduling are reported 
recent study have shown that graph based approach are effective for semi supervised learning the key idea behind many graph based approach is to enforce the consistency between the class assignment of unlabeled example and the pairwise similarity between example one major limitation with most graph based approach is that they are unable to explore dissimilarity or negative similarity this is because the dissimilar relation is not transitive and therefore is difficult to be propagated furthermore negative similarity could result in unbounded energy function which make most graph based algorithm unapplicable in this paper we propose a new graph based approach termed a mixed label propagation which is able to effectively explore both similarity and dissimilarity simultaneously in particular the new framework determines the assignment of class label by minimizing the energy function associated with positive similarity and maximizing the energy function associated with negative similarity our empirical study with collaborative filtering show promising performance of the proposed approach 
the situation calculus and the fluent calculus are successful action formalism that share many concept but until now there is no formal relation between the two calculus that would allow to formally analyze the relationship between the two approach a well a between the programming language based on them golog and flux furthermore such a formal relation would allow to combine golog and flux and to analyze which of the underlying computation principle is better suited for different class of program we develop a formal translation between domain axiomatizations of the situation calculus and the fluent calculus and present a fluent calculus semantics for golog program for domain with deterministic action our approach allows an automatic translation of golog domain description and execution of golog program with flux 
bayesian belief net bns are often used for classification task typically to return the most likely class label for a specified instance many bn iearners however attempt to find the bn that maximizes a different objective function viz likelihood rather than classification accuracy typically by first using some model selection criterion to identify an appropriate graphical structure then finding good parameter for that structure this paper considers a number of possible criterion for selecting the best structure both generative i e based on likelihood bic bde and discriminative i e conditional bic cbic resubstitution classification error ce and bias variance bv we empirically compare these criterion against a variety of different correct bn structure both real world and synthetic over a range of complexity we also explore different way to set the parameter dealing with two issue should we seek the parameter that maximize likelihood versus the one that maximize conditional likelihood should we use i the entire training sample first to learn the best parameter and then to evaluate the model versus ii only a partition for parameter estimation and another partition for evaluation cross validation our result show that the discriminative bv model selection criterion is one of the best measure for identifying the optimal structure while the discriminative cbic performs poorly that we should use the parameter that maximize likelihood and that it is typically better to use cross validation here 
this work can be seen a a rst approach to a new planning model that take into account the possibility to express action and uents with non boolean value according to this model a planning problem is dened using both graded multi valued and classical boolean uents moreover action that can have different application degree can be dened in this work a pddl extension allowing to describe such new problem is proposed and a planning algorithm for such problem is presented 
we present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm the approach address the known difficulty of tuning probabilistic search algorithm such a genetic algorithm or simulated annealing for a given search problem by the introduction of domain knowledge we show that our approach is effective for developing heuristic algorithm for difficult combinatorial problem by solving benchmark from the industrially relevant domain of vlsi detailed routing in this paper we present advanced technique for improving our knowledge acquisition approach we also present a novel method that us domain knowledge for the prioritisation of mutation operator increasing the ga s efficiency noticeably 
embedding algorithm are a method for revealing low dimensional structure in complex data most embedding algorithm are designed to handle object of a single type for which pairwise distance are specified here we describe a method for embedding object of different type such a author and term into a single common euclidean space based on their co occurrence statistic the joint distribution of the heterogenous object are modeled a exponential of squared euclidean distance in a low dimensional embedding space this construction link the problem to convex optimization over positive semidefinite matrix we quantify the performance of our method on two text datasets and show that it consistently and significantly outperforms standard method of statistical correspondence modeling such a multidimensional scaling and correspondence analysis 
in our research we investigate rational agent which consciously balance deliberation and acting and us learning to augment it reasoning it creates several partial plan us past experience to choose the best one and by executing it gam new knowledge about the world we analyse a possible application of inductive logic programming to learn how to evaluate partial plan in a resource constrained way we also discus how ilp framework can generalise partial plan 
in this paper we present a general logical framework for weighted max sat problem and study property of inference rule for branch and bound max sat solver several rule which are not equivalent but equivalent are proposed and we show that equivalent rule are also sound a an example we show how to exploit inference rule to achieve a new lower bound function for a max sat solver our new function is admissible and consistently better than the well known lower bound function based on the study of inference rule we implement an efficient solver and the experimental result demonstrate that our solver outperforms the most efficient solver that ha been implemented very recently hera and larrosa especially for large instance 
abstract conditional constraint satisfaction problem condcsps adequately capture problem change at solving time by conditionally identifying those variable and constraint that are relevant to final solution real world task with dynamic behavior such a configuration design diagnosis planning and hardware test generation have been modeled more naturally with condcsps such interest ha been matched by the development of more effective algorithm that depart from classical backtracking and incorporate local consistency checking although performance result have been reported for these specialized algorithm the experimental analysis ha been conducted separately using different test suite and little is known about the algorithm relative performance in this abstract we present a condcsp solver that implement direct and reformulation based algorithm each of which using forward checking and maintaining local consistency in our experimental analysis we have considered randomly generated condcsps of diverse topology in term of problem density and satisfiabilityof the standard and conditional problem component execution time resultsshow that there isnot one winner but that reformulation solving in conjunction with forward checking performs better on problem with larger solution set while direct solving in conjunction with maintaining arc consistency is always preferred over direct solving using forward checking a conditional constraint satisfaction problem condcsp 
in conditional visuomotor learning several arbitrary association between visual cue and motor response have to be learned by trial and error at the same time monkey a human do not achieve this task by randomly trying each possible association rather they use a strategy that organizes sequentially the acquisition of individual stimulus response association accordingly neuronal recording in the monkey striatum the main basal ganglion structure reveals two form of plasticity during learning a transient one that could constitute the neuronal correlate of the strategy and a long lasting one that could reflect the slow neuronal implementation of individual association existing model of basal ganglion function based on reinforcement learning cannot account for this dual process hence we developed a mathematical model of conditional visuomotor learning inspired from viability theory which implement both the formation of individual association and the use of strategy to organize learning 
it is often useful to represent a single example by a set of the local feature that comprise it however this representation pose a challenge to many conventional learning technique since set may vary in cardinality and the element are unordered to compare set of feature researcher often resort to solving for the least cost correspondence but this is computationally expensive and becomes impractical for large set size we have developed a general approximate matching technique called the pyramid match that measure partial match similarity in time linear in the number of feature vector per set the matching form a mercer kernel making it valid for use in many existing kernel based learning method we have demonstrated the approach for various learning task in vision and text processing and find that it is accurate and significantly more efficient than previous approach 
despite significant algorithmic advance in recent year finding optimal policy for large scale multistage stochastic combinatorial optimization problem remains far beyond the reach of existing method this paper study a complementary approach online anticipatory algorithm that make decision at each step by solving the anticipatory relaxation for a polynomial number of scenario online anticipatory algorithm have exhibited surprisingly good result on a variety of application and this paper aim at understanding their success in particular the paper derives sufficient condition under which online anticipatory algorithm achieve good expected utility and study the various type of error arising in the algorithm including the anticipativity and sampling error the sampling error is shown to be negligible with a logarithmic number of scenario the anticipativity error is harder to bound and is shown to be low both theoretically and experimentally for the existing application 
one of the main topic of research in genornics is determining the relevance of mutation described in haplotype data a cause of some genetic disease however due to technological limitation genotype data rather than haplotype data is usually obtained the haplotype inference by pure parsimony hipp problem consists in inferring haplotype from genotype s t the number of required haplotype is minimum previous approach to the hipp problem have focused on integer programming model and branch and bound algorithm in contrast this paper proposes the utilization of boolean satisfiability sat the proposed solution entail a sat model a number of key pruning technique and an iterative algorithm that enumerates the possible solution value for the target optimization problem experimental result obtained on a wide range of instance demonstrate that the sat based approach can be several order of magnitude faster than existing solution besides being more efficient the sat based approach is also the only capable of computing the solution for a large number of instance 
freebase is a practical scalable graph shaped database of structured general human knowledge inspired by semantic web research and collaborative data community such a the wikipedia freebase allows public read and write access through an http based graph query api for research the creation and maintenance of structured data and application building access is free and all data in freebase ha a very open e g creative common gfdl license 
the predictive representation hypothesis hold that particularly good generalization will result from representing the state of the world in term of prediction about possible future experience this hypothesis ha been a central motivation behind recent research in for example psrs and td network in this paper we present the first explicit investigation of this hypothesis we show in a reinforcement learning example a grid world navigation task that a predictive representation in tabular form can learn much faster than both the tabular explicit state representation and a tabular history based method 
we present a data structure and an algorithm for real time path planning of a humanoid robot due to the many degree of freedom the robot shape and available action are approximated for finding solution efficiently the resulting dimensional configuration space is searched by the a algorithm finding solution in tenth of a second on lowperformance embedded hardware experimental result demonstrate our solution for a robot in a world containing obstacle with different height stair and a higher level platform 
we introduce a new collaborative machine learning paradigm in which the user directs a learning algorithm by manually editing the automatically induced model we identify a generic architecture that support seamless interweaving of automated learning from training sample and manual edits of the model and we discus the main difficulty that the framework address we describe augmentation based learning abl the first learning algorithm that support interweaving of edits and learning from training sample we use example based on abl to outline selected advantage of the approach dealing with bad data by manually removing their effect from the model and learning a model with fewer training sample 
planning graph have been shown to be a rich source of heuristic information for many kind of planner in many case planner must compute a planning graph for each element of a set of state the naive technique enumerates the graph individually this is equivalent to solving an all pair shortest path problem by iterating a single source algorithm over each source we introduce a structure the state agnostic planning graph that directly solves the all pair problem for the relaxation introduced by planning graph the technique can also be characterized a exploiting the overlap present in set of planning graph for the purpose of exposition we first present the technique in classical planning the more prominent application of tnis technique is in belief space planning where an optimization result in drastically improved theoretical complexity our experimental evaluation quantifies this performance boost and demonstrates that heuristic belief space progression planning using our technique is competitive with the state of t the art 
this paper proposes to enhance similarity based classification by virtual attribute from imperfect domain theory we analyze how property of the domain theory such a partialness and vagueness influence classification accuracy experiment in a simple domain suggest that partial knowledge is more useful than vague knowledge however for data set from the uci machine learning repository we show that vague domain knowledge that in isolation performs at chance level can substantially increase classification accuracy when being incorporated into similarity based classification 
we report on a successful experiment of computeraided theorem discovery in the area of logic programming with answer set semantics specifically with the help of computer we discovered exact condition that capture the strong equivalence between a set of a rule and the empty set a set of a rule and another set of a rule a set s of two rule and a subset of s with one rule a set of two rule and a set of another rule and a set s of three rule and a subset of s with two rule we prove some general theorem that can help u verify the correctness of these condition and discus the usefulness of our result in program simplification 
determining the number of solution of a csp ha several application in ai in statistical physic and in guiding backtrack search heuristic it is a p complete problem for which some exact and approximate algorithm have been designed successful csp model often use high arity global constraint to capture the structure of a problem this paper exploit such structure and derives polytime evaluation of the number of solution of individual constraint these may be combined to approximate the total number of solution or used to guide search heuristic we give algorithm for several of the main family of constraint and discus the possible us of such solution count 
senseclusters is a freely available intelligent system that cluster together similar context in natural language text thereafter it assigns identifying label to these cluster based on their content it is a purely unsupervised approach that is language independent and us no knowledge other than what is available in raw un annotated corpus in addition to clustering similar context it can be used to identify synonym and set of related word it ha been applied to a diverse range of problem including proper name disambiguation word sense discrimination email organization and document clustering senseclusters is a complete system that support feature selection from large corpus several different context representation scheme various clustering algorithm the creation of descriptive and discriminating label for the discovered cluster and evaluation relative to gold standard data 
creating video recording of event such a lecture or meeting is increasingly inexpensive and easy however reviewing the content of such video may be time consuming and difficult our goal is to produce a comic book summary in which a transcript is augmented with keyframes that disambiguate and clarify accompanying text unlike most previous keyframe extraction system which rely primarily on visual cue we present a linguistically motivated approach that selects keyframes that contain salient gesture rather than learning gesture salience directly it is estimated by measuring the contribution of gesture to understanding other discourse phenomenon more specifically we bootstrap from multimodal coreference resolution to identify gesture that improve performance we then select keyframes that capture these gesture our model predicts gesture salience a a hidden variable in a conditional framework with observable feature from both the visual and textual modality this approach significantly outperforms competitive baseline that do not use gesture information 
this paper present a novel approach for extracting high quality thread title reply pair a chat knowledge from online discussion forum so a to efficiently support the construction of a chatbot for a certain domain given a forum the high quality thread title reply pair are extracted using a cascaded framework first the reply logically relevant to the thread title of the root message are extracted with an svm classifier from all the reply based on correlation such a structure and content then the extracted thread title reply pair are ranked with a ranking svm based on their content quality finally the top n thread title reply pair are selected a chatbot knowledge result from experiment conducted within a movie forum show the proposed approach is effective 
negotiation is the technique for reaching mutually beneficial agreement among agent via communication a concurrent negotiation problem occurs when an agent need to negotiate with multiple agent to reach agreement in this paper we present a protocol to support many to many bilateral multiple issue negotiation in a competitive environment the protocol is presented in the context of service oriented negotiation where one or more self interested party can provide service to one or more other party by extending existing negotiation protocol our described protocol enables both service requestors and service provider to manage several negotiation process in parallel moreover this protocol mitigates the situation where most one to many negotiation are biased in favor of one participating agent and allow the negotiation participant to make durable commitment to reduce the decommitment situation we conclude by discussing additional issue related to concurrent multiple issue negotiation 
market simulation like their real world counterpart are typically domain of high complexity high variability and incomplete information the performance of autonomous agent in these market depends both upon the strategy of their opponent and on various market condition such a supply and demand because the space for possible strategy and market condition is very large empirical analysis in these domain becomes exceedingly difficult researcher who wish to evaluate their agent must run many test game across multiple opponent set and market condition to verify that agent performance ha actually improved our approach is to improve the statistical power of market simulation experiment by controlling their complexity thereby creating an environment more conducive to structured agent testing and analysis we develop a tool that control variability across game in one such market environment the trading agent competition for supply chain management tac scm and demonstrate how it provides an efficient systematic method for tac scm researcher to analyze agent performance 
we propose a supervised maximum entropy ranking approach to pronoun resolution a an alternative to commonly used classification based approach classification approach consider only one or two candidate antecedent for a pronoun at a time whereas ranking allows all candidate to be evaluated together we argue that this provides a more natural fit for the task than classification and show that it delivers significant performance improvement on the ace datasets in particular our ranker obtains an error reduction of over the best classification approach the twin candidate model furthermore we show that the ranker offer some computational advantage over the twin candidate classifier since it easily allows the inclusion of more candidate antecedent during training this approach lead to a further error reduction of a total reduction of over the twin candidate model 
this paper present a study of the various aspect of blog reading behavior the analyzed data are obtained from a japanese weblog hosting service doblog four kind of social network are generated and analyzed citation comment trackback and blogroll network in addition the user log data are used to identify readership relation among blogger after analysis of more than user for about two year we reveal some interaction between social relation and readership relation we first show that blogger read other weblogs on a regular basis of weblogs that are read at least three time are read every five time a user log in we call this relation a regular reading relation rr relation then prediction of rr relation is done using feature from the four kind of social network lastly information diffusion on rr relation is analyzed and characterized result of this study show that the blog in rr relation have an important role in blogger activity we find the feature which have a correlation with rr relation 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
this paper is concerned with the problem of structured data extraction from web page the objective of the research is to automatically segment data record in a page extract data item field from these record and store the extracted data in a database in this paper we first introduce the extraction problem and then discus the main existing approach and their limitation after that we introduce a novel technique called depta to automatically perform web data extraction the method consists of three step identifying data record with similar pattern in a page aligning and extracting data item from the identified data record and generating tree based regular expression to facilitate later extraction from other similar page the key innovation is the proposal of a new multiple tree alignment algorithm called partial tree alignment which wa found to be particularly suitable for web data extraction this paper is based on our work published in kdd and www 
formal analysis of decentralized decision making ha become a thriving research area in recent year producing a number of multi agent extension of markov decision process while much of the work ha focused on optimizing discounted cumulative reward optimizing average reward is sometimes a more suitable criterion we formalize a class of such problem and analyze it characteristic showing that it is np complete and that optimal policy are deterministic our analysis lay the foundation for designing two optimal algorithm experimental result with a standard problem from the literature illustrate the applicability of these solution technique 
critiquing is an important form of feedback in conversational recommender system however in these system the user is usually limited to critiquing a single product feature at a time recently dynamic critiquing ha been proposed to address this shortcoming by automatically generating compound critique over multiple feature that may be presented to the user at recommendation time to date a number of different version of dynamic critiquing have been evaluated in isolation and with reference to artificial user in this paper we bring together the main flavor of dynamic critiquing and perform a large scale comparative evaluation a part of an extensive real user trial this evaluation reveals some interesting fact about the way real user interact with critique based recommenders 
we introduce an approach to autonomously creating state space abstraction for an online reinforcement learning agent using a relational representation our approach us a tree based function approximation derived from mccallum s utree algorithm we have extended this approach to use a relational representation where relational observation are represented by attributed graph mcgovern et al we address the challenge introduced by a relational representation by using stochastic sampling to manage the search space srinivasan and temporal sampling to manage autocorrelation jensen and neville relational utree incorporates iterative tree induction utgoff et al to allow it to adapt to changing environment we empirically demonstrate that relational utree performs better than similar relational learning method finney et al driessens et al in a block world domain we also demonstrate that relational utree can learn to play a sub task of the game of go called tsume go ramon et al 
this paper provides a brief introduction to recent work in statistical parsing and it application we highlight success to date remaining challenge and promising future work 
this paper proposes a novel method to characterize the performance of autonomous agent in the trading agent competition for supply chain management tac scm we create benchmarking tool that manipulate market environment to control the condition and provide guideline to test trading agent using these tool we show how developer can inspect their agent and unveil behavior that might otherwise have gone undiscovered 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
discretization defined a a set of cut over domain of attribute represents an important pre processing task for numeric data analysis some machine learning algorithm require a discrete feature space but in real world application continuous attribute must be handled to deal with this problem many supervised discretization method have been proposed but little ha been done to synthesize unsupervised discretization method to be used in domain where no class information is available furthermore existing method such a equal width or equal frequency binning are not well principled raising therefore the need for more sophisticated method for the unsupervised discretization of continuous feature this paper present a novel unsupervised discretization method that us non parametric density estimator to automatically adapt sub interval dimension to the data the proposed algorithm search for the next two sub interval to produce evaluating the best cut point on the basis of the density induced in the sub interval by the current cut and the density given by a kernel density estimator for each sub interval it us cross validated log likelihood to select the maximal number of interval the new proposed method is compared to equal width and equal frequency discretization method through experiment on well known benchmarking data 
existing graph partitioning approach are mainly based on optimizing edge cut and do not take the distribution of edge weight link distribution into consideration in this paper we propose a general model to partition graph based on link distribution this model formulates graph partitioning under a certain distribution assumption a approximating the graph affinity matrix under the corresponding distortion measure under this model we derive a novel graph partitioning algorithm to approximate a graph affinity matrix under various bregman divergence which correspond to a large exponential family of distribution we also establish the connection between edge cut objective and the proposed model to provide a unified view to graph partitioning 
grid with blocked and unblocked cell are often used to represent terrain in computer game and robotics however path formed by grid edge can be sub optimal and unrealistic looking since the possible heading are artificially constrained we present theta a variant of a that propagates information along grid edge without constraining the path to grid edge theta is simple fast and find short and realistic looking path we compare theta against both field d the only other variant of a that propagates information along grid edge without constraining the path to grid edge and a with post smoothed path although neither path planning method is guaranteed to find shortest path we show experimentally that theta find shorter and more realistic looking path than either of these existing technique 
wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than wordnet in this work we present experiment on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets existing relatedness measure perform better using wikipedia than a baseline given by google count and we show that wikipedia outperforms wordnet when applied to the largest available dataset designed for that purpose the best result on this dataset are obtained by integrating google wordnet and wikipedia based measure we also show that including wikipedia improves the performance of an nlp application processing naturally occurring text 
set variable are ubiquitous in modeling soft constraint problem but effort on practical consistency algorithm for weighted constraint satisfaction problem wcsps have only been on integer variable we adapt the classical notion of set bound consistency for wcsps and propose efficient representation scheme for set variable and common unary binary and ternary set constraint a well a cardinality constraint instead of reasoning consistency on an entire set variable directly we propose local consistency check at the set element level and demonstrate that this apparent micro management of consistency doe imply set bound consistency at the variable level in addition we prove that our framework capture classical csps with set variable and degenerate to the classical case when the weight in the problem contain only and t last but not least we verify the feasibility and efficiency of our proposal with a prototype implementation the efficiency of which is competitive against ilog solver on classical problem and order of magnitude better than wcsp model using variable to simulate set variable on soft problem 
two noteworthy model of planning in ai are probabilistic planning based on mdps and it generalization and nondeterministic planning mainly based on model checking in this paper we show that probabilistic and nondeterministic planning are extreme of a rich continuum of problem that deal simultaneously with risk and knightian uncertainty obtain a unifying model for these problem using imprecise mdps derive a simplified bellman s principle of optimality for our model and show how to adapt and analyze state of art algorithm such a l rtdp and ldfs in this unifying setup we discus example and connection to various proposal for planning under general uncertainty 
recently lakemeyer and levesque proposed a logic of only knowing which precisely capture three form of nonmonotonic reasoning moore s autoepistemic logic konolige s variant based on moderately grounded expansion and reiter s default logic default have a uniform representation under all three interpretation in the new logic moreover the logic itself is monotonic that is nonmonotonic reasoning is cast in term of validity in the classical sense while lakemeyer and levesque gave a model theoretic account of their logic a proof theoretic characterization remained open this paper fill that gap for the propositional subset a sound and complete axiom system in the new logic for all three variety of default reasoning we also present formal derivation for some example of default reasoning finally we present evidence that it is unlikely that a complete axiom system exists in the first order case even when restricted to the simplest form of default reasoning 
automation of web service composition is one of the most interesting challenge facing the semantic web today in this paper we propose a mean of performing automated web service composition by exploiting semantic matchmaking between web service parameter i e output and input to enable their connection and interaction the key idea is that the matchmaking enables at run time finding semantic compatibility among independently defined web service description to this end our approach extends existing method in order to explain misconnections between web service from this we generate web service composition that realize the goal satisfying and optimizing the semantic connection between web service moreover a process of relaxing the hard constraint is introduced in case the composition process failed our system is implemented and interacting with web service dedicated on a telecom scenario the preliminary evaluation result showed high efficiency and effectiveness of the proposed approach 
various approach have been proposed to quantify the similarity between concept in an ontology we present a novel approach that allows similarity to be asymmetric while still using only information contained in the structure of the ontology we show through experiment on the wordnet and geneontology that the new approach achieves better accuracy than existing technique 
the goal of the virtual human project at the university of southern california s institute for creative technology is to enrich virtual training environment with virtual human autonomous agent that support face to face interaction with trainee in a variety of role through bringing together many different area of research including speech recognition natural language understanding dialogue management cognitive modeling emotion modeling nonverbal behavior and speech and knowledge management the demo at aaai will focus on our work using virtual human to train negotiation skill conference attendee will negotiate with a virtual human doctor and elder to try to move a clinic out of harm s way in single and multiparty negotiation scenario using the latest iteration of our virtual human framework the user will use natural speech to talk to the embodied agent who will respond in accordance with their internal task model and state the character will carry out a multi party dialogue with verbal and nonverbal behavior a video of a single party version of the scenario wa shown at aaai this new interactive demo introduces several new feature including multiparty negotiation dynamically generated non verbal behavior and a central ontology 
we present the system nfoil it tightly integrates the na ve bayes learning scheme with the inductive logic programming rule learner foil in contrast to previous combination which have employed na ve bayes only for post processing the rule set nfoil employ the na ve bayes criterion to directly guide it search experimental evidence show that nfoil performs better than both it base line algorithm foil or the post processing approach and is at the same time competitive with more sophisticated approach 
most existing decision tree inducer are very fast due to their greedy approach in many real life application however we are willing to allocate more time to get better decision tree our recently introduced lsid contract anytime algorithm allows computation speed to be traded for better tree quality a a contract algorithm lsid must be allocated it resource a priori which is not always possible in this work we present iidt a general framework for interruptible induction of decision tree that need not be allocated resource a priori the core of our proposed framework is an iterative improvement algorithm that repeatedly selects a subtree whose reconstruction is expected to yield the highest marginal utility the algorithm then rebuilds the subtree with a higher allocation of resource iidt can also be configured to receive training example a they become available and is thus appropriate for incremental learning task empirical evaluation with several hard concept show that iidt exhibit good anytime behavior and significantly outperforms greedy inducer when more time is available a comparison of iidt to several modern decision tree learner showed it to be superior 
my research focus is on using continuous state partially observable markov decision process pomdps to perform object manipulation task using a robotic arm during object manipulation object dynamic can be extremely complex non linear and challenging to specify to avoid modeling the full complexity of possible dynamic i instead use a model which switch between a discrete number of simple dynamic model by learning these model and extending porta s continuous state pomdp framework porta et at to incorporate this switching dynamic model we hope to handle task that involve absolute and relative dynamic within a single framework this dynamic model may be applicable not only to object manipulation task but also to a number of other problem such a robot navigation by using an explicit model of uncertainty i hope to create solution to object manipulation task that more robustly handle the noisy sensory information received by physical robot 
in many framework for reasoning under inconsistency it is implicitly assumed that the formula from the belief base are connected using a weak form of conjunction when it is consistent a belief base b n where the i are propositional formula is logically equivalent to the base n however when it is not consistent both base typically lead to different conclusion this illustrates the fact that the comma used in base b ha to be considered a an additional genuine connective and not a a simple conjunction in this work we define and investigate a propositional framework with such a comma connective we give it a semantics and show how it generalizes several approach for reasoning from inconsistent belief 
the definition of a stable model ha provided a declarative semantics for prolog program with negation a failure and ha led to the development of answer set programming in this paper we propose a new definition of that concept which cover many construct used in answer set programming including disjunctive rule choice rule and conditional literal and unlike the original definition refers neither to grounding nor to fixpoints rather it is based on a syntactic transformation which turn a logic program into a formula of second order logic that is similar to the formula familiar from the definition of circumscription 
although there ha been much discussion of belief change e g gardenfors spohn goal change ha not received much attention in this paper we propose a method for goal change in the framework of reiter s theory of action in the situation calculus mccarthy and hayes levesque et al and investigate it property we extend the framework developed in shapiro et al and shapiro and lesp erance where goal and goal expansion were modelled but goal contraction wa not 
we present a framework for the debugging of logically contradicting terminology which is based on traditional modelbased diagnosis to study the feasibility of this highly general approach we prototypically implemented the hitting set algorithm presented in reiter and applied it in three different scenario first we use a description logic reasoning system a a black box to determine necessarily maximal conict set then we use our own non optimized dl reasoning engine to produce small and a specialized algorithm to determine minimal conict set in a number of experiment we show that the rst method already fails for relatively small terminology however based on small or minimal conict set we can often calculate diagnosis in reasonable time 
compilability is a measure of how effectively compilation or preprocessing can be applied to knowledge base specified in a particular knowledge representation formalism the aim of compilation is to allow for efficient on line query processing a theory of compilability ha been established for organizing knowledge representation formalism according to a scheme of compilability class and bear strong analogy to the classical theory of complexity which permit the organization of computational problem according to complexity class we develop a novel theory of compilability called parameterized compilability which incorporates the notion of parameterization a used in parameterized complexity and permit for refined analysis of compilability 
smoothing approach to the simultaneous localization and mapping slam problem in robotics are superior to the more common filtering approach in being exact better equipped to deal with non linearity and computing the entire robot trajectory however while filtering algorithm that perform map update in constant time exist no analogous smoothing method is available we aim to rectify this situation by presenting a smoothingbased solution to slam using loopy belief propagation lbp that can perform the trajectory and map update in constant time except when a loop is closed in the environment the slam problem is represented a a gaussian markov random field gmrf over which lbp is performed we prove that lbp in this case is equivalent to gauss seidel relaxation of a linear system the inability to compute marginal covariance efficiently in a smoothing algorithm ha previously been a stumbling block to their widespread use lbp enables the efficient recovery of the marginal covariance albeit approximately of landmark and pose while the final covariance are overconfident the one obtained from a spanning tree of the gmrf are conservative making them useful for data association experiment in simulation and using real data are presented 
we introduce and analyze q potential game and qcongestion game where q is a positive integer a potential congestion game is a potential congestion game we show that a game is a q potential game if and only if it is up to an isomorphism a q congestion game a a corollary we derive the result that every game in strategic form is a qcongestion game for some q i t is further shown that every q congestion game is isomorphic to a qnetwork game where the network environment is defined by a directed graph with one origin and one destination finally we discus our main agenda the issue of representing q congestion game with non negative cost function by congestion model with non negative and monotonic facility cost function we provide some initial result in this regard 
the question of how machine can be endowed with the ability to acquire and robustly manipulate commonsense knowledge is a fundamental scientic problem here we formulate an approach to this problem that we call knowledge infusion we argue that robust logic offer an appropriate semantics for this endeavor because it support provably efcient algorithm for a basic set of necessary learning and reasoning task we observe that multiple concept can be learned simultaneously from a common data set in a data efcient manner we also point out that the preparation of appropriate teaching material for training system constructed according to these principle raise new challenge 
in the field of heuristic search it is well known that improving the quality of an admissible heuristic can significantly decrease the search effort required to find an optimal solution existing literature often assumes that admissible heuristic are consistent implying that consistency is a desirable attribute to the contrary this paper show that an inconsistent heuristic can be preferable to a consistent heuristic theoretical and empirical result show that in many case inconsistency can be used to achieve large performance improvement the conventional wisdom about inconsistent heuristic is wrong 
recently the problem of dimensionality reduction ha received a lot of interest in many field of information processing we consider the case where data is sampled from a low dimensional manifold which is embedded in high dimensional euclidean space the most popular manifold learning algorithm include locally linear embedding isomap and laplacian eigenmap however these algorithm are nonlinear and only provide the embedding result of training sample in this paper we propose a novel linear dimensionality reduction algorithm called isometric projection isometric projection construct a weighted data graph where the weight are discrete approximation of the geodesic distance on the data manifold a linear subspace is then obtained by preserving the pairwise distance in this way isometric projection can be defined everywhere comparing to principal component analysis pca which is widely used in data processing our algorithm is more capable of discovering the intrinsic geometrical structure specially pca is optimal only when the data space is linear while our algorithm ha no such assumption and therefore can handle more complex data space experimental result on two real life data set illustrate the effectiveness of the proposed method 
extracting a map from a stream of experience is a key problem in robotics and artificial intelligence in general we propose a technique called subjective mapping that seek to learn a fully specified predictive model or map without the need for expert provided model of the robot s motion and sensor apparatus we briefly overview the recent advancement presented elsewhere icml ijcai and isrr that make this possible examine it significance in relationship to other development in the field and outline open issue that remain to be addressed we propose an approach to mapping which requires no a priori knowledge of model this short paper will outline this approach and the recently developed technique that make it possible a well a examine it significance for the broader artificial intelligence endeavor we first introduce the basic principle of subjective mapping we then briefly discus the recently developed algorithm called action respecting embedding bowling ghodsi wilkinson and it extension wilkinson bowling ghodsi bowling et al which form the crux of the subjective mapping approaching we then discus how subjective mapping relates to other recent advance in artificial intelligence we finally outline what open issue remain to be addressed 
ai ha had notable success in building highperformance game playing program to compete against the best human player however the availability of fast and plentiful machine with large memory and disk creates the possibility of a game this ha been done before for simple or relatively small game in this paper we present new idea and algorithm for solving the game of checker checker is a popular game of skill with a search space of possible position this paper report on our first result one of the most challenging checker opening ha been solved the white doctor opening is a draw solving roughly more opening will result in the game theoretic value of checker being determined 
we introduce a general study of routing mediator a routing mediator can act in a given multi agent encounter on behalf of the agent that give it the right of play routing mediator differ from one another according to the information they may have our study concentrate on the use of routing mediator in order to reach correlated strong equilibrium a multi agent behavior which is stable against deviation by coalition we study the relationship between the power of different routing mediator in establishing correlated strong equilibrium surprisingly our main result show a natural class of routing mediator that allow to implement fair and efficient outcome a a correlated super strong equilibrium in a very wide class of game 
arc consistency play such a key role in constraint programming for solving real life problem that it is almost the only algorithm used for reducing domain there are a few specific problem for which a stronger form of propagation often called shaving is more efficient nevertheless in many case shaving at each node of the search tree is not worth doing arc consistency filtering is much faster and the additional domain reduction inferred by shaving do not pay off in this paper we propose a new kind of shaving called quickshaving which is guided by the search a quickshaving may infer some additional domain reduction compared with arc consistency it can improve the search for a solution by an exponential ratio moreover the advantage of quick shaving is that in practice unlike a standard form of shaving the additional domain reduction deduced by quickshaving come at a very low overhead compared with arc consistency 
online mechanism design considers the problem of sequential decision making in a multi agent system with self interested agent the agent population is dynamic and each agent ha private information about it value for a sequence of decision we introduce a method ironing to transform an algorithm for online stochastic optimization into one that is incentive compatible ironing achieves this by canceling decision that violate a form of monotonicity the approach is applied to the consensus algorithm and experimental result in a resource allocation domain show that not many decision need to be canceled and that the overhead of ironing is manageable 
in many sensing application including environmental monitoring measurement system must cover a large space with only limited sensing resource one approach to achieve required sensing coverage is to use robot to convey sensor within this space planning the motion of these robot coordinating their path in order to maximize the amount of information collected while placing bound on their resource e g path length or energy capacity is anp hard problem in this paper we present an efficient path planning algorithm that coordinate multiple robot each having a resource constraint to maximize the informativeness of their visited location in particular we use a gaussian process to model the underlying phenomenon and use the mutual information between the visited location and remainder of the space to characterize the amount of information collected we provide strong theoretical approximation guarantee for our algorithm by exploiting the submodularity property of mutual information in addition we improve the efficiency of our approach by extending the algorithm using branch and bound and a region based decomposition of the space we provide an extensive empirical analysis of our algorithm comparing with existing heuristic on datasets from several real world sensing application 
a density based clustering algorithm called outclust is presented the algorithm exploit a notion of local density in order to find homogeneous group of object a opposite to object mostly deviating from the overall population the proposed algorithm try to simultaneously consider several feature of real data set namely finding cluster of different shape and density in high dimensional data in presence of noise it is shown that the method is able to identify very meaningful cluster and experimental comparison with partitioning hierarchial and density based clustering algorithm is presented pointing out that the algorithm achieves good clustering quality 
abstract although many paper about belief update have been written it precise scope still remains unclear in this paper we aim at identifying this scope and we show that belief update is a specific case of feedback free action progression this strong connection with the field of reasoning about action lead u to reconsider belief update and investigate new issue especially reverse update which is to regression what update is to progression 
symmetry is an important factor in solving many constraint satisfaction problem one common type of symmetry is when we have symmetric value in a recent series of paper we have studied method to break value symmetry walsh a walsh our result identify computational limit on eliminating value symmetry for instance we prove that pruning all symmetric value is np hard in general nevertheless experiment show that much value symmetry can be broken in practice these result may be useful to researcher in planning scheduling and other area a value symmetry occurs in many different domain 
imitation learning also called learning by watching or programming by demonstration ha emerged a a mean of accelerating many reinforcement learning task previous work ha shown the value of imitation in domain where a single mentor demonstrates execution of a known optimal policy for the benefit of a learning agent we consider the more general scenario of learning from mentor who are themselves agent seeking to maximize their own reward we propose a new algorithm based on the concept of transferable utility for ensuring that an observer agent can learn efficiently in the context of a selfish not necessarily helpful mentor we also address the question of when an imitative agent should request help from a mentor and when the mentor can be expected to acknowledge a request for help in analogy with other type of active learning we call the proposed approach active imitation learning 
most previous study on active learning focused on the problem of model selection i e how to identify the optimal classification model from a family of predefined model using a small carefully selected training set in this paper we address the problem of active algorithm selection the goal of this problem is to efficiently identify the optimal learning algorithm for a given dataset from a set of algorithm using a small training set in this study we present a general framework for active algorithm selection by extending the idea of the hedge algorithm it employ the worst case analysis to identify the example that can effectively increase the weighted loss function defined in the hedge algorithm we further extend the framework by incorporating the correlation information among unlabeled example to accurately estimate the change in the weighted loss function and maximum entropy discrimination to automatically determine the combination weight used by the hedge algorithm our empirical study with the datasets of wcci performance prediction challenge show promising performance of the proposed framework for active algorithm selection 
time series data abounds in real world problem measuring the similarity of time series is a key to solving these problem one state of the art measure is the longest common subsequence t his measure advocate using the length of the longest common subsequence a an indication of similarity between sequence but ignores information contained in the second third longest subsequence in order to capture the common information in sequence maximally we propose a novel measure of sequence similarity the number of all common subsequence we show that this measure satisfies the common property of similarity function calculating this measure is not trivial a a brute force approach is exponential in time we present a novel dynamic programming algorithm to calculate this number in polynomial time we also suggest a different way of extending a class of such measure to multidimensional real valued time series in the spirit of probabilistic metric space we conducted an experimental study on the new similarity measure and the extension method for classification it wa found that both the new similarity and the extension method are consistently competitive 
we study the problem of evaluating the goodness of a kernel matrix for a classification task a kernel matrix evaluation is usually used in other expensive procedure like feature and model selection the goodness measure must be calculated efficiently most previous approach are not efficient except for kernel target alignment kta that can be calculated in o n time complexity although kta is widely used we show that it ha some serious drawback we propose an efficient surrogate measure to evaluate the goodness of a kernel matrix based on the data distribution of class in the feature space the measure not only overcomes the limitation of kta but also posse other property like invariance efficiency and error bound guarantee comparative experiment show that the measure is a good indication of the goodness of a kernel matrix 
in this paper we focus on extending the expressive power of constraint based temporal reasoning formalism we begin with the well known simple temporal problem with uncertainty and incorporate three extension prior observability in which the value of uncontrollable event become known prior to their actual occurrence partial shrinkage in which an observation event trigger the reduction of a contingent temporal interval and a generalization of partial shrinkage to requirement link making it possible to express certain type of uncertainty that may arise even when the time point in a problem are themselves fully controllable we describe level of controllability in the resulting formalism the generalized stpu and relate this formalism to related development in disjunctive temporal reasoning throughout we motivate our approach with simple real world example that illustrate the limitation of existing formalism and the flexibility of our proposed extension 
in this paper we present a general scheme to create mechanism that approximate the social welfare in the presence of selfish but rational behavior of agent the usual approach is to design truthful mechanism in which an agent can only lose by impersonating a another agent in contrast our approach is to allow an agent to impersonate several different agent we design the mechanism such that only a limited set of impersonation are reasonable to rational agent our mechanism make sure that for any choice of such impersonation by the agent an approximation to the social welfare is achieved we demonstrate our result on the well studied domain of combinatorial auction ca our mechanism are algorithmic implementation a notion recently suggested in babaioff lavi pavlov 
successful negotiator look beyond a purely utilitarian view we propose a new agent architecture that integrates the utilitarian information and semantic view allowing the definition of strategy that take these three dimension into account information based agency value the information in dialogue in the context of a communication language based on a structured ontology and on the notion of commitment this abstraction unifies measure such a trust reputation and reliability in a single framework 
we study the effect of feature selection and human feedback on feature in active learning setting our experiment on a variety of text categorization task indicate that there is significant potential in improving classifier performance by feature reweighting beyond that achieved via selective sampling alone standard active learning if we have access to an oracle that can point to the important most predictive feature consistent with previous finding we find that feature selection based on the labeled training set ha little effect but our experiment on human subject indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant feature furthermore these experiment show that feature labeling take much le about th time than document labeling we propose an algorithm that interleaf labeling feature and document which significantly accelerates active learning 
mechanism design ha traditionally focused almost exclusively on the design of truthful mechanism there are several drawback to this in certain setting e g voting setting no desirable strategyproof mechanism exist truthful mechanism are unable to take advantage of the fact that computationally bounded agent may not be able to find the best manipulation and when designing mechanism automatically this approach lead to constrained optimization problem for which current technique do not scale to very large instance in this paper we suggest an entirely different approach we start with a na ive manipulable mechanism and incrementally make it more strategyproof over a sequence of iteration we give example of mechanism that variant of our approach generate including the vcg mechanism in general setting with payment and the plurality with runoff voting rule we also provide several basic algorithm for automatically executing our approach in general setting finally we discus how computationally hard it is for agent to find any remaining beneficial manipulation 
we identify some weak point of the lrta k algorithm in the propagation of heuristic change to solve them we present a new algorithm lrta l k that is based on the selection and updating of the interior state of a local space around the current state it keep the good theoretical property of lrta k while improving substantially it performance it is related with a lookahead depth greater than we provide experimental evidence of the benefit of the new algorithm on real time benchmark with respect to existing approach 
possibilistic network and possibilistic logic are two standard framework of interest for representing uncertain piece of knowledge possibilistic network exhibit relationship between variable while possibilistic logic rank logical formula according to their level of certainty for multiply connected network it is well known that the inference process is a hard problem this paper study a new representation of possibilistic network called hybrid possibilistic network it result from combining the two semantically equivalent type of standard representation we first present a propagation algorithm through hybrid possibilistic network this inference algorithm on hybrid network is strictly more efficient and confirmed by experimental study than the one of standard propagation algorithm 
the conditional independence assumption of naive bayes essentially ignores attribute dependency and is often violated on the other hand although a bayesian network can represent arbitrary attribute dependency learning an optimal bayesian network from data is intractable the main reason is that learning the optimal structure of a bayesian network is extremely time consuming thus a bayesian model without structure learning is desirable in this paper we propose a novel model called hidden naive bayes hnb in an hnb a hidden parent is created for each attribute which combine the influence from all other attribute we present an approach to creating hidden parent using the average of weighted one dependence estimator hnb inherits the structural simplicity of naive bayes and can be easily learned without structure learning we propose an algorithm for learning hnb based on conditional mutual information we experimentally test hnb in term of classification accuracy using the uci data set recommended by weka witten frank and compare it to naive bayes langley iba thomas c quinlan sbc langley sage nbtree kohavi cl tan friedman geiger goldszmidt and aode webb boughton wang the experimental result show that hnb outperforms naive bayes c sbc nbtree and cl tan and is competitive with aode 
in this paper we argue that it may be possible to help searcher to better understand the relevance of search result by generating explanation that highlight how other user have interacted with such result under similar search condition in the past we propose the use of the search history of a community of online user a a source of these explanation we describe the result of a recent study to examine the use of such explanation based technique to help web searcher better appreciate the relevancy of search result we highlight shortcoming of this approach in it current form and offer suggestion a to how it may be improved in future work 
there are numerous application where we need to ensure that multiple moving object are sufficiently far apart furthermore in many moving object domain there is positional indeterminacy we are not sure exactly when a given moving object will be at a given location yaman et al provided a logic of motion but did not provide algorithm to ensure that moving object are kept sufficiently far apart in this paper we extend their logic to include a far predicate we develop the checkfar algorithm that check if any given two object will always be sufficiently far apart at during a time interval we have run a set of experiment showing that our checkfar algorithm scale very well 
this paper present a self supervised algorithm for learning perceptual structure based upon correlation in different sensory modality the brain and cognitive science have gathered an enormous body of neurological and phenomenological evidence in the past half century that demonstrates the extraordinary degree of interaction between sensory modality during the course of ordinary perception this paper present a new framework for creating artificial perceptual system inspired by these finding where the primary architectural motif is the cross modal transmission of perceptual information to enhance each sensory channel individually the basic hypothesis underlying this approach is that the world ha regularity natural law tend to correlate physical property and biological perceptory system have evolved to take advantage of this they share information continually and opportunistically across seemingly disparate perceptual channel not epiphenomenologically but rather a a fundamental component of normal perception it is therefore essential that their artificial counterpart be able to share information synergistically within their perceptual channel if they are to approach degree of biological sophistication this paper is a preliminary step in that direction 
various task in decision making and decision support require selecting a preferred subset of item from a given set of item recent work in this area considered method for specifying such preference based on the attribute value of individual element within the set of these the approach of brafman et al appears to be the most general in this paper we consider the problem of computing an optimal subset given such a specification the problem is shown to be np hard in the general case necessitating heuristic search method we consider two algorithm class for this problem direct set construction and implicit enumeration a solution to appropriate csps new algorithm are presented in each class and compared empirically against previous result 
in this paper we introduce a multi context variant of reiter s default logic the logic provides a syntactical counterpart of roelofsen and serafini s information chain approach ijcai yet ha several advantage it is closer to standard way of representing nonmonotonic inference and a number of result from that area come for free it is closer to implementation in particular the restriction to logic programming give u a computationally attractive framework and it allows u to handle a problem with the information chain approach related to skeptical reasoning 
markov model have been a keystone in artificial intelligence for many decade however they remain unsatisfactory when the environment modelled is partially observable there are pathological example where no history of fixed length is sufficient for accurate prediction or decision making on the other hand working with a hidden state like in hidden markov model or partially observable markov decision process ha a high computational cost in order to circumvent this problem we suggest the use of a context based model our approach replaces strict transition probability by influence on transition the method proposed provides a trade off between a fully and partially observable model we also discus the capacity of our framework to model hierarchical knowledge and abstraction simple example are given in order to show the advantage of the algorithm 
a general n ary constraint is usually represented explicitly a a set of it solution tuples which may need exponential space in this paper we introduce a new representation for general n ary constraint called constrained decision diagram cdd cdd generalizes bdd style representation and the main feature is that it combine constraint reasoning consistency technique with a compact data structure we present an application of cdd for recording all solution of a conjunction of constraint instead of an explicit representation we can implicitly encode the solution by mean of constraint propagation our experiment confirm the scalability and demonstrate that cdds can drastically reduce the space needed over explicit and zbdd representation 
consistency are property of constraint network cns that can be exploited in order to make inference when a signicant amount of such inference can be performed cns are much easier to solve in this paper we interest ourselves in relation ltering consistency for binary constraint i e consistency that allow to identify inconsistent pair of value we propose a new consistency called dual consistency dc and relate it to path consistency pc we show that conservative dc cdc i e dc with only relation associated with the constraint of the network considered is more powerful in term of ltering than conservative pc cpc following the approach of mac gregor we introduce an algorithm to establish strong cdc with a very low worst case space complexity even if the relative efcienc y of the algorithm introduced to establish strong cdc partly depends on the density of the constraint graph the experiment we have conducted show that on many series of csp instance cdc is largely faster than cpc up to more than one order of magnitude besides we have observed that enforcing cdc in a preprocessing stage can signicantly speed up the resolution of hard structured instance 
animated text is an appealing field of creative graphical design manually designed text animation is largely employed in advertising movie title and web page in this paper we propose to link through state of the art nlp technique the affective content detection of a piece of text to the animation of the word in the text itself this methodology allows u to automatically generate affective text animation and open some new perspective for advertising internet application and intelligent interface an actor read a script he read those word with the intention of transforming cold print into living speech vocal inflection tone of voice gesture and facial expression are all part of the actor s contribution to the play with the body s subtle vibration and frequency he express the hidden emotional meaning we can say that through his interpretation he brings the script to life in this paper we will show that through automatic detection of the affective meaning of text using state of the art nlp technique we can consequently animate the word that compose them in automated text animation the text itself is capable of augmenting it expressivity and of moving in an 
deictic representation is a representational paradigm based on selective attention and pointer that allows an agent to learn and reason about rich complex environment in this article we present a hierarchical reinforcement learning framework that employ aspect of deictic representation we also present a bayesian algorithm for learning the correct representation for a given sub problem and empirically validate it on a complex game environment 
this paper is concerned with the interaction between word sense disambiguation and the interpretation of noun compound nc in english we develop technique for disambiguating word sense specifically in nc and then investigate whether word sense information can aid in the semantic relation interpretation of nc to disambiguate word sense we combine the one sense per collocation heuristic with the grammatical role of polysemous noun and analysis of word sense combinatorics we built supervised and unsupervised classifier for the task and demonstrate that the supervised method are superior to a number of baseline and also a benchmark state of the art wsd system finally we show that wsd can significantly improve the accuracy of nc interpretation 
in this paper we propose the directed graph embedding dge method that embeds vertex on a directed graph into a vector space by considering the link structure of graph the basic idea is to preserve the locality property of vertex on a directed graph in the embedded space we use the transition probability together with the stationary distribution of markov random walk to measure such locality property it turn out that by exploring the directed link of the graph using random walk we can get an optimal embedding on the vector space that preserve the local affinity which is inherent in the directed graph experiment on both synthetic data and real world web page data are considered the application of our method to web page classification problem get a significant improvement comparing with state of art method 
automatically discovering semantic relation between ontology is an important task with respect to overcoming semantic heterogeneity on the semantic web existing ontology matching system however often produce erroneous mapping in this paper we address the problem of error in mapping by proposing a completely automatic debugging method for ontology mapping the method us logical reasoning to discover and repair logical inconsistency caused by erroneous mapping we describe the debugging method and report experiment on mapping submitted to the ontology alignment evaluation challenge that show that the proposed method actually improves mapping created by different matching system without any human intervention 
supervised machine learning technique developed in the probably approximately correct maximum a posteriori and structural risk minimiziation framework typically make the assumption that the test data a learner is applied to is drawn from the same distribution a the training data in various prominent application of learning technique from robotics to medical diagnosisto processcontrol this assumptionis violated we consider a novel framework where a learner may influence the test distribution in a bounded way from this framework we derive an efficient algorithm that act a a wrapper around a broad class of existing supervised learning algorithm while guarranteeing more robustbehavior under change inthe input distribution 
a formula in first order logic can be viewed a a tree with a logical connective at each node and a knowledge base can be viewed a a tree whose root is a conjunction markov logic richardson and domingo make this conjunction probabilistic a well a the universal quantifier directly under it but the rest of the tree remains purely logical this cause an asymmetry in the treatment of conjunction and disjunction and of universal and existential quantifier we propose to overcome this by allowing the feature of markov logic network mlns to be nested mlns we call this representation recursive random field rrfs rrfs can represent many first order distribution exponentially more compactly than mlns we perform inference in rrfs using mcmc and icm and weight learning using a form of backpropagation weight learning in rrfs is more powerful than structure learning in mlns applied to first order knowledge base it provides a very flexible form of theory revision we evaluate rrfs on the problem of probabilistic integrity constraint in database and obtain promising result for example an mln with the formula r x s x can treat world that violate both r x and s x a le probable than world that only violate one since an mln act a a soft conjunction the grounding of r x and s x simply appear a distinct formula mlns convert the knowledge base to cnf before performing learning or inference this is not possible for the disjunction r x s x no distinction is made between satisfying both r x and s x and satisfying just one since a universally quantified formula is effectively a conjunction over all it grounding while an existentially quantified formula is a disjunction over them this lead to the two quantifier being handled differently this asymmetry can be avoided by softening disjunction and existential quantification in the same way that markov logic softens conjunction and universal quantification the result is a representation where mlns can have nested mlns a feature we call these recursive markov logic network or recursive random field rrfs for short 
caching symmetry and search with decomposition are powerful technique for pruning the search space of constraint problem in this paper we present an innovative way of efficiently combining these technique with branch and bound for solving certain type of constraint optimization problem cop our new method significantly reduces the overhead of performing decomposition during search when dynamic variable ordering are employed in addition it support the exploitation of dynamic symmetry that appear only during search symmetry have not previously been combined with decomposition finally we achieve a superior integration of decomposition and caching with branch and bound than previous approach we test our method on the maximum density still life problem and show that each of our idea yield a significant gain in search performance 
this paper present a model sharedactivity for collaborative agent acting in a group the model suggests mental state for agent with different level of cooperation and permit the formation of group in which member increase individual benefit unlike previous model the model cover group member behavior where group member do not have a joint goal but act collaboratively the model defines key component of a collaborative activity and provides a platform for supporting such activity we studied the behavior of the model in a simulation environment result show how the benefit attained by cooperation is influenced by the complexity of the environment the number of group member and the social dependency between the member the result demonstrate that the model cover social behavior both in setting previously addressed a well a in novel setting 
a situated conversational agent sca is an agent that engages in dialog about the context within which it is embedded situated dialog is characterized by it deep connection to the embedding context and the precise cross timing of linguistic and non linguistic action this paper describes initial research into the construction of an sca that engages in dialog about collaborative physical task in which agent engage in dialog with the joint goal of manipulating the physical context in some manner constructing an sca that can interact naturally in such task requires an agent with the ability to interleave planning action and observation while operating in a partially observable environment consequently i propose to model an sca a a partially observable markov decision process pomdp 
stability against potential deviation by set of agent is a most desired property in the design and analysis of multi agent system however unfortunately this property is typically not satisfied in game theoretic term a strong equilibrium which is a strategy profile immune to deviation by coalition rarely exists this paper suggests the use of mediator in order to enrich the set of situation where we can obtain stability against deviation by coalition a mediator is defined to be a reliable entity which can ask the agent for the right to play on their behalf and is guaranteed to behave in a pre specified way based on message received from the agent however a mediator cannot enforce behavior that is agent can play in the game directly without the mediator s help a mediator generates a new game for the player the mediated game we prove some general result about mediator and mainly concentrate on the notion of strong mediated equilibrium which is just a strong equilibrium at the mediated game we show that desired behavior which are stable against deviation by coalition can be obtained using mediator in several class of setting 
memoization is a fundamental technique in computer science providing the basis for dynamic programming this paper explores using memoization to improve the performance of rejection sampling algorithm it is shown that reusing value produced in previous sample and stored in a cache is beneficial the paper go on to explore the idea of recursive memoization in which value are aggressively reused from the cache even in the process of computing a value to store in the cache this lead to the value in the cache becoming dependent on each other and therefore produce a biased sampler however in practice this seems to be quite beneficial furthermore we show that the error in the cache tends to zero in the long run we demonstrate the usefulness of memoized sampling in a duplicate bridge simulation and in experiment with probabilistic grammar 
symmetry breaking ha been shown to be an important method to speed up the search in constraint satisfaction problem that contain symmetry when breaking symmetry by dominance detection a computationally efficient symmetry breaking scheme can be achieved if we can solve the dominance detection problem in polynomial time we study the complexity of dominance detection when value and variable symmetry appear simultaneously in constraint satisfaction problem csps with single valued variable and set csps we devise an efficient dominance detection algorithm for csps with single valued variable that yield symmetry free search tree and that is based on the abstraction to the actual intuitive structure of a symmetric csp 
we describe a new sequential learning scheme called stacked sequential learning stacked sequential learning is a meta learning algorithm in which an arbitrary base learner is augmented so a make it aware of the label of nearby example we evaluate the method on several sequential partitioning problem which are characterized by long run of identical label we demonstrate that on these problem sequential stacking consistently improves the performance of non sequential base learner that sequential stacking often improves performance of learner such a crfs that are designed specifically for sequential task and that a sequentially stacked maximum entropy learner generally outperforms crfs 
we introduce a flexible framework to specify problem solution outcome and preference among them the proposal combine idea from answer set programming asp answer set optimization aso and cp net the problem domain is structured into component asp technique are used to specify value of component a well a global intercomponent constraint among these value aso method are used to describe preference among the value of a component and cp net technique to represent inter component dependency and corresponding preference 
my thesis will contribute to the field of constraint processing and multiagent system by proposing novel scheme for achieving privacy in constraint optimization a well a new way of analyzing and understanding the privacy property of constraint optimization algorithm motivation it is my belief that there are many domain within constraint processing that motivate the need for more private algorithm and that this work will be broadly applicable however i would like to present two particular salient example in the following paragraph one domain that naturally motivates privacy preserving constraint optimization algorithm is scheduling in this domain a number of agent including say sally in human resource wish to schedule event subject to their individual constraint they wish to keep these constraint private sally s calendar for one contains the intersection of multiple professional endeavor a well a item relevant to her personal life in this case the agent involved including sally want a solution that will optimize the global social welfare but that doesn t mean they want everyone to know how sally s preference for attending cancer support group meeting led to that result 
there are numerous case where we need to reason about vehicle whose intention and itinerary are not known in advance to u for example coast guard agent tracking boat don t always know where they are headed likewise in drug enforcement application it is not always clear where drug carrying airplane which do often show up on radar are headed and how legitimate plane with an approved flight manifest can avoid them likewise traffic planner may want to understand how many vehicle will be on a given road at a given time past work on reasoning about vehicle such a the logic of motion by yaman et al yaman et al only deal with vehicle whose plan are known in advance and don t capture such situation in this paper we develop a formal probabilistic extension of their work and show that it capture both vehicle whose itinerary are known and those whose itinerary are not known we show how to correctly answer certain query against a set of statement about such vehicle a prototype implementation show our system to work efficiently in practice 
research on preference elicitation and reasoning typically focus on preference over single object of interest however in a number of application the outcome of interest are set of such atomic object for instance when creating the program for a film festival editing a newspaper or putting together a team we need to select a set of film resp article member that is optimal with respect to quality diversity cohesiveness etc this paper describes an intuitive approach for specifying preference over set of object an algorithm for computing an optimal subset given a set of candidate object and a preference specification is developed and evaluated 
a modal logic is any logic for handling modality concept like possibility necessity and knowledge artificial int elligence us modal logic most heavily to represent and reason about knowledge of agent about others knowledge this type of reasoning occurs in dialog collaboration and competition in many application it is also important to be able t o reason about the probability of belief and event in this paper we provide a formal system that represents probabilistic knowledge about probabilistic knowledge we also present exact and approximate algorithm for reasoning about the truth value of query that are encoded a probabilistic modal logic formula we provide an exact algorithm which take a probabilistic kripke structure and answer probabilistic modal query in polynomial time in the size of the model then we introduce an approximate method for application in which we have very many or infinitely many state exact method are impractical in these application and we show that our method return a close estimate efficiently over multinomial distribution blei ng jordan but are not granular enough for multiple level of complex belief about belief furthermore reasoning with these r epresentations is computationally hard because they mix structure continuous variable and discrete variable neces sary for distribution over distribution in this paper we address the need for granular representation of probabilistic belief about probabilistic belief we develop and present a representation and reasoning algorithm for nested probabilistic modality we describe syntax semantics and tractable algorithm for reasoning wit h our representation our reasoning algorithm evaluate the value of a query on a state given a model on the way to these contribution we provide a theory for probabilistic modal logic we introduce a framework for modeling probabilistic knowledge that is based on modal logic and possible world semantics with a probability distribution over the accessibility relation we introduce t wo exact method top down and bottom up that we can choose from based on the property of our application we show that when the number of nested modal function is small our top down method work faster whereas when we have complex nesting the bottom up approach which is polynomial time in the number of state is faster for very large even infinite model neither of these method is tractable therefore we use sampling to estimate the truth value of the query in our approximation method we reduce our problem to inference in bayesian network so all the exact and variational method applicable to bayesian network can be used most previous related work are limited to combining probability with a special case of modal logic in which accessibility relation are equivalence relation we call t his special case probabilistic knowledge among those fagin halpern heifetz mongin are mainly concerned with providing a sound and complete axiomatization for the logic of knowledge and probability in contrast we focus on providing tractable reasoning method to answer query on one model for the general probabilistic modal logic a well a the special case of probabilistic knowledg e another work related to ours is milch koller in which probabilistic epistemic logic is used to reason about the mental state of an agent this logic is a special case of probabilistic knowledge with the additional assumption of 
in this work we define a new framework in order to improve the knowledge representation power of answer set programming paradigm our proposal is to use notion from possibility theory to extend the stable model semantics by taking into account a certainty level expressed in term of necessity measure on each rule of a normal logic program first of all we introduce possibilistic definite logic program and show how to compute the conclusion of such program both in syntactic and semantic way the syntactic handling is done by help of a fix point operator the semantic part relies on a possibility distribution on all set of atom and we show that the two approach are equivalent in a second part we define what is a possibilistic stable model for a normal logic program with default negation again we define a possibility distribution allowing to determine the stable model 
auction are a class of multi party negotiation protocol classical auction try to maximize social welfare by select ing the highest bidder a the winner if bidder are rational this ensures that the sum of profit for all bidder and the seller is maximized in all such auction however only the winner and the seller make any profit we believe that so cial welfare distribution is a desired goal of any multi party protocol in the context of auction this goal translates into a rather radical proposal of profit sharing between all bidder and the seller we propose a profit sharing auction psa where a part of the selling price paid by the winner is paid back to the bidder the obvious criticism of this mecha nism is the incentive for the seller to share it profit with non winning bidder we claim that this loss can be compensated by attracting more bidder to such an auction resulting in an associated increase in selling price we run several set of experiment where equivalent item are concurrently sold at a first price sealed bid a vickrey and a psa auction a population of learning bidder repeatedly choose to go to one of these auction based on their valuation for the good being auctioned and their learned estimate of profit from these auction result show that seller make more or equivalent profit by using psa a compared to the classical auction additionally psa always attracts more bidder which might create auxiliary revenue stream and a desirable lower vari ability in selling price interestingly then a rational seller ha the incentive to share profit and offer an auction like psa which maximizes and distributes social welfare 
unlike the case for sequential and conditional planning much of the work on iterative planning planning where loop may be needed lean heavily on theorem proving this paper doe the following it proposes a different approach where generating plan is decoupled from verifying them describes the implementation of an iterative planner based on the situation calculus present a few example illustrating the sort of plan that can be generated show some of the strength and weakness of the approach and finally sketch the beginning of a theory where validation of plan is done offline 
we add a limited but useful form of quantification to coalition logic a popular formalism for reasoning about cooperation in game like multi agent system the basic construct of quantified coalition logic qcl allow u to express property a there exists a coalition c satisfying property p such that c can achieve we give an axiomatization of qcl and show that while it is no more expressive than coalition logic it is exponentially more succinct the time complexity of qcl model checking for symbolic and explicit state representation is shown to be no worse than that of coalition logic we illustrate the formalism by showing how to succinctly specify such social choice mechanism a majority voting which in coalition logic require specification that are exponentially long in the number of agent 
a new algorithm neighborhood minmax projection nmmp is proposed for supervised dimensionality reduction in this paper the algorithm aim at learning a linear transformation and focus only on the pairwise point where the two point are neighbor of each other after the transformation the considered pairwise point within the same class are a close a possible while those between different class are a far a possible we formulate this problem a a constrained optimization problem in which the global optimum can be effectively and efficiently obtained compared with the popular supervised method linear discriminant analysis lda our method ha three significant advantage first it is able to extract more discriminative feature second it can deal with the case where the class distribution are more complex than gaussian third the singularity problem existing in lda doe not occur naturally the performance on several data set demonstrates the effectiveness of the proposed method 
we present a new approach to ensemble classification that requires learning only a single base classifier the idea is to learn a classifier that simultaneously predicts pair of test label a opposed to learning multiple predictor for single test label then coordinating the assignment of individual label by propagating belief on a graph over the data we argue that the approach is statistically well motivated even for independent identically distributed iid data in fact we present experimental result that show improvement in classification accuracy over single example classifier across a range of iid data set and over a set of base classifier like boosting the technique increase representational capacity while controlling variance through a principled form of classifier combination 
logical filtering is the problem of tracking the possible state of a world belief state after a sequence of action and observation it is fundamental to application in partially observable dynamic domain this paper present the first exact logical filtering algorithm that is tractable for all deterministic domain our tractability result is interesting because it contrast sharply with intractability result for structured stochastic domain the key to this advance lie in using logical circuit to represent belief state we prove that both filtering time and representation size are linear in the sequence length and the input size they are independent of the domain size if the action have compact representation the number of variable in the resulting formula is at most the number of state feature we also report on a reasoning algorithm answering propositional question for our circuit which can handle question about past time step smoothing we evaluate our algorithm extensively on aiplanning domain our method outperforms competing method sometimes by order of magnitude 
restart strategy are commonly used for minimizing the computational cost of randomized algorithm but require prior knowledge of the run time distribution in order to be effective we propose a portfolio of two strategy one fixed with a provable bound on performance the other based on a model of run time distribution updated a the two strategy are run on a sequence of problem computational resource are allocated probabilistically to the two strategy based on their performance using a well known k armed bandit problem solver we present bound on the performance of the resulting technique and experiment with a satisfiability problem solver showing rapid convergence to a near optimal execution time 
in this paper we address the problem of deriving sensible information from a collection of argumentation system coming from different agent a general framework for merging argumentation system from dung s theory of argumentation is presented each argumentation system give both a set of argument and the way they interact i e attack or non attack according to the corresponding agent the aim is to define the argument system or the set of argument system that best represents the group our framework is general enough to handle the case when agent do not share the same set of argument merging argumentation system is shown a a valuable approach for defining set of argument acceptable by the group 
binary voting tree provide a succinct representation for a large and prominent class of voting rule in this paper we investigate the pac learnability of this class of rule we show that while in general a learning algorithm would require an exponential number of sample if the number of leaf is polynomial in the size of the set of alternative then a polynomial training set suffices we apply these result in an emerging theory automated design of voting rule by learning 
agent must form and update mental model about each other in a wide range of domain team coordination plan recognition social simulation user modeling game of incomplete information etc existing research typically treat the problem of forming belief about other agent a an isolated subproblem where the modeling agent start from an initial set of possible model for another agent and then maintains a belief about which of those model applies this initial set of model is typically a full specification of possible agent type although such a rich space give the modeling agent high accuracy in it belief it will also incur high cost in maintaining those belief in this paper we demonstrate that by taking this modeling problem out of it isolation and placing it back within the overall decision making context the modeling agent can drastically reduce this rich model space without sacrificing any performance our approach comprises three method the first method cluster model that lead to the same behavior in the modeling agent s decision making context the second method cluster model that may produce different behavior but produce equally preferred outcome with respect to the utility of the modeling agent the third technique sacrifice a fixed amount of accuracy by clustering model that lead to performance loss that are below a certain threshold we illustrate our framework using a social simulation domain and demonstrate it value by showing the minimal mental model space that it generates 
this paper describes work on the detection of anomalous material in text we show several variant of an automatic technique for identifying an unusual segment within a document and consider text which are unusual because of author genre biber topic or emotional tone we evaluate the technique using many experiment over large document collection created to contain randomly inserted anomalous segment in order to successfully identify anomaly in text we define more than stylistic feature to characterize writing some of which are well established stylistic determiner but many of which are novel using these feature with each of our method we examine the effect of segment size on our ability to detect anomaly allowing segment of size word word and word we show substantial improvement over a baseline in all case for all method and identify the method variant which performs consistently better than others 
knowledge worker spend the majority of their working hour processing and manipulating information these user face continual cost a they switch between task to retrieve and create information the tasktracer project at oregon state university investigates the possibility of a desktop software system that will record in detail how knowledge worker complete task and intelligently leverage that information to increase efficiency and productivity our approach assigns each observed user interface action to a task for which it is likely being performed in this demonstration we show how we have applied machine learning in this environment 
temporal reasoning is widely used within both computer science and a i however the underlying complexity of temporal proof in discrete temporal logic ha led to the use of simplified formalism and technique such a temporal interval algebra or model checking in this paper we show that tractable sub class of propositional linear temporal logic can be developed based on the use of xor fragment of the logic we not only show that such fragment can be decided tractably via clausal temporal resolution but also show the benefit of combining multiple xor fragment for such combination we establish completeness and complexity of the resolution method and also describe how such a temporal language might be used in application area for example the verification of multi agent system this new approach to temporal reasoning provides a framework in which tractable temporal logic can be engineered by intelligently combining appropriate xor fragment 
an ontology consists of a set of concept a set of constraint imposing on instance of concept and the subsumption relation it is assumed that an ontology is a tree under the subsumption relation between concept to preserve structural property of ontology the ontology revision is not only contracting ontology by discarding statement inconsistent with a revising statement but also extracting statement consistent with the revising statement and adding some other statement in the ontology revision the consistency of a revising statement with the theory of the logical closure of the ontology under the closed world assumption is discussed the basic postulate of the ontology revision are proposed and a concrete ontology revision is given based on the consistence or inconsistence of an ontology and a revising statement 
over the past few year some embedding method have been proposed for feature extraction and dimensionality reduction in various machine learning and pattern classification task among the method proposed are neighborhood preserving embedding npe locality preserving projection lpp and local discriminant embedding lde which have been used in such application a face recognition and image video retrieval however although the data in these application are more naturally represented a higher order tensor the embedding method can only work with vectorized data representation which may not capture well some useful information in the original data moreover highdimensional vectorized representation also suffer from the curse of dimensionality and the high computational demand in this paper we propose some novel tensor embedding method which unlike previous method take data directly in the form of tensor of arbitrary order a input these method allow the relationship between dimension of a tensor representation to be efficiently characterized moreover they also allow the intrinsic local geometric and topological property of the manifold embedded in a tensor space to be naturally estimated furthermore they do not suffer from the curse of dimensionality and the high computational demand we demonstrate the effectiveness of the proposed tensor embedding method on a face recognition application and compare them with some previous method extensive experiment show that our method are not only more effective but also more efficient 
in the research to date the performance of recommender system ha been extensively evaluated across various dimension increasingly the issue of robustness against malicious attack is receiving attention from the research community in previous work we have shown that knowledge of certain domain statistic is sufficient to allow successful attack to be mounted against recommender system in this paper we examine the extent of domain knowledge that is actually required and find that even when little such knowledge is known it remains possible to mount successful attack 
representing and reasoning about knowledge is critical in artificial intelligence there is a distinction between factual and ontological knowledge factual knowledge represents a set of fact about individual object that are known or believed whereas ontological background knowledge represents implicit concept and relationship that are assumed to exist in the world ontological knowledge is often represented a a hierarchy of concept because splitting thing of the real world into category and sub category is a natural way of human thinking one example of conceptual hierarchy in ai is ontology that are widely used in such area a natural language processing semantic web etc representation of both type of knowledge becomes difficult when the knowledge is imprecise one example of imprecision is granularity i e inability to distinguish between the individual object in this case knowledge cannot be represented precisely but can be approximated with respect to the granularity of the domain approximation of factual knowledge ha been extensively researched and often employ rough set theory pawlak for dealing with indiscernibility of object similar approach ha been applied to ontology to approximate concept in the hierarchy doherty et al the open problem is the approximation of hierarchical relationship such a is a part of between concept this paper address this issue using rough mereology polkowski skowron complemented with interval analysis moore the principal contribution is to provide rough mereological function that can be used for representation and reasoning with formal ontology in approximation space specifically approximate concept membership and approximate concept subsumption function will be provided it can be demonstrated that the interval based function are free of the shortcoming of the previously suggested definition cao sui zhang klinov mazlack 
web search is a challenging task previous research mainly exploit text on the web page or link information between the page while multimedia information is largely ignored this paper proposes a new framework for web search which exploit image content to help improve the search performance in this framework candidate image are retrieved at first by considering their associated text information then image related to the query are identified by analyzing the density of the visual feature space after that an image based rank of the web page is generated which is combined with the traditional keyword based search result to produce the final search result experiment demonstrate the promise of the proposed framework 
to facilitate interactive design the solution to configuration problem can be compiled into a decision diagram we develop three heuristic for reducing the time and space required to do this these heuristic are based on the distinctive clustered and hierarchical structure of the constraint graph of configuration problem the first heuristic attempt to limit the growth in the size of the decision diagram by providing an order in which constraint are added to the decision diagram the second heuristic provides an initial order for the variable within the decision diagram finally the third heuristic group variable together so that they can be reordered by a dynamic variable reordering procedure used during the construction of the decision diagram these heuristic provide one to two order magnitude improvement in the time to compile a wide range of configuration 
this paper describes a simple complete search for cumulative scheduling based on the detection and resolution of minimal critical set mc the heuristic for selecting mc relies on an estimation of the related reduction of the search space an extension of the search procedure using self adapting shaving is proposed the approach wa implemented on top of classical constraint propagation algorithm and tested on resource constrained project scheduling problem rcpsp we were able to close more than of the previously open problem of the psplib kolisch and sprecher and improve more than of the best known lower bound on those heavily studied problem other new result on open shop and cumulative job shop scheduling are reported 
recent study have shown that graph based approach are effective for semi supervised learning the key idea behind many graph based approach is to enforce the consistency between the class assignment of unlabeled example and the pairwise similarity between example one major limitation with most graph based approach is that they are unable to explore dissimilarity or negative similarity this is because the dissimilar relation is not transitive and therefore is difficult to be propagated furthermore negative similarity could result in unbounded energy function which make most graph based algorithm unapplicable in this paper we propose a new graph based approach termed a mixed label propagation which is able to effectively explore both similarity and dissimilarity simultaneously in particular the new framework determines the assignment of class label by minimizing the energy function associated with positive similarity and maximizing the energy function associated with negative similarity our empirical study with collaborative filtering show promising performance of the proposed approach 
the situation calculus and the fluent calculus are successful action formalism that share many concept but until now there is no formal relation between the two calculus that would allow to formally analyze the relationship between the two approach a well a between the programming language based on them golog and flux furthermore such a formal relation would allow to combine golog and flux and to analyze which of the underlying computation principle is better suited for different class of program we develop a formal translation between domain axiomatizations of the situation calculus and the fluent calculus and present a fluent calculus semantics for golog program for domain with deterministic action our approach allows an automatic translation of golog domain description and execution of golog program with flux 
bayesian belief net bns are often used for classification task typically to return the most likely class label for a specified instance many bn iearners however attempt to find the bn that maximizes a different objective function viz likelihood rather than classification accuracy typically by first using some model selection criterion to identify an appropriate graphical structure then finding good parameter for that structure this paper considers a number of possible criterion for selecting the best structure both generative i e based on likelihood bic bde and discriminative i e conditional bic cbic resubstitution classification error ce and bias variance bv we empirically compare these criterion against a variety of different correct bn structure both real world and synthetic over a range of complexity we also explore different way to set the parameter dealing with two issue should we seek the parameter that maximize likelihood versus the one that maximize conditional likelihood should we use i the entire training sample first to learn the best parameter and then to evaluate the model versus ii only a partition for parameter estimation and another partition for evaluation cross validation our result show that the discriminative bv model selection criterion is one of the best measure for identifying the optimal structure while the discriminative cbic performs poorly that we should use the parameter that maximize likelihood and that it is typically better to use cross validation here 
this work can be seen a a rst approach to a new planning model that take into account the possibility to express action and uents with non boolean value according to this model a planning problem is dened using both graded multi valued and classical boolean uents moreover action that can have different application degree can be dened in this work a pddl extension allowing to describe such new problem is proposed and a planning algorithm for such problem is presented 
we present a new incremental knowledge acquisition approach that incrementally improves the performance of a probabilistic search algorithm the approach address the known difficulty of tuning probabilistic search algorithm such a genetic algorithm or simulated annealing for a given search problem by the introduction of domain knowledge we show that our approach is effective for developing heuristic algorithm for difficult combinatorial problem by solving benchmark from the industrially relevant domain of vlsi detailed routing in this paper we present advanced technique for improving our knowledge acquisition approach we also present a novel method that us domain knowledge for the prioritisation of mutation operator increasing the ga s efficiency noticeably 
embedding algorithm are a method for revealing low dimensional structure in complex data most embedding algorithm are designed to handle object of a single type for which pairwise distance are specified here we describe a method for embedding object of different type such a author and term into a single common euclidean space based on their co occurrence statistic the joint distribution of the heterogenous object are modeled a exponential of squared euclidean distance in a low dimensional embedding space this construction link the problem to convex optimization over positive semidefinite matrix we quantify the performance of our method on two text datasets and show that it consistently and significantly outperforms standard method of statistical correspondence modeling such a multidimensional scaling and correspondence analysis 
in our research we investigate rational agent which consciously balance deliberation and acting and us learning to augment it reasoning it creates several partial plan us past experience to choose the best one and by executing it gam new knowledge about the world we analyse a possible application of inductive logic programming to learn how to evaluate partial plan in a resource constrained way we also discus how ilp framework can generalise partial plan 
in this paper we present a general logical framework for weighted max sat problem and study property of inference rule for branch and bound max sat solver several rule which are not equivalent but equivalent are proposed and we show that equivalent rule are also sound a an example we show how to exploit inference rule to achieve a new lower bound function for a max sat solver our new function is admissible and consistently better than the well known lower bound function based on the study of inference rule we implement an efficient solver and the experimental result demonstrate that our solver outperforms the most efficient solver that ha been implemented very recently hera and larrosa especially for large instance 
abstract conditional constraint satisfaction problem condcsps adequately capture problem change at solving time by conditionally identifying those variable and constraint that are relevant to final solution real world task with dynamic behavior such a configuration design diagnosis planning and hardware test generation have been modeled more naturally with condcsps such interest ha been matched by the development of more effective algorithm that depart from classical backtracking and incorporate local consistency checking although performance result have been reported for these specialized algorithm the experimental analysis ha been conducted separately using different test suite and little is known about the algorithm relative performance in this abstract we present a condcsp solver that implement direct and reformulation based algorithm each of which using forward checking and maintaining local consistency in our experimental analysis we have considered randomly generated condcsps of diverse topology in term of problem density and satisfiabilityof the standard and conditional problem component execution time resultsshow that there isnot one winner but that reformulation solving in conjunction with forward checking performs better on problem with larger solution set while direct solving in conjunction with maintaining arc consistency is always preferred over direct solving using forward checking a conditional constraint satisfaction problem condcsp 
in conditional visuomotor learning several arbitrary association between visual cue and motor response have to be learned by trial and error at the same time monkey a human do not achieve this task by randomly trying each possible association rather they use a strategy that organizes sequentially the acquisition of individual stimulus response association accordingly neuronal recording in the monkey striatum the main basal ganglion structure reveals two form of plasticity during learning a transient one that could constitute the neuronal correlate of the strategy and a long lasting one that could reflect the slow neuronal implementation of individual association existing model of basal ganglion function based on reinforcement learning cannot account for this dual process hence we developed a mathematical model of conditional visuomotor learning inspired from viability theory which implement both the formation of individual association and the use of strategy to organize learning 
it is often useful to represent a single example by a set of the local feature that comprise it however this representation pose a challenge to many conventional learning technique since set may vary in cardinality and the element are unordered to compare set of feature researcher often resort to solving for the least cost correspondence but this is computationally expensive and becomes impractical for large set size we have developed a general approximate matching technique called the pyramid match that measure partial match similarity in time linear in the number of feature vector per set the matching form a mercer kernel making it valid for use in many existing kernel based learning method we have demonstrated the approach for various learning task in vision and text processing and find that it is accurate and significantly more efficient than previous approach 
despite significant algorithmic advance in recent year finding optimal policy for large scale multistage stochastic combinatorial optimization problem remains far beyond the reach of existing method this paper study a complementary approach online anticipatory algorithm that make decision at each step by solving the anticipatory relaxation for a polynomial number of scenario online anticipatory algorithm have exhibited surprisingly good result on a variety of application and this paper aim at understanding their success in particular the paper derives sufficient condition under which online anticipatory algorithm achieve good expected utility and study the various type of error arising in the algorithm including the anticipativity and sampling error the sampling error is shown to be negligible with a logarithmic number of scenario the anticipativity error is harder to bound and is shown to be low both theoretically and experimentally for the existing application 
one of the main topic of research in genornics is determining the relevance of mutation described in haplotype data a cause of some genetic disease however due to technological limitation genotype data rather than haplotype data is usually obtained the haplotype inference by pure parsimony hipp problem consists in inferring haplotype from genotype s t the number of required haplotype is minimum previous approach to the hipp problem have focused on integer programming model and branch and bound algorithm in contrast this paper proposes the utilization of boolean satisfiability sat the proposed solution entail a sat model a number of key pruning technique and an iterative algorithm that enumerates the possible solution value for the target optimization problem experimental result obtained on a wide range of instance demonstrate that the sat based approach can be several order of magnitude faster than existing solution besides being more efficient the sat based approach is also the only capable of computing the solution for a large number of instance 
freebase is a practical scalable graph shaped database of structured general human knowledge inspired by semantic web research and collaborative data community such a the wikipedia freebase allows public read and write access through an http based graph query api for research the creation and maintenance of structured data and application building access is free and all data in freebase ha a very open e g creative common gfdl license 
the predictive representation hypothesis hold that particularly good generalization will result from representing the state of the world in term of prediction about possible future experience this hypothesis ha been a central motivation behind recent research in for example psrs and td network in this paper we present the first explicit investigation of this hypothesis we show in a reinforcement learning example a grid world navigation task that a predictive representation in tabular form can learn much faster than both the tabular explicit state representation and a tabular history based method 
we present a data structure and an algorithm for real time path planning of a humanoid robot due to the many degree of freedom the robot shape and available action are approximated for finding solution efficiently the resulting dimensional configuration space is searched by the a algorithm finding solution in tenth of a second on lowperformance embedded hardware experimental result demonstrate our solution for a robot in a world containing obstacle with different height stair and a higher level platform 
we introduce a new collaborative machine learning paradigm in which the user directs a learning algorithm by manually editing the automatically induced model we identify a generic architecture that support seamless interweaving of automated learning from training sample and manual edits of the model and we discus the main difficulty that the framework address we describe augmentation based learning abl the first learning algorithm that support interweaving of edits and learning from training sample we use example based on abl to outline selected advantage of the approach dealing with bad data by manually removing their effect from the model and learning a model with fewer training sample 
planning graph have been shown to be a rich source of heuristic information for many kind of planner in many case planner must compute a planning graph for each element of a set of state the naive technique enumerates the graph individually this is equivalent to solving an all pair shortest path problem by iterating a single source algorithm over each source we introduce a structure the state agnostic planning graph that directly solves the all pair problem for the relaxation introduced by planning graph the technique can also be characterized a exploiting the overlap present in set of planning graph for the purpose of exposition we first present the technique in classical planning the more prominent application of tnis technique is in belief space planning where an optimization result in drastically improved theoretical complexity our experimental evaluation quantifies this performance boost and demonstrates that heuristic belief space progression planning using our technique is competitive with the state of t the art 
this paper proposes to enhance similarity based classification by virtual attribute from imperfect domain theory we analyze how property of the domain theory such a partialness and vagueness influence classification accuracy experiment in a simple domain suggest that partial knowledge is more useful than vague knowledge however for data set from the uci machine learning repository we show that vague domain knowledge that in isolation performs at chance level can substantially increase classification accuracy when being incorporated into similarity based classification 
we report on a successful experiment of computeraided theorem discovery in the area of logic programming with answer set semantics specifically with the help of computer we discovered exact condition that capture the strong equivalence between a set of a rule and the empty set a set of a rule and another set of a rule a set s of two rule and a subset of s with one rule a set of two rule and a set of another rule and a set s of three rule and a subset of s with two rule we prove some general theorem that can help u verify the correctness of these condition and discus the usefulness of our result in program simplification 
determining the number of solution of a csp ha several application in ai in statistical physic and in guiding backtrack search heuristic it is a p complete problem for which some exact and approximate algorithm have been designed successful csp model often use high arity global constraint to capture the structure of a problem this paper exploit such structure and derives polytime evaluation of the number of solution of individual constraint these may be combined to approximate the total number of solution or used to guide search heuristic we give algorithm for several of the main family of constraint and discus the possible us of such solution count 
senseclusters is a freely available intelligent system that cluster together similar context in natural language text thereafter it assigns identifying label to these cluster based on their content it is a purely unsupervised approach that is language independent and us no knowledge other than what is available in raw un annotated corpus in addition to clustering similar context it can be used to identify synonym and set of related word it ha been applied to a diverse range of problem including proper name disambiguation word sense discrimination email organization and document clustering senseclusters is a complete system that support feature selection from large corpus several different context representation scheme various clustering algorithm the creation of descriptive and discriminating label for the discovered cluster and evaluation relative to gold standard data 
creating video recording of event such a lecture or meeting is increasingly inexpensive and easy however reviewing the content of such video may be time consuming and difficult our goal is to produce a comic book summary in which a transcript is augmented with keyframes that disambiguate and clarify accompanying text unlike most previous keyframe extraction system which rely primarily on visual cue we present a linguistically motivated approach that selects keyframes that contain salient gesture rather than learning gesture salience directly it is estimated by measuring the contribution of gesture to understanding other discourse phenomenon more specifically we bootstrap from multimodal coreference resolution to identify gesture that improve performance we then select keyframes that capture these gesture our model predicts gesture salience a a hidden variable in a conditional framework with observable feature from both the visual and textual modality this approach significantly outperforms competitive baseline that do not use gesture information 
this paper present a novel approach for extracting high quality thread title reply pair a chat knowledge from online discussion forum so a to efficiently support the construction of a chatbot for a certain domain given a forum the high quality thread title reply pair are extracted using a cascaded framework first the reply logically relevant to the thread title of the root message are extracted with an svm classifier from all the reply based on correlation such a structure and content then the extracted thread title reply pair are ranked with a ranking svm based on their content quality finally the top n thread title reply pair are selected a chatbot knowledge result from experiment conducted within a movie forum show the proposed approach is effective 
negotiation is the technique for reaching mutually beneficial agreement among agent via communication a concurrent negotiation problem occurs when an agent need to negotiate with multiple agent to reach agreement in this paper we present a protocol to support many to many bilateral multiple issue negotiation in a competitive environment the protocol is presented in the context of service oriented negotiation where one or more self interested party can provide service to one or more other party by extending existing negotiation protocol our described protocol enables both service requestors and service provider to manage several negotiation process in parallel moreover this protocol mitigates the situation where most one to many negotiation are biased in favor of one participating agent and allow the negotiation participant to make durable commitment to reduce the decommitment situation we conclude by discussing additional issue related to concurrent multiple issue negotiation 
market simulation like their real world counterpart are typically domain of high complexity high variability and incomplete information the performance of autonomous agent in these market depends both upon the strategy of their opponent and on various market condition such a supply and demand because the space for possible strategy and market condition is very large empirical analysis in these domain becomes exceedingly difficult researcher who wish to evaluate their agent must run many test game across multiple opponent set and market condition to verify that agent performance ha actually improved our approach is to improve the statistical power of market simulation experiment by controlling their complexity thereby creating an environment more conducive to structured agent testing and analysis we develop a tool that control variability across game in one such market environment the trading agent competition for supply chain management tac scm and demonstrate how it provides an efficient systematic method for tac scm researcher to analyze agent performance 
we propose a supervised maximum entropy ranking approach to pronoun resolution a an alternative to commonly used classification based approach classification approach consider only one or two candidate antecedent for a pronoun at a time whereas ranking allows all candidate to be evaluated together we argue that this provides a more natural fit for the task than classification and show that it delivers significant performance improvement on the ace datasets in particular our ranker obtains an error reduction of over the best classification approach the twin candidate model furthermore we show that the ranker offer some computational advantage over the twin candidate classifier since it easily allows the inclusion of more candidate antecedent during training this approach lead to a further error reduction of a total reduction of over the twin candidate model 
this paper present a study of the various aspect of blog reading behavior the analyzed data are obtained from a japanese weblog hosting service doblog four kind of social network are generated and analyzed citation comment trackback and blogroll network in addition the user log data are used to identify readership relation among blogger after analysis of more than user for about two year we reveal some interaction between social relation and readership relation we first show that blogger read other weblogs on a regular basis of weblogs that are read at least three time are read every five time a user log in we call this relation a regular reading relation rr relation then prediction of rr relation is done using feature from the four kind of social network lastly information diffusion on rr relation is analyzed and characterized result of this study show that the blog in rr relation have an important role in blogger activity we find the feature which have a correlation with rr relation 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
this paper is concerned with the problem of structured data extraction from web page the objective of the research is to automatically segment data record in a page extract data item field from these record and store the extracted data in a database in this paper we first introduce the extraction problem and then discus the main existing approach and their limitation after that we introduce a novel technique called depta to automatically perform web data extraction the method consists of three step identifying data record with similar pattern in a page aligning and extracting data item from the identified data record and generating tree based regular expression to facilitate later extraction from other similar page the key innovation is the proposal of a new multiple tree alignment algorithm called partial tree alignment which wa found to be particularly suitable for web data extraction this paper is based on our work published in kdd and www 
formal analysis of decentralized decision making ha become a thriving research area in recent year producing a number of multi agent extension of markov decision process while much of the work ha focused on optimizing discounted cumulative reward optimizing average reward is sometimes a more suitable criterion we formalize a class of such problem and analyze it characteristic showing that it is np complete and that optimal policy are deterministic our analysis lay the foundation for designing two optimal algorithm experimental result with a standard problem from the literature illustrate the applicability of these solution technique 
critiquing is an important form of feedback in conversational recommender system however in these system the user is usually limited to critiquing a single product feature at a time recently dynamic critiquing ha been proposed to address this shortcoming by automatically generating compound critique over multiple feature that may be presented to the user at recommendation time to date a number of different version of dynamic critiquing have been evaluated in isolation and with reference to artificial user in this paper we bring together the main flavor of dynamic critiquing and perform a large scale comparative evaluation a part of an extensive real user trial this evaluation reveals some interesting fact about the way real user interact with critique based recommenders 
we introduce an approach to autonomously creating state space abstraction for an online reinforcement learning agent using a relational representation our approach us a tree based function approximation derived from mccallum s utree algorithm we have extended this approach to use a relational representation where relational observation are represented by attributed graph mcgovern et al we address the challenge introduced by a relational representation by using stochastic sampling to manage the search space srinivasan and temporal sampling to manage autocorrelation jensen and neville relational utree incorporates iterative tree induction utgoff et al to allow it to adapt to changing environment we empirically demonstrate that relational utree performs better than similar relational learning method finney et al driessens et al in a block world domain we also demonstrate that relational utree can learn to play a sub task of the game of go called tsume go ramon et al 
this paper provides a brief introduction to recent work in statistical parsing and it application we highlight success to date remaining challenge and promising future work 
this paper proposes a novel method to characterize the performance of autonomous agent in the trading agent competition for supply chain management tac scm we create benchmarking tool that manipulate market environment to control the condition and provide guideline to test trading agent using these tool we show how developer can inspect their agent and unveil behavior that might otherwise have gone undiscovered 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
discretization defined a a set of cut over domain of attribute represents an important pre processing task for numeric data analysis some machine learning algorithm require a discrete feature space but in real world application continuous attribute must be handled to deal with this problem many supervised discretization method have been proposed but little ha been done to synthesize unsupervised discretization method to be used in domain where no class information is available furthermore existing method such a equal width or equal frequency binning are not well principled raising therefore the need for more sophisticated method for the unsupervised discretization of continuous feature this paper present a novel unsupervised discretization method that us non parametric density estimator to automatically adapt sub interval dimension to the data the proposed algorithm search for the next two sub interval to produce evaluating the best cut point on the basis of the density induced in the sub interval by the current cut and the density given by a kernel density estimator for each sub interval it us cross validated log likelihood to select the maximal number of interval the new proposed method is compared to equal width and equal frequency discretization method through experiment on well known benchmarking data 
existing graph partitioning approach are mainly based on optimizing edge cut and do not take the distribution of edge weight link distribution into consideration in this paper we propose a general model to partition graph based on link distribution this model formulates graph partitioning under a certain distribution assumption a approximating the graph affinity matrix under the corresponding distortion measure under this model we derive a novel graph partitioning algorithm to approximate a graph affinity matrix under various bregman divergence which correspond to a large exponential family of distribution we also establish the connection between edge cut objective and the proposed model to provide a unified view to graph partitioning 
grid with blocked and unblocked cell are often used to represent terrain in computer game and robotics however path formed by grid edge can be sub optimal and unrealistic looking since the possible heading are artificially constrained we present theta a variant of a that propagates information along grid edge without constraining the path to grid edge theta is simple fast and find short and realistic looking path we compare theta against both field d the only other variant of a that propagates information along grid edge without constraining the path to grid edge and a with post smoothed path although neither path planning method is guaranteed to find shortest path we show experimentally that theta find shorter and more realistic looking path than either of these existing technique 
wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than wordnet in this work we present experiment on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets existing relatedness measure perform better using wikipedia than a baseline given by google count and we show that wikipedia outperforms wordnet when applied to the largest available dataset designed for that purpose the best result on this dataset are obtained by integrating google wordnet and wikipedia based measure we also show that including wikipedia improves the performance of an nlp application processing naturally occurring text 
set variable are ubiquitous in modeling soft constraint problem but effort on practical consistency algorithm for weighted constraint satisfaction problem wcsps have only been on integer variable we adapt the classical notion of set bound consistency for wcsps and propose efficient representation scheme for set variable and common unary binary and ternary set constraint a well a cardinality constraint instead of reasoning consistency on an entire set variable directly we propose local consistency check at the set element level and demonstrate that this apparent micro management of consistency doe imply set bound consistency at the variable level in addition we prove that our framework capture classical csps with set variable and degenerate to the classical case when the weight in the problem contain only and t last but not least we verify the feasibility and efficiency of our proposal with a prototype implementation the efficiency of which is competitive against ilog solver on classical problem and order of magnitude better than wcsp model using variable to simulate set variable on soft problem 
two noteworthy model of planning in ai are probabilistic planning based on mdps and it generalization and nondeterministic planning mainly based on model checking in this paper we show that probabilistic and nondeterministic planning are extreme of a rich continuum of problem that deal simultaneously with risk and knightian uncertainty obtain a unifying model for these problem using imprecise mdps derive a simplified bellman s principle of optimality for our model and show how to adapt and analyze state of art algorithm such a l rtdp and ldfs in this unifying setup we discus example and connection to various proposal for planning under general uncertainty 
recently lakemeyer and levesque proposed a logic of only knowing which precisely capture three form of nonmonotonic reasoning moore s autoepistemic logic konolige s variant based on moderately grounded expansion and reiter s default logic default have a uniform representation under all three interpretation in the new logic moreover the logic itself is monotonic that is nonmonotonic reasoning is cast in term of validity in the classical sense while lakemeyer and levesque gave a model theoretic account of their logic a proof theoretic characterization remained open this paper fill that gap for the propositional subset a sound and complete axiom system in the new logic for all three variety of default reasoning we also present formal derivation for some example of default reasoning finally we present evidence that it is unlikely that a complete axiom system exists in the first order case even when restricted to the simplest form of default reasoning 
automation of web service composition is one of the most interesting challenge facing the semantic web today in this paper we propose a mean of performing automated web service composition by exploiting semantic matchmaking between web service parameter i e output and input to enable their connection and interaction the key idea is that the matchmaking enables at run time finding semantic compatibility among independently defined web service description to this end our approach extends existing method in order to explain misconnections between web service from this we generate web service composition that realize the goal satisfying and optimizing the semantic connection between web service moreover a process of relaxing the hard constraint is introduced in case the composition process failed our system is implemented and interacting with web service dedicated on a telecom scenario the preliminary evaluation result showed high efficiency and effectiveness of the proposed approach 
various approach have been proposed to quantify the similarity between concept in an ontology we present a novel approach that allows similarity to be asymmetric while still using only information contained in the structure of the ontology we show through experiment on the wordnet and geneontology that the new approach achieves better accuracy than existing technique 
the goal of the virtual human project at the university of southern california s institute for creative technology is to enrich virtual training environment with virtual human autonomous agent that support face to face interaction with trainee in a variety of role through bringing together many different area of research including speech recognition natural language understanding dialogue management cognitive modeling emotion modeling nonverbal behavior and speech and knowledge management the demo at aaai will focus on our work using virtual human to train negotiation skill conference attendee will negotiate with a virtual human doctor and elder to try to move a clinic out of harm s way in single and multiparty negotiation scenario using the latest iteration of our virtual human framework the user will use natural speech to talk to the embodied agent who will respond in accordance with their internal task model and state the character will carry out a multi party dialogue with verbal and nonverbal behavior a video of a single party version of the scenario wa shown at aaai this new interactive demo introduces several new feature including multiparty negotiation dynamically generated non verbal behavior and a central ontology 
we present the system nfoil it tightly integrates the na ve bayes learning scheme with the inductive logic programming rule learner foil in contrast to previous combination which have employed na ve bayes only for post processing the rule set nfoil employ the na ve bayes criterion to directly guide it search experimental evidence show that nfoil performs better than both it base line algorithm foil or the post processing approach and is at the same time competitive with more sophisticated approach 
most existing decision tree inducer are very fast due to their greedy approach in many real life application however we are willing to allocate more time to get better decision tree our recently introduced lsid contract anytime algorithm allows computation speed to be traded for better tree quality a a contract algorithm lsid must be allocated it resource a priori which is not always possible in this work we present iidt a general framework for interruptible induction of decision tree that need not be allocated resource a priori the core of our proposed framework is an iterative improvement algorithm that repeatedly selects a subtree whose reconstruction is expected to yield the highest marginal utility the algorithm then rebuilds the subtree with a higher allocation of resource iidt can also be configured to receive training example a they become available and is thus appropriate for incremental learning task empirical evaluation with several hard concept show that iidt exhibit good anytime behavior and significantly outperforms greedy inducer when more time is available a comparison of iidt to several modern decision tree learner showed it to be superior 
my research focus is on using continuous state partially observable markov decision process pomdps to perform object manipulation task using a robotic arm during object manipulation object dynamic can be extremely complex non linear and challenging to specify to avoid modeling the full complexity of possible dynamic i instead use a model which switch between a discrete number of simple dynamic model by learning these model and extending porta s continuous state pomdp framework porta et at to incorporate this switching dynamic model we hope to handle task that involve absolute and relative dynamic within a single framework this dynamic model may be applicable not only to object manipulation task but also to a number of other problem such a robot navigation by using an explicit model of uncertainty i hope to create solution to object manipulation task that more robustly handle the noisy sensory information received by physical robot 
in many framework for reasoning under inconsistency it is implicitly assumed that the formula from the belief base are connected using a weak form of conjunction when it is consistent a belief base b n where the i are propositional formula is logically equivalent to the base n however when it is not consistent both base typically lead to different conclusion this illustrates the fact that the comma used in base b ha to be considered a an additional genuine connective and not a a simple conjunction in this work we define and investigate a propositional framework with such a comma connective we give it a semantics and show how it generalizes several approach for reasoning from inconsistent belief 
the definition of a stable model ha provided a declarative semantics for prolog program with negation a failure and ha led to the development of answer set programming in this paper we propose a new definition of that concept which cover many construct used in answer set programming including disjunctive rule choice rule and conditional literal and unlike the original definition refers neither to grounding nor to fixpoints rather it is based on a syntactic transformation which turn a logic program into a formula of second order logic that is similar to the formula familiar from the definition of circumscription 
in this paper we study the classification problem involving information spanning multiple private database the privacy challenge lie in the fact that data cannot be collected in one place and the classifier itself may disclose private information we present a novel solution that build the same decision tree classifier a if data are collected in a central place but preserve the privacy of participating site 
in medical diagnosis doctor must often determine what medical test e g x ray blood test should be ordered for a patient to minimize the total cost of medical test and misdiagnosis in this paper we design cost sensitive machine learning algorithm to model this learning and diagnosis process medical test are like attribute in machine learning whose value may be obtained at cost attribute cost and misdiagnoses are like misclassifications which may also incur a cost misclassification cost we first propose an improved decision tree learning algorithm that minimizes the sum of attribute cost and misclassification cost then we design several novel test strategy that may request to obtain value of unknown attribute at cost similar to doctor ordering of medical test at cost in order to minimize the total cost for test example new patient we empirically evaluate and compare these test strategy and show that they are effective and that they outperform previous method a case study on heart disease is given 
we present black box technique for learning how to interleave the execution of multiple heuristic in order to improve average case performance in our model a user is given a set of heuristic whose only observable behavior is their running time each heuristic can compute a solution to any problem instance but it running time varies across instance the user solves each instance by interleaving run of the heuristic according to a task switching schedule we present i exact and approximation algorithm for computing an optimal task switching schedule offline ii sample complexity bound for learning a task switching schedule from training data and iii a no regret strategy for selecting task switching schedule online we demonstrate the power of our result using data from recent solver competition we outline how to extend our result to the case in which the heuristic are randomized and the user may periodically restart each heuristic with a fresh random seed 
this paper considers strategy for external memory based optimal planning an external breadth first search exploration algorithm is devised that is guaranteed to find the costoptimal solution we contribute a procedure for finding the upper bound on the locality of the search in planning graph that dictate the number of layer that have to be kept to avoid re opening we also discus an external variant of enforced hill climbing using relaxed plan heuristic without helpful action pruning we have been able to perform large exploration on metric planning problem providing better plan length than have been reported earlier a novel approach to plan reconstruction in external setting with linear i o complexity is proposed we provide external exploration result on some recently proposed planning domain 
if modal logic for coalition need to be applied one must know how to translate coalition power into agent action to these day the connection between coalition power and the action of the agent ha not been studied in the literature this paper fill the gap by presenting a semantic translation from pauly s coalition logic to a fragment of an action logic the interpretation of the empty coalition in pauly s system and the representation of the notion of ability are discussed 
this dissertation investigates the use of hierarchy and concurrency in solving sequential decision problem that require several complex activity to be done in parallel the approach is based on constraining an agent s behaviour using a multithreaded partial program the first major contribution is a language concurrent alisp for writing partial program unlike previous such language concurrent alisp is designed for problem involving the control of multiple effector each of which may be engaged in performing a different task the language allows the specification of multiple thread of control which automatically coordinate their decision in order to guarantee jointly optimal behaviour a formal semantics is provided that show that the problem of finding the optimal completion of a concurrent alisp program is equivalent to that of finding an optimal stationary policy in a related semi markov decision process the concurrent alisp language ha been implemented in lisp a part of an overall system for hierarchical reinforcement learning this system apart from serving a a reference implementation of the language contains several learning and planning algorithm and ha been designed to be modular and extensible so that new environment algorithm for planning and learning and representation for policy value function and q function can all be added very easily since it is probably the first system with these feature it architecture is described next particular learning method for concurrent alisp program are investigated first a version of a standard algorithm known a q learning is considered and shown to converge to the optimal completion next method are considered for making better use of the structure inherent in the domain it is shown that given a threadwise decomposition of the reward function the learning process itself can be decomposed across thread while maintaining global optimality furthermore within each thread the learning process can be further decomposed based on the task structure of the thread the value of these decomposition is that often each piece to be learnt need only pay attention to a small number of state variable thus significantly reducing the number of parameter to be learnt the method are then evaluated empirically on a large computer game domain that is extremely challenging for existing algorithm several technique are used to achieve a practical algorithm on this large domain including coordination graph for efficient joint decision making relational linear function approximation to reduce the number of parameter to learn and reward shaping to speed learning together the method presented in this work increase the set of domain that may be feasibly handled by reinforcement learning method they allow the specification of prior knowledge have strong formal guarantee and take advantage of both the threadwise and temporal structure of the problem 
we consider applying hierarchical reinforcement learning technique to problem in which an agent ha several effector to control simultaneously we argue that the kind of prior knowledge one typically ha about such problem is best expressed using a multithreaded partial program and present concurrent alisp a language for specifying such partial program we describe algorithm for learning and acting with concurrent alisp that can be efficient even when there are exponentially many joint choice at each decision point finally we show result of applying these method to a complex computer game domain 
we present the continuous time particle filter ctpf an extension of the discrete time particle filter for monitoring continuous time dynamic system our method apply to hybrid system containing both discrete and continuous variable the dynamic of the discrete state system are governed by a markov jump process observation of the discrete process are intermittent and irregular whenever the discrete process is observed ctpf sample a trajectory of the underlying markov jump process this trajectory is then used to estimate the continuous variable using the system dynamic determined by the discrete state in the trajectory we use the unscented kalman bucy filter to handle nonlinearities and continuous time we present result showing that ctpf is more stable in it performance than discrete time particle filtering even when the discrete time algorithm is allowed to update many more time than ctpf we also present a method for online learning of the markov jump process model that governs the discrete state 
inverse reinforcement learning irl is the problem of learning the reward function underlying a markov decision process given the dynamic of the system and the behaviour of an expert irl is motivated by situation where knowledge of the reward is a goal by itself a in preference elicitation and by the task of apprenticeship learning learning policy from an expert in this paper we show how to combine prior knowledge and evidence from the expert s action to derive a probability distribution over the space of reward function we present efficient algorithm that find solution for the reward learning and apprenticeship learning task that generalize well over these distribution experimental result show strong improvement for our method over previous heuristic based approach 
translation to boolean satisfiability is an important approach for solving state space reachability problem that arise in planning and verification many important problem however involve numeric variable for example c program or planning with resource focussing on planning we propose a method for translating such problem into propositional sat based on an approximation of reachable variable domain we compare to a more direct translation into sat modulo theory smt that is sat extended with numeric variable and arithmetic constraint though translation to sat generates much larger formula we show that it typically outperforms translation to smt almost up to the point where the formula don t fit into memory any longer we also show that even though our planner is optimal it tends to outperform state of the art sub optimal heuristic planner in domain with tightly constrained resource finally we present encouraging initial result on applying the approach to model checking 
heuristic search is used to efficiently solve the single node shortest path problem in weighted graph in practice however one is not only interested in finding a short path but an optimal path according to a certain cost notion we propose an algebraic formalism that capture many cost notion like typical quality of service attribute we thus generalize a the popular heuristic search algorithm for solving optimal path problem the paper provides an answer to a fundamental question for ai search namely to which general notion of cost heuristic search algorithm can be applied we proof correctness of the algorithm and provide experimental result that validate the feasibility of the approach 
entity matching is the problem of deciding if two given mention in the data such a helen hunt and h m hunt refer to the same real world entity numerous solution have been developed but they have not considered in depth the problem of exploiting integrity constraint that frequently exist in the domain example of such constraint include a mention with age two cannot match a mention with salary k and if two paper citation match then their author are likely to match in the same order in this paper we describe a probabilistic solution to entity matching that exploit such constraint to improve matching accuracy at the heart of the solution is a generative model that take into account the constraint during the generation process and provides well defined interpretation of the constraint we describe a novel combination of em and relaxation labeling algorithm that efficiently learns the model thereby matching mention in an unsupervised way without the need for annotated training data experiment on several real world domain show that our solution can exploit constraint to significantly improve matching accuracy by f and that the solution scale up to large data set 
we first show that the optimal and undominated outcome of an unconstrained and possibly cyclic cp net are the solution of a set of hard constraint we then propose a new algorithm for finding the optimal outcome of a constrained cp net which make use of hard constraint solving unlike previous algorithm this new algorithm work even with cyclic cp net in addition the algorithm is not tied to cp net but can work with any preference formalism which produce a preorder over the outcome we also propose an approximation method which weakens the preference ordering induced by the cp net returning a larger set of outcome but provides a significant computational advantage finally we describe a weighted constraint approach that allows to find good solution even when optimals do not exist 
we present a case based approach to multilabel ranking a recent extension of the well known problem of multilabel classification roughly speaking a multilabel ranking refines a multilabel classification in the sense that while the latter only split a predefined label set into relevant and irrelevant label the former furthermore put the label within both part of this bipartition in a total order we introduce a conceptually novel framework essentially viewing multilabel ranking a a special case of aggregating ranking which are supplemented with an additional virtual label and in which tie are permitted even though this framework is amenable to a variety of aggregation procedure we focus on a particular technique which is computationally efficient and prove that it computes optimal aggregation with respect to the generalized spearman rank correlation a an underlying loss utility function moreover we propose an elegant generalization of this loss function and empirically show that it increase accuracy for the subtask of multilabel classification 
markov model are commonly used for joint inference of label sequence unfortunately inference scale quadratically in the number of label which is problematic for training method where inference is repeatedly preformed and is the primary computational bottleneck for large label set recent work ha used output coding to address this issue by converting a problem with many label to a set of problem with binary label model were independently trained for each binary problem at a much reduced computational cost and then combined for joint inference over the original label here we revisit this idea and show through experiment on synthetic and benchmark data set that the approach can perform poorly when it is critical to explicitly capture the markovian transition structure of the large label problem we then describe a simple cascade training approach and show that it can improve performance on such problem with negligible computational overhead 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the unexplored part of the search tree will be similar to the part we have so far explored we compare these method against an old method due to knuth based on random probing we show that these method can reliably estimate the size of search tree explored by both optimization and decision procedure we also demonstrate that these method for estimating search tree size can be used to select the algorithm likely to perform best on a particular problem instance 
we describe a number of effort to engage university student with robotics through teaching and outreach teaching run the gamut from undergraduate introductory computer science to graduate level artificial intelligence course outreach involves collaboration between student and new york city public school classroom our effort have always involved team based project that culminate in demonstration or competition usually based on challenge from robocupjunior several research project have followed from these initiative 
in this paper we present a method for abstracting an environment represented using constrained delaunay triangulation in a way that significantly reduces pathfinding search effort a well a better representing the basic structure of the environment the technique shown here are ideal for object of varying size and environment that are not axis aligned or that contain many dead end long corridor or jagged wall that complicate other search technique in fact the abstraction simplifies pathfinding to deciding to which side of each obstacle to go this technique is suited to real time computation both because of it speed and because it lends itself to an anytime algorithm allowing it to work when varying amount of resource are assigned to pathfinding we test search algorithm running on both the base triangulation triangulation a ta and our abstraction triangulation reduction a tra against a and pra on grid based map from the commercial game baldur s gate and warcraft iii we find that in these case almost all path are found much faster using ta and more so using tra 
agent based modeling of human social behavior is an increasingly important research area a key factor in human social interaction is our belief about others a theory of mind whether we believe a message depends not only on it content but also on our model of the communicator how we act depends not only on the immediate effect but also on how we believe others will react in this paper we discus psychsim an implemented multiagent based simulation tool for modeling interaction and influence while typical approach to such modeling have used first order logic psych sim agent have their own decision theoretic model of the world including belief about it environment and recursive model of other agent using these quantitative model of uncertainty and preference we have translated existing psychological theory into a decision theoretic semantics that allow the agent to reason about degree of believability in a novel way we discus psychsim s underlying architecture and describe it application to a school violence scenario for illustration 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
mitchell and ternovska proposed a constraint programming framework based on classical logic extended with inductive definition they formulate a search problem a the problem of model expansion mx which is the problem of expanding a given structure with new relation so that it satisfies a given formula their long term goal is to produce practical tool to solve combinatorial search problem especially those in np in this framework a problem is encoded in a logic an instance of the problem is represented by a finite structure and a solver generates solution to the problem this approach relies on propositionalisation of high level specification and on the efficiency of modern sat solver here we propose an efficient algorithm which combine grounding with partial evaluation since the mx framework is based on classical logic we are able to take advantage of known result for the so called guarded fragment in the case of k guarded formula with inductive definition under a natural restriction the algorithm performs much better than naive grounding by relying on connection between k guarded formula and tree decomposition 
we describe how a physical robot can learn about object from it own autonomous experience in the continuous world the robot identifies statistical regularity that allow it to represent a physical object with a cluster of sensation that violate a static world model track that cluster over time extract percept from that cluster form concept from similar percept and learn reliable action that can be applied to object we present a formalism for representing the ontology for object and action a learning algorithm and the result of an evaluation with a physical robot 
choosing good feature to represent object can be crucial to the success of supervised machine learning algorithm good high level feature are those that concentrate information about the classification task such feature can often be constructed a non linear combination of raw or native input feature such a the pixel of an image using many nonlinear combination a do svms can dilute the classification information necessitating many training example on the other hand searching even a modestly expressive space of nonlinear function for high information one can be intractable we describe an approach to feature construction where task relevant discriminative feature are automatically constructed guided by an explanation based interaction of training example and prior domain knowledge we show that in the challenging task of distinguishing handwritten chinese character our automatic feature construction approach performs particularly well on the most difficult and complex character pair 
we consider the problem of identifying equivalence of two knowledge base which are capable of abductive reasoning here a knowledge base is written in either first order logic or nonmonotonic logic programming in this work we will give two definition of abductive equivalence the first one explainable equivalence requires that two abductive program have the same explainability for any observation another one explanatory equivalence guarantee that any observation ha exactly the same explanation in each abductive framework explanatory equivalence is a stronger notion than explainable equivalence in first order abduction explainable equivalence can be verified by the notion of extensional equivalence in default theory in nonmonotonic logic program explanatory equivalence can be checked by mean of the notion of relative strong equivalence we also show the complexity result for abductive equivalence 
we present a bayesian formulation of locally weighted learning lwl using the novel concept of a randomly varying coefficient model based on this we propose a mechanism for multivariate non linear regression using spatially localised linear model that learns completely independent of each other us only local information and adapts the local model complexity in a data driven fashion we derive online update for the model parameter based on variational bayesian em the evaluation of the proposed algorithm against other state of the art method reveal the excellent robust generalization performance beside surprisingly efficient time and space complexity property this paper for the first time brings together the computational efficiency and the adaptability of non competitive locally weighted learning scheme and the modelling guarantee of the bayesian formulation 
automatic tool for finding software error require knowledge of the rule a program must obey or specification before they can identify bug we present a method that combine factor graph and static program analysis to automatically infer specification directly from program we illustrate the approach on inferring function in c program that allocate and release resource and evaluate the approach on three codebases sdl openssh and the o kernel for mac o x xnu the inferred specification are highly accurate and with them we have discovered numerous bug 
enforcing arc consistency ac during search ha proven to be a very effective method in solving constraint satisfaction problem and it ha been widely used in many constraint programming system although much effort ha been made to design efficient standalone ac algorithm there is no systematic study on how to efficiently enforce ac during search a far a we know the significance of the latter is clear given the fact that ac will be enforced million of time in solving hard problem in this paper we propose a framework for enforcing ac during search ac and complexity measurement of ac algorithm based on this framework several ac algorithm are designed to take advantage of the residual data left in the data structure by the previous invocation s of ac the algorithm vary in the worst case time and space complexity and other complexity measurement empirical study show that some of the new ac algorithm perform better than the conventional implementation of ac algorithm in a search procedure 
for many classification task a large number of instance available for training are unlabeled and the cost associated with the labeling process varies over the input space meanwhile virtually all these problem require classifier that minimize a nonuniformloss functionassociatedwith the classification decision rather than the accuracy or number of error for example to train pattern classification model for a network intrusion detection task expert need to analyze network event and assign them label this can be a very costly procedure if the instance to be labeled are selected at random in the meantime the loss associated with mislabeling an intrusion is much higher than the loss associated with the opposite error i e labeling a legal event a being an intrusion a a result to address these type of task practitioner need tool that minimize the total cost computed a a sum of the cost of labeling and the loss associated with the decision this paper describes an approach for addressing this problem 
this paper describes initial experiment in applying knowledge derived from hypertext structure to domain specific automatic keyphrase extraction it is found that hyperlink information can improve the effectiveness of automatic keyphrase extraction by however the primary goal of this project is to apply similar technique to information retrieval task such a web searching these initial result show promise for the applicability of these technique to more far reaching task 
analogy is a powerful boundary transcending process that exploit a conceptual system s ability to perform controlled generalization in one domain and re specialization into another the result of this semantic leap is the transference of meaning from one concept to another from which metaphor derives it name literally to carry over such generalization and respecialization can be achieved using a variety of representation and technique most notably abstraction via a taxonomic backbone or selective projection via structure mapping on propositional content in this paper we explore the extent to which a bilingual lexical ontology for english and chinese called hownet can support both approach to analogy 
in this paper we provide an algebraic approach to markov decision process mdps which allows a unified treatment of mdps and includes many existing model quantitative or qualitative a particular case in algebraic mdps reward are expressed in a semiring structure uncertainty is represented by a decomposable plausibility measure valued on a second semiring structure and preference over policy are represented by generalized expected utility we recast the problem of finding an optimal policy at a finite horizon a an algebraic path problem in a decision rule graph where arc are valued by function which justifies the use of the jacobi algorithm to solve algebraic bellman equation in order to show the potential of this general approach we exhibit new variation of mdps admitting complete or partial preference structure a well a probabilistic or possibilistic representation of uncertainty 
efficient query answering over ontology is one of the most useful and important service to support semantic web application approximation ha been identified a a potential way to reduce the complexity of query answering over owl dl ontology existing approach are mainly based on syntactic approximation of ontological axiom and query in this paper we propose to recast the idea of knowledge compilation into approximating owl dl ontology with dl lite ontology against which query answering ha only polynomial data complexity we identify a useful category of query for which our approach guarantee also completeness furthermore this paper report on the implementation of our approach in the ontosearch system and preliminary but encouraging benchmark result which compare ontosearch s response time on a number of query with those of existing ontology reasoning system 
cutset conditioning is one of the method of solving reasoning task for graphical model especially when space restriction make inference e g jointree clustering algorithm infeasible the w cutset is a natural extension of the method to a hybrid algorithm that performs search on the conditioning variable and inference on the remaining problem of induced width bounded by w this paper take a fresh look at these method through the spectrum of and or search space for graphical model the resulting and or cutset method is a strict improvement over the traditional one often by exponential amount 
recovering from power outage is an essential task in distribution of electricity our industrial partner postulate that the recovery should be interactive rather than automatic supporting the operator by preventing choice that destabilize the network interactive configurators successfully used in specifying product and service support user in selecting logically constrained parameter in a sound complete and backtrack free manner interactive restoration algorithm based on reduced ordered binary decision diagram bdds had been developed also for power distribution network however they did not scale to the large instance a bdds representing these could not be compiled we discus the theoretical hardness of the interactive configuration and then provide technique used to compile two class of network we handle the largest industrial instance available our technique rely on symbolic reachability computation early variable quantification domain specific ordering heuristic and conjunctive decomposition 
we analyse pollack and blair s hc gammon backgammon program using a new technique that performs monte carlo simulation to derive a markov chain model for imperfect comparison algorithm called the m ical method which model the behavior of the algorithm using a markov chain each of whose state represents a class of player of similar strength the markov chain transition matrix is populated using monte carlo simulation once generated the matrix allows fairly accurate prediction of the expected solution quality standard deviation and time to convergence of the algorithm this allows u to make some observation on the validity of pollack and blair s conclusion and also show the application of the m ical method on a previously published work 
support vector machine svm have been highly successful in many machine learning problem recently it is also used for multi instance mi learning by employing a kernel that is defined directly on the bag a only the bag but not the instance have known label this mi kernel implicitly assumes all instance in the bag to be equally important however a fundamental property of mi learning is that not all instance in a positive bag necessarily belong to the positive class and thus different instance in the same bag should have different contribution to the kernel in this paper we address this instance label ambiguity by using the method of marginalized kernel it first assumes that all the instance label are available and defines a label dependent kernel on the instance by integrating out the unknown instance label a marginalized kernel defined on the bag can then be obtained a desirable property is that this kernel weight the instance pair by the consistency of their probabilistic instance label experiment on both classification and regression data set show that this marginalized mi kernel when used in a standard svm performs consistently better than the original mi kernel it also outperforms a number of traditional mi learning method 
in a team based multiagent system the ability to construct a model of an opponent team s joint behavior can be useful for determining an agent s expected distribution over future world state and thus can inform it planning of future action this paper present an approach to team opponent modeling in the context of the robocup simulation coach competition specifically it introduces an autonomous coach agent capable of analyzing past game of the current opponent advising it own team how to play against this opponent and identifying pattern or weakness on the part of the opponent our approach is fully implemented and tested within the robocup soccer server and wa the champion of the robocup simulation coach competition 
entity resolution is a critical component of data integration where the goal is to reconcile database reference corresponding to the same real world entity given the abundance of publicly available database that have unresolved entity we motivate the problem of quick and accurate resolution for answering query over such unclean database since collective entity resolution approach where related reference are resolved jointly have been shown to be more accurate than independent attribute based resolution we focus on adapting collective resolution for answering query we propose a two stage collective resolution strategy for processing query we then show how it can be performed on the fly by adaptively extracting and resolving those database reference that are the most helpful for resolving the query we validate our approach on two large real world publication database where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategy for query processing we then show how the same query can be answered in real time using our adaptive approach while preserving the gain of collective resolution this work extends work presented in bhattacharya licamele getoor 
in this paper nogood recording is investigated within the randomization and restart framework our goal is to avoid the same situation to occur from one run to the next one more precisely no good are recorded when the current cutoff value is reached i e before restarting the search algorithm such a set of nogoods is extracted from the last branch of the current search tree interestingly the number of nogoods recorded before each new run is bounded by the length of the last branch of the search tree a a consequence the total number of recorded nogoods is polynomial in the number of restarts experiment over a wide range of csp instance demonstrate the effectiveness of our approach 
with increasing deployment of system involving multiple coordinating agent there is a growing need for diagnosing coordination failure in such system previous work presented centralized method for coordination failure diagnosis however these are not always applicable due to the significant computational and communication requirement and the brittleness of a single point of failure in this paper we propose a distributed approach to model based coordination failure diagnosis we model the coordination between the agent a a constraint graph and adapt several algorithm from the distributed csp area to use a the basis for the diagnosis algorithm we evaluate the algorithm in extensive experiment with simulated and real sony aibo robot and show that in general a trade off exists between the computational requirement of the algorithm and their diagnosis result surprisingly in contrast to result in distributed csps the asynchronous backtracking algorithm outperforms stochastic local search in term of both quality and runtime 
nonparametric bayesian mixture model in particular dirichlet process dp mixture model have shown great promise for density estimation and data clustering given the size of today s datasets computational efficiency becomes an essential ingredient in the applicability of these technique to real world data we study and experimentally compare a number of variational bayesian vb approximation to the dp mixture model in particular we consider the standard vb approximation where parameter are assumed to be independent from cluster assignment variable and a novel collapsed vb approximation where mixture weight are marginalized out for both vb approximation we consider two different way to approximate the dp by truncating the stick breaking construction and by using a finite mixture model with a symmetric dirichlet prior 
we present a novel approach to solving quantified boolean formula qbf that combine a search based qbf solver with marhine learning technique we show how classification method can be used to predict run time and to choose optimal heuristic both within a portfolio based and within a dynamic online approach in the dynamic method variable are set to a truth value according to a scheme that try to maximize the probability of successfully solving the remaining sub problem efficiently since each variable assignment can drastically change the problem structure new heuristic are chosen dynamically and a classifier is used online to predict the usefulness of each heuristic experimental result on a large corpus of example problem show the usefulness of our approach in term of run time a well a the ability to solve previously unsolved problem instance 
model based diagnosis ha largely operated on hard ware system however in most complex system today hardware is augmented with software function that influence the system s behavior in this paper hard ware model are extended to include the behavior of associated embedded software resulting in more comprehensive diagnosis prior work introduced probabilistic hierarchical constraint based automaton phca to allow the uniform and compact encoding of both hard ware and software behavior this paper focus on phca based monitoring and diagnosis to ensure the robustness of complex system we introduce a novel approach that frame diagnosis over a finite time horizon a a soft constraint optimization problem cop allowing u to leverage an extensive body of efficient solution method for cop the solution to the cop correspond to the most likely evolution of the complex system we demonstrate our approach on a vision based rover navigation system and model of the sphere and earth observing one spacecraft 
linear discriminant analysis lda is a popular data analytic tool for studying the class relationship between data point a major disadvantage of lda is that it fails to discover the local geometrical structure of the data manifold in this paper we introduce a novel linear algorithm for discriminant analysis called locality sensitive discriminant analysis lsda when there is no sufficient training sample local structure is generally more important than global structure for discriminant analysis by discovering the local manifold structure lsda find a projection which maximizes the margin between data point from different class at each local area specifically the data point are mapped into a subspace in which the nearby point with the same label are close to each other while the nearby point with different label are far apart experiment carried out on several standard face database show a clear improvement over the result of lda based recognition 
traditional supervised learning deal with labeled instance in many application such a physiological data modeling and speaker identification however training example are often labeled object and each of the labeled object consists of multiple unlabeled instance when classifying a new object it class is determined by the majority of it instance class a a consequence of this decision rule one challenge to learning with labeled object or session is to determine during training which subset of the instance inside an object should belong to the class of the object we call this type of learning session based learning to distinguish it from the traditional supervised learning in this paper we introduce session based learning problem give a formal description of session based learning in the context of related work and propose an approach that is particularly designed for session based learning empirical study with uci datasets and real world data show that the proposed approach is effective for session based learning 
beam search reduces the memory consumption of bestrst search at the cost of nding longer path but it memory consumption can still exceed the given memory capacity quickly we therefore develop bulb beam search using limited discrepancy backtracking a complete memory bounded search method that is able to solve more problem instance of large search problem than beam search and doe so with a reasonable runtime at the same time bulb tends to nd shorter path than beam search because it is able to use larger beam width without running out of memory we demonstrate these property of bulb experimentally for three standard benchmark domain 
we demonstrate textrank a system for unsupervised extractive summarization that relies on the application of iterative graphbased ranking algorithm to graph encoding the cohesive structure of a text an important characteristic of the system is that it doe not rely on any language specific knowledge resource or any manually constructed training data and thus it is highly portable to new language or domain 
finding a constraint network that will be efficiently solved by a constraint solver requires a strong expertise in constraint programming hence there is an increasing interest in automatic reformulation this paper present a general framework for learning implied global constraint in a constraint network assumed to be provided by a non expert user the learned global constraint can then be added to the network to improve the solving process we apply our technique to global cardinality constraint experiment show the significance of the approach 
we present a general machine learning framework for modelling the phenomenon of missing information in data we propose a masking process model to capture the stochastic nature of information loss learning in this context is employed a a mean to recover a much of the missing information a is recoverable we extend the probably approximately correct semantics to the case of learning from partial observation with arbitrarily hidden attribute we establish that simply requiring learned hypothesis to be consistent with observed value suffices to guarantee that hidden value are recoverable to a certain accuracy we also show that in some sense this is an optimal strategy for achieving accurate recovery we then establish that a number of natural concept class including all the class of monotone formula that are pac learnable by monotone formula and the class of conjunction disjunction k cnf k dnf and linear threshold are consistently learnable from partial observation we finally show that the concept class of parity and monotone term decision list are not properly consistently learnable from partial observation if rp np this implies a separation of what is consistently learnable from partial observation versus what is learnable in the complete or noisy setting 
i hypothesize that learning a vocabulary to communicate between component of a system is equivalent to general learning moreover i assert that some problem of general learning such a eliminating bad hypothesis deepening shallow representation and generation of near miss will become simpler when refactored into communication learning problem 
we investigate how inverse feature can be added to a boolean complete description logic with path functional dependency in way that avoid undecidability of the associated logical implication problem in particular we present two condition that ensure the problem remains exptime complete the first is syntactic in nature and limit the form that dependency may have in argument terminology the second is a coherence condition on terminology that is sufficiently weak to allow the transfer of relational and emerging object oriented normalization technique 
this paper aim to investigate methodology to utilize an agent s intention a a mean to guide the revision of it belief for this purpose we develop a collection of belief revision operator that employ the effect of the revision on the agent s intention a the selection criterion these operator are then assessed for rationality against the traditional agm postulate there is a large volume of work concerned with classical beliefrevision the primary issue of which is the mitigation of the uncertainty inherent in environment in which belief revision is necessary traditional approach attempt to ass the explanatory power of belief and utilize this a a heuristic to resolve this ambiguity we argue that for practical reasoning system whose primary focus lie in the maintenance of behavior and not information an agent s intention provide a better guide 
the agm postulate for belief revision augmented by the dp postulate for iterated belief revision provide widely accepted criterion for the design of operator by which intelligent agent adapt their belief incrementally to new information these postulate alone however are too permissive they support operator by which all newly acquired information is canceled a soon a an agent learns a fact that contradicts some of it current belief in this paper we present a formal analysis of the deficiency of the standard postulate alone and we show how to solve the problem by an additional postulate of independence we give a representation theorem for this postulate and prove that it is compatible with agm and dp 
in this paper we present an unsupervised model for learning arbitrary relation between concept of a molecular biology ontology for the purpose of supporting text mining and manual ontology building relation between named entity are learned from the genia corpus by mean of several standard natural language processing technique an in depth analysis of the output of the system show that the model is accurate and ha good potential for text mining and ontology building application 
many dynamic system involve a number of entity that are largely independent of each other but interact with each other via a subset of state variable we present global local dynamic model gldms to capture these kind of system in a gldm the state of an entity is decomposed into a globally influenced state that depends on other entity and a locally influenced state that depends only on the entity itself we present an inference algorithm for gldms called global local particle filtering that introduces the principle of reasoning globally about global dynamic and locally about local dynamic we have applied gldms to an asymmetric urban warfare environment in which enemy unit form team to attack important target and the task is to detect such team a they form experimental result for this application show that global local particle filtering outperforms ordinary particle filtering and factored particle filtering 
although nogood learning in csps and clause learning in sat are formally equivalent nogood learning ha not been a successful a technique in csp solver a clause learning ha been for sat solver we show that part of the reason for this discrepancy is that nogoods in csps a standardly defined are too restrictive in this paper we demonstrate that these restriction can be lifted so that a csp solver can learn more general and powerful nogoods nogoods generalized in this manner yield a provably more powerful csp solver we also demonstrate how generalized nogoods facilitate learning useful nogoods from global constraint finally we demonstrate empirically that generalized nogoods can yield significant improvement in performance 
logical filtering is the process of updating a belief state set of possible world state after a sequence of executed action and perceived observation in general it is intractable in dynamic domain that include many object and relationship still potential application for such domain e g semantic web autonomous agent and partial knowledge game encourage research beyond immediate intractability result in this paper we present polynomial time algorithm for filtering belief state that are encoded a first order logic fol formula we sidestep previous discouraging result and show that our algorithm are exact in many case of interest these algorithm accept belief state in full fol which allows natural representation with explicit reference to unidentified object and partially known relationship our algorithm keep the encoding compact for important class of action such a strip action these result apply to most expressive modeling language such a partial database and belief revision in fol 
the genome rearrangement problem is to find the most economical explanation for observed difference between the gene order of two genome such an explanation is provided in term of event that change the order of gene in a genome we present a new approach to the genome rearrangement problem according to which this problem is viewed a the problem of planning rearrangement event that transform one genome to the other this method differs from the existing one in that we can put restriction on the number of event specify the cost of event with function possibly based on the length of the gene fragment involved and add constraint controlling search with this approach we have described genome rearrangement in the action description language adl and studied the evolution of metazoan mitochondrial genome and the evolution of campanulaceae chloroplast genome using the planner tlplan we have observed that the phylogeny reconstructed using this approach conform with the most widely accepted one 
this paper introduces an innovative approach for automated negotiating using the gender of human opponent our approach segment the information acquired from previous opponent store it in two database and model the typical behavior of male and of female the two model are used in order to match an optimal strategy to each of the two subpopulation in addition to the basic separation we propose a learning algorithm which supply an online indicator for the gender separability level of the population which tune the level of separation the algorithm activates the algorithm we present can be generally applied in different environment with no need for configuration of parameter experiment in different one shot domain comparing the performance of the gender based separation approach with a basic approach which is not gender sensitive revealed higher payoff of the former in almost all the domain moreover using the proposed learning algorithm further improved the result 
in recent year metric learning in the semisupervised setting ha aroused a lot of research interest one type of semi supervised metric learning utilizes supervisory information in the form of pairwise similarity or dissimilarity constraint however most method proposed so far are either limited to linear metric learning or unable to scale up well with the data set size in this paper we propose a nonlinear metric learning method based on the kernel approach by applying low rank approximation to the kernel matrix our method can handle significantly larger data set moreover our low rank approximation scheme can naturally lead to out of sample generalization experiment performed on both artificial and real world data show very promising result 
we study auction like algorithm for the distributed allocation of task to cooperating agent to reduce the team cost of sequential single item auction algorithm we generalize them to assign more than one additional task during each round which increase their similarity to combinatorial auction algorithm we show that for a given number of additional task to be assigned during each round every agent need to submit only a constant number of bid per round and the runtime of winner determination is linear in the number of agent the communication and winner determination cost do not depend on the number of task and thus scale to a large number of task for small bundle size we then demonstrate empirically that the team cost of sequential bundle bid single sale single item auction algorithm can be substantially smaller than that without bundle for multi agent routing problem with capacity constraint 
the one against all reduction from multiclass classification to binary classification is a standard technique used to solve multiclass problem with binary classifier we show that modifying this technique in order to optimize it error transformation property result in a superior technique both experimentally and theoretically this algorithm can also be used to solve a more general classification problem multi label classification which is the same a multiclass classification except that it allows multiple correct label for a given example 
an important issue for temporal planner is the ability to handle temporal uncertainty we revisit the question of how to determine whether a given set of temporal requirement are feasible in the light of uncertain duration of some process in particular we consider how best to determine whether a network is dynamically controllable i e whether a dynamic strategy exists for executing the network that is guaranteed to satisfy the requirement previous work ha shown the existence of a pseudo polynomial algorithm for testing dynamic controllability here we simplify the previous framework and present a strongly polynomial algorithm with a termination criterion based on the structure of the network 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
the em algorithm heavily relies on the interpretation of observation a incomplete data but it doe not have any control on the uncertainty of missing data to effectively reduce the uncertainty of missing data we present a regularized em algorithm that penalizes the likelihood with the mutual information between the missing data and the incomplete data or the conditional entropy of the missing data given the observation the proposed method maintains the advantage of the conventional em algorithm such a reliable global convergence low cost per iteration economy of storage and ease of programming we also apply the regularized em algorithm to fit the finite mixture model our theoretical analysis and experiment show that the new method can efficiently fit the model and effectively simplify over complicated model 
the robot intelligence kernel rik is a portable reconfigurable suite of perceptual behavioral and cognitive capability that can be used across many different platform environment and task the rik coupled with a virtual d interface have been shown to dramatically improve human robot interaction across a variety of navigation and exploration task 
a novel training algorithm for nonlinear discriminants for classification and regression in reproducing kernel hilbert space rkhss is presented it is shown how the overdetermined linear least square problem in the corresponding rkhs may be solved within a greedy forward selection scheme by updating the pseudoinverse in an order recursive way the described construction of the pseudoinverse give rise to an update of the orthogonal decomposition of the reduced gram matrix in linear time regularization in the spirit of ridge regression may then easily be applied in the orthogonal space various experiment for both classification and regression are performed to show the competitiveness of the proposed method 
research on inductive process modeling combine background knowledge with time series data to construct explanatory model but previous work ha placed few constraint on search through the model space we present an extended formalism that organizes process knowledge in a hierarchical manner and we describe hipm a system that carry out constrained search for hierarchical process model we report experiment that suggest this approach produce more accurate and plausible model with le effort we conclude by discussing related research and direction for future work 
although robotics researcher commonly contend that robot should not look too humanlike many artforms have successfully depicted people and have come to be accepted a great and important work with example such a rodin s thinker mary cassat s infant and disney s abe lincoln simulacrum extending this tradition to intelligent robotics the author have depicted late sci fi writer philip k dick with an autonomous intelligent android in doing so the author aspire to bring robotic system up to the level of great art while using the technology a a mirror for examining human nature in social ai development and cognitive science experiment 
we introduce relational gram r gram they upgrade n gram for modeling relational sequence of atom a n gram r gram are based on smoothed n th order markov chain smoothed distribution can be obtained by decreasing the order of the markov chain a well a by relational generalization of the r gram to avoid sampling object identifier in sequence r gram are generative model at the level of variablized sequence with local object identity constraint these sequence define equivalence class of ground sequence in which element are identical up to local identifier renaming the proposed technique is evaluated in several domain including mobile phone communication log unix shell user modeling and protein fold prediction based on secondary protein structure 
in most research on reasoning about action and reasoning about narrative one either reason about hypothetical execution of action or about action that actually occurred in this paper we develop a high level language that allows the expression of intended or planned action sequence unlike observed action occurrence planned or intended action occurrence may not actually take place but often when they do not take place they persist and happen at an opportune future time we give the syntax and semantics for expressing such intention we then give a logic programming axiomatization and show the correspondence between the semantics of a description in the high level language and the answer set of the corresponding logic programming axiomatization we illustrate the application of our formalism with respect to reasoning about trip 
this paper present a logical axiomatization of bargaining solution a bargaining situation is described in propositional logic and the bargainer preference are quantified in term of the logical structure of the bargaining situation a solution to the n person bargaining problem is proposed based on the maxmin rule over the degree of bargainer satisfaction we show that the solution is uniquely characterized by four natural and intuitive axiom a well a three other fundamental assumption all the axiom and assumption are represented in logical statement and most of them have a game theoretic counterpart the framework would help u to identify the logical and numerical reasoning behind bargaining process this paper aim to develop a logical theory of bargaining in cooperative model we shall represent bargainer negotiation item in propositional logic and quantify the bargainer preference over their negotiation item in accordance with the logical structure of the item we propose a solution to the n person bargaining problem based 
solo is a cognitive assistive device which provides support in remembering when to perform task executing the step in a task and recovering from unexpected event the system includes an interface for client to receive reminder an interface for caregiver to enter information about the client s scheduled task and a cognition manager which provides reminder and task guidance at appropriate time 
in this extended abstract we present a framework of reasoning with inconsistent ontology in which pre defined selection function are used to deal with concept relevance we examine how the notion of concept relevance can be used for reasoning with inconsistent ontology we have implemented a prototype called pion processing inconsistent ontology which is based on a syntactic relevance based selection function in this paper we also report the experiment with pion framework the classical entailment in logic is explosive any formula is a logical consequence of a contradiction therefore conclusion drawn from an inconsistent knowledge base by classical inference may be completely meaningless the general task of an inconsistency reasoner is given an inconsistent ontology return meaningful answer to query in this paper we propose a general framework for reasoning with inconsistent ontology our approach borrows some idea from schaerf and marco cadoli s approximation approach marquis and porquet s paraconsistent reasoning approach and chopra parikh and wassermann s relevance approach however our main idea is given a selection function which can be defined on the syntactic or semantic relevance we select some consistent sub theory from an inconsistent ontology then we apply standard reasoning on the selected sub theory to find meaningful answer if a satisfying answer cannot be found the relevance degree of the selection function is made le restrictive thereby extending the consistent subtheory for further reasoning the main contribution of this paper are a set of formal definition to capture reasoning with inconsistent ontology a general framework for reasoning with inconsistent ontology based on selection function and some preliminary experiment with an implementation of this framework using a rather simple selection function for reasoning with inconsistent ontology we argue that it is more suitable to use belnap s four valued logic to distinguish the following four epistemic state for query answer i overdetermined both a query and it negation are derivable ii accepted only a query and no it negation is derivable iii rejected only the negation of a query is derivable and iv undetermined both a query and it negation are not derivable 
feature interaction present a challenge to feature selection for classification a feature by itself may have little correlation with the target concept but when it is combined with some other feature they can be strongly correlated with the target concept unintentional removal of these feature can result in poor classification performance handling feature interaction can be computationally intractable recognizing the presence of feature interaction we propose to efficiently handle feature interaction to achieve efficient feature selection and present extensive experimental result of evaluation 
the particle filter ha emerged a a useful tool for problem requiring dynamic state estimation the efficiency and accuracy of the filter depend mostly on the number of particle used in the estimation and on the propagation function used to re allocate these particle at each iteration both feature are specified beforehand and are kept fixed in the regular implementation of the filter in practice this may be highly inappropriate since it ignores error in the model and the varying dynamic of the process this work present a self adaptive version of the particle filter that us statistical method to adapt the number of particle and the propagation function at each iteration furthermore our method present similar computational load than the standard particle filter we show the advantage of the self adaptive filter by applying it to a synthetic example and to the visual tracking of target in a real video sequence 
the valued vcsp framework is a generic optimization framework with a wide range of application soft arc consistency operation transform a vcsp into an equivalent problem by shifting weight between cost function the principal aim is to produce a good lower bound on the cost of solution an essential ingredient of a branch and bound search but soft ac is much more complex than traditional ac there may be several closure fixpoints and finding the closure with a maximum lower bound ha been shown to be np hard for integer cost cooper and schiex we introduce a relaxed variant of soft arc consistency using rational cost in this case an optimal closure can be found in polynomial time furthermore for finite rational cost the associated lower bound is shown to provide an optimal arc consistent reformulation of the initial problem preliminary experiment on random and structured problem are reported showing the strength of the lower bound produced 
computer have already eclipsed the level of human play in competitive scrabble but there remains room for improvement in particular there is much to be gained by incorporating information about the opponent s tile into the decision making process in this work we quantify the value of knowing what letter the opponent ha we use observation from previous play to predict what tile our opponent may hold and then use this information to guide our play our model of the opponent based on bayes theorem sacrifice accuracy for simplicity and ease of computation but even with this simplified model we show significant improvement in play over an existing scrabble program these empirical result suggest that this simple approximation may serve a a suitable substitute for the intractable partially observable markov decision process although this work focus on computer v computer scrabble play the tool developed can be of great use in training human to play against other human 
we describe a point based policy iteration pbpi algorithm for infinite horizon pomdps pbpi replaces the exact policy improvement step of hansen s policy iteration with point based value iteration pbvi despite being an approximate algorithm pbpi is monotonic at each iteration before convergence pbpi produce a policy for which the value increase for at least one of a finite set of initial belief state and decrease for none of these state in contrast pbvi cannot guarantee monotonic improvement of the value function or the policy in practice pbpi generally need a lower density of point coverage in the simplex and tends to produce superior policy with le computation experiment on several benchmark problem up to state demonstrate the scalability and robustness of the pbpi algorithm 
in machine translation document alignment refers to finding correspondence between document which are exact translation of each other we define pseudo alignment a the task of finding topical a opposed to exact correspondence between document in different language we apply semisupervised method to pseudo align multilingual corpus specifically we construct a topicbased graph for each language then given exact correspondence between a subset of document we project the unaligned document into a shared lower dimensional space we demonstrate that close document in this lower dimensional space tend to share the same topic this ha application in machine translation and cross lingual information analysis experimental result show that pseudo alignment of multilingual corpus is feasible and that the document alignment produced are qualitatively sound our technique requires no linguistic knowledge of the corpus on average when of the corpus consists of exact correspondence an on topic correspondence occurs within the top foreign neighbor in the lowerdimensional space while the exact correspondence occurs within the top foreign neighbor in this this space we also show how to substantially improve these result with a novel method for incorporating language independent information 
there are many framework for modelling argumentation in logic they include a formal representation of individual argument and technique for comparing conflicting argument a problem with these proposal is that they do not consider argument for and against first order formula we present a framework for first order logic argumentation based on argument tree that provide a way of exhaustively collating argument and counter argument a difficulty with first order argumentation is that there may be many argument and counterargument even with a relatively small knowledgebase we propose rationalizing the argument under consideration with the aim of reducing redundancy and highlighting key point 
we describe a novel approach to parallelizing graph search using structured duplicate detection structured duplicate detection wa originally developed a an approach to externalmemory graph search that reduces the number of expensive disk i o operation needed to check stored node for duplicate by using an abstraction of the search graph to localize memory reference in this paper we show that this approach can also be used to reduce the number of slow synchronization operation needed in parallel graph search in addition we describe several technique for integrating parallel and external memory graph search in an efficient way we demonstrate the effectiveness of these technique in a graphsearch algorithm for domain independent strip planning 
recently it ha been shown that the small description logic dl el which allows for conjunction and existential restriction ha better algorithmic property than it counterpart fl which allows for conjunction and value restriction whereas the subsumption problem infl becomes already intractable in the presence of acyclic tboxes it remains tractable in el even with general concept inclusion axiom gcis on the one hand we extend the positive result forel by identifying a set of expressive mean that can be added toel without sacrificing tractability on the other hand we show that basically all other addition of typical dl constructor toel with gcis make subsumption intractable and in most case even e xptimecomplete in addition we show that subsumption infl with gcis is exptime complete 
the modelling and reformulation of constraint network are recognised a important problem the task of automatically acquiring a constraint network formulation of a problem from a subset of it solution and non solution ha been presented in the literature however the choice of such a subset wa assumed to be made independently of the acquisition process we present an approach in which an interactive acquisition system actively selects a good set of example we show that the number of example required to acquire a constraint network is significantly reduced using our approach 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
in this paper we present a system which learns to recognize object through interaction by exploiting the principle of sensorimotor coordination the system us a learning architecture which is composed of reactive and deliberative layer the reactive layer consists of a database of behavior that aremodulated to produce a desired behavior in this work we have implemented and installed in our architecture an object manipulation behavior inspired by the concept that infant learn about their environment through manipulation while manipulating object both proprioceptive data and exteroceptive data are recorded both of these type of data are combined and statistically analyzed in order to extract important parameter that distinctively describe the object being manipulated this data is then clustered using the standard k mean algorithm and the resulting cluster are labeled the labeling is used to train a radial basis function network for classifying the cluster the performance of the system ha been tested on a kinematically complex walking robot capable of manipulating object with two leg used a arm and it ha been found that the trained neural network is able to classify object even when only partial sensory data is available to the system our preliminary result demonstrate that this method can be effectively used in a robotic system which learns from experience about it environment 
table on web page contain a huge amount of semantically explicit information which make them a worthwhile target for automatic information extraction and knowledge acquisition from the web however the task of table extraction from web page is difficult because of html s design purpose to convey visual instead of semantic information in this paper we propose a robust technique for table extraction from arbitrary web page this technique relies upon the positional information of visualized dom element node in a browser and hereby separate the intricacy of code implementation from the actual intended visual appearance the novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the cs visual box model which show a high level of robustness even without any form of learning f measure we describe the idea behind our approach the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction and general observation on web table that should be borne in mind by any automatic web table extraction mechanism 
the exponential growth and reliability of wikipedia have made it a promising data source for intelligent system the first challenge of wikipedia is to make the encyclopedia machine processable in this study we address the problem of extracting relation among entity from wikipedia s english article which in turn can serve for intelligent system to satisfy user information need our proposed method first anchor the appearance of entity in wikipedia article using some heuristic rule that are supported by their encyclopedic style therefore it us neither the named entity recognizer ner nor the coreference resolution tool which are source of error for relation extraction it then classifies the relationship among entity pair using svm with feature extracted from the web structure and subtrees mined from the syntactic structure of text the innovation behind our work are the following a our method make use of wikipedia characteristic for entity allocation and entity classification which are essential for relation extraction b our algorithm extract a core tree which accurately reflects a relationship between a given entity pair and subsequently identifies key feature with respect to the relationship from the core tree we demonstrate the effectiveness of our approach through evaluation of manually annotated data from actual wikipedia article 
in this paper we present a novel methodology for textual case based reasoning this technique is unique in that it automatically discovers case and similarity knowledge is language independent is scaleable and facilitates semantic similarity between case to be carried out inherently without the need for domain knowledge in addition it provides an insight into the thematical content of the casebase a a whole which enables user to better structure query we present an analysis of the competency of the system by assessing the quality of the similarity knowledge discovered and show how it is ideally suited to case based retrieval querying by example 
description logic dl are the formal foundation of the standard web ontology language owl dl and owl lite in the semantic web and other domain ontology are increasingly seen also a a mechanism to access and query data repository this novel context pose an original combination of challenge that ha not been addressed before i sufficient expressive power of the dl to capture common data modeling construct ii well established and flexible query mechanism such a conjunctive query cqs iii optimization of inference technique with respect to data size which typically dominates the size of ontology this call for investigating data complexity of query answering in expressive dl while the complexity of dl ha been studied extensively data complexity ha been characterized only for answering atomic query and wa still open for answering cqs in expressive dl we tackle this issue and prove a tight conp upper bound for the problem in shiq a long a no transitive role occur in the query we thus establish that for a whole range of dl from al to shiq answering cqs with no transitive role ha conp complete data complexity we obtain our result by a novel tableau based algorithm for checking query entailment inspired by the one in but which manages the technical challenge of simultaneous inverse role and number restriction which lead to a dl lacking the finite model property 
in this paper we present a scheme based on feature mining and neuro fuzzy inference system for detecting lsb matching steganography in grayscale image which is a very challenging problem in steganalysis four type of feature are proposed and a dynamic evolving neural fuzzy inference system denfis based feature selection is proposed a well a the use of support vector machine recursive feature elimination svm rfe to obtain better detection accuracy in comparison with other well known feature overall our feature perform the best denfis outperforms some traditional learning classifier svm rfe and denfis based feature selection outperform statistical significance based feature selection such a t test experimental result also indicate that it remains very challenging to steganalyze lsb matching steganography in grayscale image with high complexity 
for decision theoretic planning problem with an indefinite horizon plan execution terminates after a finite number of step with probability one but the number of step until termination i e the horizon is uncertain and unbounded in the traditional approach to modeling such problem called a stochastic shortest path problem plan execution terminates when a particular state is reached typically a goal state we consider a model in which plan execution terminates when a stopping action is taken we show that an action based model of termination ha several advantage for partially observable planning problem it doe not require a goal state to be fully observable it doe not require achievement of a goal state to be guaranteed and it allows a proper policy to be found more easily this framework allows many partially observable planning problem to be modeled in a more realistic way that doe not require an artificial discount factor 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
knowledge based classification and regression method are especially powerful form of learning they allow a system to take advantage of prior domain knowledge supplied either by a human user or another algorithm combining that knowledge with data to produce accurate model a limitation of the use of prior knowledge occurs when the provided knowledge is incorrect such knowledge likely still contains useful information but knowledge based learner might not be able to fully exploit such information in fact incorrect knowledge can lead to poorer model than result from knowledge free learner we present a support vector method for incorporating and refining domain knowledge that not only allows the learner to make use of that knowledge but also suggests change to the provided knowledge our approach is built on the knowledge based classification and regression method presented by fung mangasarian shavlik and by mangasarian shavlik wild experiment on artificial data set with known property a well a on a real world data set demonstrate that our method learns more accurate model while also adjusting the provided rule in intuitive way our new algorithm provides an appealing extension to knowledge based support vector learning that is not only able to combine knowledge from rule with data but is also able to use the data to modify and change those rule to better fit the data 
many practical problem have random variable with a large number of value that can be hierarchically structured into an abstraction tree of class this paper considers how to represent and exploit hierarchical structure in probabilistic reasoning we represent the distribution for such variable by specifying for each class the probability distribution over it immediate subclass we represent the conditional probability distribution of any variable conditioned on hierarchical variable using inheritance we present an approach for reasoning in bayesian network with hierarchically structured variable that dynamically construct a flat bayesian network given some evidence and a query by collapsing the hierarchy to include only those value necessary to answer the query this can be done with a single pas over the network we can answer the query from the flat bayesian network using any standard probabilistic inference algorithm such a variable elimination or stochastic simulation the domain size of the variable in the flat bayesian network is independent of the size of the hierarchy it depends on how many of the class in the hierarchy are directly associated with the evidence and query thus the representation is applicable even when the hierarchy is conceptually infinite 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
when reasoning about action and sensor in realistic domain the ability to cope with uncertainty often play an essential role among the approach dealing with uncertainty the one by bacchus halpern and levesque which us the situation calculus is perhaps the most expressive however there are still some open issue for example it remains unclear what an agent s knowledge base would actually look like the formalism also requires second order logic to represent uncertain belief yet a first order representation clearly seems preferable in this paper we show how these issue can be addressed by incorporating noisy sensor and action into an existing logic of only knowing 
this paper deal with the monitoring and diagnosis of large discrete event system the problem is to determine on line all fault and state that explain the flow of observation model based diagnosis approach that first compile the diagnosis information off line suffer from space explosion and those that operate on line without any prior compilation have poor time performance our contribution is a broader spectrum of approach that suit application with diverse time and space requirement approach on this spectrum differ in the amount of reasoning and compilation performed off line and therefore in the way they resolve the tradeoff between the space occupied by the compiled information and the time taken to produce a diagnosis we tackle the space and time complexity of diagnosis by encoding all approach in a symbolic framework based on binary decision diagram this allows for the compact representation of the compiled diagnosis information and for it handling across many state at once rather than for each state individually our experiment demonstrate the diversity and scalability of our symbolic method spectrum a well a it superiority over the corresponding enumerative implementation 
the problem of most probable explanation mpe arises in the scenario of probabilistic inference finding an assignment to all variable that ha the maximum likelihood given some evidence we consider the more general cnf based mpe problem where each literal in a cnf formula is associated with a weight we describe reduction between mpe and weighted max sat and show that both can be solved by a variant of weighted model counting the mpe sat algorithm is quite competitive with the state of the art max sat wcsp and mpe solver on a variety of problem 
voting or rank aggregation is a general method for aggregating the preference of multiple agent one important voting rule is the slater rule it selects a ranking of the alternative or candidate to minimize the number of pair of candidate such that the ranking disagrees with the pairwise majority vote on these two candidate the use of the slater rule ha been hindered by a lack of technique to compute slater ranking in this paper we show how we can decompose the slater problem into smaller subproblems if there is a set of similar candidate we show that this technique suffices to compute a slater ranking in linear time if the pairwise majority graph is hierarchically structured for the general case we also give an efficient algorithm for finding a set of similar candidate we provide experimental result that show that this technique significantly sometimes drastically speed up search algorithm finally we also use the technique of similar set to show that computing an optimal slater ranking is np hard even in the absence of pairwise tie 
temporally extended goal teg refer to property that must hold over intermediate and or final state of a plan the problem of planning with teg is of renewed interest because it is at the core of planning with temporal preference currently the fastest domain independent classical planner employ some kind of heuristic search however existing planner for teg are not heuristic and are only able to prune the search space by progressing the teg in this paper we propose a method for planning with teg using heuristic search we represent teg using a rich and compelling subset of a first order linear temporal logic we translate a planning problem with teg to a classical planning problem with this translation in hand we exploit heuristic search to determine a plan our translation relies on the construction of a parameterized nondeterministic finite automaton for the teg we have proven the correctness of our algorithm and analyzed the complexity of the resulting representation the translator is fully implemented and available our approach consistently outperforms tlplan on standard benchmark domain often by order of magnitude 
most word sense disambiguation wsd method require large quantity of manually annotated training data and or do not exploit fully the semantic relation of thesaurus we propose a new unsupervised wsd algorithm which is based on generating spreading activation network sans from the sens of a thesaurus and the relation between them a new method of assigning weight to the network link is also proposed experiment show that the algorithm outperforms previous unsupervised approach to wsd 
there are many system and technique that address stochastic scheduling problem based on distinct and sometimes opposite approach especially in term of how scheduling and schedule execution are combined and if and when knowledge about the uncertainty are taken into account in many real life problem it appears that all these approach are needed and should be combined which to our knowledge ha never been done hence it it first desirable to define a thorough classification of the technique and system exhibiting relevant feature in this paper we propose a tree dimension typology that distinguishes between proactive progressive and revision technique then a theoretical representation model integrating those three distinct approach is defined this model serf a a general template within which parameter can be tuned to implement a system that will fit specific application need we briefly introduce in this paper our first experimental prototype which validate our model 
we study an approach to learning heuristic for planning domain from example solution there ha been little work on learning heuristic for the type of domain used in deterministic and stochastic planning competition perhaps one reason for this is the challenge of providing a compact heuristic language that facilitates learning here we introduce a new representation for heuristic based on list of set expression described using taxonomic syntax next we review the idea of a measure of progress parmar which is any heuristic that is guaranteed to be improvable at every state we take finding a measure of progress a our learning goal and describe a simple learning algorithm for this purpose we evaluate our approach across a range of deterministic and stochastic planning competition domain the result show that often greedily following the learned heuristic is highly effective we also show our heuristic can be combined with learned rule based policy producing still stronger result 
this paper describes a dynamic simulation of a space habitat the simulation is configurable and controllable via external program several group have been using the simulation to study the impact of artificial intelligence tool on space habitat design and control we outline some of the ai challenge and invite the ai community to use our simulation to further nasa s exploration goal 
this paper describes a new approach to computation in a semiring based system which includes semiring based csps in particular weighted csps fuzzy csps and standard csps a well a bayesian network the approach to computation is based on what we call semiring labelled decision diagram sldds these can be generated in a similar way to a standard search tree decision tree for solving a csp but some node are merged creating a more compact representation for certain class of csps the number of node in the resulting network will be a tiny fraction of the number of node in the corresponding search tree a method is given for generating an sldd that represents e g a particular instance of a semiring based csp it is shown how this can be used to perform various computation of interest such a solving a semiring based csp finding optimal solution determining the possible value of each variable and counting solution of a csp 
abductive diagnosis is an important method for identifying possible cause which explain a given set of observation unfortunately abduction suffers from the fact that most of the algorithmic problem in this area are intractable we have recently obtained very promising result for a strongly related problem in the database area specifically the primality problem becomes efficiently solvable and highly parallelizable if the underlying functional dependency have bounded treewidth gottlob pichler wei b in the current paper we show that these favorable result can be carried over to logic based abduction in fact we even show a further generalization of these result 
planning for temporally extended goal teg expressed a formula of linear time temporal logic ltl is a proper generalization of classical planning not only allowing to specify property of a goal state but of the whole plan execution additionally ltl formula can be used to represent domain specific control knowledge to speed up planning in this paper we extend satbased planning for ltl goal akin to bounded ltl model checking in verification to partially ordered plan thus significantly increasing planning efficiency compared to purely sequential sat planning we consider a very relaxed notion of partial ordering and show how planning for ltl goal without the next time operator can be translated into a sat problem and solved very efficiently the result extend the practical applicability of satbased planning to a wider class of planning problem in addition they could be applied to solving problem in bounded ltl model checking more efficiently 
next generation mar rover will have the ability to autonomously navigate for distance of kilometer at these scale a day s traverse take the rover over it local horizon into region where only low resolution orbital data is available this improved navigation provides both an opportunity and a challenge we need new technique for performing effective science while over the horizon and out of contact this work deal with science autonomy sa the ability of the rover to reason about science goal and the science data it collect in order to make more effective exploration decision the proposed work ha two major component first we will define a set of rover sa operational mode and assist in developing and field testing a system that implement these mode we will create an overall architecture for the sa system and develop the planning component in that architecture research question include how do we define the sa operational mode how useful is each mode and under what circumstance should it be used what kind of planner is most appropriate for an sa system second we will extend partially observable markov decision process pomdp planning algorithm in way that bridge the gap between the current state of art and the planning requirement of the sa domain pomdp planner can generate high quality plan that take into account action and sensing uncertainty but realistic problem in the sa domain are far beyond the ability of existing algorithm research question include can heuristic search be combined with efficient representation of the pomdp value function to speed planning can we improve scalability when most of the world state is observable can we effectively use continuous planning technique in the pomdp context 
this paper present a method for inducing transformation rule that map natural language sentence into a formal query or command language the approach assumes a formal grammar for the target representation language and learns transformation rule that exploit the non terminal symbol in this grammar the learned transformation rule incrementally map a natural language sentence or it syntactic parse tree into a parse tree for the target formal language experimental result are presented for two corpus one which map english instruction into an existing formal coaching language for simulated robocup soccer agent and another which map english u s geography question into a database query language we show that our method performs overall better and faster than previous approach in both domain 
named entity recognition ner is the task of locating and classifying name in text in previous work ner wa limited to a small number of pre defined entity class e g people location and organization however ner on the web is a far more challenging problem complex name e g film or book title can be very difficult to pick out precisely from text further the web contains a wide variety of entity class which are not known in advance thus hand tagging example of each entity class is impractical this paper investigates a novel approach to the first step in web ner locating complex named entity in web text our key observation is that named entity can be viewed a a specie of multiword unit which can be detected by accumulating n gram statistic over the web corpus we show that this statistical method s f score is higher than that of supervised technique including conditional random field crfs and conditional markov model cmms when applied to complex name the method also outperforms cmms and crfs by on entity class absent from the training data finally our method outperforms a semi supervised crf by 
graphical game capture some of the key aspect relevant to the study and design of multi agent system it is often of interest to find the condition under which a game is stable i e the player have reached a consensus on their action in this paper we characterize how different topology of the interaction network affect the probability of existence of a pure nash equilibrium in a graphical game with random payoff we show that for tree topology with unbounded diameter the probability of a pure nash equilibrium vanishes a the number of player grows large on the positive side we define several family of graph for which the probability of a pure nash equilibrium is at least e even a the number of player go to infinity we also empirically show that adding a small number of connection shortcut can increase the probability of pure nash 
modeling complex architecture is quite challenging we introduce a novel intelligent system which can generate semi style or semi structure chinese ancient architecture automatically by using an ontology based approach to analyze the style of different architecture geometry primitive e g point line triangle etc are converted into semantic architecture component e g window gate roof etc a knowledge the following modeling process can be performed at different semantic level and it is appealing to user having domain knowledge this intelligent architecture modeling system ha been successfully applied in the digital heritage project for ancient architecture in southeast china 
state abstraction is a useful tool for agent interacting with complex environment good state abstraction are compact reuseable and easy to learn from sample data this paper combine and extends two existing class of state abstraction method to achieve these criterion the first class of method search for mdp homomorphism ravindran which produce model of reward and transition probability in an abstract state space the second class of method like the utree algorithm mccallum learn compact model of the value function quickly from sample data model based on mdp homomorphism can easily be extended such that they are usable across task with similar reward function however value based method like utree cannot be extended in this fashion we present result showing a new combined algorithm that fulfills all three criterion the resulting model are compact can be learned quickly from sample data and can be used across a class of reward function 
in many planning situation a planner is required to return a diverse set of plan satisfying the same goal which will be used by the external system collectively we take a domain independent approach to solving this problem we propose different domain independent distance function among plan that can provide meaningful insight about the diversity in the plan set we then describe how two representative state of the art domain independent planning approach one based on compilation to csp and the other based on heuristic local search can be adapted to produce diverse plan we present empirical evidence demonstrating the effectiveness of our approach 
we will demonstrate the gem system for automated development and evaluation of high quality cancer diagnostic model and biomarker discovery from microarray gene expression data the development of gem wa informed by the result of an extensive algorithmic evaluation using microarray datasets the system wa further evaluated in two cross dataset application and using microarray datasets the performance of model produced by gem is comparable or better than the result obtained by human analyst and these model generalize well to independent sample in cross dataset application the system is freely available for download from http www gem system org for noncommercial use 
personalized ranking system and trust system are an essential tool for collaboration in a multi agent environment in these system trust relation between many agent are aggregated to produce a personalized trust rating of the agent in this paper we introduce the first extensive axiomatic study of this setting and explore a wide array of well known and new personalized ranking system we adapt several axiom basic criterion from the literature on global ranking system to the context of personalized ranking system and prove strong property implied by the combination of these axiom 
ontosearch a full text search engine that exploit ontological knowledge for document retrieval is presented in this paper different from other ontology based search engine ontosearch doe not require a user to specify the associated concept of his her query domain ontology in ontosearch is in the form of a semantic network given a keyword based query ontosearch infers the related concept through a spreading activation process in the domain ontology to provide personalized information access we further develop algorithm to learn and exploit user ontology model based on a customized view of the domain ontology the proposed system ha been applied to the domain of searching scientific publication in the acm digital library the experimental result support the efficacy of the ontosearch system by using domain ontology and user ontology for enhanced search performance 
in this paper we study the effectiveness of using multiple classifier combination for eeg signal classification aiming to obtain more accurate result than it possible from single classifier system the developed system employ different feature vector fused at the abstract and measurement level for integrating information to reach a collective decision for making decision the majority voting scheme ha been used while at the measurement level fuzzy integral majority vote decision template and some other type of combination method have been investigated the ensemble classification task is completed by feeding the support vector machine with redial basis kernel function classifier with different feature extracted from the eeg signal for imagination of right and left hand movement i e at eeg channel c and c the parameter of svm classifier were optimized by genetic algorithm the result show that using classifier fusion method improved the overall classification performance 
social network allow user getting personalized recommendation for interesting resource like website or scientific paper by using review of user they trust search engine rank document by using the reference structure to compute a visibility for each document with reference structure based function like pagerank personalized document visibility can be computed by integrating both approach we present a framework for incorporating the information from both network and ranking algorithm using this information for personalized recommendation because the computation of document visibility is costly and therefore cannot be done in runtime i e when a user search a document repository we pay special attention to develop algorithm providing an efficient calculation of personalized visibility at query time based on precalculated global visibility the presented ranking algorithm are evaluated by a simulation study 
recently best first search algorithm have been introduced that store their node on disk to avoid their inherent memory limitation we introduce several improvement to the best of these including parallel processing to reduce their storage and time requirement we also present a linear time algorithm for bijectively mapping permutation to integer in lexicographic order we use breadth first search of sliding tile puzzle a testbeds on the fourteen puzzle we reduce both the storage and time needed by a factor of on two processor we also performed the first complete breadth first search of the fifteen puzzle with over state 
cognitive assistive technology that aid people with dementia such a alzheimer s disease hold the promise to provide such people with an increased level of independence however to realize this promise such system must account for the specific need and preference of individual we argue that this form of customization requires a sequential decision theoretic model of interaction we describe both fully and partially observable markov decision process pomdp model of a handwashing task and show that despite the potential computational complexity these can be effectively solved and produce policy that are evaluated a useful by professional caregiver 
we present an implementation of a model of very early sensory motor development guided by result from developmental psychology behavioral acquisition and growth is demonstrated through constraint lifting mechanism initiated by global state variable the result show how staged competence can be shaped by qualitative behavior change produced by anatomical computational and maturational constraint 
distributed constraint optimization dcop is rapidly emerging a a prominent technique for multi agent coordination however despite agent privacy being a key motivation for applying dcops in many application rigorous quantitative evaluation of privacy loss in dcop algorithm have been lacking recently maheswaran et al introduced a framework for quantitative evaluation of privacy in dcop algorithm showing that some dcop algorithm lose more privacy than purely centralized approach and questioning the motivation for applying dcops this paper address the question of whether state of the art dcop algorithm suffer from a similar shortcoming by investigating several of the most efficient dcop algorithm including both dpop and adopt furthermore while previous work investigated the impact on efficiency of distributed contraint reasoning design decision e g constraint graph topology asynchrony message content this paper examines the privacy aspect of such decision providing an improved understanding of privacy efficiency tradeoff 
recently a method based on laplacian eigenfunctions wa proposed to automatically construct a basis for value function approximation in mdps we show that it success may be explained by drawing a connection between the spectrum of the laplacian and the value function of the mdp this explanation help u to identify more precisely the condition that this method requires to achieve good performance based on this we propose a modification of the laplacian method for which we derive an analytical bound on the approximation error further we show that the method is related the augmented krylov method commonly used to solve sparse linear system finally we empirically demonstrate that in basis construction the augmented krylov method may significantly outperform the laplacian method in term of both speed and quality 
this paper present a personal cognitive assistant called disciple lta that can acquire expertise in intelligence analysis directly from intelligence analyst can train new analyst and can help analyst find solution to complex problem through mixed initiative reasoning making possible the synergistic integration of a human s experience and creativity with an automated agent s knowledge and speed and facilitating the collaboration with complementary expert and their agent 
in this paper we consider the problem of producing balanced clustering with respect to a submodular objective function submodular objective function occur frequently in many application and hence this problem is broadly applicable we show that the result of patkar and narayanan can be applied to case when the submodular function is derived from a bipartite object feature graph and moreover in this case we have an efficient flow based algorithm for finding local improvement we show the effectiveness of this approach by applying it to the clustering of word in language model 
visual cortex neuron have receptive field resembling oriented bandpass filter and their response distribution on natural image are non gaussian inspired by this we previously showed that comparing the response distribution to normal distribution with the same variance give a good thresholding criterion for detecting salient level of edginess in image however the result were based on comparison with human data thus an objective quantitative performance measure wa not taken furthermore why a normal distribution would serve a a good baseline wa not investigated in full in this paper we first conduct a quantitative analysis of the normal distribution baseline using artificial image that closely mimic the statistic of natural image since in these artificial image we can control and obtain the exact saliency information the performance of the thresholding algorithm can be measured objectively we then interpret the issue of the normal distribution being an effective baseline for thresholding under the general concept of suspicious coincidence proposed by barlow it turn out that salience defined our way can be understood a a deviation from the unsuspicious baseline our result show that the response distribution on white noise image where there is no structure thus zero salience and nothing suspicious ha a near gaussian distribution we then show that the response threshold directly calculated from the response distribution to white noise image closely match that of human providing further support for the analysis in sum our result and analysis show an intimate relationship among subjective perceptual measure of salience objective measure of salience using normal distribution a a baseline and the theory of suspicious coincidence 
in an arc consistency ac algorithm a residual support or residue is a support that ha been stored during a previous execution of the procedure which determines if a value is supported by a constraint the point is that a residue is not guaranteed to represent a lower bound of the smallest current support of a value in this paper we study the theoretical impact of exploiting residue with respect to the basic algorithm ac first we prove that ac rm ac with multi directional residue is optimal for low and high constraint tightness second we show that when ac ha to be maintained during a backtracking search mac present with respect to mac rm an overhead in o ed per branch of the binary tree built by mac where denotes the number of refutation of the branch e the number of constraint and d the greatest domain size of the constraint network one consequence is that mac rm admits a better worst case time complexity than mac for a branch involving refutation when either d or d and the tightness of any constraint is either low or high our experimental result clearly show that exploiting residue allows enhancing mac and sac algorithm 
this paper describes the software architecture of stanley an autonomous land vehicle developed for high speed desert driving without human intervention the vehicle recently won the darpa grand challenge a major robotics competition the article describes the software architecture of the robot which relied pervasively on state of the art ai technology such a machine learning and probabilistic reasoning 
protocol are modular abstraction that capture pattern of interaction among agent the compelling vision behind protocol is to enable creating customized interaction by refining and composing existing protocol realizing this vision presupposes maintaining repository of protocol and refining and composing selected protocol to this end this paper synthesizes recent advance on protocol and on the knowledge representation of action this paper present mad p a modular action description language tailored for protocol mad p enables building an aggregation hierarchy of protocol via composition this paper demonstrates the value of such composition via a simplified but realistic business scenario 
this paper present a new modeling approach for imp program with operator component matrix ocm model which can be used in imp program diagnosis using this model and model based diagnosis method some logic error can be found in imp program the model can also be extended to all kind of imperative program the advantage of this diagnosis method lie in it simple and regular presentation uniform diagnosed object usage of isomorphism assumption in structure and usage of assertion about the expected program these advantage make diagnosis more accurate and even help to correct the fault by mutation of operator component 
although natural language processing nlp for request for information ha been well studied there ha been little prior work on understanding request to update information in this paper we propose an intelligent system that can process natural language website update request semi automatically in particular this system can analyze request posted via email to update the factual content of individual tuples in a database backed website user message are processed using a scheme decomposing their request into a sequence of entity recognition and text classification task using a corpus generated by human subject experiment we experimentally evaluate the performance of this system a well a it robustness in handling request type not seen in training or user specific language style not seen in training 
recognizing object color in a variety of lighting condition is a challenging area of pattern recognition neural network have been found to be a good solution for that problem and they are also quick and accurate and can be used in real time we use a lego mindstorms robot to sort object based on color in a variety of lighting condition we will start from simpler object lego piece and move onto more complex object apple orange etc this project is in progress and we hope to achieve classification accuracy of at least 
sparsity promoting l regularization ha recently been succesfully used to learn the structure of undirected graphical model in this paper we apply this technique to learn the structure of directed graphical model specifically we make three contribution first we show how the decomposability of the mdl score plus the ability to quickly compute entire regularization path allows u to efficiently pick the optimal regularization parameter on a per node basis second we show how to use l variable selection to select the markov blanket before a dag search stage finally we show how l variable selection can be used inside of an order search algorithm the effectiveness of these l based approach are compared to current state of the art method on datasets 
language engineer often point to tight connection between their system linguistic representation and accumulated sensor data a a sign that their system really mean what they say while we believe such connection are an important piece in the puzzle of meaning we argue that perceptual grounding alone doe not suffice to explain the specific stable meaning human speaker attribute to each other instead human attribution of meaning depend on a process of societal grounding by which individual language speaker coordinate their perceptual experience and linguistic usage with other member of their linguistic community for system builder this suggests that implementing a strategy of societal grounding would justify the attribution of bona fide linguistic meaning to a system even if it had little perceptual experience and only modest perceptual accuracy we illustrate the importance and role of societal grounding using an implemented dialogue system that collaboratively identifies visual object with human user 
performing experiment with human robot interface often requires the allocation of expensive and complex hardware and large physical space those cost constrain development and research to the currently affordable resource and they retard the testing and redevelopment cycle in order to explore research free from mundane allocation constraint and speed up our platform development cycle we have developed a platform for research of multi human robot spoken dialog in coherent real and virtual space we describe the system and speculate on how it will further research in this domain 
several bootstrapping based relation extraction algorithm working on large corpus or on the web have been presented in the literature a crucial issue for such algorithm is to avoid the introduction of too much noise into further iteration typically this is achieved by applying appropriate pattern and tuple evaluation measure henceforth called filtering function thereby selecting only the most promising pattern and tuples in this paper we systematically compare different filtering function proposed across the literature although we also discus our own implementation of a pattern learning algorithm the main contribution of the paper is actually the extensive comparison and evaluation of the different filtering function proposed in the literature with respect to seven datasets our result indicate that some of the commonly used filter do not outperform a trivial baseline filter in a statistically significant manner 
in recent year probabilistic approach have found many successful application to mobile robot localization and to object state estimation for manipulation in this paper we propose a unified approach to these two problem that dynamically model the object to be manipulated and localizes the robot at the same time our approach applies in the common setting where only a lowresolution cm grid map of a building is available but we also have a high resolution cm model of the object to be manipulated our method is based on defining a unifying probabilistic model over these two representation the resulting algorithm work in real time and estimate the position of object with sufficient precision for manipulation task we apply our approach to the task of navigating from one office to another including manipulating door our approach successfully tested on multiple door allows the robot to navigate through a hallway to an office door grasp and turn the door handle and continuously manipulate the door a it move into the office 
this paper deal with the problem of identifying direct causal effect in recursive linear structural equation model using technique developed for graphical causal model we show that a model can be decomposed into a set of submodels such that the identification problem can be solved independently in each submodel we provide a new identification method that identifies causal effect by solving a set of algebraic equation 
modularity is a key requirement for collaborative ontology engineering and for distributed ontology reuse on the web modern ontology language such a owl are logic based and thus a useful notion of modularity need to take the semantics of ontology and their implication into account we propose a logic based notion of modularity that allows the modeler to specify the external signature of their ontology whose symbol are assumed to be defined in some other ontology we define two restriction on the usage of the external signature a syntactic and a slightly le restrictive semantic one each of which is decidable and guarantee a certain kind of black box behavior which enables the controlled merging of ontology analysis of real world ontology suggests that these restriction are not too onerous 
learning real time search which interleaf planning and acting allows agent to learn from multiple trial and respond quickly such algorithm require no prior knowledge of the environment and can be deployed without pre processing we introduce prioritized lrta p lrta a learning real time search algorithm based on prioritized sweeping p lrta focus learning on important area of the search space where the importance of a state is determined by the magnitude of the update made to neighboring state empirical test on path planning in commercial game map show a substantial learning speed up over state of the art real time search algorithm 
this paper present a novel approach for providing automated trading agent to a population focusing on bilateral negotiation with unenforceable agreement a new type of agent called semicooperative sc agent is proposed for this environment when these agent negotiate with each other they reach a pareto optimal solution that is mutually beneficial through extensive experiment we demonstrate the superiority of providing such agent for human over supplying equilibrium agent or letting people design their own agent these result are based on our observation that most people do not modify sc agent even though they are not in equilibrium our finding introduce a new factor human response to provided agent that should be taken into consideration when developing agent that are provided to a population 
stochastic complexity of a data set is defined a the shortest possible code length for the data obtainable by using some fixed set of model this measure is of great theoretical and practical importance a a tool for task such a model selection or data clustering in the case of multinomial data computing the modern version of stochastic complexity defined a the normalized maximum likelihood nml criterion requires computing a sum with an exponential number of term furthermore in order to apply nml in practice one often need to compute a whole table of these exponential sum in our previous work we were able to compute this table by a recursive algorithm the purpose of this paper is to significantly improve the time complexity of this algorithm the technique used here are based on the discrete fourier transformand the convolution theorem 
recently substantial effort have been devoted to the subspace learning technique based on tensor representation such a dlda ye et al dater yan et al and tensor subspace analysis tsa he et al in this context a vital yet unsolved problem is that the computational convergency of these iterative algorithm is not guaranteed in this work we present a novel solution procedure for general tensor based subspace learning followed by a detailed convergency proof of the solution projection matrix and the objective function value extensive experiment on real world database verify the high convergence speed of the proposed procedure a well a it superiority in classification capability over traditional solution procedure 
the determination of appropriate value for free algorithm parameter is a challenging and tedious task in the design of effective algorithm for hard problem such parameter include categorical choice e g neighborhood structure in local search or variable value ordering heuristic in tree search a well a numerical parameter e g noise or restart timing in practice tuning of these parameter is largely carried out manually by applying rule of thumb and crude heuristic while more principled approach are only rarely used in this paper we present a local search approach for algorithm configuration and prove it convergence to the globally optimal parameter configuration our approach is very versatile it can e g be used for minimising run time in decision problem or for maximising solution quality in optimisation problem it further applies to arbitrary algorithm including heuristic tree search and local search algorithm with no limitation on the number of parameter experiment in four algorithm configuration scenario demonstrate that our automatically determined parameter setting always outperform the algorithm default sometimes by several order of magnitude our approach also show better performance and greater flexibility than the recent calibra system our paramils code along with instruction on how to use it for tuning your own algorithm is available on line at http www c ubc ca lab beta project paramils 
abstract preference can be aggregated using voting rule we consider here the family of rule which perform a sequence of pairwise majority comparison between two candidate the winner thus depends on the chosen sequence of comparison which can be represented by a binary tree we address the difficulty of computing candidate that win for some tree and then introduce and study the notion of fair winner i e candidate who win in a balanced tree we then consider the situation where we lack complete information about preference and determine the computational complexity of computing winner in this case 
diagnosability of system is an essential property that determines how accurate any diagnostic reasoning can be on a system given any sequence of observation generally in the literature of dynamic event driven system diagnosability analysis is performed by algorithm that consider a system a a whole and their response is either a positive answer or a counter example in this paper we present an original framework for diagnosability checking the diagnosability problem is solved in a distributed way in order to take into account the distributed nature of realistic problem a opposed to all other approach our algorithm also provides an exhaustive and synthetic view of the reason why the system is not diagnosable finally the presented algorithm is scalable in practice it provides an approximate and useful solution if the computational resource are not sufficient 
we present a conceptual framework for creating q learning based algorithm that converge to optimal equilibrium in cooperative multiagent setting this framework includes a set of condition that are sufficient to guarantee optimal system performance we demonstrate the efficacy of the framework by using it to analyze several well known multi agent learning algorithm and conclude by employing it a a design tool to construct a simple novel multi agent learning algorithm 
weblogs or blog are an important new way to publish information engage in discussion and form community on the internet the blogosphere ha unfortunately been infected by several variety of spam like content blog search engine for example are inundated by post from splogs false blog with machine generated or hijacked content whose sole purpose is to host ad or raise the pagerank of target site we discus how svm model based on local and link based feature can be used to detect splogs we present an evaluation of learned model and their utility to blog search engine system that employ technique differing from those of conventional web search engine 
we consider closed loop solution to stochastic optimization problem of resource allocation type they concern with the dynamic allocation of reusable resource over time to non preemtive interconnected task with stochastic duration the aim is to minimize the expected value of a regular performance measure first we formulate the problem a a stochastic shortest path problem and argue that our formulation ha favorable property e g it ha finite horizon it is acyclic thus all policy are proper and moreover the space of control policy can be safely restricted then we propose an iterative solution essentially we apply a reinforcement learning based adaptive sampler to compute a sub optimal control policy we suggest several approach to enhance this solution and make it applicable to large scale problem the main improvement are the value function is maintained by feature based support vector regression the initial exploration is guided by rollout algorithm the state space is partitioned by clustering the task while keeping the precedence constraint satisfied the action space is decomposed and consequently the number of available action in a state is decreased and finally we argue that the sampling can be effectively distributed among several processor the effectiveness of the approach is demonstrated by experimental result on both artificial benchmark and real world industry related data 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
in this paper we consider a hybrid solution to the sensor network position inference problem which combine a real time filtering system with information from a more expensive global inference procedure to improve accuracy and prevent divergence many online solution for this problem make use of simplifying assumption such a gaussian noise model and linear system behaviour and also adopt a filtering strategy which may not use available information optimally these assumption allow near real time inference while also limiting accuracy and introducing the potential for ill conditioning and divergence we consider augmenting a particular real time estimation method the extended kalman filter ekf with a more complex but more highly accurate inference technique based on markov chain monte carlo mcmc methodology conventional mcmc technique applied to this problem can entail significant and time consuming computation to achieve convergence to address this we propose an intelligent bootstrapping process and the use of parallel communicative chain of different temperature commonly referred to a parallel tempering the combined approach is shown to provide substantial improvement in a realistic simulated mapping environment and when applied to a complex physical system involving a robotic platform moving in an office environment instrumented with a camera sensor network 
in this paper we augment the max sat solver of larrosa hera with three new inference rule the three of them are special case of max sat resolution with which better lower bound and more value pruning is achieved our experimental result on several domain show that the resulting algorithm can be order of magnitude faster than state of the art max sat solver and the best weighted csp solver 
our work link chinese calligraphy to computer science through an integrated intelligence approach we first extract stroke of existent calligraphy using a semi automatic twophase mechanism the first phase try to do the best possible extraction using a combination of algorithmic technique the second phase present an intelligent user interface to allow the user to provide input to the extraction process for the difficult case such a those in highly random cursive or distorted style having derived a parametric representation of calligraphy we employ a supervised learning based method to explore the space of visually pleasing calligraphy a numeric grading method for judging the beauty of calligraphy is then applied to the space we integrate such a grading unit into an existent constraint based reasoning system for calligraphy generation which result in a significant enhancement in term of visual quality in the automatically generated calligraphic character finally we construct an intelligent calligraphy tutoring system making use of the above this work represents our first step towards understanding the human process of appreciating beauty through modeling the process with an integration of available ai technique more result and supplementary material are provided at http www c hku hk songhua calligraphy 
although many powerful ai and machine learning technique exist it remains difficult to quickly create ai for embodied virtual agent that produce visually lifelike behavior this is important for application e g game simulator interactive display where an agent must behave in a manner that appears human like we present a novel technique for learning reactive policy that mimic demonstrated human behavior the user demonstrates the desired behavior by dictating the agent s action during an interactive animation later when the agent is to behave autonomously the recorded data is generalized to form a continuous state to action mapping combined with an appropriate animation algorithm e g motion capture the learned policy realize stylized and natural looking agent behavior we empirically demonstrate the efficacy of our technique for quickly producing policy which result in lifelike virtual agent behavior 
we investigate the property of an abstract negotiation framework where agent autonomously negotiate over allocation of discrete resource in this framework reaching an optimal allocation potentially requires very complex multilateral deal therefore we are interested in identifying class of utility function such that any negotiation conducted by mean of deal involving only a single resource at at time is bound to converge to an optimal allocation whenever all agent model their preference using these function we show that the class of modular utility function is not only sufficient but also maximal in this sense 
the gap in automation between mip sat solver and those for constraint programming and constraint based local search hinders experimentation and adoption of these technology and slows down scientific progress this paper address this important issue it show how effective local search procedure can be automatically synthesized from model expressed in a rich constraint language the synthesizer analyzes the model and derives the local search algorithm for a specific meta heuristic by exploiting the structure of the model and the constraint semantics experimental result suggest that the synthesized procedure only induce a small loss in efficiency on a variety of realistic application in sequencing resource allocation and facility location 
few temporal planner handle both concurrency and uncertain duration but these feature commonly co occur in real world domain in this paper we discus the challenge caused by concurrent durative action whose duration are uncertain we present five implemented algorithm including durprun a planner guaranteed to find the optimal policy an empirical comparison reveals that durexp our fastest planner obtains order of magnitude speed up compared to durprun with little loss in solution quality importantly our algorithm can handle probabilistic effect in addition to stochastic duration and they are effective even when duration distribution are multi modal 
we take the category system in wikipedia a a conceptual network we label the semantic relation between category using method based on connectivity in the network and lexicosyntactic matching a a result we are able to derive a large scale taxonomy containing a large amount of subsumption i e isa relation we evaluate the quality of the created resource by comparing it with researchcyc one of the largest manually annotated ontology a well a computing semantic similarity between word in benchmarking datasets 
this paper present new result on the complexity of graph theoretical model that represent probability bayesian network and that represent interval and set valued probability credal network we define a new class of network with bounded width and introduce a new decision problem for bayesian network the maximin a posteriori we present new link between the bayesian and credal network and present new result both for bayesian network most probable explanation with observation maximin a posteriori and for credal network bound on probability a posteriori most probable explanation with and without observation maximum a posteriori 
with the increased use of the web ha come a corresponding increase in information overload that user face when trying to locate specific webpage especially a a majority of visit to webpage are revisits while automatically created browsing history list offer a potential low cost solution to re locating webpage even short browsing session generate a glut of webpage that do not relate to the user s information need or have no revisit value we address how we can better support web user who want to return to information on a webpage that they have previously visited by building more useful history list the paper report on a combination technique that semi automatically segment the webpage browsing history list into task applies heuristic to remove webpage that carry no intrinsic revisit value and us a learning model sensitive to individual user and task that predicts which webpage are likely to be revisited again we present result from an empirical evaluation that report the likely revisit need of user and that show that adequate overall prediction accuracy can be achieved this approach can be used to increase utility of history list by removing information overload to user when revisiting webpage 
in this paper we provide a logical framework for using computer to discover theorem in two person finite game in strategic form and apply it to discover class of game that have unique pure strategy nash equilibrium we consider all possible class of game that can be expressed by a conjunction of two binary clause and our program re discovered kat and thisse s class of weakly unilaterally competitive two person game and came up with several other class of game that have unique nash equilibrium it also came up with new class of strict game that have unique nash equilibrium where a game is strict if for each player different profile have different payoff partly motivated by these finding we also manually prove a result that say that a strict game ha a unique nash equilibrium iff it is best response equivalent to a strictly competitive game class of game with unique nash equilibrium we did not expect much a these condition are rather simple but to our surprise our program returned a condition that is more general than the strict competitiveness condition a it turned out it exactly corresponds to kat and thisse s class of weakly unilaterally competitivetwo person game our program also returned some other condition two of them capture a class of unfair game where one player ha advantage over the other the remaining one capture game where everyone get what he want each receives his maximum payoff in every equilibrium state thus there is no real competition among the player thus one conclusion that we can draw from this experiment is that among all class of game that can be expressed by a conjunction of two binary clause the class of weakly unilaterally competitive game is the most general class of competitive and fair game that have unique nash equilibrium of course this doe not mean that the other condition are not worth investigating for instance sometimes one may be forced to play an unfair game for the same set of condition we also consider strict twoperson game where different profile have different payoff for each player among the result returned by our program two of them are exactly the two conjuncts in kat and thisse s weakly unilaterally competitive condition but the others all turn out to be special case of game with dominant strategy motivated by these result we consider certain equivalent class of game and show that a strict game ha a unique nash equilibrium iff it is best response equivalent rosenthal to a strictly competitive game the rest of the paper is organized a follows we first review some basic concept in two person game in strategic form and then reformulate them in first order logic we then show that for a class of condition whether any of them entail the uniqueness of nash equilibrium need only to be checked on game up to certain size we then describe a computer program based on this result and report our experimental result 
traditional probabilistic mixture model such a latent dirichlet allocation imply that data record such a document are fully exchangeable however data are naturally collected along time thus obey some order in time in this paper we present dynamic mixture model dmms for online pattern discovery in multiple time series dmms do not have the noticeable drawback of the svd based method for data stream negative value in hidden variable are often produced even with all non negative input we apply dmm model to two real world datasets and achieve significantly better result with intuitive interpretation 
complex planning domain push the boundary of the expressive power of planning domain modelling language recent extension to the standard planning language have included expression for temporal metric and resource structure other work ha also considered how process model can be incorporated into domain model in this paper we consider the problem of expressing and validating model containing event which are triggered a a consequence of the action of physical process we focus primarily on the validation of plan in the context of exogenous event discussing the modelling semantic and implementation issue that arise event impact not only on plan but on domain model a a whole and we also consider the problem that arise in considering the validation of event structure in domain model 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
forming effective coalition is a major research challenge in the field of multi agent system central to this endeavour is the problem of determining the best set of agent that should participate in a given team to this end in this paper we present a novel anytime algorithm for coalition structure generation that is faster than previous anytime algorithm designed for this purpose our algorithm can generate solution that either have a tight bound from the optimal or are optimal depending on the objective and work by partitioning the space in term of a small set of element that represent structure which contain coalition of particular size it then performs an online heuristic search that prune the space and only considers valid and non redundant coalition structure we empirically show that we are able to find solution that are in the worst case efficient in of the time to find the optimal value by the state of the art dynamic programming dp algorithm for agent using le memory 
in timed zero sum game the goal is to maximize the probability of winning which is not necessarily the same a maximizing our expected reward we consider cumulative intermediate reward to be the difference between our score and our opponent s score the true reward of a win loss or tie is determined at the end of a game by applying a threshold function to the cumulative intermediate reward we introduce thresholded reward problem to capture this dependency of the final reward outcome on the cumulative intermediate reward thresholded reward problem reflect different real world stochastic planning domain especially zero sum game in which time and score need to be considered we investigate the application of thresholded reward to finite horizon markov decision process mdps in general the optimal policy for a thresholded reward mdp will be non stationary depending on the number of time step remaining and the cumulative intermediate reward we introduce an efficient value iteration algorithm that solves thresholded reward mdps exactly but with running time quadratic on the number of state in the mdp and the length of the time horizon we investigate a number of heuristic based technique that efficiently find approximate solution for mdps with large state space or long time horizon 
although anticipation is an important part of creating believable behaviour it ha had but a secondary role in the field of life like character in this paper we show how a simple anticipatory mechanism can be used to control the behaviour of a synthetic character implemented a a software agent without disrupting the user s suspension of disbelief we describe the emotivector an anticipatory mechanism coupled with a sensor that us the history of the sensor to anticipate the next sensor state interprets the mismatch between the prediction and the sensed value by computing it attention grabbing potential and associating a basic qualitative sensation with the signal sends it interpretation along with the signal when a signal from the sensor reach the processing module of the agent it carry recommendation such a you should seriously take this signal into consideration a it is much better than we had expected or just forget about this one it is a bad a we predicted we delineate several strategy to manage several emotivectors at once and show how one of these strategy meta anticipation transparently introduces the concept of uncertainty finally we describe an experiment in which an emotivector controlled synthetic character interacts with the user in the context of a word puzzle game and present the evaluation supporting the adequacy of our approach 
word sense disambiguation wsd ha been a long standing research objective for natural language processing in this paper we are concerned with developing graph based unsupervised algorithm for alleviating the data requirement for large scale wsd under this framework finding the right sense for a given word amount to identifying the most important node among the set of graph node representing it sens we propose a variety of measure that analyze the connectivity of graph structure thereby identifying the most relevant word sens we ass their performance on standard datasets and show that the best measure perform comparably to state of the art 
this paper present a methodology for integrating feature within the occupancy grid og framework the og map provide a dense representation of the environment in particular they give information for every range measurement projected onto a grid however independence assumption between cell during update a well a not considering sonar model lead to inconsistent map which may also lead the robot to take some decision which may be unsafe or which may introduce an unnecessary overhead of run time collision avoidance behavior feature based map provide more consistent representation by implicitly considering correlation between cell but they are sparse due to sparseness of feature in a typical environment this paper provides a method for integrating feature based representation within the standard bayesian framework of og and provides a dense more accurate and safe representation than standard og method 
machine learning for predicting user click in web based search offer automated explanation of user activity we address click prediction in the web search scenario by introducing a method for click prediction based on observation of past query and the clicked document due to the sparsity of the problem space commonly encountered when learning for web search new approach to learn the probabilistic relationship between document and query are proposed two probabilistic model are developed which differ in the interpretation of the query document co occurrence a novel technique namely conditional probability hierarchy flexibly adjusts the level of granularity in parsing query and a a result leverage the advantage of both model 
the purpose of this paper is to extend the notion of prime implicates and prime implicants to the basic modal logic we consider a number of different potential definition of clause and term for which we evaluate with respect to their syntactic semantic and complexity theoretic property we then continue our analysis by comparing the definition with respect to the property of the notion of prime implicates and prime implicants that they induce we provide algorithm and complexity result for the prime implicate generation and recognition task for the two most satisfactory definition 
we describe a technique for comparing distribution without the need for density estimation a an intermediate step our approach relies on mapping the distribution into a reproducing kernel hilbert space we apply this technique to construct a two sample test which is used for determining whether two set of observation arise from the same distribution we use this test in attribute matching for database using the hungarian marriage method where it performs strongly we also demonstrate excellent performance when comparing distribution over graph for which no alternative test currently exist 
contact center agent typically respond to email query from customer by selecting predefined answer template that relate to the question present in the customer query in this paper we present a technique to automatically select the answer template corresponding to a customer query email given a set of query response email pair we find the association between the actual question and answer within them and use this information to map future question to their answer template we evaluate the system on a small subset of the publicly available pine info discussion list email archive and also on actual contact center data comprising customer query agent response and template 
we describe an approach to distributed knowledge acquisition using an ontology the ontology is used to represent and reason about soldier performance these method are embedded in an immersive web based knowledge elicitation and analysis system 
in this work we propose three unsupervised measure to automatically identify the number of distinct entity a given ambiguous name refers to in a corpus we experiment with artificially created name conflations and observe that the measure pk formulated a the ratio of two successive clustering criterion function value outperforms the other two measure we also describe a method to assign a unique label to each discovered cluster so a to identify the underlying entity that it refers to 
in text categorization term weighting method assign appropriate weight to the term to improve the classification performance in this study we propose an effective term weighting scheme i e tf rf and investigate several widely used unsupervised and supervised term weighting method on two popular data collection in combination with svm and knn algorithm from our controlled experimental result not all supervised term weighting method have a consistent superiority over unsupervised term weighting method specifically the three supervised method based on the information theory i e tf tf ig and tf or perform rather poorly in all experiment on the other hand our proposed tf rf achieves the best performance consistently and outperforms other method substantially and significantly the popularly used tf idf method ha not shown a uniformly good performance with respect to different data corpus 
a robot become a mass consumer product they will need to learn new skill by interacting with typical human user past approach have adapted reinforcement learning rl to accept a human reward signal however we question the implicit assumption that people shall only want to give the learner feedback on it past action we present finding from a human user study showing that people use the reward signal not only to provide feedback about past action but also to provide future directed reward to guide subsequent action given this we made specific modification to the simulated rl robot to incorporate guidance we then analyze and evaluate it learning performance in a second user study and we report significant improvement on several measure this work demonstrates the importance of understanding the human teacher robot learner system a a whole in order to design algorithm that support how people want to teach while simultaneously improving the robot s learning performance 
a general method for defining informative prior on statistical model is presented and applied specifically to the space of classification and regression tree a bayesian approach to learning such model from data is taken with the metropolis hastings algorithm being used to approximately sample from the posterior by only using proposal distribution closely tied to the prior acceptance probability are easily computable via marginal likelihood ratio whatever the prior used our approach is empirically tested by varying i the data ii the prior and iii the proposal distribution a comparison with related work is given 
a case based reasoning system wa created to support customer who purchased appliance from general electric when a customer call general electric for help a call taker us the system to diagnose the problem and step the customer through it solution the system ha been in use by call taker since it ha resulted in a percent increase in the probability the customer s problem can be solved over the phone this ha greatly improved customer satisfaction and saved ge million between and from reduced cost of visit of field service technician to customer s home 
this paper contributes to the emerging literature at the border between ai and cognitive neuroscience analyzing the resource constraint that shape brain function the brain is conceptualized a a set of area that collaborate to perform complex cognitive task both computation within individual area and communication between collaborating area are viewed a resource consuming activity the efficient deployment of limited resource is formalized a a linear programming problem which the brain is hypothesized to solve on a moment by moment basis a model of language processing is analyzed within this framework and found to exhibit resource utilization profile consistent with those observed in functional neuroimaging study of human 
this paper present a mechanism for acquiring a case base for a cbr system that ha to deal with a limited perception of the environment the construction of case base in these domain is very complex and requires mechanism for autonomously adjusting the scope of the existing case and for acquiring new case the work presented in this paper address these two goal to find out the right scope of existing case and to introduce new case when no appropriate solution is found we have tested the mechanism in the robot soccer domain performing experiment both under simulation and with real robot 
this paper describes a technique for the probabilistic self localization of a sensor network based on noisy inter sensor range data our method is based on a number of parallel instance of markov chain monte carlo mcmc by combining estimate drawn from these parallel chain we build up a representation of the underlying probability distribution function pdf for the network pose our approach includes sensor data incrementally in order to avoid local minimum and is shown to produce meaningful result efficiently we return a distribution over sensor location rather than a single maximum likelihood estimate this can then be used for subsequent exploration and validation 
the discovery of meaningful change point finding segment in both categorical and real value data time series is a well studied problem prior segmentation algorithm and task operate under overly restrictive assumption e g a priori knowledge of the number of segment trivial input and in singular domain e g finding common region in image speaker change detection we introduce a domain independent algorithm undertow which discovers segment boundary in real valued time series and construct hierarchy of segment to form macro segment 
the large number of data source on the internet can be used to augment and verify the accuracy of geospatial source such a gazetteer and annotated satellite imagery data source such a satellite imagery map gazetteer and vector data have been traditionally used in geographic infonnation system gi but nontraditional geospatial data such a online phone book and property record are more difficult to relate to imagery in this paper we present a novel approach to combining extracted information from imagery road vector data and online data source we represent the problem of identifying building in satellite image a a constraint satisfing problem csp and use constraint programming to solve it we apply this technique to real world data source in ei segundo ca and our experimental evaluation show how this approach can accurately identify building when provided with both traditional and nontraditional data source 
we introduce cui network a compact graphical representation of utility function over multiple attribute cui network model multiattribute utility function using the well studied and widely applicable utility independence concept we show how conditional utility independence lead to an effective functional decomposition that can be exhibited graphically and how local compact data at the graph node can be used to calculate joint utility we discus aspect of elicitation network construction and optimization and contrast our new representation with previous graphical preference modeling 
we analyze the asymptotic conditional validity of modal formula i e the probability that a formula is valid in the finite kripke structure in which a given modal formula is valid when the size of these kripke structure grows to infinity we characterize the formula that are almost surely valid i e with probability in case is a flat s consistent formula and show that these formula are exactly those which follow from according to the nonmonotonic modal logic s g our result provide for the first time a probabilistic semantics to a well known nonmonotonic modal logic establishing a new bridge between nonmonotonic and probabilistic reasoning and give a computational account of the asymptotic conditional validity problem in kripke structure 
knowledge compilation is one of the more traditional approach to model based diagnosis where a compiled system model is obtained in an off line phase and then used to efficiently answer diagnostic query on line the choice of a suitable representation for the compiled model is critical to the success of this approach and two of the main proposal have been decomposable negation normal form dnnf and ordered binary decision diagram obdd the contribution of this paper is twofold first we show that in the current state of the art dnnf dominates obdd in efficiency and scalability for some typical diagnostic task this result is based on a step by step comparison of the complexity of diagnostic algorithm for dnnf and obdd together with a known succinctness relation between the two representation second we present a tool for model based diagnosis which is based on a state of the art dnnf compiler and our implementation of dnnf diagnostic algorithm we demonstrate the efficiency of this tool against recent result reported on diagnosis using obdd 
the paper report on new algorithm for solving partially observable game whereas existing algorithm apply and or search to a tree of blackbox belief state our incremental version treat uncertainty a a new search dimension examining the physical state within a belief state to construct solution tree incrementally on a newly created database of checkmate problem for kriegspiel a partially observable form of chess incrementalization yield speedup of two or more order of magnitude on hard instance 
in many real world collective decision problem the set of alternative is a cartesian product of finite value domain for each of a given set of variable the prohibitive size of such domain make it practically impossible to represent preference relation explicitly now ai ha been developing language for representing preference on such domain in a succinct way exploiting structural property such a conditional preferential independence here we reconsider voting and aggregation rule in the case where voter preference have a common preferential independence structure and address the decompossition a voting rule or an aggregation function following a linear order over variable 
we propose a real time computer vision system that enables a ugv to safely cross urban road intersection specifically when the ugv approach the stop sign at a way intersection it must be aware of the vehicle at the other three road and adhere to traffic rule by waiting for it turn to proceed the proposed solution consists of three main component a vehicle detector a tracker and a finite state machine to model the traffic we use an ot mach filter to detect the leading vehicle in each of three camera view of the corresponding road then we track the vehicle using an edge enhanced dynamic correlation tracker which estimate the current and next position velocity and acceleration of the vehicle finally the finite state machine describes the traffic in each road with one of four possible state i e no vehicle waiting arriving waiting and passing and signal an autopilot system when it is safe to pas the intersection we provide the result from an actual intersection with real traffic to demonstrate that the ugv is able to automatically navigate the intersection using the proposed system 
computational humor is a challenge with implication for many classical field in ai such a for example natural language processing intelligent human computer interaction reasoning not to mention cognitive science linguistics and psychology in this paper we summarize our experience in developing hahacronym a system devoted to produce humorous acronym and we discus some concrete prospect for this field 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum of eliminability criterion from strict dominance when the set are a small a possible to nash equilibrium when the set are a large a possible we show that checking whether a strategy is eliminable according to this criterion is conp complete both when all the set are a large a possible and when the dominator set each have size we then give an alternative definition of the eliminability criterion and show that it is equivalent using the minimax theorem we show how this alternative definition can be translated into a mixed integer program of polynomial size with a number of binary integer variable equal to the sum of the size of the eliminee set implying that checking whether a strategy is eliminable according to the criterion can be done in polynomial time given that the eliminee set are small finally we study using the criterion for iterated elimination of strategy 
we present in this paper a hybrid planning system which combine constraint satisfaction technique and planning heuristic to produce optimal sequential plan it integrates it own consistency rule and filtering and decomposition mechanism suitable for planning given a fixed bound on the plan length our planner work directly on a structure related to graphplan s planning graph this structure is incrementally built each time it is extended a sequential plan is searched different search strategy may be employed currently it is a forward chaining search based on problem decomposition with action set partitioning various technique are used to reduce the search space such a memorizing nogood state or estimating goal reachability in addition the planner implement two different technique to avoid enumerating some equivalent action sequence empirical evaluation show that our system is very competitive on many problem especially compared to other optimal sequential planner 
in this paper we investigate the face recognition problem via the overlapping energy histogram of the dct coefficient particularly we investigate some important issue relating to the recognition performance such a the issue of selecting threshold and the number of bin these selection method utilise information obtained from the training dataset experimentation is conducted on the yale face database and result indicate that the proposed parameter selection method perform well in selecting the threshold and number of bin furthermore we show that the proposed overlapping energy histogram approach outperforms the eigenfaces dpca and energy histogram significantly 
a natural representation of data is given by the parameter which generated the data if the space of parameter is continuous then we can regard it a a manifold in practice we usually do not know this manifold but we just have some representation of the data often in a very high dimensional feature space since the number of internal parameter doe not change with the representation the data will effectively lie on a low dimensional submanifold in feature space however the data is usually corrupted by noise which particularly in high dimensional feature space make it almost impossible to find the manifold structure this paper review a method called manifold denoising which project the data onto the submanifold using a diffusion process on a graph generated by the data we will demonstrate that the method is capable of dealing with non trival high dimensional noise moreover we will show that using the denoising method a a preprocessing step one can significantly improve the result of a semi supervised learning algorithm 
a virtual organization vo is a dynamic collection of entity individual enterprise and information resource collaborating on some computational activity vos are an emerging mean to model enact and manage large scale computation vos consist of autonomous heterogeneous member often dynamic exhibiting complex behavior thus vos are best modeled via multiagent system an agent can be an individual such a a person business partner or a resource an agent may also be a vo a vo is an agent that comprises other agent contract provide a natural arm length abstraction for modeling interaction among autonomous and heterogeneous agent the interplay of contract and vos is the subject of this paper the core of this paper is an approach to formalize vos and contract based on commitment our main contribution are a formalization of vos a discussion of certain key property of vos and an identification of a variety of vo structure and an analysis of how they support contract enactment we evaluate our approach with an analysis of several scenario involving the handling of exception and conflict in contract 
we demonstrate modular robot prototype developed a part of the claytronics project goldstein et al among the novel feature of these robot catoms is their ability to reconfigure move relative to one another without moving part the absence of moving part is central to one key aim of our work namely plausible manufacturability at smaller and smaller physical scale using high volume low unit cost technique such a batch photolithography multimaterial submicron d lithographic processing and self assembly claytronics envisions multi million module robot ensemble able to form into three dimensional scene eventually with sufficient fidelity so a to convince a human observer the scene are real this work present substantial challenge in mechanical and electronic design control programming reliability power delivery and motion planning among other area and hold the promise of radically altering the relationship between computation human and the physical world 
a multi agent system grow in size and complexity social network that govern the interaction among the agent will directly impact system behavior at the individual and collective level example of such large scale networked multi agent system include peer to peer network distributed information retrieval and agent based supply chain one way of dealing with the uncertain and dynamic nature of such environment is to endow agent with the ability to modify the agent social network by autonomously adapting their local connectivity structure in this paper we present a framework for agent organized network aons in the context of multi agent production and exchange and experimentally evaluate the feasibility and efficiency of specific aon strategy we find that decentralized network adaptation can significantly improve organizational performance additionally we analyze several property of the resulting network structure and consider their relationship to the observed increase in organizational performance 
a key problem in playing strategy game is learning how to allocate resource effectively this can be a difficult task for machine learning when the connection between action and goal output are indirect and complex we show how a combination of structural analogy experimentation and qualitative modeling can be used to improve performance in optimizing food production in a strategy game experimentation bootstrap a case library and drive variation while analogical reasoning support retrieval and transfer a qualitative model serf a a partial domain theory to support adaptation and credit assignment together these technique can enable a system to learn the effect of it action the range of quantity and to apply training in one city to other structurally different city we describe experiment demonstrating this transfer of learning 
multiagent distributed resource allocation requires that agent act on limited localized information with minimum communication overhead in order to optimize the distribution of available resource when requirement and constraint are dynamic learning agent may be needed to allow for adaptation one way of accomplishing learning is to observe past outcome using such information to improve future decision when limit in agent memory or observation capability are assumed one must decide on how large should the observation window be we investigate how this decision influence both agent and system s performance in the context of a special class of distributed resource allocation problem namely dispersion game we show by numerical experiment over a specific dispersion game the minority game that in such scenario an agent s performance is non monotonically correlated with her memory size when all other agent are kept unchanged we then provide an information theoretic explanation for the observed behavior showing that a downward causation effect take place 
extensional table constraint are an important tool for attacking combinatorial problem with constraint programming recently there ha been renewed interest in fast propagation algorithm for these constraint we describe the use of two alternative data structure for maintaining generalised arc consistency on extensional constraint the first the next difference list is novel and ha been developed with this application in mind the second the trie is well known but it use in this context is novel empirical analysis demonstrate the efficiency of the resulting approach both in gac schema and in the watched literal table constraint in minion 
interactive dynamic influence diagram i dids offer a transparent and semantically clear representation for the sequential decision making problem over multiple time step in the presence of other interacting agent solving i dlds exactly involves knowing the solution of possible model of the other agent which increase exponentially with the number of time step we present a method of solving i dlds approximately by limiting the number of other agent candidate model at each time step to a constant we do this by clustering the model and selecting a representative set from the cluster we discus the error bound of the approximation technique and demonstrate it empirical performance 
many current state of the art planner rely on forward heuristic search the success of such search typically depends on heuristic distance to the goal estimate derived from the plangraph such estimate are effective in guiding search for many domain but there remain many other domain where current heuristic are inadequate to guide forward search effectively in some of these domain it is possible to learn reactive policy from example plan that solve many problem however due to the inductive nature of these learning technique the policy are often faulty and fail to achieve high success rate in this work we consider how to effectively integrate imperfect learned policy with imperfect heuristic in order to improve over each alone we propose a simple approach that us the policy to augment the state expanded during each search step in particular during each search node expansion we add not only it neighbor but all the node along the trajectory followed by the policy from the node until some horizon empirical result show that our proposed approach benefit both of the leveraged automated technique learning and heuristic search outperforming the state of the art in most benchmark planning domain 
color segmentation is a challenging subtask in computer vision most popular approach are computationally expensive involve an extensive off line training phase and or rely on a stationary camera this paper present an approach for color learning on board a legged robot with limited computational and memory resource a key defining feature of the approach is that it work without any labeled training data rather it train autonomously from a color coded model of it environment the process is fully implemented completely autonomous and provides high degree of segmentation accuracy 
this paper deal with system of parametric equation over the real in the framework of interval constraint programming a parameter vary within interval the solution set of a problem may have a non null volume in these case an inner box i e a box included in the solution set instead of a single punctual solution is of particular interest because it give greater freedom for choosing a solution our approach is able to build an inner box for the problem starting with a single point solution by consistently extending the domain of every variable the key point is a new method called generalized projection the requirement are that each parameter must occur only once in the system variable domain must be bounded and each variable must occur only once in each constraint our extension is based on an extended algebraic structure of interval called generalized interval where improper interval are allowed e g 
in this paper we present a discriminative framework based on conditional random field for stochastic modeling of image in a hierarchical fashion the main advantage of the proposed framework is it ability to incorporate a rich set of interaction among the image site we achieve this by inducing a hierarchy of hidden variable over the given label field the proposed tree like structure of our model eliminates the need for a huge parameter space and at the same time permit the use of exact and efficient inference procedure based on belief propagation we demonstrate the generality of our approach by applying it to two important computer vision task namely image labeling and object detection the model parameter are trained using the contrastive divergence algorithm we report the performance on real world image and compare it with the existing approach 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
streamlined constrained reasoning powerfully boost the performance of backtrack search method for finding hard combinatorial object we use so called spatially balanced latin square to show how streamlining can also be very effective for local search our approach is much faster and generates considerably larger spatially balanced latin square than previously reported approach up to order the previous best result could only generate solution up to order we also provide a detailed characterization of our streamliner and solution topology for small order we believe that streamlined local search is a general technique suitable for solving a wide range of hard combinatorial design problem 
sourcing professional buy several trillion dollar worth of good and service yearly we introduced a new paradigm called expressive commerce and applied it to sourcing it combine the advantage of highly expressive human negotiation with the advantage of electronic reverse auction the idea is that supply and demand are expressed in drastically greater detail than in traditional electronic auction and are algorithmically cleared this creates a pareto efficiency improvement in the allocation a win win between the buyer and the seller but the market clearing problem is a highly complex combinatorial optimization problem we developed the world s fastest tree search algorithm for solving it we have hosted billion of sourcing using the technology and created billion of hard dollar saving the supplier also benefited by being able to express production efficiency and creativity and through exposure problem removal supply network were redesigned with quantitative understanding of the tradeoff and implemented in week instead of month 
the inference of temporal information from past event occurences in imagistic domain is relevant in several application in knowledge engineering in such application the order in which event have happened is imprinted in the domain a visual spatial relation among it element therefore the interpretation of the relative ordering in which those event have occured is essential for understanding the domain evolution we propose a cognitive model for event ordering reasoning within domain whose element have been modified by past event from the analysis of cognitive ability of expert we propose new ontology construct for knowledge modelling associated to problem solving method we illustrate the effectiveness of the model by mean of an application to an imagistic domain 
we introduce data structure called responsibility set and kernel we present an algorithm fcrk which is a modification of fc that maintains these structure and us them for pruning of the search space according to our experimental evaluation fc rk outperforms fc cbj on constraint network encoding graph k coloring instance and on non dense random binary constraint network 
a matrix formulation for an adaptive genetic algorithm is developed using mutation matrix and crossover matrix selection mutation and crossover are all parameter free in the sense that the problem at a particular stage of evolution will choose the parameter automatically this time dependent selection process wa first developed in moga mutation only genetic algorithm szeto and zhang and now is extended to include crossover the remaining parameter needed are population size and chromosome length the adaptive behavior is based on locus statistic and fitness ranking of chromosome in crossover two method are introduced long hamming distance crossover lhdc and short hamming distance crossover shdc lhdc emphasizes exploration of solution space shdc emphasizes exploitation of local search process the one dimensional random coupling ising spin glass problem which is similar to a knapsack problem is used a a benchmark test for the comparison of various realization of the adaptive genetic algorithm our result show that lhdc is better than shdc but both are superior to moga which ha been shown to be better than many traditional method 
in this paper i describe work for my ph d dissertation whieh is currently in progress the overarching goal of the work is to develop a methodology for empirically evaluating the effect of different interface design decision in spoken dialogue system the methodology i will use is the dual task method borrowed from cognitive psychology which is advantageous because it provides fine grained information about the cognitive load of the user while he she is engaged in interacting with the system for my dissertation i will focus specifically on the use of definite referring expression and the question of whether natural or fully specified definite referring expression are easier for user to generate and or understand the answer are important because both strategy are used in system on the market today more importantly i hope my work will provide a tool for software developer and encourage them to carefully weigh the empirically observed cost and benefit of various design decision 
developing efficient approach for reasoning under inconsistency is an important issue in many application several method have been proposed to compile possibly inconsistent weighted or stratified base this paper focus on the well known linear order and possibilistic logic strategy it provides a way for compiling a stratified belief base in order to be able to process inference from it in polynomial time the resulting extra compilation cost is very low in particular the number of additional variable that are added to original stratified base corresponds exactly to the number of priority level existing in the base moreover our compilation approach allows an efficient computation of weighted possibilistic conclusion and possibilistic conditioning 
the modal logic s f provides an account for the default logic of reiter and several modal nonmonotonic logic of knowledge and belief in this paper we focus on a fragment of the logic s f concerned with modal formula called modal default and on set of modal default modal default theory we present characterization of s f expansion of modal default theory and show that strong and uniform equivalence of modal default theory can be expressed in term of the logical equivalence in the logic s f we argue that the logic s f can be viewed a the general default logic of nested default we also study special modal default theory called modal program and show that this fragment of the logic s f generalizes the logic here and there 
ensemble method such a bagging and boosting have been successfully applied to classification problem two important issue associated with an ensemble approach are how to generate model to construct an ensemble and how to combine them for classification in this paper we focus on the problem of model generation for heterogeneous data classification if we could partition heterogeneous data into a number of homogeneous partition we will likely generate reliable and accurate classification model over the homogeneous partition we examine different way of forming homogeneous subset and propose a novel method that allows a data point to be assigned multiple time in order to generate homogeneous partition for ensemble learning we present the detail of the new algorithm and empirical study over the uci benchmark datasets and datasets of image classification and show that the proposed approach is effective for heterogeneous data classification 
abstraction is a powerful form of domain knowledge that allows reinforcement learning agent to cope with complex environment but in most case a human must supply this knowledge in the absence of such prior knowledge or a given model we propose an algorithm for the automatic discovery of state abstraction from policy learned in one domain for use in other domain that have similar structure to this end we introduce a novel condition for state abstraction in term of the relevance of state feature to optimal behavior and we exhibit statistical method that detect this condition robustly finally we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
we consider the iterated belief change that occurs following an alternating sequence of action and observation at each instant an agent ha some belief about the action that occurs a well a belief about the resulting state of the world we represent such problem by a sequence of ranking function so an agent assigns a quantitative plausibility value to every action and every state at each point in time the resulting formalism is able to represent fallible knowledge erroneous perception exogenous action and failed action we illustrate that our framework is a generalization of several existing approach to belief change and it appropriately capture the non elementary interaction between belief update and belief revision 
in this paper we extend active monte carlo recognition amcr a recently proposed framework for object recognition the approach is based on the analogy between mobile robot localization and object recognition up to now amcr wa only shown to work for shape recognition on binary image in this paper we significantly extend the approach to work on realistic image of real world object we accomplish recognition under similarity transforms and even severe non rigid and non affine deformation we show that our approach work on database with thousand of object that it can better discriminate between object than state of the art approach and that it ha significant conceptual advantage over existing approach it allows iterative recognition with simultaneous tracking iteratively guiding attention to discriminative part the inclusion of feedback loop the simultaneous propagation of multiple hypothesis multiple object recognition and simultaneous segmentation and recognition while recognition take place triangular mesh are constructed that precisely define the correspondence between input and prototype object even in the case of strong non rigid deformation 
complex network exist in a wide array of diverse domain ranging from biology sociology and computer science these real world network while disparate in nature often comprise of a set of loose cluster a k a community whose member are better connected to each other than to the rest of the network discovering such inherent community structure can lead to deeper understanding about the network and therefore ha raised increasing interest among researcher from various discipline this paper describes gwn lda generic weighted network latent dirichlet allocation model a hierarchical bayesian model derived from the widely received lda model for discovering probabilistic community profile in social network in this model community are modeled a latent variable and defined a distribution over the social actor space in addition each social actor belongs to every community with different probability this paper also proposes two different network encoding approach and explores the impact of these two approach to the community discovery performance this model is evaluated on two research collaborative network citeseer and nanosci the experimental result demonstrate that this approach is promising for discovering community structure in large scale network 
formalization of familiarity contributes to formalization of trust through a value centric trust model however familiarity wa assumed to be the similarity of value fixed for two agent and stability of the trust model wa relatively low to increase the stability we propose an improved familiarity measurement experiment are carried out to compare the stability of the trust model with the improved familiarity measurement and with the fixed familiarity value it is observed that the stability is increased by through the improved familiarity measurement 
scaling ai algorithm to large problem requires that these algorithm work together to harness their respective strength we introduce a method of automatically constructing hhmms using the output of a sequential data mining algorithm and sequential prediction algorithm we present the theory of this technique and demonstrate result using the mavhome intelligent environment 
we investigate the property of logic program with aggregate we mainly focus on program with monotone and antimonotone aggregate lpm aa program we define a new notion of unfounded set for lpm aa program and prove that it is a sound generalization of the standard notion of unfounded set for aggregate free program we show that the answer set of an lpm aa program are precisely it unfounded free model we define a well founded operator wp for lpm aa program we prove that it total fixpoints are precisely the answer set of p and it least fixpoint wpw is contained in the intersection of all answer set if p admits an answer set wpw is efficiently computable and for aggregate free program it coincides with the well founded model we carry out an in depth complexity analysis in the general framework including also nonmonotone aggregate we prove that monotone and anti monotone aggregate do not increase the complexity of cautious reasoning which remains in co np nonmonotone aggregate instead do increase the complexity by one level in the polynomial hierarchy our result allow also to generalize and speed up asp system with aggregate 
our work explores the transfer of knowledge at multiple level of abstraction to improve learning by exploiting the similarity between object at various level of detail multiresolution learning can facilitate transfer between image classification task we extract feature from image at multiple level of resolution then use these feature to create model at different resolution upon receiving a new task the closest matching stored model can be generalized adapted to the appropriate resolution and transferred to the new task 
there is currently much interest in using external memory such a disk storage to scale up graph search algorithm recent work show that the local structure of a graph can be leveraged to substantially improve the efficiency of external memory graph search this paper introduces a technique called edge partitioning which exploit a form of local structure that ha not been considered in previous work the new technique improves the scalability of structured approach to external memory graph search and also guarantee the applicability of these approach to any graph search problem we show it effectiveness in an external memory graph search algorithm for domain independent strip planning 
in this paper we present an algorithm to identify different type of object from d and d laser range data our method is a combination of an instance based feature extraction similar to the nearest neighbor classifier nn and a collective classification method that utilizes associative markov network amns compared to previous approach we transform the feature vector so that they are better separable by linear hyperplanes which are learned by the amn classifier we present result of extensive experiment in which we evaluate the performance of our algorithm on several recorded indoor scene and compare it to the standard amn approach a well a the nn classifier the classification rate obtained with our algorithm substantially exceeds those of the amn and the nn 
health care official are increasingly concerned with knowing early whether an outbreak of a particular disease is unfolding we often have daily count of some variable that are indicative of the number of individual in a given community becoming sick each day with a particular disease by monitoring these daily count we can possibly detect an outbreak in an early stage a number of classical time series method have been applied to outbreak detection based on monitoring daily count of some variable these classical method only give u an alert a to whether there may be an outbreak they do not predict property of the outbreak such a it size duration and how far we are into the outbreak knowing the probable value of these variable can help guide u to a cost effective decision that maximizes expected utility bayesian network have become one of the most prominent architecture for reasoning under uncertainty in artificial intelligence we present an intelligent system implemented using a bayesian network which not only detects an outbreak but predicts it size and duration and estimate how far we are into the outbreak we show result of investigating the performance of the system using simulated outbreak based on real outbreak data these result indicate that the system show promise of being able to predict property of an outbreak 
we investigate the problem of mining closed set in multi relational database previous work introduced different semantics and associated algorithm for mining closed set in multirelational database however insight into the implication of semantic choice and the relationship among them wa still lacking our investigation show that the semantic choice are important because they imply different property which in turn affect the range of algorithm that can mine for such set of particular interest is the question whether the seminal lcm algorithm by uno et al can be upgraded towards multi relational problem lcm is attractive since it run time is linear in the number of closed set and it doe not need to store output in order to avoid duplicate we provide a positive answer to this question for some of the semantic choice and report on experiment that evaluate the scalability and applicability of the upgraded algorithm on benchmark problem 
we consider a resource selection game with incomplete information about the resource cost function all the player know is the set of player an upper bound on the possible cost and that the cost function are positive and nondecreasing the game is played repeatedly and after every stage each player observes her cost and the action of all player for every we prove the existence of a learning equilibrium which is a profile of algorithm one for each player such that a unilateral deviation of a player is up to not beneficial for her regardless of the actual cost function furthermore the learning equilibrium yield an optimal social cost 
proper name whether english or non english have several different spelling when transliterated from a non english source language into english knowing the different variation can significantly improve the result of name search on various source text especially when recall is important in this paper we propose two novel phonetic model to generate numerous candidate variant spelling of a name our method show threefold improvement over the baseline and generate four time a many good name variant compared to a human while maintaining a respectable precision of 
the autofeed system automatically extract data from semistructured web site previously researcher have developed two type of supervised learning approach for extracting web data method that create precise site specific extraction rule and method that learn le precise site independent extraction rule in either case significant training is required autofeed follows a third more ambitious approach in which unsupervised learning is used to analyze site and discover their structure our method relies on a set of heterogeneous expert each of which is capable of identifying certain type of generic structure each expert represents it discovery a hint based on these hint our system cluster the page and identifies semi structured data that can be extracted to identify a good clustering we use a probabilistic model of the hint generation process this paper summarizes our formulation of the fully automatic web extraction problem our clustering approach and our result on a set of experiment 
we describe our approach for generating expressive music performance of monophonic jazz melody it consists of three component a a melodic transcription component which extract a set of acoustic feature from monophonic recording b a machine learning component which induces an expressive transformation model from the set of extracted acoustic feature and c a melody synthesis component which generates expressive monophonic output midi or audio from inexpressive melody description using the induced expressive transformation model in this paper we concentrate on the machine learning component in particular on the learning scheme we use for generating expressive audio from a score 
an application of relational instance based learning to the complex task of expressive music performance is presented we investigate to what extent a machine can automatically build expressive profile of famous pianist using only minimal performance information extracted from audio cd recording by pianist and the printed score of the played music it turn out that the machine generated expressive performance on unseen piece are substantially closer to the real performance of the trainer pianist than those of all others two other interesting application of the work are discussed recognizing pianist from their style of playing and automatic style replication 
logistic regression with l regularization ha been proposed a a promising method for feature selection in classification problem several specialized solution method have been proposed for l regularized logistic regression problem lrps however existing method do not scale well to large problem that arise in many practical setting in this paper we describe an efficient interior point method for solving l regularized lrps small problem with up to a thousand or so feature and example can be solved in second on a pc a variation on the basic method that us a preconditioned conjugate gradient method to compute the search step can solve large sparse problem with a million feature and example e g the newsgroups data set in a few ten of minute on a pc numerical experiment show that our method outperforms standard method for solving convex optimization problem a well a other method specifically designed for l regularized lrps 
we define a new general rule based nonmonotonic framework which allows an external acyclic priority relation between rule to be interpreted in several way several entailment semantics are defined via a constructive digraph with one being given a declarative fixed point characterisation a well each of these semantics satisfies principle of brewka and eiter the framework encompasses default logic reiter ground answer set programming asp baral and defeasible logic nute default logic is provided with a new semantics which is ambiguity blocking rather than the usual ambiguity propagating semantics also reiter extension are given a new fixed point characterisation and lukaszewicz s m extension are given a much simpler construction and fixed point characterisation 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
markov decision process are a powerful framework for planning under uncertainty but current algorithm have difficulty scaling to large problem we present a novel probabilistic planner based on the notion of hybridizing two algorithm in particular we hybridize gpt an exact mdp solver with mbp a planner that plan using a qualitative nondeterministic model of uncertainty whereas exact mdp solver produce optimal solution qualitative planner sacrifice optimality to achieve speed and high scalability our hybridized planner hybplan is able to obtain the best of both technique speed quality and scalability moreover hybplan ha excellent anytime property and make effective use of available time and memory 
much of artificial intelligence research is focused on devising optimal solution for challenging and well defined but highly constrained problem however a we begin creating autonomous agent to operate in the rich environment of modern videogames and computer simulation it becomes important to devise agent behavior that display the visible attribute of intelligence rather than simply performing optimally such visibly intelligent behavior is difficult to specify with rule or characterize in term of quantifiable objective function but it is possible to utilize human intuition to directly guide a learning system toward the desired sort of behavior policy induction from human generated example is a promising approach to training such agent in this paper such a method is developed and tested using lamarckian neuroevolution artificial neural network are evolved to control autonomous agent in a strategy game the evolution is guided by human generated example of play and the system effectively learns the policy that were used by the player to generate the example i e the agent learn visibly intelligent behavior in the future such method are likely to play a central rule in creating autonomous agent for complex environment making it possible to generate rich behavior derived from nothing more formal than the intuitively generated example of designer player or subject matter expert 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
several propositional fragment have been considered so far a target language for knowledge compilation and used for improving computational task from major ai area like inference diagnosis and planning among them are the quite influential ordered binary decision diagram prime implicates prime implicants formula in decomposable negation normal form on the other hand the validity problem qbf for quantified boolean formula qbf ha been acknowledged for the past few year a an important issue for ai and many solver have been designed for this purpose in this paper the complexity of restriction of qbf obtained by imposing the matrix of the input qbf to belong to such propositional fragment is identified both tractability and intractability result pspace completeness are obtained 
a mathematical programming formulation is proposed to eliminate irrelevant and redundant feature for collaborative computer aided diagnosis which requires to detect multiple clinically related malignant structure from medical image a probabilistic interpretation is described to justify our formulation the proposed formulation is optimized through an effective alternating optimization algorithm that is easy to implement and relatively fast to solve this collaborative prediction approach ha been implemented and validated on the automatic detection of solid lung nodule by jointly detecting ground glass opacity 
the process of forming coalition of software agent generally requires calculating a value for every possible coalition which indicates how beneficial that coalition would be if it wa formed now since the number of possible coalition increase exponentially with the number of agent involved having one agent calculate all the value is inefficient given this we present a novel algorithm for distributing this calculation among agent in cooperative environment specifically by using our algorithm each agent is assigned some part of the calculation such that the agent share are exhaustive and disjoint moreover the algorithm is decentralized requires no communication between the agent and ha minimal memory requirement to evaluate the effectiveness of our algorithm we compare it with the only other algorithm available in the literature due to shehory and kraus this show that for the case of agent the distribution process of our algorithm took of the time the value were calculated using of the memory the calculation redundancy wa reduced from to and the total number of byte sent between the agent dropped from to note that for larger number of agent these improvement become exponentially better 
we integrate a number of new and recent advance in heuristic search and apply them to the fourpeg tower of hanoi problem these include frontier search disk based search parallel processing multiple compressed disjoint and additive pattern database heuristic and breadth first heuristic search new idea include pattern database heuristic based on multiple goal state a method to reduce coordination among multiple parallel thread and a method for reducing the number of heuristic calculation we perform the first complete breadth first search of the and disc fourpeg tower of hanoi problem and extend the verification of presumed optimal solution to this problem from to disc verification of the disc problem is in progress 
in this paper a new complete technique to compute maximal satisfiable subset ms and minimally unsatisfiable subformulas mu of set of boolean clause is introduced the approach improves the currently most efficient complete technique in several way it make use of the powerful concept of critical clause and of a computationally inexpensive local search oracle to boost an exhaustive algorithm proposed by liffiton and sakallah these feature can allow exponential efficiency gain to be obtained accordingly experimental study show that this new approach outperforms the best current existing exhaustive one 
we propose a new approach for stereo matching in autonomous mobile robot application in this framework an accurate but slow reconstruction of the d scene is not needed rather it is more important to have a fast localization of the obstacle to avoid them all the method in the literature are based on a punctual correspondence but they are inefficient in realistic context for the presence of uniform pattern or some perturbation between the two image of the stereo pair our idea is to face the stereo matching problem a a matching between homologous region instead of a point matching the stereo image are represented a graph and a graph matching is computed to find homologous region we present some result on a standard stereo database and also on a more realistic stereo sequence acquired from a robot moving in an indoor environment and a performance comparison with other approach in the literature is reported and discussed our method is strongly robust in case of some fluctuation of the stereo pair homogeneous and repetitive region and is fast the result is a semi dense disparity map leaving only a few region in the scene unmatched 
the action language golog ha been applied successfully to the control of robot among other thing perhaps it greatest advantage is that a user can write program which constrain the search for an executable plan in a flexible manner however when general planning is needed golog support this only in principle but doe not measure up with state of the art planner in this paper we propose an integration of golog and planning in the sense that planning problem formulated a part of a golog program are solved by a modern planner during the execution of the program here we focus on the adl subset of the plan language pddl first we show that the semantics of adl can be understood a progression in the situation calculus which underlies golog thus providing u with a correct embedding of adl within golog we then show how golog can be integrated with an existing adl planner for closed world initial database and compare the performance of the resulting system with the original golog 
we introduce a general setting for information elicitation in multi agent system where agent may be approached both sequentially and simultaneously in order to compute a function that depends on their private secret we consider oblivious mechanism for sequential simultaneous information elicitation in such mechanism the ordering of agent to be approached is fixed in advance surprisingly we show that these mechanism which are easy to represent and implement are sufficient for very general setting such a for the classical uniform model where agent secret bit are uniformly distributed and for the computation of the majority function and other classical threshold function moreover we provide efficient algorithm for the verification of the existence of the desired elicitation mechanism and for synthesizing such mechanism 
in machine learning ml and evolutionary computation ec it is often beneficial to approximate a complicated function by a simpler one such a a linear or quadratic function for computational efficiency or feasibility reason cf jin a complicated function the target function in ml or the fitness function in ec may require an exponential amount of computation to learn evaluate and thus approximation by simpler function are needed we consider the problem of approximating pseudo boolean function by simpler e g linear function when the instance space is associated with a probability distribution we consider n a a sample space with a possibly nonuniform probability measure on it thus making pseudo boolean function into random variable this is also in the spirit of the pac learning framework of valiant valiant where the instance space ha a probability distribution on it the best approximation to a target function f is then defined a the function g from all possible approximating function of the simpler form that minimizes the expected distance to f in an example we use method from linear algebra to find in this more general setting the best approximation to a given pseudo boolean function by a linear function 
real time dynamic programming rtdp is a heuristic search algorithm for solving mdps we present a modified algorithm called focused rtdp with several improvement while rtdp maintains only an upper bound on the long term reward function frtdp maintains two sided bound and base the output policy on the lower bound frtdp guide search with a new rule for outcome selection focusing on part of the search graph that contribute most to uncertainty about the value of good policy frtdp ha modified trial termination criterion that should allow it to solve some problem within that rtdp cannot experiment show that for all the problem we studied frtdp significantly outperforms rtdp and lrtdp and converges with up to six time fewer backup than the state of the art hdp algorithm 
knn and svm are two machine learning approach to text categorization tc based on the vector space model in this model borrowed from information retrieval document are represented a a vector where each component is associated with a particular word from the vocabulary traditionally each component value is assigned using the information retrieval tfidf measure while this weighting method seems very appropriate for ir it is not clear that it is the best choice for tc problem actually this weighting method doe not leverage the information implicitly contained in the categorization task to represent document in this paper we introduce a new weighting method based on statistical estimation of the importance of a word for a specific categorization problem this method also ha the benefit to make feature selection implicit since useless feature for the categorization problem considered get a very small weight extensive experiment reported in the paper show that this new weighting method improves significantly the classification accuracy a measured on many categorization task 
question answering qa is a highly complex task that brings together classification clustering retrieval and extraction question answering system include various statistical and rule based component that combine and form multiple strategy for finding answer however in real life scenario efficiency constraint make it infeasible to simultaneously use all available strategy in a qa system to address this issue we present an approach for carefully selecting answering strategy that are likely to benefit individual question without significantly reducing performance we evaluate the impact of strategy selection on question answering performance at several important qa stage document retrieval answer extraction and answer merging we present strategy selection experiment using a statistical question answering system and we show significant efficiency improvement by selecting of the available answering strategy we obtained similar performance when compared to using all of the strategy combined 
recommender system are an emerging technology that help consumer to find interesting product a recommender system make personalized product suggestion by extracting knowledge from the previous user interaction in this paper we present itemrank a random walk based scoring algorithm which can be used to rank product according to expected user preference in order to recommend top rank item to potentially interested user we tested our algorithm on a standard database the movielens data set which contains data collected from a popular recommender system on movie that ha been widely exploited a a benchmark for evaluating recently proposed approach to recommender system e g fouss et al sarwar et al we compared itemrank with other state of the art ranking technique in particular the algorithm described in fouss et al our experiment show that itemrank performs better than the other algorithm we compared to and at the same time it is le complex than other proposed algorithm with respect to memory usage and computational cost too 
physically based rendering is the process of generating a d image from the abstract description of a d scene despite the development of various new technique and algorithm the computational requirement of generating photorealistic image still do not allow to render in real time moreover the configuration of good render quality parameter is very difficult and often too complex to be done by nonexpert user this paper describes a novel approach called magarro standing for multi agent approach to rendering optimization which utilizes principle and technique known from the field of multi agent system to optimize the rendering process experimental result are presented which show the benefit of magarro based rendering optimization 
constraint programming is a commonly used technology for solving complex combinatorial problem however user of this technology need significant expertise in order to model their problem appropriately we propose a basis for addressing this problem a new sat based version space algorithm for acquiring constraint network from example of solution and non solution of a target problem an important advantage of the algorithm is the ease with which domain specific knowledge can be exploited 
given the common use of restarts in today s clause learning sat solver the task of choosing a good restart policy appears to have attracted remarkably little interest on the other hand result have been reported on the use of different restart policy for combinatorial search algorithm such result are not directly applicable to clause learning sat solver a the latter are now understood a performing a form of resolution something fundamentally different from search in the sense of backtracking search for satisfying assignment in this paper we provide strong evidence that a clause learning sat solver could benefit substantially from a carefully designed restart policy which may not yet be available we begin by pointing out that the restart policy work together with other aspect of a sat solver in determining the sequence of resolution step performed by the solver and hence it efficiency in this spirit we implement a prototype clause learning sat solver that facilitates restarts at arbitrary point and conduct experiment on an extensive set of industrial benchmark using various restart policy including those used by well known sat solver a well a a universal policy proposed in by luby et al the result indicate a substantial impact of the restart policy on the efficiency of the solver and provide motivation for the design of better restart policy particularly dynamic one 
it ha recently been proposed that it is advantageous to have model of dynamical system be based solely on observable quantity predictive state representation psrs are a type of model that us prediction about future observation to capture the state of a dynamical system however psrs do not use memory of past observation we propose a model called memory psrs that us both memory of the past and prediction of the future we show that the use of memory provides a number of potential advantage it can reduce the size of the model in comparison to a psr model in addition many dynamical system have memory that can serve a landmark that completely determine the current state the detection and recognition of landmark is advantageous because they can serve to reset a model that ha gotten off track a often happens when the model is learned from sample this paper develops both memory psrs and the use and detection of landmark 
this paper address the problem of concept sampling in many real world application a large collection of mixed concept is available for decision making however the collection is often so large that it is difficult if not unrealistic to utilize those concept directly due to the domain specific limitation of available space or time this naturally yield the need for concept reduction in this paper we introduce the novel problem of concept sampling to find the optimal subset of a large collection of mixed concept in advance so that the performance of future decision making can be best preserved by selectively combining the concept remained in the subset the problem is formulized a an optimization process based on our derivation of a target function which tie a clear connection between the composition of the concept subset and the expected error of future decision making upon the subset then based on this target function a sampling algorithm is developed and it effectiveness is discussed extensive empirical study suggest that the proposed concept sampling method well preserve the performance of decision making while dramatically reduces the number of concept maintained and thus justify it usefulness in handling large scale mixed concept 
a long lived agent continually face new task in it environment such an agent may be able to use knowledge learned in solving earlier task to produce candidate policy for it current task there may however be multiple reasonable policy suggested by prior experience and the agent must choose between them potentially without any a priori knowledge about their applicability to it current situation we present an expert algorithm for efficiently choosing amongst candidate policy in solving an unknown markov decision process task we conclude with the result of experiment on two domain in which we generate candidate policy from solution to related task and use our expert algorithm to choose amongst them 
in this paper we analyze the process of allocating task to self interested agent in uncertain changing open environment the allocator in our model is responsible for the performance of dynamically arriving task using a second price reverse auction a the allocation protocol since the agent are self interested i e each agent attempt to maximize it own revenue previous model concerning cooperative agent aiming for a joint goal are not applicable thus the main challenge is to identify a set of equilibrium strategy a stable solution where no agent can benefit from changing it strategy given the other agent strategy for any specific environmental setting we formulate the model and discus the difficulty in extracting the agent equilibrium strategy directly from the model s equation consequently we propose an efficient algorithm to accurately approximate the agent equilibrium strategy a comparative illustration through simulation of the system performance in a closed and open environment is given emphasizing the advantage of the allocator operating in the latter environment reaching result close to those obtained by a central enforceable allocation 
estimating the rate of web page update help in improving the web crawler s scheduling policy but most of the web source are autonomous and updated independently client like web crawler are not aware of when and how often the source change unlike other study the process of web page update is modeled a nonhomogeneous poisson process and focus on determining localized rate of update then various rate estimator are discussed showing experimentally how precise they are this paper explores two class of problem firstly the localized rate of update is estimated by dividing the given sequence of independent and inconsistent update point into consistent window from various experimental comparison the proposed weibull estimator outperforms duane plot another proposed estimator and other estimator proposed by cho et al and norman matloff in of the whole window for synthetic real web datasets secondly the future update point are predicted based on most recent window and it is found that weibull estimator ha higher precision compared to other estimator 
we develop an algorithm for reducing universally quantified situation calculus query to a form more amenable to automated reasoning universal quantification in the situation calculus requires a second order induction axiom making automated reasoning difficult for such query we show how to reduce query about property persistence a common family of universally quantified query to an equivalent form that doe not quantify over situation the algorithm for doing so utilizes only first order reasoning we give several example of important reasoning task that are facilitated by our approach including checking for goal impossibility and reasoning about knowledge with partial observability of action 
we propose odpop a new distributed algorithm for open multiagent combinatorial optimization that feature unbounded domain faltings macho gonzalez the odpop algorithm explores the same search space a the dynamic programming algorithm dpop petcu faltings b or adopt modi et at but doe so in an incremental best first fashion suitable for open problem odpop ha several advantage over dpop first it us message whose size only grows linearly with the treewidth of the problem second by letting agent explore value in a best first order it avoids incurring always the worst case complexity a dpop and on average it save a significant amount of computation and information exchange to show the merit of our approach we report on experiment with practically sized distributed meeting scheduling problem on a multiagent system 
sentiment analysis is an example of polarity learning most research on learning to identify sentiment ignores neutral example and instead performs training and testing using only example of significant polarity we show that it is crucial to use neutral example in learning polarity for a variety of reason and show how neutral example help u obtain superior classification result in two sentiment analysis test bed 
topic focused multi document summarization aim to produce a summary biased to a given topic or user profile this paper present a novel extractive approach based on manifold ranking of sentence to this summarization task the manifold ranking process can naturally make full use of both the relationship among all the sentence in the document and the relationship between the given topic and the sentence the ranking score is obtained for each sentence in the manifold ranking process to denote the biased information richness of the sentence then the greedy algorithm is employed to impose diversity penalty on each sentence the summary is produced by choosing the sentence with both high biased information richness and high information novelty experiment on duc and duc are performed and the rouge evaluation result show that the proposed approach can significantly outperform existing approach of the top performing system in duc task and baseline approach 
in this paper we describe icarus a cognitive architecture for physical agent that integrates idea from a number of tradition but that ha been especially influenced by result from cognitive psychology we review icarus commitment to memory and representation then present it basic process for performance and learning we illustrate the architecture s behavior on a task from in city driving that requires interaction among it various component in addition we discus icarus consistency with qualitative finding about the nature of human cognition in closing we consider the framework s relation to other cognitive architecture that have been proposed in the literature 
in the neuroevolving robotic operative nero video game the player train a team of virtual robot for combat against other player team the virtual robot learn in real time through interacting with the player since nero wa originally released in june it ha been downloaded over time appeared on slashdot and won several honor the real time neuroevolution of augmenting topology rt neat method which can evolve increasingly complex artificial neural network in real time a a game is being played drive the robot learning making possible this entirely new genre of video game the live demo will show how agent in nero adapt in real time a they interact with the player in the future rtneat may allow new kind of educational and training application through interactive and adapting game 
the majority of the existing algorithm for learning decision tree are greedy a tree is induced top down making locally optimal decision at each node in most case however the constructed tree is not globally optimal furthermore the greedy algorithm require a fixed amount of time and are not able to generate a better tree if additional time is available to overcome this problem we present a lookahead based algorithm for anytime induction of decision tree which allows trading computational speed for tree quality the algorithm us a novel strategy for evaluating candidate split a stochastic version of id is repeatedly invoked to estimate the size of the tree in which each split result and the split that minimizes the expected size is preferred experimental result indicate that for several hard concept our proposed approach exhibit good anytime behavior and yield significantly better decision tree when more time is available 
we address dimensionality estimation and nonlinear manifold inference starting from point input in high dimensional space using tensor voting the proposed method operates locally in neighborhood and doe not involve any global computation it is based on information propagation among neighboring point implemented a a voting process unlike other local approach for manifold learning the quantity propagated from one point to another is not a scalar but is in the form of a tensor that provides considerably richer information the accumulation of vote at each point provides a reliable estimate of local dimensionality a well a of the orientation of a potential manifold going through the point reliable dimensionality estimation at the point level is a major advantage over competing method moreover the absence of global operation allows u to process significantly larger datasets we demonstrate the effectiveness of our method on a variety of challenging datasets 
significant work ha been done on computational aspect of solving game under various solution concept such a nash equilibrium subgame perfect nash equilibrium correlated equilibrium and iterated dominance however the fundamental concept of rationalizability and curb closed under rational behavior set have not to our knowledge been studied from a computational perspective first for rationalizability we describe an lp based polynomial algorithm that find all strategy that are rationalizable against a mixture over a given set of opponent strategy then we describe a series of increasingly sophisticated polynomial algorithm for finding all minimal curb set one minimal curb set and the smallest minimal curb set finally we give theoretical result regarding the relationship between curb set and nash equilibrium showing that finding a nash equilibrium can be exponential only in the size of the smallest curb set we show that this can lead to an arbitrarily large reduction in the complexity of finding a nash equilibrium on the downside we also show that the smallest curb set can be arbitrarily larger than the support of the enclosed nash equilibrium 
in human robot interaction hri it is essential that the robot interprets and reacts to a human s utterance in a manner that reflects their intended meaning in this paper we present a collection of novel technique that allow a robot to interpret and execute spoken command describing manipulation goal involving qualitative spatial constraint e g put the red ball near the blue cube the resulting implemented system integrates computer vision potential field model of spatial relationship and action planning to mediate between the continuous real world and discrete qualitative representation used for symbolic reasoning 
wrapper is a traditional method to extract useful information from web page most previous work rely on the similarity between html tag tree and induced template dependent wrapper when hundred of information source need to be extracted in a specific domain like news it is costly to generate and maintain the wrapper in this paper we propose a novel template independent news extraction approach to easily identify news article based on visual consistency we first represent a page a a visual block tree then by extracting a series of visual feature we can derive a composite visual feature set that is stable in the news domain finally we use a machine learning approach to generate a template independent wrapper experimental result indicate that our approach is effective in extracting news across website even from unseen website the performance is a high a around in term of f value 
usual numerical learning method are primarily concerned with finding a good numerical fit to data and often make prediction that do not correspond to qualitative law in the domain of modelling or expert intuition in contrast the idea of q learning is to induce qualitative constraint from training data and use the constraint to guide numerical regression the resulting numerical prediction are consistent with a learned qualitative model which is beneficial in term of explanation of phenomenon in the modelled domain and can also improve numerical accuracy this paper proposes a method for combining the learning of qualitative constraint with an arbitrary numerical learner and explores the accuracy and explanation benefit of learning monotonic qualitative constraint in a number of domain we show that q learning can correct for error caused by the bias of the learning algorithm and discus the potential of similar hierarchical learning scheme 
in many application domain it is useful to be able to represent and reason about a user s preference over set of object we present a representation language dd pref for diversity and depth preference for specifying the desired diversity and depth of set of object where each object is represented a a vector of feature value a strong diversity preference for a particular feature indicates that the user would like the set to include object whose value are evenly dispersed across the range of possible value for that feature a strong depth preference for a feature indicates that the user is interested in specific target value or range diversity and depth are complementary but are not necessarily opposite we define an objective function that when maximized identifies the subset of object that best satisfies a statement of preference in dd pref exhaustively searching the space of all possible subset is intractable for large problem space therefore we also present an efficient greedy algorithm for generating preferred object subset we demonstrate the expressive power of dd pref and the performance of our greedy algorithm by encoding and applying qualitatively different preference for multiple task on a block world data set finally we provide experimental result for a collection of mar rover image demonstrating that we can successfully capture individual preference of different user and use them to retrieve high quality image subset 
this paper present multi conditional learning mcl a training criterion based on a product of multiple conditional likelihood when combining the traditional conditional probability of label given input with a generative probability of input given label the later act a a surprisingly effective rerularizer when applied to model with latent variable mcl combine the structure discovery capability of generative topic model such a latent dirichlet allocation and the exponential family harmonium with the accuracy and robustness of discriminative classifier such a logistic regression and conditional random field we present result on several standard text data set showing significant reduction in classification error due to mcl regularization and substantial gain in precision and recall due to the latent structure discovered under mcl 
we have introduced a search engine that can extract opinion sentence relevant to an open domain query from japanese blog page the engine identifies opinion based not only on positive or negative measurement but also on neutral opinion request advice and thought to retrieve a number of opinion sentence that a user could reasonably be expected to read we attempted to extract only explicitly stated writer s opinion at the sentence level and to exclude quoted or implicational opinion in our search engine opinion sentence are identified based on feature such a opinion clue expression and then the relevance to the query of each identified opinion sentence is checked the experimental result for various topic obtained by comparing the output of the proposed opinion search engine with that of human judgment a to whether the sentence were opinion showed that the proposed engine ha promise a a practical application 
various study within nlp and semantic web use the so called google count which is the hit count on a query returned by a search engine not only google however sometimes the google count is unreliable especially when the count is large or when advanced operator such a or and not are used in this paper we propose a novel algorithm that estimate the google count robustly it i us the co occurrence of term a evidence to estimate the occurrence of a given word and ii integrates multiple evidence for robust estimation we evaluated our algorithm for more than query on three datasets using google yahoo and msn search engine our algorithm also provides estimate count for any classifier that judge a web page a positive or negative consequently we can estimate the number of document with included reference of a particular person among namesake on the entire web 
the problem of locating motif in real valued multivariate time series data involves the discovery of set of recurring pattern embedded in the time series each set is composed of several non overlapping subsequence and constitutes a motif because all of the included subsequence are similar the ability to automatically discover such motif allows intelligent system to form endogenously meaningful representation of their environment through unsupervised sensor analysis in this paper we formulate a unifying view of motif discovery a a problem of locating region of high density in the space of all time series subsequence our approach is efficient sub quadratic in the length of the data requires fewer user specified parameter than previous method and naturally allows variable length motif occurrence and non linear temporal warping we evaluate the performance of our approach using four data set from different domain including on body inertial sensor and speech 
for many large system the computational complexity of complete model based diagnosis is prohibitive in this paper we investigate the speedup of the diagnosis process by exploiting the hierarchy locality a is typically present in well engineered system the approach comprises a compile time and a run time step in the first step a hierarchical cnf representation of the system is compiled to hierarchical dnf of adjustable hierarchical depth in the second step the diagnosis are computed from the hierarchical dnf and the actual observation our hierarchical algorithm while sound and complete allows large model to be diagnosed where compiletime investment directly translates to run time speedup the benefit of our approach are illustrated by using weak fault model of real world system including the iscas combinatorial circuit even for these non optimally partitioned problem the speedup compared to traditional approach range in the hundred 
this paper address the problem of recovering the location of both mobile device and access point from radio signal that come in a stream manner a problem which we call online co localization by exploiting both labeled and unlabeled data from mobile device and access point many tracking system function in two phase an offline training phase and an online localization phase in the training phase model are built from a batch of data that are collected offline many of them can not cope with a dynamic environment in which calibration data may come sequentially in such case these system may gradually become inaccurate without a manually costly re calibration to solve this problem we proposed an online co localization method that can deal with labeled and unlabeled data stream based on semi supervised manifold learning technique experiment conducted in wireless local area network show that we can achieve high accuracy with le calibration effort a compared to several previous system furthermore our method can deal with online stream data relatively faster than it two phase counterpart 
developing scalable algorithm for solving partially observable markov decision process pomdps is an important challenge one promising approach is based on representing pomdp policy a finite state controller this method ha been used successfully to address the intractable memory requirement of pomdp algorithm we illustrate some fundamental theoretical limitation of existing technique that use controller we then propose a new approach that formulates the problem a a quadratically constrained linear program qclp the solution of which provides an optimal controller of a desired size we evaluate several optimization method for solving qclps and compare their performance with existing pomdp optimization method while the optimization algorithm used in this paper only guarantee locally optimal solution the result show consistent improvement of solution quality over the state of the art technique the result show that powerful nonlinear programming algorithm can be used eectively to improve the performance and scalability of pomdp algorithm 
string kernel directly model sequence similarity without the necessity of extracting numerical feature in a vector space since they better capture complex trait in the sequence string kernel often achieve better prediction performance rna interference is an important biological mechanism with many therapeutical application where string can be used to represent target messenger rna and initiating short rna and string kernel can be applied for learning and prediction however existing string kernel are not particularly developed for rna application moreover most existing string kernel are n gram based and suffer from high dimensionality and inability of preserving subsequence ordering we propose a randomized string kernel for use with support vector regression with a purpose of better predicting silencing efficacy score for the candidate sequence and eventually improving the efficiency of biological experiment we show the positive definiteness of this kernel and give an analysis of randomization error rate empirical result on biological data demonstrate that the proposed kernel performed better than existing string kernel and achieved significant improvement over kernel computed from numerical descriptor extracted according to structural and thermodynamic rule in addition it is computationally more efficient 
this paper present a computational model of negotiation based on nebel s syntax based belief revision the model guarantee a unique bargaining solution for each bargaining game without using lottery it game theoretic property are discussed against the existence and uniqueness of nash equilibrium and subgame perfect equilibrium we also study essential computational property in relation to our negotiation model in particular we show that the deal membership checking is dp complete and the corresponding agreement inference problem is p hard 
our current understanding of the primate cerebral cortex neocortex and in particular the posterior sensory association cortex ha matured to a point where it is possible to develop a family of graphical model that capture the structure scale and power of the neocortex for purpose of associative recall sequence prediction and pattern completion among other function implementing such model using readily available computing cluster is now within the grasp of many lab and would provide scientist with the opportunity to experiment with both hard wired connection scheme and structure learning algorithm inspired by animal learning and developmental study while neural circuit involving structure external to the neocortex such a the thalamic nucleus are le well understood the availability of a computational model on which to test hypothesis would likely accelerate our understanding of these circuit furthermore the existence of an agreed upon cortical substrate would not only facilitate our understanding of the brain but enable researcher to combine lesson learned from biology with state of the art graphical model and machine learning technique to design hybrid system that combine the best of biological and traditional computing approach 
realistic domain for learning posse regularity that make it possible to generalize experience across related state this paper explores an environment modeling framework that represents transition a state independent outcome that are common to all state that share the same type we analyze a set of novel learning problem that arise in this framework providing lower and upper bound we single out one particular variant of practical interest and provide an efficient algorithm and experimental result in both simulated and robotic environment 
this research is motivated by the need to support inference across multiple intelligence system involving uncertainty our objective is to develop a theoretical framework and related inference method to map semantically similar variable between separate bayesian network in a principled way the work is to be conducted in two step in the first step we investigate the problem of formalizing the mapping between variable in two separate bns with different semantics and distribution a pair wise linkage in the second step we aim to justify the mapping between network a a set of selected variable linkage and then conduct inference along it 
there is growing interest in scaling up the widely used decision tree learning algorithm to very large data set although numerous diverse technique have been proposed a fast tree growing algorithm without substantial decrease in accuracy and substantial increase in space complexity is essential in this paper we present a novel fast decision tree learning algorithm that is based on a conditional independence assumption the new algorithm ha a time complexity of o m n where m is the size of the training data and n is the number of attribute this is a significant asymptotic improvement over the time complexity o m n of the standard decision tree learning algorithm c with an additional space increase of only o n experiment show that our algorithm performs competitively with c in accuracy on a large number of uci benchmark data set and performs even better and significantly faster than c on a large number of text classification data set the time complexity of our algorithm is a low a naive bayes indeed it is a fast a naive bayes but outperforms naive bayes in accuracy according to our experiment our algorithm is a core tree growing algorithm that can be combined with other scaling up technique to achieve further speedup 
we present a machine learning approach to identifying and resolving one anaphora in this approach the system first learns to distinguish different us of instance of the word one in the second stage the antecedent of those instance of one that are classified a anaphoric are then determined we evaluated our approach on written text drawn from the informative domain of the british national corpus bnc and achieved encouraging result to our knowledge this is the first learning based system for the identification and resolution of one anaphora 
in this paper we propose a new approach to establish singleton arc consistency sac on constraint network while the principle of existing sac algorithm involves performing a breadth first search up to a depth equal to the principle of the two algorithm introduced in this paper involves performing several run of a greedy search where at each step arc consistency is maintained it is then an original illustration of applying inference i e establishing singleton arc consistency by search using a greedy search allows benefiting from the incrementality of arc consistency learning relevant information from conflict and potentially finding solution s during the inference process further more both space and time complexity are quite competitive 
we introduce a human computer interaction system which collaborates with a user by providing feed back during user activity the goal of the system is to help a user complete a high level activity that ha been represented hierarchically while a user is performing the high level activity our system analyzes what sub event a user ha already completed and what sub event are needed next in order for the user to finish the activity the representation of human activity are constructed using a previously developed context free grammar based representation scheme we focus on a game named pentagram game to illustrate our system in the experiment our system showsthe ability to guide the user to complete the pentagram game by providing explicit feedback the feedback not only contains atomic level instruction but also describes higher level long term goal of the composite activity 
the development of a hybrid knowledge based system with a coupling between knowledge based and numerical method for multiobjective optimization of power distribution operation is described the advantage of a hybrid knowledge based system are described followed by the system objective mean of control and constraint a framework is provided that describes the necessary development stage of a commercial knowledge based package an overview of the utility knowledge acquisition procedure is provided to appreciate the complexity of defining the rule base this is followed by a description of the flow of information in a three level hierarchical rule base and a summary of network radiality parameter and performance rule employed in this rule base after a heuristic preprocessor identifies a list of switch closure that would seem to reduce total system loss network radiality rule ass if a particular search path ha identified a switch that can be closed and a corresponding switch opened to maintain the radiality of the system or if the path is worth pursuing further network parameter rule ensure the system operates within original design parameter network performance rule ass the reduction in total system loss of each proposed switching operation where there is a coupling between knowledge based and numerical method the integration of numerical method is described finally the validation and simulation a well a the benefit of this hybrid knowledge based system are described 
we consider an automated agent that need to coordinate with a human partner when communication between them is not possible or is undesirable tactic coordination game specifically we examine situation where an agent and human attempt to coordinate their choice among several alternative with equivalent utility we use machine learning algorithm to help the agent predict human choice in these tactic coordination domain learning to classify general human choice however is very difficult nevertheless human are often able to coordinate with one another in communication free game by using focal point prominent solution to coordination problem we integrate focal point into the machine learning process by transforming raw domain data into a new hypothesis space this result in classifier with an improved classification rate and shorter training time integration of focal point into learning algorithm also result in agent that are more robust to change in the environment 
in multi agent system agent often need to make decision about how to interact with each other when negotiating over task allocation in this paper we present oar a formal framework to address the question of how the agent should interact in an evolving environment in order to achieve their different goal the traditional categorization of self interested and cooperative agent is unified by adopting a utility view we illustrate mathematically that the degree of cooperativeness of an agent and the degree of it selfdirectness are not directly related we also show how oar can be used to evaluate different negotiation strategy and to develop distributed mechanism that optimize the performance dynamically this research demonstrates that sophisticated probabilistic modeling can be used to understand the behavior of a system with complex agent interaction 
this paper present a logical extension of nash s cooperative bargaining theory we introduce a concept of entrenchment measurement which map proposition to real number a a vehicle to represent agent s belief state and attitude towards bargaining situation we show that nash s bargaining solution can be restated in term of bargainer belief state negotiable item bargaining outcome and conflicting argument can then be explicitly expressed in propositional logic meanwhile nash s numerical solution to bargaining problem is still applicable 
proactive scheduling seek to generate high quality solution despite execution time uncertainty building on work in beck and wilson we conduct an empirical study of a number of algorithm for the job shop scheduling problem with probabilistic duration the main contribution of this paper are the introduction and empirical analysis of a novel constraint based search technique that can be applied beyond probabilistic scheduling problem the introduction and empirical analysis of a number of deterministic filtering algorithm for probabilistic job shop scheduling and the identification of a number of problem characteristic that contribute to algorithm performance 
the main focus in the area of action language such a golog wa put on expressive power while the development in the area of action planning wa focused on efficient plan generation an integration of golog and planning language would provide great advantage a user could constrain a system s behavior on a high level using golog while the actual low level action are planned by an efficient planning system first endeavor have been made by eyerich et al by identifying a subset of the situation calculus which is the basis of golog with the same expressiveness a the adl fragment of pddl however it wa not proven that the identified restriction define a maximum subset the most severe restriction appears to be that function are limited to constant we will show that this restriction is indeed necessary in most case 
this paper describes our research on automatic semantic argument classification using the propbank data kingsbury et al previous research employed feature that were based either on a full parse or shallow parse of a sentence these feature were mostly based on an individual semantic argument and the relation between the predicate and a semantic argument but they did not capture the interdependence among all argument of a predicate in this paper we propose the use of the neighboring semantic argument of a predicate a additional feature in determining the class of the current semantic argument our experimental result show significant improvement in the accuracy of semantic argument classification after exploiting argument interdependence argument classification accuracy on the standard section test set improves to representing a relative error reduction of 
we consider the problem of computing optimal schedule in multi agent system in these problem action of one agent can influence the action of other agent while the objective is to maximize the total quality of the schedule more specifically we focus on multi agent scheduling problem with time window hard and soft precedence relation and a nonlinear objective function we show how we can model and efficiently solve these problem with constraint programming technology element of our proposed method include constraint based reasoning search strategy problem decomposition scheduling algorithm and a linear programming relaxation we present experimental result on realistic problem instance to display the different element of the solution process 
we present a corpus based approach for the automation of help desk response to user email request automation is performed on the basis of the similarity between a request and previous request which affect both the content included in a response and the strategy used to produce it the latter is the focus of this paper which introduces a meta learning mechanism that selects between different information gathering strategy such a document retrieval and multidocument summarization our result show that this mechanism outperforms a random strategy selection policy and performs competitively with a gold baseline that always selects the best strategy 
several method for breaking value symmetry have been proposed recently in the constraint programming community they can be used in conjunction with variable symmetry breaking method however this combination doe not break all symmetry in general we present a combination of lex constraint and element constrants that can be used to break all combination of variable and value symmetry it is the first time to our knowledge that it is possible to break all combination of value and variable symmetry by adding constraint this method is quite efficient when the number of symmetry is not too large a shown by experiment using graceful graph problem we also present a new global constraint that deal with the case where there are too many value symmetry experiment show that this is highly effective 
a common obstacle preventing the rapid deployment of supervised machine learning algorithm is the lack of labeled training data this is particularly expensive to obtain for structured prediction task where each training instance may have multiple interacting label all of which must be correctly annotated for the instance to be of use to the learner traditional active learning address this problem by optimizing the order in which the example are labeled to increase learning efficiency however this approach doe not consider the difficulty of labeling each example which can vary widely in structured prediction task for example the labeling predicted by a partially trained system may be easier to correct for some instance than for others we propose a new active learning paradigm which reduces not only how many instance the annotator must label but also how difficult each instance is to annotate the system also leverage information from partially correct prediction to efficiently solicit annotation from the user we validate this active learning framework in an interactive information extraction system reducing the total number of annotation action by 
we present a corpus based hybrid approach to music analysis and composition which incorporates statistical connectionist and evolutionary component our framework employ artificial music critic which may be trained on large music corpus and then pas aesthetic judgment on music artifact music artifact are generated by an evolutionary music composer which utilizes music critic a fitness function to evaluate this approach we conducted three experiment first using music feature based on zipf s law we trained artificial neural network to predict the popularity of musical piece with accuracy then assuming that popularity correlate with aesthetic we incorporated such neural network into a genetic programming system called nevmuse nevmuse autonomously composed novel variation of j s bach s invention in a minor bwv variation which many listener found to be aesthetically pleasing finally we compared aesthetic judgment from an artificial music critic with emotional response from human subject significant correlation were found we provide evaluation result and sample of generated music these result have implication for music information retrieval and computer aided music composition 
optimal use of energy is a primary concern in fielddeployable sensor network artificial intelligence algorithm offer the capability to improve the performance or sensor network in dynamic environment by minimizing energy utilization while not compromising overall performance however they have been used only to a limited extent in sensor network primarily due to their expensive computing requirement we describe the use of markov decision process for the adaptive control of sensor sampling rate in a sensor network used for human health monitoring the mdp controller is designed to gather optimal information about the patient s health while guaranteeing a minimum lifetime of the system at every control step the mdp controller varies the frequency at which the data is collected according to the criticality of the patient s health at that time we present a stochastic model that is used to generate the optimal policy offline in case where a model of the observed process is not available a priori we descrihe a q learning technique to learn the control policy by using a pre existing master controller simulation result that illustrate the performance of the controller are presented 
when animal including human first explore a new environment what they remember is fragmentary knowledge about the place visited yet they have to use such fragmentary knowledge to find their way home human naturally use more powerful heuristic while lower animal have shown to develop a variety of method that tend to utilize two key piece of information namely distance and orientation information their method differ depending on how they sense their environment could a mobile robot be used to investigate the nature of such a process commonly referred to in the psychological literature a cognitive mapping what might be computed in the initial exploration and how is the resulting cognitive map be used to return home in this paper we presented a novel approach using a mobile robot to do cognitive mapping our robot computes a cognitive map and us distance and orientation information to find it way home the process developed provides interesting insight into the nature of cognitive mapping and encourages u to use a mobile robot to do cognitive mapping in the future a opposed to it popular use in robot mapping 
automated recommender system predict user preference by applying machine learning technique to data on product user and past user preference for product such system have become increasingly popular in entertainment and e commerce domain but have thus far had little success in information seeking domain such a identifying published research of interest we report on several recent publication that show how recommenders can be extended to more effectively address information seeking task by expanding the focus from accurate prediction of user preference to identifying a useful set of item to recommend in response to the user s specific information need specific research demonstrates the value of diversity in recommendation list show how user value list of recommendation a something different from the sum of the individual recommendation within and present an analytic model for customizing a recommender to match user information seeking need 
in this paper we outline the development of a system that automatically construct ontology by extracting knowledge from dictionary definition sentence using robust minimal recursion semantics rmrs a semantic formalism that permit underspecification we show that by combining deep and shallow parsing resource through the common formalism of rmrs we can extract ontological relation in greater quality and quantity our approach also ha the advantage of requiring a very small amount of rule and being easily adaptable to any language with rmrs resource 
gait optimization is a basic yet challenging problem for both quadrupedal and bipedal robot although technique for automating the process exist most involve local function optimization procedure that suffer from three key drawback local optimization technique are naturally plagued by local optimum make no use of the expensive gait evaluation once a local step is taken and do not explicitly model noise in gait evaluation these drawback increase the need for a large number of gait evaluation making optimization slow data inefficient and manually intensive we present a bayesian approach based on gaussian process regression that address all three drawback it us a global search strategy based on a posterior model inferred from all of the individual noisy evaluation we demonstrate the technique on a quadruped robot using it to optimize two different criterion speed and smoothness we show in both case our technique requires dramatically fewer gait evaluation than state of the art local gradient approach 
various approach for dealing with missing data have been developed so far in this paper two strategy are proposed for cost sensitive iterative imputing missing value with optimal ordering experimental result demonstrate that proposed strategy outperform the existing method in term of imputation cost and accuracy 
this paper describes recent result from the robotics community that develop a theory similar in spirit to the theory of computation for analyzing sensor based agent system the central element to this work is a notion of dominance of one such system over another this relation is formally based on the agent progression through a derived information space but may informally be understood a describing one agent s ability to simulate another we present some basic property of this dominance relation and demonstrate it usefulness by applying it to a basic problem in robotics we argue that this work is of interest to a broad audience of artificial intelligence researcher for two main reason first it call attention to the possibility of studying belief space in way that generalizes both probabilistic and nondeterministic uncertainty model second it provides a mean for evaluating the information that an agent is able to acquire via it sensor and via conformant action independent of any optimality criterion and of the task to be completed 
this paper present a self supervised framework for perceptual learning based upon correlation in different sensory modality we demonstrate this with a system that ha learned the vowel structure of american english i e the number of vowel and their phonetic description by simultaneously watching and listening to someone speak it is highly non parametric knowing neither the number of vowel nor their input distribution in advance and it ha no prior linguistic knowledge this work is the first example of unsupervised phonetic acquisition of which we are aware outside of that done by human infant this system is based on the cross modal clustering framework introduced by which ha been significantly enhanced here this paper present our result and focus on the mathematical framework that enables this type of intersensory self supervised learning 
a fundamental problem for artificial intelligence is identifying perceptual primitive from raw sensory signal that are useful for higher level reasoning we equate these primitive with initially unknown recurring pattern called motif autonomously learning the motif is difficult because their number location length and shape are all unknown furthermore nonlinear temporal warping may be required to ensure the similarity of motif occurrence in this paper we extend a leading motif discovery algorithm by allowing it to operate on multidimensional sensor data incorporating automatic parameter estimation and providing for motif specific similarity adaptation we evaluate our algorithm on several data set and show how our approach lead to faster real world discovery and more accurate motif compared to other leading method 
an efficient parsing technique for hpsg is presented recent research ha shown that supertagging is a key technology to improve both the speed and accuracy of lexicalized grammar parsing we show that further speed up is possible by eliminating non parsable lexical entry sequence from the output of the supertagger the parsability of the lexical entry sequence is tested by a technique called cfg filtering where a cfg that approximates the hpsg is used to test it those lexical entry sequence that passed through the cfg filter are combined into parse tree by using a simple shift reduce parsing algorithm in which structural ambiguity are resolved using a classifier and all the syntactic constraint represented in the original grammar are checked experimental result show that our system give comparable accuracy with a speed up by a factor of six msec sentence compared with the best published result using the same grammar 
this paper present a novel framework for simultaneously learning representation and control in continuous markov decision process our approach build on the framework of proto value function in which the underlying representation or basis function are automatically derived from a spectral analysis of the state space manifold the proto value function correspond to the eigenfunctions of the graph laplacian we describe an approach to extend the eigenfunctions to novel state using the nystr m extension a least square policy iteration method is used to learn the control policy where the underlying subspace for approximating the value function is spanned by the learned proto value function a detailed set of experiment is presented using classic benchmark task including the inverted pendulum and the mountain car showing the sensitivity in performance to various parameter and including comparison with a parametric radial basis function method 
one of the main way to specify goal of agent is to use temporal logic most existing temporal logic are monotonic however in representing goal of agent we often require that goal be changed non monotonically for example the initial goal of the agent may be to be always in state where p is true the agent may later realize that under certain condition exception it is ok to be in state where p is not true in this paper we propose a simple extension of ltl which we call n ltl that allows non monotonic specification of goal we study property of n ltl we also consider a translation from n ltl to logic program and study the relationship between n ltl and logic program 
a key challenge in multiagent environment is the construction of agent that are able to learn while acting in the presence of other agent that are simultaneously learning and adapting these domain require on line learning method without the benefit of repeated training example a well a the ability to adapt to the evolving behavior of other agent in the environment the difficulty is further exacerbated when the agent are in an adversarial relationship demanding that a robust i e winning non stationary policy be rapidly learned and adapted we propose an on line sequence learning algorithm elph based on a straightforward entropy pruning technique that is able to rapidly learn and adapt to non stationary policy we demonstrate the performance of this method in a non stationary learning environment of adversarial zero sum matrix game 
we present a novel low overhead framework for encoding and utilizing structural symmetry in propositional satisfiability algorithm sat solver we use the notion of complete multi class symmetry and demonstrate the efficacy of our technique through a solver symchaff that achieves exponential speedup by using simple tag in the specification of problem from both theory and practice efficient implementation of dpll based sat solver are routinely used in area a diverse a planning scheduling design automation model checking verification testing and algebra a natural feature of many application domain is the presence of symmetry such a that amongst all truck at a certain location in logistics planning and all wire connecting two switch box in an fpga circuit many of these problem turn out to have a concise description in many sorted first order logic this description can be easily specified by the problem designer and almost a easily inferred automatically symchaff an extension of the popular sat solver zchaff us information obtained from the sort in the first order logic constraint to create symmetry set that are used to partition variable into class and to maintain and utilize symmetry information dynamically current approach designed to handle symmetry include a symmetry breaking predicate sbps b pseudo boolean solver with implicit representation for counting c modification of dpll that handle symmetry dynamically and d technique based on zbdds sbps are prohibitively many often large and expensive to compute for problem such a the one we report experimental result for pseudo boolean solver are provably exponentially slow in certain symmetric situation and their implicit counting representation is not always appropriate suggested modification of dpll either work on limited global symmetry and are difficult to extend or involve expensive algebraic group computation finally technique based on zbdds often do not compare well even with ordinary dpll based solver sym chaff address and overcomes most of these limitation 
in open and distributed system agent must engage in interaction of which they have no previous experience deontic model are widely used to describe aspect of permission obligation and trust anticipated by such agent but no practical mechanism ha been developed for testing deontic trust specification against model of multi agent interaction this paper describes a way of doing this an implementation of it via model checking and some preliminary result on a realistic example 
we consider the fundamental problem of monitoring i e tracking the belief state in a dynamic system when the model is only approximately correct and when the initial belief state might be unknown in this general setting where the model is perhaps only slightly mi specified monitoring and consequently planning may be impossible a error might accumulate over time we provide a new characterization the value of observation which allows u to bound the error accumulation the value of observation is a parameter that governs how much information the observation provides for instance in partially observable mdps when it is the pomdp is an mdp while for an unobservable markov decision process the parameter is thus the new parameter characterizes a spectrum from mdps to unobservable mdps depending on the amount of information conveyed in the observation 
in this paper we address the problem of the automated composition of web service by planning on their knowledge level model we start from description of web service in standard process modeling and execution language like bpel w and automatically translate them into a planning domain that model the interaction among service at the knowledge level this allows u to avoid the explosion of the search space due to the usually large and possibly infinite range of data value that are exchanged among service and thus to scale up the applicability of state of the art technique for the automated composition of web service we present the theoretical framework implement it and provide an experimental evaluation that show the practical advantage of our approach w r t technique that are not based on a knowledgelevel representation 
the textual entailment problem is to determine if a given text entail a given hypothesis this paper describes first a general generative probabilistic setting for textual entailment we then focus on the sub task of recognizing whether the lexical concept present in the hypothesis are entailed from the text this problem is recast a one of text categorization in which the class are the vocabulary word we make novel use of na ve bayes to model the problem in an entirely unsupervised fashion empirical test suggest that the method is effective and compare favorably with state of the art heuristic scoring approach 
action graph game aggs bhat leyton brown are a fully expressive game representation which can compactly express strict and context specific independence and anonymity structure in player utility function we present an efficient algorithm for computing expected payoff under mixed strategy profile this algorithm run in time polynomial in the size of the agg representation which is itself polynomial in the number of player when the in degree of the action graph is bounded we also present an extension to the agg representation which allows u to compactly represent a wider variety of structured utility function 
we study the bidding behavior of spiteful agent who contrary to the common assumption of self interest maximize a convex combination of their own profit and their competitor loss the motivation for this assumption stem from inherent spitefulness or for example from competitive scenario such a in closed market where the loss of a competitor will likely result in future gain for oneself we derive symmetric bayes nash equilibrium for spiteful agent in st price and nd price sealedbid auction in st price auction bidder become more truthful the more spiteful they are surprisingly the equilibrium strategy in nd price auction doe not depend on the number of bidder based on these equilibrium we compare the revenue in both auction type it turn out that expected revenue in nd price auction is higher than expected revenue in st price auction in the case of even the most modestly spiteful agent provided they still care at least at little for their own profit in other word revenue equivalence only hold for auction in which all agent are either self interested or completely malicious we furthermore investigate the impact of common knowledge on spiteful bidding divulging the bidder valuation reduces revenue in nd price auction whereas it ha the opposite effect in st price auction 
detecting and tracking latent factor from temporal data is an important task most existing algorithm for latent topic detection such a nonnegative matrix factorization nmf have been designed for static data these algorithm are unable to capture the dynamic nature of temporally changing data stream in this paper we put forward an online nmf onmf algorithm to detect latent factor and track their evolution while the data evolve by leveraging the already detected latent factor and the newly arriving data the latent factor are automatically and incrementally updated to reflect the change of factor furthermore by imposing orthogonality on the detected latent factor we can not only guarantee the unique solution of nmf but also alleviate the partial data problem which may cause nmf to fail when the data are scarce or the distribution is incomplete experiment on both synthesized data and real data validate the efficiency and effectiveness of our onmf algorithm 
many reasoning task and combinatorial problem exhibit symmetry exploiting such symmetry ha been proved to be very important in reducing search effort breaking symmetry using additional constraint is currently one of the most used approach extending such symmetry breaking technique to quantified boolean formula qbf is a very challenging task in this paper an approach to break symmetry in quantified boolean formula is proposed it make an original use of universally quantified auxiliary variable to generate new symmetry breaking predicate and a new ordering of the qbf prefix is then computed leading to a new equivalent qbf formula with respect to validity experimental evaluation of the state of the art qbf solver semprop show significant improvement up to several order of magnitude on many qbfs instance 
this paper formalizes a well known psychological model of emotion in an agent specification language this is done by introducing a logical language and it semantics that are used to specify an agent model in term of mental attitude including emotion we show that our formalization render a number of intuitive and plausible property of emotion we also show how this formalization can be used to specify the effect of emotion on an agent s decision making process ultimately the emotion in this model function a heuristic a they constrain an agent s model 
td falcon is a self organizing neural network that incorporates temporal difference td method for reinforcement learning despite the advantage of fast and stable learning td falcon still relies on an iterative process to evaluate each available action in a decision cycle to remove this deficiency this paper present a direct code access procedure whereby td falcon conduct instantaneous search for cognitive node that match with the current state and at the same time provide maximal reward value our comparative experiment show that td falcon with direct code access produce comparable performance with the original td falcon while improving significantly in computation efficiency and network complexity 
the world is unpredictable and acting intelligently requires anticipating possible consequence of action that are taken assuming that the action and the world are deterministic planning can be represented in the classical propositional logic introducing nondeterminism but not probability or several initial state increase the complexity of the planning problem and requires the use of quantified boolean formula qbf the currently leading logic based approach to conditional planning use explicitly or implicitly a qbf with the prefix we present formalization of the planning problem a qbf which have an asymptotically optimal linear size and the optimal number of quantifier alternation in the prefix and this is in accordance with the fact that the planning problem under the restriction to polynomial size plan is on the second level of the polynomial hierarchy not on the third 
proteome analyst pa is a publicly available high throughput web based system for automatically predicting the function and property of protein biologist can use pa to predict for example the gene ontology go molecular function and subcellular localization of a protein based on sequence information using sequence analysis tool and machine learning pa give high accuracy and broad coverage for both molecular function and subcellular localization prediction 
this paper describes seggen a new algorithm for linear text segmentation on general corpus it aim to segment text into thematic homogeneous part several existing method have been used for this purpose based on a sequential creation of boundary here we propose to consider boundary simultaneously thanks to a genetic algorithm seggen us two criterion maximization of the internal cohesion of the formed segment and minimization of the similarity of the adjacent segment first experimental result are promising and seggen appears to be very competitive compared with existing method 
this paper present a formalism to describe practical reasoning in term of an action based alternating transition system aats the starting point is a previously specified account of practical reasoning that treat reasoning about what action should be chosen a presumptive argumentation using argument scheme and associated critical question this paper describes how this account can be extended to situation where the effect of an action is partially dependent upon the choice of another agent in this context we see practical reasoning a proceeding in three stage the first involves determining the representation of the particular problem scenario a an aats next the agent must resolve it uncertainty a to it position in the scenario finally the agent move to choosing a particular action to achieve it end proposing presumptive reason for particular action and subjecting them to a critique to establish their suitability taking into account the choice that can be made by the other agent involved this account thus provides a well specified basis for addressing the problem of practical reasoning a presumptive argumentation in a multi agent context 
in this paper we consider dynamical property of simple iterative relational classifier we conjecture that for a class of algorithm that use label propagation the iterative procedure can lead to nontrivial dynamic in the number of newly classified instance the underlaying reason for this nontriviality is that in relational network true class label are likely to propagate faster than false one we suggest that this phenomenon which we call two tiered dynamic for binary classifier can be used for establishing a self consistent classification threshold and a criterion for stopping iteration we demonstrate this effect for two unrelated binary classification problem using a variation of a iterative relational neighbor classifier we also study analytically the dynamical property of the suggested classifier and compare it result to the numerical experiment on synthetic data 
reasoning about the knowledge of an agent is an important problem in many area of ai for example in diagnosis a basic question about a system is whether it is possible to diagnose it that is whether it is always possible to know whether a faulty behavior ha occurred in this paper we investigate the complexity of this diagnosability problem and the size of automaton that perform diagnosis there are algorithm for testing diagnosability in polynomial time in the number of state in the system for succinct system representation which may be exponentially smaller than the state space of the system the diagnosability problem is consequently in exptime we show that this upper bound is not tight and that the decision problem is in fact pspace complete on line diagnosis can be carried out by diagnosers which are automaton that recognize faulty behavior we show that diagnosers in the worst case have a size that is exponential in the number of state both for explicit and succinct system representation this is a consequence of the diagnoser having to maintain belief about the state of the system 
the map maximum a posteriori hypothesis problem in bayesian network is to find the most likely state of a set of variabls given partial evidence on the complement of that set standard structure based inference method for finding exact solution to map such a variable elimination and join tree algorithm have complexity that are exponential in the constrained treewidth of the network a more recent algorithm proposed by park and darwiche is exponential only in the treewidth and ha been shown to handle network whose constrained treewidth is quite high in this paper we present a new algorithm for exact map that is not necessarily limited in scalability even by the treewidth this is achieved by leveraging recent advance in compilation of bayesian network into arithmetic circuit which can circumvent treewidth imposed limit by exploiting the local structure present in the network specifically we implement a branch and bound search where the bound are computed using linear time operation on the compiled arithmetic circuit on network with local structure we observe order of magnitude improvement over the algorithm of park and darwiche in particular we are able to efficiently solve many problem where the latter algorithm run out of memory because of high treewidth 
we explore an application to the game of go of a reinforcement learning approach based on a linear evaluation function and large number of binary feature this strategy ha proved effective in game playing program and other reinforcement learning application we apply this strategy to go by creating over a million feature based on template for small fragment of the board and then use temporal difference learning and self play this method identifies hundred of low level shape with recognisable significance to expert go player and provides quantitive estimate of their value we analyse the relative contribution to performance of template of different type and size our result show that small translation invariant template are surprisingly effective we ass the performance of our program by playing against the average liberty player and a variety of computer opponent on the computer go server our linear evaluation function appears to outperform all other static evaluation function that do not incorporate substantial domain knowledge 
modeling and simulation have great potential a technology capable of aiding analyst in making accurate prediction of future situation to help provide competitive advantage and avoid strategic surprise however to make modeling and simulation effective an evidence marshaling process is needed that address the information need of the modeling task a detailed by subject matter expert we suggest that such an evidence marshaling process can be obtained by combining natural language processing and content analysis technique to provide quantified qualitative content assessment and describe a case study on the acquisition and marshaling of frame from unstructured text 
evaluating text fragment for positive and negative subjective expression and their strength can be important in application such a singleor multidocument summarization document ranking data mining etc this paper look at a simplified version of the problem classifying online product review into positive and negative class we discus a series of experiment with different machine learning algorithm in order to experimentally evaluate various trade offs using approximately k product review from the web 
we present to our knowledge the first mixed integer program mip formulation for finding nash equilibrium in game specifically two player normal form game we study different design dimension of search algorithm that are based on those formulation our mip nash algorithm outperforms lemke howson but not porter nudelman shoham pns on gamut data we argue why experiment should also be conducted on game with equilibrium with medium sized support only and present a methodology for generating such game on such game mip nash drastically outperforms pns but not lemke howson certain mip nash formulation also yield anytime algorithm for equilibrium with provable bound another advantage of mip nash is that it can be used to find an optimal equilibrium according to various objective the prior algorithm can be extended to that setting but they are order of magnitude slower 
in the majority of case steel production constitutes the inception of the supply chain they are involved just a in automotive cluster or aerospace steel manufactunng company are affected strongest by bull whip effect or other unpredictable influence along the production chain to the customer therefore flexible planning and realisation a well a fast reorganisation after interference are indispensable requirement for a competitive position on the market in this paper masdispo an agent based decision support system for production and control inside the steel work of saarstahl ag a globally respected steel manufacturer is presented it is based on a distributed online planning and online scheduling algorithm to calculate solution supporting production and control inside the melting shop it monitor the execution of their chosen solution and responds to unpredicted change during production by dynamically adapting the schedule this paper give an overview of the system the approach for solving the complex problem of steel production and control the development process the main experience a well a lesson learned 
querying description logic knowledge base ha received great attention in the last year in such a problem the need of coping with incomplete information is the distinguishing feature with respect to querying database due to this feature we have to deal with two conflicting need on the one hand we would like to query the knowledge base with sophisticated mechanism provided by full first order logic fol on the other hand the presence of incomplete information make query answering a much more difficult task than in database in this paper we advocate the use of a nonmonotonic epistemic fol query language a a mean for expressing sophisticated query over description logic knowledge base we show that through a controlled use of the epistemic operator resulting in the language called eql lite we are able to formulate full fol query over description logic knowledge base while keeping computational complexity of query answering under control in particular we show that eql lite query over dl lite knowledge base are fol reducible i e compilable into sql and hence can be answered in logspace through standard database technology 
this paper proposes a methodology for learning joint probability estimate regarding the effect of sensorimotor feature on the predicated quality of desired behavior these relationship can then be used to choose action that will most likely produce success relational dependency network are used to learn statistical model of procedural task knowledge an example task expert for picking up object is learned through actual experience with a humanoid robot we believe that this approach is widely applicable and ha great potential to allow a robot to autonomously determine which feature in the world are salient and should be used to recommend policy for action 
combining multiple classifier via combining scheme or meta learner ha led to substantial improvement in many classification problem one of the challenging task is to choose appropriate combining scheme and classifier involved in an ensemble of classifier in this paper we propose a novel evidential approach to combining decision given by multiple classifier we develop a novel evidence structure a focal triplet examine it theoretical property and establish computational formulation for representing classifier output a piece of evidence to be combined the evaluation on the effectiveness of the established formalism have been carried out over the data set of newsgroup and reuters demonstrating the advantage of this novel approach in combining classifier 
in recent year certain formalization of combinatorial negotiation setting most notably combinatorial auction have become an important research topic in the ai community a pervasive assumption ha been that of no externality the agent deciding on a variable such a whether a trade take place between them are the only one affected by how this variable is set to date there ha been no widely studied formalization of combinatorial negotiation setting with externality in this paper we introduce such a formalization we show that in a number of key special case it is np complete to find a feasible nontrivial solution and therefore the maximum social welfare is completely inapproximable however for one important special case we give an algorithm which converges to the solution with the maximal concession by each agent in a linear number of round for utility function that decompose into piecewise constant function maximizing social welfare however remains np complete even in this setting we also demonstrate a special case which can be solved in polynomial time by linear programming 
intelligent tutoring system it can provide effective instruction but learner do not always use such system effectively in the present study high school student action sequence with a mathematics it were machine classified into five finite state machine indicating guessing strategy appropriate help use and independent problem solving over of problem event were categorized student were grouped via cluster analysis based on self report of motivation motivation grouping predicted it strategic approach better than prior math achievement a rated by classroom teacher learner who reported being disengaged in math were most likely to exhibit appropriate help use while working with the it relative to average and high motivation learner the result indicate that learner can readily report their motivation state and that these data predict how learner interact with the it 
developing scalable coordination algorithm for multi agent system is a hard computational challenge one useful approach demonstrated by the coverage set algorithm csa exploit structured interaction to produce significant computational gain empirically csa exhibit very good anytime performance but an error bound on the result ha not been established we reformulate the algorithm and derive both online and offline error bound for approximate solution moreover we propose an effective way to automatically reduce the complexity of the interaction our experiment show that this is a promising approach to solve a broad class of decentralized decision problem the general formulation used by the algorithm make it both easy to implement and widely applicable to a variety of other ai problem 
in this paper we first provide a new theoretical understanding of the evidence pre propagated importance sampling algorithm epi bn yuan druzdzel b and show that it importance function minimizes the kl divergence between the function itself and the exact posterior probability distribution in polytrees we then generalize the method to deal with inference in general hybrid bayesian network consisting of deterministic equation and arbitrary probability distribution using a novel technique called soft arc reversal the new algorithm can also handle evidential reasoning with observed deterministic variable 
computing semantic relatedness of natural language text requires access to vast amount of common sense and domain specific world knowledge we propose explicit semantic analysis esa a novel method that represents the meaning of text in a high dimensional space of concept derived from wikipedia we use machine learning technique to explicitly represent the meaning of any text a a weighted vector of wikipedia based concept assessing the relatedness of text in this space amount to comparing the corresponding vector using conventional metric e g cosine compared with the previous state of the art using esa result in substantial improvement in correlation of computed relatedness score with human judgment from r to for individual word and from r to for text importantly due to the use of natural concept the esa model is easy to explain to human user 
case based reasoning in short is the process of solving new problem based on solution of similar past problem much like human solve many problem mycbr an extension of the ontology editor prot g provides such similarity based retrieval functionality moreover the user is supported in modelling appropriate similarity measure by forward and backward explanation 
table are ubiquitous in web page and scientific document with the explosive development of the web table have become a valuable information repository therefore effectively and efficiently searching table becomes a challenge existing search engine do not provide satisfactory search result largely because the current ranking scheme are inadequate for table search and automatic table understanding and extraction are rather difficult in general in this work we design and evaluate a novel table ranking algorithm tablerank to improve the performance of our table search engine table seer given a keyword based table query tablerank facility tableseer to return the most relevant table by tailoring the classic vector space model tablerank adopts an innovative term weighting scheme by aggregating multiple weighting factor from three level term table and document the experimental result show that our table search engine out performs existing search engine on table search in addition incorporating multiple weighting factor can significantly improve the ranking result 
rotating workforce scheduling is a typical constraint satisfaction problem which appears in a broad range of work place e g industrial plant solving this problem is of a high practical relevance in this paper we propose the combination of tabu search with random walk and min conflict strategy to solve this problem computational result for benchmark example in literature and other real life example show that combination of tabu search with random walk and min conflict strategy improves the performance of tabu search for this problem the method proposed in this paper improve performance of the state of art commercial system for generation of rotating workforce schedule 
nonmonotonic causal logic invented by mccain and turner is a formalism well suited for representing knowledge about action and the definite fragment of that formalism ha been implemented in the reasoning and planning system called ccalc a theorem due to mccain show howto translate definite causal theory into logic programming under the answer set semantics and thus open the possibility of using answer set programming for the implementation of such theory in this paper we propose a generalization of mccain s theorem that extends it in two direction first it is applicable to arbitrary causal theory not only definite second it cover causal theory of a more general kind which can describe non boolean fluents 
the asknet project us a combination of nlp tool and spreading activation to transform natural language text into semantic knowledge network network fragment are generated from input sentence using a parser and semantic analyser then these fragment are combined using spreading activation based algorithm the ultimate goal of the project is to create a semantic resource on a scale that ha never before been possible we have already managed to create network more than twice a large a any comparable resource million node million edge in le than day this report provides a summary of the project and it current state of development 
this paper present a generic architecture for proof planning system in term of an interaction between a customisable proof module and search module these refer to both global and local information contained in reasoning state 
this paper address the problem of activity recognition for physically embodied agent team we define team activity recognition a the process of identifying team behavior from trace of agent position over time for many physical domain military or athletic coordinated team behavior create distinctive spatio temporal pattern that can be used to identify low level action sequence this paper focus on the novel problem of recovering agent to team assignment for complex team task where team composition the mapping of agent into team change over time without a priori knowledge of current team assignment the behavior recognition problem is challenging since behavior are characterized by the aggregate motion of the entire team and cannot generally be determined by observing the movement of a single agent in isolation to handle this problem we introduce a new algorithm simultaneous team assignment and behavior recognition stabr that generates behavior annotation from spatio temporal agent trace the proposed approach is able to perform accurate team behavior recognition without an exhaustive search over the combinatorial space of potential team assignment a demonstrated on several scenario of simulated military maneuver 
combinatorial design problem arise in many application area and are naturally modelled in term of set variable and constraint traditionally the domain of a set variable is specified by two set r e and denotes all set containing r and disjoint from e this representation ha inherent difficulty in handling cardinality and lexicographic constraint so important in combinatorial design this paper take a dual view of set variable it proposes a representation that encodes directly cardinality and lexicographic information by totally ordering a set domain with a length lex ordering the solver can then enforce bound consistency on all unary constraint in time k where k is the set cardinality in analogy with finite domain solver non unary constraint can be viewed a inference rule generating new unary constraint the resulting set solver achieves a pruning at least comparable to the hybrid domain of sadler and gervet at a fraction of the computational cost 
efficient learning equilibrium ele is a natural solution concept for multi agent encounter with incomplete information it requires the learning algorithm themselves to be in equilibrium for any game selected from a set of initially unknown game in an optimal ele the learning algorithm would efficiently obtain the surplus the agent would obtain in an optimal nash equilibrium of the initially unknown game which is played the crucial part is that in an ele deviation from the learning algorithm would become nonbeneficial after polynomial time although the game played is initially unknown while appealing conceptually the main challenge for establishing learning algorithm based on this concept is to isolate general class of game where an ele exists unfortunately it ha been shown that while an ele exists for the setting in which each agent can observe all other agent action and payoff an ele doe not exist in general when the other agent payoff cannot be observed in this paper we provide the first positive result on this problem constructively proving the existence of an optimal ele for the class of symmetric game where an agent can not observe other agent payoff 
a variety of business relationship in open setting can be understood in term of the creation and manipulation of commitment among the participant these include b c and b b contract and process a realized via web service and other such technology business protocol an interaction oriented approach for modeling business process are formulated in term of the commitment commitment can support other form of semantic service composition a well this paper show how to represent and reason about commitment in a general manner unlike previous formalization the proposed formalization accommodates complex and nested commitment condition and concurrent commitment operation in this manner a rich variety of open business scenario are enabled 
collaborative application require protocol that specify how distributed entity interact with one another in order to achieve a specified behavior many different kind of relationship can be established between these entity a a result of such interaction distributive and collective reading are two important way to characterize group interaction starting from an attempt based semantics of group communicative act we distinguish between these two concept and evaluate group protocol with respect to formation of different type of team during the interaction 
in this paper we describe an evolutionary approach to one of the most challenging problem in computer music modeling the knowledge applied by a musician when performing a score of a piece in order to produce an expressive performance of the piece we extract a set of acoustic feature from jazz recording thereby providing a symbolic representation of the musician s expressive performance by applying a sequential covering evolutionary algorithm to the symbolic representation we obtain an expressive performance computational model capable of endowing a computer generated music performance with the timing and energy expressiveness that characterizes human generated music 
this paper present a method for learning a semantic parser from ambiguous supervision training data consists of natural language sentence annotated with multiple potential meaning representation only one of which is correct such ambiguous supervision model the type of supervision that can be more naturally available to language learning system given such weak supervision our approach produce a semantic parser that map sentence into meaning representation an existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision experimental result show that the resulting system is able to cope up with ambiguity and learn accurate semantic parser 
we propose a framework for multi issue bilateral negotiation where issue are expressed and related to each other via description logic agent goal are expressed through complex concept and the worth of goal a weight over concept we adopt a very general setting with incomplete information by letting agent keep both goal and worth of goal a private information we introduce a negotiation protocol for such a setting and discus different possible strategy that agent can adopt during the negotiation process we show that such a protocol converges if the description logic used enjoys the finite implicants property 
we develop an efficient incremental version of an existing cost based filtering algorithm for the knapsack constraint on a universe of n element m invocation of the algorithm require a total of o n log n mk log n k time where k n depends on the specific knapsack instance we show that the expected value of k is significantly smaller than n on several interesting input distribution hence while keeping the same worst case complexity on expectation the new algorithm is faster than the previously best method which run in amortized linear time after a theoretical study we introduce heuristic enhancement and demonstrate the new algorithm s performance experimentally 
in this paper we apply spectral technique to clustering biological sequence data that ha proved more difficult to cluster effectively for this purpose we have to extend spectral clustering algorithm to deal with asymmetric affinity like the alignment score used in the comparison of biological sequence and devise a hierarchical algorithm that can handle many cluster with imbalanced size robustly we present an algorithm for clustering asymmetric affinity data and demonstrate the performance of this algorithm at recovering the higher level of the structural classification of protein scop on a data base of highly conserved subsequence 
argumentation theory ha become an important topic in the field of ai the basic idea is to construct argument in favor and against a statement to select the acceptable one and finally to determine whether the statement can be accepted or not dung s elegant account of abstract argumentation dung may have caused some to believe that defining an argumentation formalism is simply a matter of determining how argument and their defeat relation can be constructed from a given knowledge base unfortunately thing are not that simple many straightforward instantiation of dung s theory can lead to very unintuitive result a is discussed in this paper in order to avoid such anomaly in this paper we are interested in defining some rule called rationality postulate or axiom that govern the well definition of an argumentation system in particular we define two important rationality postulate that any system should satisfy the consistency and the closeness of the result returned by that system we then provide a relatively easy way in which these quality postulate can be warranted by our argumentation system 
a new spectral approach to value function approximation ha recently been proposed to automatically construct basis function from sample global basis function called proto value function are generated by diagonalizing a diffusion operator such a a reversible random walk or the laplacian on a graph formed from connecting nearby sample this paper address the challenge of scaling this approach to large domain we propose using kronecker factorization coupled with the metropolis hastings algorithm to decompose reversible transition matrix the result is that the basis function can be computed on much smaller matrix and combined to form the overall base we demonstrate that in several continuous markov decision process compact basis function can be constructed without significant loss in performance in one domain basis function were compressed by a factor of a theoretical analysis relates the quality of the approximation to the spectral gap our approach generalizes to other basis construction a well 
reinforcement learning problem are commonly tackled with temporal difference method which attempt to estimate the agent s optimal value function in most real world problem learning this value function requires a function approximator which map state action pair to value via a concise parameterized function in practice the success of function approximators depends on the ability of the human designer to select an appropriate representation for the value function a recently developed approach called evolutionary function approximation us evolutionary computation to automate the search for effective representation while this approach can substantially improve the performance of td method it requires many sample episode to do so we present an enhancement to evolutionary function approximation that make it much more sample efficient by exploiting the off policy nature of certain td method empirical result in a server job scheduling domain demonstrate that the enhanced method can learn better policy than evolution or td method alone and can do so in many fewer episode than standard evolutionary function approximation 
a key challenge in applying kernel based method for discriminative learning is to identify a suitable kernel given a problem domain many method instead transform the input data into a set of vector in a feature space and classify the transformed data using a generic kernel however finding an effective transformation scheme for sequence e g time series data is a difficult task in this paper we introduce a scheme for directly designing kernel for the classification of sequence data such a that in handwritten character recognition and object recognition from sensor reading ordering information is represented by value of a parameter associated with each input data element a similarity metric based on the parametric distance between corresponding element is combined with their problemspecific similarity metric to produce a mercer kernel suitable for use in method such a support vector machine svm this scheme directly embeds extraction of feature from sequence of varying cardinality into the kernel without needing to transform all input data into a common feature space before classification we apply our method to object and handwritten character recognition task and compare against current approach the result show that we can obtain at least comparable accuracy to state of the art problem specific method using a systematic approach to kernel design our contribution is the introduction of a general technique for designing svm kernel tailored for the classification of sequence data 
in distributed system learning doe not necessarily involve the participation of agent directly in the inductive process itself instead many system frequently employ multiple instance of induction separately in this paper we develop and evaluate a new approach for learning in distributed system that tightly integrates process of induction between agent based on inductive logic programming technique the paper s main contribution is the integration of an epistemic approach to reasoning about knowledge with inverse entailment during induction the new approach facilitates a systematic approach to the sharing of knowledge and invention of predicate only when required we illustrate the approach using the well known path planning problem and compare result empirically to multiple instance of single agent based induction over varying distribution of data given a chosen path planning algorithm our algorithm enables agent to combine their local knowledge in an effective way to avoid central control while significantly reducing communication cost 
abstract this paper considers the problem of stochastic robustness testing for plan although plan generation system might be proven sound the resulting plan are valid only with respect to the abstract domain model it is well understood that unforseen execution time variation both in the effect of action and in the time at which they occur can result in a valid plan failing to execute correctly other author have investigated the stochastic validity of plan with nondeterministic action outcome in this paper we focus on the uncertainty that arises a a result of inaccuracy in the measurement of time and other numeric quantity we describe a probing strategy that produce a stochastic estimate of the robustness of a temporal plan this strategy is based on gupta henzinger and jagadeesan s gupta henzinger jagadeesan notion of the fuzzy robustness of trace through timed hybrid automaton 
although there are some research effort toward resource allocation in multi agent system ma most of these work assume that each agent ha complete information about other agent this research investigates interaction among selfish rational and autonomous agent in resource allocation each with incomplete information about other entity and each seeking to maximize it expected utility this paper present a proportional resource allocation mechanism and give a game theoretical analysis of the optimal strategy and the analysis show the existence of equilibrium in the incomplete information setting by augmenting the resource allocation mechanism with a deal optimization mechanism trading agent can be programmed to optimize resource allocation result by updating belief and resubmitting bid experimental result showed that by having a deal optimization stage the resource allocation mechanism produced generally optimistic outcome close to market equilibrium 
a computer system continue to increase in complexity the need for ai based solution is becoming more urgent for example high end server that can be partitioned into logical subsystem and repartitioned on the fly are now becoming available this development raise the possibility of reconfiguring distributed system online to optimize for dynamically changing workload however it also introduces the need to decide when and how to reconfigure this paper present one approach to solving this online reconfiguration problem in particular we learn to identify from only low level system statistic which of a set of possible configuration will lead to better performance under the current unknown workload this approach requires no instrumentation of the system s middleware or operating system we introduce an agent that is able to learn this model and use it to switch configuration online a the workload varies our agent is fully implemented and tested on a publicly available multi machine multi process distributed system the online transaction processing benchmark tpc w we demonstrate that our adaptive configuration is able to outperform any single fixed configuration in the set over a variety of workload including gradual change and abrupt workload spike 
in modern urban setting automobile traffic and collision lead to endless frustration a well a significant loss of life property and productivity recent advance in artificial intelligence suggest that autonomous vehicle navigation may soon be a reality in previous work we have demonstrated that a reservation based approach can efficiently and safely govern interaction of multiple autonomous vehicle at intersection such an approach alleviates many traditional problem associated with intersection in term of both safety and efficiency however the system relies on all vehicle being equipped with the requisite technology a restriction that would make implementing such a system in the real world extremely difficult in this paper we extend this system to allow for incremental deployability the modified system is able to accommodate traditional human operated vehicle using existing infrastructure furthermore we show that a the number of autonomous vehicle on the road increase traffic delay decrease monotonically toward the level exhibited in our previous work finally we develop a method for switching between various human usable configuration while the system is running in order to facilitate an even smoother transition the work is fully implemented and tested in our custom simulator and we present detailed experimental result attesting to it effectiveness 
the planning domain definition language pddl ha become a common language to specify planning problem facilitating the formulation of benchmark and a direct comparison of planner over the year pddl ha been extended beyond strip and adl in various direction for example by adding time and concurrent action the current semantics of pddl is purely meta theoretic and quite complex which make an analysis difficult moreover relating the language to other action formalism is also nontrivial we propose an alternative semantics for an expressive fragment of pddl within the situation calculus this yield at least two advantage for one the new semantics is purely declarative making it amenable to an analysis in term of logical entailment for another it facilitates the comparison with and mapping to other formalism that are defined on top of the same logic such a the agent control language golog in particular we obtain the semantical foundation for embedding efficient pddl based planner into the more expressive yet computationally expensive golog thus combining the benefit of both other by product of our investigation are a simpler account of durative action in the situation calculus and a new notion of compulsory action 
reasoning with time need more than just a list of temporal expression timeml an emerging standard for temporal annotation a a language capturing property and relationship among timedenoting expression and event in text is a good starting point for bridging the gap between temporal analysis of document and reasoning with the information derived from them hard a timeml compliant analysis is the small size of the only currently available annotated corpus make it even harder we address this problem with a hybrid timeml annotator which us cascaded finite state grammar for temporal expression analysis shallow syntactic parsing and feature generation together with a machine learning component capable of effectively using large amount of unannotated data 
this paper address the problem of recovering the location of both mobile device and access point from radio signal a problem which we call colocalization by exploiting both labeled and unlabeled data from mobile device and access point we first propose a solution using latent semantic indexing to construct the relative location of the mobile device and access point when their absolute location are unknown we then propose a semi supervised learning algorithm based on manifold to obtain the absolute location of the device both solution are finally combined together in term of graph laplacian extensive experiment are conducted in wireless local area network wireless sensor network and radio frequency identification network the experimental result showthat we can achieve high accuracy with much le calibration effort a compared to several previous system 
the case based learning paradigm relies upon memorizing case in the form of successful problem solving experience such a e g a pattern along with it classification in pattern recognition or a problem along with a solution in case based reasoning when it come to solving a new problem each of these case serf a an individual piece of evidence that give an indication of the solution to that problem in this paper we elaborate on issue concerning the proper combination aggregation of such piece of evidence particularly we argue that case retrieved from a case library must not be considered a independent information source a implicitly done by most case based learning method focusing on the problem of prediction a a performance task we propose a new inference principle that combine potentially interacting piece of evidence by mean of the so called discrete choquet integral our method called cho k nn take interdependency between the stored case into account and can be seen a a generalization of weighted nearest neighbor estimation 
behavioral norm are key ingredient that allow agent coordination where societal law do not sufficiently constrain agent behavior whereas social law need to be enforced in a top down manner norm evolve in a bottom up manner and are typically more self enforcing while effective norm can significantly enhance performance of individual agent and agent society there ha been little work in multiagent system on the formation of social norm we propose a model that support the emergence of social norm via learning from interaction experience in our model individual agent repeatedly interact with other agent in the society over instance of a given scenario each interaction is framed a a stage game an agent learns it policy to play the game over repeated interaction with multiple agent we term this mode of learning social learning which is distinct from an agent learning from repeated interaction against the same player we are particularly interested in situation where multiple action combination yield the same optimal payoff the key research question is to find out if the entire population learns to converge to a consistent norm in addition to studying such emergence of social norm among homogeneous learner via social learning we study the effect of heterogeneous learner population size multiple social group etc 
this paper report on a project that explored reasoning with textual case in the context of legal reasoning the work is anchored in both case based reasoning cbr and ai and law it introduces the smile ibp framework that generates a case based analysis and prediction of the outcome of a legal case given a brief textual summary of the case fact the focal research question in this work wa to find a good text representation for text classification an evaluation showed that replacing case specific name by role and adding nlp lead to higher performance for assigning cbr index the nlp based representation produced the best result for reasoning with the automatically indexed case 
multimedia content present special challenge for the search engine and could benefit from semantic annotation of image unfortunately manual labeling is too tedious and time consuming for human whereas automatic image annotation is too difficult for the computer in this paper we explore the power of human computation by designing a multi player online game photoslap to achieve the task of annotating metadata for a collection of digital photo photoslap engages user in an interactive game that capitalizes on human ability in deciphering quickly whether the same person show up in two consecutive image presented by the computer the game mechanism support the objection and trap action to encourage truthful input from the player this research extends human computation research in two aspect game theoretic design principle and quantitative evaluation metric in particular photoslap can be shown to reach subgame perfect equilibrium with the target strategy when player are rational and without collusion experiment involving four focus group have been conducted and the preliminary result demonstrated the game to be fun and effective in annotating people metadata for photo collection 
uncertainty arises in preference aggregation in several way there may for example be uncertainty in the vote or the voting rule such uncertainty can introduce computational complexity in determining which candidate or candidate can or must win the election in this paper we survey recent work in this area and give some new result we argue for example that the set of possible winner can be computationally harder to compute than the necessary winner a a second example we show that even if the unknown vote are assumed to be single peaked it remains computationally hard to compute the possible and necessary winner or to manipulate the election 
stochastic filtering is the problem of estimating the state of a dynamic system after time pass and given partial observation it is fundamental to automatic tracking planning and control of real world stochastic system such a robot program and autonomous agent this paper present a novel sampling based filtering algorithm it expected error is smaller than sequential monte carlo sampling technique given a fixed number of sample a we prove and show empirically it doe so by sampling deterministic action sequence and then performing exact filtering on those sequence these result are promising for application in stochastic planning natural language processing and robot control 
many real world decision making task require u to choose among several expensive observation in a sensor network for example it is important to select the subset of sensor that is expected to provide the strongest reduction in uncertainty it ha been general practice to use heuristic guided procedure for selecting observation in this paper we present the first efficient optimal algorithm for selecting observation for a class of graphical model containing hidden markov model hmms we provide result for both selecting the optimal subset of observation and for obtaining an optimal conditional observation plan for both problem we present algorithm for the filtering case where only observation made in the past are taken into account and the smoothing case where all observation are utilized furthermore we prove a surprising result in most graphical model task if one design an efficient algorithm for chain graph such a hmms this procedure can be generalized to polytrees we prove that the value of information problem is nppp hard even for discrete polytrees it also follows from our result that even computing conditional entropy which are widely used to measure value of information is a p complete problem on polytrees finally we demonstrate the effectiveness of our approach on several real world datasets 
most nonlinear dimensionality reduction approach such a isomap heavily depend on the neighborhood structure of manifold they determine the neighborhood graph using euclidean distance so that they often fail to nicely deal with sparsely sampled or noise contaminated data this paper applies the graph algebra to optimize the neighborhood structure for isomap the improved isomap outperforms the classic isomap in visualization and time complexity a it provides good neighborhood structure that can speed up the subsequent dimensionality reducing process it also ha stronger topological stability and le sensitive to parameter this indicates that the more complicated or even time consuming approach can be applied to construct the better neighborhood structure whilst the whole time complexity will not raise the conducted experiment on benchmark data set have validated the proposed approach 
indoor environment can typically be divided into place with different functionality like kitchen office or seminar room we believe that such semantic information enables a mobile robot to more efficiently accomplish a variety of task such a human robot interaction path planning or localization this paper present a supervised learning approach to label different location using boosting we train a classifier using feature extracted from vision and laser range data furthermore we apply a hidden markov model to increase the robustness of the final classification our technique ha been implemented and tested on real robot a well a in simulation the experiment demonstrate that our approach can be utilized to robustly classify place into semantic category we also present an example of localization using semantic labeling 
we present the result from experiment with a new family of random formula for the satisfiability problem our proposal is a generalisation of the random k sat model that introduces non clausal formula and exhibit interesting feature such a experimentally observed sharp phase transition and the easy hard easy pattern the experimental result provide some insight on how the use of different clausal translation can affect the performance of satisfiability solving algorithm we also expect our model to provide diverse and challenging benchmark for developer of sat procedure for non clausal formula 
the design of interactive mobile robot is a multidisciplinary endeavor that profit from putting robot with people and studying their effect and impact to do so two main issue must be addressed giving robot capability in order to interact in meaningful and efficient way with people and the ability to move in human setting this paper briefly describes four robotic platform that are going to be demonstrated at the aaai robot competition 
a local spatial context is an area currently under consideration in a spatial reasoning process the boundary between this area and the surrounding space together with the spatial granularity of the representation separate what is spatially relevant from what is irrelevant at a given time the approach discussed in this article differs from other approach to spatial granularity a it focus not on a partitioning of the spatial domain but on the notion of grain size and the limited extent of a spatial context a primary factor of spatial granularity starting from a mereotopological characterization of these concept the notion of relevant and irrelevant extension in a context are defined the approach is qualitative in the sense that quantitative metric concept are not required the axiomatic characterization is thoroughly evaluated it is compared to other mereotopological characterization of spatial granularity soundness is proven with an example model and applicability for knowledge representation is illustrated with definition for common sense conceptualization of sameness and adjacency of location 
the main objective of this paper is to present and evaluate a method that help to calibrate the parameter of an evolutionary algorithm in a systematic and semi automated manner the method for relevance estimation and value calibration of ea parameter revac is empirically evaluated in two different way first we use abstract test case reflecting the typical property of ea parameter space here we observe that revac is able to approximate the exact hand coded relevance of parameter and it work robustly with measurement noise that is highly variable and not normally distributed second we use revac for calibrating gas for a number of common objective function here we obtain a common sense validation revac find mutation rate pm much more sensitive than crossover rate pc and it recommends intuitively sound value pm between and and pc 
admissible heuristic are critical for effective domain independent planning when optimal solution must be guaranteed two useful heuristic are the hm heuristic which generalize the reachability heuristic underlying the planning graph and pattern database heuristic these heuristic however have serious limitation reachability heuristic capture only the cost of critical path in a relaxed problem ignoring the cost of other relevant path while pdb heuristic additive or not cannot accommodate too many variable in pattern and method for automatically selecting pattern that produce good estimate are not known we introduce two refinement of these heuristic first the additive hm heuristic which yield an admissible sum of hm heuristic using a partitioning of the set of action second the constrained pdb heuristic which us constraint from the original problem to strengthen the lower bound obtained from abstraction the new heuristic depend on the way the action or problem variable are partitioned we advance method for automatically deriving additive hm and pdb heuristic from strip encoding evaluation show improvement over existing heuristic in several domain although not surprisingly no heuristic dominates all the others over all domain 
many learning task in adversarial domain tend to be highly dependent on the opponent predefined strategy optimized for play against a specific opponent are not likely to succeed when employed against another opponent learning a strategy for each new opponent from scratch though is inefficient a one is likely to encounter the same or similar opponent again we call this particular variant of inductive transfer a concept recall problem we present an extension to adaboost called expboost that is especially designed for such a sequential learning task it automatically balance between an ensemble of expert each trained on one known opponent and learning the concept of the new opponent we present and compare result of exp boost and other algorithm on both synthetic data and in a simulated robot soccer task expboost can rapidly adjust to new concept and achieve performance comparable to a classifier trained exclusively on a particular opponent with far more data 
one of the major task in swarm intelligence is to design decentralized but homogenoeus strategy to enable controlling the behaviour of swarm of agent it ha been shown in the literature that the point of convergence and motion of a swarm of autonomous mobile agent can be controlled by using cyclic pursuit law in cyclic pursuit there exists a predefined cyclic connection between agent and each agent pursues the next agent in the cycle in this paper we generalize this idea to a case where an agent pursues a point which is the weighted average of the position of the remaining agent this point correspond to a particular pursuit sequence using this concept of centroidal cyclic pursuit the behavior of the agent is analyzed such that by suitably selecting the agent gain the rendezvous point of the agent can be controlled directed linear motion of the agent can be achieved and the trajectory of the agent can be changed by switching between the pursuit sequence keeping some of the behavior of the agent invariant simulation experiment are given to support the analytical proof 
in this paper we introduce a novel algorithm for the induction of the markov network structure of a domain from the outcome of conditional independence test on data such algorithm work by successively restricting the set of possible structure until there is only a single structure consistent with the conditional independence test executed existing independence based algorithm have wellknown shortcoming such a rigidly ordering the sequence of test they perform resulting in potential inefficiency in the number of test required and committing fully to the test outcome resulting in lack of robustness in case of unreliable test we address both problem through a bayesian particle filtering approach which us a population of markov network structure to maintain the posterior probability distribution over them given the outcome of the test performed instead of a fixed ordering our approach greedily selects at each step the optimally informative from a pool of candidate test according to information gain in addition it maintains multiple candidate structure weighed by posterior probability which make it more robust to error in the test outcome the result is an approximate algorithm due to the use of particle filtering that is useful in domain where independence test are uncertain such a application where little data is available or expensive such a case of very large data set and or distributed data 
in this paper we discus how our work on evaluating semantic web knowledge base system kb contributes to address some broader ai problem first we show how our apprcach provides a benchmarking solution to the semantic web a new application area of ai second we discus how the approach is also beneficial in a more traditional ai context we focus on issue such a scalability performance tradeoff and the comparison of different class of system 
cognitive psychologist have long recognized that the acquisition of a motor skill involves a transition from attention demanding controlled processing to more fluent automatic processing neuroscientific study suggest that controlled and automatic processing rely on two largely distinct neural pathway the controlled pathway which includes the prefrontal cortex is seen a acquiring declarative representation of skill in comparison the automatic pathway is thought to develop procedural representation automaticity in motor skill learning involves a reduction in dependence on frontal system and an increased reliance on the automatic pathway in this paper we propose a biologically plausible computational model of motor skill automaticity this model offer a dual pathway neurocomputational account of the translation of declarative knowledge into procedural knowledge during motor learning in support of the model we review some previously reported human experimental result involving the learning of a sequential key pressing task and we demonstrate through simulation howthe model provides a parsimonious explanation for these result 
existing method for single document summarization usually make use of only the information contained in the specified document this paper proposes the technique of document expansion to provide more knowledge to help single document summarization a specified document is expanded to a small document set by adding a few neighbor document close to the document and then the graph ranking based algorithm is applied on the expanded document set for extracting sentence from the single document by making use of both the within document relationship between sentence of the specified document and the cross document relationship between sentence of all document in the document set the experimental result on the duc dataset demonstrate the effectiveness of the proposed approach based on document expansion the cross document relationship between sentence in the expanded document set are validated to be very important for single document summarization 
we consider example critiquing system that help people search for their most preferred item in a large catalog we compare existing approach in term of user or systemcentric implicit or explicit use of preference assumption used and their behavior in underconstrained and overconstrained situation we consider several type of explicit passive analysis to guide the user in their search that is information offered to the user about his current search but without any action taken by the system we suggest that such a user centric system together with the right analysis make the user feel more confident in their decision and reduces session time and cognitive effort we have implemented a prototype to evaluate the impact of explicit passive analysis in a query building and a preference based approach 
this paper investigates the effect of predefining semantics in modelling the evolution of compositional language versus allowing agent to develop these semantics in parallel with the development of language the study is done using a multi agent model of language evolution that is based on the talking head experiment the experiment show that when allowing a co evolution of semantics with language compositional language develop faster than when the semantics are predefined but compositionality appears more stable in the latter case the paper concludes that conclusion drawn from simulation with predefined meaning which most study use may need revision 
constraint optimization underlies many problem in ai we present a novel algorithm for finite domain constraint optimization that generalizes branch and bound search by reasoning about set of assignment rather than individual assignment because in many practical case set of assignment can be represented implicitly and compactly using symbolic technique such a decision diagram the set based algorithm can compute bound faster than explicitly searching over individual assignment while memory explosion can be avoided by limiting the size of the set varying the size of the set yield a family of algorithm that includes known search and inference algorithm a special case furthermore experiment on random problem indicate that the approach can lead to significant performance improvement 
many planning problem contain collection of symmetric object action and structure which render them difficult to solve efficiently it ha been shown that the detection and exploitation of symmetric structure in planning problem can dramatically reduce the size of the search space and the time taken to find a solution we present the idea of using an abstraction of the problem domain to reveal symmetric structure and guide the navigation of the search space we show that this is effective even in domain in which there is little accessible symmetric structure available for pruning proactive exploitation represents a flexible and powerful alternative to the symmetry breaking strategy exploited in earlier work in planning and csps the notion of almost symmetry is defined and result are presented showing that proactive exploitation of almost symmetry can improve the performance of a heuristic forward search planner 
we introduce the distributed penalty driven local search algorithm dispel for solving distributed constraint satisfaction problem dispel is a novel distributed iterative improvement algorithm which escape local optimum by the use of both temporary and incremental penalty and a tabu like no good store we justify the use of these feature and provide empirical result which demonstrate the competitiveness of the algorithm 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
transfer learning is the ability of an agent to apply knowledge learned in previous task to new problem or domain we approach this problem by focusing on model formulation i e how to move from the unruly broad set of concept used in everyday life to a concise formal vocabulary of abstraction that can be used effectively for problem solving this paper describes how the companion cognitive architecture us analogical model formulation to learn to solve ap physic problem our system start with some basic mathematical skill a broad common sense ontology and some qualitative mechanic but no equation our system us worked solution to learn how to use equation and modeling assumption to solve ap physic problem we show that this process of analogical model formulation can facilitate learning over a range of type of transfer in an experiment administered by the educational testing service 
we show that the problem ofmodel checking multi dimensional modal logic can be reduced to the problem of model checking arctl an extension of the temporal logic ctl with action label and operator to reason about action in particular we introduce a methodology for model checking a temporal epistemic logic by building upon an extension of the model checker nusmv that enables the verification of arctl we briefly present the implementation and report experimental result for the verification of a typical security protocol involving temporal epistemic property the protocol of the dining cryptographer 
among various feature extraction algorithm those based on genetic algorithm are promising owing to their potential parallelizability and possible application in large scale and high dimensional data classification however existing genetic algorithm based feature extraction algorithm are either limited in searching optimal projection basis vector or costly in both time and space complexity and thus not directly applicable to high dimensional data in this paper a direct evolutionary feature extraction algorithm is proposed for classifying high dimensional data it construct projection basis vector using the linear combination of the basis of the search space and the technique of orthogonal complement it also constrains the search space when seeking for the optimal projection basis vector it evaluates individual according to the classification performance on a subset of the training sample and the generalization ability df the projection basis vector represented by the individual we compared the proposed algorithm with some representative feature extraction algorithm in face recognition including the evolutionary pursuit algorithm eigenfaces and fisherfaces the result on the widely used yale and orl face database show that the proposed algorithm ha an excellent performance in classification while reducing the space complexity by an order of magnitude 
vickrey clarke grove vcg mechanism are a framework for finding a solution to a distributed optimization problem in system of self interested agent vcg mechanism have received wide attention in the ai community because they are efficient and strategy proof a special case of the grove family of mechanism vcg mechanism are the only direct revelation mechanism that are allocatively efficient and strategy proof unfortunately they are only weakly budget balanced we consider self interested agent in a network flow domain and show that in this domain it is possible to design a mechanism that is both allocatively efficient and almost completely budget balanced this is done by choosing a mechanism that is not strategy proof but rather strategy resistant instead of using the vcg mechanism we propose a mechanism in which finding a beneficial manipulation is an np complete problem and the payment from the agent to the mechanism may be minimized a much a desired 
and or search space have recently been introduced a a unifying paradigm for advanced algorithmic scheme for graphical model the main virtue of this representation is it sensitivity to the structure of the model which can translate into exponential time saving for search algorithm and or branch and bound aobb is a new algorithm that explores the and or search tree for solving optimization task in graphical model in this paper we extend the algorithm to explore an and or search graph by equipping it with a context based adaptive caching scheme similar to good and no good recording the efficiency of the new graph search algorithm is demonstrated empirically on various benchmark including the very challenging one that arise in genetic linkage analysis 
automated domain factoring and planning method that utilize them have long been of interest to planning researcher recent work in this area yielded new theoretical insight and algorithm but left many question open how to decompose a domain into factor how to work with these factor and whether and when decomposition based method are useful this paper provides theoretical analysis that answer many of these question it proposes a novel approach to factored planning prof it theoretical superiority over previous method provides insight into how to factor domain and us it novel complexity result to analyze when factored planning is likely to perform well and when not it also establishes the key role played by the domain s causal graph in the complexity analysis of planning algorithm 
this paper develops a statistical inference approach bayesian tensor inference for style transformation between photo image and sketch image of human face motivated by the rationale that image appearance is determined by two cooperative factor image content and image style we first model the interaction between these factor through learning a patch based tensor model second by introducing a common variation space we capture the inherent connection between photo patch space and sketch patch space thus building bidirectional mapping inferring between the two space subsequently we formulate a bayesian approach accounting for the statistical inference from sketch to their corresponding photo in term of the learned tensor model comparative experiment are conducted to contrast the proposed method with state of the art algorithm for facial sketch synthesis in a novel face hallucination scenario sketch based facial photo hallucination the encouraging result obtained convincingly validate the effectiveness of our method 
we describe a system for generating extractive summary of text in the legal domain focusing on the relevance classifier which determines which sentence are abstract worthy we experiment with na ve bayes and maximum entropy estimation toolkits and explore method for selecting abstract worthy sentence in rank order evaluation using standard accuracy measure and using correlation confirm the utility of our approach but suggest different optimal configuration 
like many other application area task based domain that employ digital imagery are faced with the problem of information overload modeling the relationship between image and the task being performed is an important step in addressing this problem we have developed an interactive approach for the capture and reuse of image context information that leverage a measure of a user s intention with regard to task that they address we analyze aspect of human computer interaction information that enables u to infer why image content are important in a particular context and how specific image have been used to address particular domain goal 
we present the problem of learning to communicate in decentralized and stochastic environment analyzing it formally in a decision theoretic context and illustrating the concept experimentally our approach allows agent to converge upon coordinated communication and action over time 
classical search algorithm such a a or ida are useful for computing optimal solution in a single pas which can then be executed but in many domain agent either do not have the time to compute complete plan before acting or should not spend the time to do so due to the dynamic nature of the environment extension to a such a lrta address this problem by gradually learning an exact heuristic function but the learning process is quite slow in this paper we introduce partial refinement a pra which can fully interleave planning and acting through path abstraction and refinement we demonstrate the etfectiveness of pra in the domain of real time strategy rts game in map taken from popular rts game we show that pra is not only able to cleanly interleave planning and execution but it is also able to do so with only minimal loss of optimality 
in this paper we present a novel algorithm to learn a score distribution over the node of a labeled graph directed or undirected markov chain theory is used to define the model of a random walker that converges to a score distribution which depends both on the graph connectivity and on the node label a supervised learning task is defined on the given graph by assigning a target score for some node and a training algorithm based on error backpropagation through the graph is devised to learn the model parameter the trained model can assign score to the graph node generalizing the criterion provided by the supervisor in the example the proposed algorithm ha been applied to learn a ranking function for web page the experimental result show the effectiveness of the proposed technique in reorganizing the rank accordingly to the example provided in the training set 
in this paper we propose a family of operator for merging stratified knowledge base under integrity constraint the operator are defined in a model theoretic way our merging operator can be used to merge stratified knowledge base where no numerical information is available furthermore the original knowledge base to be merged can be individually inconsistent both logical property and computational complexity issue of the operator are studied 
classic direct mechanism suffer from the drawback of requiring full type or utility function revelation from participating agent in complex setting with multi attribute utility assessing utility function can be very difficult a problem addressed by recent work on preference elicitation in this work we propose a framework for incremental partial revelation mechanism and study the use of minimax regret a an optimization criterion for allocation determination with type uncertainty we examine the incentive property of incremental mechanism when minimax regret is used to determine allocation with no additional elicitation of payment information and when additional payment information is obtained we argue that elicitation effort can be focused simultaneously on reducing allocation and payment uncertainty 
this paper describes a novel negotiation protocol for cellular network which intelligently improves the performance of the network our proposed reactive mechanism enables the dynamic adaptation of the base station to continuous change in service demand thereby improving the overall network performance this mechanism is important when a frequent global optimization is infeasible or substantially costly the proposed local negotiation mechanism is incorporated into a simulated network based on cutting edge industry technology experimental result suggest a rapid adjustment to change in bandwidth demand and over all improvement in the number of served user over time although we tested our algorithm based on the service level which is measured a the number of covered handset our algorithm support negotiation for any set of parameter aiming to optimize network s performance according to any measure of performance specified by the service provider 
dimensionality reduction is a much studied task in machine learning in which high dimensional data is mapped possibly via a non linear transformation onto a low dimensional manifold the resulting embeddings however may fail to capture feature of interest one solution is to learn a distance metric which prefers embeddings that capture the salient feature we propose a novel approach to learning a metric from side information to guide the embedding process our approach admits the use of two kind of side information the first kind is class equivalence information where some limited number of pairwise same different class statement are known the second form of side information is a limited set of distance between pair of point in the target metric space we demonstrate the effectiveness of the method by producing embeddings that capture feature of interest 
a part of a larger machine ethic project we are developing an ethical advisor that provides guidance to health care worker faced with ethical dilemma medethex is an implementation of beauchamp s and childress principle of biomedical ethic that harness machine learning technique to abstract decision principle from case in a particular type of dilemma with conflicting prima facie duty and us these principle to determine the correct course of action in similar and new case we believe that accomplishing this will be a useful first step towards creating machine that can interact with those in need of health care in a way that is sensitive to ethical issue that may arise 
many useful planning task are handled by plan execution tool such a pr that expand procedure definition and keep track of several interacting goal and task learning by instruction is a promising approach to help user modify the definition of the procedure however the impact of the set of possible instruction on the performance of such system is not well understood we develop a framework in which instruction template may be characterized in term of syntactic transforms on task definition and use it to explore the property of coverage ambiguity and efficiency in the set of instruction that are understood by an implemented task learning system we determine what kind of ambiguity is affected by the instruction set and show how context dependent interpretation can increase efficiency and coverage without increasing ambiguity 
multi context system mc represent contextual information flow we show that the semantics of an mc is completely determined by the information that is obtained when simulating the mc in such a way that a minimal amount of information is deduced at each step of the simulation in mc the acquisition of new information is based on the presence of other information only we give a generalized account to model situation in which information can be obtained a a result of the absence of other information a well 
constraint satisfaction problem csps are ubiquitous in artificial intelligence the backtrack algorithm that maintain some local consistency during search have become the de facto standard to solve csps maintaining higher level of consistency generally reduces the search effort however due to ineffective constraint propagation it often penalises the search algorithm in term of time if we can reduce ineffective constraint propagation then the effectiveness of a search algorithm can be enhanced significantly in order to do so we use a probabilistic approach to resolve when to propagate and when not to the idea is to perform only the useful consistency checking by not seeking a support when there is a high probability that a support exists the idea of probabilistic support inference is general and can be applied to any kind of local consistency algorithm however we shall study it impact with respect to arc consistency and singleton arc consistency sac experimental result demonstrate that enforcing probabilistic sac almost always enforces sac but it requires significantly le time than sac likewise maintaining probabilistic arc consistency and maintaining probabilistic sac require significantly le time than maintaining arc consistency and maintaining sac 
we consider the problem of coordinating the behavior of multiple self interested agent it involves constraint optimization problem that often can only be solved by local search algorithm using local search pose problem of incentivecompatibility and individual rationality we thus define a weaker notion of bounded rational incentive compatibility where manipulation is made impossible with high probability through computational complexity we observe that in real life manipulation of complex situation is often impossible because the effect of the manipulation cannot be predicted with sufficient accuracy we show how randomization scheme in local search can make predicting it outcome hard and thus form a bounded rational incentive compatible coordination algorithm 
recent year have seen a revived interest in semantic parsing by applying statistical and machine learning method to semantically annotated corpus such a the framenet and the proposition bank so far much of the research ha been focused on english due to the lack of semantically annotated resource in other language in this paper we report first result on semantic role labeling using a pre release version of the chinese proposition bank since the chinese proposition bank is superimposed on top of the chinese tree bank i e the semantic role label are assigned to constituent in a treebank parse tree we start by reporting result on experiment using the handcrafted par in the treebank this will give u a measure of the extent to which the semantic role label can be bootstrapped from the syntactic annotation in the treebank we will then report experiment using a fully automatic chinese parser that integrates word segmentation po tagging and parsing this will gauge how successful semantic role labeling can be done for chinese in realistic situation we show that our result using hand crafted par are slightly higher than the result reported for the state of the art semantic role labeling system for english using the penn english proposition bank data even though the chinese proposition bank is smaller in size when an automatic parser is used however the accuracy of our system is much lower than the english state of the art this reveals an interesting cross linguistic difference between the two language which we attempt to explain we also describe a method to induce verb class from the proposition bank frame file that can be used to improve semantic role labeling 
we show how one class compression neural network and one class svm can be applied to fmri data to learn the classification of brain activity associated with a specific motor activity for comparison purpose we use two labeled data and see what degree of classification ability is lost compared with the usual two class svm 
the notion of a conservative extension play a central role in ontology design and integration it can be used to formalize ontology refinement safe merging of two ontology and independent module inside an ontology regarding reasoning support the most basic task is to decide whether one ontology is a conservative extension of another it ha recently been proved that this problem is decidable and exptime complete if ontology are formulated in the basic description logic alc we consider more expressive description logic and begin to map out the boundary between logic for which conservativity is decidable and those for which it is not we prove that conservative extension are exptime complete in alcqi but undecidable in alcqio we also show that if conservative extension are defined model theoretically rather than in term of the consequence relation they are undecidable already in alc 
in this paper we study search strategy of agent that represent buyer agent coalition in electronic marketplace the representative agent operate in environment where numerous potential complex opportunity can be found each opportunity is associated with several different term and condition thus differing from other opportunity by it value for the coalition given a search cost the goal of the representative agent is to find the best set of opportunity which fulfills the coalition s demand with the maximum overall utility to be divided among the coalition member given the option of side payment this strategy will always be preferred by all coalition member thus no conflict of interest regardless of the coalition s payoff division protocol we analyze the incentive to form such coalition and extract the optimal search strategy for their representative agent with a distinction between operating in b c and c c market based on our finding we suggest efficient algorithm to be used by the representative agent for calculating a strategy that maximizes their expected utility a computational based example is given illustrating the achieved performance a a function of the coalition s member heterogeneity level 
tree decomposition can solve weighted csp but with a high spatial complexity to improve it practical usage we present function filtering a strategy to decrease memory consumption function filtering dtects and remove some tuples that appear to be consistent but that will become inconsistent when extended to other variable we show empirically the benefit of our approach 
this paper present first step towards a simple robust computational model of automatic melody identification based on result from music psychology that indicate a relationship between melodic complexity and a listener s attention we postulate a relationship between musical complexity and the probability of a musical line to be perceived a the melody we introduce a simple measure of melodic complexity present an algorithm for predicting the most likely melody note at any point in a piece and show experimentally that this simple approach work surprisingly well in rather complex music 
this paper address the problem of identifying causal effect from nonexperimental data in a causal bayesian network i e a directed acyclic graph that represents causal relationship the identifiability question asks whether it is possible to compute the probability of some set of effect variable given intervention on another set of intervention variable in the presence of non observable i e hidden or latent variable it is well known that the answer to the question depends on the structure of the causal bayesian network the set of observable variable the set of effect variable and the set of intervention variable our work is based on the work of tian pearl huang and valtorta tian pearl a b huang valtorta a and extends it we show that the identify algorithm that tian and pearl define and prove sound for semi markovian model can be transfered to general causal graph and is not only sound but also complete this result effectively solves the identifiability question for causal bayesian network that pearl posed in pearl by providing a sound and complete algorithm for identifiability 
in this paper we present a novel method that fuse the ensemble meta technique of stacking and dynamic integration di for regression problem without adding any major computational overhead the intention of the technique is to benefit from the varying performance of stacking and di for different data set in order to provide a more robust technique we detail an empirical analysis of the technique referred to a weighted meta combiner wmetacomb and compare it performance to stacking and the di technique of dynamic weighting with selection the empirical analysis consisted of four set of experiment where each experiment recorded the cross fold evaluation of each technique for a large number of diverse data set where each base model is created using random feature selection and the same base learning algorithm each experiment differed in term of the latter base learning algorithm used we demonstrate that for each evaluation wmeta comb wa able to outperform di and stacking for each experiment and a such fuse the two underlying mechanism successfully 
due to the inherent difficulty of processing noisy text the potential of the web a a decentralized repository of human knowledge remains largely untapped during web search the access to billion of binary relation among named entity would enable new search paradigm and alternative method for presenting the search result a first concrete step towards building large searchable repository of factual knowledge is to derive such knowledge automatically at large scale from textual document generalized contextual extraction pattern allow for fast iterative progression towards extracting one million fact of a given type e g person bornin year from million web document of arbitrary quality the extraction start from a few a seed fact requires no additional input knowledge or annotated text and emphasizes scale and coverage by avoiding the use of syntactic parser named entity recognizers gazetteer and similar text processing tool and resource 
although even propositional strip planning is a hard problem in general many instance of the problem including many of those commonly used a benchmark are easy in spite of this they are often hard to solve for domain independent planner because the encoding of the problem into a general problem specification formalism such a strip hide structure that need to be exploited to solve problem easily we investigate the use of automatic problem transformation to reduce this accidental problem complexity the main tool is abstraction we identify a new weaker condition under which abstraction is safe in the sense that any solution to the abstracted problem can be refined to a concrete solution in polynomial time for most case and also show how different kind of problem reformulations can be applied to create greater opportunity for such safe abstraction 
the scalability of recent planning algorithm allows developer to automate planning task which so far have been reserved to human however in real world application synthesizing a plan is just the beginning of a complex life cycle management process plan must be organized in large collection where they can be grouped along different purpose and are amenable to the search inspection evaluation and modification by human expert or automated reasoning system eventually plan will outlast their utility and be replaced we present our solution to plan life cycle management for an autonomic computing application we focus in particular on the automatic synthesis of plan metadata for plan containing conditional and parallel action well structured loop and non deterministic choice the plan are of unknown origin i e their underlying action model which could provide u with preand postconditions is not available new analysis technique are presented that uniformly generate metadata for plan thus allowing a system to embed plan into context and organize them in meaningfully structured plan repository 
helping behavior in effective team is enabled by some overlapping shared mental model that are developed and maintained by member of the team in this paper we take the perspective that multiparty proactive communication is critical for establishing and maintaining such a shared mental model among teammate which is the basis for agent to offer proactive help and to achieve coherent teamwork we first provide formal semantics for multiparty proactive performatives within a team setting we then examine how such performatives result in update to mental model of teammate and how such update can trigger helpful behavior from other teammate 
interactive task such a online configuration can be modeled a constraint satisfaction problem these can be solved interactively by a user assigning value to variable explanation for failure in constraint programming tend to focus on conflict however what is often desirable is an explanation that is corrective in the sense that it provides the basis for moving forward in the problem solving process this paper defines this notion of corrective explanation and demonstrates that a greedy search approach performs very well on a large real world configuration problem 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
this paper develops new algorithm for coalition formation within multi sensor network tasked with performing wide area surveillance specifically we cast this application a an instance of coalition formation with overlapping coalition we show that within this application area subadditive coalition valuation are typical and we thus use this structural property of the problem to derive two novel algorithm an approximate greedy one that operates in polynomial time and ha a calculated bound to the optimum and an optimal branch and bound one to find the optimal coalition structure in this instance we empirically evaluate the performance of these algorithm within a generic model of a multi sensor network performing wide area surveillance these result show that the polynomial algorithm typically generated solution much closer to the optimal than the theoretical bound and prove the effectiveness of our pruning procedure 
this paper is about using multiple type of information for classification of networked data in a semi supervised setting given a fully described network node and edge with known label for some of the node predict the label of the remaining node one method recently developed for doing such inference is a guilt by association model this method ha been independently developed in two different setting relational learning and semi supervised learning in relational learning the setting assumes that the networked data ha explicit link such a hyperlink between webpage or citation between research paper the semi supervised setting assumes a corpus of non relational data and creates link based on similarity measure between the instance both use only the known label in the network to predict the remaining label but use very different information source the thesis of this paper is that if we combine these two type of link the resulting network will carry more information than either type of link by itself we test this thesis on six benchmark data set using a within network learning algorithm where we show that we gain significant improvement in predictive performance by combining the link we describe a principled way of combining multiple type of edge with different edge weight and semantics using an objective graph measure called node based assortativity we investigate the use of this measure to combine text mined link with explicit link and show that using our approach significantly improves performance of our classifier over naively combining these two type of link 
we examine the problem of transfer in reinforcement learning and present a method to utilize knowledge acquired in one markov decision process mdp to bootstrap learning in a more complex but related mdp we build on work in model minimization in reinforcement learning to define relationship between state action pair of the two mdps our main contribution in this work is to provide a way to compactly represent such mapping using relationship between state variable in the two domain we use these function to transfer a learned policy in the first domain into an option in the new domain and apply intra option learning method to bootstrap learning in the new domain we first evaluate our approach in the well known blocksworld domain we then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the robosoccer keepaway domain 
in clique tree clustering inference consists of propagation in a clique tree compiled from a bayesian network in this paper we develop an analytical approach to characterizing clique tree growth a a function of increasing bayesian network connectedness specifically i the expected number of moral edge in their moral graph or ii the ratio of the number of non root node to the number of root node in experiment we systematically increase the connectivity of bipartite bayesian network and find that clique tree size growth is well approximated by gompertz growth curve this research improves the understanding of the scaling behavior of clique tree clustering provides a foundation for benchmarking and developing improved bn inference algorithm and present an aid for analytical trade off study of tree clustering using growth curve 
advance in bioengineering have led to increasingly sophisticated prosthetic device for amputee and paralyzed individual control of such device necessitates real time classification of biosignals e g electromyographic emg signal recorded from intact muscle in this paper we show that a degree of freedom robotic arm can be controlled in real time using non invasive surface emg signal recorded from the forearm the innovative feature of our system include a physiologically informed selection of forearm muscle for recording emg signal intelligent choice of hand gesture for easy classification and fast simple feature extraction from emg signal our selection of gesture is meant to intuitively map to appropriate degree of freedom in the robotic arm these design decision allow u to build fast accurate classifier online and control a dof robotic arm in real time in a study involving subject we achieved accuracy of on an class classification problem using linear svms these classifier can be learned on line in under minute including data collection and training our study also analyzes the issue and tradeoff involved in designing scheme for robotic control using emg finally we present detail of online experiment where subject successfully solved task of varying complexity using emg to control the robotic arm 
human object recognition in a physical d environment is still far superior to that of any robotic vision system we believe that one reason out of many for this one that ha not heretofore been significantly exploited in the artificial vision literature is that human use a fovea to fixate on or near an object thus obtaining a very high resolution image of the object and rendering it easy to recognize in this paper we present a novel method for identifying and tracking object in multiresolution digital video of partially cluttered environment our method is motivated by biological vision system and us a learned attentive interest map on a low resolution data stream to direct a high resolution fovea object that are recognized in the fovea can then be tracked using peripheral vision because object recognition is run only on a small foveal image our system achieves performance in real time object recognition and tracking that is well beyond simpler system 
for two integral histogram r r rd and c c cd of equal sum n the monge kantorovich distance dmk r c between r and c parameterized by a d d distance matrix t is the minimum of all cost f t taken over matrix f of the transportation polytope u r c recent result suggest that this distance is not negative definite and hence through schoenberg s well known result exp tdmk may not be a positive definite kernel for all t rather than using directly dmk to define a similarity between r and c we propose in this paper to investigate kernel on r and c based on the whole transportation polytope u r c we prove that when r and c have binary count which is equivalent to stating that r and c represent cloud of point of equal size the permanent of an adequate gram matrix induced by the distance matrix t is a positive definite kernel under favorable condition on t we also show that the volume of the polytope u r c that is the number of integral transportation plan is a positive definite quantity in r and c through the robinson schensted knuth correspondence between transportation matrix and young tableau we follow by proposing a family of positive definite kernel related to the generating function of the polytope through recent result obtained separately by a barvinok on the one hand and c berg and a j duran on the other hand we finally present preliminary result led on a subset of the mnist database to compare cloud of point through the permanent kernel 
this paper present both a semantic and a computational model for multi agent belief revision we show that these two model are equivalent but serve different purpose the semantic model display the intuition and construction of the belief revision operation in multi agent environment especially in case of just two agent the logical property of this model provide strong justification for it the computational model enables u to reassess the operation from a computational perspective a complexity analysis reveals that belief revision between two agent is computationally no more demanding than single agent belief revision 
spartacus our aaai mobile robot challenge entry integrated planning and scheduling sound source localization tracking and separation message reading speech recognition and generation and autonomous navigation capability onboard a custom made interactive robot integration of such a high number of capability revealed interesting new issue such a coordinating audio visual graphical capability monitoring the impact of the capability in usage by the robot and inferring the robot s intention and goal our entry will be used to address these issue to add new capability to the robot and to improve our software and computational architecture with the objective of increasing evaluating and improving our understanding of human robot interaction and integration with an autonomous mobile platform 
lexical paraphrasing aim at acquiring word level paraphrase it is critical for many natural language processing nlp application such a question answering qa information extraction ie and machine translation mt since the meaning and usage of a word can vary in distinct context different paraphrase should be acquired according to the context however most of the existing research focus on constructing paraphrase corpus in which little contextual constraint for paraphrase application are imposed this paper present a method that automatically acquires context specific lexical paraphrase in this method the obtained paraphrase of a word depend on the specific sentence the word occurs in two stage are included i e candidate paraphrase extraction and paraphrase validation both of which are mainly based on web mining evaluation are conducted on a news title corpus and the presented method is compared with a paraphrasing method that exploit a chinese thesaurus of synonym tongyi cilin extended ciline for short result show that the f measure of our method is significantly higher than that using ciline in addition over of the correct paraphrase derived by our method cannot be found in ciline which suggests that our method is effective in acquiring out of thesaurus paraphrase 
we show that a simple procedure based on maximizing the number of informative content word can produce some of the best reported result for multi document summarization we first assign a score to each term in the document cluster using only frequency and position information and then we find the set of sentence in the document cluster that maximizes the sum of these score subject to length constraint our overall result are the best reported on the duc summarization task for the rouge score and are the best but not statistically significantly different from the best system in mse our system is also substantially simpler than the previous best system 
in distributed combinatorial optimization problem dynamic programming algorithm like dpop petcu and faltings require only a linear number of message thus generating low communication overhead however dpop s memory requirement are exponential in the induced width of the constraint graph which may be prohibitive for problem with large width we present mb dpop a new hybrid algorithm that can operate with bounded memory in area of low width mb dpop operates like standard dpop linear number of message area of high width are explored with bounded propagation using the idea of cycle cut dechter we introduce novel dfs based mechanism for determining the cycle cutset and for grouping cycle cut node into cluster we use caching darwiche between cluster to reduce the complexity to exponential in the largest number of cycle cut in a single cluster we compare mb dpop with adopt modi et al the current state of the art in distributed search with bounded memory mb dpop consistently outperforms adopt on problem domain with respect to metric providing speedup of up to order of magnitude 
this paper present photoslap an intelligent system for semantic annotation of photo the system contains a semi automatic face detector a bulk annotation tool and a multi player online game photoslap by exploring the design principle of gameplay and applying game theoretic analysis photoslap is designed a a fun and productive game which adapts itself to different player to produce the desired output experiment involving four focus group showed the game to be fun and effective in annotating people metadata for personal photo collection 
local search algorithm for satisfiability testing are still the best method for a large number of problem despite tremendous progress observed on complete search algorithm over the last few year however their intrinsic limit doe not allow them to address unsat problem ten year ago this question challenged the community without any answer wa it possible to use local search algorithm for unsat formula we propose here a first approach addressing this issue that can beat the best resolution based completemethods we define the landscape of the search by approximating the number of filtered clause by resolution proof furthermore we add high level reasoning mechanism based on extended resolution and unit propagation look ahead to make this new and challenging approach possible our new algorithm also tends to be the first step on two other challenging problem obtaining short proof for unsat problem and build a real local search algorithm for qbf 
quantified constraint and quantified boolean formula are typically much more difficult to reason with than classical constraint because quantifier alternation make the simple classical notion of solution inappropriate a a consequence even such essential csp property a consistency or substitutability are not completely understood in the quantified case in this paper we show that most of the property which are used by solver for csp can be generalized to quantified csp we propose a systematic study of the relation which hold between these property a well a complexity result regarding the decision of these property finally and since these problem are typically intractable we generalise the approach used in csp and propose weakening of these notion based on locality which allow for a tractable albeit incomplete detecting of these property 
we introduce point based dynamic programming dp for decentralized partially observable markov decision process dec pomdps a new discrete dp algorithm for planning strategy for cooperative multi agent system our approach make a connection between optimal dp algorithm for partially observable stochastic game and point based approximation for single agent pomdps we show for the first time how relevant multi agent belief state can be computed building on this insight we then show how the linear programming part in current multi agent dp algorithm can be avoided and how multi agent dp can thus be applied to solve larger problem we derive both an optimal and an approximated version of our algorithm and we show it efficiency on test example from the literature 
we investigate a solution to the problem of multisensor perception and tracking by formulating it in the framework of bayesian model selection human robustly associate multi sensory data a appropriate but previous theoretical work ha focused largely on purely integrative case leaving segregation unaccounted for and unexploited by machine perception system we illustrate a unifying bayesian solution to multi sensor perception and tracking which account for both integration and segregation by explicit probabilistic reasoning about data association in a temporal context unsupervised learning of such a model with em is illustrated for a real world audio visual application 
we address the problem of efficient feature value acquisition for classification in domain in which there are varying cost associated with both feature acquisition and misclassification the objective is to minimize the sum of the information acquisition cost and misclassification cost any decision theoretic strategy tackling this problem need to compute value of information for set of feature having calculated this information different acquisition strategy are possible acquiring one feature at time acquiring feature in set etc however because the value of information calculation for arbitrary subset of feature is computationally intractable most traditional approach have been greedy computing value of feature one at a time we make the problem of value of information calculation tractable in practice by introducing a novel data structure called the value of information lattice voila voila exploit dependency between missing feature and make sharing of information value computation between different feature subset possible to the best of our knowledge performance difference between greedy acquisition acquiring feature in set and a mixed strategy have not been investigated empirically in the past due to inherit intractability of the problem with the help of voila we are able to evaluate these strategy on five real world datasets under various cost assumption we show that voila reduces computation time dramatically we also show that the mixed strategy outperforms both greedy acquisition and acquisition in set 
the visionary goal of an easy to use service robot implies intuitive style of interaction between human and robot such natural interaction can only be achieved if mean are found to bridge the gap between the form of object perception and spatial knowledge maintained by such robot and the form of language used by human to communicate such knowledge part of bridging this gap consists of allowing user and robot to establish joint reference on object in the environment without forcing the user to use unnatural mean for object reference we present an approach to establishing joint object reference which make use of natural object classification and a computational model of basic intrinsic and relative reference system our object recognition approach assigns natural category e g desk chair table to new object based on their functional design with basic object within the environment classified we can then make use of a computational reference model to process natural projective relation e g the briefcase to the left of the chair allowing user to refer to object which cannot be classified reliably by the recognition system alone 
we investigate the impact of time on the predictability of sentiment classification research for model created from web log we show that sentiment classifier are time dependent and through a series of methodical experiment quantify the size of the dependence in particular we measure the accuracy of different time specific sentiment classifier on different testing timeframes we use the naive bayes induction technique and the holdout validation technique using equal sized but separate training and testing data set we conducted over experiment and organize our result by the size of the interval in month between the training and testing timeframes our finding show a significant decrease in accuracy a this interval grows using a paired t test we show classifier trained on future data and tested on past data significantly outperform classifier trained on past data and tested on future data these finding are for a topic specific corpus created from political web log post originating from different web log we then define concept that classify month a examplar infrequent thread frequent thread or outlier this classification reveals knowledge on the topic s evolution and the utility of the month s data for the timeframe 
the pddl specification include soft goal and trajectory constraint for distinguishing highquality plan among the many feasible plan in a solution space to reduce the complexity of solving a large pddl planning problem constraint partitioning can be used to decompose it constraint into subproblems of much lower complexity however constraint locality due to soft goal and trajectory constraint cannot be effectively exploited by existing subgoal partitioning technique developed for solving pddl problem in this paper we present an improved partition andresolve strategy for supporting the new feature in pddl we evaluate technique for resolving violated global constraint optimizing goal preference and achieving subgoals in a multivalued representation empirical result on the th international planning competition ipc benchmark show that our approach is effective and significantly outperforms other competing planner 
fast and accurate tracking of moving object in video stream is a critical process in computer vision this problem can be formulated a exploration problem and thus can be expressed a a search into a state space based representation approach however these search problem are hard to solve because they involve search through a high dimensional space in this paper we describe an a heuristic search for computing efficient search through a space of transformation corresponding to the d motion of the object where most promising search alternative are computed by mean of integrating target dynamic into the search process and idea from information theory are used to guide the search the paper includes evaluation with video stream that illustrate the efficiency and suitability for real time vision task on general purpose hardware moreover the computational cost to carry out the tracking task is smaller than real time requirement m 
max csps are constraint optimization problem that are commonly solved using a branch and bound algorithm the b b algorithm wa enhanced by consistency maintenance procedure wallace and freuder larrosa and meseguer larrosa et al larrosa and schiex all these algorithm traverse the search space in a chronological order and gain their efficiency from the quality of the consistency maintenance procedure the present study introduces conflict directed backjumping cbj for branch and bound algorithm the proposed algorithm maintains conflict set which include only assignment whose replacement can lead to a better solution the algorithm backtracks according to these set cbj can be added to all class of the branch and bound algorithm in particular to version of branch bound that use advanced maintenance procedure of local consistency level nc ac and fdac larrosa and schiex the experimental evaluation of b b cbj on random max csps show that the performance of all algorithm is improved both in the number of assignment and in the time for completion 
this paper investigates the automatic evaluation of text coherence for machine generated text we introduce a fully automatic linguistically rich model of local coherence that correlate with human judgment our modeling approach relies on shallow text property and is relatively inexpensive we present experimental result that ass the predictive power of various discourse representation proposed in the linguistic literature our result demonstrate that certain model capture complementary aspect of coherence and thus can be combined to improve performance 
syntactic configuration used in collocation extraction are highly divergent from one system to another this questioning the validity of result and making comparative evaluation difficult we describe a corpus driven approach for inferring an exhaustive set of configuration from actual data by finding with a parser all the productive syntactic association then by appealing to human expertise for relevance judgement 
human motion is a much characterized by it low frequency shape a by it high frequency temporal discontinuity such a when a joint reach it physical limit or when a foot touch the floor wavelet are particularly efficient at capturing both high and low frequency information we introduce a method of classifying human motion using wavelet coefficient to build a representation of human motion signal the representation is computed by finding the histogram of the waveletcoefficients previouslyscaledaccording tofrequency we use support vector machine svms to classify those histogram and demonstrate the accuracy of the method on human motion gathered from both a motion capture system and accelerometer 
in this paper we present efficient algorithm to discover spatial association among feature extracted from scientific datasets in contrast to previous work in this area feature are modeled a geometric object rather than point we define multiple distance metric that take into account object extent we have developed algorithm to discover two type of spatial association pattern in scientific data we present experimental result to demonstrate the efficacy of our approach on real datasets drawn from the bioinformatic domain we also highlight the importance of the discovered pattern by integrating the underlying domain knowledge 
this paper describes comet a collaborative intelligent tutoring system for medical problembased learning comet us bayesian network to model individual student knowledge and activity a well a that of the group generic domainindependent tutoring algorithm use the model to generate tutoring hint we present an overview of the system and then the result of two evaluation study the validity of the modeling approach is evaluated in the area of head injury stroke and heart attack receiver operating characteristic roc curve analysis indicates that the model are accurate in predicting individual student action comparison of learning outcome show that student clinical reasoning gain from our system are significantly higher than those obtained from human tutored session mann whitney p 
a word sense disambiguation wsd system trained on one domain and applied to a different domain will show a decrease in performance one major reason is the different sense distribution between different domain this paper present novel application of two distribution estimation algorithm to provide estimate of the sense distribution of the new domain data set even though our training example are automatically gathered from parallel corpus the sense distribution estimated are good enough to achieve a relative improvement of when incorporated into our wsd system 
we examine a setting in which a buyer wish to purchase probabilistic information from some agent the seller must invest effort in order to gain access to the information and must therefore be compensated appropriately however the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment while it is generally easy to design information elicitation mechanism that motivate the seller to be truthful we show that if the seller ha additional relevant information it doe not want to reveal the buyer must resort to elicitation mechanism that work only some of the time the optimal design of such mechanism is shown to be computationally hard we show two different algorithm to solve the mechanism design problem each appropriate from a complexity point of view in different scenario 
a contract algorithm is an algorithm which is given a part of the input a specified amount of allowable computation time the algorithm must then compute a solution within the alloted time an interruptible algorithm in contrast can be interrupted at an arbitrary point in time and must produce a solution it is known that contract algorithm can simulate interruptible algorithm using iterative deepening technique this simulation is done at a penalty in the performance of the solution a measured by the so called acceleration ratio in this paper we give matching i e optimal upper and lower bound for the acceleration ratio under this simulation this resolve an open conjecture of bernstein et al ijcai who gave an ingenious optimal schedule under the restricted setting of round robin and length increasing processor schedule but whose optimality in the general unrestricted case remained open 
currently most word sense disambiguation wsd system are relatively individual word sense expert scarcely do these system take word sense transition between sens of linearly consecutive word or syntactically dependent word into consideration word sense transition are very important they embody the fluency of semantic expression and avoid sparse data problem effectively in this paper how net knowledge base is used to decompose every word sense into several sememes then one transition between two word sens becomes multiple transition between sememes sememe transition are much easier to be captured than word sense transition due to much le sememes when sememes are labeled wsd is done in this paper multi layered conditional random field mlcrf is proposed to model sememe transition the experiment show that mlcrf performs better than a base line system and a maximum entropy model syntactic and hypernym feature can enhance the performance significantly 
we propose an approach for extending both the terminological and the assertional part of a description logic knowledge base by using information provided by the assertional part and by a domain expert the use of technique from formal concept analysis ensures that on the one hand the interaction with the expert is kept to a minimum and on the other hand we can show that the extended knowledge base is complete in a certain sense 
this work present a model for learning inference procedure for story comprehension through inductive generalization and reinforcement learning based on classified example the learned inference procedure or strategy are represented a of sequence of transformation rule the approach is compared to three prior system and experimental result are presented demonstrating the efficacy of the model 
accurate recognition and tracking of human activity is an important goal of ubiquitous computing recent advance in the development of multi modal wearable sensor enable u to gather rich datasets of human activity however the problem of automatically identifying the most useful feature for modeling such activity remains largely unsolved in this paper we present a hybrid approach to recognizing activity which combine boosting to discriminatively select useful feature and learn an ensemble of static classifier to recognize different activity with hidden markov model hmms to capture the temporal regularity and smoothness of activity we tested the activity recognition system using over hour of wearable sensor data collected by volunteer in natural unconstrained environment the model succeeded in identifying a small set of maximally informative feature and were able identify ten different human activity with an accuracy of 
the distribution of case in the case base is critical to the performance of a case based reasoning system the case author is given little support in the positioning of new case during the development stage of a case base in this paper we argue that classification boundary represent important region of the problem space they are used to identify location where new case should be acquired we introduce two complexity guided algorithm which use a local complexity measure and boundary identification technique to actively discover case close to boundary the ability of these algorithm to discover new case that significantly improve the accuracy of case base is demonstrated on five public domain classification datasets 
searching the feature space for a subset yielding optimum performance tends to be expensive especially in application where the cardinality of the feature space is high e g text categorization this is particularly true for massive datasets and learning algorithm with worse than linear scaling factor linear support vector machine svms are among the top performer in the text classification domain and often work best with very rich feature representation even they however benefit from reducing the number of feature sometimes to a large extent in this work we propose alternative to exact re induction of svm model during the search for the optimum feature subset the approximation offer substantial benefit in term of computational efficiency we are able to demonstrate that no significant compromise in term of model quality are made and moreover in some case gain in accuracy can be achieved 
modeling the dynamic of cellular process ha recently become a important research area of many discipline one of the most important reason to model a cellular process is to enable highthroughput in silico experiment that attempt to predict or intervene in the process these experiment can help accelerate the design of therapy through their cheap replication and alteration while some technique exist for reasoning with cellular process few take advantage of the flexible and scalable algorithm popularized in ai research in this domain where scalability is crucial for feasible application we apply ai planning based search technique and demonstrate their advantage over existing enumerative method 
the textual entailment task determining if a given text entail a given hypothesis provides an abstraction of applied semantic inference this paper describes first a general generative probabilistic setting for textual entailment we then focus on the sub task of recognizing whether the lexical concept present in the hypothesis are entailed from the text this problem is recast a one of text categorization in which the class are the vocabulary word we make novel use of na ve bayes to model the problem in an entirely unsupervised fashion empirical test suggest that the method is effective and compare favorably with state of the art heuristic scoring approach 
abstract this paper proposes a hidden markov model hmm based algorithm for automatic decision of piano fingering we represent the position and form of hand and finger a hmm state and model the resulted sequence of performed note a emission associated with hmm transition optimal fingering decision is thus formulated a viterbi search to find the most likely sequence of state transition the proposed algorithm model the required effort in pressing a key with a finger followed by another key with another finger and in two dimensionalpositioningoffingerson thepiano keyboardwith diatonicand chromatickeys fundamental functionality of the algorithm wa verified through experiment with real piano piece this framework can be extended to polyphonic music containing chord 
weighted threshold game are coalitional game in which each player ha a weight intuitively corresponding to it voting power and a coalition is successful if the sum of it weight exceeds a given threshold key question in coalitional game include finding coalition that are stable in the sense that no member of the coalition ha any rational incentive to leave it and finding a division of payoff to coalition member an imputation that is fair we investigate the computational complexity of such question for weighted threshold game we study the core the least core and the nucleolus distinguishing those problem that are polynomial time computable from those that are np hard and providing pseudopolynomial and approximation algorithm for the np hard problem 
the purpose of this paper is to address the problem of maintaining coherent perceptual information in a mobile robotic system working over extended period of time interacting with a user and using multiple sensing modality to gather information about the environment and specific object we present a system which is able to use spatial and olfactory sensor to patrol a corridor and execute user requested task to cope with perceptual maintenance we present an extension of the anchoring framework capable of maintaining the correspondence between sensor data and the symbolic description referring to object it is also capable of tracking and acquiring information from observation derived from sensor data a well a information from a priori symbolic concept the general system is described and an experimental validation on a mobile robot is presented 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
one of the ground tool used to operate the mar exploration rover is a mixed initiative planning system called mapgen the role of the system is to assist operator building daily plan for each of the rover maximizing science return while maintaining rover safety and abiding by science and engineering constraint in this paper we describe the mapgen system focusing on the mixed initiative planning aspect we note important challenge both in term of human interaction and in term of automated reasoning requirement we then describe the approach taken in mapgen focusing on the novel method developed by our team 
super solution to constraint program guarantee that if a limited number of variable lose their value repair solution can be found by modifying a bounded number of assignment however in many application domain the classical super solution framework is not expressive enough since it only reason about the number of break in a solution and the number of change that are necessary to find a repair for example in combinatorial auction we may wish to guarantee that we can always find a repair solution whose revenue exceeds some threshold while limiting the cost associated with forming such a repair in this paper we present the weighted super solution framework that involves two important extension firstly the set of variable that may lose their value is determined using a probabilistic approach enabling u to find repair solution for assignment that are most likely to fail secondly we include a mechanism for reasoning about the cost of repair the proposed framework ha been successfully used to find robust solution to combinatorial auction 
this paper proposes the design of a recommender system that us knowledge stored in the form of ontology the interaction amongst the peer agent for generating recommendation are based on the trust network that exists between them recommendation about a product given by peer agent are in the form of intuitionistic fuzzy set specified using degree of membership non membership and uncertainty in literature the recommender system use database to generate recommendation the presented design us ontology a knowledge representation technique for creating annotated content for semantic web seeing the potential and popularity of ontology among researcher we believe that ontology will be build and maintained in numerous knowledge domain for the semantic web and future application the presented recommender system us temporal ontology that absorb the effect of change in the ontology due to the dynamic nature of domain in addition to the benefit of ontology a case study of tourism recommender system is chosen to generate the recommendation for the selection of destination travel agent and the flight schedule a comparison of the generated recommendation with the manual recommendation by peer establishes the validity of the presented recommender system 
the crosslingual link detection problem call for identifying news article in multiple language that report on the same news event this paper present a novel approach based on constrained clustering we discus a general way for constrained clustering using a recent graph based clustering framework called correlation clustering we introduce a correlation clustering implementation that feature linear program chunking to allow processing larger datasets we show how to apply the correlation clustering algorithm to the crosslingual link detection problem and present experimental result that show correlation clustering improves upon the hierarchical clustering approach commonly used in link detection and hierarchical clustering approach that take constraint into account 
this paper present a novel user interface for handheld mobile device by recognizing hand grip pattern particularly we consider the scenario where the device is provided with an array of capacitive touch sensor underneath the exterior cover in order to provide the user with intuitive and natural manipulation experience we use pattern recognition technique for identifying the user band grip from the touch sensor preliminary user study suggest that filtering out unintended user hand grip is one of the most important issue to be resolved we discus the detail of the prototype implementation a well a engineering challenge for practical deployment 
in this paper we develop and evaluate several probabilistic model of user click through behavior that are appropriate for modeling the click through rate of item that are presented to the user in a list potential application include modeling the click through rate of search result from a search engine item ranked by a recommendation system and search advertisement returned by a search engine our model capture contextual factor related to the presentation a well a the underlying relevance or quality of the item we focus on two type of contextual factor for a given item the positional context of the item and the quality of the other result we evaluate our model on a search advertising dataset from microsoft s live search engine and demonstrate that modeling contextual factor improves the accuracy of click through model 
many plan based autonomous robot controller generate chain of abstract action in order to achieve complex dynamically changing and possibly interacting goal the execution of these action chain often result in robot behavior that show abrupt transition between subsequent action causing suboptimal performance the resulting motion pattern are so characteristic for robot that people imitating robotic behavior will do so by making abrupt movement between action in this paper we propose a novel computation model for the execution of abstract action chain in this computation model a robot first learns situation specific performance model of abstract action it then us these model to automatically specialize the abstract action for their execution in a given action chain this specialization result in refined chain that are optimized for performance a a side effect this behavior optimization also appears to produce action chain with seamless transition between action 
the paper present a new algorithm for multiobjective heuristic graph search problem the algorithm present some nice property that are easily proven additionally empirical test show that substantial saving in memory can be achieved over previous proposal 
the paper present a new algorithm for multiobjective heuristic graph search problem the algorithm present some nice property that are easily proven additionally empirical test show that substantial saving in memory can be achieved over previous proposal 
the aim of this paper is to propose a new resolution framework for the sat and max sat problem which introduces a third truth value undefined in order to improve the resolution efficiency using this framework we have adapted the classic algorithm tabu search andwalksat promising result are obtained and show the interest of our approach 
over the past decade functional magnetic resonance imaging fmri ha emerged a a powerful technique to locate activity of human brain while engaged in a particular task or cognitive state we consider the inverse problem of detecting the cognitive state of a human subject based on the fmri data we have explored classification technique such a gaussian naive bayes k nearest neighbour and support vector machine in order to reduce the very high dimensional fmri data we have used three feature selection strategy discriminating feature and activity based feature were used to select feature for the problem of identifying the instantaneous cognitive state given a single fmri scan and correlation based feature were used when fmri data from a single time interval wa given a case study of visuo motor sequence learning is presented the set of cognitive state we are interested in detecting are whether the subject ha learnt a sequence and if the subject is paying attention only towards the position or towards both the color and position of the visual stimulus we have successfully used correlation based feature to detect position color related cognitive state with accuracy and the cognitive state related to learning with accuracy 
we propose a new algorithm for finding a target node in a network whose topology is known only locally we formulate this task a a problem of decision making under uncertainty and use the statistical property of the graph to guide this decision this formulation us the homophily and degree structure of the network simultaneously differentiating our algorithm from those previously proposed in the literature because homophily and degree disparity are characteristic frequently observed in real world network the algorithm we propose is applicable to a wide variety of network including two family that have received much recent attention small world and scale free network 
decentralized reputation system have recently emerged a a prominent method of establishing trust among self interested agent in online environment a key issue is the efficient aggregation of data in the system several approach have been proposed but they are plagued by major shortcoming we put forward a novel decentralized data management scheme grounded in gossip based algorithm rumor mongering is known to posse algorithmic advantage and indeed our framework inherits many of their salient feature scalability robustness globality and simplicity we also demonstrate that our scheme motivates agent to maintain a sparkling clean reputation and is inherently impervious to certain kind of attack 
the closed world assumption cwa on database express that an atom not in the database is false a more appropriate assumption for database that are sound but partially incomplete is the local closed world assumption lcwa which is a local form of the cwa expressing that the database is complete in a certain area called the window of expertise database consisting of a standard database instance augmented with a collection of lcwa s are called locally closed database in this paper we investigate the complexity of certain and possible query answering in such database a it tum out that these problem are intractlble we develop efficient approximate method to underestimate certain answer and overestimate possible answer we prove that under certain condition our method produce complete answer 
we analyze the problem of computing pure nash equilibrium in action graph game aggs which are a compact game theoretic representation while the problem is np complete in general for certain class of aggs there exist polynomial time algorithm we propose a dynamic programming approach that construct equilibrium of the game from equilibrium of restricted game played on subgraphs of the action graph in particular if the game is symmetric and the action graph ha bounded treewidth our algorithm determines the existence of pure nash equilibrium in polynomial time 
we present an unsupervised method for resolving word sense ambiguity in one language by using statistical evidence assembled from other language it is crucial for this approach that text are mapped into a language independent interlingual representation we also show that the coverage and accuracy resulting from multilingual source outperform analysis where only monolingual training data is taken into account 
coalition formation is a problem of great interest in ai allowing group of autonomous individually rational agent to form stable team automating the negotiation underlying coalition formation is naturally of special concern however research to date in both ai and economics ha largely ignored the potential presence of uncertainty in coalitional bargaining we present a model of discounted coalitional bargaining where agent are uncertain about the type or capability of potential partner and hence the value of a coalition we cast the problem a a bayesian game in extensive form and describe it perfect bayesian equilibrium a the solution to a polynomial program we then present a heuristic algorithm using iterative coalition formation to approximate the optimal solution and evaluate it performance 
power management technique for mobile appliance put the component of the system into low power state to maximize battery life while minimizing the impact on the perceived performance of the device static timeout policy are the state of the art approach for solving power management problem in this work we propose adaptive timeout policy a a simple and efficient solution for fine grained power management a discussed in the paper the policy reduce the latency of static timeout policy by nearly one half at the same power saving this result can be also viewed a increasing the power saving of static timeout policy at the same latency target the main objective of our work is to propose practical adaptive policy therefore our adaptive solution is fast enough to be executed within le than one millisecond and sufficiently simple to be deployed directly on a microcontroller we validate our idea on two recorded cpu activity trace which involve more than million entry each 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
for ontology represented a description logic tboxes optimised dl reasoner are able to detect logical error but there is comparatively limited support for resolving such problem one possible remedy is to weaken the available information to the extent that the error disappear but to limit the weakening process a much a possible the most obvious way to do so is to remove just enough tbox sentence to eliminate the error in this paper we propose a tableau like procedure for finding maximally concept satisfiable terminology represented in the description logic alc we discus some optimisation technique and report on preliminary but encouraging experimental result 
in this paper we propose an empirical likelihood el based strategy for building confidence interval for difference between two contrasting group the proposed method can deal with the situation when we know little prior knowledge about the two group which are referred to a non parametric situation we experimentally evaluate our method on uci datasets and observe that proposed el based method outperforms other method 
this paper examines the effect of lifetime learning on population evolving genetically in a series of changing environmets the analysis of both fitness and diversity of the population provides an insight into the improved performance provided by lifetime learning the nk fitness landscape model is employed a the problem task which ha the advantage of being able to generate a variety of fitness landscape of varying difficulty experiment observe the response of population in an environment where problem difficulty increase and decrease with varying frequency result show that lifetime learning is capable of overall higher fitness level and in addition that lifetime learning stimulates the diversity of the population this increased diversity allows lifetime learning a greater level of recovery and stability than evolutionary learning alone 
in multiple criterion markov decision process mdp where multiple cost are incurred at every decision point current method solve them by minimising the expected primary cost criterion while constraining the expectation of other cost criterion to some critical value however system are often faced with hard constraint where the cost criterion should never exceed some critical value at any time rather than constraint based on the expected cost criterion for example a resource limited sensor network no longer function once it energy is depleted based on the semi mdp smdp model we study the hard constrained hc problem in continuous time state and action space with respect to both finite and infinite horizon and various cost criterion we show that the hcsmdp problem is np hard and that there exists an equivalent discrete time mdp to every hcsmdp hence classical method such a reinforcement learning can solve hcsmdps 
compiling bayesian network ha proven an effective approach for inference that can utilize both global and local network structure in this paper we define a new method of compiling based on variable elimination ve and algebraic decision diagram add the approach is important for the following reason first it exploit local structure much more effectively than previous technique based on ve second the approach allows any of the many ve variant to compute answer to multiple query simultaneously third the approach make a large body of research into more structured representation of factor relevant in many more circumstance than it ha been previously finally experimental result demonstrate that ve can exploit local structure a effectively a state of the art algorithm based on conditioning on the network considered and can sometimes lead to much faster compilation time 
recent work on compiling bayesian network ha reduced the problem to that of factoring cnf encoding of these network providing an expressive framework for exploiting local structure for network that have local structure large cpts yet no excessive determinism the quality of the cnf encoding and the amount of local structure they capture can have a significant effect on both the offline compile time and online inference time we examine the encoding of such bayesian network in this paper and report on new finding that allow u to significantly scale this compilation approach in particular we obtain order of magnitude improvement in compile time compile some network successfully for the first time and obtain ordersof magnitude improvement in online inference for some network with local structure a compared to baseline jointree inference which doe not exploit local structure 
organizational model within multi agent system literature are of a static nature depending upon circumstance adaptation of the organizational model can be essential to ensure a continuous successful function of the system this paper present an approach based on max flow network to dynamically adapt organizational model to environmental fluctuation first a formal mapping between a well known organizational modeling framework and max flow network is presented having such a mapping maintains the insightful structure of an organizational model whereas specifying efficient adaptation algorithm based on max flow network can be done a well thereafter two adaptation mechanism based on max flow network are introduced each being appropriate for different environmental characteristic 
we present a proactive communication approach that allows cbr agent to gauge the strength and weakness of other cbr agent the communication protocol allows cbr agent to learn from communicating with other cbr agent in such a way that each agent is able to retain certain case provided by other agent that are able to improve their individual performance without need to disclose all the content of each case base the selection and retention of case is modeled a a case bartering process where each individual cbr agent autonomously decides which case offer for bartering and which offered barter accepts experimental evaluation show that the sum of all these individual decision result in a clear improvement in individual cbr agent performance with only a moderate increase of individual case base 
pervasive computing environment are dynamic and heterogeneous they are required to be self managing and autonomic demanding minimal user s guidance in pervasive computing contextaware adaptation is a key concept to meet the varying requirement of different client in order to enable context aware adaptation context information must be gathered and eventually presented to the application performing the adaptation it is clear that some form of context categorization will be required given the wide range of heterogeneous context information categorization can be made from different viewpoint such a conceptual viewpoint measurement viewpoint temporal characteristic viewpoint and so on to facilitate the programming of context aware application modelling of contextual information is highly necessary most of the existing model fail both to represent dependency relation between the diverse context information and to utilize these dependency relation a number of them support narrow class of context and applied to limited type of application and most do not consider the issue of quality of contextual information qoci along with a detailed context categorization this paper will analyse existing context model and discus their handling of dependency issue it us this analysis to derive a methodology for quality context information modelling in context aware computing 
generality or refinement relation between different theory have important application to generalization in inductive logic programming refinement of ontology and coordination in multi agent system we study generality relation in disjunctive default logic by comparing the amount of information brought by default theory intuitively a default theory is considered more general than another default theory if the former brings more information than the latter using technique in domain theory we introduce different type of generality relation over default theory we show that generality relation based on the smyth and hoare ordering reflect ordering on skeptical and credulous consequence respectively and that two default theory are equivalent if and only if they are equally general under these ordering these result naturally extend both generality relation over first order theory and those for answer set programming 
within the larger area of automatic acquisition of knowledge from the web we introduce a method for extracting relevant attribute or quantifiable property for various class of object the method extract attribute such a capital city and president for the class country or cost manufacturer and side effect for the class drug without relying on any expensive language resource or complex processing tool in a departure from previous approach to large scale information extraction we explore the role of web query log rather than web document a an alternative source of class attribute the quality of the extracted attribute recommends query log a a valuable albeit little explored resource for information extraction 
horn description logic horn dl have recently started to attract attention due to the fact that their worst case data complexity are in general lower than their overall i e combined complexity which make them attractive for reasoning with large aboxes however the natural question whether horn dl also provide advantage for tbox reasoning ha hardly been addressed so far in this paper we therefore provide a thorough and comprehensive analysis of the combined complexity of horn dl while the combined complexity for many horn dl turn out to be the same a for their non horn counterpart we identify subboolean dl where hornness simplifies reasoning 
in this paper we consider scenario such a web service composition where a planner need to discover it operator by querying a potentially very large and dynamically changing directory our contribution is a directory system that represents service advertisement and request a propositional formula and provides a flexible query language allowing complex selection and ranking expression the internal structure of the directory enables efficient selection and ranking in the presence of a large number of service thanks to it organization a a balanced tree with an extra intersection predicate in order to optimally exploit the index structure of the directory a transformation scheme is applied to the original query experimental result on randomly generated service composition problem illustrate the benefit of our approach 
a major goal for ai is to allow user to interact with agent that learn in real time making new kind of interactive simulation training application and digital entertainment possible this paper describes such a learning technology called real time neuroevolution of augmenting topology rtneat and describes how rtneat wa used to build the neuroevolving robotic operative nero video game this game represents a new genre of machine learning game where the player train agent in real time to perform challenging task in a virtual environment providing layman the capability to effectively train agent in real time with no prior knowledge of ai or machine learning ha broad implication both in promoting the field of ai and making it achievement accessible to the public at large 
in many computing system information is produced and processed by many people knowing how much a user trust a source can be very useful for aggregating filtering and ordering of information furthermore if trust is used to support decision making it is important to have an accurate estimate of trust when it is not directly available a well a a measure of confidence in that estimate this paper describes a new approach that give an explicit probabilistic interpretation for confidence in social network we describe sunny a new trust inference algorithm that us a probabilistic sampling technique to estimate our confidence in the trust information from some designated source sunny computes an estimate of trust based on only those information source with high confidence estimate in our experiment sunny produced more accurate trust estimate than the well known trust inference algorithm tidaltrust golbeck demonstrating it effectiveness 
most ai diagnostic reasoning approach model component and but not their interconnection and when they do model interconnection they model the possibility that a connection can break not that two connection may join e g through fluid leakage or electrical short circuit two reason for this limitation are that modeling these interconnection failure could require an exponential number in the number of interconnection failure possibility and that modeling interconnection failure requires modeling the system at a more precise level which requires far more complex model a fundamental contribution of this paper is a more powerful approach to modeling connection which doe not require special case post processing and is computationally tractable we illustrate our approach in the context of digital system 
this paper proposes a new combinatorial auction protocol called average max minimal bundle am mb protocol the characteristic of the am mb protocol are a follows i it is strategyproof i e truth telling is a dominant strategy ii the computational overhead is very low since it allocates bundle greedily thereby avoiding an explicit combinatorial optimization problem and iii it can obtain higher social surplus and revenue than can the max minimal bundle m mb protocol which also satisfies i and ii furthermore this paper extends the am mb protocol to an open ascending price protocol in which straightforward bidding is an ex post nash equilibrium 
in this paper we describe a computer supported cooperative learning system in education and the result of it deployment the system called i mind consists of a set of teacher agent group agent and student agent while the agent posse individual intelligent capability the novel invention of i mind lie in multiagent intelligence and coalition formation i mind support student participation and collaboration and help the instructor manage large distance classroom specifically it us a vickrey auction based and learning enabled algorithm called valcam to form student group in a structured cooperative learning setting we have deployed i mind in an introductory computer science course c and conducted experiment in the spring and fall semester of to study how i minos supported collaboration fare against traditional face to face collaboration result showed that student using i mind performed and outperformed in some aspect a well a student in traditional setting 
in single value domain each agent ha the same private value for all desired outcome we formalize this notion and give new example for such domain including a sat domain and a single value combinatorial auction domain we study two informational model where the set of desired outcome is public information the known case and where it is private information the unknown case under the known assumption we present several truthful approximation mechanism additionally we suggest a general technique to convert any bitonic approximation algorithm for an unweighted domain where agent value are either zero or one to a truthful mechanism with only a small approximation loss in contrast we show that even positive result from the unknown single minded combinatorial auction literature fail to extend to the unknown single value case we give a characterization of truthfulness in this case demonstrating that the difference is subtle and surprising 
intelligent non player character that exhibit realistic ambient behavior produce more captivating and immersive story for the player however the creation of nonrepetitive and entertaining behavior is challenging since it involves writing complex custom scripting code for thousand of character in a common game adventure this demonstration describes the generation of motivational behavior script using generative behavior pattern with scriptease we demonstrate interruptible and resumable motivational ambient and latent behavior for a tavern scene in a custom neverwinter night game module 
this paper present a method for designing a semisupervised classifier for multi component data such a web page consisting of text and link information the proposed method is based on a hybrid of generative and discriminative approach to take advantage of both approach with our hybrid approach for each component we consider an individual generative model trained on labeled sample and a model introduced to reduce the effect of the bias that result when there are few labeled sample then we construct a hybrid classifier by combining all the model based on the maximum entropy principle in our experimental result using three test collection such a web page and technical paper we confirmed that our hybrid approach wa effective in improving the generalization performance of multi component data classification 
a machine learning ml system known a roams ranker for open auto maintenance scheduling wa developed to create failure susceptibility ranking for almost one thousand kv kv energy distribution feeder cable that supply electricity to the borough of new york city in manhattan ranking are updated every minute and displayed on distribution system operator screen additionally a separate system make seasonal prediction of failure susceptibility these feeder failure known a open auto or o a are a significant maintenance problem a year s sustained research ha led to a system that demonstrates high accuracy of the feeder that actually failed over the summer of were in the of feeder ranked a most at risk by the end of the summer the most susceptible feeder a ranked by the ml system were accounting for up to of all o a that subsequently occurred each day the system s algorithm also identifies the factor underlying failure which change over time and with varying condition especially temperature providing insight into the operating property and failure cause in the feeder system 
team of robot are more fault tolerant than single robot and auction appear to be promising mean for coordinating them in a recent paper at robotics science and system we analyzed a coordination system based on sequential single item auction we showed that the coordination system is simple to implement and computation and communication efficient and that the resulting sum of all travel distance in known terrain is guaranteed to be only a constant factor away from optimum in this paper we put these result in perspective by comparing our coordination system against those based on either parallel single item auction or combinatorial auction demonstrating that it combine the advantage of both 
in recent year the size of combinatorial application and the need to produce high quality solution quickly have increased steadily providing significant challenge for optimization algorithm this paper address this issue for large scale vehicle routing problem with time window a class of very difficult optimization problem involving complex spatial and temporal dependency it proposes a randomized adaptive spatial decoupling rasd scheme for vehicle routing with time window in order to produce high quality solution quickly experimental result on hard instance with customer and vehicle show that the rasd scheme together with large neighborhood search significantly improves the quality of the solution under time constraint interestingly the rasd scheme when allowed to run longer also improves the best available solution in almost all the tested instance 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
this paper introduces a novel approach to model warrant computation in a skeptical abstract argumentation framework we show that such search space can be defined a a lattice and illustrate how the so called dialectical constraint can play a role for guiding the efficient computation of warranted argument 
an emerging empirical methodology bridge the gap between game theory and simulation for practical strategic reasoning 
agent with partial observability need to share information to achieve decentralised coordination however in resource constrained system indiscriminate communication can create performance bottleneck by consuming valuable bandwidth therefore there is a tradeoff between the utility attained by communication and it cost here we address this tradeoff by developing a novel strategy to make communication selective based on information redundancy ensuring communication only occurs when necessary while maintaining acceptable coordination we apply this strategy to a state of the art communication protocol to evaluate it resource saving benefit in a distributed network routing problem furthermore we design a mechanism to adapt it selectivity level to the prevailing resource constraint to ensure further improvement empirical study show our selective strategy achieves relative saving in bandwidth usage of with only a relative reduction in coordination effectiveness and the adaptive strategy further improves relative bandwidth usage by up to and also relative coordination effectiveness by up to over the non adaptive approach 
although necessary learning to discover new solution is often long and difficult even for supposedly simple task such a counting on the other hand learning by imitation provides a simple way to acquire knowledge by watching other agent do in order to learn more complex task by imitation than mere sequence of action a think aloud protocol is introduced with a new neuro symbolic network the latter us time in the same way a in a time delay neural network and is added basic first order logic capacity tested on a benchmark counting task learning is very fast generalization is accurate whereas there is no initial bias toward counting 
a any other problem solving task that employ search ai planning need heuristic to efficiently guide the problem space exploration machine learning ml provides several technique for automatically acquiring those heuristic usually a planner solves a problem and a ml technique generates knowledge from the search episode in term of complete plan macro operator or case or heuristic also named control knowledge in planning in this paper we present a novel way of generating planning heuristic we learn heuristic in one planner and transfer them to another planner this approach is based on the fact that different planner employ different search bias we want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that us a different search bias the goal is to improve the efficiency of the second planner by capturing regularity of the domain that it would not capture by itself due to it bias we employ a deductive learning method ebl that is able to automatically acquire control knowledge by generating bounded explanation of the problem solving episode in a graphplan based planner then we transform the learned knowledge so that it can be used by a bidirectional planner 
spam deobfuscation is a processing to detect obfuscated word appeared in spam email and to convert them back to the original word for correct recognition lexicon tree hidden markov model lthmm wa recently shown to be useful in spam deobfuscation however lt hmm suffers from a huge number of state which is not desirable for practical application in this paper we present a complexity reduced hmm referred to a dynamically weighted hmm dw hmm where the state involving the same emission probability are grouped into super state while preserving state transition probability of the original hmm dwhmm dramatically reduces the number of state and it state transition probability are determined in the decoding phase we illustrate how we convert a lt hmm to it associated dw hmm we confirm the useful behavior of dw hmm in the task of spam deobfuscation showing that it significantly reduces the number of state while maintaining the high accuracy 
propositionalization of a first order theory followed by satisfiability testing ha proved to be a remarkably efficient approach to inference in relational domain such a planning kautz selman and verification jackson more recently weighted satisfiability solver have been used successfully for mpe inference in statistical relational learner singla domingo however fully instantiating a finite first order theory requires memory on the order of the number of constant raised to the arity of the clause which significantly limit the size of domain it can be applied to in this paper we propose lazysat a variation of the walk sat solver that avoids this blowup by taking advantage of the extreme sparseness that is typical of relational domain i e only a small fraction of ground atom are true and most clause are trivially satisfied experiment on entity resolution and planning problem show that lazysat reduces memory usage by order of magnitude compared to walk sat while taking comparable time to run and producing the same solution 
since the s ai a a science ha progressively fragmented into many activity that are very narrowly focused it is not clear that work done within these fragment can be combined in the design of a human like integrated system long held a one of the goal of ai a science a strategy is proposed for reintegrating ai based around a backward chaining anal ysis to produce a roadmap with partially ordered milestone based on detailed scenario that everyone can agree are worth achieving even when they disagree about mean 
numerous logic have been developed for reasoning about inconsistency which differ in i the logic to which they apply and ii the criterion used to draw inference in this paper we propose a general framework for reasoning about inconsistency in a wide variety of logic including one for which inconsistency resolution method have not yet been studied e g various temporal and epistemic logic we start with tarski and scott s axiomatization of logic but drop their monotonicity requirement that we believe are too strong for ai for such a logic l we define the concept of an option option are set of formula in l that are closed and consistent according to the notion of consequence and consistency in l we show that by defining an appropriate preference relation on option we can capture several existing work such a brewka s subtheories we also provide algorithm to compute most preferred option 
the ability to detect failure and to analyze their cause is one of the precondition of truly autonomous mobile robot especially online failure detection is a complex task since the effect of failure are typically difficult to model and often resemble the noisy system behavior in a fault free operational mode the extremely low a priori likelihood of failure pose additional challenge for detection algorithm in this paper we present an approach that applies gaussian process classification and regression technique for learning highly effective proposal distribution of a particle filter that is applied to track the state of the system a a result the efficiency and robustness of the state estimation process is substantially improved in practical experiment carried out with a real robot we demonstrate that our system is capable of detecting collision with unseen obstacle while at the same time estimating the changing point of contact with the obstacle 
in this paper we describe our entrant in the travel division of the trading agent competition tac at a high level the design of many successful autonomous trading agent can be summarized a follows i price prediction build a model of market price and ii optimization solve for an approximately optimal set of bid given this model to predict we simulate simultaneous ascending auction to optimize we apply the sample average approximation method both of these procedure might naturally be abbreviated saa hence the title of this paper our agent dominated the preliminary and seeding round of tac travel in and emerged a champion in the final in a photo finish 
in this paper we explore the use of the web a an environment for electronic commerce in particular we develop a novel mechanism that creates incentive for honesty in electronic marketplace where human user are represented by buying and selling agent in our mechanism buyer model other buyer and select the most trustworthy one a their neighbor from which they can ask advice about seller in addition however seller model the reputation of buyer reputable buyer provide fair rating of seller and are likely to be neighbor of many other buyer seller will provide more attractive product to reputable buyer in order to build their reputation we discus how a marketplace operating with our mechanism lead to better profit both for honest buyer and seller with honesty encouraged our work promotes the acceptance of web based agent oriented e commerce by human user 
qualitative reasoning about mereotopological relation ha been extensively investigated while more recently geometrical and spatio temporal reasoning are gaining increasing attention we propose to consider mathematical morphology operator a the inspiration for a new language and inference mechanism to reason about space interestingly the proposed morpho logic capture not only traditional mereotopological relation but also notion of relative size and morphology the proposed representational framework is a hybrid arrow logic theory for which we define a resolution calculus which is to the best of our knowledge the first such calculus for arrow logic 
scene understanding address the issue of what a scene contains existing research on scene understanding is typically focused on classifying a scene into class that are of the same category type these approach although they solve some scene understanding task successfully in general fail to address the semantics in scene understanding for example how doe an agent learn the concept label red and ball without being told that it is a color or a shape label in advance to cope with this problem we have proposed a novel research called semantic scene concept learning our proposed approach model the task of scene understanding a a multi labeling classification problem each scene instance perceived by the agent may receive multiple label coming from different concept category where the goal of learning is to let the agent discover the semantic meaning i e the set of relevant visual feature of the scene label received our preliminary experiment have shown the effectiveness of our proposed approach in solving this special intraand inter category mixing learning task 
in this paper we introduce a generic form of structural decomposition for the constraint satisfaction problem which we call a guarded decomposition we show that many existing decomposition method can be characterized in term of finding guarded decomposition satisfying certain specified additional condition using the guarded decomposition framework we are also able to define a new form of decomposition which we call a spread cut we show that discovery of width k spread cut decomposition is tractable for each k and that the spread cut decomposition strongly generalize all existing decomposition except hypertrees finally we exhibit a family of hypergraphs hn for n where the width of the best hypertree decomposition of each hn is at least n but the width of the best spreadcut decomposition is at most n 
we introduce a distributed negotiation framework for multi agent resource allocation where interaction between agent are limited by a graph defining a negotiation topology a group of agent may only contract a deal if that group is fully connected according to the negotiation topology an important criterion for assessing the quality of an allocation of resource in term of fairness is envy freeness an agent is said to envy another agent if it would prefer to swap place with that other agent we analyse under what circumstance a sequence of deal respecting the negotiation topology may be expected to converge to a state where no agent envy any of the agent it is directly connected to we also analyse the computational complexity of a related decision problem namely the problem of checking whether a given negotiation state admits any deal that would both be beneficial to every agent involved and reduce envy in the agent society 
mechanism for dividing a set of good amongst a number of autonomous agent need to balance efficiency and fairness requirement a common interpretation of fairness is envy freeness while efficiency is usually understood a yielding maximal overall utility we show how to set up a distributed negotiation framework that will allow a group of agent to reach an allocation of good that is both efficient and envy free 
we present an asymptotically optimal algorithm for the max variant of the k armed bandit problem given a set of k slot machine each yielding payoff from a fixed but unknown distribution we wish to allocate trial to the machine so a to maximize the expected maximum payoff received over a series of n trial subject to certain distributional assumption we show that o k ln k ln n trial are sufficient to identify with probability at least a machine whose expected maximum payoff is within of optimal this result lead to a strategy for solving the problem that is asymptotically optimal in the following sense the gap between the expected maximum payoff obtained by using our strategy for n trial and that obtained by pulling the single best arm for all n trial approach zero a n 
we present in this paper a new complete method for distributed constraint optimization based on dynamic programming it is a utility propagation method inspired by the sum product algorithm which is correct only for tree shaped constraint network in this paper we show how to extend that algorithm to arbitrary topology using a pseudotree arrangement of the problem graph our algorithm requires a linear number of message whose maximal size depends on the induced width along the particular pseudotree chosen we compare our algorithm with backtracking algorithm and present experimental result for some problem type we report order of magnitude fewer message and the ability to deal with arbitrarily large problem our algorithm is formulated for optimization problem but can be easily applied to satisfaction problem a well 
random problem distribution have played a key role in the study and design of algorithm for constraint satisfaction and boolean satisfiability a well a in our understanding of problem hardness beyond standard worst case complexity we consider random problem distribution from a highly structured problem domain that generalizes the quasigroup completion problem qcp and quasigroup with hole qwh a widely used domain that capture the structure underlying a range of real world application our problem domain is also a generalization of the well known sudoku puzzle we consider sudoku instance of arbitrary order with the additional generalization that the block region can have rectangular shape in addition to the standard square shape we evaluate the computational hardness of generalized sudoku instance for different parameter setting our experimental hardness result show that we can generate instance that are considerably harder than qcp qwh instance of the same size more interestingly we show the impact of different balancing strategy on problem hardness we also provide insight into backbone variable in generalized sudoku instance and how they correlate to problem hardness 
learning capability of computer system still lag far behind biological system one of the reason can be seen in the inefficient re use of control knowledge acquired over the lifetime of the artificial learning system to address this deficiency this paper present a learning architecture which transfer control knowledge in the form of behavioral skill and corresponding representation concept from one task to subsequent learning task the presented system us this knowledge to construct a more compact state space representation for learning while assuring bounded optimality of the learned task policy by utilizing a representation hierarchy experimental result show that the presented method can significantly outperform learning on a flat state space representation and the maxq method for hierarchical reinforcement learning 
this paper present an any time scheme for computing lower and upper bound on posterior marginals in bayesian network the scheme draw from two previously proposed method bounded conditioning horvitz suermondt cooper and bound propagation leisink kappen following the principle of cutset conditioning pearl our method enumerates a subset of cutset tuples and applies exact reasoning in the network instance conditioned on those tuples the probability mass of the remaining tuples is bounded using a variant of bound propagation we show that our new scheme improves on the earlier scheme 
the result of experiment in scientific domain such a material science are often depicted a graph the graph we refer to plot a dependent versus an independent variable showing the behavior of the experimental process they serve a good visual tool for analysis and comparison of the corresponding process performing an experiment in a laboratory and plotting such graph consumes significant time and resource motivating the need for computational estimation this is precisely the aim of this research more specifically the research goal are a follows given the input condition of an experimental process 
for this demonstration participant have the opportunity to control a humanoid robot located hundred of mile away the general task is to reach grasp and transport various object in the vicinity of the robot although remote pick and place operation of this sort form the basis of numerous practical application they are frequently error prone and fatiguing for human operator participant can experience the relative difficulty of remote manipulation both with and without the use of an assistive interface this interface simplifies the task by injecting artificial intelligence in key place without seizing higher level control from the operator in particular we demonstrate the benefit of two key component of the system a video display of predicted operator intention and a haptic based controller for automated grasping 
p log is a probabilistic logic programming language which combine both logic programming style knowledge representation and probabilistic reasoning in earlier paper various advantage of p log have been discussed in this paper we further elaborate on the kr prowess of p log by showing that i it can be used for causal and counterfactual reasoning and ii it provides an elaboration tolerant way for non naive conditioning 
this paper report on our recent work on modeling and automatically extracting vague implicit event duration from text pan et al a b it is a kind of commonsense knowledge that can have a substantial impact on temporal reasoning problem we have also proposed a method of using normal distribution to model judgment that are interval on a scale and measure their interannotator agreement this should extend from time to other kind of vague but substantive information in text and commonsense reasoning 
we study the notion of update of an ontology expressed a a description logic knowledge base such a knowledge base is constituted by two component called tbox and abox the former express general knowledge about the concept and their relationship whereas the latter describes the state of affair regarding the instance of concept we investigate the case where the update affect only the instance level of the ontology i e the abox building on classical approach on knowledge base update our first contribution is to provide a general semantics for instance level update in description logic we then focus on dl lite a specific description logic where the basic reasoning task are computationally tractable we show that dl lite is closed with respect to instance level update in the sense that the result of an update is always expressible a a new dl lite abox finally we provide an algorithm that computes the result of an update in dl lite and we show that it run in polynomial time with respect to the size of both the original knowledge base and the update formula 
over the past decade general satisfiability testing algorithm have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem such a planning and scheduling kautz and selman solving such np complete task by compilation to sat ha turned out to be an approach that is of both practical and theoretical interest recently sang et al have shown that state of the art sat algorithm can be efficiently extended to the harder task of counting the number of model satisfying assignment of a formula by employing a technique called component caching this paper begin to investigate the question of whether compilation to model counting could be a practical technique for solving real world p complete problem in particular bayesian inference we describe an efficient translation from bayesian network to weighted model counting extend the best model counting algorithm to weighted model counting develop an efficient method for computing all marginals in a single counting pas and evaluate the approach on computationally challenging reasoning problem 
we present transucp a formalism for transformational analogy in the context of classical domain independent planning transucp defines precisely possible plan modification operation for transformational analogy and cover a wide range of existing implementation we use transucp to analyze the implication for transformational analogy of well known result about the complexity of general plan adaptation 
this paper is a comparative study of game theoretic solution concept in strictly competitive multiagent scenario a commonly encountered in the context of parlor game competitive economic situation and some social choice setting we model these scenario a ranking game in which every outcome is a ranking of the player with higher rank being preferred over lower one rather than confining our attention to one particular solution concept we give matching upper and lower bound for various comparative ratio of solution concept within ranking game the solution concept we consider in this context are security level strategy maximin nash equilibrium and correlated equilibrium additionally we also examine quasistrict equilibrium an equilibrium refinement proposed by harsanyi which remedy some apparent shortcoming of nash equilibrium when applied to ranking game in particular we compute the price of cautiousness i e the worst possible loss an agent may incur by playing maximin instead of the worst quasi strict nash equilibrium the mediation value i e the ratio between the social welfare obtained in the best correlated equilibrium and the best nash equilibrium and the enforcement value i e the ratio between the highest obtainable social welfare and that of the best correlated equilibrium 
query learning model from computational learning theory clt can be adopted to perform elicitation in combinatorial auction indeed a recent elicitation framework demonstrated that the equivalence query of clt can be usefully simulated with price based demand query in this paper we validate the flexibility of this framework by defining a learning algorithm for atomic bidding language a class that includes xor and or we also handle incentive characterizing the communication requirement of the vickrey clarke grove outcome rule this motivates an extension to the earlier learning framework that brings truthful response to query into an equilibrium 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
maintaining environmental stability in a dynamic system is a difficult challenge in your living room when you set your thermostat to degree the actual temperature cycle above and below degree we attempt to use a recurrent neural network rnn in an aquarium control system that reduces such environmental swing 
in many interactive decision making scenario there is often no solution that satisfies all of the user s preference the decision process can be helped by providing explanation relaxation show set of consistent preference and thus indicate which preference can be enforced while exclusion set show which preference can be relaxed to obtain a solution we propose a new approach to explanation based on the notion of a representative set of explanation the size of the set of explanation we compute is exponentially more compact than that found using common approach from the literature based on finding all minimal conflict 
while pomdps partially observable markov decision problem are a popular computational model with wide ranging application the computational cost for optimal policy generation is prohibitive researcher are investigating ever more efficient algorithm yet many application demand such algorithm bound any loss in policy quality when chasing efficiency to address this challenge we present two new technique the first approximates in the value space to obtain solution efficiently for a pre specified error bound unlike existing technique our technique guarantee the resulting policy will meet this bound furthermore it doe not require costly computation to determine the quality loss of the policy our second technique prune large tract of belief space that are unreachable allowing faster policy computation without any sacrifice in optimality the combination of the two technique which are complementary to existing optimal policy generation algorithm provides solution with tight error bound efficiently in domain where competing algorithm fail to provide such tight bound 
planner reason with abstracted model of the behaviour they use to construct plan when plan are turned into the instruction that drive an executive the real behaviour interacting with the unpredictable uncertainty of the environment can lead to failure one of the challenge for intelligent autonomy is to recognise when the actual execution of a behaviour ha diverged so far from the expected behaviour that it can be considered to be a failure in this paper we present an approach by which a trace of the execution of a behaviour is monitored by tracking it most likely explanation through a learned model of how the behaviour is normally executed in this way possible failure are identified a deviation from common pattern of the execution of the behaviour we perform an experiment in which we inject error into the behaviour of a robot performing a particular task and explore how well a learned model of the task can detect where these error occur 
we introduce problog a probabilistic extension of prolog a problog program defines a distribution over logic program by specifying for each clause the probability that it belongs to a randomly sampled program and these probability are mutually independent the semantics of problog is then defined by the success probability of a query which corresponds to the probability that the query succeeds in a randomly sampled program the key contribution of this paper is the introduction of an effective solver for computing success probability it essentially combine sld resolution with method for computing the probability of boolean formula our implementation further employ an approximation algorithm that combine iterative deepening with binary decision diagram we report on experiment in the context of discovering link in real biological network a demonstration of the practical usefulness of the approach 
this paper proposes a new method to estimate the class membership probability of the case classified by a decision tree this method provides smooth class probability estimate without any modification of the tree when the data are numerical it applies a posteriori and doesn t use additional training case it relies on the distance to the decision boundary induced by the decision tree the distance is computed on the training sample it is then used a an input for a very simple one dimension kernel based density estimator which provides an estimate of the class membership probability this geometric method give good result even with pruned tree so the intelligibility of the tree is fully preserved 
the vickrey clarke grove vcg protocol is a theoretically well founded protocol that can be used for combinatorial auction however the vcg ha several limitation such a a vulnerability to false name bid b vulnerability to loser collusion and c the outcome is not in the core yokoo matsutani iwasaki presented a new combinatorial auction protocol called the grove mechanism with submodular approximation gm sma this protocol satisfies the following characteristic it is false name proof each winner is included in a pareto efficient allocation and a long a a pareto efficient allocation is achieved the protocol is robust against the collusion of loser and the outcome is in the core the gm sma is the first protocol that satisfies all three of these characteristic the basic idea of the gm sma are a follows i it is based on the vcg protocol i e the payment of a winner in this protocol is identical to the payment in one instance of the grove mechanism which is a class of protocol that includes the vcg ii when calculating the payment of a bidder we approximate the valuation of other bidder by using a submodular valuation function submodular approximation this paper show a high level presentation of the gm sma protocol and discus open problem and the relationship to other work in ai 
agile autonomous system are emerging such a unmanned aerial vehicle uavs that must robustly perform tightly coordinated time critical mission for example military surveillance or search and rescue scenario in the space domain execution of temporally flexible plan ha provided an enabler for achieving the desired coordination and robustness we address the challenge of extending plan execution to underactuated system that are controlled indirectly through the setting of continuous state variable our solution is a novel model based executive that take a input a temporally flexible state plan specifying intended state evolution and dynamically generates a near optimal control sequence to achieve optimality and safety the executive plan into the future framing planning a a disjunctive programming problem to achieve robustness to disturbance and tractability planning is folded within a receding horizon continuous planning framework key to performance is a problem reduction method based on constraint pruning we benchmark performance through a suite of uav scenario using a hardware in the loop testbed 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
qr factorization is most often used a a black box algorithm but is in fact an elegant computation on a factor graph by computing a rooted clique tree on this graph the computation can be parallelized across subtrees which form the basis of so called multifrontal qr method by judiciously choosing the order in which variable are eliminated in the clique tree computation we show that one straightforwardly obtains a method for performing inference in distributed sensor network one obvious application is distributed localization and mapping with a team of robot we phrase the problem a inference on a large scale gaussian markov random field induced by the measurement factor graph and show how multifrontal qr on this graph solves for the global map and all the robot pose in a distributed fashion the method is illustrated using both small and large scale simulation and validated in practice through actual robot experiment 
we present the formal evaluation of a framework that help student learn from analogical problem solving i e from problem solving activity that involve worked out example the framework incorporates an innovative example selection mechanism which tailor the choice of example to a given student so a to trigger studying behavior that are known to foster learning this involves a two phase process based on a probabilistic user model and a decision theoretic mechanism that selects the example with the highest overall utility for learning and problem solving success we describe this example selection process and present empirical finding from it evaluation 
we present a fast algorithm for learning the parameter of the abstract hidden markov model a type of hierarchical activity recognition model learning using exact inference scale poorly a the number of level in the hierarchy increase therefore an approximation is required for large model we demonstrate that variational inference is well suited to solve this problem not only doe this technique scale but it also offer a natural way to leverage the context specific independence property inherent in the model via the fixed point equation experiment confirm that the variational approximation significantly reduces the time necessary for learning while estimating parameter value that can be used to make reliable prediction 
abduction is a fundamental form of nonmonotonic reasoning that aim at finding explanation for observed manifestation application of this process range from car configuration to medical diagnosis we study here it computational complexity in the case where the application domain is described by a propositional theory built upon a fixed constraint language and the hypothesis and manifestation are described by set of literal we show that depending on the language the problem is either polynomial time solvable np complete or p complete in particular we show that under the assumption p np only language that are affine of width have a polynomial algorithm and we exhibit very weak condition for np hardness 
there are vast amount of free text on the internet that are neither grammatical nor formally structured such a item description on ebay or internet classified like craig s list these source of data called post are full of useful information for agent scouring the semantic web but they lack the semantic annotation to make them searchable annotating these post is difficult since the text generally exhibit little formal grammar and the structure of the post varies however by leveraging collection of known entity and their common attribute called reference set we can annotate these post despite their lack of grammar and structure to use this reference data we align a post to a member of the reference set and then exploit this matched member during information extraction we compare this extraction approach to more traditional information extraction method that rely on structural and grammatical characteristic and we show that our approach outperforms traditional method on this type of data 
we consider a special type of continuous time markov decision process mdps that arise when phase type distribution are used to model the timing of non markovian event and action we focus primarily on the execution of phase dependent policy phase are introduced into a model to represent relevant execution history but there is no physical manifestation of phase in the real world we treat phase a partially observable state feature and show how a belief distribution over phase configuration can be derived from observable state feature through the use of transient analysis for markov chain this result in an efficient method for phase tracking during execution that can be combined with the qmdp value method for pomdps to make action choice we also discus briefly how the structure of mdps with phase transition can be exploited in structured value iteration with symbolic representation of vector and matrix 
whenever mobile robot act in the real world they need to be able to deal with non static object in the context of mapping a common technique to deal with dynamic object is to filter out the spurious measurement corresponding to such object in this paper we present a novel approach to estimate typical configuration of dynamic area in the environment of a mobile robot our approach cluster local grid map to identify the possible configuration we furthermore describe how these cluster can be utilized within a rao blackwellized particle filter to localize a mobile robot in a non static environment in practical experiment carried out with a mobile robot in a typical office environment we demonstrate the advantage of our approach compared to alternative technique for mapping and localization in dynamic environment 
to be effective an agent that collaborates with human need to be able to learn new task from human they work with this paper describes a system that learns executable task model from a single collaborative learning session consisting of demonstration explanation and dialogue to accomplish this the system integrates a range of ai technology deep natural language understanding knowledge representation and reasoning dialogue system planning agent based system and machine learning a formal evaluation show the approach ha great promise 
arguably analogy is one of the most important aspect of intelligent reasoning it ha been hypothesized that given suitable background knowledge analogy can be viewed a a logical inference process this study follows another school of thought that argues that similarity can provide a probabilistic basis for inference and analogy most similarity measure which are frequently viewed a being conceptually equivalent to distance measure are restricted to either nominal or ordinal attribute and some are confined to classification task this paper proposes a flexible similarity measure that is task independent and applies to both nominal and ordinal data in a conceptually uniform way the proposed similarity measure is derived from a probability function and corresponds to the intuition that if we consider all neighborhood around a data point the data point closer to this point should be included in more of these neighborhood than more distant point experiment we have conducted to demonstrate the usefulness of this measure indicate that it fare very competitively with commonly used similarity measure 
understanding common sense reasoning about the physical world is one of the goal of qualitative reasoning research this paper describes how we combine qualitative mechanic and analogy to solve everyday physical reasoning problem posed a sketch the problem are drawn from the bennett mechanical comprehension test which is used to evaluate technician candidate we discus sketch annotation which define conceptual quantity in term of visual measurement how modeling decision are made by analogy and how analogy can be used to frame comparative analysis problem experimental result support the plausibility of this approach 
the ability to express derived predicate in the formalization of a planning domain is both practically and theoretically important in this paper we propose an approach to planning with derived predicate where the search space consists of rule action graph particular graph of action and rule representing derived predicate we present some technique for representing rule and reasoning with them which are integrated into a method for planning through local search and rule action graph we also propose some new heuristic for guiding the search and some experimental result illustrating the performance of our approach our proposed technique are implemented in a planner that took part in the fourth international planning competition showing good performance in many benchmark problem 
in semi supervised learning a number of labeled example are usually required for training an initial weakly useful predictor which is in turn used for exploiting the unlabeled example however in many real world application there may exist very few labeled training example which make the weakly useful predictor difficult to generate and therefore these semisupervised learning method cannot be applied this paper proposes a method working under a two view setting by taking advantage of the correlation between the view using canonical component analysis the proposed method can perform semi supervised learning with only one labeled training example experiment and an application to content based image retrieval validate the effectiveness of the proposed method 
in this paper we introduce two new algorithm for approximate inference of bayesian network that use edge deletion technique the first reduces a network to it maximal weight spanning tree using the kullback leibler information divergence a edge weight and then run pearl s algorithm on the resulting tree for linear time inference the second algorithm deletes edge from the triangulated graph until the biggest clique in the triangulated graph is below a desired bound thus placing a polynomial time bound on inference when tested for efficiency these two algorithm perform up to time faster than exact technique see www ci ksu edu jas research html for more information 
this paper considers the task reallocation problem where k agent are to be extracted from a coordinated group of n agent in order to perform a new task the interaction between the team member and the cost associated with this interaction are represented by a weighted graph consider a group of n robot organized in a formation the graph is the monitoring graph which represents the sensorial capability of the robot i e which robot can sense the other and at what cost following this example the team member reallocation problem this paper deal with is the extraction of k robot from the group in order to acquire a new target while minimizing the cost of the interaction of the remaining group in general the method proposed here shift the utility from the team member itself to the interaction between the member and calculates the reallocation according to this interaction utility we found that this can be done optimally by a deterministic polynomial time algorithm under several constraint the first constraint is that k o log n we describe several other domain in which this method is applicable 
kernel based nonlinear feature extraction kfe or dimensionality reduction is a widely used preprocessing step in pattern classification and data mining task given a positive definite kernel function it is well known that the input data are implicitly mapped to a feature space with usually very high dimensionality the goal of kfe is to find a low dimensional subspace of this feature space which retains most of the information needed for classification or data analysis in this paper we propose a subspace kernel based on which the feature extraction problem is transformed to a kernel parameter learning problem the key observation is that when projecting data into a low dimensional subspace of the feature space the parameter that are used for describing this subspace can be regarded a the parameter of the kernel function between the projected data therefore current kernel parameter learning method can be adapted to optimize this parameterized kernel function experimental result are provided to validate the effectiveness of the proposed approach 
in this work we recast some design principle commonly used in software engineering and adapt them to the design and analysis of domain description in reasoning about action we show how the informal requirement of cohesion and coupling can be turned into consistency test of several different arrangement of module this give u new criterion for domain description evaluation and clarifies the link between software and knowledge engineering in what concern the meta theory of action 
in this paper we present a unified knowledge based approach for sense disambiguation and semantic role labeling our approach performs both task through a single algorithm that match candidate semantic interpretation to background knowledge to select the best matching candidate we evaluate our approach on a corpus of sentence collected from various domain and show how our approach performs well on both sense disambiguation and semantic role labeling 
despite the significant progress to extend markov decision process mdp to cooperative multi agent system developing approach that can deal with realistic problem remains a serious challenge existing approach that solve decentralized markov decision process dec mdps suffer from the fact that they can only solve relatively small problem without complex constraint on task execution oc dec mdp ha been introduced to deal with large dec mdps under resource and temporal constraint however the proposed algorithm to solve this class of dec mdps ha some limit it suffers from overestimation of opportunity cost and restricts policy improvement to one sweep or iteration in this paper we propose to overcome these limit by first introducing the notion of expected opportunity cost to better ass the influence of a local decision of an agent on the others we then describe an iterative version of the algorithm to incrementally improve the policy of agent leading to higher quality solution in some setting experimental result are shown to support our claim 
in this paper we present a learning based approach for enabling domain awareness for a generic natural language interface our approach automatically acquires domain knowledge from user interaction and incorporates the knowledge learned to improve the generic system we have embedded our approach in a generic natural language interface and evaluated the extended system against two benchmark datasets we found that the performance of the original generic system can be substantially improved through automatic domain knowledge extraction and incorporation we also show that the generic system with domain awareness enabled by our approach can achieve performance similar to that of previous learning based domain specific system 
we describe a briefing system that learns to predict the content of report generated by user who create periodic weekly report a part of their normal activity the system observes content selection choice that user make and build a predictive model that could for example be used to generate an initial draft report using a feature of the interface the system also collect information about potential user specific feature the system wa evaluated under realistic condition by collecting data in a project based university course where student group leader were tasked with preparing weekly report for the benefit of the instructor using the material from individual student report this paper address the question of whether data derived from the implicit supervision provided by end user is robust enough to support not only model parameter tuning but also a form of feature discovery result indicate that this is the case system performance improves based on the feedback from user activity we find that individual learned model and feature are user specific although not completely idiosyncratic thismay suggest that approach which seek to optimizemodels globally say over a large corpus of data may not in fact produce result acceptable to all individual 
i propose that the notion of cognitive state be broadened from the current predicate symbolic language of thought framework to a multi modal one where perception and kinesthetic modality participate in thinking in contrast to the role assigned to perception and motor activity a module external to central cognition in the currently dominant theory in ai and cognitive science in the proposed approach central cognition incorporates part of the perceptual machinery i motivate and describe the proposal schematically and describe the implementation of a bi modal version in which a diagrammatic representation component is added to the cognitive state the proposal explains our rich multimodal internal experience and can be a key step in the realization of embodied agent the proposed multimodal cognitive state can significantly enhance the agent s problem solving 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
transfer learning address the problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain this paper considers transfer learning with markov logic network mlns a powerful formalism for learning in relational domain we present a complete mln transfer system that first autonomously map the predicate in the source mln to the target domain and then revise the mapped structure to further improve it accuracy our result in several real world domain demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch 
coalition formation is a key aspect of automated negotiation among self interested agent in order for coalition to be stable a key question that must be answered is how the gain from cooperation are to be distributed various solution concept such a the shapley value core least core and nucleolus have been proposed in this paper we demonstrate how these concept are vulnerable to various kind of manipulation in open anonymous environment such a the internet these manipulation include submitting false name one acting a many collusion many acting a one and the hiding of skill to address these threat we introduce a new solution concept called the anonymity proof core which is robust to these manipulation we show that the anonymity proof core is characterized by certain simple axiomatic condition furthermore we show that by relaxing these condition we obtain a concept called the least anonymity proof core which is guaranteed to be non empty we also show that computational hardness of manipulation may provide an alternative barrier to manipulation 
the virtual solar terrestrial observatory is a production semantic web data framework providing access to observational datasets from field spanning upper atmospheric terrestrial physic to solar physic the observatory allows virtual access to a highly distributed and heterogeneous set of data that appears a if all resource are organized stored and retrieved used in a common way the end user community comprises scientist student data provider numbering over out of an estimated community of we present detail on the case study our technological approach including the semantic web language tool and infrastructure deployed benefit of ai technology to the application and our present evaluation after the initial nine month of use 
one of the core component in information retrieval ir is the document term weighting scheme in this paper we will propose a novel learning based term weighting approach to improve the retrieval performance of vector space model in homogeneous collection we first introduce a simple learning system to weighting the index term of document then we deduce a formal computational approach according to some theory of matrix computation and statistical inference our experiment on collection will show that our approach out performs classic tfidf weighting about 
in this paper we consider the exploration of topological environment by a robot with weak sensory capability we assume only that the robot can recognize when it ha reached a vertex and can assign a cyclic ordering to the edge leaving a vertex with reference to the edge it arrived from given this limited sensing capability and without the use of any marker or additional information we will show that the construction of a topological map is still feasible this is accomplished through both the exploration strategy which is designed to reveal model inconsistency and by a search process that maintains a bounded set of believable world model throughout the exploration process plausible model are selected through the use of a ranking heuristic function based on the principle of occam s razor we conclude with numerical simulation demonstrating the performance of the algorithm 
this paper present a representation system for maintaining interacting durative state to replicate realistic emotional control our model the dynamic emotion representation der integrates emotional response and keep track of emotion intensity changing over time the developer can specify an interacting network of emotional state with appropriate onset sustains and decay the level of these state can be used a input for action selection including emotional expression we present both a general representational framework and a specific instance of a der network constructed for a virtual character the character s der us three type of emotional state a classified by duration timescales in keeping with current emotional theory the system is demonstrated with a virtual actor 
the future success of application layer video multicast depends on the availability of video stream distribution method that can scale in the number of stream sender and receiver previous work on the problem of application layer video streaming ha not effectively addressed scalability in the number of receiver and sender therefore new solution that are amenable to analysis and can achieve scalable p p video streaming are needed in this work we propose the use of automated negotiation algorithm to construct video streaming tree at the application layer we show that automated negotiation can effectively solve the problem of distributing a video stream to a large number of receiver 
in this paper we address the problem of inferring the topology or inter node navigability of a sensor network given non discriminating observation of activity in the environment by exploiting motion present in the environment our approach is able to recover a probabilistic model of the sensor network connectivity graph and the underlying traffic trend we employ a reasoning system made up of a stochastic expectation maximization algorithm and a higher level search strategy employing the principle of occam s razor to look for the simplest solution explaining the data the technique is assessed through numerical simulation and experiment conducted on a real sensor network 
we extend traditional description logic dl with a simple mechanism to handle approximate concept definition in a qualitative way often for example in medical application concept are not definable in a crisp way but can fairly exhaustively be constrained through a particular suband a particular super concept we introduce such lower and upper approximation based on rough set semantics and show that reasoning in these language can be reduced to standard dl satisfiability this allows u to apply rough description logic in a study of medical trial about sepsis patient which is a typical application for precise modeling of vague knowledge the study show that rough dl based reasoning can be done in a realistic use case and that modeling vague knowledge help to answer important question in the design of clinical trial 
in recent year planning and scheduling research ha paid increasing attention to problem that involve resource oversubscription where cumulative demand for resource outstrips their availability and some subset of goal or task must be excluded two basic class of technique to solve oversubscribed scheduling problem have emerged searching directly in the space of possible schedule and searching in an alternative space of task permutation by relying on a schedule builder to provide a mapping to schedule space in some problem context permutation based search method have been shown to outperform schedule space search method while in others the opposite ha been shown to be the case we consider two technique for which this behavior ha been observed taskswap t a schedule space repair search procedure and squeaky wheel optimization swo a permutation space scheduling procedure we analyze the circumstance under which one can be expected to dominate the other starting from a real world scheduling problem where swo ha been shown to outperform t we construct a series of problem instance that increasingly incorporate characteristic of a second real world scheduling problem where t ha been found to outperform swo experimental result provide insight into when schedule space method and permutation based method may be most appropriate 
the ability to interpret demonstration from the perspective of the teacher play a critical role in human learning robotic system that aim to learn effectively from human teacher must similarly be able to engage in perspective taking we present an integrated architecture wherein the robot s cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner a well a it own the performance of this architecture on a set of learning task is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning perspective taking both in human and in our architecture focus the agent s attention on the subset of the problem space that is important to the teacher this constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstration and thus learn what the teacher intends to teach 
in order to evaluate ontology matching algorithm it is necessary to confront them with test ontology and to compare the result with some reference the most prominent comparison criterion are precision and recall originating from information retrieval precision and recall are thought of a some degree of correction and completeness of result however when the object to compare are semantically defined like ontology and alignment it can happen that a fully correct alignment ha low precision this is due to the restricted set theoretic foundation of these measure drawing on previous syntactic generalization of precision and recall semantically justified measure that satisfy maximal precision and maximal recall for correct and complete alignment is proposed these new measure are compatible with classical precision and recall and can be computed 
most method for classifier design assume that the training sample are drawn independently and identically from an unknown data generating distribution although this assumption is violated in several real life problem relaxing this i i d assumption we consider algorithm from the statistic literature for the more realistic situation where batch or sub group of training sample may have internal correlation although the sample from different batch may be considered to be uncorrelated next we propose simpler more efficient variant that scale well to large datasets theoretical result from the literature are provided to support their validity experimental result from real life computer aided diagnosis cad problem indicate that relaxing the i i d assumption lead to statistically significant improvement in the accuracy of the learned classifier surprisingly the simpler algorithm proposed here is experimentally found to be even more accurate than the original version 
collaborative web search exploit repetition and regularity within the query space of a community of like minded individual in order to improve the quality of search result in short search result that have been judged to be relevant for past query are promoted in response to similar query that occur in the future in this paper we present the result of a large scale evaluation of this approach in a corporate web search scenario which show that significant benefit are available to it user 
this paper motivated by functional brain imaging application is interested in the discovery of stable spatio temporal pattern this problem is formalized a a multi objective multi modal optimization problem on one hand the target pattern must show a good stability in a wide spatio temporal region antagonistic objective on the other hand expert are interested in finding all such pattern global and local optimum the proposed algorithm termed d miner is empirically validated on artificial and real world datasets it show good performance and scalability detecting target spatiotemporal pattern within minute from mo datasets 
the paper present the theoretical foundation and an algorithm to reduce the effort of testing physical system a test is formally described a a set of stimulus input to the system to shift the system into a particular situation or state and a set of varia bles whose observation or measurement refutes hypothesis about the behavior mode the system is operating in test either generated automatically or by human may contain redundancy in the sense that some of it stimulus and or observables maybe irrelevant for achieving the result of the test identifying and dropping them contributes to redu cing the cost of set up action and measurement we define different kind of irrelevant stimulus discus their practical importance and present criterion and algorithm for computing reduced test 
research on macro operator ha a long history in planning and other search application there ha been a revival of interest in this topic leading to system that successfully combine macrooperators with current state of the art planning approach based on heuristic search however research is still necessary to make macro become a standard widely used enhancement of search algorithm this article introduces sequence of macro action called iterative macro iterative macro exhibit both the potential advantage e g travel fast towards goal and the potential limitation e g utility problem of classical macro only on a much larger scale a family of technique are introduced to balance this trade off in favor of faster planning experiment on a collection of planning benchmark show that when compared to low level search and even to search with classical macro operator iterative macro can achieve an impressive speed up in search 
improving ai planning algorithm relies on the ability to exploit the structure of the problem at hand a promising direction is that of factored planning where the domain is partitioned into subdomains with a little interaction a possible recent work in this field ha led to an detailed theoretical analysis of such approach and to a couple of high level planning algorithm but with no practical implementation or with limited experimentation this paper present dtreeplan a new generic factored planning algorithm which us a decomposition tree to efficiently partition the domain we discus some of it aspect progressively describing a specific implementation before presenting experimental result this prototype algorithm is a promising contribution with major possible improvement and help enrich the picture of factored planning approach 
reasoning about agent preference on a set of alternative and the aggregation of such preference into some social ranking is a fundamental issue in reasoning about multi agent system when the set of agent and the set of alternative coincide we get the ranking system setting a famous type of ranking system are page ranking system in the context of search engine in this paper we present an extensive axiomatic study of ranking system in particular we consider two fundamental axiom transitivity and ranked independence of irrelevant alternative surprisingly we find that there is no general social ranking rule that satisfies both requirement furthermore we show that our impossibility result hold under various restriction on the class of ranking problem considered each of these axiom can be individually satisfied moreover we show a complete axiomatization of approval voting using one of these axiom 
we introduce a new technique for counting model of boolean satisfiability problem our approach incorporates information obtained from sampling the solution space unlike previous approach our method doe not require uniform or near uniform sample it instead convert local search sampling without any guarantee into very good bound on the model count with guarantee we give a formal analysis and provide experimental result showing the effectiveness of our approach 
partially observable markov decision process pomdps are an intuitive and general way to model sequential decision making problem under uncertainty unfortunately even approximate planning in pomdps is known to be hard and developing heuristic planner that can deliver reasonable result in practice ha proved to be a significant challenge in this paper we present a new approach to approximate value iteration for pomdp planning that is based on quadratic rather than piecewise linear function approximators specifically we approximate the optimal value function by a convex upper bound composed of a fixed number of quadratic and optimize it at each stage by semidefinite programming we demonstrate that our approach can achieve competitive approximation quality to current technique while still maintaining a bounded size representation of the function approximator moreover an upper bound on the optimal value function can be preserved if required overall the technique requires computation time and space that is only linear in the number of iteration horizon time 
we analyze the asymptotic behavior of agent engaged in an infinite horizon partially observable stochastic game a formalized by the interactive pomdp framework we show that when agent initial belief satisfy a truth compatibility condition their behavior converges to a subjective equilibrium in a finite time and subjective equilibrium in the limit this result is a generalization of a similar result in repeated game to partially observable stochastic game however it turn out that the equilibrating process is difficult to demonstrate computationally because of the difficulty in coming up with initial belief that are both natural and satisfy the truth compatibility condition our result therefore shed some negative light on using equilibrium a a solution concept for decision making in partially observable stochastic game 
many interesting tractable problem are identified under the model of constraint satisfaction problem these problem are usually solved by forcing a certain level of local consistency in this paper for the class of connected row convex constraint we propose a novel algorithm which is based on the idea of variable elimination and efficient composition of row convex and connected constraint compared with the existing work including randomized algorithm the new algorithm ha better worst case time and space complexity 
the cyc project is predicated on the idea that effective machine learning depends on having a core of knowledge that provides a context for novel learned information what is known informally a common sense over the last twenty year a sufficient core of common sense knowledge ha been entered into cyc to allow it to begin effectively and flexibly supporting it most important task increasing it own store of world knowledge in this paper we present initial work on a method of using a combination of cyc and the world wide web accessed via google to assist in entering knowledge into cyc the long term goal is automating the process of building a consistent formalized representation of the world in the cyc knowledge base via machine learning we present preliminary result of this work and describe how we expect the knowledge acquisition process to become more accurate faster and more automated in the future 
in many real world multiagent application such a distributed sensor net a network of agent is formed based on each agent s limited interaction with a small number of neighbor while distributed pomdps capture the real world uncertainty in multiagent domain they fail to exploit such locality of interaction distributed constraint optimization dcop capture the locality of interaction but fails to capture planning under uncertainty this paper present a new model synthesized from distributed pomdps and dcops called networked distributed pomdps nd pomdps exploiting network structure enables u to present two novel algorithm for nd pomdps a distributed policy generation algorithm that performs local search and a systematic policy search that is guaranteed to reach the global optimal 
retrieving relevant document over the web is an overwhelming task when search engine return thousand of web document sifting through these document is time consuming and sometimes lead to an unsuccessful search one problem is that most search engine rely on matching a query to document based solely on topical keywords however many user of search engine have a particular genre in mind for the desired document the genre of a document concern aspect of the document such a the style or readability presentation layout and meta content such a word in the title or the existence of graph or photo by including genre in web search we hypothesize that web document retrieval could greatly improve accuracy by better matching document to the user s information need before implementing a search engine capable of discriminating on both genre and topic a feasibility analysis of genre classification is needed our previous research achieved classification accuracy across ten genre whereas similar research range between and accuracy however the ten genre used in our research were mostly distinct and only exemplar web document consisting of only one genre were chosen this paper discus our current work which involves an in depth analysis of maintaining high accuracy rate among genre that are very similar 
in this paper we deal with the problem of mining large distributed database we show that the aggregation of model i e set of disjoint classification rule each built over a subdatabase is quite enough to get an aggregated model that is both predictive and descriptive that present excellent prediction capability and that is conceptually much simpler than the comparable technique these result are made possible by lifting the disjoint cover constraint on the aggregated model and by the use of a confidence coefficient associated with each rule in a weighted majority vote 
in the context of operative disruption management decision support system have to evaluate the typically manifold option of responding to disturbance the temporal shift of activity and the allocation of alternative resource can be assessed by the application of generic scheduling framework such a the resource constrained project scheduling problem rcpsp however switch from one process variant to another one are usually not supported by the corresponding model even though they represent a common way of repairing broken schedule in many practical domain in this paper we thus show how the rcpsp can be extended by the concept of alternative activity making it possible to model and search within alternative process execution path beside a formal description of the conceptual extension we show how such generalized rescheduling problem can be solved by a novel genetic algorithmand summarize the promising result of a detailed evaluation 
we report on our experience in developing a query answering system that integrates multiple knowledge source the system is based on a novel architecture for combining knowledge source in which the source can produce new subgoals a well a ground fact in the search for answer to existing subgoals the system us a query planner that take into account different query processing capability of individual source and augments them gracefully a reusable ontology provides a mediated schema that serf a the basis for integration we have evaluated the system on a suite of test query in a realistic application to verify the practicality of our approach 
amazon is a perfect information board game with simple rule and large branching factor two player alternately move chess queen like piece and block square on a playing field the player who make the last move win amazon endgame usually decompose into independent subgames therefore the game is a natural testbed for combinatorial game theory it wa known that determining the winner of simple generalized amazon endgame is np equivalent this paper present two proof for the pspace completeness of the generalized version of the full game 
bayesian network classifier bnc have received considerable attention in machine learning field some special structure bncs have been proposed and demonstrate promise performance however recent research show that structure learning in bns may lead to a non negligible posterior problem i e there might be many structure have similar posterior score in this paper we propose a generalized additive bayesian network classifier which transfer the structure learning problem to a generalized additive model gam learning problem we first generate a series of very simple bns and put them in the framework of gam then adopt a gradient based algorithm to learn the combining parameter and thus construct a more powerful classifier on a large suite of benchmark data set the proposed approach outperforms many traditional bncs such a naive bayes tan etc and achieves comparable or better performance in comparison to boosted bayesian network classifier 
in recent year there ha been substantial research on exploring how ai can contribute to human computer interaction by enabling an interface to understand a user s need and act accordingly understanding user need is especially challenging when it involves assessing the user s high level mental state not easily reflected by interface action in this paper we present our result on using eye tracking data to model such mental state during interaction with adaptive educational software we then discus the implication of our research for intelligent user interface 
sequence alignment is a common subtask in many application such a genetic matching and music information retrieval crucial to the performance of any sequence alignment algorithm is an accurate model of the reward of transforming one sequence into another using this model we can find the optimal alignment of two sequence or perform query based selection from a database of target sequence with a dynamic programming approach in this paper we describe a new algorithm to learn the reward model from positive and negative example of matching sequence we develop a gradient boosting approach that reduces sequence learning to a series of standard function approximation problem that can be solved by any function approximator a key advantage of this approach is that it is able to induce complex feature using function approximation rather than relying on the user to predefine such feature our experiment on synthetic data and a fairly complex real world music retrieval domain demonstrate that our approach can achieve better accuracy and faster learning compared to a state of the art structured svm approach 
we first provide a mapping from pearce s equilibrium logic and ferraris s general logic program to lin and shoham s logic of knowledge and justified assumption a nonmonotonic modal logic that ha been shown to include a special case both reiter s default logic in the propositional case and moore s autoepistemic logic from this mapping we obtain a mapping from general logic program to circumscription both in the propositional and first order case furthermore we show that this mapping can be used to check the strong equivalence between two propositional logic program in classical logic 
this research focus on the development of a machine learning technique based on time delay neural network tdnn and independent component analysis ica to analyze eeg signal dynamic related to the initiation and propagation of epileptic seizure we aim at designing a generative model to simulate eeg time series after alteration of specific localized channel electrode in order to explore the effect of brain surgery ex vivo 
by introducing the concept of a loop and a loop formula lin and zhao showed that the answer set of a nondisjunctive logic program are exactly the model of it clark s completion that satisfy the loop formula of all loop recently gebser and schaub showed that the lin zhao theorem remains correct even if we restrict loop formula to a special class of loop called elementary loop in this paper we simplify and generalize the notion of an elementary loop and clarify it role we propose the notion of an elementary set which is almost equivalent to the notion of an elementary loop for nondisjunctive program but is simpler and unli ke elementary loop can be extended to disjunctive program without producing unintuitive result we show that the maximal unfounded elementary set for the relevant part of a program are exactly the minimal set among the nonempty unfounded set we also present a graph theoretic characterization of elementary set for nondisjunctive program which is simpler than the one proposed in gebser schaub unlike the case of nondisjunctive program we show that the problem of deciding an elementary set is conp complete for disjunctive program 
term weighting system are of crucial importance in information extraction and information retrieval application common approach to term weighting are based either on statistical or on natural language analysis in this paper we present a new algorithm that capitalizes from the advantage of both the strategy by adopting a machine learning approach in the proposed method the weight are computed by a parametric function called context function that model the semantic influence exercised amongst the term of the same context the context function is learned from example allowing the use of statistical and linguistic information at the same time the novel algorithm wa successfully tested on crossword clue which represent a case of single word question answering 
much of the work on opponent modeling for game tree search ha been unsuccessful in two player zero sum game the gain from opponent modeling are often outweighed by the cost of modeling opponent modeling solution simply cannot search a deep a the highly optimized minimax search with alpha beta pruning recent work ha begun to look at the need for opponent modeling in n player or general sum game we introduce a probabilistic approach to opponent modeling in n player game called prob maxn which can robustly adapt to unknown opponent we implement prob maxn in the game of spade showing that prob maxn is highly effective in practice beating out the maxn and soft max n algorithm when faced with unknown opponent 
although epistemic logic programming ha an enhanced capacity to handle complex incomplete information reasoning and represent agent epistemic behaviour it embeds a significantly higher computational complexity than non disjunctive and disjunctive answer set programming in this paper we investigate some important property of epistemic logic program in particular we show that lee and lifschitz s result on loop formula for disjunctive logic program can be extended to a special class of epistemic logic program we also study the polysize model property for epistemic logic program based on these discovery we identify two non trivial class of epistemic logic program whose consistency checking complexity is reduced from pspace complete to np complete and p complete respectively we observe that many important application on epistemic representation fall into these two class of epistemic logic program 
when deciding what to do agent must choose among alternative action and different agent may make different choice according to what they wish to achieve in the light of their preference and value it cannot be assumed however that agent have a conscious understanding of their value preference independent of the reasoning situation in which they engage in this paper we consider an extension to a generic framework for reasoning about argument justifying action in term of value in which the preference amongst value emerge from the reasoning process 
we present new result on the efficiency of no regret algorithmsin the context of multiagent learning we use a known approach to augment a large class of no regret algorithm to allow stochastic sampling of action and observation of scalar reward of only the action played we show that the average actual payoff of the resulting learner get close to the best response against eventually stationary opponent close to the asymptotic optimal payoff against opponent that playa converging sequence of policy and close to at least a dynamic variant of minimax payoff against arbitrary opponent with a high probability in polynomial time in addition the polynomial bound are shown to be significantly better than previously known bound furthermore we do not need to assume that the learner know the game matrix and can observe the opponent action unlike previous work 
we study a monotone np decision problem the dominating clique problem whose phase transition occurs at a very dense stage of the random graph evolution process we establish the exact threshold of the phase transition and propose an efficient search algorithm that run in super polynomial time with high probability our empirical study reveal two even more intriguing phenomenon in it typical case complexity the problem is uniformly hard with a tiny runtime variance on negative instance our algorithm and it cnf tailored implementation outperform several sat solver by a huge margin on dominating clique and some other sat problem with similar structure 
in recent year informal online communication ha transformed the way in which we connect and collaborate with friend and colleague with million of individual communicating online each day we have a unique opportunity to observe the formation and evolution of role and relationship in networked group and organization yet a number of challenge arise when attempting to infer the underlying social network from data that is often ambiguous incomplete and context dependent in this paper we consider the problem of collaborative network discovery from domain such a intelligence analysis and litigation support where the analyst is attempting to construct a validated representation of the social network we specifically address the challenge of relationship identification where the objective is to identify relevant communication that substantiate a given social relationship type we propose a supervised ranking approach to the problem and ass it performance on a manager subordinate relationship identification task using the enron email corpus by exploiting message content the ranker routinely cue the analyst to relevant communication relationship and message traffic that are indicative of the social relationship 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
statistical relational learning srl construct probabilistic model from relational database a key capability of srl is the learning of arc in the bayes net sense connecting entry in different row of a relational table or in different table nevertheless srl approach currently are constrained to use the existing database schema for many database application user find it profitable to define alternative view of the database in effect defining new field or table such new field or table can also be highly useful in learning we provide srl with the capability of learning new view 
l regularized logistic regression is now a workhorse of machine learning it is widely used for many classification problem particularly one with many feature l regularized logistic regression requires solving a convex optimization problem however standard algorithm for solving convex optimization problem do not scale well enough to handle the large datasets encountered in many practical setting in this paper we propose an efficient algorithm for l regularized logistic regression our algorithm iteratively approximates the objective function by a quadratic approximation at the current point while maintaining the l constraint in each iteration it us the efficient lars least angle regression algorithm to solve the resulting l constrained quadratic optimization problem our theoretical result show that our algorithm is guaranteed to converge to the global optimum our experiment show that our algorithm significantly outperforms standard algorithm for solving convex optimization problem moreover our algorithm outperforms four previously published algorithm that were specifically designed to solve the l regularized logistic regression problem 
teaching robotics to undergraduate student requires a course framework that allows student to learn about robotics in stage without being overwhelmed with detail such a framework must also provide the student with a motivating application environment that challenge them to apply what they have learned robotics competition have proven to be an excellent method for motivating student so the framework should be portable and robust enough to be used for competition and flexible enough to provide a range of environment that can become more challenging a student become more adept finally the framework should provide repeatability and control for evaluating the student s work a well a for performing research in this paper we overview a mixed reality approach that meet these criterion and describe it use in an advanced undergraduate course 
we derive a recursive formula for expected utility value in imperfectinformation game tree and an imperfect information game tree search algorithm based on it the formula and algorithm are general enough to incorporate a wide variety of opponent model we analyze two opponent model the paranoid model is an information set analog of the minimax rule used in perfect information game the overconfident model assumes the opponent move randomly our experimental test in the game of kriegspiel chess an imperfect information variant of chess produced surprising result against each other and against one of the kriegspiel algorithm presented at ijcai the overconfident model usually outperformed the paranoid model the performance of both model depended greatly on how well the model corresponded to the opponent s behavior these result suggest that the usual assumption of perfect information game tree search that the opponent will choose the best possible move isn t a useful in imperfect information game 
recently the problem of learning volumetric map from three dimenisional range data ha become quite popular in the context of mobile robotics one of the key challenge in this context is to reduce the overall amount of data the smaller the namber of data point however the fewer information is available to register the scan and to conputer a consistent map in this paper we present a novel approach that estimate global constaints from the data and utilizes these contraints to improve the registration process in our current system we simultaneously minimize the distance between scan and the distance of edge from plane extracted from the edge to obtain highly accurate three dimensional modele of the environment several experiment carried out in simulation a well a with three dimensional data obtained with a mobile robot in an outdoor environment we show that our approach yield seriously more accurate model compared to a standard apporach that doe not utilize the global constraint 
the mobile internet is a massive opportunity for mobile operator and content provider but despite significant improvement in handset infrastructure content and charging model mobile user are still struggling to access and locate relevant content and service the core of this so called content discovery problem is the navigation effort that user must invest in browsing and searching for mobile content in this paper we describe one successfully deployed solution which us personalization technology to profile subscriber interest in order to automatically adapt mobile portal to their learned preference we present summary result from our deployment experience with more than mobile operator and million of subscriber around the world which demonstrate how this solution can have a significant impact on portal usability subscriber usage and mobile operator revenue 
we present our work on using statistical corpus based machine learning technique to simultaneously recognize an agent s current goal schema at various level of a hierarchical plan our recognizer is based on a novel type of graphical model a cascading hidden markov model which allows the algorithm to do exact inference and make prediction at each level of the hierarchy in time quadratic to the number of possible goal schema we also report result of our recognizer s performance on a plan corpus 
back of the envelope bote reasoning involves generating quantitative answer in situation where exact data and model are unavailable and where available data is often incomplete and or inconsistent a rough estimate generated quickly is more valuable and useful than a detailed analysis which might be unnecessary impractical or impossible because the situation doe not provide enough time information or other resource to perform one such reasoning is a key component of commonsense reasoning about everyday physical situation we present an implemented system bote solver that can solve about a dozen estimation question like what is the annual cost of healthcare in usa from different domain using a library of strategy and the cyc knowledge base bote solver is a general purpose problem solving framework that us strategy represented a suggestion and keep track of problem solving progress in an and or tree a key contribution of this paper is a knowledge level analysis newell of the strategic knowledge used in bote reasoning we present a core collection of seven powerful estimation strategy that provides broad coverage for such problem solving we hypothesize that this is the complete set of back of the envelope problem solving strategy we present twofold support for this hypothesis an empirical analysis of all problem n on force and pressure rotation and mechanic heat and astronomy from clifford swartz s back of the envelope physic swartz and an analysis of strategy used by bote solver 
automating the task of scoring short handwritten student essay is considered the goal is to assign score which are comparable to those of human scorer by coupling two ai technology optical handwriting recognition and automated essay scoring the test bed is that of essay written by child in reading comprehension test the process involves several image level operation removal of pre printed matter segmentation of handwritten text line and extraction of word recognition constraint are provided by the reading passage the question and the answer rubric scoring is based on using a vector space model and machine learning of parameter from a set of human scored sample system performance is comparable to that of scoring based on perfect manual transcription 
we investigate an environment where self interested agent have to find high quality service resource agent have common knowledge about resource which are able to provide these service the performance of resource is measured by the satisfaction obtained by agent using them the performance of a resource depends on it intrinsic capability and it current load we use a satisfying rather than an optimizing framework where agent are content to receive service quality above a threshold we introduce a formal framework to characterize the convergence of agent to a state where each agent is satisfied with the performance of the service it is currently using we analyzed the convergence behavior of such a system and identified a mechanism to speed up convergence 
forgetting irrelevant problematic action in a domain description can be useful in solving reasoning problem such a query answering planning conftict resolution prediction postdiction etc motivated by such application we study what forgetting is how forgetting can be done and for which application forgetting can be useful and how in the context of reasoning about action we study these question in the action language c a formalism based on causal explanation and relate it to forgetting in classical logic and logic programming 
many problem in ai are simplified by clever representation of sensory or symbolic input how to discover such representation automatically from large amount of unlabeled data remains a fundamental challenge the goal of statistical method for dimensionality reduction is to detect and discover low dimensional structure in high dimensional data in this paper we review a recently proposed algorithm maximum variance unfolding for learning faithful low dimensional representation of high dimensional data the algorithm relies on modem tool in convex optimization that are proving increasingly useful in many area of machine learning 
depth estimation in computer vision and robotics is most commonly done via stereo vision stereopsis in which image from two camera are used to triangulate and estimate distance however there are also numerous monocular visual cue such a texture variation and gradient defocus color haze etc that have heretofore been little exploited in such system some of these cue apply even in region without texture where stereo would work poorly in this paper we apply a markov random field mrf learning algorithm to capture some of these monocular cue and incorporate them into a stereo system we show that by adding monocular cue to stereo triangulation one we obtain significantly more accurate depth estimate than is possible using either monocular or stereo cue alone this hold true for a large variety of environment including both indoor environment and unstructured outdoor environment containing tree forest building etc our approach is general and applies to incorporating monocular cue together with any off the shelf stereo system 
this paper describes an obstacle recognition system based on svm and vision the basic component of the detected object are first located in the image and then combined with a svm based classifier a distributed learning approach is proposed in order to better deal with object variability illumination condition partial occlusion and rotation a large database containing thousand of object example extracted from real road image ha been created for learning purpose we present and discus the result achieved up to date 
a traditional goal of artificial intelligence research ha been a system that can read unrestricted natural language text on a given topic build a model of that topic and reason over the model natural language processing advance in syntax and semantics have made it possible to extract a limited form of meaning from sentence knowledge representation research ha shown that it is possible to model and reason over topic in interesting area of human knowledge it is useful for these two community to reunite periodically to see where we stand with respect to the common goal of text understanding in this paper we describe a coordinated effort among researcher from the natural language and knowledge representation and reasoning community we routed the output of existing nl software into existing kr software to extract knowledge from text for integration with engineered knowledge base we tested the system on a suite of roughly small english text about the form and function of the human heart a well a a handful of confuser text from other domain we then manually evaluated the knowledge extracted from novel text our conclusion is that the technology from these field is mature enough to start producing unified machine reading system the result of our exercise provide a performance baseline for system attempting to acquire model from text 
machine learned classifier are important component of many data mining and knowledge discovery system in several application domain an explanation of the classifier s reasoning is critical for the classifier s acceptance by the end user we describe a framework explaind for explaining decision made by classifier that use additive evidence explaind applies to many widely used classifier including linear discriminants and many additive model we demonstrate our explaind framework using implementation of na ve bayes linear support vector machine and logistic regression classifier on example application explaind us a simple graphical explanation of the classification process to provide visualization of the classifier decision visualization of the evidence for those decision the capability to speculate on the effect of change to the data and the capability wherever possible to drill down and audit the source of the evidence we demonstrate the effectiveness of explaind in the context of a deployed web based system proteome analyst and using a downloadable python based implementation 
thanks to recent advance ai planning ha become the underlying technique for several application amongst these a prominent one is automated web service composition wsc one important issue in this context ha been hardly addressed so far wsc requires dealing with background ontology the support for those is severely limited in current planning tool we introduce a planning formalism that faithfully represents wsc we show that unsurprisingly planning in such a formalism is very hard we then identify an interesting special case that cover many relevant wsc scenario and where the semantics are simpler and easier to deal with this open the way to the development of effective support tool for wsc furthermore we show that if one additionally limit the amount and form of output that can be generated then the set of possible state becomes static and can be modelled in term of a standard notion of initial state uncertainty for this effective tool exist these can realize scalable wsc with powerful background ontology in an initial experiment we show how scaling wsc instance are comfortably solved by a tool incorporating modern planning heuristic 
in this paper we define a general framework for activity recognition by building upon and extending relational markov network using the example of activity recognition from location data we show that our model can represent a variety of feature including temporal information such a time of day spatial information extracted from geographic database and global constraint such a the number of home or workplace of a person we develop an efficient inference and learning technique based on mcmc using gps location data collected by multiple people we show that the technique can accurately label a person s activity location furthermore we show that it is possible to learn good model from le data by using prior extracted from other people s data 
lrta is a real time heuristic search algorithm widely used in each iteration it update the heuristic estimate of the current state in this paper we present three version of lrta k a new lrta based algorithm that is able to update the heuristic estimate of up to k state not necessarily distinct based on bounded propagation this updating strategy maintains heuristic admissibility so lrta k keep the good theoretical property of lrta the new algorithm produce better solution in the first trial and converges faster when compared with other state of the art algorithm on classical benchmark for real time search we provide experimental evidence of the improvement in performance of these version at the extra cost of longer planning step 
this paper present a distributed knowledge representation and data fusion system designed for highly integrated ambient intelligence application the architecture based on the idea of an ecosystem of interacting artificial entity is a framework for collaborating agent to perform an intelligent multi sensor data fusion in particular we focus on the cognitive layer leading the overall data acquisition process the approach ha been thoroughly tested in simulation and part of it ha been already exploited in successful application 
model of dynamical system based on predictive state representation psrs use prediction of future observation a their representation of state a main departure from traditional model such a partially observable markov decision process pomdps is that the psr model state is composed entirely of observable quantity psrs have recently been extended to a class of model called memory psrs mpsrs that use both memory of past observation and prediction of future observation in their state representation thus mpsrs preserve the psr property of the state being composed of observable quantity while potentially revealing structure in the dynamical system that is not exploited in psrs in this paper we demonstrate that the structure captured by mpsrs can be exploited quite naturally for stochastic planning based on value iteration algorithm in particular we adapt the incremental pruning ip algorithm defined for planning in pomdps to mpsrs our empirical result show that our modified ip on mpsrs outperforms in most case ip on both psrs and pomdps 
we present an enhanced direct linear discriminant analysis edlda solution to effectively and efficiently extract discriminatory feature from high dimensional data the edlda integrates two type of class wise weighting term in estimating the average within class and between class scatter matrix in order to relate the resulting fisher criterion more closely to the minimization of classification error furthermore the extracted discriminant feature are weighted by mutual information between feature and class label experimental result on four biometric datasets demonstrate the promising performance of the proposed method 
in auction theory agent are typically presumed to have perfect knowledge of their valuation in practice though they may face barrier to this knowledge due to transaction cost or bounded rationality modeling and analyzing such setting ha been the focus of much recent work though a canonical model of such domain ha not yet emerged we begin by proposing a taxonomy of auction model with valuation uncertainty and showing how it categorizes previous work we then restrict ourselves to single good sealed bid auction in which agent have uncertain independent private value and can introspect about their own but not others valuation through possibly costly and imperfect query we investigate second price auction performing equilibrium analysis for case with both discrete and continuous valuation distribution we identify case where every equilibrium involves either randomized or asymmetric introspection we contrast the revenue property of different equilibrium discus step the seller can take to improve revenue and identify a form of revenue equivalence across mechanism 
belief change is concerned with modelling the way in which a rational reasoner maintains it belief a it acquires new information of particular interest is the way in which new belief are acquired and determined and old belief are retained or discarded a parallel can be drawn to symbolic machine learning approach where example to be categorised are presented to the learning system and a theory is subsequently derived usually over a number of iteration it is therefore not surprising that the term theory revision is used to describe this process ourston and mooney viewing a machine learning system a a rational reasoner allows u to begin seeing these seemingly disparate mechanism in a similar light in this paper we are concerned with characterising the well known inverse resolution operation muggleton and more recently inverse entailment muggleton a agm style belief change operation in particular our account is based on the abductive expansion operation pagnucco et al pagnucco and characterised by using the notion of epistemic entrenchment g rdenfors and makinson extended for this operation this work provides a basis for reconciling work in symbolic machine learning and belief revision moreover it allows machine learning technique to be understood a form of nonmonotonic reasoning 
we introduce a new data mining problem redescription mining that unifies consideration of conceptual clustering constructive induction and logical formula discovery redescription mining begin with a collection of set view it a a propositional vocabulary and identifies cluster of data that can be defined in at least two way using this vocabulary the primary contribution of this paper are conceptual and theoretical i we formally study the space of redescriptions underlying a dataset and characterize their intrinsic structure ii we identify impossibility a well a strong possibility result about when mining redescriptions is feasible iii we present several scenario of how we can custom build redescription mining solution for various bias and iv we outline how many problem studied in the larger machine learning community are really special case of redescription mining by highlighting it broad scope and relevance we aim to establish the importance of redescription mining and make the case for a thrust in this new line of research 
we propose using domain independent task decomposition technique for situation in which case are the sole or the main source for domain knowledge our work is motivated by project planning domain where hierarchical case are readily available but neither a planning domain theory nor case adaptation knowledge is available we present dincad domain independent system for case based task decomposition a system that encompasses case retrieval refinement and reuse following from the idea of reusing generalized case to solve new problem dincad consists of a case refinement procedure that reduces case over generalization and a similarity criterion that take advantage of the refinement to improve case retrieval precision we will analyze the property of the system and present an empirical evaluation 
we consider a modified version of the situation calculus built using a two variable fragment of the first order logic extended with counting quantifier we mention several additional group of axiom that can be introduced to capture taxonomic reasoning we show that the regression operator in this framework can be defined similarly to regression in the reiter s version of the situation calculus using this new regression operator we show that the projection and executability problem are decidable in the modified version even if an initial knowledge base is incomplete and open for an incomplete knowledge base and for context dependent action we consider a type of progression that is sound with respect to the classical progression we show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows action with local effect only we mention possible application to formalization of semantic web service 
we propose a case based reasoning approach for action selection in the robot soccer domain presented in the th european conference on case based reasoning based on the current state of a game the robot retrieve the most similar past situation and then the team reproduces the sequence of action performed in that occasion in this domain we have to deal with all the difficulty that a real environment involves 
multi label learning deal with ambiguous example each may belong to several concept class simultaneously in this learning framework the inherent ambiguity of each example is explicitly expressed in the output space by being associated with multiple class label while on the other hand it ambiguity is only implicitly encoded in the input space by being represented by only a single instance based on this recognition we hypothesize that if the inherent ambiguity can be explicitly expressed in the input space appropriately the problem of multi label learning can be solved more effectively we justify this hypothesis by proposing a novel multi label learning approach named in dif the core of insdif is instance differentiation that transforms an example into a bag of instance each of which reflects the example s relationship with one of the possible class in this way insdif directly address the inherent ambiguity of each example in the input space a two level classification strategy is employed to learn from the transformed example application to automatic web page categorization natural scene classification and gene functional analysis show that our approach outperforms several well established multi label learning algorithm 
in this paper we propose a framework for decentralized model based diagnosis of complex system modeled with qualitative constraint and whose model are distributed among their subsystem we assume that local diagnosers are associated with subsystem and are coordinated by a supervisor which act a the diagnoser for the complex system the local diagnosers and the supervisor communicate via a standard interface and share a common modeling ontology in this diagnostic architecture connection between subsystem only need to be known at runtime thus allowing for dynamic re configuration of the system the approach is designed to compute partial hypothesis in order to avoid unnecessary query to local diagnosers 
we introduce a weakening of standard game theoretic dominance condition called dominance which enables more aggressive pruning of candidate strategy at the cost of solution accuracy equilibrium of a game obtained by eliminating a dominated strategy are guaranteed to be approximate equilibrium of the original game with degree of approximation bounded by the dominance parameter we can apply elimination of dominated strategy iteratively but the for which a strategy may be eliminated depends on prior elimination we discus implication of this order independence and propose greedy heuristic for determining a sequence of elimination to reduce the game a far a possible while keeping down cost a case study analysis of an empirical player game serf to illustrate the technique and demonstrate the utility of weaker than weak dominance pruning 
protein fold recognition is a crucial step in inferring biological structure and function this paper focus on machine learning method for predicting quaternary structural fold which consist of multiple protein chain that form chemical bond among side chain to reach a structurally stable domain the complexity associated with modeling the quaternary fold pose major theoretical and computational challenge to current machine learning method we propose method to address these challenge and show how domain knowledge is encoded and utilized to characterize structural property using segmentation conditional graphical model and model complexity is handled through efficient inference algorithm our model follows a discriminative approach so that any informative feature such a those representative of overlapping or long range interaction can be used conveniently the model is applied to predict two important quaternary fold the triple spiral and double barrel trimer cross family validation show that our method outperforms other state of the art algorithm 
this paper deal with the problem of identifying direct causal effect in recursive linear structural equation model the paper provides a procedure for solving the identification problem in a special class of model 
the mean running time of a la vega algorithm can often be dramatically reduced by periodically restarting it with a fresh random seed the optimal restart schedule depends on the la vega algorithm s run length distribution which in general is not known in advance and may differ across problem instance we consider the problem of selecting a single restart schedule to use in solving each instance in a set of instance we present offline algorithm for computing an approximately optimal restart schedule given knowledge of each instance s run length distribution generalization bound for learning a restart schedule from training data and online algorithm for selecting a restart schedule adaptively a new problem instance are encountered 
it is well known that in unidentifiable model the bayes estimation ha the advantage of generalization performance to the maximum likelihood estimation however accurate approximation of the posterior distribution requires huge computational cost in this paper we consider an empirical bayes approach where a part of the parameter are regarded a hyperparameters which we call a subspace bayes approach and theoretically analyze the generalization error of three layer linear neural network we show that a subspace bayes approach is asymptotically equivalent to a positivepart james stein type shrinkage estimation and behaves similarly to the bayes estimation in typical case 
the human visual system can interpret two dimensional d line drawing like the necker cube a three dimensional d wire frame we focus attention on a principle to minimize the entropy of angle distribution between line segment in a d wire frame a a concrete definition of the law of pragnanz in gestalt psychology and we implement the principle with the perceptual preference of planarity to the loop of wire frame using a genetic algorithm experimental result show the good coincidence with human perception 
counterfactual quantity representing path specific effect arise in case where we are interested in computing the effect of one variable on another only along certain causal path in the graph in other word by excluding a set of edge from consideration a recent paper pearl detail a method by which such an exclusion can be specified formally by fixing the value of the parent node of each excluded edge in this paper we derive simple graphical condition for experimental identifiability of path specific effect namely condition under which path specific effect can be estimated consistently from data obtained from controlled experiment 
this paper present a new distributed solution protocol called dislrp f or thegeneralized mutual assignment problem gmap the gmap is a typical distributed combinatorial optimization problem whose goal is to maximize social welfare of the agent unlike the previous protocol for the gmap dislrp can provide a theoretical guarantee on global solution quality in dislrp a with in the previous protocol the agent repeatedly solve their local problem while coordinating their local solution using a distributed constraint satisfaction technique the key difference is that in dislrp each agent is required to produce a feasible solution whose local objective value is not lower than time the local optimal value our experimental result on benchmark problem instance show that dislrp can certainly find a solution whose global objective value is higher than that theoretically guaranteed furthermore they also show that while spending extra communication and computation cost dislrp can produce a significantly better solution than the previous protocol if we set appropriately 
neighborhood interchangeability ni identifies the equivalent value in the domain of a variable of a constraint satisfaction problem csp by considering only the constraint that directly apply to the variable freuder described an algorithm for efficiently computing ni value in binary csps in this paper we show that the generalization of this algorithm to non binary csps is not straightforward and introduce an efficient algorithm for computing ni value in the presence of non binary constraint further we show how to interleave this mechanism with search for solving csps thus yielding a dynamic bundling strategy while the goal of dynamic bundling is to produce multiple robust solution we empirically show that it doe not increase but significantly decrease the cost of search 
while even strip planner must search for plan of unbounded length temporal planner must also cope with the fact that action may start at any point in time most temporal planner cope with this challenge by restricting action start time to a small set of decision epoch because this enables search to be carried out in state space and leverage powerful state based reachability heuristic originally developed for classical planning indeed decision epoch planner won the international planning competition s temporal planning track in and however decision epoch planner have a largely unrecognized weakness they are incomplete in order to characterize the cause of incompleteness we identify the notion of required concurrency which separate expressive temporal action language from simple one we show that decisionepoch planner are only complete for language in the simpler class and we prove that the simple class is equivalent to strip surprisingly no problem with required concurrency have been included in the planning competition we conclude by designing a complete state space temporal planning algorithm which we hope will be able to achieve high performance by leveraging the heuristic that power decision epoch planner 
active fusion is a process that purposively selects the most informative information from multiple source a well a combine these information for achieving a reliable result efficiently this paper present a general mathematical framework based on influence diagram id for active fusion and timely decision making within this framework an approximation algorithm is proposed to efficiently compute nonmyopic value of information voi for multiple sensory action meanwhile a sensor selection algorithm is proposed to choose optimal sensory action set efficiently both the experiment with synthetic data and real data from a real world application demonstrate that the proposed framework together with the algorithm are well suited to application where the decision must be made efficiently and timely from dynamically available information of diverse and disparate source 
in multi agent system agent need to share information in order to make good decision who doe what in order to achieve this matter a lot the assignment of responsibility influence delay and consequently affect agent ability to make timely decision it is often unclear which approach are best we develop a model where one can easily test the impact of different assignment and information sharing protocol by focusing only on the delay caused by computation and communication using the model we obtain interesting result that provide insight about the type of assignment that perform well in various domain and how slight variation in protocol can make great difference in feasibility 
most work on predictive representation of state psrs ha focused on learning and planning in unstructured domain for example those represented by flat pomdps this paper extends psrs to represent relational knowledge about domain so that they can use policy that generalize across different task capture knowledge that ignores irrelevant attribute of object and represent policy in a way that is independent of the size of the state space using a block world domain we show how generalized prediction about the future can compactly capture relation between object which in turn can be used to naturally specify relational style option and policy because our representation is expressed solely in term of action and observation it ha extensive semantics which are statistic about observable quantity 
this paper present a methodology for designing trading agent for complex game we compute for the first time bayes nash equilibrium for first price single unit auction and mth price multiunit auction when the auction ha a set of possible closing time one of which is chosen randomly for the auction to end at to evaluate this approach we used our analysis to generate strategy for the international trading agent competition one of these wa evaluated a the best overall and wa subsequently used very successfully by our agent whitebear in the competition 
recent research ha focused on bridging the gap between the satisfiability sat and constraint satisfaction problem csp formalism one approach ha been to develop a many valued sat formula mv sat a an intermediate paradigm between sat and csp and then to translate existing highly efficient sat solver to the mv sat domain experimental result have shown this approach can achieve significant improvement in performance compared with the traditional sat and csp approach in this paper we follow a different route developing sat solver that can automatically recognise csp structure hidden in sat encoding this allows u to look more closely at how constraint weighting can be implemented in the sat and csp domain our experimental result show that a sat based approach to handle weight together with csp based approach to variable instantiation is superior to other combination of sat and csp based approach a further experiment on the round robin scheduling problem indicates that this many valued constraint weighting approach outperforms other state of the art solver 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
the earth observing one spacecraft ha been under the control of ai software for several year experimentally since and since november a the primary operation system this software includes model based planning and scheduling procedural execution and event detection software learned by support vector machine svm technique this software ha enabled a x increase in the mission science return per data downlinked and a m year reduction in operation cost in this paper we discus the ai software used the impact of the software and lesson learned with implication for future ai research 
we present an efficient dynamic programming algorithm for synchronous parsing of sentence pair from a parallel corpus with a given word alignment unless there is a large proportion of word without a correspondence in the other language the worstcase complexity is significantly reduced over standard synchronous parsing the theoretical complexity result are corroborated by a quantitative experimental evaluation 
in this paper we present an improved version of the probabilistic ant based clustering algorithm for distributed database pace the most important feature of this algorithm is the formation of numerous zone in different site based on corresponding user query to the distributed database keywords extracted out of the query are used to assign a range of value according to their corresponding probability of occurrence or hit ratio at each site we propose the introduction of weight for individual or group of data item in each zone according to their relevance to the query along with the concept of familial pheromone trail a part of an ant odor identification model to bias the movement of different type of ant towards the member of their own family it performance is compared against pace and other known clustering algorithm for different evaluation measure and an improvement is shown in term of convergence speed and quality of solution obtained 
the goal of my research is to understand speech input in a continuous manner by treating the input stream a fragmental utterance this allows u to use various approach to predict what come downstream possible interpretation are trimmed by such prediction which in turn also allow u to complete information not readily available in the fragmental utterance semantic frame can encode all possible argument for domain action a utterance are processed continuously appropriate frame can be activated so that fragment interpretation can fill correct or extend frame under consideration in turn feedback can be provided to the parser a the frame are manipulated possibly based on the completeness of the semantic frame construction 
we present a new propagator achieving bound consistency for the inter distance constraint this constraint ensures that among a set of variable x xn the difference between two variable is at least p this restriction model in particular scheduling problem in which task require p contiguous unit of a resource to be completed until now the best known propagator for bound consistency had time complexity o n in this work we propose a quadratic propagator for the same level of consistency we then show that this theoretical gain give saving of an order of magnitude in our benchmark of scheduling problem 
in this paper we prove that the well known correspondence between the forward backward algorithm for hidden markov model hmms and belief propagation bp applied to hmms can be generalized to one between bp for junction tree and the generalized inside outside probability computation for probabilistic logic program applied to junction tree 
ai problem such a autonomous robotic exploration automatic diagnosis and activity recognition have in common the need for choosing among a set of informative but possibly expensive observation when monitoring spatial phenomenon with sensor network or mobile robot for example we need to decide which location to observe in order to most effectively decrease the uncertainty at minimum cost these problem usually are np hard many observation selection objective satisfy submodularity an intuitive diminishing return property adding a sensor to a small deployment help more than adding it to a large deployment in this paper we survey recent advance in systematically exploiting this submodularity property to efficiently achieve near optimal observation selection under complex constraint we illustrate the effectiveness of our approach on problem of monitoring environmental phenomenon and water distribution network 
in mobile computing communicative act are not free cost such a power and bandwidth consumption are prominent issue in addition resource vary widely across hardware and operating context agent in these setting must account for these cost and adapt to available capability this poster present a planning optimization formalization of this problem enabling service based agent to reason about and conduct communication using local and network accessible resource 
we define a new heuristic hds for asp and implement it in the disjunctive asp system dlv the new heuristic improves the evaluation of p p hard asp program while maintaining the benign behaviour of the well assessed heuristic of dlv on np problem we experiment with the new heuristic on qbfs hds significantly outperforms the heuristic of dlv on hard qbf problem we compare also the dlv system with the new heuristic hds to three prominent qbf solver the result of the comparison performed on instance used in the last qbf competition indicate that asp system can be faster than qbf system on p p hard problem 
we consider the most realistic reinforcement learning setting in which an agent start in an unknown environment the pomdp and must follow one continuous and uninterrupted chain of experience with no access to reset or offline simulation we provide algorithm for general connected pomdps that obtain near optimal average reward one algorithm we present ha a convergence rate which depends exponentially on a certain horizon time of an optimal policy but ha no dependence on the number of unobservable state the main building block of our algorithm is an implementation of an approximate reset strategy which we show always exists in every pomdp an interesting aspect of our algorithm is how they use this strategy when balancing exploration and exploitation 
a hybrid algorithm is devised to boost the performance of complete search on under constrained problem we suggest to use random variable selection in combination with restarts augmented by a coarse grained local search algorithm that learns favorable value heuristic over the course of several restarts numerical result show that this method can speed up complete search by order of magnitude 
with policy management becoming popular a a mean of providing flexible web security the number of policy language being proposed for the web is constantly increasing we recognize the importance of policy for securing the web and believe that the future will only bring more policy language we do not however believe that user should be forced to conform the description of their policy relationship to a single standard policy language instead there should be a way of encompassing different policy language and supporting heterogeneous policy system a a step in this direction we propose rein a policy framework grounded in semantic web technology which leverage the distributed nature and linkability of the web to provide web based policy management rein provides ontology for describing policy domain in a decentralized manner and provides an engine for reasoning over these description both of which can be used to develop domain and policy language specific security system we describe the rein policy framework and discus how a rein policy management system can be developed for access control in an online photo sharing application 
due to large search space diagnosis of combinational circuit is often practical for finding only single and double fault in principle system model can be compiled into a tractable representation such a dnnf on which fault of arbitrary cardinality can be found efficiently for large circuit however compilation can become a bottleneck due to the large number of variable necessary to model the health of individual gate we propose a novel method that greatly reduces this number allowing the compilation a well a the diagnosis to scale to larger circuit the basic idea is to identify region of a circuit called cone that are dominated by single gate and model the health of each cone with a single health variable when a cone is found to be possibly faulty we diagnose it by again identifying the cone inside it and so on until we reach a base case we show that result combined from these hierarchical session are sound and complete with respect to minimum cardinality diagnosis we implement this method on top of the diagnoser developed by huang and darwiche in and present evidence that it significantly improves the efficiency and scalability of diagnosis on the iscas circuit 
hypertree decomposition is the most general approach in the literature for identifying tractable computation problem encoded a hypergraphs we show how the heuristic branch decomposition approach for ordinary graph of cook and seymour can be used for the heuristic construction of hypertree decomposition 
recently defined resolution calculus for max sat and signed max sat have provided a logical characterization of the solving technique applied by max sat and wcsp solver in this paper we first define a new resolution rule called signed max sat parallel resolution and prove that it is sound and complete for signed max sat second we define a restriction and a generalization of the previous rule called respectively signed max sat i consistency resolution and signed max sat i j consistency resolution these rule have the following property if a wcsp signed encoding is closed under signed max sat i consistency then the wcsp is i consistent and if it is closed under signed max sat i j consistency then the wcsp is i j consistent a new and practical insight derived from the definition of these new rule is that algorithm for enforcing high order consistency should incorporate an efficient and effective component for detecting minimal unsatisfiable core finally we describe an algorithm that applies directional soft consistency with the previous rule 
decentralized decision making under uncertainty ha been shown to be intractable when each agent ha different partial information about the domain thus improving the applicability and scalability of planning algorithm is an important challenge we present the first memory bounded dynamic programming algorithm for finite horizon decentralized pomdps a set of heuristic is used to identify relevant point of the infinitely large belief space using these belief point the algorithm successively selects the best joint policy for each horizon the algorithm is extremely efficient having linear time and space complexity with respect to the horizon length experimental result show that it can handle horizon that are multiple order of magnitude larger than what wa previously possible while achieving the same or better solution quality these result significantly increase the applicability of decentralized decision making technique 
many application of autonomous agent require group to work in tight coordination to be dependable these group must plan carry out and adapt their activity in a way that is robust to failure and uncertainty previous work ha produced contingent plan execution system that provide robustness during their plan extraction phase by choosing between functionally redundant method and during their execution phase by dispatching temporally flexible plan previous contingent execution system use a centralized architecture in which a single agent conduct planning for the entire group this can result in a communication bottleneck at the time when plan activity are passed to the other agent for execution and state information is returned this paper introduces the plan extraction component of a robust distributed executive for contingent plan contingent plan are encoded a temporal plan network tpn which use a non deterministic choice operator to compose temporally flexible plan fragment into a nested hierarchy of contingency to execute a tpn the tpn is first distributed over multiple agent by creating a hierarchical ad hoc network and by mapping the tpn onto this hierarchy second candidate plan are extracted from the tpn using a distributed parallel algorithm that exploit the structure of the tpn third the temporal consistency of each candidate plan is tested using a distributed bellman ford algorithm each stage of plan extraction distributes communication to adjacent agent in the tpn and in so doing eliminates communication bottleneck in addition the distributed algorithm reduces the computational load on each agent the algorithm is empirically validated on a range of randomly generated contingent plan 
the task of learning model for many real world problem requires incorporating domain knowledge into learning algorithm to enable accurate learning from a realistic volume of training data domain knowledge can come in many form for example expert knowledge about the relevance of variable relative to a certain problem can help perform better feature selection domain knowledge about the conditional independence relationship among variable can help learning of the bayesian network structure this paper considers a different type of domain knowledge for constraining parameter estimate when learning bayesian network in particular we consider domain knowledge that come in the form of inequality constraint among subset of parameter in a bayesian network with known structure these parameter constraint are incorporated into learning procedure for bayesian network by formulating this task a a constrained optimization problem the main contribution of this paper is the derivation of closed form maximum likelihood parameter estimator in the above setting 
we propose a novel scheme for function based classification of object in d image the classification process call for constructing a generic multi level hierarchical description of object class in term of functional component functionality is derived from a large set of geometric attribute and relationship between object part initially the input range data describing each object instance is segmented each object part is labeled a one of a few possible primitive and each group of primitive part is tagged by a functional symbol connection between primitive part and functional part at the same level in the hierarchy are labeled a well then the generic multi level hierarchical description of object class is built using the functionality of a number of object instance during classification a search through a finite graph using a probabilistic fitness measure is performed to find the best assignment of object part to the functional structure of each class an object is assigned to a class providing the highest fitness value the scheme doe not require a priori knowledge about any class we tested the proposed scheme on a database of about one thousand different d object the result show high accuracy in classification 
we study the backbone and the backdoor of propositional satisfiability problem we make a number of theoretical algorithmic and experimental contribution from a theoretical perspective we prove that backbone are hard even to approximate from an algorithmic perspective we present a number of different procedure for computing backdoor from an empirical perspective we study the correlation between being in the backbone and in a backdoor experiment show that there tends to be very little overlap between backbone and backdoor we also study problem hardness for the davis putnam procedure problem hardness appears to be correlated with the size of strong backdoor and weakly correlated with the size of the backbone but doe not appear to be correlated to the size of weak backdoor nor their number finally to isolate the effect of backdoor we look at problem with no backbone 
agent modelling is a challenging problem in many modern artificial intelligence application the agent modelling task is especially difficult when handling stochastic choice deliberately hidden information dynamic agent and the need for fast learning state estimation technique such a kalman filtering and particle filtering have addressed many of these challenge but have received little attention in the agent modelling literature this paper look at the use of particle filtering for modelling a dynamic opponent in kuhn poker a simplified version of texas hold em poker we demonstrate effective modelling both against static opponent a well a dynamic opponent when the dynamic are known we then examine an application of rao blackwellized particle filtering for doing dual estimation inferring both the opponent s state a well a a model of it dynamic finally we examine the robustness of the approach to incorrect belief about the opponent and compare it to previous work on opponent modelling in kuhn poker 
we explore the connection between machine learning and human learning in one form of semi supervised classification human subject completed a novel class categorization task in which they were first taught to categorize a single labeled example from each category and subsequently were asked to categorize without feedback a large set of additional item stimulus were visually complex and unrecognizable shape the unlabeled example were sampled from a bimodal distribution with mode appearing either to the left left shift condition or right right shift condition of the two labeled example result showed that although initial decision boundary were near the middle of the two labeled example after exposure to the unlabeled example they shifted in different direction in the two group in this respect the human behavior conformed well to the prediction of a gaussian mixture model for semi supervised learning the human behavior differed from model prediction in other interesting respect suggesting some fruitful avenue for future inquiry 
this paper present a novel form of dynamically constructed bayes net developed for multi domain sketch recognition our sketch recognition engine integrates shape information and domain knowledge to improve recognition accuracy across a variety of domain using an extendible hierarchical approach our bayes net framework integrates the influence of stroke data and domain specific context in recognition enabling our recognition engine to handle noisy input we illustrate this behavior with qualitative and quantitative result in two domain hand drawn family tree and circuit 
sensor that know their location from microphone to vibration sensor can support a wider arena of application than their location unaware counterpart we offer a method for sensor to determine their own location relative to one another by using only exogenous sound and the difference in the arrival of these sound at different sensor we present a distributed and computationally efficient solution that offer accuracy on par with more active and computationally intense method 
the weighted csp framework is a soft constraint framework with a wide range of application most current state of the art complete solver can be described a a basic depth first branch and bound search that maintain some form of arc consistency during the search in this paper we introduce a new stronger form of arc consistency that we call existential directional arc consistency and we provide an algorithm to enforce it the efficiency of the algorithm is empirically demonstrated in a variety of domain 
we propose a novel approach to self regenerating system which require continuous operation such a security surveillance for that aim we introduce hades a self regenerating cooperative multi agent system with local monitoring when agent of hades find local failure they repair them however in extreme case repair may not be possible and irregular aggressive agent will multiply these irregular agent may use all of the system s resource and thus take over the system to optimize system longevity we identify protocol for killing these irregular agent our primary contribution is a double communication protocol of alert and death signal among the agent making the multi agent system robust to failure and attack 
communication among participant agent robot is central to an appearance of collective ai in this work we deal with the development of local communication mechanism for real microrobotic swarm we demonstrate that despite of very limited capability of the microrobot the specific construction of communication hardware and software allows very extended collective capability of the whole swarm we propose mechanism providing information content and context for collective navigation coordination and spatial perception in a group of microrobots 
coordinating a team of autonomous agent is one of the major challenge in building effective multiagcnt system many technique have been devised for this problem and coordinated teamwork ha been demonstrated even in highly dynamic and adversarial environment a key assumption of these technique though is that the team member are developed together a a whole in many multi agent scenario this assumption is violated we study the problem of coordination in impromptu team where a team is composed of independent agent each unknown to the others the team member have their own skill model strategy and coordination mechanism and no external organization is imposed upon them in particular we propose two technique one adaptive and one predictive for coordinating a single agent that join an unknown team of existing agent we experimentally evaluate these mechanism in the robot soccer domain while introducing useful baseline for evaluating the performance of impromptu team we show some encouraging success while demonstrating this is a very fertile area of research 
chemist often use hand drawn structural diagram to capture and communicate idea about organic compound however the software available today for specifying these structure to a computer relies on a traditional mouse and keyboard interface and a a result lack the ease of use naturalness and speed of drawing on paper in response we have developed a novel sketch based system capable of interpreting hand drawn organic chemistry diagram allowing user to draw molecule with a pen based input device in much the same way that they would on paper the system s ability to interpret a sketch is based on knowledge about both chemistry and chemical drawing convention the system employ a trainable symbol recognizer incorporating both feature based and image based method to locate and identify symbol in the sketch analysis of the spatial context around each symbol allows the system to choose among competing interpretation and determine an initial structure for the molecule finally knowledge of chemistry in particular chemical valence enables the system to check the validity of it interpretation and when necessary refine it to recover from inconsistency we demonstrate that the system is capable of recognizing diagram of common organic molecule and show that using domain knowledge produce a noticeable improvement in recognition accuracy 
a artificial intelligence ai system and behavior model in military simulation become increasingly complex it ha been difficult for user to understand the activity of computer controlled entity prototype explanation system have been added to simulator but designer have not heeded the lesson learned from work in explaining expert system behavior these new explanation system are not modular and not portable they are tied to a particular ai system in this paper we present a modular and generic architecture for explaining the behavior of simulated entity we describe it application to the virtual human a simulation designed to teach soft skill such a negotiation and cultural awareness 
the ability to accurately detect the location of a mobile node in a sensor network is important for many artificial intelligence ai task that range from robotics to context aware computing many previous approach to the location estimation problem assume the availability of calibrated data however to obtain such data requires great effort in this paper we present a manifold regularization approach known a leman to calibration effort reduction for tracking a mobile node in a wireless sensor network we compute a subspace mapping function between the signal space and the physical space by using a small amount of labeled data and a large amount of unlabeled data this mapping function can be used online to determine the location of mobile node in a sensor network based on the signal received we use crossbow mica to setup the network and usb camera array to obtain the ground truth experimental result show that we can achieve a higher accuracy with much le calibration effort a compared to several previous system 
recent application of plan recognition face several open challenge i matching observation to the plan library is costly especially with complex multi featured observation ii computing recognition hypothesis is expensive we present technique for addressing these challenge first we show a novel application of machine learning decision tree to efficiently map multi featured observation to matching plan step second we provide efficient lazy commitment recognition algorithm that avoid enumerating hypothesis with every observation instead only carrying out bookkeeping incrementally the algorithm answer query a to the current state of the agent a well a it history of selected state we provide empirical result demonstrating their efficiency and capability 
in many shortest path problem of practical interest insufficient time is available to find a provably optimal solution one can only hope to achieve a balance between search time and solution cost that respect the user s preference expressed a a utility function over time and cost current stateof the art approach to this problem rely on anytime algorithm such a anytime a or ara these algorithm require the use of extensive training data to compute a termination policy that respect the user s utility function we propose a more direct approach called bugsy that incorporates the utility function directly into the search obviating the need for a separate termination policy experiment in several challenging problem domain including sequence alignment and temporal planning demonstrate that this direct approach can surpass anytime algorithm without requiring expensive performance profiling 
the purpose of this paper is to describe an investigation into an ontology based computational recognition of child s joke while humor ha been studied for century computational humor ha received very little attention this may in part be due to the difficulty of the task at the very least it requires formal method for humor generation recognition and being able to produce interpret natural language being capable of subtle and flexible inference and having a vast store of knowledge about the real world ritchie there are some humor generator see ritchie for a review and a handful of humor recognizers yet if computer are ever going to communicate naturally and effectively with human they must be able to use humor binsted we are interested in recognition not generation of humor recognition of all verbally expressed humor is an overly broad task to narrow the task only joke for young child are considered the reduction of the domain size to young child s joke is expected to decrease the complexity and sophistication of the language to be analyzed this in turn decrease the knowledge that need to be captured for text interpretation 
finding the exact treewidth of a graph is central to many operation in a variety of area including probabilistic reasoning and constraint satisfaction treewidth can be found by searching over the space of vertex elimination order this search space differs from those where best first search is typically applied because a solution path is evaluated by it maximum edge cost instead of the sum of it edge cost we show how to make best first search admissible on max cost problem space we also employ breadth first heuristic search to reduce the memory requirement while still eliminating all duplicate node in the search space our empirical result show that our algorithm find the exact treewidth an order of magnitude faster than the previous state of the art algorithm on hard benchmark graph 
mobile robot map building is the task of generating a model of an environment from sensor data most existing approach to mobile robot mapping either build topological representation or generate accurate metric map of an environment in this paper we introduce relational object map a novel approach to building metric map that represent individual object such a door or wall we show how to extend relational markov network in order to reason about a hierarchy of object and the spatial relationship between them markov chain monte carlo is used for efficient inference and to learn the parameter of the model we show that the spatial constraint modeled by our mapping technique yield drastic improvement for labeling line segment extracted from laser range finder 
givealink org is a social bookmarking site where user may donate and view their personal bookmark file online securely the bookmark are analyzed to build a new generation of intelligent information retrieval technique to recommend search and personalize the web givealink doe not use tag content or link in the submitted web page instead we present a semantic similarity measure for url that take advantage both of the hierarchical structure in the bookmark file of individual user and of collaborative filtering across user in addition we build a recommendation and search engine from ranking algorithm based on popularity and novelty measure extracted from the similarity induced network search result can be personalized using the bookmark submitted by a user we evaluate a subset of the proposed ranking measure by conducting a study with human subject 
concurrent action execution is important for plan length minimization however action specification are often limited to avoid conflict arising from precondition effect interaction pddl the planning domain definition language for example implement the no moving target rule which mean that no two action can simultaneously make use of a value if one of the two is updating the value this rule pose problem for resource allocation planning in which resource value are accessed in precondition and effect a simple example is construction action that consume certain amount of a resource for speeding up plan execution we would like to be able to dispatch several construction action simultaneously because action precondition depend on resource value and action effect change them the no moving target rule doe not allow concurrent execution however if sufficient resource are available executing action simultaneously pose no problem this paper address the problem of deciding whether a set of action produced by a planning system can be executed concurrently in the presence of fluent variable that occur in both action precondition and effect we first motivate the concurrent action execution problem by introducing a fair action scheduling algorithm for real time strategy rts game then we prove that the general decision problem when restricting effect and precondition to polynomial time computation is co np complete thereafter we focus on problem restriction based on commutative operator which allow u to specify sufficient condition for concurrent executability that can be checked quickly if the number of shared fluents is small finally we apply these finding to action execution with shared resource in rts game 
we propose new method of preference elicitation for constraint based optimization problem based on the use of minimax regret specifically we assume a constraintbased optimization problem e g product configuration in which the objective function e g consumer preference are unknown or imprecisely specified assuming a graphical utility model we describe several elicitation strategy that require the user to answer only binary bound query on the utility model parameter while a theoretically motivated algorithm can provably reduce regret quickly in term of number of query we demonstrate that in practice heuristic strategy perform much better and are able to find optimal or near optimal configuration with far fewer query 
coronary heart disease can be diagnosed by measuring and scoring regional motion of the heart wall in ultrasound image of the left ventricle lv of the heart we describe a completely automated and robust technique that detects diseased heart based on detection and automatic tracking of the endocardium and epicardium of the lv the local wall region and the entire heart are then classified a normal or abnormal based on the regional and global lv wall motion in order to leverage structural information about the heart we applied bayesian network to this problem and learned the relation among the wall region off of the data using a structure learning algorithm we checked the validity of the obtained structure using anatomical knowledge of the heart and medical rule a described by doctor the resultant bayesian network classifier depends only on a small subset of numerical feature extracted from dual contour tracked through time and selected using a filter based approach our numerical result confirm that our system is robust and accurate on echocardiogram collected in routine clinical practice at one hospital our system is built to be used in real time 
we introduce our research on learning browsing behavior model for inferring a user s information need corresponding to a set of word based on the action he ha taken during his current web session this information is then used to find relevant page from essentially anywhere on the web the model learned from over one hundred user during a fiveweek user study are session specific but independent of both the user and website our empirical result suggest that these model can identify and satisfy the current information need of user even if they browse previously unseen page containing unfamiliar word 
many problem in nlp require solving a cascade of subtasks traditional pipeline approach yield to error propagation and prohibit joint training decoding between subtasks existing solution to this problem do not guarantee nonviolation of hard constraint imposed by subtasks and thus give rise to inconsistent result especially in case where segmentation task precedes labeling task we present a method that performs joint decoding of separately trained conditional random field crf model while guarding against violation of hard constraint evaluated on chinese word segmentation and part of speech po tagging task our proposed method achieved state of the art performance on both the penn chinese treebank and first sighan bakeoff datasets on both segmentation and po tagging task the proposed method consistently improves over baseline method that do not perform joint decoding 
one important aspect in directing cognitive robot or agent is to formally specify what is expected of them this is often referred to a goal specification temporal logic such a ltl and ctl have been used to specify goal of cognitive robot and agent when their action have deterministic consequence it ha been suggested that in domain where action have non deterministic effect temporal logic may not be able to express many intuitive and useful goal in this paper we first show that this is indeed true with respect to existing temporal logic such a ltl ctl and ctl we then propose the language p ctl which includes the quantifier exist a policy and for all policy we show that this language allows for the specification of richer goal including many intuitive and useful goal mentioned in the literature which cannot be expressed in existing temporal language we generalize our approach of showing the limitation of ctl to develop a framework to compare expressiveness of goal language 
bidding for multi item in simultaneous auction raise challenging problem in multi auction setting the determination of optimal bid by potential buyer requires combinatorial calculation while an optimal bidding strategy is known when bidding in sequential auction only suboptimal strategy are available when bidding for item being sold in simultaneous auction we investigate a multi dimensional bid improvement scheme motivated by optimization technique to derive optimal bid for item bundle in simultaneous auction given a vector of initial bid the proposed scheme systematically improves bid for each item such multi dimensional improvement result in locally optimal bid vector globally optimal bid vector are guaranteed in the limit for infinite restarts for ease of presentation we use two item scenario to explain the working of the algorithm experimental result show polynomial complexity of variant of this algorithm under different type of bidder valuation for item bundle 
both explanation based and inductive learning technique have proven successful in a variety of distributed domain however learning in multi agent system doe not necessarily involve the participation of other agent directly in the inductive process itself instead many system frequently employ multiple instance of induction separately or single agent learning in this paper we present a new framework named the multi agent inductive learning system mail that tightly integrates process of induction between agent the mail framework combine inverse entailment with an epistemic approach to reasoning about knowledge in a multi agent setting facilitating a systematic approach to the sharing of knowledge and invention of predicate when required the benefit of the new approach are demonstrated for inducing declarative program fragment in a multi agent distributed programming system 
general game playing ggp is the art of designing program that are capable of playing previously unknown game of a wide variety by being told nothing but the rule of the game this is in contrast to traditional computer game player like deep blue which are designed for a particular game and can t adapt automatically to modification of the rule let alone play completely different game general game playing is intended to foster the development of integrated cognitive information processing technology in this article we present an approach to general game playing using a novel way of automatically constructing a position evaluation function from a formal game description our system is being tested with a wide range of different game most notably it is the winner of the aaai ggp competition 
although clustering is probably the most frequently used tool for data mining gene expression data existing clustering approach face at least one of the following problem in this domain a huge number of variable gene a compared to the number of sample high noise level the inability to naturally deal with overlapping cluster the instability of the resulting cluster w r t the initialization of the algorithm a well a the difficulty in clustering gene and sample simultaneously in this paper we show that all of these problem can be elegantly dealt with by using nonnegative matrix factorization to cluster gene and sample simultaneously while allowing for bicluster overlap and by employing positive tensor factorization to perform a two way meta clustering of the biclusters produced in several different clustering run thereby addressing the above mentioned instability the application of our approach to a large lung cancer dataset proved computationally tractable and wa able to recover the histological classification of the various cancer subtypes represented in the dataset 
unsupervised information extraction uie is the task of extracting knowledge from text without using hand tagged training example a fundamental problem for both uie and supervised ie is assessing the probability that extracted information is correct in massive corpus such a the web the same extraction is found repeatedly in different document how doe this redundancy impact the probability of correctness this paper introduces a combinatorial ball andurns model that computes the impact of sample size redundancy and corroboration from multiple distinct extraction rule on the probability that an extraction is correct we describe method for estimating the model s parameter in practice and demonstrate experimentally that for uie the model s log likelihood are time better on average than those obtained by pointwise mutual information pmi and the noisy or model used in previous work for supervised ie the model s performance is comparable to that of support vector machine and logistic regression 
this paper deal with learning to classify by using an approximation of the analogical proportion between four object these object are described by binary and nominal attribute firstly the paper recall what is an analogical proportion between four object then it introduces a measure called analogical dissimilarity reflecting how close four object are from being in an analogical proportion secondly it present an analogical instance based learning method and describes a fast algorithm thirdly a technique to assign a set of weight to the attribute of the object is given a weight is chosen according to the type of the analogical proportion involved the weight are obtained from the learning sample then some result of the method are presented they compare favorably to standard classification technique on six benchmark finally the relevance and complexity of the method are discussed 
we consider problem of geometric exploration and self deployment for simple robot that can only sense the combinatorial non metric feature of their surroundings even with such a limited sensing we show that robot can achieve complex geometric reasoning and perform many non trivial task specifically we show that one robot equipped with a single pebble can decide whether the workspace environment is a simply connected polygon and if not it can also count the number of hole in the environment highlighting the subtlety of our sensing model we show that a robot can decide whether the environment is a convex polygon yet it cannot resolve whether a given vertex is convex finally we show that by using such local and minimal sensing a robot can compute a proper triangulation of a polygon and that the triangulation algorithm can be implemented collaboratively by a group of m such robot each with n m word memory a a corollary of the triangulation algorithm we derive a distributed analog of the well known art gallery theorem a group of n bounded memory robot in our minimal sensing model can self deploy to achieve visibility coverage of an n vertex art gallery polygon this resolve an open question raised recently 
many interesting web based ai problem require the ability to collect store and process large text datasets to address this problem we have developed slashpack an integrated toolkit for collecting and managing hypertext data currently we are using slashpack to study the effectiveness of tagging a a mechanism for organizing and searching blog and also to study community structure in the blogosphere 
we present and investigate a new method for the traveling salesman problem tsp that incorporates backbone information into the well known and widely applied lin kernighan lk local search family of algorithm for the problem we consider how heuristic backbone information can be obtained and develop method to make biased local perturbation in the lk algorithm and it variant by exploiting heuristic backbone information to improve their efficacy we present extensive experimental result using large instance from the tsp challenge suite and real world instance in tsplib showing the significant improvement that the new method can provide over the original algorithm 
we present a fully connectionist system for the learning of first order logic program and the generation of corresponding model given a program and a set of training example we embed the associated semantic operator into a feed forward network and train the network using the example this result in the learning of first order knowledge while damaged or noisy data is handled gracefully 
we present the syntax and semantics of a modular ontology language shoiqp to support context specific reuse of knowledge from multiple ontology a shoiqp ontology consists of multiple ontology module each of which can be viewed a a shoiq ontology and concept role and nominal name can be shared by importing relation among module shoiqp support contextualized interpretation i e interpretation from the point of view of a specific package we establish the necessary and sufficient constraint on domain relation i e the relation between individual in different local domain to preserve the satisfiability of concept formula monotonicity of inference and transitive reuse of knowledge 
clustering under constraint is a recent innovation in the artificial intelligence community that ha yielded significant practical benefit however recent work ha shown that for some negative form of constraint the associated subproblem of just finding a feasible clustering is np complete these worst case result for the entire problem class say nothing of where and how prevalent easy problem instance are in this work we show that there are large pocket within these problem class where clustering under constraint is easy and that using easy set of constraint yield better empirical result we then illustrate several sufficient condition from graph theory to identify a priori where these easy problem instance are and present algorithm to create large and easy to satisfy constraint set 
we combine the modal logic s with the description logic dl alcqi the resulting multi dimensional dl s alcqi support reasoning about change by allowing to express that concept and role change over time it cannot however discriminate between change in the past and in the future our main technical result is that satisfiability of s alcqi concept with respect to general tboxes including gcis is decidable and exptime complete in contrast reasoning in temporal dl that are able to discriminate between past and future is inherently undecidable we argue that our logic is sufficient for reasoning about temporal conceptual model with time stamping constraint 
mechanism design is the study of preference aggregation protocol that work well in the face of self interested agent we present the first general purpose technique for automatically designing multistage mechanism these can reduce elicitation burden by only querying agent for information that is relevant given their answer to previous query we first show how to turn a given e g automatically designed using constrained optimization technique single stage mechanism into the most efficient corresponding multistage mechanism given a specified elicitation tree we then present greedy and dynamic programming dp algorithm that determine the elicitation tree optimal in the dp case next we show how the query saving inherent in the multistage model can be used to design the underlying single stage mechanism to maximally take advantage of this approach finally we present negative result on the design of multistage mechanism that do not correspond to dominant strategy single stage mechanism an optimal multistage mechanism in general ha to randomize over query to hide information from the agent 
we introduce a novel search based decision procedure for quantified boolean formula qbfs called abstract branching a opposed to standard search based procedure it escape the burdensome need for branching on both child of every universal node in the search tree this is achieved by branching on existential variable only while admissible universal assignment are inferred running example and experimental result are reported 
the paper introduces a new type of compression for decision diagram data structure such a bdds mdds and aomdds the compression take advantage of repeated substructure within the decision diagram in order to lessen redundancy beyond what is possible using simple subfunction sharing the resulting compressed data structure allows traversal of the original decision diagram with no significant overhead specifically it allows the efficient computation of valid domain that is the assignment for each encoded variable that can participate in a solution which is critical when the decision diagram is used to support an interactive configurator we relate these result to application for interactively configurable memory limited device and give empirical result on the amount of saved space for a wide variety of instance 
we introduce a new np complete problem asking if a query hypercube is not covered by a set of other evidence hypercubes this come down to a form of constraint reasoning asking for the satisfiability of a cnf formula where the logical atom are inequality over single variable with possibly infinite variable domain we empirically investigate the location of the phase transition region in two random distribution of problem instance we introduce a solution method that iteratively construct a representation of the non covered part of the query cube in particular the method is not based on backtracking our experiment show that the method is in a significant range of instance superior to the backtracking method that result from translation to sat and application of a state of the art dp based sat solver 
we study the problem of learning largemargin half space in various setting using coresets and show that coresets are a widely applicable tool for large margin learning a large margin coreset is a subset of the input data sufficient for approximating the true maximum margin solution in this work we provide a direct algorithm and analysis for constructing large margin coresets we show various application including a novel coreset based analysis of large margin active learning and a polynomial time in the number of input data and the amount of noise algorithm for agnostic learning in the presence of outlier noise we also highlight a simple extension to multi class classification problem and structured output learning 
recently there ha been a renewed interest in ao a planning problem involving uncertainty and feedback can he naturally formulated a and or graph in this work we carry out what is prohably the first detailed empirical evaluation of ao in relation to other and or search algorithm we compare ao with two other method the well known value iteration vi algorithm and a new algorithm learning in depth first search ldfs we consider instance from four domain usc three different heuristic function and focus on the optimization of cost in the worst case max and or graph roughly we find that while ao doe better than vi in the presence of informed heuristic vi doe better than recent extension of ao in the presence of cycle in the and or graph at the same time lofs and it variant bounded lofs which can be regarded a extension of ida are almost never slower than either ao or vi and in many case are order of magnitude faster 
the asknet system is an attempt to automatically generate large scale semantic knowledge network from natural language text state of the art language processing tool including parser and semantic analyser are used to turn input sentence into fragment of semantic network these network fragment are combined using spreading activation based algorithm which utilise both lexical and semantic information the emphasis of the system is on wide coverage and speed of construction in this paper we show how a network consisting of over million node and million edge more than twice a large a any network currently available can be created in le than day we believe that the method proposed here will enable the construction of semantic network on a scale never seen before and in doing so reduce the knowledge acquisition bottleneck for ai 
the long term goal of project halo is to build an application called digital aristotle that can answer question on a variety of science topic and provide user and domain appropriate explanation a a near term goal we are focusing on enabling subject matter expert smes to construct declarative knowledge base kb from page of a science textbook in the domain of physic giancoli chemistry brown et al and biology campbell et al in a way that the system can answer question similar to those on an advanced placement ap exam we will demonstrate the current state of a system called aura that we have been developing a a contributing technology toward the goal of digital aristotle the innovative feature of aura are that it support knowledge formulation for a mixture of textual and nontextual knowledge and question formulation using an interactive dialog based on simplified english the nontextual knowledge may contain table chemical reaction and mathematical equation in an extensive usability testing of aura we have established the basic viability of the approach 
the limited visual and computational resource available during the perception of a human action make a visual attention mechanism essential in this paper we propose an attention mechanism that combine the saliency of top down or goal directed element based on multiple hypothesis about the demonstrated action with the saliency of bottom up or stimulus driven component furthermore we use the bottom up part to initialise the top down hence resulting in a selection of the behaviour that rightly require the limited computational resource this attention mechanism is then combined with an action understanding model and implemented on a robot where we examine it performance during the observation of object directed human action 
learning application specific distance metric from labeled data is critical for both statistical classification and information retrieval most of the earlier work in this area ha focused on finding metric that simultaneously optimize compactness and separability in a global sense specifically such distance metric attempt to keep all of the data point in each class close together while ensuring that data point from different class are separated however particularly when class exhibit multimodal data distribution these goal conflict and thus cannot be simultaneously satisfied this paper proposes a local distance metric ldm that aim to optimize local compactness and local separability we present an efficient algorithm that employ eigenvector analysis and bound optimization to learn the ldm from training data in a probabilistic framework we demonstrate that ldm achieves significant improvement in both classification and retrieval accuracy compared to global distance learning and kernel based knn 
subset of the negation normal form formula nnfs of propositional logic have received much attention in ai and proved a valuable representation language for boolean function in this paper we present a new framework called vnnf for the representation of a much more general class of function than just boolean one this framework support a larger family of query and transformation than in the nnf case including optimization one a such it encompasses a number of existing setting e g nnfs semiring csps mixed csps sldds add aadds we show how the property imposed on nnfs to define more tractable fragment decomposability determinism decision read once can be extended to vnnfs giving rise to subset for which a number of query and transformation can be achieved in polynomial time 
forming effective coalition is a major research challenge in the field of multi agent system central to this endeavour is the problem of determining the best group of agent to select to achieve some goal to this end in this paper we present a novel optimal anytime algorithm for this coalition structure generation problem that is significantly faster than previous algorithm designed for this purpose specifically our algorithm can generate solution by partitioning the space of all potential coalition into sub space that contain coalition structure that are similar according to some criterion such that these sub space can be pruned by identifying their bound using this representation the algorithm then search through only valid and unique coalition structure and selects the best among them using a branch and bound technique we empirically show that we are able to find solution that are optimal in of the time taken by the state of the art dynamic programming algorithm for agent using much le memory o a instead of o a for the set of agent a moreover our algorithm is the first to be able to solve the coalition structure generation problem for number of agent bigger than in reasonable time le than minute for agent a opposed to around month for the best previous solution 
the paper analyzes the extension of frontier search to the multiobjective framework a frontier multiobjective a search algorithm is developed some formal property are presented and it performance is compared to those of other multiobjective search algorithm the new algorithm is adequate for both monotone and non monotone heuristic 
assume that we are trying to build a visual recognizer for a particular class of object chair for example using existing induction method assume the assistance of a human teacher who can label an image of an object a a positive or a negative example a positive example we can obviously use image of real chair it is not clear however what type of object we should use a negative example this is an example of a common problem where the concept we are trying to learn represents a small fraction of a large universe of instance in this work we suggest learning with the help of near miss negative example that differ from the learned concept in only a small number of significant point and we propose a framework for automatic generation of such example we show that generating near miss in the feature space is problematic in some domain and propose a methodology for generating example directly in the instance space using modification operator function over the instance space that produce new instance by slightly modifying existing one the generated instance are evaluated by mapping them into the feature space and measuring their utility using known active learning technique we apply the proposed framework to the task of learning visual concept from range image 
yaman et al yaman et al introduce go theory to reason about moving object in this paper we show that this logic often doe not allow u to infer that an object is not present at a given place or region even though common sense would dictate that this is a reasonable inference to make we define a class of model of go theory called coherent model we use this concept to define a motion closed world assumption mcwa and develop a notion of mcwa entailment we show that checking if a go theory ha a coherent model is np complete an in atom check if a given object is present in a given region sometime in a given time interval we provide sound and complete algorithm to check if a ground in literal positive or negative in atom can be inferred from a go theory using the mcwa in our experiment our algorithm answer such query in le than second when there are up to go atom per object 
toy world involving action such a the block world and the missionary and cannibal puzzle are often used by researcher in the area of commonsense reasoning and planning to illustrate and test their idea we would like to create a datahase of general purpose knowledge about action that encodes common feature of many action domain of this kind in the same way a abstract algebra and topology represent common feature of specific number system this paper is a report on the first stage of this project the design of an action description language in which this database will be written the new language is an extension of the action language c it main distinctive feature is the possibility of referring to other action description in the definition of a new action domain 
we present a sound and complete logic for reasoning about simple apl program simple apl is a fragment of the agent programming language apl designed for the implementation of cognitive agent with belief goal and plan our logic is a variant of pdl and allows the specification of safety and liveness property of agent program we prove a correspondence between the operational semantics of simple apl and the model of the logic for two example program execution strategy we show how to translate agent program written in simpleapl into expression of the logic and give an example in which we show how to verify correctness property for a simple agent program 
parzen window a a nonparametric method ha been applied to a variety of density estimation a well a classification problem similar to nearest neighbor method parzen window doe not involve learning while it converges to true but unknown probability density in the asymptotic limit there is a lack of theoretical analysis on it performance with finite sample in this paper we establish a finite sample error bound for parzen window we first show that parzen window is an approximation to regularized least square rls method that have been well studied in statistical learning theory we then derive the finite sample error bound for parzen window and discus the property of the error bound and it relationship to the error bound for rls this analysis provides interesting insight to parzen window a well a the nearest neighbor method from the point of view of learning theory finally we provide empirical result on the performance of parzen window and other method such a nearest neighbor rls and svms on a number of real data set these result corroborate well our theoretical analysis 
most work on predictive representation of state psrs focus on learning a complete model of the system that can be used to answer any question about the future however we may be interested only in answering certain kind of abstract question for instance we may only care about the presence of object in an image rather than pixel level detail in such case we may be able to learn substantially smaller model that answer only such abstract question we present the framework of psr homomorphism for model abstraction in psrs a homomorphism transforms a given psr into a smaller psr that provides exact answer to abstract question in the original psr a we shall show this transformation capture structural and temporal abstraction in the original psr 
the ability to understand and communicate spatial relationship is central to many human level reasoning task people often describe spatial relationship using preposition i e in on under being able to use and interpret spatial preposition could help create interactive system for many task including knowledge capture here i describe my thesis work modeling the learning and use of spatial preposition and applying this model to the task of knowledge capture 
we present a rationale for studying long term human robot interaction and explain why new application are necessary for this type of experimentation the design and implementation of a robot that ha been implemented is briefly described with the outline of a study that is under way 
for autonomous artificial decision maker to solve realistic task they need to deal with searching through large state and action space under time pressure we study the problem of planning in such domain and show how structured representation of the environment s dynamic can help partition the action space into a set of equivalence class at run time the partitioned action space is then used to produce a reduced set of action this technique speed up search and can yield significant gain in planning efficiency 
we present a novel method for creating a estimate for structured search problem originally described in haghighi denero klein in our approach we project a complex model onto multiple simpler model for which exact inference is efficient we use an optimization framework to estimate parameter for these projection in a way which bound the true cost similar to klein manning we then combine completion estimate from the simpler model to guide search in the original complex model we apply our approach to bitext parsing and demonstrate it effectiveness 
activity recognition fit within the bigger framework of context awareness in this paper we report on our effort to recognize user activity from accelerometer data activity recognition is formulated a a classification problem performance of base level classifier and meta level classifier is compared plurality voting is found to perform consistently well across different setting 
designing revenue maximizing combinatorial auction ca is a recognized open problem in mechanism design it is unsolved even for two bidder and two item for sale rather than attempting to characterize the optimal auction we focus on designing approximation suboptimal auction mechanism which yield high revenue our approximation belong to the family of virtual valuation combinatorial auction vvca vvca is a vickrey clarke grove vcg mechanism run on virtual valuation that are linear transformation of the bidder real valuation we pursue two approach to constructing approximately optimal ca the first is to construct a vvca with worst case and average case performance guarantee we give a logarithmic approximation auction for basic important special case of the problem limited supply of item on sale with additive valuation and unlimited supply the second approach is to search the parameter space of vvcas in order to obtain high revenue mechanism for the general problem we introduce a series of increasingly sophisticated algorithm that use economic insight to guide the search and thus reduce the computational complexity our experiment demonstrate that in many case these algorithm perform almost a well a the optimal vvca yield a substantial increase in revenue over the vcg mechanism and drastically outperform the straightforward algorithm in run time 
overhearing is an approach for monitoring open distributed multi agent system by listening to the routine communication taking place within them previous investigation of overhearing assumed that all inter agent communication are accessible to a single overhearing agent however a multi agent system grow both in size and distribution two problem arise first in large scale setting an overhearing agent cannot monitor all agent and their conversation and must therefore be selective in carefully choosing it target second a single overhearer would encounter difficulty overhearing agent acting in a geographically distributed environment this paper tackle these challenge by addressing distributed team of overhearing agent involved in selective overhearing building on prior work on centralized selective overhearing we consider the consequence of transitioning from overhearing team working in a centrally coordinated manner to distributed overhearing team in doing so we distinguish the various factor influencing the level of distribution within these team and determine their importance in term of effective selective overhearing 
the impact of learning algorithm optimization by mean of parameter tuning is studied to do this two quality attribute sensitivity and classification performance are investigated and two metric for quantifying each of these attribute are suggested using these metric a systematic comparison ha been performed between four induction algorithm on eight data set the result indicate that parameter tuning is often more important than the choice of algorithm and there doe not seem to be a trade off between the two quality attribute moreover the study provides quantitative support to the assertion that some algorithm are more robust than others with respect to parameter configuration finally it is briefly described how the quality attribute and their metric could be used for algorithm selection in a systematic way 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
dipra distributed practical reasoning architecture implement the main principle of practical reasoning via the distributed action selection paradigm we introduce and motivate the underlying theoretical and computational peculiarity of dipra and we describe it component also providing a a case study a guard and thief task 
biological movement control and planning is based upon motor primitive in our approach we presume that each motor primitive take responsibility for controlling a small sub block of motion containing coherent muscle activation output a central timing controller cue these subroutine of movement creating complete movement strategy that are built up by overlaying primitive thus creating synergy of muscle activation this partitioning allows the movement to be defined by a sparse code representing the timing of primitive activation this paper show that it is possible to use a factorial hidden markov model to infer primitive in handwriting data the variation in the handwriting data can to a large extent be explained by timing variation in the triggering of the primitive once an appropriate set of primitive ha been inferred the character can be represented a a set of timing of primitive activation along with variance giving a very compact representation of the character the model is naturally partitioned into a low level primitive output stage and a top down primitive timing stage this partitioning give u an insight into behaviour such a scribbling and what is learnt in order to write a new character 
we are interested in the belief change that occurs due to a sequence of ontic action and epistemic action in order to represent such problem we extend an existing epistemic action language to allow erroneous initial belief we define a non markovian semantics for our action language that explicitly respect the interaction between ontic action and epistemic action further we illustrate how to solve epistemic projection problem in our new language by translating action description into extended logic program we conclude with some remark about a prototype implementation of our work 
we propose an approach to recommendation system that optimizes over possible set of recommended alternative in a decision theoretic manner our approach selects the alternative set that maximizes the expected valuation of the user s choice from the recommended set the set based optimization explicitly recognizes the opportunity for passing residual uncertainty about preference back to the user to resolve implicitly the approach chooses a set with a diversity of alternative that optimally cover the uncertainty over possible user preference the approach can be used with several preference representation including utility theory qualitative preference model and informal scoring we develop a specific formulation for multi attribute utility theory which we call maximization of expected max mem we go on to show that this optimization is np complete when user preference are described by discrete distribution and suggest two efficient method for approximating it these approximation have complexity of the same order a the traditional k max operator and for both synthetic and real world data perform better than the approach of recommending the k individually best alternative which is not a surprise and very close to the optimum set which is le expected 
heuristic search is a leading approach to domain independent planning for cost optimal planning however existing admissible heuristic are generally too weak to effectively guide the search pattern database heuristic pdbs which are based on abstraction of the search space are currently one of the most promising approach to developing better admissible heuristic the informedness of pdb heuristic depends crucially on the selection of appropriate abstraction pattern although pdbs have been applied to many search problem including planning there are not many insight into how to select good pattern even manually what constitutes a good pattern depends on the problem domain making the task even more difficult for domain independent planning where the process need to be completely automatic and general we present a novel way of constructing good pattern automatically from the specification of planning problem instance we demonstrate that this allows a domain independent planner to solve planning problem optimally in some very challenging domain including a strip formulation of the sokoban puzzle 
in this paper we perform an empirical study of the impact of noise on cost sensitive c learning through observation on how a c learner reacts to the mislabeled training example in term of misclassification cost and classification accuracy our empirical result and theoretical analysis indicate that mislabeled training example can raise serious concern for cost sensitive classification especially when misclassifying some class becomes extremely expensive compared to general inductive learning the problem of noise handling and data cleansing is more crucial and should be carefully investigated to ensure the success of c learning 
protocol structure interaction among communicating agent a commitment machine model a protocol in term of how the commitment of the various party evolve commitment machine thus support flexible behavior while providing a meaningful basis for compliance with a protocol unfortunately current formulation of commitment machine are not sufficiently general or rigorous this paper develops generalized commitment machine gcms whose element are described generically in term of inference and whose computation are infinite thus supporting nonterminating protocol this paper show how a gcm can be understood a a nondeterministic b chi automaton ba whose acceptance condition accommodates infinite a well a finite execution deterministic ba are readily emulated by conventional software e g a script running in a web browser in general nondeterministic ba may have no equivalent deterministic ba however under well motivated technical condition a gcm yield a deterministic b chi automaton that although not necessarily equivalent in automaton theory term is sound produce only computation allowed by the gcm and complete produce the effect of any computation allowed by the gcm 
the scalability of graph search algorithm can be greatly extended by using external memory such a disk to store generated node we consider structured duplicate detection an approach to external memory graph search that limit the number of slow disk i o operation needed to access search node stored on disk by using an abstract representation of the graph to localize memory reference for graph with sufficient locality structured duplicate detection outperforms other approach to external memory graph search we develop an automatic method for creating an abstract representation that reveals the local structure of a graph we then integrate this approach into a domain independent strip planner and show that it dramatically improves scalability for a wide range of planning problem the success of this approach strongly suggests that similar local structure can be found in many other graph search problem 
in this paper we introduce examination dialogue an addition to the dialogue typology of walton and krabbe in educational setting the purpose of dialogue is often to elicit the position of a student e g to test understanding in other setting a frequently adopted tactic is to attack an opponent s stance by exposing internal inconsistency in their argument in real debate such inconsistency will often be rather more subtle than elementary logical fallacy since they arise from contradiction apparent in the opponent s value system protagonist will be better positioned to judge the applicability of this tactic a more information is determined concerning the exact nature of their opponent s case e g the argument favoured and value endorsed one obstacle however is that following a request to state a view the challenged party may refuse to comment in this paper we present an approach to modelling the evolution of examination dialogue based on the concept of value based argument framework and outline some algorithmic issue regarding argument selection 
interactive configurators are decision support system assisting user in selecting value for parameter that respect given constraint the underlying knowledge can be conveniently formulated a a constraint satisfaction problem where the constraint are propositional formula the problem of interactive configuration wa originally inspired by the product configuration problem with the emergence of the masscustomization paradigm in product manufacturing but ha also been applied to other task requiring user interaction such a specifying service or setting up complex equipment the user friendly requirement of complete backtrack free and real time interaction make the problem computationally challenging therefore it is beneficial to compile the configuration constraint into a tractable representation such a binary decision diagram bod bryant to support efficient user interaction the compilation deal with the np hardness such that the online interaction is in polynomial time in the size of the bod in this paper we address the problem of extending configurators so that a user can interactively limit configuration choice based on a maximum cost such a price or weight of a product of any valid configuration in a complete backtrack free and real time manner the current bod compilation approach is not adequate for this purpose since adding the total cost information to the constraint description can dramatically increase the size of the compiled bod we show how to extend this compilation approach to solve the problem while keeping the polynomial time guarantee 
a pattern database pdb is a heuristic function stored a a lookup table symmetry of a state space are often used to enable multiple value to be looked up in a pdb for a given state this paper introduces an additional pdb lookup called the dual pdb lookup a dual pdb lookup is always admissible but can return inconsistent value the paper also present an extension of the well known pathmax method so that inconsistency in heuristic value are propagated in both direction child to parent and parent to child in the search tree experiment show that the addition of dual lookup and bidirectional pathmax propagation can reduce the number of node generated by ida by over one order of magnitude in the topspin puzzle and rubik s cube and by about a factor of two for the sliding tile puzzle 
planning in partially observable dynamical system such a pomdps and psrs is a computationally challenging task popular approximation technique that have proven successful are point based planning method including pointbased value iteration pbvi which work by approximating the solution at a finite set of point these point based method typically are anytime algorithm whereby an initial solution is obtained using a small set of point and the solution may be incrementally improved by including additional point we introduce a family of anytime pbvi algorithm that use the information present in the current solution for identifying and adding new point that have the potential to best improve the next solution we motivate and present two different method for choosing point and evaluate their performance empirically demonstrating that high quality solution can be obtained with significantly fewer point than previous pbvi approach 
a key issue in artificial intelligence lie in finding the amount of input detail needed to do successful learning too much detail cause overhead and make learning prone to over fitting too little detail and it may not be possible to learn anything at all the issue is particularly relevant when the input are relational case description and a very expressive vocabulary may also lead to inconsistent representation for example in the whodunit problem the task is to form hypothesis about the identity of the perpetrator of an event described using relational proposition the training data consists of arbitrary relational description of many other similar case in this paper we examine the possibility of translating the case description into an alternative vocabulary which ha a reduced number of predicate and therefore produce more consistent case description we compare how the reduced vocabulary affect three different learning algorithm exemplar based analogy prototype based analogy and association rule learning we find that it ha a positive effect on some algorithm and a negative effect on others which give u insight into all three algorithm and indicates when reduced vocabulary might be appropriate 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
symmetry is a widespread phenomenon that can offer opportunity for powerful exploitation in area a diverse a molecular chemistry pure mathematics circuit design biology and architecture graph are an abstract way to represent relational structure the search for symmetry in many context can thus be reduced to the attempt to find graph automorphisms brendan mckay s nauty system mckay is an example of one of the highly successful product of research into this task erd s and r nyi showed that almost all large graph are asymmetric but it is readily observed that many graph representing structure of real interest contain symmetry even more graph are nearly symmmetric in the sense that to each graph there is a closely similar graph that is symmetric in this paper we explore the problem of finding near symmetry in graph and describe the technique we are developing for performing this task 
in behavioural science the problem that a sequence of stimulus is followed by a sequence of reward r t is considered the subject is to learn the full sequence of reward from the stimulus where the prediction is modelled by the sutton barto rule in a sequence of n trial this prediction rule is learned iteratively by temporal difference learning we present a closed formula of the prediction of reward at trial time t within trial n from that formula we show directly that for n the prediction converge to the real reward in this approach a new quality of correlation type toeplitz matrix is proven we give learning rate which optimally speed up the learning process 
we identify tractable class of constraint based on the following simple property of a constraint at every infeasible point there exist two direction such that with respect to any other feasible point moving along at least one of these two direction decrease a certain distance metric to it we show that connected row convex crc constraint arc consistent consecutive tree convex acctc constraint etc fit this characterization and are therefore amenable to extremely simple polynomial time randomized algorithm the complexity of which are shown to be much le than that of the corresponding known deterministic algorithm and the generic lower bound for establishing path consistency on a related note we also provide a simple polynomial time deterministic algorithm for finding tree embeddings of variable domain if they exist for establishing tree convexity in path consistent network 
we introduce hex program which are nonmonotonic logic program admitting higher order atom a well a external atom and we extend the well known answer set semantics to this class of program higher order feature are widely acknowledged a useful for performing meta reasoning among other task furthermore the possibility to exchange knowledge with external source in a fully declarative framework such a answer set programming asp is nowadays important in particular in view of application in the semantic web area through external atom hex program can model some important extension to asp and are a useful kr tool for expressing various application finally complexity and implementation issue for a preliminary prototype are discussed 
object and scene learning and recognition is a major issue in computer vision in robotics and in cognitive science this paper present the principle and result of an approach which extract structured view based representation for multi purpose recognition the structure are hierarchical and distributed and provide for generalization and categorization a tracking process enables to bind view over time and to link consecutive view scene can also be recognized using object a component illustrative result are presented 
typically autonomous believable agent are implemented using static hand authored reactive behavior or script this hand authoring allows designer to craft expressive behavior for character but can lead to excessive authorial burden a well a result in character that are brittle to changing world dynamic in this paper we present an approach for the runtime adaptation of reactive behavior for autonomous believable character extending transformational planning our system allows autonomous character to monitor and reason about their behavior execution and to use this reasoning to dynamically rewrite their behavior in our evaluation we transplant two character in a sample tag game from the original world they were written for into a different one resulting in behavior that violates the author intended personality the reasoning layer successfully adapts the character s behavior so a to bring it long term behavior back into agreement with it personality 
driven by planning problem with both disjunctive constraint and contingency we define the disjunctive temporal problem with uncertainty dtpu an extension of the dtp that includes contingent event generalizing existing work on simple temporal problem with uncertainty we divide the time point into controllable and uncontrollable class and propose varying notion of controllability to replace the notion of consistency 
we show how testing whether a system is diagnosable can be reduced to the satisfiability problem and how satisfiability algorithm yield a very efficient approach to testing diagnosability diagnosability is the question whether it is always possible to know whether a given system ha exhibited a failure behavior this is a basic question that underlies diagnosis and it is also closely related to more general question about the possibility to know given fact about system behavior the work combine the twin plant construct of jiang et al which is the basis of diagnosability testing of system with an enumerative representation and sat based technique to ai planning which form a very promising approach to finding path in very large transition graph 
the marchitecture is a cognitive architecture for autonomous development of representation the goal of the marchitecture are domain independence operating in the absence of knowledge engineering learning an ontology of parameterized relational concept and elegance of design to this end the marchitecture integrates classification parsing reasoning and explanation the marchitecture assumes an ample amount of raw data to develop it representation and it is therefore appropriate for long lived agent 
we present a novel framework for multi label learning that explicitly address the challenge arising from the large number of class and a small size of training data the key assumption behind this work is that two example tend to have large overlap in their assigned class membership if they share high similarity in their input pattern we capitalize this assumption by first computing two set of similarity one based on the input pattern of example and the other based on the class membership of the example we then search for the optimal assignment of class membership to the unlabeled data that minimizes the difference between these two set of similarity the optimization problem is formulated a a constrained non negative matrix factorization nmf problem and an algorithm is presented to efficiently find the solution compared to the existing approach for multi label learning the proposed approach is advantageous in that it is able to explore both the unlabeled data and the correlation among different class simultaneously experiment with text categorization show that our approach performs significantly better than several state of the art classification technique when the number of class is large and the size of training data is small 
the use of mutual exclusion mutex ha led to significant advance in propositional planning however previous mutex can only detect pair of action or fact that cannot be arranged at the same time step in this paper we introduce a new class of constraint that significantly generalizes mutex and can be efficiently computed the proposed long distance mutual exclusion londex can capture constraint over action and fact not only at the same time step but also across multiple step londex provides a powerful and general approach for improving planning efficiency a an application we have integrated londex into satplan a leading optimal planner experimental result show that londex can effectively prune the search space and reduce the planning time the resulting planner maxplan ha won the first place award in the optimal track of the th international planning competition 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
in normal scenario computer scientist often consider the number of state in a game to capture the difficulty of learning an equilibrium however player do not see game in the same light most consider go or chess to be more complex than monopoly in this paper we discus a new measure of game complexity that link existing state of the art algorithm for computing approximate equilibrium to a more human measure in particular we consider the range of skill in a game i e how many different skill level exist we then modify existing technique to design a new algorithm to compute approximate equilibrium whose performance can be captured by this new measure we use it to develop the first near nash equilibrium for a four round abstraction of poker and show that it would have been able to win handily the bankroll competition from last year s aaai poker competition 
the ability to generate narrative is of importance to computer system that wish to use story effectively for entertainment training or education one way to generate narrative is to use planning however story planner are limited by the fact that they can only operate on the story world provided which impact the ability of the planner to find a solution story plan and the quality and structure of the story plan if one is found we present a planning algorithm for story generation that can nondeterministically make decision about the description of the initial story world state in a least commitment fashion 
we describe a user study evaluating two critiquing based recommender agent based on three criterion decision accuracy decision effort and user confidence result show that user motivated critique were more frequently applied and the example critiquing system employing only this type of critique achieved the best result in particular the example critiquing agent significantly improves user decision accuracy with le cognitive effort consumed than the dynamic critiquing recommender with system proposed critique additionally the former is more likely to inspire user confidence of their choice and promote their intention to purchase and return to the agent for future use 
we show how in the propositional case both reiter s and scherl levesque s solution to the frame problem can be modelled in dynamic epistemic logic del and provide an optimal regression algorithm for the latter our method is a follows we extend reiter s framework by integrating observation action and modal operator of knowledge and encode the resulting formalism in del with announcement and assignment operator by extending lutz recent satisfiability preserving reduction to our logic we establish optimal decision procedure for both reiter s and scherl levesque s approach satisfiability is np complete for one agent pspace complete for multiple agent and exptimecomplete when common knowledge is involved 
several researcher have attempted extract the principle behind hormone gradient found in biological system and apply them to control physically manifested distributed system this paper present an efficient implementation of a graph based version of hormone gradient mechanism the algorithm is based on hop counting of the topological distance between any given vertex and the hormone emitting vertex and can deal with dynamic change to the topology of the system performance of the described algorithm is documented through a number of experiment the algorithm wa developed for use in self reconfigurable robotics but might very well be useful for many other application the use of the algorithm to provide a common coordinate system for a collection of self recon figurable robot module is described that provides pose relative to the emitting module for all module affected by the hormone 
continuous time bayesian network ctbns nodelman shelton koller are an elegant modeling language for structured stochastic process that evolve over continuous time the ctbn framework is based on homogeneous markov process and defines two distribution with respect to each local variable in the system given it parent an exponential distribution over when the variable transition and a multinomial over what is the next value in this paper we present two extension to the framework that make it more useful in modeling practical application the first extension model arbitrary transition time distribution using erlang coxian approximation while maintaining tractable learning we show how the censored data problem arises in learning the distribution and present a solution based on expectation maximization initialized by the kaplan meier estimate the second extension is a general method for reasoning about negative evidence by introducing update that assert no observable event occur over an interval of time such update were not defined in the original ctbn framework and we show that their inclusion can significantly improve the accuracy of filtering and prediction we illustrate and evaluate these extension in two real world domain email use and gps trace of a person traveling about a city 
to produce multimedia encyclopedic content we propose a method to search the web for image associated with a specific word sense we use text in an html file which link to an image a a pseudocaption for the image and perform text based indexing and retrieval we use term description in a web search site called cyclone a query and correspond image and text based on word sens 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
weighted a wa is a popular search technique that scale up a while sacrificing solution quality recently researcher have proposed two variant of wa kwa add diversity to wa and msc wa add commitment to wa in this paper we demonstrate that there is benefit in combining them the resulting msc kwa scale up to larger domain than wa kwa and msc wa which is rather surprising since diversity and commitment at first glance seem to be opposing concept 
probabilistic planning with observability restriction a formalized for example a partially observable markov decision process pomdp ha a wide range of application but it is computationally extremely difficult for pomdps the most general decision problem about existence of policy satisfying certain property are undecidable we consider a computationally easier form of planning that ignores exact probability and give an algorithm for a class of planning problem with partial observability we show that the basic backup step in the algorithm is np complete then we proceed to give an algorithm for the backup step and demonstrate how it can be used a a basis of an efficient algorithm for constructing plan 
abduction is an important method of non monotonic reasoning with many application in ai and related topic in this paper we concentrate on propositional abduction where the background knowledge is given by a propositional formula decision problem of great interest are the existence and the relevance problem the complexity of these decision problem ha been systematically studied while the counting complexity of propositional abduction ha remained obscure the goal of this work is to provide a comprehensive analysis of the counting complexity of propositional abduction in various class of theory 
social insect such a ant and termite collectively build large and complex structure with many individual following simple rule and no centralized control or planning theraulaz and bonabeau camazine et al such swarm system have many desirable property a high level of parallelism cheap and expendable individual and robustness to loss addition and error of individual insect our goal is to design system for automating construction that are similarly adaptive and robust but build what we want automated construction will impact our ability to operate in inhospitable habitat from outer space to under water and allow automated disassembly and repair 
although recent year have seen a surge of interest in the computational aspect of social choice no attention ha previously been devoted to election with multiple winner e g election of an assembly or committee in this paper we fully characterize the worst case complexity of manipulation and control in the context of four prominent multi winner voting system additionally we show that several tailor made multi winner voting scheme are impractical a it is np hard to select the winner in these scheme 
current knowledge acquisition tool have limited understanding of how user enter knowledge and how acquired knowledge is used and provide limited assistance in organizing various knowledge authoring task in this paper we present a novel extension to existing knowledge acquisition tool where the system capture the episode of knowledge acquisition and knowledge use through a set of declarative reflection pattern performs assessment on how to improve the future knowledge acquisition and knowledge use based on captured episode and provides assistance to the user by combining the assessment result 
fully decentralized algorithm for distributed constraint optimization often require excessive amount of communication when applied to complex problem the optapo algorithm of mailler and lesser us a strategy of partial centralization to mitigate this problem we introduce pc dpop a new partial centralization technique based on the dpop algorithm of petcu and faltings pc dpop provides better control over what part of the problem are centralized and allows this centralization to be optimal with respect to the chosen communication structure unlike optapo pc dpop allows for a priory exact prediction about privacy loss communication memory and computational requirement on all node and link in the network upper bound on communication and memory requirement can be specified we also report strong efficiency gain over optapo in experiment on three problem domain 
this paper describes mexec an implemented micro executive that compiles a device model that can have feedback into a structure for subsequent evaluation this system computes both the most likely current device mode from n set of sensor measurement and the n step reconfiguration plan that is most likely to result in reaching a target mode if such a plan exists a user tune the system by increasing n to improve system capability at the cost of real time performance 
we introduce a rule selection algorithm called roccer which operates by selecting classification rule from a larger set of rule for instance found by apriori using roc analysis experimental comparison with rule induction algorithm show that roccer tends to produce considerably smaller rule set with compatible area under the roc curve auc value the individual rule that compose the rule set also have higher support and stronger association index 
qualitative model are often more suitable than classical quantitative model in task such a model based diagnosis mbd explaining system behavior and designing novel device from first principle monotonicity is an important feature to leverage when constructing qualitative model detecting monotonic piece robustly and efficiently from sensor or simulation data remains an open problem this paper present scale based monotonicity the notion that monotonicity can be defined relative to a scale real valued function defined on a finite set of real e g sensor data or simulation result can be partitioned into quasimonotonic segment i e segment monotonic with respect to a scale in linear time a novel segmentation algorithm is introduced along with a scalebased definition of flatness 
related object may look similar at low resolution difference begin to emerge naturally a the resolution is increased by learning across multiple resolution of input knowledge can be transfered between related object my dissertation develops this idea and applies it to the problem of multitask transfer learning 
we present a novel algorithm for extracting a high quality case base from raw data while preserving and sometimes improving the competence of case based reasoning we extend the framework of smyth and keane s case deletion policy with two additional feature first we build a case base using a statistical distribution that is mined from the input data so that the case base competence can be preserved or even increased for future problem second we introduce a nonlinear transformation of the data set so that the case base size can be further reduced while ensuring that the competence be preserved and even increased we show that smyth and keane s deletion based algorithm is sensitive to noisy case and that our solution solves this problem more satisfactorily we show the theoretical foundation and empirical evaluation on several data set 
we introduce a new approach to computing answer set of logic program based on concept from constraint processing csp and satisfiability checking sat the idea is to view inference in answer set programming asp a unit propagation on nogoods this provides u with a uniform constraintbased framework for the different kind of inference in asp it also allows u to apply advanced technique from the area of csp and sat we have implemented our approach in the new asp solver clasp our experiment show that the approach is competitive with state of the art asp solver 
this paper considers a novel application domain for reinforcement learning that of autonomic computing i e selfmanaging computing system rl is applied to an online resource allocation task in a distributed multi application computing environment with independent time varying load in each application the task is to allocate server in real time so a to maximize the sum of performance based expected utility in each application this task may be treated a a composite mdp and to exploit the problem structure a simple localized rl approach is proposed with better scalability than previous approach the rl approach is tested in a realistic prototype data center comprising real server real http request and realistic time varying demand this domain pose a number of major challenge associated with live training in a real system including the need for rapid training exploration that avoids excessive penalty and handling complex potentially non markovian system effect the early result are encouraging in overnight training rl performs a well a or slightly better than heavily researched model based approach derived from queuing theory 
in game playing program relying on the minimax principle deeper search generally produce better evaluation theoretical analysis however suggest that in many case minimaxing amplifies the noise introduced by the heuristic function used to evaluate the leaf of the game tree leading to what is known a pathological behavior where deeper search produce worse evaluation in most of the previous research position were evaluated a loss or win dependence between the value of position close to each other wa identified a the property of realistic game tree that eliminates the pathology and explains why minimax is successful in practice in this paper we present an alternative explanation that doe not rely on value dependence we show that if real number are used for position value position value tend to be further apart at lower level of the game tree which lead to a larger proportion of more extreme position where error is le probable decreased probability of error in search to greater depth is sufficient to eliminate the pathology and no additional property of game tree are required 
recognizing similarity between literary work for copyright infringement detection requires evaluating similarity in the expression of content copyright law protects expression of content similarity in content alone are not enough to indicate infringement expression refers to the way people convey particular information it capture both the information and the manner of it presentation in this paper we present a novel set of linguistically informed feature that provide a computational definition of expression and that enable accurate recognition of individual title and their paraphrase more than of the time in comparison baseline feature e g tfidf weighted keywords function word etc give an accuracy of at most our computational definition of expression us linguistic feature that are extracted from po tagged text using context free grammar without incurring the computational cost of full parser the result indicate that informative linguistic feature do not have to be computationally prohibitively expensive to extract 
logician frequently use axiom schema to encode potentially infinite set of sentence with particular syntactic form in this paper we examine a first order language in which it is possible to write expression that both describe sentence and assert the truth of the sentence so described the effect of adding such expression to a knowledge base is the same a directly including the set of described sentence 
we present a brief overview of an architecture for a complex affective robot for human robot interaction 
collective classification can significantly improve accuracy by exploiting relationship among instance although several collective inference procedure have been reported they have not been thoroughly evaluated for their commonality and difference we introduce novel generalization of three existing algorithm that allow such algorithmic and empirical comparison our generalization permit u to examine how cautiously or aggressively each algorithm exploit intermediate relational data which can be noisy we conjecture that cautious approach that identify and preferentially exploit the more reliable intermediate data should outperform aggressive approach we explain why caution is useful and introduce three parameter to control the degree of caution an empirical evaluation of collective classification algorithm using two base classifier on three data set support our conjecture 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
the importance of the problem of contingent planning with action that have non deterministic effect and of planning with goal preference ha been widely recognized and several work address these two problem separately however combining conditional planning with goal preference add some new difficulty to the problem indeed even the notion of optimal plan is far from trivial since plan in nondeterministic domain can result in several different behavior satisfying condition with different preference planning for optimal conditional plan must therefore take into account the different behavior and conditionally search for the highest preference that can be achieved in this paper we address this problem we formalize the notion of optimal conditional plan and we describe a correct and complete planning algorithm that is guaranteed to find optimal solution we implement the algorithm using bdd based technique and show the practical potentiality of our approach through a preliminary experimental evaluation 
constraint can serve a a unifying force in artificial intelligence 
we present a new probabilistic framework for finding likely variable assignment in difficult constraint satisfaction problem finding such assignment is key to efficient search but practical effort have largely been limited to random guessing and heuristically designed weighting system in contrast we derive a new version of belief propagation bp using the method of expectation maximization em this allows u to differentiate between variable that are strongly biased toward particular value and those that are largely extraneous using em also eliminates the threat of non convergence associated with regular bp theoretically the derivation exhibit appealing primal dual semantics empirically it produce an embp based heuristic for solving constraint satisfaction problem a illustrated with respect to the quasigroup with hole domain embp outperforms existing technique for guiding variable and value ordering during backtracking search on this problem 
in this paper we consider an extension of the multiarmed bandit problem in this generalized setting the decision maker receives some side information performs an action chosen from a finite set and then receives a reward unlike in the standard bandit setting performing an action take a random period of time the environment is assumed to be stationary stochastic and memoryless the goal is to maximize the average reward received in one unit time that is to maximize the average rate of return we consider the on line learning problem where the decisionmaker initially doe not know anything about the environment but must learn about it by trial and error we propose an upper confidence bound style algorithm that exploit the structure of the problem we show that the regret of this algorithm relative to the optimal algorithm that ha perfect knowledge about the problem grows at the optimal logarithmic rate in the number of decision and scale polynomially with the parameter of the problem 
in the last decade several approach have been proposed for merging multiple and potentially conflicting piece of information egalitarist fusion mode privilege solution that minimize the local dissatisfaction of each agent source expert who is involved in the fusion process this paper proposes useful strategy for an egalitarist fusion of incommensurable ranked belief base under constraint we show that the fusion process can equivalently be characterized either by mean of the notion of compatible ranked base or by mean of a pareto like ordering on the set of possible solution lastly rational postulate for our merging operator are studied 
symmetry in constraint satisfaction problem csps ha been considered in two fundamentally different way a an operation preserving the solution of a csp instance or a an operation preserving the constraint to reflect these two view we define solution symmetry and constraint symmetry we discus how these concept are related and show that some csp instance have many more solution symmetry than constraint symmetry 
most existing sketch understanding system require a closed domain to achieve recognition this paper describes an incremental learning technique for open domain recognition our system build generalization for category of object based upon previous sketch of those object and us those generalization to classify new sketch we represent sketch qualitatively because we believe qualitative information provides a level of description that abstract away detail that distract from classification such a exact dimension bayesian reasoning is used in building representation to deal with the inherent uncertainty in perception qualitative representation are compared using sme a computational model of analogy and similarity that is supported by psychological evidence including study of perceptual similarity we use seql to produce generalization based on the common structure found by sme in different sketch of the same object we report on the result of testing the system on a corpus of sketch of everyday object drawn by ten different people 
in this paper we present a novel approach for tuning power mode of wireless interface we use k mean and simple correlation technique to analyze user s interaction with application based on mouse click this provides valuable contextual hint that are used to anticipate future network access pattern and intent of user based on those hint we adapt the power mode of the wireless network interface to optimize both energy usage and bandwidth usage evaluation result based on real data gathered from interaction with a desktop show significant improvement over earlier power management scheme 
we are demonstrating several intelligent agent built according to the mglair modal grounded layered architecture with integrated reasoning agent architecture the top layer of mglair is implemented in sneps and it acting subsystem snere the sneps rational engine the major demonstration will be act of the trial the trail an interactive drama running on an immersive virtual reality system in which a human participant interacts with several mglair actor agent we will also demonstrate several olher mglair agent that operate in non vr graphical environment all these agent illustrate our approach to building agent with integrated first person on line reasoning and acting 
belief revision and belief update are two of the most basic type of belief change operation we need to select either revision or update when we accept new information into the current belief however such decision making ha not been considered in this paper we propose a unified framework of revision and update based on state transition model that enable u to do such decision making this framework provides a hybrid operation of revision and update called acceptance 
active learning reduces the amount of manually annotated sentence necessary when training state of the art statistical parser one popular method uncertainty sampling selects sentence for which the parser exhibit low certainty however this method doe not quantify confidence about the current statistical model itself in particular we should be le confident about selection decision based on low frequency event we present a novel two stage method which first target sentence which cannot be reliably selected using uncertainty sampling and then applies standard uncertainty sampling to the remaining sentence an evaluation show that this method performs better than pure uncertainty sampling and better than an ensemble method based on bagged ensemble member only 
we provide an experimental study of the role of syntactic parsing in semantic role labeling our conclusion demonstrate that syntactic parse information is clearly most relevant in the very first stage the pruning stage in addition the quality of the pruning stage cannot be determined solely based on it recall and precision instead it depends on the characteristic of the output candidate that make downstream problem easier or harder motivated by this observation we suggest an effective and simple approach of combining different semantic role labeling system through joint inference which significantly improves the performance 
developing technology and system for perception and perspicacious automated control of home and workplace environment is a challenging problem we present a complete agent architecture for learning to automate the intelligent environment and discus the development deployment and technique utilized in our working intelligent environment empirical evaluation of our approach ha proven it effectiveness at reducing inhabitant interaction by 
we present and study the contribution selection algorithm csa a novel algorithm for feature selection the algorithm is based on the multiperturbation shapley analysis a framework which relies on game theory to estimate usefulness the algorithm iteratively estimate the usefulness of feature and selects them accordingly using either forward selection or backward elimination empirical comparison with several other existing feature selection method show that the backward eliminati nation variant of csa lead to the most accurate classification result on an array of datasets 
with increasing deployment of multi agent and distributed system there is an increasing need for failure diagnosis system while successfully tackling key challenge in multi agent setting model based diagnosis ha left open the diagnosis of coordination failure where failure often lie in the boundary between agent and thus the input to the model with which the diagnoser simulates the system to detect discrepancy are not known however it is possible to diagnose such failure using a model of the coordination between agent this paper formalizes model based coordination diagnosis using two coordination primitive concurrence and mutual exclusion we define the consistency based and abductive diagnosis problem within this formalization and show that both are np hard by mapping them to other known problem 
we consider the problem of policy optimization for a resource limited agent with multiple time dependent objective represented a an mdp with multiple discount factor in the objective function and constraint we show that limiting search to stationary deterministic policy coupled with a novel problem reduction to mixed integer programming yield an algorithm for finding such policy that is computationally feasible where no such algorithm ha heretofore been identified in the simpler case where the constrained mdp ha a single discount factor our technique provides a new way for finding an optimal deterministic policy where previous method could only find randomized policy we analyze the property of our approach and describe implementation result 
in this work a generalized method for learning from sequence of unlabelled data point based on unsupervised order preserving regression is proposed sequence learning is a fundamental problem which cover a wide area of research topic including e g handwritten character recognition or speech and natural language processing for this one may compute feature vector from sequence and learn a function in feature space or directly match sequence using method like dynamic time warping the former approach is not general in that they rely on set of application dependent feature while in the latter matching is often inefficient or ineffective our method take the latter approach while providing a very simple and robust matching result obtained from applying our method on a few different type of data show that the method is gerneral while accuracy is enhanced or comparable 
user can often naturally express their preference in term of ideal or non ideal solution we show how to reason about logical combination of distance constraint on ideal and non ideal using a novel global constraint we evaluate our approach on both randomly generated and real world configuration problem instance 
a embedded system grow increasingly complex there is a pressing need for diagnosing and monitoring capability that estimate the system state robustly this paper is based on approach that address the problem of robustness by reasoning over declarative model of the physical plant represented a a variant of factored hidden markov model called probabilistic concurrent constraint automaton prior work on mode estimation of pccas is based on a best first trajectory enumeration bfte algorithm two algorithm have since made improvement to the bfte algorithm the best first belief state update bfbsu algorithm ha improved the accuracy of bfte and the mexec algorithm ha introduced a polynomial time bounded algorithm using a smooth deterministic decomposable negation normal form sd dnnf representation this paper introduces a new dnnf based belief state estimation dbse algorithm that merges the polynomial time bound of the mexec algorithm with the accuracy of the bfbsu algorithm this paper also present an encoding of a pcca a a cnf with probabilistic data suitable for compilation into an sd dnnf based representation the sd dnnf representation support computing k belief state from k previous belief state in the dbse algorithm 
we describe a decentralized algorithm for coordinating a swarm of identically programmed mobile agent to spatially self aggregate into arbitrary shape using only local interaction our approach called shapebugs generates a consensus coordinate system by agent continually performing local trilaterations and achieves shape formation by simultaneously allowing agent to disperse within the defined d shape using a contained gas model this approach ha several novel feature agent can easily aggregate into arbitrary user specified shape using a formation process that is independent of the number of agent the system automatically adapts to influx and death of agent a well a accidental displacement we show that the consensus coordinate system is robust and provides reasonable accuracy in the face of significant sensor and movement error 
plan provide an explicit expectation of future observed behavior based upon the domain knowledge and a set of action model available to a planner incorrect or missing model lead to faulty plan usually characterized by catastrophic goal failure non critical anomaly occur however when actual behavior during plan execution differs only slightly from expectation and plan still achieve the given goal conjunct such anomaly provide the basis for model adjustment that represent small adaptation to the planner s background knowledge in a multi agent environment where or more individual plan can be executing at any one time automation is required to support model anomaly detection evaluation and revision we provide an agent based algorithm that generates hypothesis about the cause of plan anomaly this algorithm leverage historical plan data and a hierarchy of model in a novel integration of hypothesis generation and verification because many hypothesis can be generated by the software agent we provide a mechanism where only the most important hypothesis are presented to a user a suggestion for model repair 
deterministic dependency parsing ha often been regarded a an efficient parsing algorithm while it parsing accuracy is a little lower than the best result reported by more complex parsing model in this paper we compare deterministic dependency parser with complex parsing method such a generative and discriminative parser on the standard data set of penn chinese treebank the result show that for chinese dependency parsing deterministic parser outperform generative and discriminative parser furthermore based on the observation that deterministic parsing algorithm are greedy algorithm which choose the most probable parsing action at every step we propose three kind of ungreedy deterministic dependency parsing algorithm to globally model parsing action result show that ungreedy deterministic dependency parser perform better than original deterministic dependency parser while maintaining the same time complexity and our best parser improves much over all other parser 
to operate in natural environmental setting autonomous mobile robot need more than just the ability to navigate in the world react to perceived situation or follow pre determined strategy they must be able to plan and to adapt those plan according to the robot s capability and the situation encountered navigation simultaneous localization and mapping perception motivation planning etc are capability that contribute to the decision making process of an autonomous robot how can they be integrated while preserving their underlying principle and not make the planner or other capability a central element on which everything else relies on in this paper we address this question with an architectural methodology that us a planner along with other independent motivational source to influence the selection of behavior producing module influence of the planner over other motivational source are demonstrated in the context of the aaai challenge 
many planning and design problem can be characterized a optimal search over a constrained network of conditional choice with preference to draw upon the advanced method of constraint satisfaction to solve these type of problem many dynamic and flexible csp variant have been proposed one such variant is the weighted conditional csp wccsp so far however little work ha been done to extend the full suite of csp search algorithm to solve these csp variant in this paper we extend dynamic backtracking and similar backjumping based csp search algorithm to solve wccsps by utilizing activity constraint and soft constraint in order to quickly prune infeasible and suboptimal region of the search space we provide experimental result on randomly generated wccsp instance to prove these claim 
developing model based automatic debugging strategy ha been an active research area for several year we present a model based debugging approach that is based on abstract interpretation a technique borrowed from program analysis the abstract interpretation mechanism is integrated with a classical model based reasoning engine we test the approach on sample program and provide the first experimental comparison with earlier model used for debugging the result show that the abstract interpretation based model provides more precise explanation than previous model or standard non model based approach 
in this paper we present polynomial time algorithm that translate first order logic fol theory to smaller propositional encoding than achievable before in polynomial time for example we can sometimes reduce the number of proposition to o p c or o p k log p for p predicate of arity k and c constant symbol the guarantee depends on availability of some graphical structure in the fol representation our algorithm accept all fol theory and preserve soundness and completeness sometimes requiring the domain closure assumption our experiment show significant speedup in inference with a sat solver on real world problem our result address a common approach that translates inference and decision problem that originate in fol into propositional logic later applying efficient sat solver standard translation technique result in very large propositional encoding o p c k for predicate of arity k that are often infeasible to solve our approach scale up inference for many object and ha potential application in planning probabilistic reasoning and formal verification 
people frequently use the world wide web to find their most preferred item among a large range of option we call this task preference based search the most common tool for preference based search on the www today obtains user preference by asking them to fill in a form it then return a list of item that most closely match these preference recently several researcher have proposed tool for preference based search that elicit preference from the critique a user actively make on example shown to them we carried out a user study in order to compare the performance of traditional preference based search tool using form filling with two different version of an example critiquing tool the result show that example critiquing achieves almost three time the decision accuracy while requiring only slightly higher interaction effort 
this paper introduces a new bootstrapping method closely related to co training and scoped learning the method is tested on a web information extraction task of learning course name from web page in which we use very few labelled item a seed data web page and combine with an unlabelled set web page the overall performance improved the precision recall from for a baseline em based method to for intimate learning 
while traditional information extraction system have been built to answer question about fact subjective information extraction system will answer question about feeling and opinion a crucial step towards this goal is identifying the word and phrase that express opinion in text indeed although much previous work ha relied on the identification of opinion expression for a variety of sentiment based nlp task none ha focused directly on this important supporting task moreover none of the proposed method for identification of opinion expression ha been evaluated at the task that they were designed to perform we present an approach for identifying opinion expression that us conditional random field and we evaluate the approach at the expression level using a standard sentiment corpus our approach achieves expression level performance within of the human interannotator agreement 
we use a transition system approach to reason about the evolution of an agent s belief a action are executed some action cause an agent to perform belief revision and some action cause an agent to perform belief update but the interaction between revision and update can be non elementary we present a set of basic postulate describing the interaction of revision and update and we introduce a new belief evolution operator that give a plausible interpretation to alternating sequence of revision and update 
we consider the problem of attribution of knowledge to artificial agent and their legal principal when can we say that an artificial agent x know p and that it principal can be attributed the knowledge of p we offer a pragmatic analysis of knowledge attribution and apply it to the legal theory of artificial agent and their principal 
in this paper we develop fringe saving a fsa an incremental version of a that repeatedly find shortest path in a known gridworld from a given start cell to a given goal cell while the traversability cost of cell increase or decrease the first search of fsa is the same a that of a however fsa is able to find shortest path during the subsequent search faster than a because it reuses the beginning of the immediately preceeding a search tree that is identical to the current a search tree fsa doe this by restoring the content of the open list of a at the point in time when an a search for the current search problem could deviate from the a search for the immediately preceeding search problem we present first experimental result that demonstrate that fsa can have a runtime advantage over a and lifelong planning a lpa an alternative incremental version of a 
the complexity of existing planner is bounded by the length of the resulting plan a fact that limit planning to domain with relatively short solution we present a novel planning algorithm that us the causal graph of a domain to decompose it into subproblems and store subproblem plan in memory a macro in many domain the resulting plan can be expressed using relatively few macro making it possible to generate exponential length plan in polynomial time we show that our algorithm is complete and that there exist special case for which it is optimal and polynomial experimental result demonstrate the potential of using macro to solve planning domain with long solution plan 
the quantified boolean formula qbf problem is a powerful generalization of the boolean satisfiability sat problem where variable can be both universally and existentially quantified inspired by the fruitfulness of the established model for generating random sat instance we define and study a general model for generating random qbf instance we exhibit experimental result showing that our model bear certain desirable similarity to the random sat model a well a a number of theoretical result concerning our model 
undetected error in the expression measurement from high throughput dna microarrays and protein spectroscopy could seriously affect the diagnostic reliability in disease detection in addition to a high resilience against such error diagnostic model need to be more comprehensible so that a deeper understanding of the causal interaction among biological entity like gene and protein may be possible in this paper we introduce a robust knowledge discovery approach that address these challenge first the causal interaction among the gene and protein in the noisy expression data are discovered automatically through bayesian network learning then the diagnosis of a disease based on the network is performed using a novel error handling procedure which automatically identifies the noisy measurement and account for their uncertainty during diagnosis an application to the problem of ovarian cancer detection show that the approach effectively discovers causal interaction among cancer specific protein with the proposed error handling procedure the network perfectly distinguishes between the cancer and normal patient 
it is now well known that the feasibility of inductive learning is ruled by statistical property linking the empirical risk minimization principle and the capacity of the hypothesis space the discovery a few year ago of a phase transition phenomenon in inductive logic programming prof that other fundamental characteristic of the learning problem may similarly affect the very possibility of learning under very general condition our work examines the case of grammatical inference we show that while there is no phase transition when considering the whole hypothesis space there is a much more severe gap phenomenon affecting the effective search space of standard grammatical induction algorithm for deterministic finite automaton dfa focusing on the search heuristic of the rpni and red blue algorithm we show that they overcome this problem to some extent but that they are subject to overgeneralization the paper last suggests some direction for new generalization operator suited to this phase transition phenomenon 
the factored state representation and concurrency semantics of petri net are closely related to those of concurrent planning domain yet planning and petri net analysis have developed independently with minimal and usually unconvincing attempt at cross fertilisation in this paper we investigate and exploit the relationship between the two area focusing on petri net unfolding which is an attractive reachability analysis method a it naturally enables the recognition and separate resolution of independent subproblems on the one hand based on unfolding we develop a new forward search method for cost optimal partial order planning which can be exponentially more efficient than state space search on the other hand inspired by well known planning heuristic we investigate the automatic generation of heuristic to guide unfolding resulting in a more efficient directed reachability analysis tool for petri net 
the factored state representation and concurrency semantics of petri net are closely related to those of concurrent planning domain yet planning and petri net analysis have developed independently with minimal and usually unconvincing attempt at cross fertilisation in this paper we investigate and exploit the relationship between the two area fo cusing on petri net unfolding which is an attractive reachability analysis method a it naturally enables the recognition and separate resolution of indepen dent subproblems on the one hand based on un folding we develop a new forward search method for cost optimal partial order planning which can be exponentially more efficient than state space search on the other hand inspired by well known planning heuristic we investigate the automatic generation of heuristic to guide unfolding result ing in a more efficient directed reachability analy si tool for petri net 
work in partial satisfaction planning psp ha hitherto assumed that goal are independent thus implying that they have additive utility value in many real world problem we cannot make this assumption in this paper we motivate the need for handling various type of goal utility dependence in psp we provide a framework for representing them using the general additive independence model and investigate two different approach to handle this problem compiling psp with utility dependency to integer programming extending forward heuristic search planning to handle psp goal dependency to guide the forward planning search we introduce a novel heuristic framework that combine costpropagation and integer programming to select beneficial goal to find an informative heuristic estimate the two implemented planner ipud and spud using the approach discussed above are compared empirically on several benchmark domain while ipud is more readily amendable to handle goal utility dependency and can provide bounded optimality guarantee spud scale much better 
in this article we introduce a new approach and several implementation to the task of question classification the approach extract structural information using machine learning technique and the pattern found are used to classify the question the approach fit in between the machine learning and handcrafting of regular expression a it wa done in the past and combine the best of both classifier can be generated automatically and the output can be investigated and manually optimised if needed 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
in this paper two family of merging operator are considered quota operator and gmin operator quota operator rely on a simple idea any possible world is viewed a a model of the result of the merging when it satisfies sufficiently many base from the given profile a multi set of base different interpretation of the sufficiently many give rise to specific operator each gmin operator is parameterized by a pseudo distance and each of them is intended to refine the quota operator i e to preserve more information quota and gmin operator are evaluated and compared along four dimension rationality computational complexity strategy proofness and discriminating power those two family are shown a interesting alternative to the formula based merging operator which selects some formula in the union of the base 
given sensor to detect object use commonsense prior of object usage in activity can reduce the need for labeled data in learning activity model it is often useful however to understand how an object is being used i e the action performed on it we show how to add personal sensor data e g accelerometer to obtain this detail with little labeling and feature selection overhead by synchronizing the personal sensor data with object use data it is possible to use easily specified commonsense model to minimize labeling overhead further combining a generative common sense model of activity with a discriminative model of action can automate feature selection on observed activity data automatically trained action classifier give precision recall on action adding action to pure object use improves precision recall from to over activity 
in probabilistic reasoning the problem of existence and identity are important to many different query for example the probability that something that fit some description exists the probability that some description refers to an object you know about or to a new object or the probability that an object fulfils some role many interesting query reduce to reasoning about the role of object being able to talk about the existence of part and sub part and the relationship between these part allows for probability distribution over complex description rather than trying to define a new language this paper show how the integration of multiple object ontology and role can be achieved cleanly this solves two main problem reasoning about existence and identity while preserving the clarity principle that specifies that probability must be over well defined proposition and the correspondence problem that mean that we don t need to search over all possible correspondence between object said to exist and thing in the world 
knowledge of the physical location of mobile device such a laptop or pda s is becoming increasingly important with the rise of location based service such a specialized web search navigation and social network application furthermore location information is a key foundation for high level activity inferencing in this paper we propose a novel technique for accurately estimating the location of mobile device and their wearer from wireless signal strength our technique estimate time varying device location on a spatial connectivity graph whose outdoor edge correspond to street and whose indoor edge represent hallway staircase elevator etc use of a hierarchical bayesian framework for learning a signal strength sensor model allows u not only to achieve higher accuracy than existing approach but to overcome many of their limitation in particular our technique is able to seamlessly integrate new access point into the model make use of negative information not detecting an access point and bootstrap a sensor model from sparse training data experiment demonstrate various property of our system 
stochastic shortest path problem ssps a subclass of markov decision problem mdps can be efficiently dealt with using real time dynamic programming rtdp yet mdp model are often uncertain obtained through statistic or guessing the usual approach is robust planning searching for the best policy under the worst model this paper show how rtdp can be made robust in the common case where transition probability are known to lie in a given interval 
this paper introduces and illustrates b log a formal language for defining probability model over world with unknown object and identity uncertainty blog unifies and extends several existing approach subject to certain acyclicity constraint every b log model specifies a unique probability distribution over first order model structure that can contain varying and unbounded number of object furthermore complete inference algorithm exist for a large fragment of the language we also introduce a probabilistic form of skolemization for handling evidence 
we investigate the computational complexity of a number of question relating to deductive argument system in particular the complexity of linking deductive and more abstract argument system we start by presenting a simple model of deductive argument based on propositional logic and define logical equivalence and defeat over individual argument we then extend logical equivalence to set of argument and show that the problem of checking equivalence of argument set is co np complete we also show that the problem of checking that an argument set contains no two logically equivalent argument is np complete while the problem of checking that a set of argument is maximal i e that no argument could be added without such an argument being logically equivalent to one that is already present is co np complete we then show that checking whether a digraph over an argument set is sound with respect to the defeat relation is co np complete while the problem of showing that such a digraph is complete is np complete and the problem of showing both soundness and completeness is dp complete 
intuitively one might expect that a seller s revenue from an auction weakly increase a the number of bidder grows a this increase competition however it is known that for combinatorial auction that use the vcg mechanism a seller can sometimes increase revenue by dropping bidder in this paper we investigate the extent to which this problem can occur under other dominant strategy combinatorial auction mechanism our main result is that such failure of revenue monotonicity are not limited to mechanism that achieve efficient allocation instead they can occur under any dominant strategy direct mechanism that set price using critical value and that always chooses an allocation that cannot be augmented to make some bidder better off while making none worse off 
we describe a system in which simple identical autonomous robot assemble two dimensional structure using prefabricated module a building block module are capable of some information processing enabling them to share longrange structural information and communicate it to robot this communication allows arbitrary solid structure to be rapidly built using a few fixed local robot behavior module are identical in shape but may be functionally distinct with constraint governing the location of different class we present algorithm for assembly of solid structure of any shape both when the layout of module class is fully specified in advance and when functional constraint are satisfied during the building process allowing for adaptive structure this approach demonstrates a decentralized autonomous flexible simple and adaptive approach to construction 
we describe a novel approach to incomplete information board game which is based on the concept of metaposition a the merging of a very large set of possible game state into a single entity which contains at least every state in the current information set this merging operation allows an artificial player to apply traditional perfect information game theory tool such a the minimax theorem we apply this technique to the game of kriegspiel a variant of chess characterized by strongly incomplete information a player cannot see their opponent s piece but can only try to guess their position by listening to the message of a referee we provide a general representation of kriegspiel state through metaposition tree and describe a weighed maximax algorithm for evaluating metapositions we have tested our approach competing against both human and computer player 
we present a general framework for augmenting instance of the disjunctive temporal problem dtp with finite domain constraint in this new formalism the bound of the temporal constraint become conditional on the finite domain assignment this hybridization make it possible to reason simultaneously about temporal relationship between event a well a their nontemporal property we provide a special case of this hybridization that allows reasoning about a limited form of spatial constraint namely the travel time induced by the location of a set of activity we develop a least commitment algorithm for efficiently finding solution to this combined constraint system and provide empirical result demonstrating the effectiveness of our approach 
we discus the problem of finding a good state representation in stochastic system with observation we develop a duality theory that generalizes existing work in predictive state representation a well a automaton theory we discus how this theoretical framework can be used to build learning algorithm approximate planning algorithm a well a to deal with continuous observation 
in this paper we focus on the identification of biomedical abstract related to protein protein interaction we propose a novel feature representation contextual bag of word to exploit named entity information our method outperforms well known method that use named entity information a additional feature furthermore we have improved the performance by extracting reliable and informative instance from unlabeled and likely positive data to provide additional training data 
multi issue negotiation protocol have been studied widely and represent a promising field since most negotiation problem in the real world involve interdependent multiple issue the vast majority of this work ha assumed that negotiation issue are independent so agent can aggregate the utility of the issue value by simple summation producing linear utility function in the real world however such aggregation are often unrealistic we cannot for example just add up the value of car s carburetor and the value of car s engine when engineer negotiate over the design a car these value of these choice are interdependent resulting in nonlinear utility function in this paper we address this important gap in current negotiation technique we propose a negotiation protocol where agent employ adjusted sampling to generate proposal and a bidding based mechanism is used to find social welfare maximizing deal our experimental result show that our method substantially outperforms existing method in large non linear utility space like those found in real world context 
dynamic programming algorithm provide a basic tool identifying optimal solution in markov decision process mdp the paper develops a representation for decision diagram suitable for describing value function transition probability and domain dynamic of first order or relational mdps fomdp by developing appropriate operation for such diagram the paper show how value iteration can be performed compactly for such problem this improves on previous approach since the representation combine compact form with efficient operation the work also raise interesting issue on suitability of different representation to different fomdps task 
quantitative temporal constraint are an essential requirement for many planning domain the htn planning paradigm ha proven to be better suited than other approach to many application to date however efficiently integrating temporal reasoning with htn planning ha been little explored this paper describes a mean to exploit the structure of a htn plan in performing temporal propagation on an associated simple temporal network by exploiting the natural restriction on permitted temporal constraint the time complexity of propagation can be sharply reduced while completeness of the inference is maintained empirical result indicate an order of magnitude improvement on real world plan 
a description logic knowledge base is constituted by two component called tbox and abox where the former express general knowledge about the concept and their relationship and the latter describes the property of instance of concept we address the problem of how to deal with change to a description logic knowledge base when these change affect only it abox we consider two type of change namely update and erasure and we characterize the semantics of these operation on the basis of the approach proposed by winslett and by katsuno and mendelzon it is well known that in general description logic are not closed with respect to update in the sense that the set of model corresponding to an update applied to a knowledge base in a description logic l may not be expressible by aboxes in l we show that this is true also for erasure to deal with this problem we introduce the notion of best approximation of an update erasure in a dl l with the goal of characterizing the l aboxes that capture the update erasure at best we then focus on dl litef a tractable description logic and present polynomial algorithm for computing the best approximation of update and erasure in this logic which show that the nice computational property of dl litef are retained in dealing with the evolution of the abox 
error bound for decision tree are generally based on depth or breadth of the tree in this paper we propose a bound for error rate that depends both on the depth and the breadth of a specific decision tree constructed from the training sample this bound is derived from sample complexity estimate based on pac learnability the proposed bound is compared with other traditional error bound on several machine learning benchmark data set a well a on an image data set used in content based image retrieval cbir experimental result demonstrate that the proposed bound give tighter estimation of the empirical error 
the paper describes a simple but effective framework for visual object tracking in video sequence the main contribution of this work lie in the introduction of a case based reasoning cbr method to maintain an accurate target model automatically and efficiently under significant appearance change without drifting away specifically an automatic case base maintenance algorithm is proposed to dynamically update the case base manage the case base to be competent and representative and to maintain the case base in a reasonable size for real time performance furthermore the method can provide an accurate confidence measurement for each tracked object so that the tracking failure can be identified in time under the framework a real time face tracker is built to track human face robustly under various face orientation significant facial expression and illumination change 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
a recently proposed machine learning approach to reference resolution the twin candidate approach ha been shown to be more pormising than the traditional single candidate approach this paper present a pronoun interpretation system that extends the twin candidate framework by equippmg it with the ability to identify non referential pronoun training different model for handling different type of pronoun and incorporating linguistic knowledge source that are generally not employed in traditional pronoun resolvers the resulting system when evaluated on a standard coreference corpus outpreforms not only the original twin candidate approach but also a state of the art pronoun resolver 
people have faced conflict for shifting scheduled meeting to other time slot in order to fit incoming meeting or to find a time slot to book a meeting the goal of this research is to develop a personal distributed meeting scheduler in fcvw federated collaborative virtual workspace that assist user to deal with these situation fcvw is an extension of cvw developed by mitre in our approach to meeting scheduling is to provide each user with meeting scheduler agent each agent is able to manage negotiate and schedule task meeting event appointment for it user this paper describes the objective of our research to develop an automated distributed meeting scheduler 
we present a bounded policy iteration algorithm for infinite horizon decentralized pomdps policy are represented a joint stochastic finite state controller which consist of a local controller for each agent we also let a joint controller include a correlation device that allows the agent to correlate their behavior without exchanging information during execution and show that this lead to improved performance the algorithm us a fixed amount of memory and each iteration is guaranteed to produce a controller with value at least a high a the previous one for all possible initial state distribution for the case of a single agent the algorithm reduces to poupart and boutilier s bounded policy iteration for pomdps 
in this paper we investigate method to detect and repair concavity in roc curve by manipulating model prediction the basic idea is that if a point or a set of point lie below the line spanned by two other point in roc space we can use this information to repair the concavity this effectively build a hybrid model combining the two better model with an inversion of the poorer model in the case of ranking classifier it mean that certain interval of the score are identified a unreliable and candidate for inversion we report very encouraging result on uci data set particularly for naive bayes where the use of two validation fold yielded significant improvement on more than half of them with only one loss 
today the demand of service for comic content increase because paper magazine and book are bulky while digital content can be read anytime and anywhere with cellular phone and pda to convert existing print comic material into digital format such that they can be read using the cellular phone and the pda with small screen it is necessary to divide each page into scene frame and to determine reading order of the scene frame the division of comic image into the scene frame can be considered a a type of document layout analysis we analyzed layout of comic image using density gradient the method can be applied to comic in which comic balloon or picture are drawn over scene frame in this research a method for detecting the scene frame division in comic image using the density gradient after filling the quadrangle region in each image with black is proposed experimental result show that percent of page in four print comic booklet are successfully divided into scene frame by the proposed method 
learning word sense class ha been shown to be useful in fine grained word sense disambiguation kohomban and lee however the common choice for sense class wordnet lexicographer file are not designed for machine learning based word sense disambiguation in this work we explore the use of clustering technique in an effort to construct sense class that are more suitable for word sense disambiguation end task our result show that these class can significantly improve classifier performance over the state of the art result of unrestricted word sense disambiguation 
local search is widely applied to satisfiable sat problem and on some problem class outperforms backtrack search an intriguing challenge posed by selman kautz and mcallester in is to use it instead to prove unsatisfiability we design a greedy randomised resolution algorithm called ranger that will eventually refute any unsatisfiable instance while using only bounded memory ranger can refute some problem more quickly than systematic resolution or backtracking with clause learning we believe that non systematic but greedy inference is an interesting research direction for powerful proof system such a general resolution 
partially observed action are observation of action execution in which we are uncertain about the identity of object agent or location involved in the action e g we know that action move o x y occurred but do not know o y observed action reasoning is the problem of reasoning about the world state after a sequence of partial observation of action and state in this paper we formalize observed action reasoning prove intractability result for current technique and find tractable algorithm for strip and other action our new algorithm update a representation of all possible world state the belief state in logic using new logical constant for unknown object a straightforward application of this idea is incorrect and we identify and add two key amendment we also present successful experimental result for our algorithm in block world domain of varying size and in kriegspiel partially observable chess these result are promising for relating sensor with symbol partial knowledge game multi agent decision making and ai planning 
we describe method to solve partially observable markov decision process pomdps with continuous or large discrete observation space realistic problem often have rich observation space posing significant problem for standard pomdp algorithm that require explicit enumeration of the observation this problem is usually approached by imposing an a priori discretisation on the observation space which can be sub optimal for the decision making task however since only those observation that would change the policy need to be distinguished the decision problem itself induces a lossless partitioning of the observation space this paper demonstrates how to find this partition while computing a policy and how the resulting discretisation of the observation space reveals the relevant feature of the application domain the algorithm are demonstrated on a toy example and on a realistic assisted living task 
there are a number of framework for modelling argumentation in logic they incorporate a formal representation of individual argument and technique for comparing conflicting argument a common assumption for logic based argumentation is that an argument is a pair where is minimal subset of the knowledgebase such that is consistent and entail the claim however real argument i e argument presented by human usually do not have enough explicitly presented premise for the entailment of the claim this is because there is some common knowledge that can be assumed by a proponent of an argument and the recipient of it this allows the proponent of an argument to encode an argument into a real argument by ignoring the common knowledge and it allows a recipient of a real argument to decode it into an argument by drawing on the common knowledge if both the proponent and recipient use the same common knowledge then this process is straightforward unfortunately this is not always the case and raise the need for an approximation of the notion of an argument for the recipient to cope with the disparity between the different view on what constitutes common knowledge 
we investigate the modelling of workflow plan and other event generating process a discrete event source and reason about the possibility of having event sequence ending in undesirable state in previous research the problem is shown to be np complete even if the number of event to occur is fixed in advance in this paper we consider possible event sequence of indefinite length and show that many interesting case of such reasoning task are solvable in polynomial time 
many inductive logic programming system have operator reorganizing the program so far inferred such a the intra construction operator of cigol at the same time there is a similar reorganizing operator called the folding rule developed in program transformation we argue that there are advantage in using an extended folding rule a a reorganizing operator for inductive inference system such an extended folding rule allows an inductive inference system not only to recognize already learned concept but also to increase the efficiently of execution of inferred program 
image alignment refers to finding the best transformation from a fixed reference image to a new image of a scene this process is often guided by similarity measure between image computed based on the image data however in time critical application state of the art method for computing similarity are too slow instead of using all the image data to compute similarity one can use a subset of pixel to improve the speed but often this come at the cost of reduced accuracy this make the problem of image alignment a natural application domain for deliberation control using anytime algorithm however almost no research ha been done in this direction in this paper we present anytime version for the computation of two common image similarity measure mean squared difference and mutual information off line we learn a performance profile specific to each measure which is then used on line to select the appropriate amount of pixel to process at each optimization step when tested against existing technique our method achieves comparable quality and robustness with significantly le computation 
in this paper we present a generalization of the problem of interactive configuration the usual interactive configuration problem is the problem of given some variable on small finite domain and an increasing set of assignment of value to a subnet of the variable to compute for each of the unassigned variable which value in it domain that participate in some solution for some assignment of value to the other unassigned variable in this paper we consider how to extend this scheme to handle infinite regular domain using string variable and constraint that involves regular expression check on the string variable we first show how to do this by using one single dfa since this approach is vastly space consuming we construct a data structure that simulates the large dfa and is much more space efficient a an example a configuration problem on n string variable with only one solution in which each string variable is assigned a value oflength k the former structure will use kn space whereas the latter only need o kn we also show how this framework can be combined with the recent bdd technique to allow both boolean integer and string variable in the configuration problem 
arc consistency algorithm are widely used to prune the search space of constraint satisfaction problem csps coarse grained arc consistency algorithm like ac ac d and ac are efficient when it come to transforming a csp to it arc consistent equivalent these algorithm repeatedly carry out revision revision require support check for identifying and deleting all unsupported value from the domain of a variable in revision for difficult problem most value have some support indeed most revision are ineffective i e they cannot delete any value and consume a lot of check and time we propose two solution to overcome these problem first we introduce the notion of a support condition sc which guarantee that a value ha some support sc reduce support check while maintaining arc consistency during search second we introduce the notion of a revision condition rc which guarantee that all value have support a rc avoids a candidate revision and queue maintenance overhead for random problem sc reduce the check required by mac mac up to rcs avoid at least of the total revision combining the two result in reducing of the solution time 
current question answering task handle definitional question by seeking answer which are factual in nature while factual answer are a very important component in defining entity a wealth of qualitative data is often ignored in this incipient work we define qualitative dimension credibility sentiment contradiction etc for evaluating answer to definitional question and we explore potential benefit to user these qualitative dimension are leveraged to uncover indirect and implicit answer and can help satisfy the user s information need 
we present integration mechanism for combining heterogeneous component in a situated information processing system illustrated by a cognitive robot able to collaborate with a human and display some understanding of it surroundings these mechanism include an architectural schema that encourages parallel and incremental information processing and a method for binding information from distinct representation that when faced with rapid change in the world can maintain a coherent though distributed view of it provisional result are demonstrated in a robot combining vision manipulation language planning and reasoning capability interacting with a human and manipulable object 
coordinating a group of robot to work in formation ha been suggested for a number of task such a urban search and rescue traffic control and harvesting solar energy algorithm for controlling robot formation have been inspired by biological and organizational system in our approach to robot formation control each robot is treated like a cell in a cellular automaton where local interaction between robot result in a global organization the algorithm ha been demonstrated in simulation in this paper we present a physical implementation 
interaction between heterogeneous agent can raise some problem since agent may not use the same model and concept therefore the use of some mechanism to achieve interoperability between model allows agent to interact in this paper we consider the case of reputation model by describing an experience of using several existing technology to allow agent to interoperate when they use reputation notion value during interaction for this purpose we have implemented agent on the art testbed and we make them use a functional ontology of reputation which wa developed to allow the interoperability among reputation model 
recently strong equivalence for answer set programming ha been studied intensively and wa shown to be beneficial for modular programming and automated optimization in this paper we define the novel notion of strong equivalence for logic program with preference based on this definition we give for several semantics for preference handling necessary and sufficient condition for program to be strongly equivalent these result provide a clear picture of the relationship of these semantics with respect to strong equivalence which differs considerably from their relationship with respect to answer set finally based on these result we present for the first time simplification method for logic program with preference 
the load n limit lnl and release n sequence rn module are rule based sub system designed to validate various configuration of aircraft store loading and weapon release planning lnl in conjunction with verifying and validating the load presented return usage restriction and limitation of the aircraft and the weapon load the rn module evaluates user selected weapon employment planning between the two module the planner can create legal and safe load determine flight restriction create safe weapon release sequence and determine weapon delivery restriction throughout the entire flight scenario this paper is a description of the two module the development environment system growth change management problem and the scope of change incorporated between the first release of weapon employment planning software weps in and the latest certified version 
in general knowledge intensive data mining method exploit background knowledge to improve the quality of their result then in knowledge rich domain often the interestingness of the mined pattern can be increased significantly in this paper we categorize several class of background knowledge for subgroup discovery and present how the necessary knowledge element can be modelled furthermore we show how subgroup discovery method benefit from the utilization of background knowledge and discus it application in an incremental process model the context of our work is to identify interesting diagnostic pattern to supplement a medical documentation and consultation system we provide a case study in the medical domain using a case base from a realworld application 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
aggregating the preference of self interested agent is a key problem for multiagent system and one general method for doing so is to vote over the alternative candidate unfortunately the gibbard satterthwaite theorem show that when there are three or more candidate all reasonable voting rule are manipulable in the sense that there exist situation in which a voter would benefit from reporting it preference insincerely to circumvent this impossibility result recent research ha investigated whether it is possible to make finding a beneficial manipulation computationally hard this approach ha had some limited success exhibiting rule under which the problem of finding a beneficial manipulation is np hard p hard or even pspace hard thus under these rule it is unlikely that a computationally efficient algorithm can be constructed that always find a beneficial manipulation when it exists however this still doe not preclude the existence of an efficient algorithm that often find a successful manipulation when it exists there have been attempt to design a rule under which finding a beneficial manipulation is usually hard but they have failed to explain this failure in this paper we show that it is in fact impossible to design such a rule if the rule is also required to satisfy another property a large fraction of the manipulable instance are both weakly monotone and allow the manipulator to make either of exactly two candidate win we argue why one should expect voting rule to have this property and show experimentally that common voting rule clearly satisfy it we also discus approach for potentially circumventing this impossibility result 
we describe a strong connection between maximally satisfiable and minimally unsatisfiable subset of constraint system using this relationship we develop a two phase algorithm employing powerful constraint satisfaction technique for the identification of conflicting set of constraint in infeasible constraint system we apply this technique to overconstrained instance of the disjunctive temporal problem dtp an expressive form of temporal constraint satisfaction problem using randomly generated benchmark we provide experimental result that demonstrate how the algorithm scale with problem size and constraint density 
the predictive linear gaussian model or plg improves upon traditional linear dynamical system model by using a predictive representation of state which make consistent parameter estimation possible without any loss of modeling power and while using fewer parameter this work extends the plg to model non linear dynamical system through the use of a kernelized nonlinear mixture technique the resulting generative model ha been named the mplg for mixture of plgs we also develop a novel technique to perform inference in the model which consists of a hybrid of sigma point approximation and analytical statistic we show that the technique lead to fast and accurate approximation and that it is general enough to be applied in other context we empirically explore the mplg and demonstrate it viability on several real world and synthetic task 
the proliferation of wireless network ha underscored the need for system capable of coping with sporadic network connectivity the restriction of communication to neighboring host make determining the global state especially difficult if not impractical this paper address the problem of coordinating the position of an arbitrary number of service encapsulated by mobile agent in a dynamic peer to peer network the agent collective goal is to minimize the distance between host and service even if the topology is changing constantly we propose a distributed algorithm to efficiently calculate the stationary distribution of the network this can be used a a hill climbing heuristic for agent to find near optimal location at which to provide service finally we show that the agent based hill climbing approach is temporally stable relative to the instantaneous optimum 
we define reconsideration a non prioritized belief change operation on a finite set of base belief reconsideration is a hindsight belief change repair that eliminates negative effect caused by the order of previously executed belief change operation belief that had previously been removed are returned to the base if there no longer are valid reason for their removal this might result in le preferred belief being removed and additional belief being returned the end product is an optimization of the belief base converting the result of a series of revision to the very base that would have resulted from a batch revision performed after all base belief were entered added reconsideration can be done by examining the entire set of all base belief both currently believed and retracted or if the believed base is consistent by examining all retracted belief for possible return this however is computationally expensive we present a more efficient tm friendly algorithm dependency directed reconsideration ddr which can produce the same result by examining only a dynamically determined subset of base belief that are actually affected by change made since the last base optimization process ddr is an efficient anytime belief base optimizing algorithm that eliminates operation order effect 
we propose a novel approach to the problem of simultaneous localization and mapping slam based on incremental smoothing that is suitable for real time application in large scale environment the main advantage over filter based algorithm are that we solve the full slam problem without the need for any approximation and that we do not suffer from linearization error we achieve efficiency by updating the square root information matrix a factored version of the naturally sparse smoothing information matrix we can efficiently recover the exact trajectory and map at any given time by back substitution furthermore our approach allows access to the exact covariance a it doe not suffer from under estimation of uncertainty which is another problem inherent to filter we present simulation based result for the linear case showing constant time update for exploration task we further evaluate the behavior in the presence of loop and discus how our approach extends to the non linear case finally we evaluate the overall non linear algorithm on the standard victoria park data set 
in this paper we describe the knowledge based recommender application fsadvisor financial service advisor which assist sale representative in determining personalized financial service portfolio for their customer commercially introduced in fsadvisor is licensed to a number of major financial service provider in austria it support the dialog between a sale representative and a customer by guaranteeing the consistency and appropriateness of proposed solution identifying additional selling opportunity and by providing intelligent explanation for solution in the financial service domain especially in the retail sector sale representative can differ greatly in their expertise and level of knowledge therefore financial service provider ask for tool effectively supporting sale representative in the dialog with the customer knowledge based recommender approach meet these requirement by allowing an intuitive and flexible mapping of marketing and sale knowledge to the representation of a recommender knowledge base in fsadvisor we integrate model based diagnosis constraint satisfaction and personalization thus supporting customer oriented sale dialog a graphical development environment enables the implementation of financial service knowledge base for non programmer which lead to significant reduction of development and maintenance cost 
we consider the problem of rewriting a query efficiently using materialized view in the context of information integration this problem ha received significant attention in the scope of emerging infrastructure such a www semantic web grid and p p which require efficient algorithm the problem is in general intractable and the current algorithm do not scale well when the number of view or the size of the query grow we show however that this problem can be encoded a a propositional theory in cnf such that it model are in correspondence with the rewriting of the query the theory is then compiled into a normal form that is called d dnnf and support several operation like model counting and enumeration in polynomial time in the size of the compiled theory for computing the rewriting although this method is also intractable in the general case it is not necessarily so in all case we have developed along these line and from off the shelf propositional engine novel algorithm for finding maximally contained rewriting of the query given the set of accessible resource view the algorithm scale much better than the current state of the art algorithm the minicon algorithm over a large number of benchmark and show in some case improvement in performance of a couple order of magnitude 
in this paper we present a multi agent control method for a large scale network system we propose an extension of a token based coordination technique to improve the tradeoff between two conflicting objective of the network system reducing the lead time and increasing throughput in our system cab information about an agent s urgency of job to fulfill demanded throughput and to maintain it utilization is passed from downstream agent in the network so that upstream agent can provide necessary and sufficient job to bottleneck agent whose loss of capacity degrades the total system performance we empirically evaluate cab performance using a benchmark problem of the semiconductor fabrication process which is a good example of a large scale network system 
we describe an approach to statistically verifying complex controller this approach is based on deriving practical vapnik chervonenkis style vc generalization bound for binary classifier with weighted loss an important case is deriving bound on the probability of false positive we show how existing method to derive bound on classification error can be extended to derive similar bound on the probability of false positive a well a bound in a decision theoretic setting that allows tradeoff between false negative and false positive we describe experiment using these bound in statistically verifying computational property of an iterative controller for an organic air vehicle oav 
in peer to peer inference system each peer can reason locally but also solicit some of it acquaintance sharing part of it vocabulary this paper study both theoretically and experimentally the problem of computing proper prime implicates for propositional peer to peer system the global theory union of all peer theory of which is not known a opposed to partition based reasoning 
a popular approach to cost sensitive learning is to rescale the class according to their misclassification cost although this approach is effective in dealing with binary class problem recent study show that it is often not so helpful when being applied to multi class problem directly this paper analyzes that why the traditional rescaling approach is often helpless on multi class problem which reveals that before applying rescaling the consistency of the cost must be examined based on the analysis a new approach is presented which should be the choice if the user want to use rescaling for multi class cost sensitive learning moreover this paper show that the proposed approach is helpful when unequal misclassification cost and class imbalance occur simultaneously and can also be used to tackle pure class imbalance learning thus the preposed approach provides a unified framework for using rescaling to address multi class cost sensitive learning a well a multi class class imbalance learning 
cycsecure is a network risk assessment and network monitoring application that relies on knowledge based artificial intelligence technology to improve on traditional network vulnerability assessment cycsecure integrates public report of software fault from online database data gathered automatically from computer on a network and hand ontologized information about computer and computer network this information is stored in the cyc knowledge base kb and reasoned about by the cyc inference engine and planner to provide detailed analysis of the security and vulnerability of network 
buyer and seller in online auction are faced with the task of deciding who to entrust their business to based on a very limited amount of information current trust rating on ebay average over positive and are presented a a single number on a user s profile this paper present a system capable of extracting valuable negative information from the wealth of feedback comment on ebay computing personalized and feature based trust and presenting this information graphically 
recent research in nonmonotonic logic programming under the answer set semantics focus on different notion of program equivalence however previous result do not address the important class of stratified program and it subclass of acyclic i e recursion free program although they are recognized a important tool for knowledge representation and reasoning in this paper we consider such program possibly augmented with constraint our result show that in the propositional setting where reasoning is well known to be polynomial deciding strong and uniform equivalence is a hard a for arbitrary normal logic program and thus conp complete but is polynomial in some restricted case nonground program behave similarly however exponential lower bound already hold for small program i e with constantly many rule in particular uniform equivalence is undecidable even for small horn program plus a single negative constraint 
example of figurative language can range from the explicit and the obvious to the implicit and downright enigmatic some simpler form like simile often wear their meaning on their sleeve while more challenging form like metaphor can make cryptic allusion more akin to those of riddle or crossword puzzle in this paper we argue that because the same concept and property are described in either case a computational agent can learn from the easy case explicit simile how to comprehend and generate the hard case nonexplicit metaphor we demonstrate that the markedness of simile allows for a large case base of illustrative example to be easily acquired from the web and present a system called sardonicus that us this case base both to understand property attribution metaphor and to generate apt metaphor for a given target on demand in each case we show how the text of the web is used a a source of tacit knowledge about what categorization are allowable and what property are most contextually appropriate overall we demonstrate that by using the web a a primary knowledge source a system can achieve a robust and scalable competence with metaphor while minimizing the need for handcrafted resource like wordnet 
we present a fault tolerant formula interpreter that aim at finding plausibly intended formally correct and contextually meaningful specification from user statement containing formal inaccuracy 
we study the effect of problem structure on the practical performance of optimal dynamic programming for decentralized decision problem it is shown that restricting agent influence over problem dynamic can make the problem easier to solve experimental result establish that agent influence correlate with problem difficulty a the gap between the influence of different agent grows problem tend to become much easier to solve the measure thus provides a general purpose automatic characterization of decentralized problem identifying those for which optimal method are more or le likely to work such a measure is also of possible use a a heuristic in the design of algorithm that create task decomposition and control hierarchy in order to simplify multiagent problem 
a formal structural representation of speech is presented in this paper the representation is developed within the evolving transformation system ets formalism and encapsulates speech process at the articulatory level we show how the class structure of several consonantal phoneme of english can be expressed via articulatory gesture element of these class can be detected in a corresponding structural representation of continuous speech our experimental result on the mocha articulatory corpus support the hypothesis that the proposed articulatory representation capture sufficient information for the accurate structural identification of phonemic class 
we describe a recommender system which us a unique combination of content based and collaborative method to suggest item of interest to user and also to learn and exploit item semantics recommender system typically use technique from collaborative filtering in which proximity measure between user are formulated to generate recommendation or content based filtering in which user are compared directly to item our approach us similarity measure between user but also directly measure the attribute of item that make them appealing to specific user this can be used to directly make recommendation to user but equally importantly it allows these recommendation to be justified we introduce a method for predicting the preference of a user for a movie by estimating the user s attitude toward feature with which other user have described that movie we show that this method allows for accurate recommendation for a sub population of user but not for the entire user population we describe a hybrid approach in which a user specific recommendation mechanism is learned and experimentally evaluated it appears that such a recommender system can achieve significant improvement in accuracy over alternative method while also retaining other advantage 
agent often have to construct plan that obey deadline or more generally resource limit for real valued resource whose consumption can only be characterized by probability distribution such a execution time or battery power these planning problem can be modeled with continuous state markov decision process mdps but existing solution method are either inefficient or provide no guarantee on the quality of the resulting policy we therefore present cph a novel solution method that solves the planning problem by first approximating with any desired accuracy the probability distribution over the resource consumption with phasetype distribution which use exponential distribution a building block it then us value iteration to solve the resulting mdps by exploiting property of exponential distribution to calculate the necessary convolution accurately and efficiently while providing strong guarantee on the quality of the resulting policy our experimental feasibility study in a mar rover domain demonstrates a substantial speedup over lazy approximation which is currently the leading algorithm for solving continuous state mdps with quality guarantee 
we propose a novel variant of the conjugate gradient algorithm kernel conjugate gradient kcg designed to speed up learning for kernel machine with differentiable loss function this approach lead to a better conditioned optimization problem during learning we establish an upper bound on the number of iteration for kcg that indicates it should require le than the square root of the number of iteration that standard conjugate gradient requires in practice for various differentiable kernel learning problem we find kcg consistently and significantly outperforms existing technique the algorithm is simple to implement requires no more computation per iteration than standard approach and is well motivated by reproducing kernel hilbert space rkhs theory we further show that data structure technique recently used to speed up kernel machine approach are well matched to the algorithm by reducing the dominant cost of training function evaluation and rkhs inner product computation 
a fundamental difficulty in recognizing human activity is obtaining the labeled data needed to learn model of those activity given emerging sensor technology however it is possible to view activity data a a stream of natural language term activity model are then mapping from such term to activity name and may be extracted from text corpus such a the web we show that model so extracted are sufficient to automatically produce labeled segmentation of activity data with an accuracy of over activity well above the baseline the segmentation so obtained is sufficient to bootstrap learning with accuracy of learned model increasing to to our knowledge this is the first human activity inferencing system shown to learn from sensed activity data with no human intervention per activity learned even for labeling 
the association of perception and action is key to learning by observation in general and to program level task imitation in particular the question is how to structure this information such that learning is tractable for resource bounded agent by introducing a combination of symbolic representation with bayesian reasoning we demonstrate both theoretical and empirical improvement to a general purpose imitation system originally based on a model of infant social learning we also show how prior task knowledge and selective attention can be rigorously incorporated via loss matrix and automatic relevance determination respectively 
e connection are a robust framework for combining in a decidable way several family of decidable logic including description logic dl modal logic and many logic of time and space e connection have also proved to be useful for supporting modular distributed modeling such a is becoming common on the semantic web in this paper we present an extension toe connection of dl that provides more flexibility in the way link property can be defined and used in a combination of ontology we also provide mean for defining transitive relation across domain and for simulating some of the expressivity of the transitive closure operator finally we provide a tableau based decision procedure for two relevante connection language involving the influential dlsshiq shoq andshio which are at the basis of the web ontology language owl 
planning under uncertainty involves two distinct source of uncertainty uncertainty about the effect of action and uncertainty about the current state of the world the most widely developed model that deal with both source of uncertainty is that of partially observable markov decision process pomdps simplifying pomdps by getting rid of the second source of uncertainty lead to the well known framework of fully observable mdps getting rid of the first source of uncertainty lead to a le widely studied framework namely decision process where action cannot change the state of the world and are only intended to bring some information about the static state of the world such purely epistemic process are very relevant since many practical problem such a diagnosis database querying or preference elicitation fall into this class however it is not known whether this specific restriction of pomdp is computationally simpler than pomdps in this paper we establish several complexity result for purely epistemic mdps emdps we first show that short horizon policy existence in emdps is pspace complete then we focus on the specific case of emdps with reliable observation and show that in this case policy existence is only np complete however we show that this problem cannot be approximated with a bounded performance ratio by a polynomial time algorithm 
often remote investigation use autonomous agent to observe an environment on behalf of absent scientist predictive exploration improves these system efficiency with onboard data analysis agent can learn the structure of the environment and predict future observation reducing the remote exploration problem to one of experimental design in our formulation information gain over a map guide exploration decision while a similar criterion suggests the most informative data product for downlink ongoing work will develop appropriate model for surface exploration by planetary robot experiment will demonstrate these algorithm on kilometer scale autonomous geology task 
planning with concurrent durative action and probabilistic effect or probabilistic temporal planning is a relatively new area of research the challenge is to replicate the success of modern temporal and probabilistic planner with domain that exhibit an interaction between time and uncertainty we present a general framework for probabilistic temporal planning in which effect the time at which they occur and action duration are all probabilistic this framework includes a search space that is designed for solving probabilistic temporal planning problem via heuristic search an algorithm that ha been tailored to work with it and an effective heuristic based on an extension of the planning graph data structure prottle is a planner that implement this framework and can solve problem expressed in an extension of pddl 
planning a satisfiability is one of the most well known and effective technique for classical planning satplan ha been the winning system in the deterministic track for optimal planner in the th international planning competition ipc and a co winner in the th ipc in this paper we extend the planning a satisfiability approach in order to handle preference and satplan in order to solve problem with simple preference the resulting system satplan p is competitive with sgplan the winning system in the category simple preference at the last ipc further we show that satplan p performance are almost always comparable to those of satplan when solving the same problem without preference in other word introducing simple preference in satplan doe not affect it performance this latter result is due both to the particular mechanism we use in order to incorporate preference in sat plan and to the relative low number of soft goal each corresponding to a simple preference usually present in planning problem indeed if we consider the issue of determining minimal plan corresponding to problem with thousand of preference the performance of satplan p are comparable to those of satplan in many case but can be significantly worse when the number of preference is very high compared to the total number of variable in the problem our analysis is conducted considering both qualitative and quantitative preference different reduction from quantitative to qualitative one and most of the propositional planning domain from the ipcs and that satplan can handle 
can we assign attitude to a computer based on it beep if so which kind of beep are perceived a specific attitude such a disagreement hesitation or agreement to examine this issue i carried out an experiment to observe how participant perceive or assign an attitude to a computer according to beep of different duration and f contour s slope the result revealed that beep with increasing intonation regardless of duration were perceived by participant a disagreement flat sound with longer duration were interpreted a hesitation and decreasing intonation with shorter duration were a agreement 
we enhance machine learning algorithm for text categorization with generated feature based on domain specific and common sense knowledge this knowledge is represented using publicly available ontology that contain hundred of thousand of concept such a the open directory these ontology are further enriched by several order of magnitude through controlled web crawling prior to text categorization a feature generator analyzes the document and map them onto appropriate ontology concept which in turn induce a set of generated feature that augment the standard bag of word feature generation is accomplished through contextual analysis of document text implicitly performing word sense disambiguation coupled with the ability to generalize concept using the ontology this approach address the two main problem of natural language processing synonymy and polysemy categorizing document with the aid of knowledge based feature leverage information that cannot be deduced from the document alone experimental result confirm improved performance breaking through the plateau previously reached in the field 
this paper present a novel approach to improve the named entity translation by combining a transliteration approach with web mining using web information a a source to complement transliteration and using transliteration information to guide and enhance web mining a maximum entropy model is employed to rank translation candidate by combining pronunciation similarity and bilingual contextual co occurrence experimental result show that our approach effectively improves the precision and recall of the named entity translation by a large margin 
in this paper we discus several representation issue that we came across while modelling molecular interaction in cell of living organism one of the issue wa that the triggering of event inside cell an important modelling component are not necessarily immediate leading to multiple evolution model in the absence of additional information second often an action or a trigger at one level of granularity of representation can be elaborated and refined we show the problem that existing representation and modelling formalism have in dealing with the above issue we then present an action language which build up on a previous language and ha the ability to express event ordering knowledge we show that our language is able to adequately address the above mentioned issue 
a recent special issue of ai magazine aaai wa dedicated to the topic of semantic integration the problem of sharing data across disparate source at the core of the solution lie the discovery the semantics of different data source ideally the semantics of data are captured by a formal ontology of the domain together with a semantic mapping connecting the schema describing the data to the ontology however establishing the semantic mapping from a database schema to a formal ontology in term of formal logic expression is inherently difficult to automate so the task wa left to human in this paper we report on our study an borgida mylopoulos a b of a semi automatic tool called maponto that assist user to discover plausible semantic relationship between a database schema relational or xml and an ontology expressing them a logical formula rule 
several distributed constraint reasoning algorithm employ depth first search dfs tree on the constraint graph that span involved agent in this article we show that it is possible to dynamically detect a minimal dfs tree compatible with the current order on agent during the distributed constraint reasoning process of the adopt algorithm this also allows for shorter dfs tree during the initial step of the algorithm while some constraint did not yet prove useful given visited combination of assignment earlier distributed algorithm for finding spanning tree on agent did not look to maintain compatibility with an order already used we also show that announcing a nogood to a single optional agent is bringing significant improvement in the total number of message the dynamic detection of the dfs tree brings improvement in simulated time 
with the advent of compositional programming model in computer science applying planning technology to automatically build workflow for solving large and complex problem in such a paradigm becomes not only technically appealing but also feasible approach the application area that will benefit from automatic composition include among others web service grid computing and stream processing system although the classical planning formalism is expressive enough to describe planning problem that arise in a large variety of different application it can pose significant limitation on planner performance in compositional application in particular in stream processing system in this paper we exlend the classical planning formalism by introducing new language construct that support the structure of stream processing domain exposing this structure to the planner can result in dramatic performance improvement our experiment show exponential planning time reduction in comparison to most recent metric planner 
current word sense disambiguation wsd system based on supervised learning are still limited in that they do not work well for all word in a language one of the main reason is the lack of sufficient training data in this paper we investigate the use of unlabeled training data for wsd in the framework of semi supervised learning four semisupervised leaming algorithm are evaluated on noun of senseval se english lexical sample task and se english all word task empirical result show that unlabeled data can bring significant improvement in wsd accuracy 
description logic and in particular the web ontology language owl ha been proposed a an appropriate basis for computing match between structured object for the sake of information integration and service discovery a drawback of the direct use of subsumption a a matching criterion is the inability to compute partial match and qualify the degree of mismatch in this paper we describe a method for overcoming these problem that is based on approximate logical reasoning in particular we approximate the subsumption relation by defining the notion of subsumption with respect to a certain subset of the concept and relation name we present the formal semantics of this relation describe a sound and complete algorithm for computing approximate subsumption and discus it application to matching task 
many application of distributed autonomous robotic system can benefit from or even may require the team of robot staying within communication connectivity for example consider the problem of multirobot surveillance ahmadi stone in which a team of robot must collaboratively patrol a given area if any two robot can directly communicate at all time the robot can coordinate for efficient behavior this condition hold trivially in environment that are smaller than the robot communication range however in larger environment the robot must actively maintain physical location such that any two robot can communicate possibly through a series of other robot otherwise the robot may lose track of each others activity and become miscoordinated furthermore since robot are relatively unreliable and or may need to change task for example if a robot is suddenly called by a human user to perform some other task in a stable multirobot surveillance system if one of the robot leaf or crash the rest should still be able to communicate some example of other task that could benefit from any pair of robot being able to communicate with each other are multi robot exploration search and rescue and cleaning robot we say that robot r is connected to robot r if there is a series of robot each within communication range of the previous which can pas a message from r to r it is not possible to maintain connectivity in the face of arbitrary number of robot departure if there are any two robot that are not within communication of one another and all other robot simultaneously depart the system becomes disconnected thus we focus on the property of remaining robust to any single failure under the assumption that the team can readjust it positioning in response to a departure more quickly than a second departure will occur in order for the team to stay connected even in the face of any single departure it must be the case that every robot is connected to each other robot either directly or via two distinct path that do not share any robot in common we call this property biconnectivity the removal of any one robot from the system doe not disconnect the remaining robot from each other 
many constraint toolkits provide logical connective like disjunction negation and implication these permit complex constraint expression to be built from primitive constraint however the propagation of such complex constraint expression is typically limited we therefore present a simple and light weight method for propagating complex constraint expression we provide a precise characterization of when this method enforces generalized arc consistency in addition we demonstrate that with our method many different global constraint can be easily implemented 
reinforcement learning rl method have become popular in recent year because of their ability to solve complex task with minimal feedback both genetic algorithm gas and temporal difference td method have proven effective at solving difficult rl problem but few rigorous comparison have been conducted thus no general guideline describing the method relative strength and weakness are available this paper summarizes a detailed empirical comparison between a ga and a td method in keepaway a standard rl benchmark domain based on robot soccer the result from this study help isolate the factor critical to the performance of each learning method and yield insight into their general strength and weakness 
the ability to build map of indoor environment is extremely important for autonomous mobile robot in this paper we introduce voronoi random field vrfs a novel technique for mapping the topological structure of indoor environment our map describe environment in term of their spatial layout along with information about the different place and their connectivity to build these map we extract a voronoi graph from an occupancy grid map generated with a laser range finder and then represent each point on the voronoi graph a a node of a conditional random field which is a discriminatively trained graphical model the resulting vrf estimate the label of each node integrating feature from both the map and the voronoi topology the label provide a segmentation of an environment with the different segment corresponding to room hallway or doorway experiment using different map show that our technique is able to label unknown environment based on parameter learned from other environment 
a planning system must reason about the uncertainty of continuous variable in order to accurately project the possible system state over time a method is devised for directly reasoning about the uncertainty in continuous activity duration and resource usage for planning problem by representing random variable a parametric distribution computing projected system state can be simplified common approximation and novel method are compared for over constrained and lightly constrained domain within an iterative repair planner result show improvement in robustness over the conventional non probabilistic representation by reducing the number of constraint violation during execution the improvement is more significant for larger problem and those with higher resource subscription level but diminishes a the system is allowed to accept higher risk level 
in most mechanism design setting optimal general purpose mechanism are not known thus the automated design of mechanism tailored to specific instance of a decision scenario is an important problem existing technique for automated mechanism design amd require the revelation of full utility information from agent which can be very difficult in practice in this work we study the automated design of mechanism that only require partial revelation of utility each agent s type space is partitioned into a finite set of partial type and agent should report the partial type within which their full type lie we provide a set of optimization routine that can be combined to address the trade offs between the amount of communication approximation of incentive property and objective value achieved by a mechanism this allows for the automated design of partial revelation mechanism with worst case guarantee on incentive property for any objective function revenue social welfare etc 
importance of contraction for belief change notwithstanding literature on iterated belief change ha by and large centered around the issue of iterated belief revision ignoring the problem of iterated belief contraction in this paper we examine iterated belief contraction in a principled way starting with qualified insertion a proposal by han rott we show that a judicious combination of qualified insertion with a well known factoring principle lead to what is arguably a pivotal principle of iterated belief contraction we show that this principle is satisfied by the account of iterated belief contraction modelled by lexicographic state contraction and outline it connection with lexicographic revision darwiche pearl s account of revision a well a spohn s ordinal ranking theory 
voting or rank aggregation is a general method for aggregating the preference of multiple agent one voting rule of particular interest is the kemeny rule which minimizes the number of case where the final ranking disagrees with a vote on the order of two alternative unfortunately kemeny ranking are np hard to compute recent work on computing kemeny ranking ha focused on producing good bound to use in search based method in this paper we extend on this work by providing various improved bounding technique some of these are based on cycle in the pairwise majority graph others are based on linear program we completely characterize the relative strength of all of these bound and provide some experimental result 
strong planning under full or partial observability ha been addressed in the literature but this research line is carried out under the hypothesis that the set of observation variable is fixed and compulsory in most real world domain however observation variable are optional and many of them are useless in the execution of a plan on the other side information acquisition may require some kind of cost so it is significant to find a minimal set of observation variable which are necessary for the execution of a plan and to best of our knowledge it is still an open problem in this paper we present a first attempt to solve the problem namely we define an algorithmthat find an approximateminimal set of observation variable which are necessary for the execution of a strong plan under full observability i e a state action table and transforms the plan into a strong plan under partial observability i e a conditional plan branching on the observation built on these observation variable 
recent work on stochastic local search sl for the sat and csp domain ha shown the importance of a dynamic non markovian strategy for weighting clause in order to escape from local minimum in this paper we improve the performance of two best contemprorary clause weighting solver paw and sap by integrating a propositional resolution procedure we also extend the work to adaptnovelty the best non weighting sl solver in the gsat walksat series one outcome is that our system can solve some highly structured problem such a quasigroup existence and parity learning problem which were previously thought unsuitable for local search and which are completely out of reach of traditional solver such a gsat here we present empirical result showing that for a range of random and real world benchmark problem resolution enhanced sl solver clearly outperform the alternative 
island parsing is a bidirectional parsing strategy mostly used in speech analysis a well a in application where robustness is highly relevant and or processing resource are limited although there exists an efficient redundancy free island parsing algorithm for string input it ha not yet been applied to word graph input an application which is central for speech analysis system this paper describes how the established algorithm can be generalized from string input to word graph increasing it flexibility by integrating the selection of island seed into the search process inherent to parsing 
monitoring a diffuse event with a wireless sensor network differs from well studied application such a target tracking and habitat monitoring and therefore we suggest that new approach are needed in this paper we propose a novel low power technique based on a multiple agent framework we show how a set of simple rule can produce complex behavior that encompasses event characterization and data routing we demonstrate the approach and examine it accuracy and scalability using a simulated gaseous plume monitoring scenario 
this paper present a conceptual graph cg framework to the generation of referring expression gre employing conceptual graph a the underlying formalism allows a rigorous semantically rich approach to gre a number of advantage over existing work are discussed the new framework is also used to revisit existing complexity result in a fully rigorous way showing that the expressive power of cgs doe not increase the theoretical complexity of gre 
reasoning with both probabilistic and deterministic dependency is important for many real world problem and in particular for the emerging field of statistical relational learning however probabilistic inference method like mcmc or belief propagation tend to give poor result when deterministic or near deterministic dependency are present and logical one like satisfiability testing are inapplicable to probabilistic one in this paper we propose mc sat an inference algorithm that combine idea from mcmc and satisfiability mc sat is based on markov logic which defines markov network using weighted clause in first order logic from the point of view of mcmc mc sat is a slice sampler with an auxiliary variable per clause and with a satisfiability based method for sampling the original variable given the auxiliary one from the point of view of satisfiability mcsat wrap a procedure around the samplesat uniform sampler that enables it to sample from highly non uniform distribution over satisfying assignment experiment on entity resolution and collective classification problem show that mc sat greatly outperforms gibbs sampling and simulated tempering over a broad range of problem size and degree of determinism 
identifying intrinsic structure in large network is a fundamental problem in many field such a engineering social science and biology in this paper we are concerned with community which are densely connected sub graph in a network and address two critical issue for finding community structure from large experimental data first most existing network clustering method assume sparse network and network with strong community structure in contrast we consider sparse and dense network with weak community structure we introduce a set of simple operation that capture local neighborhood information of a node to identify weak community second we consider the issue of automatically determining the most appropriate number of community a crucial problem for all clustering method this requires to properly evaluate the quality of community structure built atop a function for network cluster evaluation by newman and girvan we extend their work to weighted graph we have evaluated our method on many network of known structure and applied them to analyze a collaboration network and a genetic network the result showed that our method can find superb community structure and correct number of community comparing to the existing approach our method performed significantly better on network with weak community structure and equally well on network with strong community structure 
rule based reasoning and case based reasoning have emerged a two important and complementary reasoning methodology in artificial intelligence ai this paper describes the approach for the development of corm ai a decision support system which employ rule based and case based reasoning to assist noaa s center for operational oceanographic product and service watch standing personnel in monitoring the quality of marine environmental data and information corm ai ha been in operation since july the system accurately and reliably identifies suspect data and network disruption and ha decreased the amount of time it take to identify and troubleshoot sensor network and server failure corm ai ha proven to be robust extendable and cost effective it is estimated that corm ai will save government over one million dollar per year when it full range of quality control monitoring capability is implemented 
in this paper we present a simple yet novel method of exploiting unlabeled text to further improve the accuracy of a high performance state of the art named entity recognition ner system the method utilizes the empirical property that many named entity occur in one name class only using only unlabeled text a the additional resource our improved ner system achieves an f score of an improvement of in f score and a error reduction on the conll english ner official test set this accuracy place our ner system among the top system in the conll english shared task 
modern artifact are typically composed of many system component and exhibit a complex pattern of continuous discrete behavior a concurrent hybrid automaton is a powerful modeling concept to capture such a system s behavior in term a concurrent composition of hybrid automaton for the individual system component because of the potentially large number of mode of the concurrent automaton model it is non trivial to validate the composition such that every possible operational mode lead to a causally valid dynamic model for the overall system this paper present a novel model analysis method that validates the automaton composition without the necessity to analyze a prohibitively large number of mode we achieve this by formulating the exhaustive causal analysis of hybrid automaton a a diagnosis problem this provides causal specification of the component automaton and enables u to efficiently calculate the causal relationship for their concurrent composition and thus validate a concurrent automaton model 
pomdps provide a principled framework for sequential planning in single agent setting an extension of pomdps to multi agent setting called interactive pomdps i pomdps replaces pomdp belief space with interactive hierarchical belief system which represent an agent s belief about the physical world about belief of the other agent s about their belief about others belief and so on this modification make the difficulty of obtaining solution due to complexity of the belief and policy space even more acute we describe a method for obtaining approximate solution to ipomdps based on particle filtering pf we utilize the interactive pf which descends the level of interactive belief hierarchy and sample and propagates belief at each level the interactive pf is able to deal with the belief space complexity but it doe not address the policy space complexity we provide experimental result and chart future work 
the paper introduces a number of propositional argumentation system obtained by gradually extending the underlying language and associated monotonic logic an assumption based argumentation framework bondarenko et al will constitute a special case of this construction in addition a stronger argumentation system in a full classical language will be shown to be equivalent to a system of causal reasoning giunchiglia et al the implication of this correspondence for the respective nonmonotonic theory of argumentation and causal reasoning are discussed 
the definition of object e g data point similarity is critical to the performance of many machine learning algorithm both in term of accuracy and computational efficiency however it is often the case that a similarity function is unknown or chosen by hand this paper introduces a formulation that given relative similarity comparison among triple of point of the form object i is more like object j than object k it construct a kernel function that preserve the given relationship our approach is based on learning a kernel that is a combination of function taken from a set of base function these could be kernel a well the formulation is based on defining an optimization problem that can be solved using linear programming instead of a semidefinite program usually required for kernel learning we show how to construct a convex problem from the given set of similarity comparison and then arrive to a linear programming formulation by employing a subset of the positive definite matrix we extend this formulation to consider representation evaluation efficiency based on formulating a novel form of feature selection using kernel that is not much more expensive to solve using publicly available data we experimentally demonstrate how the formulation introduced in this paper show excellent performance in practice by comparing it with a baseline method and a related state of the art approach in addition of being much more efficient computationally 
clustering accuracy of partitional clustering algorithm for categorical data primarily depends upon the choice of initial data point mode to instigate the clustering process traditionally initial mode are chosen randomly a a consequence of that the clustering result cannot be generated and repeated consistently in this paper we present an approach to compute initial mode for k mode clustering algorithm to cluster categorical data set here we utilize the idea of evidence accumulation for combining the result of multiple clustering initially n f dimensional data is decomposed into a large number of compact cluster the k mode algorithm performs this decomposition with several clustering obtained by n random initialization of the kmodes algorithm the mode thus obtained from every run of random initialization are stored in a mode pool pn the objective is to investigate the contribution of those data object pattern that are le vulnerable to the choice of random selection of mode and to choose the most diverse set of mode from the available mode pool that can be utilized a initial mode for the k mode clustering algorithm experimentally we found that by this method we get initial mode that are very similar to the actual desired mode and give consistent and better clustering result with le variance of clustering error than the traditional method of choosing random mode 
to settle the disputation adr alternative disputation resolution ha been becoming popular instead of the trial however to educate the mediation skill much training is needed in this paper we introduce the overview of the online mediator education system this system navigates the user by providing material for decision making by referring to old case user can communicate with each other by using avatar and they can see the status of disputation in the form of several diagram 
object detection using haar like feature is formulated a a maximum likelihood estimation object feature are described by an arbitrary bayesian network bn of haar like feature we proposed variable translation technique transform the bn into the likelihood for the object detection the likelihood is a bn which includes a node that represents the object s position angle and scale the object detection can be achieved by inference for the node 
to prevent or alleviate conflict in multi agent environment it is important to distinguish between situation where another agent ha misbehaved intentionally and situation where the misbehavior wa accidental one situation where this problem arises is the noisy iterated prisoner s dilemma a version of the iterated prisoner s dilemma ipd in which there is a nonzero probability that a cooperate action will accidentally be changed into a defect action and vice versa tit for tat and other strategy that do quite well in the ordinary non noisy ipd can do quite badly in the noisy ipd this paper present a technique called symbolic noise detection for detecting whether anomaly in player s behavior are deliberate or accidental this idea to use player s deterministic behavior to tell whether an action ha been affected by noise we also present db an algorithm that us symbolic noise detection in the noisy ipd db construct a model of the other agent s deterministic behavior and watch for any deviation from this model if the other agent s next action is inconsistent with this model the inconsistency can be due either to noise or to a genuine change in their behavior and db can often distinguish between two case by waiting to see whether this inconsistency persists in next few move this technique is effective because many ipd player often have clear deterministic pattern of behavior we entered several different implementation of db in the iterated prisoner s dilemma competition in category noisy environment out of the contestant in this category most of db implementation ranked among top ten the best one ranked third and it wa beaten only by two master and slave strategy program that each had a large number of slave program feeding point to them 
this paper study a text mining problem comparative sentence mining csm a comparative sentence express an ordering relation between two set of entity with respect to some common feature for example the comparative sentence canon s optic are better than those of sony and nikon express the comparative relation better optic canon sony nikon given a set of evaluative text on the web e g review forum posting and news article the task of comparative sentence mining is to identify comparative sentence from the text and to extract comparative relation from the identified comparative sentence this problem ha many application for example a product manufacturer want to know customer opinion of it product in comparison with those of it competitor in this paper we propose two novel technique based on two new type of sequential rule to perform the task experimental evaluation ha been conducted using different type of evaluative text from the web result show that our technique are very promising 
this paper address the problem of evaluating student answer in intelligent tutoring environment with mixed initiative dialogue by modelling it a a textual entailment problem the problem of meaning representation and inference is a pervasive challenge in any integrated intelligent system handling communication for intelligent tutorial dialogue system we show that entailment case can be detected at various dialog turn during a tutoring session we report the performance of a lexico syntactic approach on a set of entailment case that were collected from a previous study we conducted with autotutor 
classic direct mechanism require full utility revelation from agent which can be very difficult in practical multi attribute setting in this work we study partial revelation within the framework of one shot mechanism each agent s type space is partitioned into a finite set of partial type and agent should report the partial type within which their full type lie a classic result implies that implementation in dominant strategy is impossible in this model we first show that a relaxation to bayes nash implementation doe not circumvent the problem we then propose a class of partial revelation mechanism that achieve approximate dominant strategy implementation and describe a computationally tractable algorithm for myopically optimizing the partitioning of each agent s type space to reduce manipulability and social welfare loss this allows for the automated design of one shot partial revelation mechanism with worst case guarantee on both manipulability and efficiency 
based on the study of a generalized form of representer theorem and a specific trick in constructing kernel a generic learning model is proposed and applied to support vector machine an algorithm is obtained which naturally generalizes the bias term of svm unlike the solution of standard svm which consists of a linear expansion of kernel function and a bias term the generalized algorithm map predefined feature onto a hilbert space a well and take them into special consideration by leaving part of the space unregularized when seeking a solution in the space empirical evaluation have confirmed the effectiveness from the generalization in classification task 
this paper reconsiders the most basic scheduling problem that of minimizing the makespan of a partially ordered set of activity in the context of incomplete knowledge after positioning this paper in the scope of temporal network under uncertainty we provide a complete solution to the problem of finding float of activity and of locating surely critical one a they are often isolated the minimal float problem is np hard while the maximal float problem is polynomial new complexity result and efficient algorithm are provided for the interval valued makespan minimization problem 
the purpose of this poster is to introduce a dialectical theory for plan synthesis based on a multiagent approach this approach is a promising way to devise system able to take into account partial knowledge and heterogeneous skill of agent we propose to consider the planning problem a a defeasible reasoning where agent exchange proposal and counter proposal and are able to conjecture i e formulate plan step based on hypothetical state of the world 
we consider how to combine the preference of multiple agent despite the presence of incompleteness and incomparability in their preference ordering an agent s preference ordering may be incomplete because for example there is an ongoing preference elicitation process it may also contain incomparability which can be useful for example in multi criterion scenario we focus on the problem of computing the possible and necessary winner that is those outcome which can be or always are the most preferred for the agent possible and necessary winner are useful in many scenario for example preference elicitation need only focus on the unknown relation between possible winner and can ignore completely all other outcome whilst computing the set of possible and necessary winner is in general a difficult problem we identify sufficient condition where we can obtain the necessary winner and an upper approximation of the set of possible winner in polynomial time such condition concern either the language for stating preference or general property of the preference aggregation function 
state estimation in multiagent setting involves updating an agent s belief over the physical state and the space of other agent model performance of the previous approach to state estimation the interactive particle filter degrades with large state space because it distributes the particle over both the physical state space and the other agent model we present an improved method for estimating the state in a class of multiagent setting that are characterized in part by continuous or large discrete state space we factor out the model of the other agent and update the agent s belief over these model a exactly a possible simultaneously we sample particle from the distribution over the large physical state space and project the particle in time this approach is equivalent to rao blackwellising the interactive particle filter we focus our analysis on the special class of problem where the nested belief are represented using gaussians the problem dynamic using conditional linear gaussians clgs and the observation function using softmax or clgs these distribution adequately represent many realistic application 
recent work ha shown promise in using large publicly available hand contributed commonsense database a joint model that can be used to infer human state from day to day sensor data the parameter of these model are mined from the web we show in this paper that learning these parameter using sensor data with the mined parameter a prior can improve performance of the model significantly the primary challenge in learning is scale since the model comprises roughly irregularly connected node in each time slice it is intractable either to completely label observed data manually or to compute the expected likelihood of even a single lime slice we show how to solve the resulting semi supervised learning problem by combining a variety of conventional approximation technique and a novel technique for simplifying the model called context based pruning we show empirically that the learned model is substantially better at interpreting sensor data and an detailed analysis of how various technique contribute to the performance 
programming a humanoid robot to walk is a challenging problem in robotics traditional approach rely heavily on prior knowledge of the robot s physical parameter to devise sophisticated control algorithm for generating a stable gait in this paper we provide to our knowledge the first demonstration that a humanoid robot can learn to walk directly by imitating a human gait obtained from motion capture mocap data training using human motion capture is an intuitive and flexible approach to programming a robot but direct usage of mocap data usually result in dynamically unstable motion furthermore optimization using mocap data in the humanoid full body joint space is typically intractable we propose a new modelfree approach to tractable imitation based learning in humanoid we represent kinematic information from human motion capture in a low dimensional subspace and map motor command in this lowdimensional space to sensory feedback to learn a predictive dynamic model this model is used within an optimization framework to estimate optimal motor command that satisfy the initial kinematic constraint a best a possible while at the same time generating dynamically stable motion we demonstrate the viability of our approach by providing example of dynamically stable walking learned from mocap data using both a simulator and a real humanoid robot 
by relaxing the hard goal constraint from classical planning and associating them with reward value over subscription planning allows user to concentrate on presenting what they want and leaf the task of deciding the best goal to achieve to the planner in this paper we extend the over subscription planning problem and it limited goal specification to allow numeric goal with continuous utility value and goal with mixed hard and soft constraint together they considerably extend the modeling power of goal specification and allow the user to express goal constraint that were not possible before to handle these new goal constraint we extend the sapaps planner s planning graph based technique to help it choose the best beneficial subset of goal that can include both hard or soft logical and numeric goal we also provide empirical result in several benchmark domain to demonstrate that our technique help return quality plan 
semi supervised classifier design that simultaneously utilizes both labeled and unlabeled sample is a major research issue in machine learning existing semisupervised learning method belong to either generative or discriminative approach this paper focus on probabilistic semi supervised classifier design and present a hybrid approach to take advantage of the generative and discriminative approach our formulation considers a generative model trained on labeled sample and a newly introduced bias correction model both model belong to the same model family the proposed hybrid model is constructed by combining both generative and bias correction model based on the maximum entropy principle the parameter of the bias correction model are estimated by using training data and combination weight are estimated so that labeled sample are correctly classified we use naive bayes model a the generative model to apply the hybrid approach to text classification problem in our experimental result on three text data set we confirmed that the proposed method significantly outperformed pure generative and discriminative method when the classification performance of the both method were comparable 
occam s razor is the principle that given two hypothesis consistent with the observed data the simpler one should be preferred many machine learning algorithm follow this principle and search for a small hypothesis within the version space the principle ha been the subject of a heated debate with theoretical and empirical argument both for and against it earlier empirical study lacked sufficient coverage to resolve the debate in this work we provide convincing empirical evidence for occam s razor in the context of decision tree induction by applying a variety of sophisticated sampling technique our methodology sample the version space for many real world domain and test the correlation between the size of a tree and it accuracy we show that indeed a smaller tree is likely to be more accurate and that this correlation is statistically significant across most domain 
holonic multi agent system hmas are a convenient and relevant way to analyze model and simulate complex and open system accurately simulate in real time complex system where a great number of entity interact requires extensive computational resource and often distribution of the simulation over various computer a possible solution to these issue is multilevel simulation this kind of simulation aim at dynamically adapting the level of entity behavior microscopic macroscopic while being a faithful a possible to the simulated model we propose a holonic organizational multilevel model for real time simulation of complex system by exploiting the hierarchical and distributed property of the holarchies to fully exploit this model we estimate the deviation of simulation accuracy between two adjacent level through physic based indicator these indicator will then allow u to dynamically determine the most suitable level for each entity in the application to maintain the best compromise between simulation accuracy and available resource finally a d real time multilevel simulation of pedestrian is presented a well a a discussion of experimental result 
for most real world problem the agent operates in only partially known environment probabilistic planner can reason over the missing information and produce plan that take into account the uncertainty about the environment unfortunately though they can rarely scale up to the problem that are of interest in real world in this paper however we show that for a certain subset of problem we can develop a very efficient probabilistic planner the proposed planner called ppcp is applicable to the problem for which it is clear what value of the missing information would result in the best plan in other word there exists a clear preference for the actual value of the missing information for example in the problem of robot navigation in partially known environment it is always preferred to find out that an initially unknown location is traversable rather than not the planner we propose exploit this property by using a series of deterministic a like search to construct and refine a policy in anytime fashion on the theoretical side we show that once converged the policy is guaranteed to be optimal under certain condition on the experimental side we show the power of ppcp on the problem of robot navigation in partially known terrain the planner can scale up to very large environment with thousand of initially unknown location we believe that this is several order of magnitude more unknown than what the current probabilistic planner developed for the same problem can handle also despite the fact that the problem we experimented on in general doe not satisfy the condition for the solution optimality ppcp still produce the solution that are nearly always optimal 
bin completion a bin oriented branch and bound approach wa recently shown to be promising for the bin packing problem we propose several improvement to bin completion that significantly improves search efficiency we also show the generality of bin completion for packing and covering problem involving multiple container and present bin completion algorithm for the multiple knapsack bin covering and min cost covering liquid loading problem that significantly outperform the previous state of the art however we show that for the bin packing problem bin completion is not competitive with the state of the art solver 
sensing uncertainty is a central issue in robotics sensor limitation often prevent accurate state estimation and robot find themselves confronted with a complicated infonnation belief space in this paper we define and characterize the information space of very simple robot called bitbots which have severe sensor limitation while complete estimation of the robot s state is impossible careful consideration and management of the uncertainty is presented a a search in the information space we show that these simple robot can solve several challenging online problem even though they can neither obtain a complete map of their environment nor exactly localize themselves however when placed in an unknown environment bitbots can build a topological representation of it and then perform pursuit evasion i e locate all moving target inside this environment this paper introduces bitbots and provides both theoretical analysis of their information space and simulation result 
we present an algorithm that derives action effect and precondition in partially observable relational domain our algorithm ha two unique feature an expressive relational language and an exact tractable computation an actionschema language that we present permit learning of precondition and effect that include implicit object and unstated relationship between object for example we can learn that replacing a blown fuse turn on all the light whose switch is set to on the algorithm maintains and output a relationallogical representation of all possible action schema model after a sequence of executed action and partial observation importantly our algorithm take polynomial time in the number of time step and predicate time dependence on other domain parameter varies with the action schema language our experiment show that the relational structure speed up both learning and generalization and outperforms propositional learning method it also allows establishing aprioriunknown connection between object e g light bulb and their switch and permit learning conditional effect in realistic and complex situation our algorithm take advantage of a dag structure that can be updated efficiently and preserve compactness of representation 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
planning involves using a model of an agent s action to find a sequence of decision which achieve a desired goal it is usually assumed that the model are given and such model often require expert knowledge of the domain this paper explores subjective representation for planning that are learned directly from agent observation and action requiring no initial domain knowledge a non linear embedding technique called action respecting embedding is used to construct such a representation it is then shown how to extract the effect of the agent s action a operator in this learned representation finally the learned representation and operator are combined with search to find sequence of action that achieve given goal the efficacy of this technique is demonstrated in a challenging robot vision inspired image domain 
data complexity of reasoning in description logic dl estimate the performance of reasoning algorithm measured in the size of the abox only we show that even for the very expressive dl shiq satisfiability checking is data complete for np for application with large aboxes this can be a more accurate estimate than the usually considered combined complexity which is exptime complete furthermore we identify an expressive fragment horn shiq which is data complete for p thus being very appealing for practical usage 
brain computer interface a any other interaction modality based on physiological signal and body channel e g muscular activity speech and gesture are prone to error in the recognition of subject s intent in this paper we exploit a unique feature of the brain channel namely that it carry information about cognitive state that are crucial for a purposeful interaction one of these state is the awareness of erroneous response different physiological study have shown the presence of error related potential errp in the eeg recorded right after people get aware they have made an error however for human computer interaction the central question is whether errp are also elicited when the error is made by the interface during the recognition of the subject s intent and no longer by error of the subject himself in this paper we report experimental result with three volunteer subject during a simple human robot interaction i e bringing the robot to either the left or right side of a room that seem to reveal a new kind of errp which is satisfactorily recognized in single trial these recognition rate significantly improve the performance of the brain interface 
we introduce normative temporal logic ntl a logic for reasoning about normative system ntl is a generalisation of the well known branchingtime temporal logic ctl in which the path quantifier a on all path and e on some path are replaced by the indexed deontic operator o and p where for example o mean is obligatory in the context of normative system after defining the logic we give a sound and complete axiomatisation and discus the logic s relationship to standard deontic logic we present a symbolic representation language for model and normative system and identify four different model checking problem corresponding to whether or not a model is represented symbolically or explicitly and whether or not we are given an interpretation for the normative system named in formula to be checked we show that the complexity of model checking varies from p complete up to exptime hard for these variation 
we study the performance of two representation of word meaning in learning noun modifier semantic relation one representation is based on lexical resource in particular wordnet the other on a corpus we experimented with decision tree instance based learning and support vector machine all these method work well in this learning task we report high precision recall and f score and small variation in performance across several fold cross validation run the corpus based method ha the advantage of working with data without word sense annotation and performs well over the baseline the wordnet based method requiring word sense annotated data ha higher precision 
relationship between concept account for a large proportion of semantic knowledge we present a nonparametric bayesian model that discovers system of related concept given data involving several set of entity our model discovers the kind of entity in each set and the relation between kind that are possible or likely we apply our approach to four problem clustering object and feature learning ontology discovering kinship system and discovering structure in political data 
adding symmetry breaking constraint is one of the oldest way of breaking variable symmetry for csps for instance it is well known that all the symmetry for the pigeon hole problem can be removed by ordering the variable we have generalized this result to all csps where the variable are subject to an all different constraint in such case it is possible to remove all variable symmetry with a partial ordering of the variable we show how this partial ordering can be automatically computed using computational group theory cgt we further show that partial order can be safely used together with the ge tree method of roney dougal et al experiment show the efficiency of our method 
value iteration is an inefficient algorithm for markov decision process mdps because it put the majority of it effort into backing up the entire state space which turn out to be unnecessary in many case in order to overcome this problem many approach have been proposed among them lao lrtdp and hdp are state of theart one all of these use reachability analysis and heuristic to avoid some unnecessary backup however none of these approach fully exploit the graphical feature of the mdps or use these feature to yield the best backup sequence of the state space we introduce an algorithm named topological value iteration tvi that can circumvent the problem of unnecessary backup by detecting the structure of mdps and backing up state based on topological sequence we prove that the backup sequence tvi applies is optimal our experimental result show that tvi outperforms vi lao lrtdp and hdp on our benchmark mdps 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
this abstract describes ongoing work in developing large scale knowledge repository the project address three primary aspect of such system integration of knowledge source access and retrieval of stored knowledge scalable effective repository previous result have shown the effectiveness of description logic based representation in integrating knowledge source and the role of non standard inference in supporting repository reasoning task current effort include developing general purpose mechanism for adapting reasoning algorithm for optimized inference under known domain structure and effective use of database technology a a large scale knowledge base backend 
most probabilistic inference algorithm are specified and processed on a propositional level in the last decade many proposal for algorithm accepting first order specification have been presented but in the inference stage they still operate on a mostly propositional representation level poole presented a method to perform inference directly on the first order level but this method is limited to special case in this paperwe present the first exact inference algorithm that operates directly on a first order level and that can be applied to any first order model specified in a language that generalizes undirected graphical model our experiment show superior performance in comparison with propositional exact inference 
forward model enable a robot to predict the effect of it action on it own motor system and it environment this is a vital aspect of intelligent behaviour a the robot can use prediction to decide the best set of action to achieve a goal the ability to learn forward model enables robot to be more adaptable and autonomous this paper describes a system whereby they can be learnt and represented a a bayesian network the robot s motor system is controlled and explored using motor babbling feedback about it motor system come from computer vision technique requiring no prior information to perform tracking the learnt forward model can be used by the robot to imitate human movement 
recently generative probabilistic modeling principle were extended to visualization of structured data type such a sequence the model are formulated a constrained mixture of sequence model a generalization of density based visualization method previously developed for static data set in order to effectively explore visualization plot one need to understand local directional magnification factor i e the extend to which small positional change on visualization plot lead to change in local noise model explaining the structured data magnification factor are useful for highlighting boundary between data cluster in this paper we present two technique for estimating local metric induced on the sequence space by themodel formulation we first verify our approach in two controlled experiment involving artificially generated sequence we then illustrate our methodology on sequence representing choral by j s bach 
it is often convenient to represent probabilistic model in a first order fashion using logical atom such a partner x y a random variable parameterized by logical variable de salvo braz amir roth following poole give a lifted variable elimination algorithm fove for computing marginal probability from first order probabilistic model belief assessment or ba fove is lifted because it work directly at the first order level eliminating all the instantiation of a set of atom in a single step in some case independently of the number of these instantiation previous work could treat only restricted potential function there atom instantiation cannot constrain each other predicate can appear at most once or logical variable must not interact across atom in this paper we present two contribution the first one is a significantly more general lifted variable elimination algorithm fove p that cover many case where atom share logical variable the second contribution is to use fove p for solving the most probable explanation mpe problem which consists of calculating the most probable assignment of the random variable in a model the transition from ba to mpe is straightforward in propositional model but the lifted first order case is harder we introduce the notion of lifted assignment a distribution of value to a set of random variable rather than to each individual one lifted assignment are cheaper to compute while being a useful a regular assignment over that group both contribution advance the theoretical understanding of lifted probabilistic inference 
several high level methodological debate among ai researcher linguist psychologist and philosopher appear to be endless e g about the need for and nature of representation about the role of symbolic process about embodiment about situatedness about whether symbol grounding is needed and about whether a robot need any knowledge at birth or can start simply with a powerful learning mechanism consideration of the variety of capability and development pattern on the precocial altricial spectrum in biological organism will help u to see these debate in a new light 
many ai researcher want to test the utility of their system in complex task environment defined by e g real time strategy gaming simulator and or simulator of computer generated force also many developer of commercial and military gaming simulator seek behavior that can be supported by these system however these integration require great effort we will demonstrate the late alpha version of tielt a testbed designed to fill these need 
in this paper we propose to mine query hierarchy from clickthrough data which is within the larger area of automatic acquisition of knowledge from the web when a user submits a query to a search engine and click on the returned web page the user s understanding of the query a well a it relation to the web page is encoded in the clickthrough data with million of query being submitted to search engine every day it is both important and beneficial to mine the knowledge hidden in the query and their intended web page we can use this information in various way such a providing query suggestion and organizing the query in this paper we plan to exploit the knowledge hidden in clickthrough log by constructing query hierarchy which can reflect the relationship among query our proposed method consists of two stage generating candidate query and determining generalization specialization relatinns between these query in a hierarchy we test our method on some labeled data set and illustrate the effectiveness of our proposed solution empirically 
rhode island hold em is a poker card game that ha been proposed a a testbed for ai research this game with a tree size larger than billion node feature many characteristic present in full scale poker e g texas hold em our research advance in equilibrium computation have enabled u to solve for the optimal equilibrium strategy for this game some feature of the equilibrium include poker technique such a bluffing slow playing check raising and semi bluffing in this demonstration participant will compete with our optimal opponent and will experience these strategy firsthand 
variable ordering heuristic have long been an important component of constraint satisfaction search algorithm in this paper we study the behaviour of standard variable ordering heuristic when searching an insoluble sub problem we employ the notion of an optimal refutation of an insoluble sub problem and describe an algorithm for obtaining it we propose a novel approach to empirically looking at problem hardness and typical case complexity by comparing optimal refutation with those generated by standard search heuristic it is clear from our analysis that the standard variable ordering used to solve csps behave very differently on real world problem than on random problem of comparable size our work introduces a potentially useful tool for analysing the cause of the heavy tailed phenomenon observed in the runtime distribution of backtrack search procedure 
a pattern database is a lookup table that store an exact evaluation function for a relaxed search problem which provides an admissible heuristic for the original search problem in general the larger the pattern database the more accurate the heuristic function we consider how to build large pattern database that are stored in external memory such a disk and how to use an external memory pattern database efficiently in heuristic search to limit the number of slow disk i o operation needed to construct and query an external memory pattern data base we adapt an approach to external memory graph search called structured duplicate detection that localizes memory reference by leveraging an abstraction of the state space we present result that show this approach increase the scalability of heuristic search by allowing larger and more accurate pattern database heuristic 
i propose a representation formalism and algorithm to be used in a new language generation mechanism for text to text application the generation process is driven by both text specific information encoded via probability distribution over word and phrase derived from the input text and general language knowledge captured by n gram and syntactic language model 
we present two equivalent approach for defining answer set for logic program with arbitrary abstract constraint atom c atom the first approach us an immediate consequence operator for answer set checking whose definition relies on the notion of conditional satisfactionof c atom w r t a pair of interpretation the second approach generalizes the notion of well supported model of normal logic program to program with c atom we prove that the newly defined semantics coincides with previously introduced semantics for logic program with monotone c atom and extends the original answer set semantics for normal logic program we discus different possibility for treating negation a failure c atom and characterize situation in which they yield the same answer set we study some property of answer set of program with c atom and relate our definition to several semantics for logic program with aggregate 
the stony brook robot design team ha focused on two main area of research in the creation of navbot our new robot created for the american association of artificial intelligence s aaai scavenger hunt event navigation and computer vision the purpose is to create an intelligent machine that is able to navigate the conference floor for specific object at the aaai conference in pittsburgh pennsylvania to achieve the desirable speed required in a rescue robot navbot utilizes high rpm servo motor with a manually adjustable gear train the two v servo motor enhance the maneuverability of the robot by providing minor adjustment in the robot s speed a it navigates an area the manually adjustable gear train allow navbot to function in different environment this is achieved by controlling the force and the speed required for maneuverability in the terrain navbot also utilizes a dual track motion system to handle rough terrain to enhance the power of navbot it wa designed with a light aluminum skeleton in addition to the large space provided for necessary electrical and computer equipment the skeleton serf a a secure casing that prevent damage to vital part 
we propose a novel method to detect event involving multiple agent in a video and to learn their structure in term of temporally related chain of sub event the proposed method ha three significant contribution over existing framework first in order to learn the event structure from training video we present the concept of a video event graph which is composed of temporally related sub event using the video event graph we automatically encode the event dependency graph the event dependency graph is the learnt event model that depicts the frequency of occurrence of conditionally dependent sub event second we pose the problem of event detection in novel video a clustering the maximally correlated sub event and use normalized cut to determine these cluster the piincipal assumption made in this work is that the event are composed of highly correlated chain of sub event that have high weight association within the cluster and relatively low weight disassociation between cluster these weight between sub event are the likelihood estimate obtained from the event model last we recognize the importance of representing the variation in the temporal order of sub event occurring in an event and encode the probability directly into our representation we show result of our learning detection and representation of event for video in the meeting surveillance and railroad monitoring domain 
abstract argumentation system are formalism for defeasible reasoning where some component remain unspecified the structure of argument being the main abstraction in the dialectical process carried out to identify accepted argument in the system some controversial situation may appear these relate to the reintroduction of argument into the process which cause the onset of circularity this must be avoided in order to prevent an infinite analysis some system apply the sole restriction of not allowing the introduction of previously considered argument in an argumentation line however repeating an argument is not the only possible cause for the risk mentioned a subarguments must be taken into account in this work we introduce an extended argumentation framework and a definition for progressive defeat path a credulous extension is also presented 
we embark on an initial study of a new class of strategic normal form game so called ranking game in which the payoff to each agent solely depends on his position in a ranking of the agent induced by their action this definition is motivated by the observation that in many strategic situation such a parlor game competitive economic scenario and some social choice setting player are merely interested in performing optimal relative to their opponent rather than in absolute measure a simple but important subclass of ranking game are single winner game where in any outcome one agent win and all others lose we investigate the computational complexity of a variety of common game theoretic solution concept in ranking game and deliver hardness result for iterated weak dominance and mixed nash equilibrium when there are more than two player and pure nash equilibrium when the number of player is unbounded this dash hope that multi player ranking game can be solved efficiently despite the structural restriction of these game 
this paper report on textal a deployed application that us a variety of ai technique to automate the process of determining the d structure of protein by x ray crystallography the textal project wa initiated in and the application is currently deployed in three way a web based interface called webtex operational since june a the automated model building component of an integrated crystallography software called phenix first released in july binary distribution available since september textal and it sub component are currently being used by crystallographer around the world both in the industry and in academia textal save up to week of effort typically required to determine the structure of one protein the system ha proven to be particularly helpful when the quality of the data is poor which is very often the case automated protein modeling system like textal are critical to the structural genomics initiative a worldwide effort to determine the d structure of all protein in a high throughput mode thereby keeping up with the rapid growth of genomic sequence database 
this paper present a method for measuring the semantic similarity of text using corpus based and knowledge based measure of similarity previous work on this problem ha focused mainly on either large document e g text classification information retrieval or individual word e g synonymy test given that a large fraction of the information available today on the web and elsewhere consists of short text snippet e g abstract of scientific document imagine caption product description in this paper we focus on measuring the semantic similarity of short text through experiment performed on a paraphrase data set we show that the semantic similarity method out performs method based on simple lexical matching resulting in up to error rate reduction with respect to the traditional vector based similarity metric 
russian doll search rds is a well known algorithm for combinatorial optimization in this paper we extend it from mono objective to multi objective optimization we demonstrate it practical applicability in the challenging multiple orbit spot instance besides being much more efficient than any other alternative multi objective rds can solve an instance which could not have been solved previously 
predictive state representation psr proposed by littman et al singh et al are a general representation for controlled dynamical system we present a sufficient condition under which a linear psr compress a pomdp representation 
this student abstract describes ongoing investigation regarding an approach for dealing with non stationarity in reinforcement learning rl problem we briefly propose and describe a method for managing multiple partial model of the environment and comment previous result which show that the proposed mechanism ha better convergence time comparing to standard rl algorithm current effort include the development of a more robust approach capable of dealing with noisy environment and also investigation regarding the possibility of using partial model in order to aliviate learning problem in system with an explosive number of state 
distributed real time embedded dre system perform sequence of coordination and heterogeneous data manipulation task in dynamic environment to meet specified goal autonomous operation of dre system can benefit from the integrated operation of a decision theoretic spreading activation partial order planner sa pop that combine task planning and scheduling in uncertain environment with a resource allocation and control engine race middleware framework that integrates multiple resource management algorithm for re deploying and re configuring task sequence component in these system this paper demonstrates the effectiveness of sa pop and race in managing and executing mission goal for a multisatellite application our result show that combining planning scheduling and resource constraint dynamically is the key to implementing autonomy in dre system 
since bayesian network bn wa introduced in the field of artificial intelligence in s a number of inference algorithm have been developed for probabilistic reasoning however when continuous variable are present in bayesian network their dependence relationship could be nonlinear and their probability distribution could be arbitrary so far no efficient inference algorithm could deal with this case except monte carlo simulation method such a likelihood weighting but with unlikely evidence simulation method could be very slow to converge in this paper we propose an efficient approximate inference algorithm called unscented message passing ump bn for bayesian network with arbitrary continuous variable ump bn combine unscented transformation a deterministic sampling method and pearl s message passing algorithm to provide the estimate of the first two moment of the posterior distribution we test this algorithm with several network including the one with nonlinear and or non gaussian variable the numerical experiment show that ump bn converges very fast and produce promising result 
reciprocity is a key determinant of human behavior and ha been well documented in the psychological and behavioral economics literature this paper show that reciprocity ha significant implication for computer agent that interact with people over time it proposes a model for predicting people s action in multiple bilateral round of interaction the model represents reciprocity a a tradeoff between two social factor the extent to which player reward and retaliate others past action retrospective reasoning and their estimate about the future ramification of their action prospective reasoning the model is trained and evaluated over a series of negotiation round that vary player possible strategy a well a their benefit from potential strategy at each round result show that reasoning about reciprocal behavior significantly improves the predictive power of the model enabling it to outperform alternative model that do not reason about reciprocity or that play various game theoretic equilibrium these result indicate that computer that interact with people need to represent and to learn the social factor that affect people s play when they interact over time 
in group decision making problem that involve selfinterested agent with private information reaching socially optimal outcome requires aligning the goal of individual with the welfare of the entire group the well known vcg mechanism achieves this by requiring specific payment from agent to a central coordinator however when the goal of coordination is to allow the group to jointly realize the greatest possible welfare these payment amount to an unwanted cost of implementation or waste while it ha often been stated that the payment vcg prescribes are necessary in order to implement the socially optimal outcome in dominant strategy without running a deficit this is in fact not generally true cavallo specified the mechanism that requires the minimal payment among all mechanism that are socially optimal never run a deficit and are ex post individual rational with an anonymity property the mechanism achieves significant saving over vcg in a broad range of practically relevant domain including allocation problem by using information about the structure of valuation in the domain this paper give a high level overview of that result and discus some potential application to ai 
we offer a complete characterization of the set of distribution that could be induced by local intervention on variable governed by a causal bayesian network of unknown structure in which some of the variable remain unmeasured we show that such distribution are constrained by a simply formulated set of inequality from which bound can be derived on causal effect that are not directly measured in randomized experiment 
the web is experiencing an exponential growth in the use of weblogs or blog website containing dated journal style entry blog entry are generally organised using informally defined label known a tag increasingly tag are being proposed a a grassroots alternative to semantic web standard we demonstrate that tag by themselves are weak at partitioning blog data we then show how tag may contribute useful discriminating information using content based clustering we observe that frequently occurring tag in each cluster are usually good meta label for the cluster concept we then introduce the tr score a score based on the proportion of high frequency tag in a cluster and demonstrate that it is strongly correlated with cluster strength we demonstrate how the tr score enables the detection and removal of weak cluster a such the tr score can be used a an independent mean of verifying topic integrity in a cluster based recommender system 
solving logic puzzle ha become a very popular past time particularly since the sudoku puzzle started appearing in newspaper all over the world we have developed a puzzle generator for a modification of sudoku called jidoku in which clue are binary disequalities between cell on a grid our generator guarantee that puzzle have unique solution have graded difficulty and can be solved using inference alone this demonstration provides a fun application of many standard constraint satisfaction technique such a problem formulation global constraint search and inference it is ideal a both an education and outreach tool our demonstration will allow people to generate and interactively solve puzzle of user selected difficulty with the aid of hint if required through a specifically built java applet 
billiards is a game of both strategy and physical skill to succeed a player must be able to select strong shot and then execute them accurately and consistently several robotic billiards player have recently been developed these system address the task of executing shot on a physical table but so far have incorporated little strategic reasoning they require ai to select the best shot taking into account the accuracy of the robotics the noise inherent in the domain the continuous nature of the search space the difficulty of the shot and the goal of maximizing the chance of winning this paper develops and compare several approach to establishing a strong ai for billiards the resulting program pickpocket won the first international computer billiards competition 
multiagent model of the emergence of social convention have demonstrated that global convention can arise from local coordination process without a central authority we further develop and extend previous work to address how and under what condition emerging convention are also socially efficient i e better for all agent than potential alternative convention we show with computational experiment that the clustering coefficient of the network within which agent interact is an important condition for efficiency we also develop an analytical approximation of the simulation model that shed some light to the original model behavior finally we combine two decision mechanism local optimization and imitation to study the competition between efficient and attractive action our main result is that in clustered network a society converges to an efficient convention and is stable against invasion of sub optimal convention under a much larger range of condition than in a non clustered network on the contrary in non clustered network the convention finally established heavily depends on it initial support 
the goal of transfer learning is to use the knowledge acquired in a set of source task to improve performance in a related but previously unseen target task in this paper we present a multilayered architecture named case based reinforcement learner carl it us a novel combination of case based reasoning cbr and reinforcement learning rl to achieve transfer while playing against the game ai across a variety of scenario in madrtstm a commercial real time strategy game our experiment demonstrate that carl not only performs well on individual task but also exhibit significant performance gain when allowed to transfer knowledge from previous task 
we present a method of visualizing and adjusting the evaluation function in game programming in this paper it is widely recognized that an evaluation function should assign a higher evaluation value to a position with greater probability of a win however this relation ha not been utilized directly to tune evaluation function because of the difficulty of measuring the probability of win in deterministic game we present the use of win percentage to utilize this relation in position having the same evaluation value a win probability where the position we used were stored in a large database of game record we introduce an evaluation curve formed by evaluation value and win probability to enable evaluation function to be visualized we observed that evaluation curve form a sigmoid in various kind of game and that these curve may split depending on the property of position because such split indicate that an evaluation function that is visualized misestimates position with le probability of winning we can improve this by fitting evaluation curve to one our experiment with chess and shogi revealed that deficiency in evaluation function could be successfully visualized and that improvement by automatically adjusting their weight were confirmed by self play 
over the past half decade we have been exploring the use of logic in the specification and analysis of computational economic mechanism we believe that this approach ha the potential to bring the same benefit to the design and analysis of computational economic mechanism that the use of temporal logic and model checking have brought to the specification and analysis of reactive system in this paper we give a survey of our work we first discus the use of cooperation logic such a alternating time temporal loglc atl for the specification and verification of mechanism such a social choice procedure we motivate the approach and then discus the work we have done on extension to atl to support incomplete information preference and quantification over coalition we then discus is the use of atl like cooperation logic in the development of social law 
we present a cognitive model that bridge work in analogy and category learning the model building relation through instance driven gradient error shifting bridge extends alcove an exemplar based connectionist model of human category learning kruschke unlike alcove which is limited to featural or spatial representation bridge can appreciate analogical relationship between stimulus and stored predicate representation of exemplar like alcove bridge learns to shift attention over the course of learning to reduce error and in the process alters it notion of similarity a shift toward relational source of similarity allows bridge to display what appears to be an understanding of abstract domain when in fact performance is driven by similarity based structural alignment i e analogy to stored exemplar supportive simulation of animal infant and adult learning are provided we end by considering possible extension of bridge suitable for computationally demanding application 
abstract we develop a formal theory of mereology that includes relation that change over time we show how this theory formalizes reasoning over domain of material object which include not only integral object my computer your liver but also portion of stuff the water in your glass the blood in a vial in particular we use different mereological summation relation to distinguish between the way in which i integral object ii portion of unstructured homogenous stuff e g the water in your glass and iii mixture the blood in a vial are linked to their part over time 
to enhance effectiveness in real world application autonomous agent have to develop cognitive competency and anticipatory capability here we point out their strong liaison with the functional role of affective mental state a those of humanlike metaphor not only the root element for both surprise and anticipation are expectation but also part of the effect of the former elicit effort on the latter by analyzing different kind of expectation we provide a general architecture enhancing practical reasoning with mental state describing and empirically evaluating how mental and behavioral attitude emerging from mental state can be applied for augmenting agent reactivity opportunism and efficacy in term of anticipation 
dtgolog a decision theoretic agent programming language based on the situation calculus wa proposed to ease some of the computational difficulty associated with markov decision process mdps by using natural ordering constraint on execution of action using dtgolog domain specific constraint on a set of policy can be expressed in a high level program to reduce significantly computation required tofinda policy optimal in this set we explore whether the dtgolog framework can be used to evaluate different design of a decision making agent in a large real world domain each design is understood a combination of a template expressed a a golog program for available policy and a reward function to evaluate and compare alternative design we estimate the probability of goal satisfaction for each design a a domain we choose the london ambulance service la case study that is well known in software engineering but remains unknown in ai we demonstrate that dtgolog can be applied successfully to quantitative evaluation of alternative design in term of their ability to satisfy a system goal with a high probability the full version of this paper includes a detailed axiomatization of the domain in the temporal situation calculus with stochastic action the main advantage of this representation is that neither action nor state require explicit enumeration we do an experimental analysis using an on line implementation of dtgolog coupled with a simulator that model real time action of many external agent 
using the principle of homeostasis we derive a learning rule for a specific recurrent neural network structure the so called self adjusting ring module sarm several of these ring module can be plugged together to drive segmented artificial organism for example centipede like robot controlling robot of variable morphology by sarms ha major advantage over using central pattern generator cpgs sarms are able to immediately reconfigure themselves after reassembly of the robot s morphology in addition there is no need to decide on a singular place for the robot s control processor since sarms represent inherently distributed control structure 
this paper proposes a design for our entry into the aaai scavenger hunt competition and robot exhibition we will be entering a scalable two agent system consisting of off the shelf laptop robot capable of monocular vision each robot will demonstrate the ability to localize itself recognize a set of object and communicate with peer robot to share location and coordinate exploration 
the goal of information extraction is to extract database record from text or semi structured source traditionally information extraction proceeds by first segmenting each candidate record separately and then merging record that refer to the same entity while computationally efficient this approach is suboptimal because it ignores the fact that segmenting one candidate record can help to segment similar one for example resolving a well segmented field with a le clear one can disambiguate the latter s boundary in this paper we propose a joint approach to information extraction where segmentation of all record and entity resolution are performed together in a single integrated inference process while a number of previous author have taken step in this direction eg pasula et al wellner et al to our knowledge this is the first fully joint approach in experiment on the citeseer and cora citation matching datasets joint inference improved accuracy and our approach outperformed previous one further by using markov logic and the existing algorithm for it our solution consisted mainly of writing the appropriate logical formula and required much le engineering than previous one 
reversing action is the following problem after executing a sequence of action which sequence of action brings the agent back to the state just before this execution an action reversal notably this problem is different from a vanilla planning problem since the state we have to get back to is in general unknown it emerges for example if an agent need to find out which action sequence are undoable and which one are committed choice it ha application related to plan execution and monitoring in nondeterministic domain such a recovering from a failed execution by partially undoing the plan dynamically switching from one executed plan to another or restarting plan we formalize action reversal in a logic based action framework and characterize it computational complexity since unsurprisingly the problem is intractable in general we present a knowledge compilation approach that construct offline a reverse plan library for efficient in some case linear time on line computation of action reversal our result for the generic framework can be easily applied for expressive action language such a c or 
collaborative web search us the past search behaviour query and selection of a community of user to promote search result that are relevant to the community the extent to which these promotion are likely to be relevant depends on how reliably past search behaviour can be captured we consider this issue by analysing the result of collaborative web search in circumstance where the behaviour of searcher is unreliable 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
jean is a model of early cognitive development based loosely on piaget s theory of sensori motor and pre operational thought like an infant jean repeatedly executes schema gradually transferring them to new situation and extending them a necessary to accommodate new experience we model this process of accommodation with the experimental state splitting es algorithm es learns elementary action schema which comprise controller and map of the expected dynamic of executing controller in different condition es also learns composition of action schema called gist we present test of the es algorithm in three transfer learning experiment in which jean transfer learned gist to new situation in a real time strategy military simulator 
conditional constraint satisfaction problem ccsps are generalization of classical csps that support conditional activation of variable and constraint despite the interest emerged for ccsps in the context of modelling the intrinsic dynamism of diagnosis structural design and product configuration application a complete characterization of their computational property and of their expressiveness is still missing in fact the aim of the paper is precisely to face these open research issue first ccsps are formally characterized in term of a suitable fragment of first order logic second the complexity of some basic reasoning task for ccsps is studied by establishing completeness result for the first and the second level of the polynomial hierarchy finally motivated by the hardness result an island of tractability for ccsps is identified by extending structural decomposition method originally proposed for csps 
we present a new estimator for counting the number of solution of a boolean satisfiability problem a a part of an importance sampling framework the estimator us the recently introduced samplesearch scheme that is designed to overcome the rejection problem associated with distribution having a substantial amount of determinism we show here that the sampling distribution of samplesearch can be characterized a the backtrack free distribution and propose several scheme for it computation this allows integrating sample search into the importance sampling framework for approximating the number of solution and also allows using sample search for computing a lower bound measure on the number of solution our empirical evaluation demonstrates the superiority of our new approximate counting scheme against recent competing approach 
we propose a novel kernel regression algorithm which take into account order preference on unlabeled data such preference have the form that point x ha a larger target value than that of x although the target value for x x are unknown the order preference can be viewed a side information or a form of weak label and our algorithm can be related to semi supervised learning learning consists of formulating the order preference a additional regularization in a risk minimization framework we define a linear program to effectively solve the optimization problem experiment on benchmark datasets sentiment analysis and housing price problem show that the proposed algorithm outperforms standard regression even when the order preference are noisy 
most work on value function approximation adheres to samuel s original design agent learn a task specific value function using parameter estimation where the approximation architecture e g polynomial is specified by a human designer this paper proposes a novel framework generalizing samuel s paradigm using a coordinate free approach to value function approximation agent learn both representation and value function by constructing geometrically customized task independent basis function that form an orthonormal set for the hilbert space of smooth function on the underlying state space manifold the approach rest on a technical result showing that the space of smooth function on a compact riemanian manifold ha a discrete spectrum associated with the laplace beltrami operator in the discrete setting spectral analysis of the graph laplacian yield a set of geometrically customized basis function for approximating and decomposing value function the proposed framework generalizes samuel s value function approximation paradigm by combining it with a formalization of saul amarel s paradigm of representation learning through global state space analysis 
in this paper we propose a method of automatically extracting word hierarchy based on the inclusion relation of word appearance pattern in corpus we applied the complementary similarity measure csm to determine a hierarchical structure of word meaning the csm is a similarity measure developed for recognizing degraded machine printed text there are csms for both binary and gray scale image the csm for binary image ha been applied to estimate one to many relation such a superordinate subordinate relation and to extract word hierarchy however the csm for gray scale image ha not been applied to natural language processing here we apply the latter to extract word hierarchy from corpus to do this we used frequency information for co occurring word which is not considered when using the csm for binary image we compared our hierarchy with those obtained using the csm for binary image and evaluated them by measuring their degree of agreement with the edr electronic dictionary 
this paper explores the use of analogy to learn about property of sketch sketch often convey conceptual relationship between entity via the visual relationship between their depiction in the sketch understanding these convention is an important part of adapting to a user this paper describes how learning by accumulating example can be used to make suggestion about such relationship in new sketch we describe how sketch are being used in companion cognitive system to illustrate one context in which this problem arises we describe how existing cognitive simulation of analogical matching and retrieval are used to generate suggestion for new sketch based on analogy with prior sketch two experiment provide evidence a to the accuracy and coverage of this technique 
decision making under uncertainty is usually based on the comparative evaluation of different alternative by mean of a decision criterion in a qualitative setting pessimistic and optimistic criterion have been proposed in that setting the whole decision process is compacted into a criterion formula on the basis of which alternative are compared it is thus impossible for an end user to understand why an alternative is good or better than another besides argumentation is a powerful tool for explaining inference decision etc this paper articulates optimistic and pessimistic decision criterion in term of an argumentation process that consists of constructing argument in favor against decision evaluating the strength of those argument and comparing pair of alternative on the basis of their supporting attacking argument 
effective representation of web search result remains an open problem in the information retrieval community for ambiguous query a traditional approach is to organize search result into group cluster one for each meaning of the query these group are usually constructed according to the topical similarity of the retrieved document but it is possible for document to be totally dissimilar and still correspond to the same meaning of the query to overcome this problem we exploit the thematic locality of the web relevant web page are often located close to each other in the web graph of hyperlink we estimate the level of relevance between each pair of retrieved page by the length of a path between them the path is constructed using multi agent beam search each agent start with one web page and attempt to meet a many other agent a possible with some bounded resource we test the system on two type of query ambiguous english word and people name the web appears to be tightly connected about of the agent meet with each other after only three iteration of exhaustive breadth first search however when heuristic are applied the search becomes more focused and the obtained result are substantially more accurate combined with a content driven web page clustering technique our heuristic search system significantly improves the clustering result 
in natural language processing information about a person s geographical origin is an important feature for name entity transliteration and question answering we propose a language independent name origin clustering and classification framework provided with a small amount of bilingual name translation pair with labeled origin we measure origin similarity based on the perplexity of name character language and translation model we group similar origin into cluster then train a bayesian classifier with different feature it achieves classification accuracy with source name only and with both source and target name pair we apply the origin clustering and classification technique to a name transliteration task the cluster specific transliteration model dramatically improves the transliteration accuracy from to reducing the transliteration character error rate from to adding more unlabeled name pair to the cluster specific name transliteration model further improves the transliteration accuracy 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
in the u health care provider are required to report evidence based quality measure to various governmental and independent regulatory agency abstracting appropriate fact from a patient s medical record provides the data for these measure finding and maintaining qualified staff for this vital function is a challenge to many healthcare provider emerging system and technology in large scale clinical repository and ai technique for information extraction have the potential to make the process of collecting measure more consistent accurate and efticient this paper present cm extractor a computerized system that automates the process of quality measure abstraction using natural language processing and a rule based approach an evaluation of a deployed system used for hospital inpatient case is discussed the result showed that the nlp perfomed with high accuracy across multiple type of medical document and user were able to significantly improve productivity challenge remain in the area of availability of electronic patient data and a model for deploying and supporting solution on a large scale 
technique for robot monitoring and diagnosis have been developed that perform state estimation using probabilistic hybrid discrete continuous model exact inference in hybrid dynamic system is in general intractable approximate algorithm are based on either greedy search a in the case of k best enumeration or stochastic search a in the case of rao blackwellised particle filtering rbpf in this paper we propose a new method for hybrid state estimation the key insight is that stochastic and greedy search method taken together are often particularly effective in practice the new method combine the stochastic method of rbpf with the greedy search of k best in order to create a method that is effective for a wider range of estimation problem than the individual method alone we demonstrate this robustness on a simulated acrobatic robot and show that this benefit come at only a small performance penalty 
learning the structure of large undirected graph with thousand of node from data ha been an open challenge in this paper we use graphical gaussian model ggm a the underlying model and propose a novel ard style wishart prior for the precision matrix of the ggm which encodes the graph structure we want to learn with this prior we can get the map estimation of the precision matrix by solving a modified version of lasso regression and achieve a sparse solution we use our approach to learn genetic regulatory network from genome wide expression microarray data and protein binding location analysis data evaluated on the basis of consistency with the go annotation the experiment show that our approach ha a much better performance than the clustering based approach and bn learning approach in discovering gene regulatory module 
we describe a family of kernel over untyped and typed prolog ground term and show that they can be applied for learning in structured domain presenting experimental result in a qspr task 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
a novel and simple combination of inductive logic programming with kernel method is presented the kfoil algorithm integrates the well known inductive logic programming system foil with kernel method the feature space is constructed by leveraging foil search for a set of relevant clause the search is driven by the performance obtained by a support vector machine based on the resulting kernel in this way kfoil implement a dynamic propositionalization approach both classification and regression task can be naturally handled experiment in applying kfoil to well known benchmark in chemoinformatics show the promise of the approach 
accurate entity resolution is sometimes impossible simply due to insufficient information for example in research paper author name resolution even clever use of venue title and co authorship relation are often not enough to make a confident coreference decision this paper present several method for increasing accuracy by gathering and integrating additional evidence from the web we formulate the coreference problem a one of graph partitioning with discriminatively trained edge weight and then incorporate web information either a additional feature or a additional node in the graph since the web is too large to incorporate all it data we need an efficient procedure for selecting a subset of web query and data we formally describe the problem of resource bounded information gathering in each of these context and show significant accuracy improvement with low cost 
this paper describes an efficient complete approach for solving a complex allocation and scheduling problem for multi processor system on chip mpsoc given a throughput constraint for a target application characterized a a task graph annotated with computation communication and storage requirement we compute an allocation and schedule which minimizes communication cost first and then the makespan given the minimal communication cost our approach is based on problem decomposition where the allocation is solved through an integer programming solver while the scheduling through a constraint programming solver the two solver are interleaved and their interaction regulated by no good generation experimental result show significant speedup w r t pure ip and cp solution strategy 
a central goal of robotics and ai is to be able to deploy an agent to act autonomously in the real world over an extended period of time it is commonly asserted that in order to do so the agent must be able to learn to deal with unexpected environmental condition however an ability to learn is not sufficient for true extended autonomy an agent must also be able to recognize when to abandon it current model in favor of learning a new one and how to learn in it current situation this paper present a fully implemented example of such autonomy in the context of color map learning on a vision based mobile robot for the purpose of image segmentation past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a curriculum an action sequence designed to facilitate learning this paper introduces algorithm that enable a robot to i devise it own curriculum and ii recognize when the lighting condition have changed sufficiently to warrant learning a new color map 
the goal of our current research is machine learning with the help and guidance of a knowledge base kb rather than learning numerical model our approach generates explicit symbolic hypothesis these hypothesis are subject to the constraint of the kb and are easily human readable and verifiable toward this end we have implemented algorithm that hypothesize new relation and new type of entity in a kb by examining structural regularity in the kb that represent implicit knowledge we evaluate these algorithm on a publication kb and a zoology kb 
conjunctive query play an important role a an expressive query language for description logic dl although modern dl usually provide for transitive role conjunctive query answering over dl knowledge base is only poorly understood if transitive role are admitted in the query in this paper we consider union of conjunctive query over knowledge base formulated in the prominent dlshiq and allow transitive role in both the query and the knowledge base we show decidability of query answering in this setting and establish two tight complexity bound regarding combined complexity we prove that there is a deterministic algorithm for query answering that need time single exponential in the size of the kb and double exponential in the size of the query which is optimal regarding data complexity we prove containment in co np 
we present a general framework for determining the number of solution of constraint satisfaction problem csps with a high precision our first strategy us additional binary variable for the csp and applies an xor or parity constraint based method introduced previously for boolean satisfiability sat problem in the csp framework in addition to the naive individual filtering of xor constraint used in sat we are able to apply a global domain filtering algorithm by viewing these constraint a a collection of linear equality over the field of two element our most promising strategy extends this approach further to larger domain and applies the so called generalized xor constraint directly to csp variable this allows u to reap the benefit of the compact and structured representation that csps offer we demonstrate the effectiveness of our counting framework through experimental comparison with the solution enumeration approach which we believe is the current best generic solution counting method for csps and with solution counting in the context of sat and integer programming 
developing human centered agent architecture requires the integral consideration of architectural flexibility teamwork adaptability and context reasoning capability with the integration of various form of team intelligence including shared teamwork process and progress dynamic context management and infomation dependency reasoning and recognition primed collaborative decision mechanism r cast offer a flexible solution to developing cognitive aid for the support of human centered teamwork in information and knowledge intensive domain in this paper we present the key feature of r cast a evidence of it application in complex real world problem we give two experimental evaluation of r cast a teammate and decision aid of human command and control team 
in this paper we develop a novel probabilistic model of computational trust that allows agent to exchange and combine reputation report over heterogeneous correlated multi dimensional contract we consider the specific case of an agent attempting to procure a bundle of service that are subject to correlated quality of service failure e g due to use of shared resource or infrastructure and where the direct experience of other agent within the system consists of contract over different combination of these service to this end we present a formalism based on the kalman filter that represents trust a a vector estimate of the probability that each service will be successfully delivered and a covariance matrix that describes the uncertainty and correlation between these probability we describe how the agent direct experience of contract outcome can be represented and combined within this formalism and we empirically demonstrate that our formalism provides significantly better trustworthiness estimate than the alternative of using separate single dimensional trust model for each separate service where information regarding the correlation between each estimate is lost 
accurately locating user in a wireless environment is an important task for many pervasive computing and ai application such a activity recognition in a wifi environment a mobile device can be localized using signal received from various transmitter such a access point aps most localization approach build a map between the signal space and the physical location space in a offline phase and then using the received signal strength r map to estimate the location in an online phase however the map can be outdated when the signal strength value change with time due to environmental dynamic it is infeasible or expensive to repeat data calibration for reconstructing the r map in such a case it is important to adapt the model learnt in one time period to another time period without too much recalibration in this paper we present a location estimation approach based on manifold co regularization which is a machine learning technique for building a mapping function between data we describe lemancor a system for adapting the mapping function between the signal space and physical location space over different time period based on manifold co regularization we show that lemancor can effectively transfer the knowledge between two time period without requiring too much new calibration effort we illustrate leman cor s effectiveness in a real wifi environment 
this paper considers the problem of unsupervised semantic lexicon acquisition we introduce a fully automatic approach which exploit parallel corpus relies on shallow text property and is relatively inexpensive given the english framenet lexicon our method exploit word alignment to generate frame candidate list for new language which are subsequently pruned automatically using a small set of linguistically motivated filter evaluation show that our approach can produce high precision multilingual framenet lexicon without recourse to bilingual dictionary or deep syntactic and semantic analysis 
since the description of the semantic web paradigm in technology ha been proposed to allow it deployment and use however there is not yet any large and widely deployed set of semantically annotated web resource available a a result it is not possible to evaluate the use of the technology in a real environment and several assumption about how the semantic web should work are emerging in order to further investigate these assumption and the related technology we propose a simulation and evaluation platform the platform provides tool to create semantic web simulation using different technology for different purpose and to evaluate their performance in this paper we introduce the model of the platform and describe the current implementation the implementation facilitates the integration of technology for an essential operation on the semantic web namely semantic web service discovery we illustrate the use of the platform in a case study by implementing a semantic web where the jade multi agent platform provides the framework to describe the agent and a number of existing semantic web technology are embedded in agent behavior 
many interesting human action involve multiple interacting agent and also have typical duration further there is an inherent hierarchical organization of these activity in order to model these we introduce a new family of hidden markov model hmms that provide compositional state representation in both space and time and also a recursive hierarchical structure for inference at higher level of abstraction in particular we focus on two possible layer structure the hierarchical semi parallel hidden markov model hspahmm and the hierarchical parallel hidden semi markov model hpahsmm the lower layer of hspahmm consists of multiple hmms for each agent while the top layer consists of a single hsmm hpahsmm on the other hand ha multiple hsmms at the lower layer and a markov chain at the top layer we present efficient learning and decoding algorithm for these model and then demonstrate them first on synthetic time series data and then in an application for sign language recognition 
we describe the deep space network s scheduling problem based on a user requirement language the problem is difficult to encode by almost all existing planning and scheduling system we describe how it can be mapped into a system that support metric resource durative action simple temporal network constraint and task hierarchy among other language feature we also describe how we adapted a local search scheduler to generate schedule however we argue that the application will best serve the user if local search is combined with systematic search we describe how an implemented systematic search can be effectively applied to rescheduling 
a key challenge in deploying team of robot in real world application is to automate the control of teamwork such that the designer can focus on the taskwork existing teamwork architecture seeking to address this challenge are monolithic in that they commit to interaction protocol at the architectural level and do not allow the designer to mix and match protocol for a given task we present bite a behavior based teamwork architecture that automates collaboration in physical robot in a distributed fashion bite separate task behavior that control a robot s interaction with it task from interaction behavior that control a robot s interaction with it teammate this distinction provides for flexibility and modularity in term of the interaction used by teammate to collaborate effectively it also allows bite to synthesize and significantly extend existing teamwork architecture bite also incorporates key lesson learned in applying multi agent teamwork architecture in physical robot team we present empirical result from experiment with team of sony aibo robot executing bite and discus the lesson learned 
we describe an approach to the use of case based technique for natural language understanding and for action planning in a system for dialogue between a human and a robot which in our case is a uav unmanned aerial vehicle a single case base and case based reasoning engine is used both for understanding and for planning action by the uav this approach ha been developed through the work on an experimental dialogue system called cederic dialogue experiment where a number of user have solved task by dialogue with this system showed very adequate success rate while at the same time they indicated a few weak point in the system that could then easily be corrected 
trust should be substantially based on evidence further a key challenge for multiagent system is how to determine trust based on report from multiple source who might themselves be trusted to varying degree hence an ability to combine evidence based trust report in a manner that discount for imperfect trust in the reporting agent is crucial for multiagent system this paper understands trust in term of belief and certainty a s trust in b is reflected in the strength of a s belief that b is trustworthy this paper formulates certainty in term of evidence based on a statistical measure defined over a probability distribution of the probability of positive outcome this novel definition support important mathematical property including certainty increase a conflict increase provided the amount of evidence is unchanged and certainty increase a the amount of evidence increase provided conflict is unchanged moreover despite a more subtle definition than previous approach this paper establishes a bijection between evidence and trust space enabling robust combination of trust report and provides an efficient algorithm for computing this bijection 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
while computer have defeated the best human player in many classic board game progress in go remains elusive the large branching factor in the game make traditional adversarial search intractable while the complex interaction of stone make it difficult to assign a reliable evaluation function this is why most existing program rely on hand tuned heuristic and pattern matching technique yet none of these solution perform better than an amateur player our work introduces a composite approach aiming to integrate the strength of the proved heuristic algorithm the ai based learning technique and the knowledge derived from expert game specifically this paper present an application of the support vector machine svm for training the gnugo evaluation function 
wifi localization the task of determining the physical location of a mobile device from wireless signal strength ha been shown to be an accurate method of indoor and outdoor localization and a powerful building block for location aware application however most localization technique require a training set of signal strength reading labeled against a ground truth location map which is prohibitive to collect and maintain a map grow large in this paper we propose a novel technique for solving the wifi slam problem using the gaussian process latent variable model gplvm to determine the latent space location of unlabeled signal strength data we show how gplvm in combination with an appropriate motion dynamic model can be used to reconstruct a topological connectivity graph from a signal strength sequence which in combination with the learned gaussian process signal strength model can be used to perform efficient localization 
this paper lay theoretical and software foundation for a world wide argument web wwaw a large scale web of inter connected argument posted by individual to express their opinion in a structured manner first we extend the recently proposed argument interchange format aif to express argument with a structure based on walton s theory of argumentation scheme then we describe an implementation of this ontology using the rdf schema language and demonstrate how our ontology enables the representation of network of argument on the semantic web finally we present a pilot semantic web based system argdf through which user can create argument using different argumentation scheme and can query argument using a semantic web query language user can also attack or support part of existing argument use existing part of an argument in the creation of new argument or create new argumentation scheme a such this initial public domain tool is intended to seed a variety of future application for authoring linking navigating searching and evaluating argument on the web 
resolving ambiguity in the process of query translation is crucial to cross language information retrieval when only a bilingual dictionary is available in this paper we propose a novel approach for query translation disambiguation named spectral query translation model the proposed approach view the problem of query translation disambiguation a a graph partitioning problem for a given query a weighted graph is first created for all possible translation of query word based on the co occurrence statistic of the translation word the best translation of the query is then determined by the most strongly connected component within the graph the proposed approach distinguishes from previous approach in that the translation of all query word are estimated simultaneously furthermore translation probability are introduced in the proposed approach to capture the uncertainty in translating query empirical study with trec datasets have shown that the spectral query translation model achieves a relative improvement in cross language information retrieval compared to other approach that also exploit word co occurrence statistic for query translation disambiguation 
we describe a generalization of extensive form game that greatly increase representational power while still allowing efficient computation in the zero sum setting a principal feature of our generalization is that it place arbitrary convex optimization problem at decision node in place of the finite action set typically considered the possibly infinite action set mean we must forget the exact action taken feasible solution to the optimization problem remembering instead only some statistic sufficient for playing the rest of the game optimally our new model provides an exponentially smaller representation for some game in particular we show how to compactly represent and solve extensive form game with outcome uncertainty and a generalization of markov decision process to multi stage adversarial planning game 
we evaluate a new hybrid language processing approach designed for interactive application that maintain an interaction with user over multiple turn specifically we describe a method for using a simple topic hierarchy in combination with a standard information retrieval measure of semantic similarity to reason about the selection of appropriate feedback in response to extended language input in the context of an interactive tutorial system designed to support creative problem solving our evaluation demonstrates the value of using a machine learning approach that take feedback from expert into account for optimizing the hierarchy based feedback selection strategy 
the internet is full of information source providing various type of data from weather forecast to travel deal these source can be accessed via web form web service or r feed in order to make automated use of these source one need to first model them semantically writing semantic description for web source is both tedious and error prone in this paper we investigate the problem of automatically generating such model we introduce a framework for learning datalog definition for web source in which we actively invoke source and compare the data they produce with that of known source of information we perform an inductive search through the space of plausible source definition in order to learn the best possible semantic model for each new source the paper includes an empirical evaluation demonstrating the effectiveness of our approach on real world web source 
we explore a mean to both model and reason about partial observability within the scope of constraint based temporal reasoning prior study of uncertainty in temporal csps have required the realization of all exogenous process to be made entirely visible to the agent we relax this assumption and propose an extension to the simple temporal problem with uncertainty stpu one in which the executing agent is made aware of the occurrence of only a subset of uncontrollable event we argue that such a formalism is needed to encode those complex environment whose external phenomenon share a common hidden source of temporal causality after characterizing the level of controllability in the resulting partially observable stpu and various special case we generalize a known family of reduction rule to account for this relaxation introducing the property of extended contingency and sufficient observability we demonstrate that these modification enable a polynomial filtering algorithm capable of determining a local form of dynamic controllability however we also show that there do remain some instance whose global controllability cannot yet be correctly identified by existing inference rule leaving the true computational complexity of dynamic controllability an open problem for future research 
the paper present and evaluates the power of a new framework for optimization in graphical model based on and or search space the virtue of the and or representation of the search space is that it size may be far smaller than that of a traditional or representation we develop our work on constraint optimization problem cop and introduce a new generation of depth first branch and bound algorithm that explore an and or search space and use static and dynamic mini bucket heuristic to guide the search we focus on two optimization problem solvingweighted csps wcsp and finding themost probable explanation mpe in belief network we show that the new and or approach improves considerably over the classic or space on a variety of benchmark including random and real world problem we also demonstrate the impact of different lower bounding heuristic on branch and bound exploring and or space 
we demonstrate an end to end use case of the semantic web s utility for synthesizing ecological and environmental data elvis the ecosystem location visualization and information system is a suite of tool for constructing food web for a given location elvis functionality is exposed a a collection of web service and all input and output data is expressed in owl thereby enabling it integration with other semantic web resource in particular we describe using a triple shop application to answer sparql query from a collection of semantic web document 
motivated by application such a the spread of epidemic and the propagation of influence in social network we propose a formal model for analyzing the dynamic of such network our model is a stochastic version of discrete dynamical system using this model we formulate and study the computational complexity of two fundamental problem called reachability and predecessor existence problem which arise in the context of social network we also point out the implication of our result on other computational model such a hopfield network communicating finite state machine and systolic array 
the research described in this paper focus on global tempo transformation of monophonic audio recording of saxophone jazz performance more concretely we have investigated the problem of how a performance played at a particular tempo can be automatically rendered at another tempo while preserving it expressivity to do so we have developed a case based reasoning system called tempoexpress the result we have obtained have been extensively compared against a standard technique called uniform time stretching ut and show that our approach is superior to ut 
various problem in ai and multi agent system can be tackled by finding the most desirable element of a set given some binary relation example can be found in area a diverse a voting theory game theory and argumentation theory some particularly attractive solution set are defined in term of a covering relation a transitive subrelation of the original relation we consider three different type of covering upward downward and bidirectional and the corresponding solution concept known a the uncovered set and the minimal covering set we present the first polynomial time algorithm for finding the minimal bidirectional covering set an acknowledged open problem and prove that deciding whether an alternative is in a minimal upward or downward covering set is np hard furthermore we obtain various set theoretical inclusion which reveal a strong connection between von neumann morgenstern stable set and upward covering on the one hand and the bank set and downward covering on the other hand in particular we show that every stable set is also a minimal upward covering set 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
in order to solve problem of reliability of system based on lexical repetition and problem of adaptability of language dependent system we present a context based topic segmentation system based on a new informative similarity measure based on word co occurrence in particular our evaluation with the state of the art in the domain i e the c and the texttiling algorithm show improved result both with and without the identification of multiword unit 
it is well known how challenging is the task of coding complex agent for virtual environment this difficulty in developing and maintaining complex agent ha been plaguing commercial application of advanced agent technology in virtual environment in this paper we discus development of a commercial grade integrated development environment ide and agent architecture for simulation and training in a high fidelity virtual environment specifically we focus on two key area of contribution first we discus the addition of an explicit recipe mechanism to soar allowing reflection second we discus the development and usage of an ide for building agent using our architecture the approach we take is to tightly couple the ide to the architecture the result is a complete development and deployment environment for agent situated in a complex dynamic virtual world 
diamondhelp is a reusable java framework for building collaborative task guidance system for complex device such a digitally enabled home appliance diamondhelp combine a generic conversational interface adapted from online chat program with an application specific direct manipulation interface diamondhelp provides a thing to say mechanism for use without spoken language understanding it also support extension to take advantage of speech technology diamondhelp s software architecture factor all application specific content into two modular plug in one of which includes collagen and a task model 
we present a novel approach to recognizing textual entailment structural feature are constructed from abstract tree description which are automatically extracted from syntactic dependency tree these feature are then applied in a subsequence kernel based classifier to learn whether an entailment relation hold between two text our method make use of machine learning technique using a limited data set no external knowledge base e g wordnet and no handcrafted inference rule we achieve an accuracy of for text pair in the information extraction and question answering task for the rte test data and for the ret test data 
any web user is a potential knowledge contributor but it remains a challenge to make them devote their time contributing to some purpose in order to align individual with social interest we selected the captcha web resource protection application to embed knowledge elicitation within the user main task of accessing a web resource consequently unlike previous knowledge acquisition approach no extra effort is expected from user since they are already willing to use a captcha to perform some particular task we present an application where we extract pictorial knowledge from web user and experiment suggest that our approach enables knowledge acquisition while still satisfying captcha s security requirement 
the temporal boundary of many real world event are inherently vague in this paper we discus the problem of qualitative temporal reasoning about such vague event we show that several interesting reasoning task such a checking satisfiability checking entailment and calculating the best truth value bound can be reduced to reasoning task in a well known point algebra with disjunction furthermore we identify a maximal tractable subset of qualitative relation to support efficient reasoning 
in this paper we extend the logic programming based conformant planner described in son et al a to allow it to work on planning problem with more complex description of the initial state we also compare the extended planner with other concurrent conformant planner 
treebank parsing can be seen a the search for an optimally refined grammar consistent with a coarse training treebank we describe a method in which a minimal grammar is hierarchically refined using em to give accurate compact grammar the resulting grammar are extremely compact compared to other high performance parser yet the parser give the best published accuracy on several language a well a the best generative parsing number in english in addition we give an associated coarse to fine inference scheme which vastly improves inference time with no loss in test set accuracy 
this demonstration highlight the first of seven segment designed to develop a digital entity that will posse the potential for human empathy the experience in the first phase of the digital entity zoe zero one entity parallel a subset of learning and development activity encountered by human being during their first nine month of existence a website ha been created to provide a window to observe zoe s experience and action selection process in order to make her basic learning observable cumulative and evolutionary the human observer is invited to influence her action selection by setting the intensity of zoe s digital personality trait such a assertiveness reasoning ability and disposition action generate body based and emotive based feeling which are stored in memory structure significantly these structure serve a a foundation for later stage of learning understanding and reasoning 
in this paper we present retaliate an online reinforcement learning algorithm for developing winning policy in team first person shooter game retaliate ha three crucial characteristic individual bot behavior is fixed although not known in advance therefore individual bot work a plugins retaliate model the problem of learning team tactic through a simple state formulation discount rate commonly used in q iearning are not used a a result of these characteristic the application of the q learning algorithm result in the rapid exploration towards a winning policy against an opponent team in our empirical evaluation we demonstrate that retaliate adapts well when the environment change 
answer set programming asp and constraint logic programming over finite domain clp fd are two declarative progmmming paradigm that have been extensively used to encode application involving search optimization and reasoning e g commonsense reasoning and planning this paper present experimental comparison between the declarative encoding of various computationally hard problem in both framework the objective is to investigate how the solver in the two domain respond to different problem highlighting strength and weakness of their implementation and suggesting criterion for choosing one approach over the other ultimately the work in this paper is expected to lay the foundation for transfer of technology between the two domain e g suggesting way to use clp fd in the execution of asp 
we present a model for buying agent in e marketplace to interpret evaluation of seller provided by other buying agent known a advisor the interpretation of seller evaluation is complicated by the inherent subjectivity of each advisor the possibility that advisor may deliberately provide misleading evaluation to deceive competitor and the dynamic nature of seller and advisor behaviour that may naturally change seller evaluation over time using a bayesian approach we demonstrate how to cope with subjectivity deception and change in a principled way more specifically by modeling seller property and advisor evaluation function a dynamic random variable buyer can progressively learn a probabilistic model that naturally and correctly calibrates the interpretation of seller evaluation without having to resort to heuristic to explicitely detect and filter discount unreliable seller evaluation our model called blade is shown empirically to achieve lower mean error in the estimation of seller property when compared to other model for reasoning about advisor rating of seller in electronic maketplaces 
in the past year a lot of research effort ha been put into finding tractable subset of spatial and temporal calculus it ha been shown empirically that large tractable subset of these calculus not only provide efficient algorithm for reasoning problem that can be expressed with relation contained in the tractable subset but also surprisingly efficient solution to the general np hard reasoning problem of the full calculus an important step in this direction wa the refinement algorithm which provides a heuristic for proving tractability of given subset of relation in this paper we extend the refinement algorithm and present a procedure which identifies large tractable subset of spatial and temporal calculus automatically without any manual intervention and without the need for additional np hardness proof while we can only guarantee tractability of the resulting set our experiment show that for rcc and the interval algebra our procedure automatically identifies all maximal tractable subset using our procedure other researcher and practitioner can automatically develop efficient reasoning algorithm for their spatial or temporal calculus without any theoretical knowledge about how to formally analyse these calculus 
tracking the movement of a target based on limited observation play a role in many interesting application existing probabilistic tracking technique have shown considerable success but the majority assume simplistic motion model suitable for short term local motion prediction agent movement are often governed by more sophisticated mechanism such a a goal directed path planning algorithm in such context we must go beyond estimating a target s current location to consider it future path and ultimate goal we show how to use complex black box motion model to infer distribution over a target s current position origin and destination using only limited observation of the full path our approach accommodates motion model defined over a graph including complex pathing algorithm such a a robust and practical inference is achieved by using hidden semi markov model hsmms and graph abstraction the method ha also been extended to effectively track multiple indistinguishable agent via a greedy heuristic 
we study the backbone of the travelling salesperson optimization problem we prove that it is intractable to approximate the backbone with any performance guarantee assuming that p np and there is a limit on the number of edge falsely returned nevertheless in practice it appears that much of the backbone is present in close to optimal solution we can therefore often find much of the backbone using approximation method based on good heuristic we demonstrate that such backbone information can be used to guide the search for an optimal solution however the variance in runtimes when using a backbone guided heuristic is large this suggests that we may need to combine such heuristic with randomization and restarts in addition though backbone guided heuristic are useful for finding optimal solution they are le help in proving optimality 
partially observable markov decision process pomdps provide an appropriately rich model for agent operating under partial knowledge of the environment since finding an optimal pomdp policy is intractable approximation technique have been a main focus of research among them point based algorithm which scale up relatively well up to thousand of state an important decision in a point based algorithm is the order of backup operation over belief state prioritization technique for ordering the sequence of backup operation reduce the number of needed backup considerably but involve significant overhead this paper suggests a new way to order backup based on a soft clustering of the belief space our novel soft clustering method relies on the solution of the underlying mdp empirical evaluation verifies that our method rapidly computes a good order of backup showing order of magnitude improvement in runtime over a number of benchmark 
the ability to update the structure of a bayesian network when new data becomes available is crucial for building adaptive system recent work by sang beame and kautz aaai demonstrates that the well known davis putnam procedure combined with a dynamic decomposition and caching technique is an effective method for exact inference in bayesian network with high density and width in this paper we define dynamic model counting and extend the dynamic decomposition and caching technique to multiple run on a series of problem with similar structure this allows u to perform bayesian inference incrementally a the structure of the network change experimental result show that our approach yield significant improvement over the previous model counting approach on multiple challenging bayesian network instance 
in recent year there ha been a growing interest in using rich representation such a relational language for reinforcement learning however while expressive language have many advantage in term of generalization and reasoning extending existing approach to such a relational setting is a non trivial problem in this paper we present a first step towards the online learning and exploitation of relational model we propose a representation for the transition and reward function that can be learned online and present a method that exploit thesemodels by augmenting relational reinforcement learning algorithm with planning technique the benefit and robustness of our approach are evaluated experimentally 
this paper describes a learning framework for a central pattern generator based biped locomotion controller using a policy gradient method our goal in this study are to achieve biped walking with a d hardware humanoid and to develop an efficient learning algorithm with cpg by reducing the dimensionality of the state space used for learning we demonstrate that an appropriate feed back controller can be acquired within a thousand trial by numerical simulation and the obtained controller in numerical simulation achieves stable walking with a physical robot in the real world numerical simulation and hardware experiment evaluated walking velocity and stability furthermore we present the possibility of an additional online learning using a hardware robot to improve the controller within iteration 
we report a study of word meaning that test whether dynamical aspect of movie predict word use the movie were based on a novel representation of verb semantics called map for verb we asked preschool school age child to describe the movie and demonstrated that their distribution of word could be predicted by the dynamical aspect of the movie these result lend support to the empiricist position that word meaning are learned associatively 
we present a cnf to tree of bdds tob compiler with complexity at most exponential in the tree width we then present algorithm for interesting query on tob although some of the presented query algorithm are in the worst case exponential in the tree width our experiment show that tob can answer non trivial query like clausal entailment in reasonable time for several realistic instance while our tob tool compiles all the used instance d dnnf compilation failed for or of them based on the decomposition heuristic used also on the succeeded instance a d dnnf is up to time larger than the matching tob the tob compilation are often an order of magnitude faster than the d dnnf compilation this make tob a quite interesting knowledge compilation form 
often an agent that ha to solve a problem must choose which heuristic or strategy will help it the most in achieving it objective sometimes the agent wish to obtain additional unit of information on the possible heuristic and strategy in order to choose between them but it may be costly a a result the agent s goal is to acquire enough unit of information in order to make a decision while incurring minimal cost we focus on situation where the agent must decide in advance how many unit it would like to obtain we present an algorithm for choosing between two option and then formulate three method for the general case where there are k option to choose from we investigate the option algorithm and the general k option method effectiveness in two domain the sat domain and the ct computer game in both domain we present the experimental performance of our model result will show that applying the option algorithm is beneficial and provides the agent a substantial gain in addition applying the k option method in the domain investigated result in a moderate gain 
knowledge of which lexical item convey the same meaning in a given context is important for many natural language processing task this paper concern the substitutability of discourse connective in particular this paper proposes a datadriven method based on a minimum description length mdl criterion for automatically learning substitutability of connective the method is shown to outperform two baseline classifier 
in this paper we present new search strategy for agent with diverse preference searching cooperatively in complex environment with search cost the uniqueness of our proposed mechanism is in the integration of the coalition s ability to partition itself into sub coalition which continue the search autonomously into the search strategy a capability that wa neglected in earlier cooperative search model a we show throughout the paper this strategy is always favorable in comparison to currently known cooperative and autonomous search technique it ha the potential to significantly improve the searcher performance in various environment and in any case guarantee reaching at least a good a performance a that of other known method furthermore for many common environment we manage to significantly eliminate the consequential added computational complexity associated with the partitioning option by introducing innovative efficient algorithm for extracting the coalition s optimal search strategy we illustrate the advantage of the proposed model over currently known cooperative and individual search technique using an environment based on authentic setting 
in this paper we present a method for learning the structure of bayesian network bns without making any assumption on the probability distribution of the domain this is mainly useful for continuous domain where there is little guidance and many choice for the parametric distribution family to be used for the local conditional probability of the bayesian network and only a few have been examined analytically we therefore focus on bn structure learning in continuous domain we address the problem by developing a conditional independence test for continuous variable which can be readily used by any existing independence based bn structure learning algorithm our test is non parametric making no assumption on the distribution of the domain we also provide an effective and computationally efficient method for calculating it from data we demonstrate the learning of the structure of graphical model in continuous domain from real world data to our knowledge for the first time using independence based method and without distributional assumption we also experimentally show that our test compare favorably with existing statistical approach which use prediscretization and verify desirable property such a statistical consistency 
optimol a framework for online picture collection via incremental model learning is a novel automatic dataset collecting and model learning system for object categorization our algorithm mimic the human learning process in such a way that starting from a few training example the more confident data you incorporate in the training data the more reliahle model can be learnt our system us the internet a the nearly unlimited resource for image the learning and image collection process are done via an iterative and incremental scheme the goal of this work is to use this tremendous web resource to learn robust object category model in order to detect and search for object in real world scene 
a autonomous spacecraft and other robotic system grow increasingly complex there is a pressing need for capability that more accurately monitor and diagnose system state while maintaining reactivity mode estimation address this problem by reasoning over declarative model of the physical plant represented a a factored variant of hidden markov model hmms called probabilistic concurrent constraint automaton pcca previous mode estimation approach track a set of most likely pcca state trajectory enumerating them in order of trajectory probability although best first trajectory enumeration bfte is efficient ignoring the additional trajectory that lead to the same state can significantly underestimate the true state probability and result in misdiagnosis this paper introduces an innovative belief approximation technique called best first belief state enumeration bfbse that address this limitation by computing estimate probability directly from the hmm belief state update equation theoretical and empirical result show that bfbse significantly increase estimator accuracy us le memory and requires le computation time when enumerating a moderate number of estimate for the approximate belief state of subsystem sized model 
subset selection problem are relevant in many domain unfortunately their combinatorial nature prohibits solving them optimally in most case local search algorithm have been applied to subset selection with varying degree of success this work present compset a general algorithm for subset selection that invokes an existing local search algorithm from a random subset and it complementary set exchanging information between the two run to help identify wrong move preliminary result on complex sat max clique multidimensional knapsack and vertex cover problem show that compset improves the efficient stochastic hill climbing and tabu search algorithm by up to two order of magnitude 
recent scaling up of pomdp solver towards realistic application is largely due to point based method which quickly converge to an approximate solution formedium sized problem of this family hsvi which us trial based asynchronous value iteration can handle the largest domain in this paper we suggest a new algorithm fsvi that us the underlying mdp to traverse the belief space towards reward finding sequence of useful backup and show how it scale up better than hsvi on larger benchmark 
a basic reasoning problem in dynamic system is the projection problem determine if a formula hold after a sequence of action ha been performed in this paper we propose a tractable solution to the projection problem in the presence of incomplete first order knowledge and contextdependent action our solution is based on a type of progression that is we progress the initial knowledge base kb wrt the action sequence and answer the query against the resulting kb the form of reasoning we propose is always logically sound and is also logically complete when the query is in a certain normal form and the agent ha complete knowledge about the context of any context dependent action 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
model of crowd behavior facilitate analysis and prediction of human group behavior where people are affected by each other s presence unfortunately existing model leave many open challenge in particular psychology model often offer only qualitative description while computer science model are often simplistic and are not reusable from one simulated phenomenon to the next we propose a novel model of crowd behavior based on festinger s social companson theory sct we propose a concrete algorithmic framework for sct and evaluate it implementation in several crowd behavior scenario result from task measure and human judge evaluation show that the sct model produce improved result compared to base model from the literature 
it is well known that the size of the model is a bottleneck when using model based approach to diagnose complex system to answer this problem decentralised distributed approach have been proposed another problem which is far le considered is the size of the diagnosis itself however it can be huge enough especially in the case of on line monitoring and when dealing with uncertain observation we define two independence property state and transition independence and show their relevance to get a tractable representation of diagnosis in the context of both decentralised and incremental approach to illustrate the impact of these property on the diagnosis size experimental result on a toy example are given 
many real world problem including inference in bayes net can be reduced to sat the problem of counting the number of model of a propositional theory this ha motivated the need for efficient sat solver currently such solver utilize a modified version of dpll that employ decomposition and caching technique that significantly increase the time it take to process each node in the search space in addition the search space is significantly larger than when solving sat since we must continue searching even after the first solution ha been found it ha previously been demonstrated that the size of a dpll search tree can be significantly reduced by doing more reasoning at each node however for sat the reduction gained are often not worth the extra time required in this paper we verify the hypothesis that for sat this balance change in particular we show that additional reasoning can reduce the size of a sat solver s search space that this reduction cannot always be achieved by the already utilized technique of clause learning and that this additional reasoning can be cost effective 
we present a new computational model of bdi agent called the observation based bdi model the key point of this bdi model is to express agent belief desire and intention a a set of run computing path which is exactly a system in the interpreted system model a well known agent model due to halpern and his colleague our bdi model is computationally grounded in that we are able to associate the bdi agent model with a computer program and formula involving agent belief desire goal and intention can be understood a property of program computation we present a sound and complete proof system with respect to our bdi model and explore how symbolic model checking technique can be applied to model checking bdi agent in order to make our bdi model more flexible and practically realistic we generalize it so that agent can have multiple source of belief goal and intention 
diagnosticity operates a an important selection criterion for several computational model of concept combination unfortunately it ha not been clear how the diagnosticity of property and relational predicate of the concept combined can be formalized and quantified using an information retrieval method we compute in a uniform manner diagnosticity value of concept predicate we go on to present a reasoning system that attempt to create meaningful interpretation of novel noun noun combination the system is based solely on diagnostic predicate value and a set of constraint satisfaction rule we show the effectiveness and plausibility of our method and discus their potential 
recent research show that sat propositional satisfiability technique can be employed to build efficient system to compute answer set for logic program assat and cmodels are two well known such system they find an answer set from the full model for the completion of the input program which is iteratively augmented with loop formula making use of the fact that for non tight program during the model generation a partial assignment may be extensible to a full model but may not grow into any answer set we propose to add answer set extensibility checking on partial assignment furthermore given a partial assignment we identify a class of loop formula that are active on the assignment these active formula can be used to prune the search space we also provide an efficient method to generate these formula these idea can be implemented with a moderate modification on sat solver we have developed a new answer set solver sag on top of the sat solver mchaff empirical study on well known benchmark show that in most case it is faster than the state of the art answer set solver often by an order of magnitude in the few case when it is not the winner it is close to the top performer which show it robustness 
we study the complexity of influencing election through bribery how computationally complex is it for an external actor to determine whether by a certain amount of bribing voter a specified candidate can be made the election s winner we study this problem for election system a varied a scoring protocol and dodgson voting and in a variety of setting regarding homogeneous v nonhomogeneous electorate bribability bounded size v arbitrary sized candidate set weighted v unweighted voter and succinct v nonsuccinct input specification we obtain both polynomial time bribery algorithm and proof of the intractability of bribery and indeed our result show that the complexity of bribery is extremely sensitive to the setting for example we find setting in which bribery is np complete but manipulation by voter is in p and we find setting in which bribing weighted voter is np complete but bribing voter with individual bribe threshold is in p for the broad class of election including plurality borda k approval and veto known a scoring protocol we prove a dichotomy result for bribery of weighted voter we find a simple to evaluate condition that classifies every case a either np complete or in p 
agent that handle complex process evolving over a period of time need to be able to monitor the state of the process since the evolution of a process is often stochastic this requires probabilistic monitoring of process a probabilistic process modeling language is needed that can adequately capture our uncertainty about the process execution we present a language for describing probabilistic process model this language is functional in nature and the paper argues that a functional language provides a natural way to specify process model in our framework process have both state and value process may execute sequentially or in parallel and we describe two alternative form of parallelism an inference algorithm is presented that construct a dynamic bayesian network containing a variable for every subprocess that is executed during the course of executing a process we present a detailed example demonstrating the naturalness of the language 
constructing highly realistic agent is essential if agent are to be employed in virtual training system in training for collaboration based on face to face interaction the generation of emotional expression is one key in training for guidance based on one to many interaction such a direction giving for evacuation emotional expression must be supplemented by diverse agent behavior to make the training realistic to reproduce diverse behavior we characterize agent by using a various combination of operation rule instantiated by the user operating the agent to accomplish this goal we introduce a user modeling method based on participatory simulation these simulation enable u to acquire information observed by each user in the simulation and the operating history using these data and the domain knowledge including known operation rule we can generate an explanation for each behavior moreover the application of hypothetical reasoning which offer consistent selection of hypothesis to the generation of explanation allows u to use otherwise incompatible operation rule a domain knowledge in order to validate the proposed modeling method we apply it to the acquisition of an evacuee s model in a fire drill experiment we successfully acquire a subject s model corresponding to the result of an interview with the subject 
this paper summarizes the author s recent work in distributed constraint optimization dcop new local algorithm a well a theoretical result about the type of solution that these algorithm can reach are discussed 
in this paper we revisit the classical nlp problem of prepositional phrase attachment pp attachment given the pattern v np p np in the text where v is verb np is a noun phrase p is the preposition and np is the other noun phrase the question asked is where doe p np attach v or np this question is typically answered using both the word and the world knowledge word sense disambiguation wsd and data sparsity reduction dsr are the two requirement for pp attachment resolution our approach described in this paper make use of training data extracted from raw text which make it an unsupervised approach the unambiguous v p n and n p n tuples of the training corpus teach the system how to resolve the attachment in the ambiguous v n p n tuples of the test corpus a graph based approach to word sense disambiguation wsd is used to obtain the accurate word knowledge further the data sparsity problem is addressed by i detecting synonymy using the wordnet and ii doing a form of inferencing based on the matching of v and n in the unambiguous pattern of v p np np p np for experimentation brown corpus provides the training data andwall street journal corpus the test data the accuracy obtained for pp attachment resolution is close to the novelty of the system lie in the flexible use of wsd and dsr phase 
almost all natural language generation nlg system are faced with the problem of the generation of referring expression gre given a symbol corresponding to an intended referent how do we work out the semantic content of a referring expression that uniquely identifies the entity in question this is now one of the most widely explored problem in nlg over the last year a number of algorithm have been proposed for addressing different aspect of this problem but the different approach taken make it very difficult to compare and contrast the algorithm provided in any meaningful way in this paper we show how viewing the problem of referring expression generation a a search problem allows u to recast existing algorithm in a way that make their similarity and difference clear 
a team of agent is jointly able to achieve a goal if despite any incomplete knowledge they may have about the world or each other they still know enough to be able to get to a goal state unlike in the single agent case the mere existence of a working plan is not enough a there may be several incompatible working plan and the agent may not be able to choose a share that coordinate with those of the others some formalization of joint ability ignore this issue of coordination within a coalition others including those based on game theory deal with coordination but require a complete specification of what the agent believe such a complete specification is often not available here we present a new formalization of joint ability based on logical entailment in the situation calculus that avoids both of these pitfall 
eliminating previously recommended product in critiquing limit the choice available to user when they attempt to navigate back to product they critiqued earlier in the dialogue e g in search of cheaper alternative in the worst case a user may find that the only product she is prepared to accept e g having ruled out cheaper alternative ha been eliminated however an equally serious problem if previous recommendation are not eliminated is that product that satisfy the user s requirement if any may be unreachable by any sequence of critique we present a new version of progressive critiquing that leaf open the option of repeating a previous recommendation while also addressing the unreachability problem our empirical result show that the approach is most effective when user refrain from over critiquing attribute whose current value are acceptable 
a a promising formulation to represent and reason about agent dynamic behavious logic program update have been considerably studied recently while similarity and difference between various approach were discussed and evaluated by researcher there is a lack of method to represent different logic program update approach under a common framework in this paper we continue our study on a general framework for logic program conflict solving based on notion of strong and weak forgettings zhang foo wang we show that all major logic program update approach can be transformed into our framework under which each update approach becomes a specific conflict solving case with certain constraint we also investigate related computational property for these transformation 
viterbi alignment and decoding are two fundamental search problem in statistical machine translation both the problem are known to be np hard and therefore it is unlikely that there exists an optimal polynomial time algorithm for either of these search problem in this paper we characterize exponentially large subspace in the solution space of viterbi alignment and decoding each of these subspace admits polynomial time optimal search algorithm we propose a local search heuristic using a neighbourhood relation on these subspace experimental result show that our algorithm produce better solution taking substantially le time than the previously known algorithm for these problem 
intelligent agent must be able to handle the complexity and uncertainty of the real world logical ai ha focused mainly on the former and statistical ai on the latter markov logic combine the two by attaching weight to first order formula and viewing them a template for feature of markov network inference algorithm for markov logic draw on idea from satisfiability markov chain monte carlo and knowledge based model construction learning algorithm are based on the voted perceptron pseudo likelihood and inductive logic programming markov logic ha been successfully applied to problem in entity resolution link prediction information extraction and others and is the basis of the open source alchemy system 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
this paper deal with the automatic building of personalized hypermedia we build upon idea developed for educational hypermedia the definition of a domain model and the use of overlay user model since much work ha been done on learning user model and adapting hypermedia based on such model we tackle the core problem the automatic definition of a domain model for a static hypermedia 
parametric correspondence is a technique for matching image to a three dimensional symbolic reference map an analytic camera model is used to predict the location and appearance of landmark in the image generating a projection for an assumed viewpoint correspondence is achieved by adjusting the parameter of the camera model until the appearance of the landmark optimally match a symbolic description extracted from the image the matching of image and map feature is performed rapidly by a new technique called chamfer matching that compare the shape of two collection of shape fragment at a cost proportional to linear dimension rather than area these two technique permit the matching of spatially extensive feature on the ba s of shape which reduces the risk of ambiguous match and the dependence on viewing condition inherent in conventional image based correlation matching 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
we present a new abstraction algorithm for sequential imperfect information game while most prior abstraction algorithm employ a myopic expected value computation a a similarity metric our algorithm considers a higher dimensional space consisting of histogram over abstracted class of state from later stage of the game this enables our bottom up abstraction algorithm to automatically take into account potential a hand can become relatively better or worse over time and the strength of different hand can get resolved earlier or later in the game we further improve the abstraction quality by making multiple pass over the abstraction enabling the algorithm to narrow the scope of analysis to information that is relevant given abstraction decision made for earlier part of the game we also present a custom indexing scheme based on suit isomorphism that enables one to work on significantly larger model than before we apply the technique to head up limit texas hold em poker whereas all prior game theory based work for texas hold em poker used generic off the shelf linear program solver for the equilibrium analysis of the abstracted game we make use of a recently developed algorithm based on the excessive gap technique from convex optimization this paper is to our knowledge the first to abstract and game theoretically analyze all four betting round in one run rather than splitting the game into phase the resulting player g beat bluffbot g hyperborean monash bpp sparbot teddy and vexbot each with statistical significance to our knowledge those competitor are the best prior program for the game 
we present dc ssat a sound and complete divide and conquer algorithm for solving stochastic satisfiability ssat problem that outperforms the best existing algorithm for solving such problem zander by several order of magnitude with respect to both time and space dc ssat achieves this performance by dividing the ssat problem into subproblems based on the structure of the original instance caching the viable partial assignment vpas generated by solving these subproblems and using these vpas to construct the solution to the original problem dc ssat doe not save redundant vpas and each vpa saved is necessary to construct the solution furthermore dc ssat build a solution that is already human comprehensible allowing it to avoid the costly solution rebuilding phase in zander a a result dc ssat is able to solve problem using typically order of magnitude le space than zander allowing dc ssat to solve problem zander cannot solve due to space constraint and in spite of it more parsimonious use of space dcssat is typically order of magnitude faster than zander we describe the dc ssat algorithm and present empirical result comparing it performance to that of zander on a set of ssat problem 
computer game player are typically designed to play a single game today s best chess playing program cannot play checker or even tic tac toe general game playing is the problem of designing an agent capable of playing many different previously unseen game the first aaai general game playing competition wa held at aaai in order to promote research in this area in this article we survey some of the issue involved in creating a general game playing system and introduce our entry to that event the main feature of our approach is a novel method for automatically constructing effective search heuristic based on the formal game description our agent is fully implemented and tested in a range of different game 
much work ha been done on building a parser for natural language but most of this work ha concentrated on supervised parsing unsupervised parsing is a le explored area and unsupervised dependency parser ha hardly been tried in this paper we present two approach for building an unsupervised dependency parser one approach is based on learning dependency relation and the other on learning subtrees we also propose some other application of these approach 
a variety of text processing task require or benefit from semantic resource such a ontology and lexicon creating these resource manually is tedious time consuming and prone to error we present a new algorithm for using the web to determine the correct concept in an existing ontology to lexicalize previously unknown word such a might be discovered while processing text a detailed empirical comparison of our algorithm with two existing algorithm cilibrasi vitanyi maedche et al is described leading to insight into the source of the algorithm strength and weakness 
in this paper we address a critical problem in conversation system limited input interpretation capability when an interpretation error occurs user often get stuck and cannot recover due to a lack of guidance from the system to solve this problem we present a hybrid natural language query recommendation framework that combine natural language generation with query retrieval when receiving a problematic user query our system dynamically recommends valid query that are most relevant to the current user request so that the user can revise his request accordingly compared with existing method our approach offer two main contribution first improving query recommendation quality by combining query generation with query retrieval second adapting generated recommendation dynamically so that they are syntactically and lexically consistent with the original user input our evaluation result demonstrate the effectiveness of this approach 
we describe the formulation construction and evaluation of predictive model of human information seeking from a large dataset of web search activity we first introduce an expressive language for describing searching and browsing behavior and use this language to characterize several prior study of search behavior then we focus on the construction of predictive model from the data we review several analysis including an exploration of the property of user query and search session that are most predictive of future behavior we also investigate the influence of temporal delay on user action and representational tradeoff with varying the number of step of user activity considered finally we discus application of the predictive model and focus on the example of performing principled prefetching of content 
this paper present a new approach for merging prioritized knowledge base in possibilistic logic our approach is semantically defined by a model based merging operator in propositional logic and the merged result of our approach is a normal possibility distribution we also give an algorithm to obtain the syntactical counterpart of the semantic approach the logical property of our approach are considered finally we analyze the computational complexity of our merging approach 
this paper address the problem of detecting keywords in unconstrained speech the proposed algorithm search for the speech segment maximizing the average observation probability along the most likely path in the hypothesized keyword model a known this approach sometimes referred to a sliding model method requires a relaxation of the begin endpoint of the viterbi matching a well a a time normalization of the resulting score this make solution complex i e ln basic operation for keyword hmm model with l state and utterance with n frame we present here two altemative quite simple and efficient solution to this problem a first we provide a method that find the optimal segmentation according to the criterion of maximizing the average observation probability it us dynamic programming a a step but doe not require scoring for all possible begin endpoint while the worst case remains o ln this technique converged in at most l n basic operation in each experiment for two very different application b the second proposed algorithm doe not provide a segmentation but can be used for the decision problem of whether the utterance should be classified a containing the keyword or not provided a predefined threshold on the acceptable average observation probability this allows the algorithm to be even faster with fix cost of l n 
solving markov decision process mdps with continuous state space is a challenge due to among other problem the well known curse of dimensionality nevertheless numerous real world application such a transportation planning and telescope observation scheduling exhibit a critical dependence on continuous state current approach to continuous state mdps include discretizing their transition model in this paper we propose and study an alternative discretization free approach we call lazy approximation empirical study show that lazy approximation performs much better than discretization and we successfully applied this new technique to a more realistic planetary rover planning problem 
we use nearly sound logical constraint to infer hidden state of relational process we introduce a simple transition cost model which is parameterized by weighted constraint and a statetransition cost inference for this model i e finding a minimum cost state sequence reduces to a single state minimization ssm problem for relational horn constraint we give a practical approach to ssm based on logical reasoning and bounded search we present a learning method that discovers relational constraint using claudien de raedt and dehaspe and then tune their weight using perceptron update experiment in relational video interpretation show that our learned model improve on a variety of competitor 
in finding all solution to a constraint satisfaction problem or proving that there are none with a search algorithm that backtracks chronologically and form k way branch the order in which the value are assigned is immaterial however we show that if the value of a variable are assigned instead via a sequence of binary choice point and the removal of the value just tried from the domain of the variable is propagated before another value is selected the value ordering can affect the search effort we show that this depends on the problem constraint for some type of constraint we show that the saving in search effort can be significant given a good value ordering 
non negative matrix factorization nmf and probabilistic latent semantic indexing plsi have been successfully applied to document clustering recently in this paper we show that plsi and nmf optimize the same objective function although plsi and nmf are different algorithm a verified by experiment this provides a theoretical basis for a new hybrid method that run plsi and nmf alternatively each jumping out of local minimum of the other method successively thus achieving better final solution extensive experiment on real life datasets show relation between nmf and plsi and indicate the hybrid method lead to significant improvement over nmf only or plsi only method we also show that at first order approximation nmf is identical to statistic 
deciding what to branch on at each node is a key element of search algorithm we present four family of method for selecting what question to branch on they are all information theoretically motivated to reduce uncertainty in remaining subproblems in the first family a good variable to branch on is selected based on lookahead in real world procurement optimization this entropic branching method outperforms default cplex and strong branching the second family combine this idea with strong branching the third family doe not use lookahead but instead exploit feature of the underlying structure of the problem experiment show that this family significantly outperforms the state of the art branching strategy when the problem includes indicator variable a the key driver of complexity the fourth family is about branching using carefully constructed linear inequality constraint over set of variable 
artificial intelligence ha begun to play a critical role in basic science research in high energy physic ai method can aid precision measurement that elucidate the underlying structure of matter such a measurement of the mass of the top quark top quark can be produced only in collision at high energy particle accelerator most collision however do not produce top quark and making precise measurement requires culling these collision into a sample that is rich in collision producing top quark signal and spare in collision producing other particle background collision selection is typically performed with heuristic or supervised learning method however such approach are suboptimal because they assume that the selector with the highest classification accuracy will yield a mass measurement with the smallest statistical uncertainty in practice however the mass measurement is more sensitive to some background than others this paper present a new approach that us stochastic optimization technique to directly search for selector that minimize statistical uncertainty in the top quark mass measurement empirical result confirm that stochastically optimized selector have much smaller uncertainty this new approach contributes substantially to our knowledge of the top quark s mass a the new selector are currently in use selecting real collision 
several form of reasoning in ai like abduction closed world reasoning circumscription and disjunctive logic programming are well known to be intractable in fact many of the relevant problem are on the second or third level of the polynomial hierarchy in this paper we show how the powerful notion of treewidth can be fruitfully applied to this area in particular we show that all these problem become tractable actually even solvable in linear time if the treewidth of the involved formula or of the disjunctive logic program resp is bounded by some constant experiment with a prototype implementation prove the feasibility of this new approach in principle and also give u hint for necessary improvement in many area of computer science bounded treewidth ha been shown to be a realistic and practically relevant restriction we thus argue that bounded treewidth is a key factor in the development of efficient algorithm also in knowledge representation and reasoning despite the high worst case complexity of the problem of interest 
we aim to improve the performance of a syntactic parser that us a part of speech po tagger a a preprocessor pipelined parser consisting of po tagger and syntactic parser have several advantage such a the capability of domain adaptation however the performance of such system on raw text tends to be disappointing a they are affected by the error of automatic po tagging we attempt to compensate for the decrease in accuracy caused by automatic tagger by allowing the tagger to output multiple answer when the tag cannot be determined reliably enough we empirically verify the effectiveness of the method using an hpsg parser trained on the penn treebank our result show that ambiguous po tagging improves parsing if output of tagger are weighted by probability value and the result support previous study with similar intention we also examine the effectiveness of our method for adapting the parser to the genia corpus and show that the use of ambiguous po tagger can help development of portable parser while keeping accuracy high 
we have used semantic technology to design implement and deploy an interdisciplinary virtual observatory the virtual solar terrestrial observatory is a production data framework providing access to observational datasets it is in use by a community of scientist student and data provider interested in the middle and upper earth s atmosphere and the sun the data set span upper atmospheric terrestrial physic to solar physic the observatory allows virtual access to a highly distributed and heterogeneous set of data that appears a if all resource are organized stored and accessible from a local machine the system ha been operational since the summer of and ha shown registered data access by over of the active community last count over of the estimated person active research community this demonstration will highlight how semantic technology are being used to support data integration and more efficient data access in a multi disciplinary setting a full paper on this work is being published in the iaai deployed paper track 
popular route planning system window live local yahoo map google map etc generate driving direction using a static library of road and road attribute they ignore both the time at which a route is to be traveled and more generally the preference of the driver they serve we present a set of method for including driver preference and time variant traffic condition estimate in route planning these method have been incorporated into a working prototype named trip using a large database of gps trace logged by driver trip learns time variant traffic speed for every road in a widespread metropolitan area it also leverage a driver s past gps log when responding to future route query to produce route that are more suited to the driver s individual driving preference using experiment with real driving data we demonstrate that the route produced by trip are measurably closer to those actually chosen by driver than are the route produced by router that use static heuristic 
a distributed constraint optimization problem dcop is a formalism that capture the reward and cost of local interaction within a team of agent because complete algorithm to solve dcops are unsuitable for some dynamic or anytime domain researcher have explored incomplete dcop algorithm that result in locally optimal solution one type of categorization of such algorithm and the solution they produce is koptimality a k optimal solution is one that cannot be improved by any deviation by k or fewer agent this paper present the first known guarantee on solution quality for k optimal solution the guarantee are independent of the cost and reward in the dcop and once computed can be used for any dcop of a given constraint graph structure 
dynamic time warping is not suitable for on line application because it requires complete knowledge of both series before the alignment of the first element can be computed we present a novel online time warping algorithm which ha linear time and space cost and performs incremental alignment of two series a one is received in real time this algorithm is applied to the alignment of audio signal in order to track musical performance 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
we study the problem of computing a leximin optimal solution of a constraint network this problem is highly motivated by fairness and efficiency requirement in many real world application implying human agent we compare several generic algorithm which solve this problem in a constraint programming framework the first one is entirely original and the other one are partially based on existing work adapted to fit with this problem 
this paper address the task of aligning a database with a corresponding text the goal is to link individual database entry with sentence that verbalize the same information by providing explicit semantics to text link these alignment can aid the training of natural language generation and information extraction system beyond these pragmatic benefit the alignment problem is appealing from a modeling perspective the mapping between database entry and text sentence exhibit rich structural dependency unique to this task thus the key challenge is to make use of a many global dependency a possible without sacrificing tractability to this end we cast text database alignment a a structured multilabel classification task where each sentence is labeled with a subset of matching database entry in contrast to existing multilabel classifier our approach operates over arbitrary global feature of input and proposed label we compare our model with a baseline classifier that make locally optimal decision our result show that the proposed model yield a relative reduction in error and compare favorably with human performance 
an inescapable bottleneck with learning from large data set is the high cost of labeling training data unsupervised learning method have promised to lower the cost of tagging by leveraging notion of similarity among data point to assign tag however unsupervised and semi supervised learning technique often provide poor result due to error in estimation we look at method that guide the allocation of human effort for labeling data so a to get the greatest boost in discriminatory power with increasing amount of work we focus on the application of value of information to gaussian process classifier and explore the effectiveness of the method on the task of classifying voice message 
we present a system for textual inference the task of inferring whether a sentence follows from another text that us learning and a logical formula semantic representation of the text more precisely our system begin by parsing and then transforming sentence into a logical formula like representation similar to the one used by harabagiu et al an abductive theorem prover then try to find the minimum cost set of assumption necessary to show that one statement follows from the other these cost reflect how likely different assumption are and are learned automatically using information from syntactic semantic feature and from linguistic resource such a wordnet if one sentence follows from the other given only highly plausible low cost assumption then we conclude that it can be inferred our approach can be viewed a combining statistical machine learning and classical logical reasoning in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving we give experimental result from the recent pascal rte challenge competition on recognizing textual inference where a system using this inference algorithm achieved the highest confidence weighted score 
ensemble learning constitutes one of the main direction in machine learning and data mining ensemble allow u to achieve higher accuracy which is often not achievable with single model one technique which proved to be effective for constructing an ensemble of diverse classifier is the use of feature subset among different approach to ensemble feature selection genetic search wa shown to perform best in many domain in this paper a new strategy gas sefs genetic algorithmbased sequential search for ensemble feature selection is introduced instead of one genetic process it employ a series of process the goal of each of which is to build one base classifier experiment on data set are conducted comparing the new strategy with a previously considered genetic strategy for different ensemble size and for five different ensemble integration method the experiment show that gas sefs although being more time consuming often build better ensemble especially on data set with larger number of feature 
in designing markov decision process mdp one must define the world it dynamic a set of action and a reward function mdps are often applied in situation where there is a clear choice of reward function and in these case significant care must be taken to construct a reward function that induces the desired behavior in this paper we consider an analogous design problem crafting a target distribution in targeted trajectory distribution mdps ttd mdps ttd mdps produce probabilistic policy that minimize divergence from a target distribution of trajectory from an underlying mdp they are an extension of mdps that provide variety of experience during repeated execution here we present a brief overview of ttd mdps with approach for constructing target distribution then we present a novel authorial idiom for creating target distribution using prototype trajectory we evaluate these approach on a drama manager for an interactive game 
we propose a simple mechanism for incorporating advice prior knowledge in the form of simple rule into support vector method for both classification and regression our approach is based on introducing inequality constraint associated with datapoints that match the advice these constrained datapoints can be standard example in the training set but can also be unlabeled data in a semi supervised advice taking approach our new approach is simpler to implement and more efficiently solved than the knowledge based support vector classification method of fung mangasarian and shavlik and the knowledge based support vector regression method of mangasarian shavlik and wild while performing approximately a well a these more complex approach experiment using our new approach on a synthetic task and a reinforcement learning problem within the robocup soccer simulator show that our advice taking method can significantly outperform a method without advice and perform similarly to prior advice taking support vector machine 
we present a novel correspondence based technique for efficient shape classification and retrieval shape boundary are described by a set of ad hoc equally spaced point avoiding the need to extract landmark point by formulating the correspondence problem in term of a simple generative model we are able to efficiently compute match that incorporate scale translation rotation and reflection invariance a hierarchical scheme with likelihood cut off provides additional speed up in contrast to many shape descriptor the concept of a mean prototype shape follows naturally in this setting this enables model based classification greatly reducing the cost of the testing phase equal spacing of point can be defined in term of either perimeter distance or radial angle it is shown that combining the two lead to improved classification retrieval performance 
traditionally information extraction ie ha focused on satisfying precise narrow pre specified request from small homogeneous corpus e g extract the location and time of seminar from a set of announcement shifting to a new domain requires the user to name the target relation and to manually create new extraction rule or hand tag new training example this manual labor scale linearly with the number of target relation this paper introduces open ie oie a new extraction paradigm where the system make a single data driven pas over it corpus and extract a large set of relational tuples without requiring any human input the paper also introduces textrunner a fully implemented highly scalable oie system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user query we report on experiment over a web page corpus that compare textrunner with knowitall a state of the art web ie system textrunner achieves an error reduction of on a comparable set of extraction furthermore in the amount of time it take knowitall to perform extraction for a handful of pre specified relation textrunner extract a far broader set of fact reflecting order of magnitude more relation discovered on the fly we report statistic on textrunner s highest probability tuples and show that they contain over concrete fact and over more abstract assertion 
the use of large quantity of common sense ha long been thought to be critical to the automated understanding of the world to this end various group have collected repository of common sense in machine readable form however effort to apply these large body of knowledge to enable correspondingly large scale sensor based understanding of the world have been few challenge have included semantic gap between fact in the repository and phenomenon detected by sensor fragility of reasoning in the face of noise incompleteness of repository and slowness of reasoning with these large repository we show how to address these problem with a combination of novel sensor probabilistic representation web scale information retrieval and approximate reasoning in particular we show how to use the fact hand entered open mind indoor common sense database to interpret sensor trace of day to day activity with accuracy which is easy and precision recall which is not 
the concept of stereotype seems to be really adapted when wishing to extract meaningful description from data especially when there is a high rate of missing value this paper proposes a logical framework called default clustering based on default reasoning and local search technique the first experiment deal with the rediscovering of initial description from artificial data set the second one extract stereotype of politician in a real case generated from newspaper article it is shown that default clustering is more adapted in this context than the three classical clusterers considered 
we investigate legal and philosophical notion of privacy in the context of artificial agent our analysis utilizes a normative account of privacy that defends it value and the extent to which it should be protected privacy is treated a an interest with moral value to supplement the legal claim that privacy is a legal right worthy of protection by society and the law we argue that the fact that the only entity to access my personal data such a email is an artificial agent is irrelevant to whether a breach of privacy ha occurred what is relevant are the capacity of the agent what the agent is both able and empowered to do with that information we show how concept of legal agency and attribution of knowledge gained by agent to their principal are crucial to understanding whether a violation of privacy ha occurred when artificial agent access user personal data a natural language processing and semantic extraction used in artificial agent become increasingly sophisticated so the corporation that deploy those agent will be more likely to be attributed with knowledge of their user personal information thus triggering significant potential legal liability 
cf loadingtexthtml cf contextpath cf ajaxscriptsrc cfide script ajax cf jsonprefix cf clientid bbab e c a b a ae c a strong and uniform equivalence in answer set programming function settab var mytabs coldfusion layout gettablayout citationdetails mytabs on tabchange function tabpanel activetab document cookie picked activetab id function letemknow coldfusion window show letemknow function testthis alert test function loadalert alert i am in the load alert function loadalert alert i am in the load alert google load visualization package orgchart google setonloadcallback drawchart function drawchart var data new google visualization datatable data addcolumn string name data addcolumn string manager data addcolumn string tooltip data addrows v f cc for this article 
recent year have witnessed a growing interest in interactive narrative centered virtual environment for education training and entertainment narrative environment dynamically craft engaging story based experience for user who are themselves active participant in unfolding story a key challenge posed by interactive narrative is recognizing user goal so that narrative planner can dynamically orchestrate plot element and character action to create rich customized story in this paper we present an inductive approach to predicting user goal by learning probabilistic goal recognition model this approach ha been evaluated in a narrative environment for the domain of microbiology in which the user play the role of a medical detective solving a science mystery an empirical evaluation of goal recognition based on n gram model and bayesian network suggests that the model offer significant predictive power 
in this paper we propose a very simple yet general and effective method to make any cost insensitive classifier that can produce probability estimate cost sensitive the method called thresholding selects a proper threshold from training instance according to the misclassification cost similar to other cost sensitive meta learning method thresholding can convert any existing and future costinsensitive learning algorithm and technique into costsensitive one however by comparing with the existing cost sensitive meta learning method and the direct use of the theoretical threshold thresholding almost always produce the lowest misclassification cost experiment also show that thresholding ha the least sensitivity on the misclassification cost ratio thus it is recommended to use when the difference on misclassification cost is large 
we present cluster onset detection cod a novel algorithm to aid in detection of epidemic outbreak cod employ unsupervised learning technique in an online setting to partition the population into subgroup thus increasing the ability to make a detection over the population a a whole by decreasing the signal to noise ratio the method is adaptive and able to alter it clustering in real time without the need for detailed background knowledge of the population cod attempt to detect a cluster made up primarily of infected host we argue that this technique is largely complementary to the existing method for outbreak detection and can generally be combined with one or more of them we show empirical result applying cod to the problem of detecting a worm attack on a system of networked computer and show that this method result in approximately lower infection rate at a false positive rate of per week than the best previously reported result on this data set achieved using an hmm model customized for the outbreak detection task 
we study the problem of autonomous agent negotiating the allocation of multiple indivisible resource it is difficult to reach optimal outcome in bilateral or multi lateral negotiation over multiple resource when the agent preference for the resource are not common knowledge self interested agent often end up negotiating inefficient agreement in such situation we present a protocol for negotiation over multiple indivisible resource which can be used by rational agent to reach efficient outcome our proposed protocol enables the negotiating agent to identify efficient solution using systematic distributed search that visit only a subspace of the whole solution space 
this paper discus a mean for automatically supporting human in information and task prioritizing a new generic method based on the competitive task model is described it implementation is able to calculate priority of competing information entity which provides a way of allocating task to the process operator due to a combination of dynamic state and information entity it is usable a an adaptive control mechanism for attention and task allocation 
one goal of artificial intelligence is to enable the creation of robust fully autonomous agent that can coexist with u in the real world such agent will need to be able to learn both in order to correct and circumvent their inevitable imperfection and to keep up with a dynamically changing world they will also need to be able to interact with one another whether they share common goal they pursue independent goal or their goal are in direct conflict this paper present current research direction in machine learning multiagent reasoning and robotics and advocate their unification within concrete application domain ideally new theoretical result in each separate area will inform practical implementation while innovation from concrete multiagent application will drive new theoretical pursuit and together these synergistic research approach will lead u towards the goal of fully autonomous agent 
few existing argumentation framework are designed to deal with probabilistic knowledge and none are designed to represent possibilistic knowledge making them unsuitable for many real world domain in this paper we present a subjective logic based framework for argumentation which overcomes this limitation reasoning about the state of a literal in this framework can be done in polynomial time a dialogue game making use of the framework and a utility based heuristic for playing the dialogue game are also presented we then show how these component can be applied to contract monitoring the dialogue that emerge bear some similarity to the dialogue that occur when human argue about contract and our approach is highly suited to complex partially observable domain with fallible sensor where determining environment state cannot be done for free 
this paper explores the topic of sensitivity analysis in markov network by tackling question similar to those arising in the context of bayesian network the tuning of parameter to satisfy query constraint and the bounding of query change when perturbing network parameter even though the distribution induced by a markov network corresponds to ratio of multi linear function whereas the distribution induced by a bayesian network corresponds to multi linear function the result we obtain for markov network are a effective computationally a those obtained for bayesian network this similarity is due to the fact that conditional probability have the same functional form in both bayesian and markov network which turn out to be the more influential factor the major difference we found however is in how change in parameter value should be quantified a such parameter are interpreted differently in bayesian network and markov network 
we study property of program with monotone and convex constraint we extend to these formalism concept and result from normal logic programming they include tight program and fages lemma program completion and loop formula and the notion of strong and uniform equivalence with their characterization our result form an abstract account of property of some recent extension of logic programming with aggregate especially the formalism of smodels 
the reliable authentication of user attribute is an important prerequisite for the security of web based application digital certificate are widely used for that purpose however practical certification scenario can be very complex each certiticate carry a validity period and can be revoked during this period furthermore the verifying user ha to trust the issuer of certificate and revocation this work present a formal model which cover these aspect and provides a theoretical foundation for the decision about attribute authenticity even in complex scenario the model is based on the event calculus an ai technique from the field of temporal reasoning it us clark s completion to address the frame problem an example illustrates the application of the model 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
in an important recent paper lin and zhao introduced the concept of a loop formula and showed that the answer set for a logic program are exactly the model of clark s completion of the program that satisfy the loop formula just a supported set are a model theoretic account of completion externally supported set defined in this paper are a model theoretic counterpart of loop formula this reformulation of loop formula show that they are related to assumption set sacc and zaniolo and to unfounded set van gelder ross and schlipf leone rullo and scarcello invented many year earlier other contribution of this paper includes a simplification of the definition of a loop extending it to program with classical negation and infinite program and a generalization of the definition of a loop formula 
in several domain of spatial reasoning such a medical image interpretation spatial relation between structure play a crucial role since they are le prone to variability than intrinsic property of structure moreover they constitute an important part of available knowledge we show in this paper how this knowledge can be appropriately represented by graph and fuzzy model of spatial relation which are integrated in a reasoning process to guide the recognition of individual structure in image however pathological case may deviate substantially from generic knowledge we propose a method to adapt the knowledge representation to take into account the influence of the pathology on the spatial organization of a set of structure based on learning procedure we also propose to adapt the reasoning process using graph based propagation and updating 
algorithm for pruning game tree generally rely on a game being zero sum in the case of alpha beta pruning or constant sum in the case of multi player pruning algorithm such a speculative pruning while existing algorithm can prune non zero sum game pruning is much le effective than in constant sum game we introduce the idea of leaf value table which store an enumeration of the possible leaf value in a game tree using these table we are can make perfect decision about whether or not it is possible to prune a given node in a tree leaf value table also make it easier to incorporate monotonic heuristic for increased pruning in the player perfect information variant of spade we are able to reduce node expansion by two order of magnitude over the previous best zero sum and non zero sum pruning technique 
this synopsis present harvey mudd college s entry into the aaai scavenger hunt competition we are submiting a lap controlled robot which us commodity part and limited sensor to localize itself and perform arrow following and object recognition 
local search procedure for solving satisfiability problem have attracted considerable attention since the development of gsat in however recentwork indicates that for many real world problem complete search method have the advantage because modern heuristic are able to effectively exploit problem structure indeed to develop a local search technique that can effectively deal with variable dependency ha been an open challenge since in this paper we show that local search technique can effectively exploit information about problem structure producing significant improvement in performance on structured problem instance building on the earlier work of ostrowski et al we describe how information about variable dependency can be built into a local search so that only independent variable are considered for flipping the cost effect of a flip is then dynamically calculated using a dependency lattice that model dependent variable using gate specifically and or and equivalence gate the experimental study on hard structured benchmark problem demonstrates that our new approach significantly outperforms the previously reported best local search technique 
we present a new architecture for description logic implementation a range of new optimisation technique and an empirical analysis of their effectiveness 
in this paper we motivate the need for and challenge involved in supporting imprecise query over web database then we briefly explain our solution aimq a domain independent approach for answering imprecise query that automatically learns query relaxation order by using approximate functional dependency we also describe our approach for learning similarity between value of categorical attribute finally we present experimental result demonstrating the robustness efficiency and effectiveness of aimq 
when data is scarce or the alphabet is large smoothing the probability estimate becomes inescapable when estimating n gram model in this paper we propose a method that implement a form of smoothing by exploiting similarity information of the alphabet element the idea is to view the log conditional probability function a a smooth function defined over the similarity graph the algorithm that we propose us the eigenvectors of the similarity graph a the basis of the expansion of the log conditional probability function whose coefficient are found by solving a regularized logistic regression problem the experimental result demonstrate the superiority of the method when the similarity graph contains relevant information whilst the method still remains competitive with state of the art smoothing method even in the lack of such information 
recent research in answer set programming asp focus on different notion of equivalence between program which are relevant for program optimisation and modular programming prominent among these notion is uniform equivalence which check whether two program have the same semantics when joined with an arbitrary set of fact in this paper we study a family of more fine grained version of uniform equivalence where the alphabet of the added fact a well a the projection of answer set is taken into account the latter feature in particular allows the removal of auxiliary atom in computation which is important for practical programming aspect we introduce novel semantic characterisation for the equivalence problem under consideration and analyse the computational complexity for checking these problem we furthermore provide efficient reduction to quantified propositional logic yielding a rapid prototyping system for equivalence checking 
the paper present a support vector method for estimating probability in a real world problem the prediction of probability of survival in critically ill patient the standard procedure with support vector machine us platt s method to fit a sigmoid that transforms continuous output into probability the method proposed here exploit the difference between maximizing the auc and minimizing the error rate in binary classification task the conclusion is that it is preferable to optimize the auc first using a multivariate svm to then fit a sigmoid we provide experimental evidence in favor of our proposal for this purpose we used data collected in general icu at hospital in spain of these include coronary patient while the other do not treat coronary disease the total number of patient considered in our study wa 
linear discriminant analysis lda is a popular feature extraction technique in statistical pattern recognition however it often suffers from the small sample size problem when dealing with the high dimensional data moreover while lda is guaranteed to find the best direction when each class ha a gaussian density with a common covariance matrix it can fail if the class density are more general in this paper a new nonparametric feature extraction method stepwise nearest neighbor discriminant analysis snnda is proposed from the point of view of the nearest neighbor classification snnda find the important discriminant direction without assuming the class density belong to any particular parametric family it doe not depend on the nonsingularity of the within class scatter matrix either our experimental result demonstrate that snnda outperforms the existing variant lda method and the other state of art face recognition approach on three datasets from att and feret face database 
in this paper we present the behaviosite paradigm a new approach to coordination and control of distributed agent in a multiagent system inspired by biological parasite with behavior manipulation property behaviosites are code module that infect a system attaching themselves to agent and altering the sensory activity and action of those agent these behavioral change can be used to achieve altered potentially improved performance of the overall system thus behaviosites provide a mechanism for distributed control over a distributed system behaviosites need to be designed so that they are intimately familiar with the internal working of the environment and of the agent operating within it to demonstrate our approach we use behaviosites to control the behavior of a swarm of simple agent with a relatively low infection rate a few behaviosites can engender desired behavior over the swarm a a whole keeping it in one place leading it through checkpoint or moving the swarm from one stable equilibrium to another we contrast behaviosites a a distributed swarm control mechanism with alternative such a the use of group leader herder or social norm 
in this paper we propose a graph based construction of semi supervised gaussian process classifier our method is based on recently proposed technique for incorporating the geometric property of unlabeled data within globally defined kernel function the full machinery for standard supervised gaussian process inference is brought to bear on the problem of learning from labeled and unlabeled data this approach provides a natural probabilistic extension to unseen test example we employ expectation propagation procedure for evidence based model selection in the presence of few labeled example this approach is found to significantly outperform cross validation technique we present empirical result demonstrating the strength of our approach 
this paper focus on the linguistic aspect of noun phrase coreference investigating the knowledge source that can potentially improve a learningbased coreference resolution system unlike traditional knowledge lean coreference resolvers which rely almost exclusively on morpho syntactic cue we show how to induce feature that encode semantic knowledge from labeled and unlabeled corpus experiment on the ace data set indicate that the addition of these new semantic feature to a coreference system employing a fairly standard feature set significantly improves it performance 
this paper present a new method for extracting meaningful relation from unstructured natural language source the method is based on information made available by shallow semantic parser semantic information wa used to enhance a dependency tree kernel and to build semantic dependency structure used for enhanced relation extraction for several semantic classifier in our experiment the quality of the extracted relation surpassed the result of kernel based model employing only semantic class information 
this paper extends language understanding and plan inference to information graphic we identify the kind of communicative signal that appear in information graphic describe how we utilize them in a bayesian network that hypothesizes the graphic s intended message and discus the performance of our implemented system this work is part of a larger project aimed at making information graphic accessible to individual with sight impairment 
we develop two related theme learning procedure and knowledge transfer this paper introduces two method for learning procedure and one for transferring previously learned knowledge to a slightly different task we demonstrate by experiment that transfer accelerates learning 
few concept embody the goal of artificial intelligence a well a fully autonomous robot countless film and story have been made that focus on a future filled with autonomous agent that complete menial task or run errand that human do not want or are too busy to carry out one such task is driving automobile in this paper we summarize the work we have dune towards a future of fully autonomous vehicle specifically coordinating such vehicle safely and efficiently at intersection we then discus the implication this work ha for other area of ai including planning multiagent learning and computer vision 
we define a translation from weighted csp to signed max sat and a complete resolution style calculus for solving signed max sat based on these result we then describe an original exact algorithm for solving weighted csp finally we define several derived rule and prove that they enforce the main soft arc consistency defined in the literature when applied toweighted csp instance 
finding most probable explanation mpes in graphical model such a bayesian belief network is a fundamental problem in reasoning under uncertainty and much effort ha been spent on developing effective algorithm for this np hard problem stochastic local search sl approach to mpe solving have previously been explored but were found to be not competitive with state of the art branch bound method in this work we identify the shortcoming of earlier sl algorithm for the mpe problem and demonstrate how these can be overcome leading to an sl algorithm that substantially improves the state of the art in solving hard network with many variable large domain size high degree and most importantly network with high induced width 
a the mobile internet continues to grow there is an increasing need to provide user with effective search facility in this paper we argue that the standard web search approach of providing snippet text alongside each result is not appropriate given the interface limitation of mobile device instead we evaluate an alternative approach involving the use of related query in place of snippet text for result gisting 
how can we build artificial agent that can autonomously explore and understand their environment an immediate requirement for such an agent is to learn how it own sensory state corresponds to the external world property it need to learn the semantics of it internal state i e grounding in principle we a programmer can provide the agent with the required semantics but this will compromise the autonomy of the agent to overcome this problem we may fall back on natural agent and see how they acquire meaning of their own sensory state their neural firing pattern we can learn a lot about what certain neural spike mean by carefully controlling the input stimulus while observing how the neuron fire however neuron embedded in the brain do not have direct access to the outside stimulus so such a stimulus to spike association may not be learnable at all how then can the brain solve this problem we know it doe we propose that motor interaction with the environment is necessary to overcome this conundrum further we provide a simple yet powerful criterion sensory invariance for learning the meaning of sensory state the basic idea is that a particular form of action sequence that maintains invariance of a sensory state will express the key property of the environmental stimulus that gave rise to the sensory state our experiment with a sensorimotor agent trained on natural image show that sensory invariance can indeed serve a a powerful objective for semantic grounding 
in this paper we consider the problem of composing a set of web service where the requirement are specified in term of the input and output message of the composite workflow we propose a semantic model of message using rdf graph that encode owl abox assertion we also propose a model of web service operation where the input message requirement and output message characteristic are modeled using rdf graph pattern we formulate the message oriented semantic web service composition problem and show how it can be translated into a planning problem there are however significant challenge in scalably doing planning in this domain especially since dl reasoning may be performed to check if an operation can be given a certain input message we propose a two phase planning algorithm that incorporates dlp reasoning and evaluate the performance of this planning algorithm 
in recent year we have seen significant progress in the area of boolean satisfiability sat solving and it application a a new challenge the community is now moving to investigate whether similar advance can be made in the use of quantified boolean formula qbf qbf provides a natural framework for capturing problem solving and planning in multi agent setting however contrarily to single agent planning which can be effectively formulated a sat we show that a qbf approach to planning in a multi agent setting lead to significant unexpected computational difficulty we identify a a key difficulty of the qbf approach the fact that qbf solver often end up exploring a much larger search space than the natural search space of the original problem this is in contrast to the experience with sat approach we also show how one can alleviate these problem by introducing two special qbf formulation and a new qbf solution strategy we present experiment that show the effectiveness of our approach in term of a significant improvement in performance compared to earlier work in this area our work also provides a general methodology for formulating adversarial scenario in qbf 
this paper demonstrates how a model for temporal context reasoning can be implemented the approach is to detect temporally related event in natural language text and convert the event into an enriched logical representation reasoning is provided by a first order logic theorem prover adapted to text result show that temporal context reasoning boost the performance of a question answering system 
present development in the natural science are providing enormous and challenging opportunity for various ai technology to have an unprecedented impact in the broader scientific world if taken up such application would not only stretch present ai technology to the limit but if successful could also have a radical impact on the way natural science is conducted we review our experience with the robot scientist and other machine learning application a example of such ai inspired development we also consider potential future extension of such work based on the use of uncertainty logic a a generalisation of the robot scientist we introduce the notion of a chemical universal turing machine such a machine would not only be capable of complex cell simulation but could also be the basis for programmable chemical and biological experimentation robot 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
current research on qualitative spatial representation and reasoning usually focus on one single aspect of space however in real world application several aspect are often involved together this paper extends the well known rcc constraint language to deal with both topological and directional information and then investigates the interaction between the two kind of information given a topological rcc constraint network and a directional constraint network we ask when the joint network is satisfiable we show that when the topological network is over one of the three maximal tractable subclass of rcc the problem can be reduced into satisfiability problem in the rcc algebra and the rectangle algebra ra therefore reasoning technique developed for rcc and ra can be used to solve the satisfiability problem of a joint network 
temporal difference td network are a formalism for expressing and learning grounded world knowledge in a predictive form sutton and tanner however not all partially observable markov decision process can be efficiently learned with td network in this paper we extend td network by allowing the network update process answer network to depend on the recent history of previous action and observation rather than only on the most recent action and observation we show that this extension enables the solution of a larger class of problem than can be solved by the original td network or by history based method alone in addition we apply td network to a problem that while still simple is significantly larger than ha previously been considered we show that history extended td network can learn much of the common sense knowledge of an egocentric gridworld domain with a single bit of perception 
the paper present and evaluates the power of best first search over and or search space in graphical model the main virtue of the and or representation is it sensitivity to the structure of the graphical model which can translate into significant time saving indeed in recent year depth first and or branch and bound algorithm were shown to be very effective when exploring such search space especially when using caching since best first strategy are known to be superior to depth first when memory is utilized exploring the best first control strategy is called for in this paper we introduce two class of best first and or search algorithm those that explore a context minimal and or search graph and use static variable ordering and those that use dynamic variable ordering but explore an and or search tree the superiority of the best first search approach is demonstrated empirically on various real world benchmark 
because many student experience frustration during learning it is important to develop affective strategy to support student coping with frustration in interactive learning environment first we must devise affect recognition model to detect student affect second we need to determine when to intervene these condition are likely to be different for each student to determine how much frustration a student can persist through we should utilize model of student self efficacy to predict a student s frustration threshold third we should devise technique for responding empathetically before the student reach her threshold of frustration we propose an approach to support student coping with frustration in intelligent tutoring system that utilizes induced model of affect self efficacy and empathetic behavior to effectively reason about precisely when and how to intervene in frustration ridden learning situation 
the reference reconciliation problem consists in deciding whether different identifier refer to the same data i e correspond to the same world entity the l r system exploit the semantics of a rich data model which extends rdfs by a fragment of owl dl and swrl rule in l r the semantics of the schema is translated into a set of logical rule of reconciliation which are then used to infer correct decision both of reconciliation and no reconciliation in contrast with other approach the l r method ha a precision of by construction first experiment show promising result for recall and most importantly significant increase when rule are added 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
in this paper we study a recent formal model for qualitative spatial reasoning with cardinal direction relation we give an o n algorithm to check the consistency of a network of basic cardinal constraint with variable ranging over the set of connected region homeomorphic to the closed unit disk which includes a wide variety of irregular shaped region to the best of our knowledge this wa an open problem a previous algorithm for a domain that includes also disconnected region work in o n but for the problem we consider here such an algorithm cannot be used using the new algorithm we also show that the problem of deciding the consistency of a network of disjunctive cardinal constraint with variable ranging over the set of connected region is np complete our main contribution is based on result from the field of combinatorial geometry 
a martingale framework is proposed to enable support vector machine svm to adapt to timevarying data stream the adaptive svm is a onepass incremental algorithm that i doe not require a sliding window on the data stream ii doe not require monitoring the performance of the classifier a data point are streaming and iii work well for high dimensional multi class data stream our experiment show that the novel adaptive svm is effective at handling time varying data stream simulated using both a synthetic dataset and a multiclass real dataset 
in this demonstration we will present the tactical iraqi one of the implementation of the tactical language and culture training system tlts the system help learner acquire basic communicative skill in foreign language and culture learner practice their communication skill in a simulated village where they must develop rapport with the local people who in turn will help them accomplish mission such a postwar reconstruction each learner is ac companied by a virtual aide who can provide assistance and guidance if needed the aide can also act a a virtual tutor a part of an intelligent tutoring system giving the learner feedback on their performance learner communicate via a multimodal interface which permit them to speak and choose gesture on behalf of their character in the game the system employ video game technology and design technique in order to motivate and engage learner 
temporal difference reinforcement learning rl ha been successfully applied in several domain with large state set large action set however have received considerably le attention this paper demonstrates the use of knowledge transfer between related task to accelerate learning with large action set we introduce action transfer a technique that extract the action from the near optimal solution to the first task and us them in place of the full action set when learning any subsequent task when optimal action make up a small fraction of the domain s action set action transfer can substantially reduce the number of action and thus the complexity of the problem however action transfer between dissimilar task can be detrimental to address this difficulty we contribute randomized task perturbation rtp an enhancement to action transfer that make it robust to unrepresentative source task we motivate rtp action transfer with a detailed theoretical analysis featuring a formalism of related task and a bound on the suboptimality of action transfer the empirical result in this paper show the potential of rtp action transfer to substantially expand the applicability of rl to problem with large action set 
a same syntax extension of rdf to first order logic result in a collapse of the model theory due to logical paradox resulting from diagonalization rdf is thus the wrong material for building the semantic web tower 
we give a clear picture of the tractability intractability frontier for quantified constraint satisfaction problem qcsps under structural restriction on the negative side we prove that checking qcsp satisfiability remains pspace hard for all known structural property more general than bounded treewidth and for the incomparable hypergraph acyclicity moreover if the domain is not fixed the problem is pspace hard even for tree shaped constraint scope on the positive side we identify relevant tractable class including qcsps with prefix having bounded hypertree width and qcsps with a bounded number of guard the latter are solvable in polynomial time without any bound on domain or quantifier alternation 
shapiro et al presented a framework for representing goal change in the situation calculus in that framework agent adopt a goal when requested to do so by some agent reqr and they remain committed to the goal unless the request is cancelled by reqr a common assumption in the agent theory literature e g cohen and levesque rao and georgeff is that achievement goal that are believed to be impossible to achieve should be dropped in this paper we incorporate this assumption into shapiro et al s framework however we go a step further if an agent belief a goal is impossible to achieve it is dropped however if the agent later belief that it wa mistaken about the impossibility of achieving the goal the agent might readopt the goal in addition we consider an agent s goal a a whole when making them compatible with their belief rather than considering them individually ity with belief however it could be the case that each goal individually is compatible with an agent s belief but the set of all goal of the agent is incompatible with it belief in sec we present the situation calculus and reiter s action theory which form the basis of our framework in sec we present shapiro et al s framework and in sec we show how to extend the framework to take into consideration the dynamic interaction between belief and goal some property of the new framework are presented in sec in sec we sketch how to extend the framework further so that achievement goal that are believed to have been already achieved are dropped by the agent we conclude in sec 
this paper reconsiders the travelling tournament problem a complex sport scheduling application which ha attracted significant interest recently it proposes a population based simulated annealing algorithm with both itensification and diversitication the algorithm is organized a a series of simulated annealing wave each wave being followed by a macro intensification the diversification is obtained through the concept of elite run that opportunistically survive wave a parallel implementation of the algorithm on a cluster of workstation exhibit remarkable result it improves the best known solution on all considered benchmark sometimes reduces the optimality gap by about and produce novel best solution on instance that had been stable for several year 
the handling of exception in multiclass problem is a tricky issue in inductive logic programming ilp in this paper we propose a new formalization of the ilp problem which account for default reasoning and is encoded with first order possibilistic logic we show that this formalization allows u to handle rule with exception and to prevent an example to be classified in more than one class the possibilistic logic view of ilp problem can be easily handled at the algorithmic level a an optimization problem 
we describe an analogical method for constructing a structuralmodel froman unlabelled d line drawing the source case is represented a a schema that contains it d line drawing the line and intersection in the drawing the shape in drawing and the structural model of the device depicted in the drawing given a target drawing and a relevant source case our method first construct a graphical representation of the line and the intersection in the target drawing then us the mapping at the level of line intersection to transfer the shape representation from the source case to the target next us the mapping at the level of shape to transfer the structural model of the device from the source to the target the archytas system implement and evaluates this method of compositional analogy 
in this paper a state based approach for the constraint satisfaction problem csp is proposed the key novelty is an original use of state memorization during search to prevent the exploration of similar subnetworks classical technique to avoid the resurgence of previously encountered conicts involve recording conict set this contrast with our statebased approach which record subnetworks a snapshot of some selected domain already explored this knowledge is later used to either prune inconsistent state or avoid recomputing the solution of these subnetworks interestingly enough the two approach present some complementarity different state can be pruned from the same partial instantiation or conict set whereas different partial instantiation can lead to the same state that need to be explored only once also our proposed approach is able to dynamically break some kind of symmetry e g neighborhood interchangeability the obtained experimental result demonstrate the promising prospect of state based search 
the sharedplans theory provides an axiomatic framework of collaborative plan based on four type of intentional attitude however there still lack an adequate semantics for the potential intention operator in this paper we give a formal semantics to potential intention and examine model that can validate various relation between belief intention and potential intention 
we present a transcription system that take a music signal a input and return it musical score two stage of processing are used the first employ a fundamental frequency detector and an onset detector to transform input signal into a sequence of sound event the onset detection is inherently noisy this paper focus on the second stage going from sound event to a notated score we use a family of graphical model for this task we allow the result of onset detection to be noisy necessitating a search over possible segmentation of the sound event we use a large corpus of monophonic vocal music to evaluate our system our result show that our approach is well suited to the problem of music transcription the initial onset detection reduces the number of observation and make the system le instrument specific the search over segmentation corrects the error in the onset detection without such reasoning these error are magnified in subsequent rhythm transcription 
given a winning bid withdrawal in a combinatorial auction finding an alternative repair solution of adequate revenue without causing undue disturbance to the remaining winning bid in the original solution may be difficult or even impossible this bid taker exposure problem may be preemptively addressed by finding a solution that is robust to winning bid withdrawal we introduce the concept of monotonicity in expectation we provide impossibility result concerning truthful mechanism for robust solution with bounded social welfare loss in which the bid taker cannot rescind item from winning bidder to repair a solution we also show that this result extends to combinatorial auction that include a form of leveled commitment contract however we present a positive result regarding truthfulness for combinatorial auction in a restricted setting that comprises a computationally efficient allocation algorithm that seek to maximize expected social welfare 
many and diverse combinatorial problem have been solved successfully using finite domain constraint programming however to apply constraint programming to a particular domain the problem must first be modelled a a constraint satisfaction or optimisation problem since constraint provide a rich language typically many alternative model exist formulating a good model therefore requires a great deal of expertise this paper describes conjure a system that refines a specification of a problem in the abstract constraint specification language essence into a set of alternative constraint model refinement is compositional alternative constraint model are generated by composing refinement of the component of the specification experimental result demonstrate that conjure is able to generate a variety of model for practical problem from their essence specification 
this paper proposes a new method to rank the case classified by a decision tree the method applies a posteriori without modification of the tree and doesn t use additional training case it consists in computing the distance of the case to the decision boundary induced by the decision tree and to rank them according to this geometric score when the data are numeric it is very easy to implement and efficient the distance based score is a global ass contrary to other method that evaluate the score at the level of the leaf the distance based score give good result even with pruned tree so if the tree is intelligible this property is preserved with an improved ranking ability the main reason for the efficacity of the geometric method is that in most case when the classifier is sufficiently accurate error are located near the decision boundary 
what reasoning pattern do agent use to choose their action in game this paper study this question in the context of multi agent influence diagram maid it defines several kind of reasoning pattern and associate each with a pattern of path in a maid we asks the question what reasoning pattern have to hold in order for an agent to care about it decision the answer depends on what strategy are considered for other agent decision we introduce a new solution concept called well distinguishing wd strategy that capture strategy in which all the distinction an agent make really make a difference we show that when agent are playing wd strategy all situation in which an agent care about it decision can be captured by four reasoning pattern we furthermore show that when one of these four pattern hold there are some maid parameter value such that the agent actually doe care about it decision 
the intelligent use of tool is a general and important human competence that ai research ha not yet examined in depth other field have studied the topic however with result we can compile into a broad characterization of habile tool using agent in this paper we give an overview of research on the use of physical tool using this information to motivate the development of artificial habile agent specifically we describe how research goal and method in animal cognition overlap with those in artificial intelligence we argue that analysis of activity of tool using agent offer an informative way to evaluate intelligence 
web search engine are a great help for accessing web site but they present several problem regarding semantic ambiguity in order to solve them we propose new method for polysemy disambiguation of web resource and discovery of lexicalization and synonym of search query 
we consider the problem of reinforcement learning in factored state mdps in the setting in which learning is conducted in one long trial with no reset allowed we show how to extend existing efficient algorithm that learn the conditional probability table of dynamic bayesian network dbns given their structure to the case in which dbn structure is not known in advance our method learns the dbn structure a part of the reinforcement learning process and provably provides an efficient learning algorithm when combined with factored rmax 
solving large partially observable markov decision process pomdps is a complex task which is often intractable a lot of effort ha been made to develop approximate offline algorithm to solve ever larger pomdps however even state of the art approach fail to solve large pomdps in reasonable time recent development in online pomdp search suggest that combining offline computation with online computation is often more efficient and can also considerably reduce the error made by approximate policy computed offline in the same vein we propose a new anytime online search algorithm which seek to minimize a efficiently a possible the error made by an approximate value function computed offline in addition we show how previous online computation can be reused in following time step in order to prevent redundant computation our preliminary result indicate that our approach is able to tackle large state space and observation space efficiently and under real time constraint 
while conditional random field crfs have been applied successfully in a variety of domain their training remains a challenging task in this paper we introduce a novel training method for crfs called virtual evidence boosting which simultaneously performs feature selection and parameter estimation to achieve this we extend standard boosting to handle virtual evidence where an observation can be specified a a distribution rather than a single number this extension allows u to develop a unified framework for learning both local and compatibility feature in crfs in experiment on synthetic data a well a real activity classification problem our new training algorithm outperforms other training approach including maximum likelihood maximum pseudo likelihood and the most recent boosted random field 
we consider competition between seller offering similar item in concurrent online auction through a mediating auction institution where each seller must set it individual auction parameter such a the reserve price in such a way a to attract buyer we show that in the case of two seller with asymmetric production cost there exists a pure nash equilibrium in which both seller set reserve price above their production cost in addition we show that rather than setting a reserve price a seller can further improve it utility by shill bidding i e bidding a a buyer in it own auction this shill bidding is undesirable a it introduces inefficiency within the market however through the use of an evolutionary simulation we extend the analytical result beyond the two seller case and we then show that these inefficiency can be effectively reduced when the mediating auction institution us auction fee based on the difference between the auction closing and reserve price given this competition a key research question is how a seller should select their auction setting in order to best attract buyer and so increase their expected profit in this paper we consider this issue in term of setting the seller s reserve price since the role of the reserve price ha received attention in both single isolated auction and also in case where seller compete in particular we extend the existing analysis by considering how seller may improve their profit by shill bidding i e bidding within their own auction a a mean of setting an implicit reserve price we do so analytically in the case of two seller and then develop an evolutionary simulation to enable u to solve the general case of multiple seller moreover since shill bidding is generally undesirable it undermines trust in the institution and decrease overall market efficiency we then extend our evolutionary simulation to investigate how the institution can deter shill bidding through the use of appropriate auction fee more specifically we make the following contribution we analytically describe the seller s equilibrium strategy for setting reserve price for the two seller case and we advance the current state of the art by finding nash equilibrium by iteratively discretising the search space we show that although no pure strategy exist when the seller are symmetric these can be found if production cost differ sufficiently between the two seller for the first time we investigate shill bidding within a setting of competing seller to this end we derive analytical expression for the seller s expected utility when seller shill bid using these expression we show that without auction fee a seller can considerably benefit by shill bidding when faced with competition 
abstract we consider competition between seller offering similar item in concurrent online auction through a mediating auction institution where each seller must set it individual auction parameter such a the reserve price in such a way a to attract buyer we show that in the case of two seller with asymmetric production cost there exists a pure nash equilibrium in which both seller set reserve price above their production cost in addition we show that rather than setting a reserve price a seller can further improve it utility by shill bidding i e bidding a a buyer in it own auction this shill bidding is undesirable a it introduces inefficiency within the market however through the use of an evolutionary simulation we extend the analytical result beyond the two seller case and we then show that these inefficiency can be effectively reduced when the mediating auction institution us auction fee based on the difference between the auction closing and reserve price 
method developed for the qualitative simulation of dynamical system have turned out to be powerful tool for studying genetic regulatory network a bottleneck in the application of these method is the analysis of the simulation result in this paper we propose a combination of qualitative simulation and model checking technique to perform this task systematically and efficiently we apply our approach to the analysis of the complex network controlling the nutritional stress response in the bacterium escherichia coli 
since the document understanding conference have been the forum for researcher in automatic text summarization to compare method and result on common test set over the year several type of summarization task have been addressed single document summarization multi document summarization summarization focused by question and headline generation this paper is an overview of the achieved result in the different type of summarization task we compare both the broader class of baseline system and human a well a individual pair of summarizers both human and automatic an analysis of variance model is fitted with summarizer and input set a independent variable and the coverage score a the dependent variable and simulation based multiple comparison were performed the result document the progress in the field a a whole rather then focusing on a single system and thus can serve a a future reference on the work done up to date a well a a starting point in the formulation of future task result also indicate that most progress in the field ha been achieved in generic multi document summarization and that the most challenging task is that of producing a focused summary in answer to a question topic 
monte carlo go is a promising method to improve the performance of computer go program this approach determines the next move to play based on many monte carlo sample this paper examines the relative advantage of additional sample and enhancement for monte carlo go by parallelizing monte carlo go we could increase sample size by two order of magnitude experimental result obtained in go show strong evidence that there are trade offs among these advantage and performance indicating a way for monte carlo go to go 
this paper is to investigate rank aggregation based on multiple user centered measure in the context of the web search we introduce a set of technique to combine ranking list in order of user interest termed a a user profile moreover based on the click history data a kind of taxonomic hierarchy automatically model the user profile which can include a variety of attribute of user interest we mainly focus on the topic a user is interested in and the degree of user interest in these topic the primary goal of our work is to form a broadly acceptable ranking list rather than that determined by an individual ranking measure experiment result on a real click history data set show the effectiveness of our aggregation technique to improve the web search 
social network have recently garnered considerable interest with the intention of utilizing social network for the semantic web several study have examined automatic extraction of social network however most method have addressed extraction of the strength of relation our goal is extracting the underlying relation between entity that are embedded in social network to this end we propose a method that automatically extract label that describe relation among entity fundamentally the method cluster similar entity pair according to their collective context in web document the descriptive label for relation are obtained from result of clustering the proposed method is entirely unsupervised and is easily incorporated with existing social network extraction method our experiment conducted on entity in researcher social network and political social network achieved clustering with high precision and recall the result showed that our method is able to extract appropriate relation label to represent relation among entity in the social network 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
recently significant progress ha been made on learning structured predictor via coordinated training algorithm such a conditional random field and maximum margin markov network unfortunately these technique are based on specialized training algorithm are complex to implement and expensive to run we present a much simpler approach to training structured predictor by applying a boosting like procedure to standard supervised training method the idea is to learn a local predictor using standard method such a logistic regression or support vector machine but then achieve improved structured classification by boosting the influence of misclassified component after structured prediction re training the local predictor and repeating further improvement in structured prediction accuracy can be achieved by incorporating dynamic feature i e an extension whereby the feature for one predicted component can depend on the prediction already made for some other component we apply our technique to the problem of learning dependency parser from annotated natural language corpus by using logistic regression a an efficient base classifier for predicting dependency link between word pair we are able to efficiently train a dependency parsing model via structured boosting that achieves state of the art result in english and surpasses state of the art in chinese 
the open nature of collaborative recommender system allows attacker who inject biased profile data to have a significant impact on the recommendation produced standard memory based collaborative filtering algorithm such a knearest neighbor have been shown to be quite vulnerable to such attack in this paper we examine the robustness of model based recommendation algorithm in the face of profile injection attack in particular we consider two recommendation algorithm one based on k mean clustering and the other based on probabilistic latent semantic analysis plsa these algorithm aggregate similar user into user segment that are compared to the profile of an active user to generate recommendation traditionally model based algorithm have been used to alleviate the scalability problem associated with memory based recommender system we show empirically that these algorithm also offer significant improvement in stability and robustness over the standard knearest neighbor approach when attacked furthermore our result show that particularly the plsa based approach can achieve comparable recommendation accuracy 
an autonomous robot using symbolic reasoning sensing and acting in a real environment need the ability to create and maintain the connection between symbol representing object in the world and the corresponding perceptual representation given by it sensor this connection ha been named perceptual anchoring in complex environment anchoring is not always easy to establish the situation may often be ambiguous a to which percept actually corresponds to a given symbol in this paper we extend perceptual anchoring to deal robustly with ambiguous situation by providing general method for detecting them and recovering from them we consider different kind of ambiguous situation and present planning based method to recover from them we illustrate our approach by showing experiment involving a mobile robot equipped with a color camera and an electronic nose 
using the achievement of my research group over the last year i provide evidence to support the following hypothesis by complementing each other cooperating reasoning process can achieve much more than they could if they only acted individually most of the work of my group ha been on process for mathematical reasoning and it application e g to formal method the reasoning process we have studied include proof search by meta level inference proof planning abstraction analogy symmetry and reasoning with diagram representation discovery formation and evolution by analysing diagnosing and repairing failed proof and planning attempt forming and repairing new concept and conjecture and forming logical representation of informally stated problem other learning of new proof method from example proof finding counter example reasoning under uncertainty the presentation of and interaction with proof the automation of informal argument in particular we have studied how these different kind of process can complement each other and cooperate to achieve complex goal we have applied this work to the following area proof by mathematical induction and co induction analysis equation solving mechanic problem the building of ecological model the synthesis verification transformation and editing of both hardware and software including logic functional and imperative program security protocol and process algebra the configuration of hardware game playing and cognitive modelling 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
evaluation measure play an important role in machine learning because they are used not only to compare different learning algorithm but also often a goal to optimize in constructing learning model both formal and empirical work ha been published in comparing evaluation measure in this paper we propose a general approach to construct new measure based on the existing one and we prove that the new measure are consistent with and finer than the existing one we also show that the new measure is more correlated to rms root mean square error with artificial datasets finally we demonstrate experimentally that the greedy search based algorithm such a artificial neural network trained with the new and finer measure usually can achieve better prediction performance this provides a general approach to improve the predictive performance of existing learning algorithm based on greedy search 
compiling bayesian network bns is a hot topic within probabilistic modeling and processing in this paper we propose a new method for compiling bns into multi linear function mlfs based on zero suppressed binary decision diagram zbdds which are a graph based representation of combinatorial item set our method differs from the original approach of darwiche et al which encodes bns into conjunctive normal form cnfs and then translates cnfs into factored mlfs our approach directly translates a bn into a set of factored mlfs using a zbdd based symbolic probability calculation the mlf may have exponential computational complexity but our zbdd based data structure provides a compact factored form of the mlf and arithmetic operation can be executed in a time almost linear with the zbdd size in our method it is not necessary to generate the mlf for the whole network a we can extract mlfs for only part of the network related to the query avoiding unnecessary calculation of redundant mlf term we present experimental result for some typical benchmark example although our algorithm is simply based on the mathematical definition of probability calculation performance is competitive to existing state of the art method 
we present a new efficient algorithm for obtaining utilitarian optimal solution to disjunctive temporal problem with preference dtpps the previous state of the art system achieves temporal preference optimization using a sat formulation with it creator attributing it performance to advance in sat solving technique we depart from the sat encoding and instead introduce the valued dtp vdtp in contrast to the traditional semiring based formalism that annotates legal tuples of a constraint with preference our framework instead assigns elementary cost to the constraint themselves after proving that the vdtp can express the same set of utilitarian optimal solution a the dtpp with piecewise constant preference function we develop a method for achieving weighted constraint satisfaction within a meta csp search space that ha traditionally been used to solve dtps without preference this allows u to directly incorporate several powerful technique developed in previous decision based dtp literature finally we present empirical result demonstrating that an implementation of our approach consistently outperforms the sat based solver by order of magnitude 
desktop user commonly work on multiple task the tasktracer system provides a convenient low cost way for such user to define a hierarchy of task and to associate resource with those task with this information tasktracer then support the multi tasking user by configuring the computer for the current task to do this it must detect when the user switch the task and identify the user s current task at all time this problem of task switch detection is a special case of the general problem of change point detection it involves monitoring the behavior of the user and predicting in real time when the user move from one task to another we present a framework that analyzes a sequence of observation to detect task switch first a classifier is trained discriminatively to predict the current task based only on feature extracted from the window in focus second multiple single window prediction specifically the class probability estimate are combined to obtain more reliable prediction this paper study three such combination method a simple voting b a likelihood ratio test that ass the variability of the task probability over the sequence of window and c application of the viterbi algorithm under an assumed task transition cost model experimental result show that all three method improve over the single window prediction and that the viterbi approach give the best result 
we present a novel method for detecting the boundary between object in image that us a large hierarchical semantic ontology wordnet the semantic object hierarchy in wordnet ground this ill posed segmentation problem so that true boundary are defined a edge between instance of different class and all other edge are clutter to avoid fully classifying each pixel which is very difficult in generic image we evaluate the semantic similarity of the two region bounding each edge in an initial oversegmentation semantic similarity is computed using wordnet enhanced with appearance information and is largely orthogonal to visual similarity hence two region with very similar visual attribute but from different category can have a large semantic distance and therefore evidence of a strong boundary between them and vice versa the ontology is trained with image from the uc berkeley image segmentation benchmark extended with manual labeling of the semantic content of each image segment result on boundary detection against the benchmark image show that semantic similarity computed through wordnet can significantly improve boundary detection compared to generic segmentation 
this paper describes a hong kong mtr corporation subway project to enhance and extend the current web based engineering work and traffic information management system etms with an intelligent ai engine the challenge is to be able to fully and accurately encapsulate all the necessary domain and operation knowledge on subway engineering work and to be able to apply this knowledge in an efficient manner for both validation a well a scheduling since engineering work can only be performed a few hour each night it is crucially important that the ai engine maximizes the number of job done while ensuring operational safety and resource availability previously all constraint resource checking and scheduling decision were made manually the new ai approach streamlines the entire planning scheduling and rescheduling process and extends the etms with intelligent ability to automatically detect potential conflict a work request are entered check all approved work schedule for any conflict before execution generate weekly operational schedule repair schedule after change and generate quarterly schedule for planning the ai engine us a rule representation combined with heuristic search and a genetic algorithm for scheduling an iterative repair algorithm wa used for dynamic rescheduling 
latent semantic indexing lsi ha been shown to be effective in recovering from synonymy and polysemy in text retrieval application however since lsi ignores class label of training document lsi generated representation are not a effective in classification task to address this limitation a process called sprinkling is presented sprinkling is a simple extension of lsi based on augmenting the set of feature using additional term that encode class knowledge however a limitation of sprinkling is that it treat all class and classifier in the same way to overcome this we propose a more principled extension called adaptive sprinkling a a leverage confusion matrix to emphasise the difference between those class which are hard to separate the method is tested on diverse classification task including those where class share ordinal or hierarchical relationship these experiment reveal that a can significantly enhance the performance of instance based technique knn to make them competitive with the state of the art svm classifier the revised representation generated by a also have a favourable impact on svm performance 
statistical relational learning srl algorithm learn statistical model from relational data such a that stored in a relational database we previously introduced view learning for srl in which the view of a relational database can be automatically modified yielding more accurate statistical model the present paper present sayu vista an algorithm which advance beyond the initial view learning approach in three way first it learns view that introduce new relational table rather than merely new field for an existing table of the database second new table or new field are not limited to being approximation to some target concept instead the new approach performs a type of predicate invention the new approach avoids the classical problem with predicate invention of learning many useless predicate by keeping only new field or table i e new predicate that immediately improve the performance of the statistical model third retained field or table can then be used in the definition of further new field or table we evaluate the new view learning approach on three relational classification task 
we introduce a new method for finding node semantically related to a given node in a hyperlinked graph the green method based on a classical markov chain tool it is generic adjustment free and easy to implement we test it in the case of the hyperlink structure of the english version of wikipedia the on line encyclopedia we present an extensive comparative study of the performance of our method versus several other classical method in the case of wikipedia the green method is found to have both the best average result and the best robustness 
we study the problem of building effective heuristic for achieving cunjunctive goal from heuristic for individual goal we consider a straightforward method for building conjunctive heuristic that smoothly trade off between previous common method in addition to first explicitly formulating the problem of designing conjunctive heuristic our major contribution is the discovery that this straightforward method substantially outperforms previously used method across a wide range of domain based on a single positive real parameter k our heuristic measure sum the individual heuristic value for the subgoal conjuncts each raised to the k th power varying k allows loose approximation and combination of the previous min max and sum approach while mitigating some of the weakness in those approach our empirical work show that for many benchmark planning domain there exist fixed parameter value that perform well we give evidence that these value can be found automatically by training our method applied to top level conjunctive goal show dramatic improvement over the heuristic used in the ff planner across a wide range of planning competition benchmark also our heuristic without computing landmark consistently improves upon the success ratio of a recently published landmark based planner ff l 
this paper proposes an approach for learning call admission control cac policy in a cellular network that handle several class of traffic with different resource requirement the performance measure in cellular network are long term revenue utility call blocking rate cbr and handoff failure rate cdr reinforcement learning rl can be used to provide the optimal solution however such method fails when the state space and action space are huge we apply a form of neuroevolution ne algorithm to inductively learn the cac policy which is called cn call admission control scheme using ne comparing with the q learning based cac scheme in the constant traffic load show that cn can not only approximate the optimal solution very well but also optimize the cbr and cdr in a more flexibility way additionally the simulation result demonstrate that the proposed scheme is capable of keeping the handoff dropping rate below a pre specified value while still maintaining an acceptable cbr in the presence of smoothly varying arrival rate of traffic in which the state space is too large for practical deployment of the other learning scheme 
modelling data in structured domain requires establishing the relation among pattern at multiple scale when these pattern arise from sequential data the multiscale structure also contains a dynamic component that must be modelled particularly a is often the case if the data is unsegmented probabilistic graphical model are the predominant framework for labelling unsegmented sequential data in structured domain their use requires a certain degree of a priori knowledge about the relation among pattern and about the pattern themselves this paper present a hierarchical system based on the connectionist temporal classification algorithm for labelling unsegmented sequential data at multiple scale with recurrent neural network only experiment on the recognition of sequence of spoken digit show that the system outperforms hidden markov model while making fewer assumption about the domain 
we consider the problem of semantic annotation of semi structured document according to a target xml schema the task is to annotate a document in a tree like manner where the annotation tree is an instance of a tree class defined by dtd or w c xml schema description in the probabilistic setting we cope with the tree annotation problem a a generalized probabilistic context free parsing of an observation sequence where each observation come with a probability distribution over terminal supplied by a probabilistic classifier associated with the content of document we determine the most probable tree annotation by maximizing the joint probability of selecting a terminal sequence for the observation sequence and the most probable parse for the selected terminal sequence 
human visual capability ha remained largely beyond the reach of engineered system despite intensive study and considerable progress in problem understanding algorithm and computing power we posit that significant progress can be made by combining existing technology from computer vision idea from theoretical neuroscience and the availability of large scale computing power for experimentation from a theoretical standpoint our primary point of departure from current practice is our reliance on exploiting time in order to turn an otherwise intractable unsupervised problem into a locally semi supervised and plausibly tractable learning problem from a pragmatic perspective our system architecture follows what we know of cortical neuroanatomy and provides a solid foundation for scalable hierarchical inference this combination of feature promise to provide a range of robust object recognition capability 
social law is perceived a evolving through the competition of individual social strategy held by the agent a strategy with strong authority accepted by many agent will tend to diffuse to the remaining agent the authority of a social strategy is determined by not only the number of but also the collective social position of it overlaid agent this paper present a novel collective strategy diffusion model in agent social law evolution in the model social strategy that have strong authority are impressed on the other agent the agent will accept partially or in full or reject them based on their own social strategy and social position the diffusion of social strategy proceeds in a series of step and the final result depends on the interplay between the force driving diffusion and the counteracting force 
in the past ten year boosting ha become a major field of machine learning and classification this paper brings contribution to it theory and algorithm we first unify a well known top down decision tree induction algorithm due to kearns and mansour and discrete adaboost freund and schapire a two version of a same higher level boosting algorithm it may be used a the basic building block to devise simple provable boosting algorithm for complex classifier we provide one example the first boosting algorithm for oblique decision tree an algorithm which turn out to be simpler faster and significantly more accurate than previous approach 
we consider a schedule optimization problem where each activity to be scheduled ha a duration dependent quality profile and activity duration must be determined that maximize overall quality within given deadline and resource constraint to solve this quality maximization problem prior work ha proposed a hybrid search scheme where a linear programming solver for optimally setting the duration of temporally related activity is embedded within a larger search procedure that incrementally post sequencing constraint to resolve resource conflict under this approach dual concern of establishing feasibility and optimizing quality are addressed in an integrated fashion in this paper we propose an alternative approach where feasibility and optimization concern are treated separately first we establish a resource feasible partial order schedule assuming minimum duration for all activity second these fixed duration constraint are relaxed and quality optimal duration are determined experimental result indicate a tradeoff when resource capacity constraint are loose the integrated hybrid approach performs comparably to the separated scheme however in problem with tighter capacity constraint we find that separation of concern enables both better solving capability and higher quality result following from these result we discus potential synergy between problem objective of maintaining temporal flexibility and maximizing quality 
this paper characterizes the complexity of the core in coalitional game there are different proposal for representing coalitional game in a compact way where the worth of coalition may be computed in polynomial time in all those framework it wa shown that core non emptiness is a co np hard problem however for the most general of them it wa left a an open problem whether it belongs to co np or it actually is an harder problem we solve this open problem in a positive way indeed we are able to show that for the case of transferable payoff the problem belongs to co np for any compact representation of the game where the worth of coalition may be computed in polynomial time also non deterministic polynomial time encompassing all previous proposal of this kind this is proved by showing that game with empty core have small infeasibility certificate the picture is completed by looking at coalitional game with non transferable payoff we propose a compact representation based on marginal contribution net also in this case we are able to settle the precise complexity of core non emptiness which turn out to be p complete 
we analyze the computational and communication complexity of combinatorial auction from a new perspective the degree of interdependency between the item for sale in the bidder preference denoting by gk the class of valuation displaying up to k wise dependency we consider the hierarchy g g gm where m is the number of item for sale we show that the minimum non trivial degree of interdependency wise dependency is sufficient to render np hard the problem of computing the optimal allocation but we also exhibit a restricted class of such valuation for which computing the optimal allocation is easy on the other hand bidder preference can be communicated efficiently i e exchanging a polynomial amount of information a long a the interdependency between item are limited to set of cardinality up to k where k is an arbitrary constant the amount of communication required to transmit the bidder preference becomes super polynomial under the assumption that only value query are allowed when interdependency occur between set of cardinality g m where g m is an arbitrary function such that g m a m we also consider approximate elicitation in which the auctioneer learns asking polynomially many value query an approximation of the bidder actual preference 
we investigate three parameterized algorithmic scheme for graphical model that can accommodate trade offs between time and space and or adaptive caching aoc i variable elimination and conditioning vec i and tree decomposition with conditioning tdc i we show that aoc i is better than the vanilla version of both vec i and tdc i and use the guiding principle of aoc i to improve the other two scheme finally we show that the improved version of vec i and tdc i can be simulated by aoc i which emphasizes the unifying power of the and or framework 
in this paper we introduce a new dynamic bayesian network that separate the speaker and their speaking turn in a multi person conversation we protect the speaker privacy by using only feature from which intelligible speech cannot be reconstructed the model we present combine data from multiple audio stream segment the stream into speech and silence separate the different speaker and detects when other nearby individual who are not wearing microphone are speaking no pre trained speaker specific model are used so the system can be easily applied in new and different environment we show promising result in two very different datasets that vary in background noise microphone placement and quality and conversational dynamic 
the diagnosis of a discrete event system is the problem of computing possible behavior of the system given observation of the actual behavior and testing whether the behavior are normal or faulty we show how the diagnosis problem can be translated into the propositional satisfiability problem sat and solved by algorithm for sat our experiment demonstrate that current sat algorithm can solve much bigger diagnosis problem than traditional diagnosis algorithm can 
semantic inference is an important component in many natural language understanding application classical approach to semantic inference rely on complex logical representation however practical application usually adopt shallower lexical or lexical syntactic representation but lack a principled inference framework we propose a generic semantic inference framework that operates directly on syntactic tree new tree are infened by applying entailment rule which provide a unified representation for varying type of inference rule were generated by manual and automatic method covering generic linguistic structure a well a specific lexical based inference initial empirical evaluation in a relation extraction setting support the validity of our approach 
coordinating multiple agent that need to perform a sequence of action to maximize a system level reward requires solving two distinct credit assignment problem first credit must be assigned for an action taken at time step t that result in a reward at time step t t second credit must be assigned for the contribution of agent i to the overall system performance the first credit assignment problem is typically addressed with temporal difference method such a q learning the second credit assignment problem is typically addressed by creating custom reward function to address both credit assignment problem simultaneously we propose the q update with immediate counterfactual reward learning quicr learning designed to improve both the convergence property and performance of q learning in large multi agent problem quicr learning is based on previous work on single time step counterfactual reward described by the collective framework result on a traffic congestion problem show that quicr learning is significantly better than a q learner using collective based single time step counterfactual reward in addition quicr learning provides significant gain over conventional and local q learning additional result on a multi agent grid world problem show that the improvement due to quicr learning are not domain specific and can provide up to a ten fold increase in performance over existing method 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many case but are rarely used due to their relative complexity and obscurity a challenge then is to make machine learning approach to data analysis available to the average biologist in a user friendly way this challenge is addressed by the biomind arraygenius product an easy to use web based system providing microarray analysis based on genetic prognunming kernel method and incorporation of knowledge from biological ontology and genegenius it sister product for snp data this paper focus on the obstacle faced and lesson learned in the course of creating deploying maintaining and selling arraygenius and genegenius many of which are generic to any effort involving the creation of complex ai based product addressing complex domain problem 
in most case based reasoning cbr system there ha been little research done on validating new knowledge specifically on how previous knowledge differs from current knowledge a a result of conceptual change this paper proposes two method that enable the domain expert who is nonexpert in artificial intelligence ai to interactively supervise the knowledge validation process in a cbr system and to enable dynamic updating of the system to provide the best diagnostic question the first method is based on formal concept analysis which involves a graphical representation and comparison of the concept and a summary description highlighting the conceptual difference we propose a dissimilarity metric for measuring the degree of variation between the previous and current concept when a new case is added to the knowledge base the second method involves determining unexpected classification based association rule to form critical question a the knowledge base get updated 
the paper present and discus a method for rank ordering alternative on the basis of constraint induced by generic principle expressing for instance the relative importance of criterion or by example of ordering between particular alternative without resorting to the use of an aggregation operation for evaluating the alternative the approach which remains qualitative is based on the minimal specificity principle of possibility theory in order to complete the constraint it is compared on an illustrative example to an aggregation based approach using choquet integral the way constraint expressed in the choquet integral setting translate into constraint in the proposed approach is discussed 
this paper report on recent work in the field of information retrieval that attempt to go beyond the overly simplified approach of representing document and query a bag of word simple model make it difficult to accurately model a user s information need the model presented in the paper is based on markov random field and allows almost arbitrary feature to be encoded this provides a powerful mechanism for modeling many of the implicit constraint a user ha in mind when formulating a query simple instantiation of the model that consider dependency between the term in a query have shown to significantly outperform bag of word model further extension of the model are possible to incorporate even more complex constraint based other domain knowledge finally we describe what place our model ha within the broader realm of artificial intelligence and propose several open question that may be of general interest to the field 
the use of embedded technology ha become widespread many complex engineered system comprise embedded feature to perform self diagnosis or self reconfiguration these feature require fast response time in order to be useful in domain where embedded system are typically deployed researcher often advocate the use of compilation based approach to store the set of environment resp solution to a diagnosis resp reconfiguration problem in some compact representation however the size of a compiled representation may be exponential in the treewidth of the problem in this paper we propose a novel method for compiling the most preferred environment in order to reduce the large space requirement of our compiled representation we show that approximate compilation is an effective mean of generating the highest valued environment while obtaining a representation whose size can be tailored to any embedded application the method also provides a graceful way to tradeoff space requirement with the completeness of our coverage of the environment space 
the training of support vector machine svm involves a quadratic programming problem which is often optimized by a complicated numerical solver in this paper we propose a much simpler approach based on multiplicative update this idea wa first explored in cristianini et al but it convergence is sensitive to a learning rate that ha to be fixed manually moreover the update rule only work for the hard margin svm which is known to have poor performance on noisy data in this paper we show that the multiplicative update of svm can be formulated a a bregman projection problem and the learning rate can then be adapted automatically moreover because of the connection between boosting and bregman distance we show that this multiplicative update for svm can be regarded a boosting the weighted parzen window classifier motivated by the success of boosting we then consider the use of an adaptive ensemble of the partially trained svms extensive experiment show that the proposed multiplicative update rule with an adaptive learning rate lead to faster and more stable convergence moreover the proposed ensemble ha efficient training and comparable or even better accuracy than the best tuned soft margin svm 
in game such a kriegspiel chess a chess variant where player have no direct knowledge of the opponent s piece location the belief state s size dwarf those of other partial information game like bridge scrabble and poker and there is no easy way to generate state satisfying the given observation we show that statistical sampling approach can be developed to do well in such game we show that it is not necessary for the random sample to consist only of game board that satisfy each and every one of a player s observation in fact we win more often by beginning with such completely consistent board and gradually switching a the game progressed to board that are merely consistent with the latest observation this surprising result is explained by noting that a the game progress a board that is consistent with the last move becomes more and more likely to be consistent with the entire set of observation even if we have no idea what sequence of move might have actually generated this board 
a critical porblem faced by current supervised wsd system is the lack or manually annotated training data tackling this data acquisition bottleneck is crucial in order to build high accuracy and wide coverage wsd system in this paper we show that the approach of automatically gathering training example from parallel text is scalable to a large set of noun we conducted evaluation on the noun of senseval english all word task using fine grained sense scoring our evaluation show that training on example gathered from mb of parallel text achieves accuracy comparable to the best system of senseval english all word task and significantly outperforms the baseline of always choosing sense of wordnet 
complex problem solving typically involves the generation of a procedure consisting of an ordered sequence of step analogical reasoning is one strategy for solving complex problem and visual reasoning is another visual analogy pertain to analogy based only on visual knowledge in this paper we describe the use of galatea a computational model of visual analogy in problem solving to model the problem solving of a human subject l l wa a given the task of solving a complex problem using analogy in a domain that contained both visual and non visual knowledge and wa encouraged to use visual analogy we describe how galatea model l s use of visual analogy in problem solving 
we investigate a formalism for reasoning with multiple local ontology connected by directional semantic mapping we propose a relatively small change of semantics which localizes inconsistency thereby making unnecessary global satisfiability check and preserve directionality of knowledge import a characterization of inference using a fixed point operator which can form the basis of a cache based implementation for local reasoner a truly distributed tableau algorithm for case when the local reasoner use subset of shiq throughout we indicate the applicability of the result to several recent proposal for knowledge representation and reasoning that support modularity scalability and distributed reasoning 
this paper describes a framework for recognizing contradiction between multiple text source by relying on three form of linguistic information a negation b antonymy and c semantic and pragmatic information associated with the discourse relation two view of contradiction are considered in which a novel method of recognizing contrast and of finding antonymy are described contradiction are used for informing fusion operator in question answering our experiment show promising result for the detection of contradiction 
the potentially catastrophic impact of a bioterrorist attack make developing effective detection method essential for public health in the case of anthrax attack a delay of hour in making a right decision can lead to hundred of life lost current detection method trade off reliability of alarm for early detection of outbreak the performance of these method can be improved by modem disease specific modeling technique which take into account the potential cost and effect of an attack to provide optimal warning we study this optimization problem in the reinforcement learning framework the key contribution of this paper is to apply partially observable markov decision process pomdps on outbreak detection mechanism for improving alarm function in anthrax outbreak detection our approach relies on estimating the future benefit of true alarm and the cost of false alarm and using these quantity to identify an optimal decision we present empirical evidence illustrating that the performance of detection method with respect to sensitivity and timeliness is improved significantly by utilizing pomdps 
in this research we investigated user s behavior while facing a system coping with common knowledge about keywords and compared it with not only classic word spotting method but also with random text mining we show how even a simple implementation of our idea can enrich the conversation and increase the naturalness of computer s utterance our result show that even very commonsensical utterance are more natural than classic approach and also method we developed to make a conversation more interesting for arousing opinion exchange during the session we will also briefly introduce our idea of combining latest nlp achievement into one holistic system where the main engine we want to base on commonsense processing and affective computing 
interactive evolutionary computation iec ha proven useful in a variety of application by combining the subjective evaluation of a user with the massive parallel search power of the genetic algorithm ga here we articulate a framework for an extension of iec into collaborative interactive evolution in which multiple user guide the evolutionary process in doing so we introduce the ability for user to combine their effort for the purpose of evolving effective solution to problem this necessarily give rise to the possibility of conflict between user we draw on the salient feature of the ga to resolve these conflict and lay the foundation for this new paradigm to be used a a tool for conflict resolution in complex group wise human computer interaction task 
in this paper we introduce a mobile service that extract reputation of a product from weblogs by cellular phone during shopping if the user take a photo of a product barcode on the package with a cellular phone camera ubiquitous metadata scouter first get the product metadata name manufacturer etc from the internet and collect blog that review the product also it analyzes the blog content with nlp technique and ontology then it indicates the overall reputation positive or negative and other related product that are the subject of much discussion in the blog this paper illustrates each function of this service and a public experiment and evaluation at a real consumer electronics store and book store in tokyo in march 
this paper present a new framework for self supervised sensorimotor learning we demonstrate this framework with a system that learns to mimic a zebra finch directly modeled on the dynamic of how male fledgling acquire birdsong from their father our system first listens to the song of an adult finch by listening to it own initially nascent attempt at mimicry through an articulatory synthesizer the system organizes motor map generating it vocalization our approach is founded on the notion of cross modal clustering introduced in coen a and is unusual for it recursive reuse of perceptual mechanism in developing motor control in this paper we outline this framework present it result on the unsupervised acquisition of birdsong and discus other potential application 
this paper describes the integration of robot path planning and spatial task modeling into a software system that teach the operation of a robot manipulator deployed on international space station i the system address the complexity of the manipulator the limited direct view of the i exterior and the unpredictability of lighting condition in the workspace robot path planning is used not for controlling the manipulator but for automatically checking error of a student learning to operate the manipulator and for automatically producing illustration of good and bad motion in training 
the treatment of exogenous event in planning is practically important in many domain in this paper we focus on planning with exogenous event that happen at known time and affect the plan action by imposing that the execution of certain plan action must be during some time window when action have duration handling such constraint add an extra difficulty to planning which we address by integrating temporal reasoning into planning we propose a new approach to planning in domain with duration and time window combining graph based planning and disjunctive constraint based temporal reasoning our technique are implemented in a planner that took part in the th international planning competition showing very good performance in many benchmark problem 
reasoning about agent preference on a set of alternative and the aggregation of such preference into some social ranking is a fundamental issue in reasoning about multi agent system when the set of agent and the set of alternative coincide we get the ranking system setting a famous type of ranking system are page ranking system in the context of search engine such ranking system do not exist in empty space and therefore agent incentive should be carefully considered in this paper we define three measure for quantifying the incentive compatibility of ranking system we apply these measure to several known ranking system such a pagerank and prove tight bound on the level of incentive compatibility under two basic property strong monotonicity and non imposition we also introduce two novel nonimposing ranking system one general and the other for the case of system with three participant a full axiomatization is provided for the latter 
situated agent which use learning real time search are well poised to address challenge of real time path finding in robotic and computer game application they interleave a local lookahead search with movement execution explore an initially unknown map and converge to better path over repeated experience in this paper we first investigate how three known extension of the most popular learning real time search algorithm lrta influence it performance in a path finding domain then we combine automatic state abstraction with learning real time search our scheme of dynamically building a state abstraction allows u to generalize update to the heuristic function thereby speeding up learning the novel algorithm converges up to time faster than lrta with only one fifth of the response time of a 
running several sub optimal algorithm and choosing the optimal one is a common procedure in computer science most notably in the design of approximation algorithm this paper deal with one significant flaw of this technique in environment where the input are provided by rational agent such protocol are not necessarily incentive compatible even when the underlying algorithm are we characterize sufficient and necessary condition for such best outcome protocol to be incentive compatible in a general model for agent with one dimensional private data we show how our technique apply in several setting 
we consider the combinatorial optimization problem of finding the most influential node on a large scale social network for two widely used fundamental stochastic diffusion model it wa shown that a natural greedy strategy can give a good approximate solution to this optimization problem however a conventional method under the greedy algorithm need a large amount of computation since it estimate the marginal gain for the expected number of node influenced by a set of node by simulating the random process of each model many time in this paper we propose a method of efficiently estimating all those quantity on the basis of bond percolation and graph theory and apply it to approximately solving the optimization problem under the greedy algorithm using real world large scale network including blog network we experimentally demonstrate that the proposed method can outperform the conventional method and achieve a large reduction in computational cost 
traditional classification involves building a classifier using labeled training example from a set of predefined class and then applying the classifier to classify test instance into the same set of class in practice this paradigm can be problematic because the test data may contain instance that do not belong to any of the previously defined class detecting such unexpected instance in the test set is an important issue in practice the problem can be formulated a learning from positive and unlabeled example pu learning however current pu learning algorithm require a large proportion of negative instance in the unlabeled set to be effective this paper proposes a novel technique to solve this problem in the text classification domain the technique first generates a single artificial negative document an the set p and an are then used to build a na ve bayesian classifier our experiment result show that this method is significantly better than existing technique 
we use game theory to analyze meta learning algorithm the objective of meta learning is to determine which algorithm to apply on a given task this is an instance of a more general problem that consists of allocating knowledge consumer to learning producer solving this general problem in the field of meta learning yield solution for related field such a information retrieval and recommender system 
a robot become more involved in assisting u in large and hazardous operation such a search and rescue we can anticipate that diverse robot will come together with the need to coordinate their effort these robot will come from different organization creating a heterogeneous team varying in shape size and functionality how can diverse robot forming such an impromptu team collaborate to accomplish a joint objective if they are to organize and work together method must be developed that allow them to share knowledge in a meaningful way we propose an ontology based symbolic communication protocol to provide a shared understanding of physical concept between unit coordination is then accomplished through a negotiation of task to complete individual and joint goal 
how should a robot represent and reason about spatial information when it need to collaborate effectively with a human the form of spatial representation that is useful for robot navigation may not be useful in higher level reasoning or working with human a a team member to explore this question we have extended previous work on how child and robot learn to play hide and seek to a human robot team covertly approaching a moving target we used the cognitive modeling system act r with an added spatial module to support the robot s spatial reasoning the robot interacted with a team member through voice gesture and movement during the team s covert approach of a moving target this paper describes the new robotic system and it integration of metric symbolic and cognitive layer of spatial representation and reasoning for it individual and team behavior 
the importance of the effort towards integrating the symbolic and connectionist paradigm of artificial intelligence ha been widely recognised integration may lead to more effective and richer cognitive computational model and to a better understanding of the process of artificial intelligence across the field this paper present a new model for the representation computation and learning of temporal logic in connectionist system the model allows for the encoding of past and future temporal logic operator in neural network through a neural symbolic translation algorithm introduced in the paper the network are relatively simple and can be used for reasoning about time and for learning by example with the use of standard neural learning algorithm we validate the model in a well known application dealing with temporal synchronisation in distributed knowledge system this open several interesting research path in cognitive modelling with potential application in agent technology learning and reasoning 
to be successful recommender system must gain the trust of user to do this they must demonstrate their ability to make reliable prediction we argue that collaborative filtering recommendation algorithm can benefit from explicit model of trust to inform their prediction we present one such model of trust along with a cost benefit analysis that focus on the classical trade off that exists between recommendation coverage and prediction accuracy 
in recent year there ha been a great deal of interest in modular reinforcement learning mrl typically problem are decomposed into concurrent subgoals allowing increased scalability and state abstraction an arbitrator combine the subagents preference to select an action in this work we contrast treating an mrl agent a a set of subagents with the same goal with treating an mrl agent a a set of subagents who may have different possibly conflicting goal we argue that the latter is a more realistic description of real world problem especially when building partial program we address a range of algorithm for single goal mrl and leveraging social choice theory we present an impossibility result for application of such algorithm to multigoal mrl we suggest an alternative formulation of arbitration a scheduling that avoids the assumption of comparability of preference that are implicit in single goal mrl a notable feature of this formulation is the explicit codification of the tradeoff between the subproblems finally we introduce a bl a language that encapsulates many of these idea 
a long standing goal of ai is the development of intelligent workstation based personal agent to assist user in their daily life a key impediment to this goal is the unrealistic cost of developing and maintaining a detailed knowledge base describing the user s different activity and which people meeting email etc are affiliated with each such activity this paper present a clustering approach to automatically acquiring such a knowledge base by analyzing the raw content of the workstation including email contact person name and online calendar meeting our approach analyzes the distribution of email word the social network of email sender and recipient and the result of google desktop search queried with text from online calendar entry and person contact name for each cluster it construct the program output a frame based representation of the conesponding user activity this paper describes our approach and experimentally ass it perfonnance over the workstation of three different user 
understanding user interest from text document can provide support to personalized information recommendation service typically these service automatically infer the user profile a structured model of the user interest from document that were already deemed relevant by the user traditional keyword based approach are unable to capture the semantics of the user interest this work proposes the integration of linguistic knowledge in the process of learning semantic user profile that capture concept concerning user interest the proposed strategy consists of two step the first one is based on a word sense disambiguation technique that exploit the lexical database wordnet to select among all the possible meaning sens of a polysemous word the correct one in the second step a na ve bayes approach learns semantic sense based user profile a binary text classifier user like and user dislike from disambiguated document experiment have been conducted to compare the performance obtained by keyword based profile to that obtained by sense based profile both the classification accuracy and the effectiveness of the ranking imposed by the two different kind of profile on the document to be recommended have been considered the main outcome is that the classification accuracy is increased with no improvement on the ranking the conclusion is that the integration of linguistic knowledge in the learning process improves the classification of those document whose classification score is close to the like dislike threshold the item for which the classification is highly uncertain 
we describe an approach to extract attribute value pair from product description this allows u to represent product a set of such attribute value pair to augment product database such a representation is useful for a variety of task where treating a product a a set of attribute value pair is more useful than a an atomic entity example of such application include product recommendation product comparison and demand forecasting we formulate the extraction a a classification problem and use a semi supervised algorithm co em along with na ve bayes the extraction system requires very little initial user supervision using unlabeled data we automatically extract an initial seed list that serf a training data for the supervised and semi supervised classification algorithm finally the extracted attribute and value are linked to form pair using dependency information and co location score we present promising result on product description in two category of sporting good 
computer generated text whether from natural language generation nlg or machine translation mt system are often post edited by human before being released to user the frequency and type of post edits is a measure of how well the system work and can be used for evaluation we describe how we have used post edit data to evaluate sumtime mousam an nlg system that produce weather forecast 
we present a method for applying local search to overconstrained instance of the disjunctive temporal problem dtp our objective is to generate high quality solution i e solution that violate few constraint in a little time a possible the technique presented here differs markedly from previous work on dtps a it operates within the total assignment space of the underlying csp rather than the partial assignment space of the related meta csp we provide experimental result demonstrating that the use of local search lead to substantially improved performance over systematic method 
consistency based diagnosis concern using a model of the structure and behaviour of a system in order to analyse whether or not the system is malfunctioning a well known limitation of consistency based diagnosis is that it is unable to cope with uncertainty uncertainty reasoning is nowadays done using bayesian network in this field a conflict measure ha been introduced to detect conflict between a given probability distribution and associated data in this paper we use a probabilistic theory to represent logical diagnostic system and show that in this theory we are able to determine consistent and inconsistent state a traditionally done in consistency based diagnosis furthermore we analyse how the conflict measure in this theory offer a way to favour particular diagnosis above others this enables u to add uncertainty reasoning to consistency based diagnosis in a seamless fashion 
we propose an action formalism that is based on description logic dl and may be viewed a an instance of the situation calculus sitcalc in particular description logic concept can be used for describing the state of the world and the preand post condition of action the main advantage of such a combination is that on the one hand the expressive power for describing world state and condition is higher than in other decidable fragment of the sitcalc which are usually propositional on the other hand in contrast to the full sitcalc effective reasoning is still possible in this paper we perform a detailed investigation of how the choice of the dl influence the complexity of the standard reasoning task executability and projection in the corresponding action formalism we also discus semantic and computational problem in natural extension of our framework 
in this paper we will deal with some important kind of metric temporal reasoning problem that arise in many real life situation in particular event x x xn are modeled a time point and a constraint between the execution time of two event xi and xj is either simple temporal of the form xi xj a b or ha a connected feasible region that can be expressed using a finite set of domain rule each in turn of the form xi a b xj c d and conversely xj e f xi g h we argue that such rule are useful in capturing important kind of non monotonic relationship between the execution time of event when they are governed by potentially complex external factor our polynomial time deterministic and randomized algorithm for solving such problem therefore enable u to efficiently deal with very expressive representation of time 
due to the increasing demand of massive and distributed data analysis achieving highly accurate global data analysis result with local data privacy preserved becomes an increasingly important research issue in this paper we propose to adopt a model based method gaussian mixture model for local data abstraction and aggregate the local model parameter for learning global model to support global model learning based on solely local gmm parameter instead of virtual data generated from the aggregated local model a novel em like algorithm is derived experiment have been performed using synthetic datasets and the proposed method wa demonstrated to be able to achieve the global model accuracy comparable to that of using the data regeneration approach at a much lower computational cost 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
self stabilization in distributed system is the ability of a system to respond to transient failure by eventually reaching a legal state and maintaining it afterwards this make such system particularly interesting because they can tolerate fault and are able to cope with dynamic environment we propose the first self stabilizing mechanism for multiagent combinatorial optimization which work on general network and stabilizes in a state corresponding to the optimal solution of the optimization problem our algorithm is based on dynamic programming and requires a linear number of message to find the optimal solution in the absence of fault we show how our algorithm can be made super stabilizing in the sense that while transiting from one stable state to the next our system preserve the assignment from the previous optimal state until the new optimal solution is found we offer equal bound for the stabilization and the superstabilization time furthermore we describe a general scheme for fault containment and fast response time upon low impact failure multiple isolated failure are handled effectively to show the merit of our approach we report on experiment with practically sized distributed meeting scheduling problem in a multiagent system 
in this paper we make a comprehensive study of the complexity of the problem of deciding the existence of equilibrium in strategic game with incomplete information in case of pure strategy in particular we show that this is np complete in general bayesian game in standard normal form and that it becomes pp hard and in fixed precision scenario pp complete when the game is represented succinctly in general normal form suitable restriction in case of graphical game that make the problem tractable are also discussed 
model counting is the classical problem of computing the number of solution of a given propositional formula it vastly generalizes the np complete problem of propositional satisfiability and hence is both highly useful and extremely expensive to solve in practice we present a new approach to model counting that is based on adding a carefully chosen number of so called streamlining constraint to the input formula in order to cut down the size of it solution space in a controlled manner each of the additional constraint is a randomly chosen xor or parity constraint on the problem variable represented either directly or in the standard cnf form inspired by a related yet quite different theoretical study of the property of xor constraint we provide a formal proof that with high probability the number of xor constraint added in order to bring the formula to the boundary of being unsatisfiable determines with high precision it model count experimentally we demonstrate that this approach can be used to obtain good bound on the model count for formula that are far beyond the reach of exact counting method in fact we obtain the first non trivial solution count for very hard highly structured combinatorial problem instance note that unlike other counting technique such a markov chain monte carlo method we are able to provide high confidence guarantee on the quality of the count obtained 
the recurring appearance of sequential pattern when confined by the predefined gap requirement often implies strong temporal correlation or trend among pattern element in this paper we study the problem of mining a set of gap constrained sequential pattern across multiple sequence given a set of sequence s s sk constituting a single hypersequence s we aim to find recurring pattern in s say p which may cross multiple sequence with all their matching character in s bounded by the user specified gap constraint because of the combinatorial candidate explosion traditional apriori based algorithm are computationally infeasible our research proposes a new mechanism to ensure pattern growing and pruning when combining the pruning technique with our gap constrained search gc and map based support prediction approach our method achieves a speed about time faster than it other peer 
we propose an affine extension to add aadd capable of compactly representing context specific additive and multiplicative structure we show that the aadd ha worst case time and space performance within a multiplicative constant of that of add but that it can be linear in the number of variable in case where add are exponential in the number of variable we provide an empirical comparison of tabular add and aadd representation used in standard bayes net and mdp inference algorithm and conclude that the aadd performs at least a well a the other two representation and often yield an exponential performance improvement over both when additive or multiplicative structure can be exploited these result suggest that the aadd is likely to yield exponential time and space improvement for a variety of probabilistic inference algorithm that currently use table or add 
in many sensing application we must continuously gather information to provide a good estimate of the state of the environment at every point in time a robot may tour an environment gathering information every hour in a wireless sensor network these tour correspond to packet being transmitted in these setting we are often faced with resource restriction like energy constraint the user issue query with certain expectation on the answer quality thus we must optimize the tour to ensure the satisfaction of the user constraint while at the same time minimize the cost of the query plan for a single timestep this optimization problem is np hard but recent approximation algorithm with theoretical guarantee provide good solution in this paper we present a new efficient algorithm exploiting dynamic programming and submodularity of the information collected that efficiently plan data collection tour for an entire finite horizon our algorithm can use any single step procedure a a black box and based on it property provides strong theoretical guarantee for the solution we also provide an extensive empirical analysis demonstrating the benefit of nonmyopic planning in two real world sensing application 
in game research go is considered the classical board game that is most resistant to current ai technique large scale knowledge engineering ha been considered indispensable for building state of the art program even for subproblems such a life and death or tsume go this paper describes the technology behind tsumego explorer a high performance tsume go search engine for enclosed problem in empirical testing this engine outperforms gotools which ha been the undisputedly best tsume go program for year 
we consider recommender system that filter information and only show the most preferred item good recommendation can be provided only when an accurate model of the user s preference is available we propose a novel technique for filling in missing element of a user s preference model using the knowledge captured in an ontology furthermore we show through experiment on the movielens data set that our model achieves a high prediction accuracy and personalization level when little about the user s preference is known 
the multiple fault diagnosis problem is important since the single fault assumption can lead to incorrect or failed diagnosis when multiple fault occur it is challenging for continuous system because fault can mask or compensate each other s effect and the solution space grows exponentially with the number of possible fault we present a qualitative approach to multiple fault isolation in dynamic system based on analysis of fault transient behavior our approach us the observed measurement deviation and their temporal ordering to generate multiple fault hypothesis the approach ha polynomial space requirement and prune diagnosis resulting in an efficient online fault isolation scheme 
this paper investigates how to represent and solve multiagent task scheduling a a distributed constraint optimization problem dcop recently multiagent researcher have adopted the c t m language a a standard for multiagent task scheduling we contribute an automated mapping that transforms c t m into a dcop further we propose a set of representational compromise for c t m that allow existing distributed algorithm for dcop to be immediately brought to bear on c t m problem next we demonstrate a key advantage of a constraint based representation is the ability to leverage the representation to do efficient solving we contribute a set of pre processing algorithm that leverage existing constraint propagation technique to do variable domain pruning on the dcop we show that these algorithm can result in reduction in state space size for a given set of c t m problem finally we demonstrate up to a increase in the ability to optimally solve c t m problem in a reasonable amount of time and in a distributed manner a a result of applying our mapping and domain pruning algorithm 
this paper employ state similarity to improve reinforcement learning performance this is achieved by first identifying state with similar sub policy then a tree is constructed to be used for locating common action sequence of state a derived from possible optimal policy such sequence are utilized for defining a similarity function between state which is essential for reflecting update on the action value function of a state onto all similar state a a result the experience acquired during learning can be applied to a broader context effectiveness of the method is demonstrated empirically 
this work present a new algorithm called heuristically accelerated minimax q hammq that allows the use of heuristic to speed up the well known multiagent reinforcement learning algorithm minimax q a heuristic function h that influence the choice of the action characterises the hammq algorithm this function is associated with a preference policy that indicates that a certain action must be taken instead of another a set of empirical evaluation were conducted for the proposed algorithm in a simplified simulator for the robot soccer domain and experimental result show that even very simple heuristic enhances significantly the performance of the multiagent reinforcement learning algorithm 
this paper present dd lite an efficient incremental search algorithm for problem that can capitalize on state dominance dominance relationship between node are used to prune graph in search algorithm thus exploiting state dominance relationship can considerably speed up search problem in large state space such a mobile robot path planning considering uncertainty time or energy constraint incremental search technique are useful when change can occur in the search graph such a when re planning path for mobile robot in partially known environment while algorithm such a d and d lite are very efficient incremental search algorithm they cannot be applied a formulated to search problem in which state dominance is used to prune the graph dd lite extends d lite to seamlessly support reasoning about state dominance it maintains the algorithmic simplicity and incremental search capability of d lite while resulting in order of magnitude increase in search efficiency in large state space with dominance we illustrate the efficiency of dd lite with simulation result from applying the algorithm to a path planning problem with time and energy constraint we also prove that dd lite is sound complete optimal and efficient 
agent pro attitude such a goal intention desire wish and judgement of satisfactoriness play an important role in how agent act rationally to provide a natural and satisfying formalization of these attitude is a longstanding problem in the community of agent theory most of existing modal logic approach are based on kripke structure and have to face the so called side effect problem this paper present a new modal logic formalizing agent pro attitude based on neighborhood model there are three distinguishing feature of this logic firstly this logic naturally satisfies bratman s requirement for agent belief and pro attitude a well a some interesting property that have not been discussed before secondly we give a sound and complete axiom system for characterizing all the valid property of belief and pro attitude we introduce for the first time the notion of linear neighborhood frame for obtaining the semantic model and this brings a new member to the family of non normal modal logic finally we argue that the present logic satisfies an important requirement proposed from the viewpoint of computation that is computational grounding which mean that property in this logic can be given an interpretation in term of some concrete computational model indeed the presented neighborhood frame can be naturally derived from probabilistic programming with utility 
the qcsp language we introduce extends the framework of quantified constraint satisfaction problem qcsps by enabling u to neatly express restricted quantification via a chain of nested csps to be interpreted a alternately conjuncted and disjuncted restricted quantifier turn out to be a convenient solution to the crippling modeling issue we encounter in qcsp and surprisingly they help to reuse propagation technology and to prune the search space our qcsp solver which also handle arithmetic and global constraint exhibit state of the art performance 
parthood componenthood and containment relation are commonly assumed in biomedical ontology and terminology system but are not usually clearly distinguished from another this paper contributes towards a unified theory of parthood componenthood and containment relation our goal in this is to clarify distinction between these relation a well a principle governing their interrelation we first develop a theory of these relation in first order predicate logic and then discus how description logic can be used to capture some important aspect of the first order theory 
a key problem in reinforcement learning is finding a good balance between the need to explore the environment and the need to gain reward by exploiting existing knowledge much research ha been devoted to this topic and many of the proposed method are aimed simply at ensuring that enough sample are gathered to estimate well the value function in contrast bellman and kalaba proposed constructing a representation in which the state of the original system are paired with knowledge about the current model hence knowledge about the possible markov model of the environment is represented and maintained explicitly unfortunately this approach is intractable except for bandit problem where it give rise to gittins index an optimal exploration method in this paper we explore idea for making this method computationally tractable we maintain a model of the environment a a markov decision process we sample finite length trajectory from the infinite tree using idea based on sparse sampling finding the value of the node of this sparse subtree can then be expressed a an optimization problem which we solve using linear programming we illustrate this approach on a few domain and compare it with other exploration algorithm 
the task of model based diagnosis is np complete but it is not known whether it is computationally difficult for the average real world system there ha been no systematic study of the complexity of diagnosing real world problem and few good benchmark exist to test this real world graph a mathematical framework that ha been proposed a a model for complex system have empirically been shown to capture several topological property of real world system we describe the adequacy with which a real world graph can characterise the complexity of model based diagnostic inference on real world system we empirically compare the inference complexity of diagnosing model automatically generated using the real world graph framework with comparable model from well known iscas circuit benchmark we identify parameter necessary for the real world graph framework to generate benchmark diagnosis circuit model with realistic property 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
localization is a fundamental challenge for autonomous robotics although accurate and efficient technique now exist for solving this problem they require explicit probabilistic model of the robot s motion and sensor these model are usually obtained from time consuming and error prone measurement or tedious manual tuning in this paper we examine automatic calibration of sensor and motion model from a bayesian perspective we introduce an efficient mcmc procedure for sampling from the posterior distribution of the model parameter we also present a novel extension of particle filter to make use of our posterior parameter sample finally we demonstrate our approach both in simulation and on a physical robot our result demonstrate effective inference of model parameter a well a a paradoxical result that using posterior parameter sample can produce more accurate position estimate than the true parameter 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
to test large scale socially embedded system this paper proposes a multiagent based participatory design that consists of two step participatory simulation where scenario guided agent and human controlled avatar coexist in a shared virtual space and jointly perform simulation and the extension of the participatory simulation into the augmented experiment where an experiment is performed in real space by human subject enhanced by a large scale multiagent simulation the augmented experiment proposed in this paper consist of various sensor to collect the real world activity of human subject and project them into the virtual space multiagent simulation to simulate human activity in the virtual space and communication channel to inform simulation status to human subject in the real space to create agent and interaction model incrementally from the participatory design process we propose the participatory design loop that us deductive machine learning technology indoor and outdoor augmented experiment have been actually conducted in the city of kyoto both experiment were intended to test new disaster evacuation system based on mobile phone 
a general framework for minimisation based belief change is presented a problem instance is made up of an undirected graph where a formula is associated with each vertex for example vertex may represent spatial location point in time or some other notion of locality information is shared between vertex via a process of minimisation over the graph we give equivalent semantic and syntactic characterisation of this minimisation we also show that this approach is general enough to capture existing minimisation based approach to belief merging belief revision and temporal extrapolation operator while we focus on a set theoretic notion of minimisation we also consider other approach such a cardinality based and priority based minimisation 
this goal of this paper is to defend the plausibility of the argument that passing the turing test is a sufficient condition for the presence of intelligence to this effect we put forth new objection to two famous counter argument searle s chinese room and block s aunt bertha we take searle s argument to consist of two point intelligence is not merely an ability to manipulate formal symbol it is also the ability of relating those symbol to a multi sensory real world experience and intelligence presupposes an internal capacity for generalization on the first point while we concede that multi sensory real world experience is not captured by the test we show that intuition about the relevance of this experience to intelligence are not clear cut therefore it is not obvious that the turing test should be dismissed on this basis alone on the second point we strongly disagree with the notion that the test cannot distinguish a machine with internal capacity for generalization from a machine which ha no such capacity this view is best captured by ned block who argues that a sufficiently large look up table is capable of passing any turing test of finite length we claim that contrary to block s assumption it is impossible to construct such a table and show that it is possible to ensure that a machine relying solely on such table will fail an appropriately constructed turing test 
many common web task can be automated by algorithm that are able to identify web object relevant to the user s need this paper present a novel approach to web object identificalion that find relationship between the user s action and linguistic information associated with web object from a single training example involving demonstration and a natural language description we create a parameterized object description the approach performs a well a a popular web wrapper on a routine task but it ha the additional capability of performing in dynamic environment and the attractive property of being reusable in other domain without additional training 
the presence of long gap dramatically increase the difficulty of detecting and characterizing complex event hidden in long sequence in order to cope with this problem a learning algorithm based on an abstraction mechanism is proposed it can infer a hierarchical hidden markov model from a learning set of sequence the induction algorithm proceeds bottom up progressively coarsening the sequence granularity and letting correlation between subsequence separated by long gap naturally emerge a a case study the method is evaluated on an application of user profiling the result show that the proposed algorithm is suitable for developing real application in network security and monitoring 
the ability to learn from data and to improve it performance through incremental learning make self adaptive neural network sanns a powerful tool to support knowledge discovery however the development of sanns ha traditionally focused on data domain that are assumed to be modeled by a gaussian distribution the analysis of data governed by other statistical model such a the poisson distribution ha received le attention from the data mining community based on special consideration of the statistical nature of data following a poisson distribution this paper introduces a sann poisson based self organizing tree algorithm psota which implement novel similarity matching criterion and neuron weight adaptation scheme it wa tested on synthetic and real world data serial analysis of gene expression data psota based data analysis supported the automated identification of more meaningful cluster by visualizing the dendrograms generated by psota complex interand intra cluster relationship encoded in the data were also highlighted and readily understood this study indicate that in comparison to the traditional self organizing tree algorithm sota psota offer significant improvement in pattern discovery and visualization in data modeled by the poisson distribution such a serial analysis of gene expression data 
expressive description logic dl have been advocated a formalism for modeling the domain of interest in various application area an important requirement is the ability to answer complex query beyond instance retrieval taking into account constraint expressed in a knowledge base we consider this task for positive existential path query which generalize conjunctive query and union thereof whose atom are regular expression over the role and concept of a knowledge base in the expressive dl alcqibreg using technique based on two way tree automaton we first provide an elegant characterization of tbox and abox reasoning which give u also a tight exptime bound we then prove decidability more precisely a exptime upper bound of query answering thus significantly pushing the decidability frontier both with respect to the query language and the considered dl we also show that query answering is exp space hard already in rather restricted setting 
action description language c is more expressive than adl in many way for instance it address the ramification problem on the other hand adl is based on first order logic while c is only propositional expression with variable which are frequently used when action domain are described in c are merely schema describing finite set of causal law that are formed according to the same pattern in this paper we propose a new approach to the semantics of action description with variable that combine attractive feature of adl and c 
finding model of logical formula is a challenging problem for first order formula a finite model can be found by exhaustive search for many structured problem instance there is much isomorphism in the search space this paper proposes general purpose technique for eliminating isomorphic subspace which can be helpful when the formula have many predicate the technique are based on inherent symmetry in first order clause 
cryptographic protocol are structured sequence of message that are used for exchanging information in a hostile environment many protocol have epistemic goal a successful run of the protocol is intended to cause a participant to hold certain belief a such epistemic logic have been employed for the verification of cryptographic protocol although this approach to verification is explicitly concerned with changing belief formal belief change operator have not been incorporated in previous work in this paper we introduce a new approach to protocol verification by combining a monotonic logic with a non monotonic belief change operator in this context a protocol participant is able to retract belief in response to new information and a protocol participant is able to postulate the most plausible event explaining new information we illustrate that this kind of reasoning is particularly important when protocol participant have incorrect belief protocol the paper make two main contribution to existing research first we extend the application of formal belief change technique to a new class of problem cryptographic protocol verification provides a large class of belief change problem which are not only of theoretical interest but also have great practical significance the second contribution is the introduction of a specific model of belief change that is particularly suitable for the the verification of cryptographic protocol broadly speaking researcher in belief change and researcher in protocol verification are both interested in the same kind of problem our aim is to make this salient and to illustrate how work in each area can benefit the other we proceed a follows first we introduce some preliminary background on the logical approach to cryptographic protocol verification next we argue that ban like logic can not capture the kind of non monotonic belief change that occurs in an authentication protocol and we introduce some more appropriate belief change operator finally we present a simple approach to protocol verification in term of formal belief change operator we conclude with a general discussion about belief change in the context of protocol verification 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
here we define a framework where defeasible argumentation is used for reasoning about belief desire and intention a dialectical filtering process is introduced to obtain a subset of the agent s desire containing only those that are achievable in the current situation different agent type can be defined in the framework affecting the way in which current desire are obtained the agent is provided with a set of intention rule that specifies under what condition an intention could be achieved when more than one intention is present a policy will be used to choose among them thus intention policy provide the agent with a mechanism for deciding which intention is selected in the current situation several application example will be given 
scriptease is a tool that allows author with no programming experience to create interactive story for computer role playing game instead of writing scripting code manually game author select design pattern that encapsulate frequent game scenario creating story at a higher level of abstraction and being shielded from the underlying scripting language scriptease ha been extended to support behavior pattern that generate ambient behavior for non player character this demonstration show how scriptease creates intricate non player character script to generate compelling and engaging character behavior we demonstrate our scriptease motivational ambient and pc interactive behavior for a guard character using bioware corp s neverwinter night game 
record linkage is the process of matching record across data set that refer to the same entity one issue within record linkage is determining which record pair to consider since a detailed comparison between all of the record is impractical blocking address this issue by generating candidate match a a preprocessing step for record linkage for example in a person matching problem blocking might return all people with the same last name a candidate match two main problem in blocking are the selection of attribute for generating the candidate match and deciding which method to use to compare the selected attribute these attribute and method choice constitute a blocking scheme previous approach to record linkage address the blocking issue in a largely ad hoc fashion this paper present a machine learning approach to automatically learn effective blocking scheme we validate our approach with experiment that show our learned blocking scheme outperform the ad hoc blocking scheme of non expert and perform comparably to those manually built by a domain expert 
previous work in social network analysis sna ha modeled the existence of link from one entity to another but not the language content or topic on those link we present the author recipient topic art model for social network analysis which learns topic distribution based on the direction sensitive message sent between entity the model build on latent dirichlet allocation lda and the author topic at model adding the key attribute that distribution over topic is conditioned distinctly on both the sender and recipient steering the discovery of topic according to the relationship between people we give result on both the enron email corpus and a researcher s email archive providing evidence not only that clearly relevant topic are discovered but that the art model better predicts people s role 
an important application area of detecting erroneous sentence is to provide feedback for writer of english a a second language this problem is difficult since both erroneous and correct sentence are diversified in this paper we propose a novel approach to identifying erroneous sentence we first mine labeled tree pattern and sequential pattern to characterize both erroneous and correct sentence then the discovered pattern are utilized in two way to distinguish correct sentence from erroneous sentence the pattern are transformed into sentence feature for existing classification model e g svm the pattern are used to build a rule based classification model experimental result show that both technique are promising while the second technique outperforms the first approach moreover the classification model in the second proposal is easy to understand and we can provide intuitive explanation for classification result 
in recent year there ha been a prevalence of search engine being employed to find useful information in the web a they efficiently explore hyperlink between web page which define a natural graph structure that yield a good ranking unfortunately current search engine cannot effectively rank those relational data which exists on dynamic website supported by online database in this study to rank such structured data i e find the best item we propose an integrated online system consisting of compressed data structure to encode the dominant relationship of the relational data efficient querying strategy and updating scheme are devised to facilitate the ranking process extensive experiment illustrate the effectiveness and efficiency of our method a such we believe the work in this poster can be complementary to traditional search engine 
using smart phone for ad hoc mathematical collaboration pose multiple user interface challenge in this paper software agent are used to lessen the cognitive load through automatic line labeling researcher in the human computer interaction community have attempted to alleviate the problem of general usability through user interface engineering convention myers however these engineering approach can be improved upon through the application of mixed initiative principle 
we present a generalized gelfond lifschitz transformation in order to define stable model for a logic program with arbitrary abstract constraint on set c atom the generalization is based on a formal semantics and a novel abstract representation of c atom a opposed to the commonly used power set form representation in many case the abstract representation of a c atom result in a substantial reduction of size from it power set form representation we show that any c atom a ad ac in the body of a clause can be characterized using it satisfiable set so that given an interpretation i the c atom can be handled simply by introducing a special atom a together with a new clause a a an for each satisfiable set a an of a we also prove that the latest fixpoint approach presented by son et al and our approach using the generalized gelfond lifschitz transformation are semantically equivalent in the sense that they define the same set of stable model 
in this paper we improve previous work on measuring the similarity of short segment of text in two way first we introduce a web relevance similarity measure and demonstrate it effectiveness this measure extends the web kernel similarity function introduced by sahami and heilman by using relevance weighted inner product of term occurrence rather than tf idf second we show that one can further improve the accuracy of similarity measure by using a machine learning approach our method outperform other state of the art method in a general query suggestion task for multiple evaluation metric 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
integrating description logic dl and logic programming lp would produce a very powerful and useful formalism however dl and lp are based on quite different principle so achieving a seamless integration is not trivial in this paper we introduce hybrid mknf knowledge base that faithfully integrate dl with lp using the logic of minimal knowledge and negation a failure mknf lifschitz we also give reasoning algorithm and tight data complexity bound for several interesting fragment of our logic 
we study how to find plan that maximize the expected total utility for a given mdp a planning objective that is important for decision making in high stake domain the optimal action can now depend on the total reward that ha been accumulated so far in addition to the current state we extend our previous work on functional value iteration from one switch utility function to all utility function that can be approximated with piecewise linear utility function with and without exponential tail by using functional value iteration to find a plan that maximizes the expected total utility for the approximate utility function functional value iteration doe not maintain a value for every state but a value function that map the total reward that ha been accumulated so far into a value we describe how functional value iteration represents these value function in finite form how it performs dynamic programming by manipulating these representation and what kind of approximation guarantee it is able to make we also apply it to a probabilistic blocksworld problem a standard test domain for decision theoretic planner 
recent development of location technology enables u to obtain the location history of user this paper proposes a new method to infer user longterm property from their respective location history counting the instance of sensor detection for every user we can obtain a sensor user matrix after generating feature from the matrix a machine learning approach is taken to automatically classify user into different category for each user property inspired by information retrieval research the problem to infer user property is reduced to a text categorization problem we compare weighting of several feature and also propose sensor weighting our algorithm are evaluated using experimental location data in an office environment 
geometrical symmetry are commonly exploited to improve the efficiency of search algorithm we introduce a new logical symmetry in permutation state space which we call duality we show that each state ha a dual state both state share important attribute and these property can be used to improve search efficiency we also present a new search algorithm dual search which switch between the original state and the dual state when it seems likely that the switch will improve the chance of a cutoff the decision of when to switch is very important and several policy for doing this are investigated experimental result show significant improvement for a number of application 
question classification i e putting the question into several semantic category is very important for question answering this paper introduces a new application of using subtree mining for question classification problem first we formulate this problem a classifying a tree to a certain label among a set of label we then present a use of subtrees in the forest created by the training data to the tree classification problem in which maximum entropy and a boosting model are used a classifier experiment on standard question classification data show that the us of subtrees along with either maximum entropy or boosting model are promising the result indicate that our method achieves a comparable or even better performance than kernel method and also improves testing efficiency 
this paper describes a methodology to semiautomatically acquire a taxonomy of term and term definition in a specific research domain the taxonomy is then used for semantic search and indexing of a knowledge base of scientific competence called knowledge map the kmap is a system to support research collaboration and sharing of result within and beyond a european network of excellence the methodology is general and can be applied to model any web community starting from the document shared and exchanged among the community member and to use this model for improving accessibility of data and knowledge repository 
manager of operating room or and of unit upstream e g ambulatory surgery and downstream e g intensive care and post anesthesia care of the or require real time information about or occupancy which or are in use and when will each ongoing operation end this information is used to make decision about how to assign staff when to prepare patient for the or when to schedule add on case when to move case and how to prioritize room cleanup dexter et at it is typically gathered by or manager manually by walking to each or and estimating the time to case completion this paper present a system for determining the state of an ongoing operation automatically from video support vector machine are trained to identify relevant image feature and hidden markov model are trained to use these feature to compute a sequence of or state from the video the system wa tested on video captured over a hour period in one of the operating room in baltimore s r adam crowley shock trauma center it wa found to be more accurate and have le delay while providing more fine grained state information than the current state of the art system based on patient vital sign used by the shock trauma center 
we employed a multilevel hierarchical bayesian model in the task of exploiting relevant interaction among high cardinality attribute in a classification problem without overfitting with this model we calculate posterior class probability for a pattern w combining the observation of w in the training set with prior class probability that are obtained recursively from the observation of pattern that are strictly more generic than w the model achieved performance improvement over standard bayesian network method like naive bayes and tree augmented naive bayes over bayesian network where traditional conditional probability table were substituted bynoisy or gate default table decision tree and decision graph and over bayesian network constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method 
it is useful in a wide range of situation to find solution which are diverse or similar to each other we therefore define a number of different class of diversity and similarity problem for example what is the most diverse set of solution of a constraint satisfaction problem with a given cardinality we first determine the computational complexity of these problem we then propose a number of practical solution method some of which use global constraint for enforcing diversity or similarity between solution empirical evaluation on a number of problem show promising result 
many method including supervised and unsupervised algorithm have been developed for extractive document summarization most supervised method consider the summarization task a a two class classification problem and classify each sentence individually without leveraging the relationship among sentence the unsupervised method use heuristic rule to select the most informative sentence into a summary directly which are hard to generalize in this paper we present a conditional random field crf based framework to keep the merit of the above two kind of approach while avoiding their disadvantage what is more the proposed framework can take the outcome of previous method a feature and seamlessly integrate them the key idea of our approach is to treat the summarization task a a sequence labeling problem in this view each document is a sequence of sentence and the summarization procedure label the sentence by and the label of a sentence depends on the assignment of label of others we compared our proposed approach with eight existing method on an open benchmark data set the result show that our approach can improve the performance by more than and over the best supervised baseline and unsupervised baseline respectively in term of two popular metric f and rouge detailed analysis of the improvement is presented a well 
an active learning system will sequentially decide which unlabeled instance to label with the goal of efficiently gathering the information necessary to produce a good classifier some such system greedily select the next instance based only on property of that instance and the few currently labeled point e g selecting the one closest to the current classification boundary unfortunately these approach ignore the valuable information contained in the other unlabeled instance which can help identify a good classifier much faster for the previous approach that do exploit this unlabeled data this information is mostly used in a conservative way one common property of the approach in the literature is that the active learner stick to one single query selection criterion in the whole process we propose a system mm m that selects the query instance that is able to provide the maximum conditional mutual information about the label of the unlabeled instance given the labeled data in an optimistic way this approach implicitly exploit the discriminative partition information contained in the unlabeled data instead of using one selection criterion mm m also employ a simple on line method that change it selection rule when it encounter an unexpected label our empirical result demonstrate that this new approach work effectively 
we study the behavior of the classical a search algorithm when coupled with a heuristic that provides estimate accurate to within a small multiplicative factor of the distance to a solution we prove general upper bound on the complexity of a search for both admissible and unconstrained heuristic function that depend only on the distribution of solution objective value we go on to provide nearly matching lower bound that are attained even by non adversarially chosen solution set induced by a simple stochastic model 
the ability to automatically detect activity in video is of increasing importance in application such a bank security airport tarmac security baggage area security and building site surveillance we present a stochastic activity model composed of atomic action which are directly observable through image understanding primitive we focus on answering two type of question i what are the minimal sub video in which a given action is identified with probability above a certain threshold and ii for a given video can we decide which activity from a given set most likely occurred we provide the mp algorithm for the first problem a well a two different algorithm naive mpa and mpa to solve the second our experimental result on a dataset consisting of staged bank robbery video described in vu et al show that our algorithm are both fast and provide high quality result when compared to human reviewer 
we introduce a general framework for specifying program correspondence under the answer set semantics the framework allows to define different kind of equivalence notion including previously defined notion like strong and uniform equivalence in which program are extended with rule from a given context and correspondence is determined by mean of a binary relation in particular refined equivalence notion based on projected answer set can be defined within this framework where not all part of an answer set are of relevance we study general characterization of inclusion and equivalence problem introducing novel semantical structure furthermore we deal with the issue of determining counterexample for a given correspondence problem and we analyze the computational complexity of correspondence checking 
a new knowledge based security protocol verification approach is proposed in this paper a number of predicate function assumption and rule are used to infer the knowledge of participating principal these item are implemented with isabelle which enables mechanical proving this approach can prove protocol concerning interleaving protocol session and can prove the correctness of a mediumsized security protocol in a couple of second the mechanical proof of a number of important secure property and then of the correctness of the needham schroeder lowe protocol are given a example to show the effectiveness of this method 
thomsen s search and nagai s depth first proof number dfpn search are two powerful but very different and or tree search algorithm lambda depth first proof number search ldfpn is a novel algorithm that combine idea from both algorithm search can dramatically reduce a search space by finding different level of threat sequence dfpn employ the notion of proof and disproof number to expand node expected to be easiest to prove or disprove the method wa shown to be effective for many game integrating order with proof and disproof number enables ldfpn to select move more effectively while preserving the efficiency of dfpn ldfpn ha been implemented for capturing problem in go and is shown to be more efficient than dfpn and more robust than an algorithm based on classical search 
we present a novel approach to describe and reason about stateless information processing service it can be seen a an extension of standard description which make explicit the relationship between input and output and take into account owl ontology to fix the meaning of the term used in a service description this allows u to define a notion of matching between service which yield high precision and recall for service location we explain why matching is decidable and provide biomedical example service to illustrate the utility of our approach 
we define a generalized strategy eliminability criterion for bimatrix game that considers whether a given strategy is eliminable relative to given dominator eliminee subset of the player strategy we show that this definition span a spectrum 
we introduce a model for predicting the performance of ida using pattern database heuristic a a function of the branching factor of the problem the solution depth and the size of the pattern database while it is known that the larger the pattern database the more efficient the search we provide a quantitative analysis of this relationship in particular we show that for a single goal state the number of node expanded by ida is a fraction of logb s s of the node expanded by a brute force search where b is the branching factor and s is the size of the pattern database we also show that by taking the maximum of at least two pattern database the number of node expansion decrease linearly with s compared to a brute force search we compare our theoretical prediction with empirical performance data on rubik s cube our model is conservative and overestimate the actual number of node expansion 
in this paper we consider a mobile mailbox communication scheme to reduce inter agent communication cost we employ a mailbox mobility strategy based on the ability of the mailbox to predict variation in inbound message rate and to migrate if necessary to a potentially better position in the network 
collaborative filtering system are essentially social system which base their recommendation on the judgment of a large number of people however like other social system they are also vulnerable to manipulation lie and propaganda may be spread by malicious user who may have an interest in promoting an item or downplaying the popularity of another one by doing this systematically with either multiple identity or by involving more people malicious shilling user profile can be injected into a collaborative recommender system which can significantly affect the robustness of a recommender system while current detection algorithm are able to use certain characteristic of shilling profile to detect them they suffer from low precision and require a large amount of training data the aim of this work is to explore simpler unsupervised alternative which exploit the nature of shilling profile and can be easily plugged into collaborative filtering framework to add robustness two statistical method are developed and experimentally shown to provide high accuracy in shilling attack detection 
tracking in essence consists of using sensory information combined with a motion model to estimate the position of a moving object tracking efficiency completely depends on the accuracy of the motion model and of the sensory information for a vision sensor like a camera the estimation is translated into a command to guide the camera where to look in this paper we contribute a method to achieve efficient tracking through using a tactic based motion model combined vision and infrared sensory information we use a supervised learning technique to map the state being tracked to the command that lead the camera to consistently track the object we present the probabilistic algorithm in detail and present empirical result both in simulation experiment and from their effective execution in a segway rmp robot 
in this paper a multi agent system ma is presented for providing clinical decision support to healthcare practitioner in rural or remote area of india for young infant or child up to the age of year the government is unable to appoint child specialist in rural area because of inadequate number of available pediatrician it lead to a high infant mortality rate imr in such a scenario software agent provide a realistic solution the agent based prototype ha been developed that involves a knowledge component called an intelligent pediatric assistant ipa and user agent ua along with their graphical user interface gui the gui of ua provides the interface to the healthcare practitioner for submitting sign symptom and displaying the expert opinion a suggested by ipa depending upon the observation the ipa decides the diagnosis and the treatment plan the ua and ipa form client server architecture for knowledge sharing 
tractable case of the binary csp are mainly divided in two class constraint language restriction and constraint graph restriction to better understand and identify the hardest binary csps in this work we propose method to increase their hardness by increasing the balance of both the constraint language and the constraint graph the balance of a constraint is increased by maximizing the number of domain element with the same number of occurrence the balance of the graph is defined using the classical definition from graph theory in this sense we present two graph model a first graph model that increase the balance of a graph maximizing the number of vertex with the same degree and a second one that additionally increase the girth of the graph because a high girth implies a high treewidth an important parameter for binary csps hardness our result show that our more balanced graph model and constraint result in harder instance when compared to typicai random binary csp instance by several order of magnitude also we detect at least for sparse constraint graph a higher treewidth for our graph model 
this paper deal with iterated revision of partially ordered information the first part of this paper concern the katsuno mendelzon s postulate we first point out that these postulate are not fully satisfactory since only a class of partially ordered information can be revised we then propose a suitable definition of faithful assignment followed by a new set of postulate and a representation theorem the second part of this paper investigates additional postulate dedicated to iterated revision operator of partially ordered information three extension of well known iterated belief revision operation for dealing with partially ordered information are briefly presented 
tpboscourier is the transportation procurement and bid optimization system tpbos for philip electronics to automate and optimize it procurement of courier service it wa jointly developed by red jasper limited and the hong kong university of science and technology and ha been successfully deployed in philip typically procures courier service for more than shipping lane annually and the use of the software ha resulted in significant cost and time saving in analyzing and optimizing procurement decision this paper explains the development and design of the tpboscourier 
assessing recovery from stroke ha been so far a time consuming procedure in which highly trained clinician are required this paper proposes a mechatronic platform which measure low force and torque exerted by subject class posterior probability are used a a quantitative and statistically sound tool to ass motor recovery from these force and torque measurement the performance of the patient is expressed in term of the posterior probability to belong to the class of normal subject the mechatronic platform together with the class posterior probability enables to automate motor recovery assessment without the need for highly trained clinician it is shown that the class posterior probability profile are highly correlated r with the well established fugl meyer scale assessment in motor recovery these result have been obtained through careful feature subset selection procedure in order to prune the large feature set being generated the overall approach is general and can be applied to many other health monitoring system where different category diseased v healthy can be identified 
the quantified constraint satisfaction problem qcsp is a generalization of the csp in which some variable are universally quantified it ha been shown that a solver based on an encoding of qcsp into qbf can outperform the existing direct qcsp approach by several order of magnitude in this paper we introduce an efficient qcsp solver we show how knowledge learned from the successful encoding of qcsp into qbf can be utilized to enhance the existing qcsp technique and speed up search by order of magnitude we also show how the performance of the solver can be further enhanced by incorporating advanced look back technique such a cbj and solution directed pruning experiment demonstrate that our solver is several order of magnitude faster than existing direct approach to qcsp solving and significantly outperforms approach based on encoding qcsps a qbfs 
rdf ontology are rapidly increasing in number we study the problem of integrating two rdf ontology under a given set h of horn clause that specify semantic relationship between term in the ontology a well a under a given set of negative constraint we formally define the notion of a witness to the integrability of two rdf ontology under such constraint a witness represents a way of integrating the ontology together we define a minimal witness and provide the polynomial crow computing rdf ontology witness algorithm to find a witness we report on the performance of crow both on daml schemaweb and onto broker ontology a well a on synthetically generated data the experiment show that crow work very well on real life ontology and scale to massive ontology 
in this paper we consider the generation of feature for automatic speech recognition asr that are robust to speaker variation one of the major cause for the degradation in the performance of asr system is due to inter speaker variation these variation are commonly modeled by a pure scaling relation between spectrum of speaker enunciating the same sound therefore current state of the art asr system overcome this problem of speaker variability by doing a brute force search for the optimal scaling parameter this procedure known a vocal tract length normalization vtln is computationally intensive we have recently used scale transform a variation of mellin transform to generate feature which are robust to speaker variation without the need to search for the scaling parameter however these feature have poorer performance due to loss of phase information in this paper we propose to use the magnitude of scale transform and a pre computed phase vector for each phoneme to generate speaker invariant feature we compare the performance of the proposed feature with conventional vtln on a phoneme recognition task 
unmotivated student do not reap the full reward of using a computer based intelligent tutoring system detection of improper behavior is thus an important component of an online student model to meet this challenge we present a dynamic mixture model based on item response theory this model which simultaneously estimate a student s proficiency and changing motivation level wa tested with data of high school student using a geometry tutoring system by accounting for student motivation the dynamic mixture model can more accurately estimate proficiency and the probability of a correct response the model s generality is an added benefit making it applicable to many intelligent tutoring system a well a other domain 
knowledge maintenance for case based reasoning system is an important knowledge engineering task despite the availability of initial case knowledge and new case to extend it for classification system it is essential that different scenario for the various class are well represented and decision boundary are well defined in the case knowledge a complexity based competence metric is proposed that identifies redundant and error causing case to be deleted the metric informs a maintenance tool that enables the engineer to experiment and balance conflicting objective complexity informed maintenance outperforms benchmark algorithm for redundancy and error reduction task 
we present a novel method for indoor location estimation using a vector space model based on signal received from a wireless client our aim is to obtain an accurate mapping between the signal space and the physical space without incurring too much human calibration effort this problem ha traditionally been tackled through probabilistic model trained on manually labeled data which are expensive to obtain in this paper we present a novel approach to building a mapping between the signalvector space and the physical location space using kernel canonical correlation analysis kcca it training requires much le human labor moreover unlike traditional location estimation system that treat grid point a independent and discrete target class during training we use the physical location a a continuous feedback to build a similarity mapping using kcca we test our algorithm in a wireless lan environment and demonstrate the advantage of our method in both accuracy and it ability to utilize a much smaller set of labeled training data than previous method 
the idea of only knowing a collection of sentence ha been previously shown to have a close connection with autoepistemic logic here we propose a more general account of only knowing that capture not only autoepistemic logic but default logic a well this allows u not only to study the property of default logic in term of an underlying model of belief but also the relationship among different form of nonmonotonic reasoning all within a classical monotonic logic characterized semantically in term of possible world 
different methodology have been employed to solve the multi sensor multi target detection problem in a variety of scenario in this paper we devise a time step optimal algorithm for this problem when all but a few parameter of the sensor target system are unknown using the concept of covering graph we find an optimum solution for a single sensor which is extended to multiple sensor by a tagging operation both covering graph and tagging are novel concept developed in the context of the detection problem for the first time and bring a mathematical elegance to it solution furthermore an implementation of the resulting algorithm is found to perform better than other notable approach the strong theoretical foundation combined with the practical efficacy of the algorithm make it a very attractive solution to the problem 
intrusion attempt due to self propagating code are becoming an increasingly urgent problem in part due to the homogeneous makeup of the internet recent advance in anomaly based intrusion detection system id have made use of the quickly spreading nature of these attack to identify them with high sensitivity and at low false positive fp rate however slowly propagating attack are much more difficult to detect because they are cloaked under the veil of normal network traffic yet can be just a dangerous due to their exponential spread pattern we extend the idea of using collaborative id to corroborate the likelihood of attack by imbuing end host with probabilistic graphical model and using random messaging to gossip state among peer detector we show that such a system is able to boost a weak anomaly detector d to detect an order of magnitude slower worm at false positive rate le than a few per week than would be possible using d alone at the end host or on a network aggregation point we show that this general architecture is scalable in the sense that a fixed absolute false positive rate can be achieved a the network size grows spread communication bandwidth uniformly throughout the network and make use of the increased computation power of a distributed system we argue that using probabilistic model provides more robust detection than previous collaborative counting scheme and allows the system to account for heterogeneous detector in a principled fashion 
some statistical software testing approach rely on sampling the feasible path in the control flow graph of the program the difficulty come from the tiny ratio of feasible path this paper present an adaptive sampling mechanismcalled exist for exploration exploitation inference for software testing able to retrieve distinct feasible path with high probability exist proceeds by alternatively exploiting and updating a distribution on the set of program path an original representation of path accommodating long range dependency and data sparsity and based on extended parikh map is proposed experimental validation on real world and artificial problem demonstrates dramatic improvement compared to the state of the art 
singleton arc consistency sac enhances the pruning capability of arc consistency by ensuring that the network cannot become arc inconsistent after the assignment of a value to a variable algorithm have already been proposed to enforce sac but they are far from optimal time complexity we give a lower bound to the time complexity of enforcing sac and we propose an algorithm that achieves this complexity thus being optimal however it can be costly in space on large problem we then propose another sac algorithm that trade time optimality for a better space complexity nevertheless this last algorithm ha a better worst case time complexity than previously published sac algorithm an experimental study show the good performance of the new algorithm 
some search problem are most directly specified by boolean combination of pseudo boolean constraint we study a logic pl pb whose formula are of this form and design local search method to compute model of pl pb theory in our approach we view a pl pb theory t a a data structure a concise representation of a certain propositional cnf theory cl t logically equivalent to t we show that parameter needed by local search algorithm for cnf theory such a walksat can be estimated on the basis of t without the need to compute cl t explicitly since cl t is often much larger than t running search based on t promise performance gain our experimental result confirm this expectation 
real world agent must react to changing condition a they execute planned task condition are typically monitored through time series representing state variable while some predicate on these time series only consider one measure at a time other predicate sometimes called episodic predicate consider set of measure we consider a special class of episodic predicate based on segmentation of the the measure into quasi monotonic interval where each interval is either quasi increasing quasi decreasing or quasi fiat while being scale based this approach is also computational efficient and result can be computed exactly without need for approximation algorithm our approach is compared to linear spline and regression analysis 
in this paper we address the problem of learning support vector machine svm classifier from distributed data source we identify sufficient statistic for learning svms and present an algorithm that learns svms from distributed data by iteratively computing the set of sufficient statistic we prove that our algorithm is exact with respect to it centralized counterpart and efficient in term of time complexity 
question paraphrasing is critical in many natural language processing nlp application especially for question reformulation in question answering qa however choosing an appropriate data source and developing effective method are challenging task in this paper we propose a method that exploit encarta log to automatically identify question paraphrase and extract template question from encarta log are partitioned into small cluster within which a perceptron classier is used for identifying question paraphrase experiment are conducted and the result have shown encarta log data is an eligible data source for question paraphrasing and the user click in the data are indicative clue for recognizing paraphrase the supervised method we present is effective which can evidently outperform the unsupervised method besides the feature introduced to identify paraphrase are sound the obtained question paraphrase template are quite effective in question reformulation enhancing the mrr from to with the question of trec qa 
decomposable negation normal form formula dnnfs form an interesting propositional fragment both for efficiency and succinctness reason a famous subclass of the dnnf fragment is the obdd fragment which offer many polytime query and transformation including quantifier elimination under some ordering restriction nevertheless the decomposable and node at work in obdds enable only sequential decision cluster of variable are never assigned in parallel like in full dnnfs this is an serious drawback since succinctness for the full dnnf fragment relies on such a parallelization property this is why we suggest to go a step further from sequentially ordered decision diagram to partially ordered decomposable decision graph in which any decomposable and node is allowed and not only assignment one we show that like the obdd fragment such a new class offer many tractable query and transformation including quantifier elimination under some ordering restriction furthermore we show that this class is strictly more succinct than obdd 
sequential composition of voting rule by making use of structural property of the voter preference provide computationally economical way for making a common decision over a cartesian product of finite local domain a sequential composition is usually defined on a set of legal profile following a fixed order in this paper we generalize this by order independent sequential composition and strong decomposability which are independent of the chosen order we study to which extent some usual property of voting rule transfer from the local rule to their order independent sequential composition then to capture the idea that a voting rule is neutral or decomposable on a slightly smaller domain we define nearly neutral nearly decomposable rule for both sequential composition and order independent sequential composition which lead u to defining and studying decomposable permutation we prove that any sequential composition of neutral local rule and any order independent sequential composition of neutral local rule satisfying a necessary condition are nearly neutral 
learning by reading requires integrating several strand of ai research we describe a prototype system learning reader which combine natural language processing a large scale knowledge base and analogical processing to learn by reading simplified language text we outline the architecture of learning reader and some of system level result then explain how these result arise from the component specifically we describe the design implementation and performance characteristic of a natural language understanding model dmap that is tightly coupled to a knowledge base three order of magnitude larger than previous attempt we show that knowing the kind of question being asked and what might be learned can help provide more relevant efficient reasoning finally we show that analogical processing provides a mean of generating useful new question and conjecture when the system ruminates off line about what it ha read 
in this paper we propose a new progression mechanism for a restricted form of incomplete knowledge formulated a a basic action theory in the situation calculus specifically we focus on functional fluents and deal directly with the possible value these fluents may have and how these value are affected by both physical and sensing action the method we propose is logically complete and can be calculated efficiently using database technique under certain reasonable assumption 
we describe a novel training criterion for probabilistic grammar induction model contrastive estimation smith and eisner which can be interpreted a exploiting implicit negative evidence and includes a wide class of likelihood based objective function this criterion is a generalization of the function maximized by the expectationmaximization algorithm dempster et al ce is a natural fit for log linear model which can include arbitrary feature but for which em is computationally difficult we show that using the same feature log linear dependency grammar model trained using ce can drastically outperform emtrained generative model on the task of matching human linguistic annotation the matchlinguist task the selection of an implicit negative evidence class a neighborhood appropriate to a given task ha strong implication but a good neighborhood one can target the objective of grammar induction to a specific application 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
information extraction ie system are prone to false hit for a variety of reason and we observed that many of these false hi t occur in sentence that contain subjective language e g opinion emotion and sentiment motivated by these observation we explore the idea of using subjectivity analysis to improve the precision of information extraction system in this paper we describe an ie system that us a subjective sentence classifier to filter it extraction we experimented with several different strategy for using the subjectivity classification including an aggressive strategy that discard all extraction found in subjective sentence and more complex strategy that selectively discard extraction we evaluated the performance of these different approach on the muc terrorism data set we found that indiscriminately filtering extraction from subjective sentence wa overly aggressive but more selective filtering strategy improved ie precision with minimal recall loss 
a major problem when developing neural network or machine diagnostics situation is that no data or very little data is available for training on fault condition however the domain expert often ha a good idea of what to expect in term of input and output parameter value if the expert can express these relationship in the form of rule this would provide a resource too valuable to ignore fuzzy logic is used to handle the imprecision and vagueness of natural language and provides this additional advantage to a system this paper investigates the development of a novel knowledge insertion algorithm that explores the benefit of prestructuring rbf neural network by using prior fuzzy domain knowledge and previous training experience pre structuring is accomplished by using fuzzy rule gained from a domain expert and using them to modify existing radial basis function rbf network the benefit and novel achievement of this work enable rbf neural network to be trained without actual data but to rely on input to output mapping defined through expert knowledge 
previous approach to service selection are mainly based on capturing and exchanging the rating of consumer to provider however rating reflect taste of the raters therefore service selection using rating may mislead the consumer having a taste different than that of the raters we propose to use experience instead of the rating experience are the representation of what is requested by a consumer and what is received at the end unlike rating experience do not reflect the opinion of the others but the actual story between consumer and provider concerning a service demand using experience the consumer model the service of a provider for a specific service demand and selects the provider that is expected to satisfy the consumer the most our simulation show that proposed approach significantly increase the overall satisfaction of the service consumer 
this paper present an algorithm for inferring a structured hidden markov model s hmm from a set of sequence the s hmms are a subclass of the hierarchical hidden markov model and are well suited to problem of process user profiling the learning algorithm is unsupervised and follows a mixed bottom up top down strategy in which elementary fact in the sequence motif are progressively grouped thus building up the abstraction hierarchy of a s hmm layer after layer the algorithm is validated on a suite of artificial datasets where the challenge for the learning algorithm is to reconstruct the model that generated the data then an application to a real problem of molecular biology is briefly described 
the martingale framework for detecting change in data stream currently only applicable to labeled data is extended here to unlabeled data using clustering concept the one pas incremental changedetection algorithm i doe not require a sliding window on the data stream ii doe not require monitoring the performance of the clustering algorithm a data point are streaming and iii work well for high dimensional data stream to enhance the performance of the martingale change detection method the multiple martingale test method using multiple view is proposed experimental result show i the feasibility of the martingale method for detecting change in unlabeled data stream and ii the multiple martingale test method compare favorably with alternative method using the recall and precision measure for the video shot change detection problem 
we consider the problem of synthesizing a team of local behavior controller to realize a fully controllable target behavior from a set of available partially controllable behavior that execute distributively within a shared partially predictable but fully observable environment available behavior stand for existing distributed component and are represented with finite nondeterministic transition system the target behavior is assumed to be fully deterministic and stand for the collective behavior that the system a a whole need to guarantee we formally define the problem within a general framework characterize it computational complexity and propose technique to actually generate a solution also we investigate the relationship between the distributed solution and the centralized one in which a single global controller is conceivable 
a web page may be relevant to multiple topic even when nominally on a single topic the page may attract attention and thus link from multiple community instead of indiscriminately summing the authority provided by all page we decompose a web page into separate subnodes with respect to each community pointing to it utilizing the relevance of such community allows u to better model the semantic structure of the web leading to better estimate of authority for a given query we apply a total of eighty query over two real world datasets to demonstrate that the use of community decomposition can consistently and significantly improve upon page rank s top ten result 
large real time search problem such a path finding in computer game and robotics limit the applicability of complete search method such a a a a result real time heuristic method are becoming more wide spread in practice these algorithm typically conduct a limited depth lookahead search and evaluate the state at the frontier using a heuristic action selected by such method can be suboptimal due to the incompleteness of their search and inaccuracy in the heuristic lookahead pathology occur when a deeper search decrease the chance of selecting a better action over the last two decade research on lookahead pathology ha focused on minimax search and small synthetic example in single agent search a real time search method gain ground in application the importance of understanding and remedying lookahead pathology increase this paper for the first time conduct a large scale investigation of lookahead pathology in the domain of real time path finding we use map from commercial computer game to show that deeper search often not only consumes additional in game cpu cycle but also decrease path quality a a second contribution we suggest three explanation for such pathology and support them empirically finally we propose a remedy to lookahead pathology via a method for dynamic lookahead depth selection this method substantially improves on line performance and a an added benefit spare the user from having to tune a control parameter 
max sat is an optimization version of the well known sat problem it is of great importance from both a theoretical and a practical point of view in recent year there ha been considerable interest in finding efficient solving technique alsinet et al xing and zhang shen and zhang de givry et al most of this work focus on the computation of good quality lower bound to be used within a branch and bound algorithm unfortunately lower bound are described in a procedural way because of that it is difficult to realize the logic that is behind in this paper we introduce a logical framework for max sat solving using this framework we introduce an extension of the davis putnam algorithm that we call max dpll and the resolution rule our framework ha the advantage of nicely integrating branch and bound concept such a the lower and upper bound a well a hiding away implementation detail we show that max dpll augmented with a restricted form of resolution at each branching point is an effective solving strategy we also show that the resulting algorithm is closely related with some local consistency property developed for weighted constraint satisfaction problem 
existing approach to multirobot coordination separate scheduling and task allocation but finding the optimal schedule with joint task and spatial constraint requires robot to simultaneously solve the scheduling task allocation and path planning problem we present a formal description of the multirobot joint task allocation problem with heterogeneous capability and spatial constraint and an instantiation of the problem for the search and rescue domain we introduce a novel declarative framework for modeling the problem a a mixed integer linear programming milp problem and present a centralized anytime algorithm with error bound we demonstrate that our algorithm can outperform standard milp solving technique greedy heuristic and a market based approach which separate scheduling and task allocation 
hypertree decomposition ha been shown to be the most general csp decomposition method however so far the exact method are not able to find optimal hypertree decomposition of realistic instance we present a backtracking procedure which along with isomorphic component detection result in optimal hypertree decomposition we also make the procedure generic variation of which result in two new tractable decomposition hyperspread and connected hypertree we show that the hyperspread width is bounded by both the hypertree width and the spread cut width which solves a recently stated open problem in our experiment on several realistic instance our method find many optimal decomposition while the previous method can find at most one 
document clustering is traditionally tackled from the perspective of grouping document that are topically similar however many other criterion for clustering document can be considered for example document genre or the author s mood we propose an interactive scheme for clustering document collection based on any criterion of the user s preference the user hold an active position in the clustering process first she chooses the type of feature suitable to the underlying task leading to a task specific document representation she can then provide example of feature if such example are emerging e g when clustering by the author s sentiment word like perfect mediocre awful are intuitively good feature the algorithm proceeds iteratively and the user can fix error made by the clustering system at the end of each iteration such an interactive clustering method demonstrates excellent result on clustering by sentiment substantially outperforming an svm trained on a large amount of labeled data even if feature are not provided because they are not intuitively obvious to the user e g what would be good feature for clustering by genre using part of speech trigram our multi modal clustering method performs significantly better than k mean and latent dirichlet allocation lda 
this paper describes an ontology for inland water feature built using formal concept analysis and supervaluation semantics the first is used to generate a complete lattice of the water domain whereas supervaluation semantics is used to model the variability of the concept in term of threshold parameter we also present an algorithm for a mechanism of individuation and classification of water feature from snapshot of river network according to the proposed ontology 
the definition of accurate similarity measure is a key issue of every case based reasoning application although some approach to optimize similarity measure automatically have already been applied these approach are not suited for all cbr application domain on the one hand they are restricted to classification task on the other hand they only allow optimization of feature weight we propose a novel learning approach which address both problem i e it is suited for most cbr application domain beyond simple classification and it enables learning of more sophisticated similarity measure 
we designed and evaluated multiagent control for microscopic robot nanorobots aiding the surgical repair of damaged nerve cell this repair operates on both nerve a a whole at scale of hundred of micron and individual nerve cell axon at scale of about a micron we match the robot to these size using a combination of microelectomechanical mem machine for the larger operation and nanorobots for operation on individual cell muitiagent control allows accurate and rapid repair with such robot with only modest computational and communication requirement for the nanorobots a significant benefit due to their physical limitation our simulation using physical parameter dictated by nerve biology and plausible nanorobotic capability show how specific control choice lead to trade offs in clinical outcome beyond the specific example of nerve repair treated here multi scale robot could aid a variety of medical and biological task involving both the large scale of organ or tissue and the microscopic scale of individual cell 
a facility with front room and back room operation ha the option of hiring specialized or more expensive cross trained worker assuming stochastic customer arrival and service time we seek a smallest cost combination of cross trained and specialized worker satisfying constraint on the expected customer waiting time and expected number of worker in the back room a constraint programming approach using logic based bender decomposition is presented experimental result demonstrate the strong performance of this approach across a wide variety of problem parameter this paper provides one of the first link between queueing optimization problem and constraint programming 
in this paper we propose a model for human leaming and decision making in environment of repeated cliff edge ce interaction in ce environment which include common daily interaction such a sealed bid auction and the ultimatum game ug the probability of success decrease monotonically a the expected reward increase thus ce environment are characterized by an underlying conflict between the strive to maximize profit and the fear of causing the entire deal to fall through we focus on the behavior of people who repeatedly compete in one shot ce interaction with a different opponent in each interaction our model which is based upon the deviated virtual reinforcement learning dvrl algorithm integrates the learning direction theory with the reinforcement learning algorithm we also examined several other model using an innovative methodology in which the decision dynamic of the model were compared with the empirical decision pattern of individual during their interaction an analysis of human behavior in auction and in the ug reveals that our model fit the decision pattern of far more subject than any other model 
supervised local tangent space alignment sltsa is an extension of local tangent space alignment ltsa to supervised feature extraction two algorithmic improvement are made upon ltsa for classification first a simple technique is proposed to map new data to the embedded low dimensional space and make ltsa suitable in a changing dynamic environment then sltsa is introduced to deal with data set containing multiple class with class membership information 
a basic assumption in traditional machine learning is that the training and test data distribution should be identical this assumption may not hold in many situation in practice but we may be forced to rely on a different distribution data to learn a prediction model for example this may be the case when it is expensive to label the data in a domain of interest although in a related but different domain there may be plenty of labeled data available in this paper we propose a novel transfer learning algorithm for text classification based on an em based naive bayes classifier our solution is to first estimate the initial probability under a distribution dl of one labeled data set and then use an em algorithm to revise the model for a different distribution du of the test data which are unlabeled we show that our algorithm is very effective in several different pair of domain where the distance between the different distribution are measured using the kullback leibler kl divergence moreover kl divergence is used to decide the trade off parameter in our algorithm in the experiment our algorithm outperforms the traditional supervised and semi supervised learning algorithm when the distribution of the training and test set are increasingly different 
recently a number of author have proposed criterion for evaluating learning algorithm in multiagent system while well justified each of these ha generally given little attention to one of the main challenge of a multi agent setting the capability of the other agent to adapt and learn a well we propose extending existing criterion to apply to a class of adaptive opponent with bounded memory we then show an algorithm that provably achieves an o best response against this richer class of opponent while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self play this new algorithm also demonstrates strong performance in empirical test against a variety of opponent in a wide range of environment 
in today s connected world it is possible and very common to interact with unknown people whose reliability is unknown trust metric are a recently proposed technique for answering question such a should i trust this user however most of the current research assumes that every user ha a global quality score and that the goal of the technique is just to predict this correct value we show on data from a real and large user community epinions com that such an assumption is not realistic because there is a significant portion of what we call controversial user user who are trusted and distrusted by many a global agreement about the trustworthiness value of these user cannot exist we argue using computational experiment that the existence of controversial user a normal phenomenon in society demand local trust metric technique able to predict the trustworthiness of an user in a personalized way depending on the very personal view of the judging user 
the option framework provides method for reinforcement learning agent to build new high level skill however since option are usually learned in the same state space a the problem the agent is solving they cannot be used in other task that are similar but have different state space we introduce the notion of learning option in agentspace the space generated by a feature set that is present and retains the same semantics across successive problem instance rather than in problemspace agent space option can be reused in later task that share the same agent space but have different problem space we present experimental result demonstrating the use of agent space option in building transferrable skill and show that they perform best when used in conjunction with problem space option 
np search and decision problem occur widely in ai and a number of general purpose method for solving them have been developed the dominant approach include propositional satisfiability sat constraint satisfaction problem csp and answer set programming asp here we propose a declarative constraint programming framework which we believe combine many strength of these approach while addressing weakness in each of them we formalize our approach a a model extension problem which is based on the classical notion of extension of a structure by new relation a parameterized version of this problem capture np we discus property of the formal framework intended to support effective modelling and prospect for effective solver design 
recent research on point based approximation algorithm for pomdps demonstrated that good solution to pomdp problem can be obtained without considering the entire belief simplex for instance the point based value iteration pbvi algorithm pineau et al computes the value function only for a small set of belief state and iteratively add more point to the set a needed a key component of the algorithm is the strategy for selecting belief point such that the space of reachable belief is well covered this paper present a new method for selecting an initial set of representative belief point which relies on finding first the basis for the reachable belief simplex our approach ha better worst case performance than the original pbvi heuristic and performs well in several standard pomdp task 
we study learning structured output in a discriminative framework where value of the output variable are estimated by local classifier in this framework complex dependency among the output variable are captured by constraint and dictate which global label can be inferred we compare two strategy learning independent classifier and inference based training by observing their behavior in different condition experiment and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifier are difficult to learn but may require many example before any discernible difference can be observed 
this paper proposes a systematic approach of representing abstract feature in term of low level subjective state representation we demonstrate that a mapping between the agent s predictive state representation and abstract feature can be derived automatically from high level training data supplied by the designer our empirical evaluation demonstrates that an experience oriented state representation built around a single bit sensor can represent useful abstract feature such a back against a wall in a corner or in a room a a result the agent gain virtual sensor that could be used by it control policy 
the gac scheme ha become a popular general purpose algorithm for solving n ary constraint although it may scan an exponential number of supporting tuples in this paper we develop a major improvement of this scheme when searching for a support our new algorithm is able to skip over a number of tuples exponential in the arity of the constraint by exploiting knowledge about the current domain of the variable we demonstrate the effectiveness of the method for large table constraint 
many real world classification problem involve large number of overlapping category that are arranged in a hierarchy or taxonomy we propose to incorporate prior knowledge on category taxonomy directly into the learning architecture we present two concrete multi label classification method a generalized version of perceptron and a hierarchical multi label svm learning our method work with arbitrary not necessarily singly connected taxonomy and can be applied more generally in setting where category are characterized by attribute and relation that are not necessarily induced by a taxonomy experimental result on wipo alpha collection show that our hierarchical method bring significant performance improvement 
interest based negotiation ibn is a form of negotiation in which agent exchange information about their underlying goal with a view to improving the likelihood and quality of a deal while this intuition ha been stated informally in much previous literature there is no formal analysis of the type of deal that can be reached through ibn and how they differ from those reachable using classical alternating offer bargaining this paper bridge this gap by providing a formal framework for analysing the outcome of ibn dialogue and begin by analysing a specific ibn protocol 
exact parsing with finite state automaton is deemed in apropriate because of the unbounded non locality language overwhelmingly exhibit we propose a way to structure the parsing task in order to make it amenable to local classification method this allows u to build a dynamic bayesian network which uncovers the syntactic dependency structure of english sentence experiment with the wall street journal demonstrate that the model successfully learns from labeled data 
rdf resource description framework is now a widely used world wide web consortium standard however method to index large volume of rdf data are still in their infancy in this paper we focus on providing a very lightweight indexing mechanism for certain kind of rdf query namely graph based query where there is a need to traverse edge in the graph determined by an rdf database our approach us the idea of drawing circle around selected center vertex in the graph where the circle would encompass those vertex in the graph that are within a given distance of the center vertex we come up with method of finding such center vertex and identifying the radius of the circle and then leverage this to build an index called grin we compare grin with three existing rdf indexex jena sesame and rdfbroker we compared i the time to answer graph based query ii memory needed to store the index and iii the time to build the index grin outperforms jena sesame and rdfbroker on all three measure for graph based query for other type of query it may be worth building one of these other index and using it at the expense of using a larger amount of memory when answering query 
the problem of maintaining a desired number of mobile agent on a network is not trivial especially if what is required is a completely decentralized solution decentralized control make a system more robust and le susceptible to partial failure the problem of agent population management is exacerbated on wireless ad hoc network where host mobility can result in significant change in the network size and topology system stability is also of critical importance this paper analyzes the stability of a previously proposed ecology inspired approach to agent population management and proposes improvement the stability of the new ecology based strategy is proved theoretically and the conclusion are verified with a set of experiment 
modularity is a key requirement for collaborative ontology engineering and for distributed ontology reuse on the web modern ontology language such a owl are logic based and thus a useful notion of modularity need to take the semantics of ontology and their implication into account we propose a logic based notion of modularity that allows the modeler to specify the external signature of their ontology whose symbol are assumed to be defined in some other ontology we define two restriction on the usage of the external signature a syntactic and a slightly le restrictive semantic one each of which is decidable and guarantee a certain kind of black box behavior which enables the controlled merging of ontology analysis of real world ontology suggests that these restriction are not too onerous 
information integration system combine data from multiple heterogeneous web service to answer complex user query provided a user ha semantically modeled the service first to model a service the user ha to specify semantic type of the input and output data it us and it functionality a large number of new service come online it is impractical to require the user to come up with a semantic model of the service or rely on the service provider to conform to a standard instead we would like to automatically learn the semantic model of a new service this paper address one part of the problem namely automatically recognizing semantic type of the data used by web service we describe a metadata based classification method for recognizing input data type using only the term extracted from a web service definition file we then verify the classifier s prediction by invoking the service with some sample data of that type once we discover correct classification we invoke the service to produce output data sample we then use content based classifier to recognize semantic type of the output data we provide performance result of both classification method and validate our approach on several live web service 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
memory bound may limit the ability of a reasoner to make inference and therefore affect the reasoner s usefulness in this paper we propose a framework to automatically verify the reasoning capability of propositional memory bounded reasoner which have a sequential architecture our framework explicitly account for the use of memory both to store fact and to support backtracking in the course of deduction we describe an implementation of our framework in which proof existence is recast a a strong planning problem and present result of experiment using the mbp planner which indicate that memory bound may not be trivial to infer even for simple problem and that memory bound and length of derivation are closely inter related 
we establish a declarative theory of forgetting for disjunctive logic program the suitability of this theory is justified by a number of desirable property in particular one of our result show that our notion of forgetting is completely captured by the classical forgetting a transformation based algorithm is also developed for computing the result of forgetting we also provide an analysis of computational complexity a an application of our approach a fairly general framework for resolving conflict in inconsistent knowledge base represented by disjunctive logic program is defined the basic idea of our framework is to weaken the preference of each agent by forgetting certain knowledge that cause inconsistency in particular we show how to use the notion of forgetting to provide an elegant solution for preference elicitation in disjunctive logic programming 
language game represent one of the most fascinating challenge of research in artificial intelligence in this paper we give an overview of webcrow a system that tackle crossword using the web a a knowledge base this appears to be a novel approach with respect to the available literature it is also the first solver for non english crossword and it ha been designed to be potentially multilingual although webcrow ha been implemented only in a preliminary version it already display very interesting result reaching the performance of a human beginner crossword that are easy for expert human are solved within competition time limit with of correct word and over of correct letter 
in this paper we show how using the dirichlet process mixture model a a generative model of data set provides a simple and effective method for transfer learning in particular we present a hierarchical extension of the classic naive bayes classifier that couple multiple naive bayes classifier by placing a dirichlet process prior over their parameter and show how recent advance in approximate inference in the dirichlet process mixture model enable efficient inference we evaluate the resulting model in a meeting domain in which the system decides based on a learned model of the user s behavior whether to accept or reject the request on his or her behalf the extended model outperforms the standard naive bayes model by using data from other user to influence it prediction 
hybrid approximate linear programming halp ha recently emerged a a promising framework for solving large factored markov decision process mdps with discrete and continuous state and action variable our work address it major computational bottleneck constraint satisfaction in large structured domain of discrete and continuous variable we analyze this problem and propose a novelmarkov chainmonte carlo mcmc method for finding the most violated constraint of a relaxed halp this method doe not require the discretization of continuous variable search the space of constraint intelligently based on the structure of factored mdps and it space complexity is linear in the number of variable we test the method on a set of large control problem and demonstrate improvement over alternative approach 
this paper present a tableau decision procedure for shoiq the dl underlying owl dl to the best of our knowledge this is the first goal directed decision procedure for shoiq 
in case based reasoning cbr system for product recommendation the retrieval of acceptable product based on limited information is an important and challenging problem a we show in this paper basic retrieval strategy such a nearest neighbor are potentially unreliable when applied to incomplete query to address this issue we present technique for automating the discovery of recommendation rule that are provably reliable and non conflicting while requiring minimal information for their application in a rule based approach to the retrieval of recommended case 
the popularity of current hand held digital imaging device such a camera phone pda camcorder ha promoted the use of digital camera to capture document image for daily information recording purpose however the captured image often contain photometric and geometric distortion when the document are of non planar shape which cause significant problem to various document image analysis dia task such a ocr in this paper we propose a restoration framework that remove both photometric and geometric distortion in smoothly warped document image to facilitate human perception and machine recognition first the photometric distortion are corrected by separating the shading image from the reflectance image using inpainting and surface fitting technique next a pas shape from shading sfs method is exploited to recover the document s surface shape based on the extracted shading image once the document s shape is obtained the geometric distortion are rectified through a physically based flattening process experiment on real document image show the performance of each sub task and demonstrate a complete solution to the restoration of physically distorted document image 
existing prior domain knowledge represents a valuable source of information for image interpretation problem such a classifying handwritten character such domain knowledge must be translated into a form understandable by the learner translation can be realized with explanation based learning ebl which provides a kind of dynamic inductive bias combining domain knowledge and training example the dynamic bias formed by the interaction of domain knowledge with training example can yield solution knowledge of potential higher quality than can be anticipated by the static bias designer without seeing training example we detail how ebl can be used to dynamically integrate domain knowledge training example and the learning mechanism and describe the two ebl approach in sun dejong a and sun dejong b 
we present sequential and parallel algorithm for frontier a fa algorithm augmented with a form of delayed duplicate detection ddd the sequential algorithm fa ddd overcomes the leak back problem associated with the combination of fa and ddd the parallel algorithm pfa ddd is a parallel version of fa ddd that feature a novel workload distribution strategy based on interval we outline an implementation of pfa ddd designed to run on a cluster of workstation the implementation computes interval at run time that are tailored to fit the workload at hand because the implementation distributes the workload in a manner that is both automated and adaptive it doe not require the user to specify a workload mapping function and more importantly it is applicable to arbitrary problem that may be irregular we present the result of an experimental evaluation of the implementation where it is used to solve instance of the multiple sequence alignment problem on a cluster of workstation running on top of a commodity network result demonstrate that the implementation offer improved capability in addition to improved performance 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
game ai is the decision making process of computer controlled opponent in computer game adaptive game ai can improve the entertainment value of computer game it allows computer controlled opponent to automatically fix weakness in the game ai and respond to change in human player tactic dynamic scripting is a recently developed approach for adaptive game ai that learns which tactic i e action sequence an opponent should select to play effectively against the human player in previous work these tactic were manually generated we introduce akads it us an evolutionary algorithm to automatically generate such tactic our experiment show that it improves dynamic scripting s performance on a real time strategy rts game therefore we conclude that high quality domain knowledge i e tactic can be automatically generated for strong adaptive ai opponent in rts game this reduces the time and effort required by game developer to create intelligent game ai thus freeing them to focus on other important topic e g storytelling graphic 
the generator and the unique closed pattern of an equivalence class of itemsets share a common set of transaction the generator are the minimal one among the equivalent itemsets while the closed pattern is the maximum one a a generator is usually smaller than the closed pattern in cardinality by the minimum description length principle the generator is preferable to the closed pattern in inductive inference and classification to efficiently discover frequent generator from a large dataset we develop a depth first algorithm called gr growth the idea is novel in contrast to traditional breadth first bottom up generator mining algorithm our extensive performance study show that gr growth is significantly faster an order or even two order of magnitude when the support threshold are low than the existing generator mining algorithm it can be also faster than the state of the art frequent closed itemset mining algorithm such a fpclose and closet 
ontology management and maintenance are considered cornerstone issue in current semantic web application in which semantic integration and ontological reasoning play a fundamental role the ability to deal with inconsistency and to accommodate change is of utmost importance in real world application of ontological reasoning and management wherein the need for expressing negated assertion also arises naturally for this purpose precise formal definition of the the different type of inconsistency and negation in ontology are required unfortunately ontology language based on description logic dl do not provide enough expressive power to represent axiom negation furthermore there is no single well accepted notion of inconsistency and negation in the semantic web community due to the lack of a common and solid foundational framework in this paper we propose a general framework accounting for inconsistency negation and change in ontology different level of negation and inconsistency in dl based ontology are distinguished we demonstrate how this framework can provide a foundation for reasoning with and management of dynamic ontology 
we study challenge that are imposed to mathematical domain reasoning in the context of natural language tutorial dialog on mathematical proof the focus is on proof step evaluation i how can mathematical domain reasoning support the resolution of ambiguity and underspecified part in proof step uttered by a student ii how can mathematical domain reasoning support the evaluation of a proof step with respect to the criterion soundness granularity and relevance 
in this paper we describe a system that automatically convert narrative into d scene the text written in swedish describe road accident one of the program s key feature is that it animates the generated scene using temporal relation between the event we believe that this system is the first text to scene converter that is not restricted to invented narrative the system consists of three module natural language interpretation based on information extraction ie method a planning module that produce a geometric description of the accident and finally a visualization module that render the geometric description a animated graphic an evaluation of the system wa carried out in two step first we used standard ie scoring method to evaluate the language interpretation the result are on the same level a for similar system tested previously secondly we performed a small user study to evaluate the quality of the visualization the result validate our choice of method and since this is the first evaluation of a text to scene conversion system they also provide a baseline for further study 
this paper contains two important contribution for the development of possibilistic causal network the first one concern the representation of intervention in possibilistic network we provide the counterpart of the do operator recently introduced by pearl in possibility theory framework we then show that intervention can equivalently be represented in different way in possibilistic causal network the second main contribution is a new propagation algorithm for dealing with both observation and intervention we show that our algorithm only need a small extra cost for handling intervention and is more appropriate for handling sequence of observation and intervention 
we study the decision problem facing agent in repeated matching environment with learning or two sided bandit problem and examine the dating market in which men and woman repeatedly go out on date and learn about each other a an example we consider three natural matching mechanism and empirically examine property of these mechanism focusing on the asymptotic stability of the resulting matchings when the agent use a simple learning rule coupled with an greedy exploration policy matchings tend to be more stable when agent are patient in two different way if they are more likely to explore early or if they are more optimistic however the two form of patience do not interact well in term of increasing the probability of stable outcome we also define a notion of regret for the two sided problem and study the distribution of regret under the different matching mechanism 
the primary objective of this work is to categorize the available fly ash in different part of the world into distinct group based on it compositional attribute kohonen s selforganizing feature map and radial basis function network are utilized for the classification of fly ash in term of it chemical parameter the basic procedure of the methodology consists of three stage apply self organizing neural net and delineate distinct group of fly ash and identify the group sensitive attribute find mean value of sensitive attribute of the elicited group and augment them a start up prototype for k mean algorithm and find the refined centroid of these group incorporate the centroid in a two layer radial basis function network and refine the delineation of the group and develop an indexing equation using the weight of the stabilized network further to demonstrate the utility of this classification scheme the so formed group were correlated with their performance in high volume fly ash concrete system hvfac the categorization wa found to be excellent and compare well with canadian standard association s csa a classification scheme 
spatial reasoning is a fundamental aspect of intelligent behavior which cognitive architecture must address in a problem independent way bimodal system employing both qualitative and quantitative representation of spatial information are efficient and psychologically plausible mean for spatial reasoning any such system must employ a translation from the qualitative level to the quantitative where new object image are created through the process of predicate projection this translation ha received little scrutiny we examine this issue in the context of a bimodal spatial reasoning system integrated with a cognitive architecture soar a part of this system we define an expressive language for predicate projection that support general and flexible image creation we demonstrate this system on multiple spatial reasoning problem in the orts real time strategy game environment 
we propose a simple declarative language for specifying a wide range of counting and occurrence constraint this specification language is executable since it immediately provides a polynomial propagation algorithm to illustrate the capability of this language we specify a dozen global constraint taken from the literature we observe one of three outcome we achieve generalized arc consistency we do not achieve generalized arc consistency but achieving generalized arc consistency is np hard we do not achieve generalized arc consistency but specialized propagation algorithm can do so in polynomial time experiment demonstrate that this specification language is both efficient and effective in practice 
the mobile robotics community ha traditionally addressed motion planning and navigation in term of steering decision however selecting the best speed is also important beyond it relationship to stopping distance and lateral maneuverability consider a high speed mph autonomous vehicle driving off road through challenging desert terrain the vehicle should drive slowly on terrain that pose substantial risk however it should not dawdle on safe terrain in this paper we address one aspect of risk shock to the vehicle we present an algorithm for trading off shock and speed in realtime and without human intervention the trade off is optimized using supervised learning to match human driving the learning process is essential due to the discontinuous and spatially correlated nature of the control problem classical technique do not directly apply we evaluate performance over hundred of mile of autonomous driving including performance during the darpa grand challenge this approach wa the deciding factor in our vehicle s speed for nearly of the darpa competition more than any other constraint except the darpa imposed speed limit and resulted in the fastest finishing time 
a certificate of satisfiability for a quantified boolean formula is a compact representation of one of it model which is used to provide solver independent evidence of satisfiability in addition it can be inspected to gather explicit information about the semantics of the formula due to the intrinsic nature of quantified formula such certificate demand much care to be efficiently extracted compactly represented and easily queried we show how to solve all these problem 
following verbal route instruction requires knowledge of language space action and perception we present marco an agent that follows free form natural language route instruction by representing and executing a sequence of compound action specification that model which action to take under which condition marco infers implicit action from knowledge of both linguistic conditional phrase and from spatial action and local configuration thus marco performs explicit action implicit action necessary to achieve the stated condition and exploratory action to learn about the world we gathered a corpus of route instruction from six people in three large scale virtual indoor environment thirtysix other people followed these instruction and rated them for quality these human participant finished at the intended destination on of the trial marco followed the same instruction in the same environment with a success rate of we measured the efficacy of action inference with marco variant lacking action inference executing only explicit action marco succeeded on just of the trial for this task inferring implicit action is essential to follow poor instruction but is also crucial for many highly rated route instruction 
the practical success of broadcast encryption hinge on the ability to revoke the access of compromised key and determine which key have been compromised in this work we focus on the latter the so called traitor tracing problem we present an adaptive tracing algorithm that selects forensic test according to the information gain criterion the result of the test refine an explicit bayesian model of our belief that certain key are compromised in choosing test based on this criterion we significantly reduce the number of test a compared to the state of the art technique required to identify compromised key a part of the work we developed an efficient distributable inference algorithm that is suitable for our application and also give an efficient heuristic for choosing the optimal test 
an unobservable mdp umdp is a pomdp in which there are no observation an only costly observable mdp ocomdp is a pomdp which extends an umdp by allowing a particular costly action which completely observes the state we introduce ur max a reinforcement learning algorithm with polynomial interaction complexity for unknown ocomdps 
we present an interactive web based system for musical song search and retrieval using rhythmic tapping a the primary mean of query input our approach involves encoding the input rhythm a a contour string and using approximate string matching to determine the most likely match with song in the database 
in this paper we explore the hypothesis that episodic memory is a critical component for cognitive architecture that support general intelligence episodic memory overlap with case based reasoning cbr and can be seen a a task independent architectural approach to cbr we define the design space for episodic memory system and the criterion any implementation must meet to be useful in a cognitive architecture we present an implementation and demonstrate how episodic memory combined with other component of a cognitive architecture support a wealth of cognitive capability that are difficult to attain without it 
epfc emerging pattern in food complaint is the analytical component of the consumer complaint monitoring system designed to help the food safety official to efficiently and effectively monitor incoming report of adverse effect of food on it consumer these report collected in a passive surveillance mode contain multi dimensional heterogeneous and sparse snippet of spedfic information about the consumer demographic the kind brand and source of the food involved symptom of possible sickness characteristic of foreign object which could have been found in food involved location and time of occurrence etc statistical data mining component of the system empowers it user allowing for increased accuracy specificity and timeliness of detection of naturally occurring problem a well a of potential act of agro terrorism the system s main purpose is to enhance discovery and mitigation of food borne threat to public health in the usda food safety inspection service regulated product a such it is being envisioned a one of the key component of the nationwide biosecurity protection infrastructure it ha been accepted for use and it is currently going through the final stage of deployment this paper explains the motivation key design concept and report the system s utility and performance observed so far 
recent work show that the memory requirement of a and related graph search algorithm can be reduced substantially by only storing node that are on or near the search frontier using special technique to prevent node regeneration and recovering the solution path by a divide and conquer technique when this approach is used to solve graph search problem with unit edge cost we have shown that a breadth first search strategy can be more memory efficient than a best first strategy we provide an overview of our work using this approach which we call breadth first heuristic search 
the problem of consciousness ha captured the imagination of philosopher neuroscientist and the general public but ha received little attention within ai however concept from robotics and computer vision hold great promise to account for the major aspect of the phenomenon of consciousness including philosophically problematical aspect such a the vividness of qualia the first person character of conscious experience and the property of intentionality this paper present and evaluates such an account against eleven feature of consciousness that any philosophical scientific theory should hope to explain according to the philosopher and prominent ai critic john searle 
the quantified constraint satisfaction problem qcsp is a generalisation of the classical csp in which some of variable can be universally quantified in this paper we extend two well known concept in classical constraint satisfaction to the quantified case problem relaxation and explanation of inconsistency we show that the generality of the qcsp allows for a number of different form of relaxation not available in classical csp we further present an algorithmfor computing a generalisation of conflict based explanation of inconsistency for the qcsp 
a major issue in activity recognition in a sensor network is how to automatically segment the low level signal sequence in order to optimize the probabilistic recognition model for goal and activity past effort have relied on segmenting the signal sequence by hand which is both time consuming and error prone in our view segment should correspond to atomic human activity that enable a goal recognizer to operate optimally the two are intimately related in this paper we present a novel method for building probabilistic activity model at the same time a we segment signal sequence into motion pattern we model each motion pattern a a linear dynamic model and the transition between motion pattern a a markov process conditioned on goal our em learning algorithm simultaneously learns the motion pattern boundary and probabilistic model for goal and activity which in turn can be used to accurately recognize activity in an online phase a major advantage of our algorithm is that it can reduce the human effort in segmenting and labeling signal sequence we demonstrate the effectiveness of our algorithm using the data collected in a real wireless environment 
many lower bound computation method for branch and bound max sat solver can be explained a procedure that search for disjoint inconsistent subformulas in the max sat instance under consideration the difference among them is the technique used to detect inconsistency in this paper we define five new lower bound computation method two of them are based on detecting inconsistency via a unit propagation procedure that propagates unit clause using an original ordering the other three add an additional level of forward look ahead based on detecting failed literal finally we provide empirical evidence that the new lower bound are of better quality than the existing lower bound a well a that a solver with our new lower bound greatly outperforms some of the best performing state of the art max sat solver on max sat max sat and max cut instance 
it is notoriously difficult to simultaneously deal with both probabilistic and structural representation in a i particularly because probability necessitates a uniform representation of the training example in this paper we show how to build fully specified probabilistic model from arbitrary propositional case description about terrorist activity our method facilitates both reasoning and learning our solution is to use structural analogy to build probabilistic generalization about those case we use these generalization a a framework for mapping the structural representation which are well suited for reasoning into feature which are well suited for learning and back again finally we demonstrate how probabilistic generalization are an excellent bridge for joining reasoning and learning by using them to perform a traditional machine learning technique bayesian network modeling over arbitrarily high order structural data about terrorist action and further we discus how this might be used to facilitate automatic knowledge acquisition 
this article describes the expertcop tutorial system a simulator of the crime in an urban region in expertcop the student police officer configure and allocate an available police force according to a selected geographic region and then interact with the simulation the student interprets the result with the help of an intelligent tutor the pedagogical agent observing how the crime behaves in the presence of the allocated preventive policing the interaction between domain agent representing social entity a criminal and police team drive the simulation expertcop induces student to reflect on resource allocation the pedagogical agent implents interaction strategy between the student and the geosimulator designed to make simulated phenomenon better understood in particular the agent us a machine learning algorithm to identify pattern on simulation data and to formulate question to the student about these pattern moreover it explores the reasoning process of the domain agent by providing explanation that help the student to understand simulation event 
the multiarmed bandit is often used a an analogy for the tradeoff between exploration and exploitation in search problem the classic problem involves allocating trial to the arm of a multiarmed slot machine to maximize the expected sum of reward we pose a new variation of the multiarmed bandit the max k armed bandit in which trial must be allocated among the arm to maximize the expected best single sample reward of the series of trial motivation for the max k armed bandit is the allocation of restarts among a set of multistart stochastic search algorithm we present an analysis of this max k armed bandit showing under certain assumption that the optimal strategy allocates trial to the observed best arm at a rate increasing double exponentially relative to the other arm this motivates an exploration strategy that follows a boltzmann distribution with an exponentially decaying temperature parameter we compare this exploration policy to policy that allocate trial to the observed best arm at rate faster and slower than double exponentially the result confirm for two scheduling domain that the double exponential increase in the rate of allocation to the observed best heuristic outperfonns the other approach 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
current neural network learning algorithm are limited in their ability to model non linear dynamical system most supervised gradient based recurrent neural network rnns suffer from a vanishing error signal that prevents learning from input far in the past those that do not still have problem when there are numerous local minimum we introduce a general framework for sequence learning evolution of recurrent system with linear output evolino evolino us evolution to discover good rnn hidden node weight while using method such a linear regression or quadratic programming to compute optimal linear mapping from hidden state to output using the long short term memory rnn architecture the method is tested in three very different problem domain context sensitive language multiple superimposed sine wave and the mackey glass system evolino performs exceptionally well across all task where other method show notable deficiency in some 
visual analogical mapping and transfer can be used to derive a structural model of a drawing by analogy and moreover the problem of analogical mapping can be guided by using functional knowledge we view the interpretation drawing of design a deriving a structural model of the component and connection of the depicted device this problem is not deductive in nature but rather it is abductive a there is no a priori reason a shape must represent one object and not another only with significant help from the design context can a model be derived and in particular we propose to do it by deriving the model by analogy to a similar drawing with a known structural and teleological model this requires an analogical mapping from the source known drawing to the target drawing that is derived on the basis of shape and spatial relation and a transfer and adaptation process by which the old model is transferred to the new drawing and adapted to it in this paper we are focusing in particular on the first task that of analogical mapping analogical mapping is known to be hard computing an analogical mapping from a source to a target or query with no further information to guide the search to determine what ought to correspond to what is very difficult we propose to focus the search for a mapping by employing the functional knowledge represented in the teleological model let u consider for example the task of mapping the source drawing illustrated in figure a to the target drawing illustrated in figure b both drawing of a piston and crankshaft if an agent ha a low level geometric reasoner to recognize the shape and spatial relation between them then the agent can treat this shape representation a a set of mostly binary relation a contains b c is parallel to d and so on this is then a labelled graph analogical mapping must then find a projection of the one graph onto the other that preserve some but not necessarily all of the relation this is the well known np hard problem ofmaximum common subgraph we begin with a teleological model of the source case based on structure behavior function sbf model of physical device sbf model treat functional model a consisting of three piece the structural model of the component and connection and their functionally relevant proper 
plan recognition is the process of inferring other agent plan and goal based on their observable action essentially all previous work in plan recognition ha focused on the recognition process itself with no regard to the use of the information in the recognizing agent a a result low likelihood recognition hypothesis that may imply significant meaning to the observer are ignored in existing work in this paper we present novel efficient algorithm that allows the observer to incorporate her own bias and preference in the form of a utility function into the plan recognition process this allows choosing recognition hypothesis based on their expected utility to the observer we call this utility based plan recognition upr while reasoning about such expected utility is intractable in the general case we present a hybrid symbolic decision theoretic plan recognizer whose complexity is o n dt where n is the plan library size d is the depth of the library and t is the number of observation we demonstrate the efficacy of this approach with experimental result in several challenging recognition task 
cognitive modeling technique provide a way of evaluating user interface design based on what is known about human cognitive strength and limitation cognitive modeler face a tradeoff however more detailed model require disproportionately more time and effort to develop than coarser model in this paper we describe a system g a that automatically produce translation from abstract goms model into more detailed act r model g a demonstrates how even simple ai technique can facilitate the construction of cognitive model and suggests new direction for improving modeling tool 
approximate value iteration avi is an method for solving a markov decision problem by making successive call to a supervised learning sl algorithm sequence of value representation vn are processed iteratively by vn atvn where t is the bellman operator and a an approximation operator bound on the error between the performance of the policy induced by the algorithm and the optimal policy are given a a function of weighted lp norm p of the approximation error the result extend usual analysis in l norm and allow to relate the performance of avi to the approximation power usually expressed in lp norm for p or of the sl algorithm we illustrate the tightness of these bound on an optimal replacement problem 
my research attempt to address on line action selection in reinforcement learning from a bayesian perspective the idea is to develop more effective action selection technique by exploiting information in a bayesian posterior while also selecting action by growing an adaptive sparse lookahead tree i further augment the approach by considering a new value function approximation strategy for the belief state markov decision process induced by bayesian learning 
evaluating the performance of an agent or group of agent can be by itself a very challenging problem the stochastic nature of the environment plus the stochastic nature of agent decision can result in estimate with intractably large variance this paper examines the problem of finding low variance estimate of agent performance in particular we assume that some agent environment dynamic are known such a the random outcome of drawing a card or rolling a die other dynamic are unknown such a the reasoning of a human or other black box agent using the known dynamic we describe the complete set of all unbiased estimator that is for any possible unknown dynamic the estimate s expectation is always the agent s expected utility then given a belief abcut the unknown dynamic we identify the unbiased estimator with minimum variance if the belief is correct our estimate is optimal and if the belief is wrong it is at least unbiased finally we apply our unbiased estimator to the game of poker demonstrating dramatically reduced variance and faster evaluation 
we present a reinforcement learning game player that can interact with a general game playing system and transfer knowledge learned in one game to expedite learning in many other game we use the technique of value function transfer where general feature are extracted from the state space of a previous game and matched with the completely different state space of a new game to capture the underlying similarity of vastly disparate state space arising from different game we use a game tree lookahead structure for feature we show that such feature based value function transfer learns superior policy faster than a reinforcement learning agent that doe not use knowledge transfer furthermore knowledge transfer using lookahead feature can capture opponent specific value function i e can exploit an opponent s weakness to learn faster than a reinforcement learner that us lookahead with minimax pessimistic search against the same opponent 
in my previous paper ryan i introduced the concept of subgraph decomposition a a mean of reducing the search space in multi robot planning problem i showed how partitioning a roadmap into subgraphs of known structure allows u to first plan at a level of abstraction and then resolve these plan into concrete path without the need for further search so we can solve significantly harder planning task with the same resource however the subgraph type i introduced in that paper stack and clique are not likely to occur often in realistic planning problem and so are of limited usefulness in this paper i describe a new kind of subgraph called a hall which can also be used for planning and which occurs much more commonly in real problem i explain it formal property a a planning component and demonstrate it use on a map of the patrick s container yard at the port of brisbane in queensland australia 
we propose a learning framework that actively explores creation of face space s by selecting image that are complementary to the image already represented in the face space we also construct ensemble of classifier learned from such actively sampled image set which further provides improvement in the recognition rate we not only signicantly reduce the number of image required in the training set but also improve the accuracy over learning from all the image we also show that the single face space or ensemble of face space thus constructed ha a higher generalization performance across different illumination and expression condition 
decision theoretic planning with nonlinear utility function is important since decision maker are often risk sensitive in high stake planning situation one switch utility function are an important class of nonlinear utility function that can model decision maker whose decision change with their wealth level we study how to maximize the expected utility of a markov decision problem for a given one switch utility function which is difficult since the resulting planning problem is not decomposable we first study an approach that augments the state of the markov decision problem with the wealth level the property of the resulting infinite markov decision problem then allow u to generalize the standard risk neutral version of value iteration from manipulating value to manipulating function that map wealth level to value we use a probabilistic block world example to demonstrate that the resulting risk sensitive version of value iteration is practical 
technique for augmenting the automation of routine coordination are rapidly reaching a level of effectiveness where they can simulate realistic coordination on the ground for large number of emergency response entity e g fire engine police car for the sake of training furthermore it seems inevitable that future disaster response system will utilize such technology we have constructed a new system de facto demonstrating effective flexible agent coordination of team through omnipresence that integrates state of the art agent reasoning capability and d visualization into a unique high fidelity system for training incident commander the defacto system achieves this goal via three main component i omnipresent viewer intuitive interface ii proxy framework for team coordination and iii flexible interaction between the incident commander and the team we have performed detailed preliminary experiment with defacto in the fire fighting domain in addition defacto ha been repeatedly demonstrated to key police and fire department personnel in los angeles area with very positive feedback 
this paper considers online stochastic multiple vehicle routing with time window in which request arrive dynamically and the goal is to maximize the number of serviced customer contrary to earlier algorithm which only move vehicle to known customer this paper investigates waiting and relocation strategy in which vehicle may wait at their current location or relocate to arbitrary site experimental result show that waiting and relocation strategy may dramatically improve customer service especially for problem that are highly dynamic and contain many late request the decision to wait and to relocate do not exploit any problem specific feature but rather are obtained by including choice in the online algorithm that are necessarily sub optimal in an offline setting 
we present a novel formulation for providing advice to a reinforcement learner that employ support vector regression a it function approximator our new method extends a recent advice giving technique called knowledge based kernel regression kbkr that accepts advice concerning a single action of a reinforcement learner in kbkr user can say that in some set of state an action s value should be greater than some linear expression of the current state in our new technique which we call preference kbkr pref kbkr the user can provide advice in a more natural manner by recommending that some action is preferred over another in the specified set of state specifying preference essentially mean that user are giving advice about policy rather than q value which is a more natural way for human to present advice we present the motivation for preference advice and a proof of the correctness of our extension to kbkr in addition we show empirical result that our method can make effective use of advice on a novel reinforcement learning task based on the robocup simulator which we call breakaway our work demonstrates the significant potential of advice giving technique for addressing complex reinforcement learning problem while further demonstrating the use of support vector regression for reinforcement learning 
this paper deal with method exploiting tree decomposition approach for solving constraint network we consider here the practical efficiency of these approach by defining five class of variable order more and more dynamic which preserve the time complexity bound for that we define extension of this theoretical time complexity bound to increase the dynamic aspect of these order we define a constant k allowing u to extend the classical bound from o exp w firstly to o exp w k and finally to o exp w k s with w the tree width of a csp and sthe minimum size of it separator finally we ass the defined theoretical extension of the time complexity bound from a practical viewpoint 
control of election refers to attempt by an agent to via such action a addition deletion partition of candidate or voter ensure that a given candidate win bartholdi tovey trick an election system in which such an agent s computational task is np hard is said to be resistant to the given type of control aside from election system with an np hard winner problem the only system known to be resistant to all the standard control type are highly artificial election system created by hybridization hemaspaandra hemaspaandra rothe b in this paper we prove that an election system developed by the th century mystic ramon llull and the well studied copeland election system are both resistant to all the standard type of constructive electoral control other than one variant of addition of candidate this is the most comprehensive resistance to control yet achieved by any natural election system whose winner problem is in p in addition we show that llull and copeland voting are very broadly resistant to bribery attack and we integrate the potential irrationality of voter preference into many of our result 
essence is a new formal language for specifying combinatorial problem in a manner similar to natural rigorous specification that use a mixture of natural language and discrete mathematics essence provides a high level of abstraction much of which is the consequence of the provision of decision variable whose value can be combinatorial object such a tuples set multisets relation partition and function essence also allows these combinatorial object to be nested to arbitrary depth thus providing for example set of partition set of set of partition and so forth therefore a problem that requires finding a complex combinatorial object can be directly specified by using a decision variable whose type is precisely that combinatorial object 
we present an algorithm for organizing partially ordered observation into multiple thread some of which may be concurrent the algorithm is applied to the problem of constructing career history for individual scientist from the abstract of published paper because abstract generally do not provide rich information about the content of paper we developed a novel relational method for judging the similarity of paper we report four experiment that demonstrate the advantage of this method over the traditional dice and tanimoto coefficient and that evaluate the quality of induced multi thread career history 
we summarize the continuous thread of research we have conducted over the past thirty year on human computer collaboration this research reflects many of the theme and issue in operation in the greater field of ai over this period such a knowledge representation and reasoning planning and intent recognition learning and the interplay of human theory and computer engineering 
recent work on online auction for digital good ha explored the role of optimal stopping theory particularly secretary problem in the design of approximately optimal online mechanism this work generally assumes that the size of the market number of bidder is known a priori but that the mechanism designer ha no knowledge of the distribution of bid value however in many real world application such a online ticket sale the opposite is true the seller ha distributional knowledge of the bid value e g via the history of past transaction in the market but there is uncertainty about market size adopting the perspective of automated mechanism design introduced by conitzer and sandholm we develop algorithm that compute an optimal or approximately optimal online auction mechanism given access to this distributional knowledge our main result are twofold first we show that when the seller doe not know the market size no constant approximation to the optimum efficiency or revenue is achievable in the worst case even under the very strong assumption that bid value are i i d sample from a distribution known to the seller second we show that when the seller ha distributional knowledge of the market size a well a the bid value one can do well in several sens perhaps most interestingly by combining dynamic programming with prophet inequality a technique from optimal stopping theory we are able to design and analyze online mechanism which are temporally strategyproof even with respect to arrival and departure time and approximately efficiency revenue maximizing in exploring the interplay between automated mechanism design and prophet inequality we prove new prophet inequality motivated by the auction setting 
even under polynomial restriction on plan length conformant planning remains a very hard computational problem a plan verification itself can take exponential time this heavy price cannot be avoided in general although in many case conformant plan are verifiable efficiently by mean of simple form of disjunctive inference this raise the question of whether it is possible to identify and use such form of inference for developing an efficient but incomplete planner capable of solving non trivial problem quickly in this work we show that this is possible by mapping conformant into classical problem that are then solved by an off the shelf classical planner the formulation is sound a the classical plan obtained are all conformant but it is incomplete a the inverse relation doe not always hold the translation accommodates reasoning by case by mean of an split protect and merge strategy namely atom l xi that represent conditional belief if xi then l are introduced in the classical encoding that are combined by suitable action to yield the literal l when the disjunction x xn hold and certain invariant in the plan are verified empirical result over a wide variety of problem illustrate the power of the approach 
we present mboost a novel extension to adaboost that extends boosting to use multiple weak learner explicitly and provides robustness to learning model that overfit or are poorly matched to data we demonstrate mboost on a variety of problem and compare it to cross validation for model selection 
in this paper we investigate the relationship between two prioritized knowledge base by measuring both the conflict and the agreement between them first of all a quantity of conflict and two quantity of agreement are defined the former is shown to be a generalization of the dalal distance the latter are respectively a quantity of strong agreement which measure the amount of information on which two belief base totally agree and a quantity of weak agreement which measure the amount of information that is believed by one source but is unknown to the other all three quantity measure are based on the weighted prime implicant which represents belief in a prioritized belief base we then define a degree of conflict and two degree of agreement based on our quantity of conflict and the quantity of agreement we also consider the impact of these measure on belief merging and information source ordering 
task like bomb detection search and rescue and reconnaissance in near earth environment are time cost and labor intensive aerial robot could assist in such mission and offset the demand in resource and personnel however flying in environment rich with obstacle present many more challenge which have yet to be identified for example telephone wire is one obstacle that is known to be hard to detect in mid flight this paper describes how a blimp can be used in an aerial robot competition to identify other key challenge when flying in these cluttered environment 
we present a simple greedy algorithm and a novel complete algorithm for finding utilitarian optimal solution to simple temporal problem with preference unlike previous algorithm ours doe not restrict preference function to be convex we present experimental result showing that a single iteration of the greedy algorithm produce high quality solution multiple iteration bounded by the square of the number of constraint produce near optimal solution and our complete memory boundable algorithm ha compelling anytime property and outperforms a branch andbound algorithm 
the study of forgetting for reasoning ha attracted considerable attention in ai however much of the work on forgetting and other related approach such a independence irrelevance and novelty ha been restricted to the classical logic this paper describes a detailed theoretical investigation of the notion of forgetting in the context of logic programming we first provide a semantic definition of forgetting under the answer set for extended logic program we then discus the desirable property and some motivating example an important result of this study is an algorithm for computing the result of forgetting in a logic program furthermore we present a modified version of the algorithm and show that the time complexity of the new algorithm is polynomial with respect to the size of the given logic program if the size of certain rule is fixed we show how the proposed theory of forgetting can be used to characterize the logic program update 
we are developing a corpus based approach for the prediction of help desk response from feature in customer email where response are represented at two level of granularity document and sentence we present an automatic and human based evaluation of our system s response the automatic evaluation involves textual comparison between generated response and response composed by help desk operator our result showthat both level of granularity produce good response addressing inquiry of different kind the human based evaluation measure response informativeness and confirms our conclusion that both level of granularity produce useful response 
we investigate search problem in continuous state and action space with no uncertainty action have cost and can only be taken at discrete time step unlike the case with continuous control given an admissible heuristic function and a starting state the objective is to find a minimum cost plan that reach a goal state a the continuous domain doe not allow the tight optimality result that are possible in the discrete case for example by a we instead propose and analyze an approximate forward search algorithm that ha the following provable property given a desired accuracy and a bound d on the length of the plan the algorithm computes a lower bound l on the cost of any plan it either a return a plan of cost l that is at most more than the optimal plan or b if according to the heuristic estimate there may exist a plan of cost l of length d return a partial plan that trace the first d step of such plan to our knowledge this is the first algorithm that provides optimality guarantee in continuous domain with discrete control and without uncertainty 
we present a new formulation of distributed task assignment called generalized mutual assignment problem gmap which is derived from an np hard combinatorial optimization problem that ha been studied for many year in the operation research community to solve the gmap we introduce a novel distributed solution protocol using lagrangian decomposition and distributed constraint satisfaction where the agent solve their individual optimization problem and coordinate their locally optimized solution through a distributed constraint satisfaction technique next to produce quick agreement between the agent on a feasible solution with reasonably good quality we provide a parameter that control the range of noise mixed with an increment decrement in a lagrange multiplier our experimental result indicate that the parameter may allow u to control tradeoff between the quality of a solution and the cost of finding it 
planning in partially observable dynamical system is a challenging problem and recent development in point based technique such a perseus significantly improve performance a compared to exact technique in this paper we show how to apply these technique to new model for non markovian dynamical system called predictive state representatiolls psrs and memory psrs mpsrs psrs and mpsrs are model of non markovian decision process that differ from latent variable model e g hmms pomdps by representing state using only observable quantity further mpsrs explicitly represent certain structural property of the dynamical system that are also relevant to planning we show how planning technique can be adapted to leverage this structure to improve performance both in term of execution time a well a quality of the resulting policy 
rating how well a routine activity is performed can be valuable in a variety of domain making the rating inexpensive and credible is a key aspect of the problem we formalize the problem a map estimation in hmms where the incoming trace need repair we present polynomial time algorithm for computing minimal repair with maximal likelihood for hmms hidden semi markov model hsmms and a form of hmms constrained with a fragment of the temporal logic ltl we present some result to show the promise of our approach 
in this paper a study on the suitability of an appearance based model specifically pca based model for the purpose of recognising fingerspelling sign language alphabet is made it recognition performance on a large and varied real time dataset is analysed in order to enhance the performance of a pca based model we suggest to incorporate a sort of pre processing operation both during training and recognition an exhaustive experiment conducted on a large number of fingerspelling alphabet image taken from different individual in real environment ha revealed that the suggested pre processing ha a drastic impact in improving the performance of a conventional pca based model 
to achieve robust autonomy robot must avoid getting stuck in state from which they cannot recover without external aid while this is the role of the robot s control algorithm these are often imperfect we examine how to detect failure by observing the robot s internal sensor over time for such case triggering a response when detecting the onset of a failure can increase the operational range of the robot concretely we explore the use of supervised learning technique to create a classifier that can detect a potential failure and trigger a response for a dynamically balancing robot we present a fully implemented system where the result clearly demonstrate an improved safety margin for the robot 
in this paper we propose the dynamic weighting a dwa search algorithm for solving map problem in bayesian network by exploiting asymmetry in the distribution of map variable the algorithm is able to greatly reduce the search space and offer excellent performance both in term of accuracy and efficiency 
several researcher have shown that constraint can improve the result of a variety of clustering algorithm however there can be a large variation in this improvement even for a fixed number of constraint for a given data set we present the first attempt to provide insight into this phenomenon by characterizing two constraint set property informativeness and coherence we show that these measure can help explain why some constraint set are more beneficial to clustering algorithm than others since they can be computed prior to clustering these measure can aid in deciding which constraint to use in practice 
a redesign support framework for complex technical process is described in this paper this framework employ a multi model hierarchical representation of the process to be redesigned together with a case based reasoning engine that help u to decide the element of the process that should be modified this framework ha been tested in the chemical engineering domain 
we present a method for an autonomous agent to identify dominant market condition such a oversupply or scarcity the characteristic of economic regime are learned from historic data and used together with real time observable information to identify the current market regime and to forecast market change the approach is validated with data from the trading agent competition for supply chain management 
the formation of internet based social network ha revived research on traditional social network model a well a interest matching or match making system in order to automate or augment the process of interest matching we describe a method for the comparison of preference ordering represented by cp net which allows one to determine a shared interest level between agent empirical result suggest that this distance measure for preference ordering agrees with the intuitive assessment of shared interest level 
we propose a general framework for multi context reasoning which allows u to combine arbitrary monotonic and nonmonotonic logic nonmonotonic bridge rule are used to specify the information flow among context we investigate several notion of equilibrium representing acceptable belief state for our multi context system the approach generalizes the heterogeneous monotonic multi context system developed by f giunchiglia and colleague a well a the homogeneous nonmonotonic multi context system of brewka serafini and roelofsen 
approximate policy evaluation with linear function approximation is a commonly arising problem in reinforcement learning usually solved using temporal difference td algorithm in this paper we introduce a new variant of linear td learning called incremental least square td learning or ilstd this method is more data efficient than conventional td algorithm such a td and is more computationally efficient than non incremental least square td method such a lstd bradtke barto boyan in particular we show that the per time step complexity of ilstd and td are o n where n is the number of feature whereas that of lstd is o n this difference can be decisive in modern application of reinforcement learning where the use of a large number feature ha proven to be an effective solution strategy we present empirical comparison using the test problem introduced by boyan in which ilstd converges faster than td and almost a fast a lstd 
many real world problem require richer representation than are typically studied in planning and learning for example state estimation in complex system such a vehicle or spacecraft often requires a representation that capture the rich continuous behaviour of these kind of system similarly planning for such system may require a representation of continuous resource usage particularly when planning under uncertainty in this talk i will discus some commonly used representation of these system a hybrid system examine some approach to planning and state estimation in them and finally discus some first step toward learning a hybrid model or at least parameter of such a model directly from data these note are intended a a set of background information for an ijcai tutorial talk they are made up of piece of text taken from a variety of place without necessarily having any continuity 
we show that a horn sat and logic programming approach to obtain polynomial time algorithm for problem solving can be fruitfully applied to finding plan for various kind of goal in a non deterministic domain we particularly focus on finding weak strong and strong cyclic plan for planning problem a they are the most studied one in the literature we describe new algorithm for these problem and show how non monotonic logic programming can be used to declaratively compute strong cyclic plan a a further benefit preferred plan among alternative candidate plan may be singled out this way we give complexity result for weak strong and strong cyclic planning finally we briefly discus some of the kind of goal in non deterministic domain for which the approach in the paper can be used 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
this paper present an efficient algorithm for a new variation of the point feature labeling problem the goal is to position the largest number of point label such that they do not intersect each other or their point first we present an algorithm using a greedy algorithm with limited lookahead we then present an algorithm that iteratively regroups label calling the first algorithm on each group thereby identifying a close to optimal labeling order the presented algorithm is being used in a commercial product to label chart and our evaluation show that it produce result far superior to those of other labeling algorithm 
we consider the problem of synthesizing a fully controllable target behavior from a set of available partially controllable behavior that are to execute within a shared partially predictable but fully observable environment behavior are represented with a sort of nondeterministic transition system whose transition are conditioned on the current state of the environment also represented a a nondeterministic finite transition system on the other hand the target behavior is assumed to be fully deterministic and stand for the behavior that the system a a whole need to guarantee we formally define the problem within an abstract framework characterize it computational complexity and propose a solution by appealing to satisfiability in propositional dynamic logic which is indeed optimal with respect to computational complexity we claim that this problem while novel to the best of our knowledge can be instantiated to multiple specific setting in different context and can thus be linked to different research area of ai including agent oriented programming and cognitive robotics control multi agent coordination plan integration and automatic web service composition 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
we consider the problem of learning heuristic for controlling forward state space beam search in ai planning domain we draw on a recent framework for structured output classification e g syntactic parsing known a learning a search optimization laso the laso approach us discriminative learning to optimize heuristic function for search based computation of structured output and ha shown promising result in a number of domain however the search problem that arise in ai planning tend to be qualitatively very different from those considered in structured classification which raise a number of potential difficulty in directly applying laso to planning in this paper we discus these issue and describe a laso based approach for discriminative learning of beam search heuristic in ai planning domain we give convergence result for this approach and present experiment in several benchmark domain the result show that the discriminatively trained heuristic can outperform the one used by the planner ff and another recent non discriminative learning approach 
assuming human image classification decision are based on estimating the degree of match between a small number of stored internal template and certain region of the input image we present an algorithm which infers observer classification template from their classification decision on a set of test image the problem is formulated a learning prototype from labeled data under an adjustable prototype specific elliptical metric the matrix of the elliptical metric indicates the pixel that the template responds to the model wa applied to human psychophysical data collected in a simple image classification experiment 
we consider the problem of optimal planning in stochastic domain with resource constraint where resource are continuous and the choice of action at each step may depend on the current resource level our principal contribution is the hao algorithm a generalization of the ao algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variable the search algorithm leverage knowledge of the starting state to focus computational effort on the relevant part of the state space we claim that this approach is especially effective when resource limitation contribute to reachability constraint experimental result show it effectiveness in the domain that motivates our research automated planning for planetary exploration rover 
the need for being able to talk about mapping between different ontology ha been recognized a a result of the fact that different ontology may partially overlap or even represent the same domain from different point of view un like for the case of ontology language work on mapping language ha not yet reached a state where a common understanding of the basic principle exists in this paper we propose a formal comparison of existing mapping language by trans lating them into distributed rst order logic we analyze underlying assumption and difference in the interpretation of mapping 
prism is a logic based turing complete symbolic statistical modeling language with a built in parameter learning routine in this paper we enhance the modeling power of prism by allowing general prism program to fail in the generation process of observable event introducing failure extends the class of definable distribution but need a generalization of the semantics of prism program we propose a three valued probabilistic semantics and show how failure enables u to pursue constraint based modeling of complex statistical phenomenon 
we address the problem of learning discrete hidden markov model from very long sequence of observation incremental version of the baum welch algorithm that approximate the value used in the backward procedure are commonly used for this problem since their memory complexity is independent of the sequence length we introduce an improved incremental baum welch algorithm with a new backward procedure that approximates the value based on a one step lookahead in the training sequence we justify the new approach analytically and report empirical result that show it converges faster than previous incremental algorithm 
one of the well known risk of large margin training method such a boosting and support vector machine svms is their sensitivity to outlier these risk are normally mitigated by using a soft margin criterion such a hinge loss to reduce outlier sensitivity in this paper we present a more direct approach that explicitly incorporates outlier suppression in the training process in particular we show how outlier detection can be encoded in the large margin training principle of support vector machine by expressing a convex relaxation of the joint training problem a a semide finite program one can use this approach to robustly train a support vector machine while suppressing outlier we demonstrate that our approach can yield superior result to the standard soft margin approach in the presence of outlier 
this article fall under the problem of the symbolic monitoring of real time complex system or of video interpretation system among the various technique used for the on line monitoring we are interested here in the temporal scenario recognition in order to reduce the complexity of the recognition and consequently to improve it performance we explore two method the first one is the focus on particular event in practice uncommon one and the second one is the factorization of common temporal scenario in order to do a hierarchical recognition in this article we present both concept and merge them to propose a focused hierarchical recognition this approach merges and generalizes the two main approach in symbolic recognition of temporal scenario the store totally recognized scenario strs approach and the store partially recognized scenario sprs approach 
a major challenge in robotics and artificial intelligence lie in creating robot that are to cooperate with people in human populated environment e g for domestic assistance or elderly care such robot need skill that allow them to interact with the world and the human living and working therein in this paper we investigate the question of spatial understanding of human made environment the functionality of our system comprise perception of the world natural language learning and reasoning for this purpose we integrate state of the art component from different discipline in ai robotics and cognitive system into a mobile robot system the work focus on the description of the principle we used for the integration including cross modal integration ontology based mediation and multiple level of abstraction of perception finally we present experiment with the integrated cosy explorer system and list some of the major lesson that were learned from it design implementation and evaluation 
we show that the trace of an exhaustive dpll search can be viewed a a compilation of the propositional theory with different constraint imposed or lifted on the dpll algorithm this compilation will belong to the language of d dnnf fbdd and obdd respectively these language are decreasingly succinct yet increasingly tractable supporting such polynomial time query a model counting and equivalence testing our contribution is thus twofold first we provide a uniform framework supported by empirical evaluation for compiling knowledge into various language of interest second we show that given a particular variant of dpll by identifying the language membership of it trace one gain a fundamental understanding of the intrinsic complexity and computational power of the search algorithm itself a interesting example we unveil the hidden power of several recent model counter point to one of their potential limitation and identify a key limitation of dpll based procedure in general 
in this paper we address the problem of identifying and localizing multiple instance of highly deformable object in real time video data we present an approach which us pca sift scale invariant feature transform in combination with a clustered voting scheme to achieve detection and localization of multiple object while providing robustness against rapid shape deformation partial occlusion and perspective change we test our approach in two highly deformable robot domain and evaluate it performance using roc receiver operating characteristic statistic 
we present tractable exact algorithm for learning action effect and precondition in partially observable domain our algorithm maintain a propositional logical representation of the set of possible action model after each observation and action execution the algorithm perform exact learning of precondition and effect in any deterministic action domain this includes strip action and action with conditional effect in contrast previous algorithm rely on approximation to achieve tractability and do not supply approximation guarantee our algorithm take time and space that are polynomial in the number of domain feature and can maintain a representation that stay compact indefinitely our experimental result show that we can learn efficiently and practically in domain that contain over s of feature more than state 
the talking robot experiment inspried by the talking head experiment from sony explores possibility on how to ground symbol into perception using language with two autonomous aibo robot in an unconstained environment we present here the first result of this experiment and outline in the conclusion a planned extension to social behavior grounding 
for many distributed autonomous robotic system it is important to maintain communication connectivity among the robot that is each robot must be able to communicate with each other robot perhaps through a series of other robot ideally this property should be robust to the removal of any single robot from the system in ahmadi stone a we define a property of a team s communication graph that ensures this property called biconnectivity in that paper a distributed algorithm to check if a team of robot is biconnected and it correctness proof are also presented in this paper we provide distributed algorithm to add and remove robot to from a multi robot team while maintaining the biconnected property these two algorithm are implemented and tested in the player stage simulator 
in this paper we present the agent programming language teamgolog which is a novel approach to programming a team of cooperative agent under partial observability every agent is associated with a partial control program in golog which is completed by the teamgolog interpreter in an optimal way by assuming a decision theoretic semantics the approach is based on the key concept of a synchronization state and a communication state which allow the agent to passively resp actively coordinate their behavior while keeping their belief state observation and activity invisible to the other agent we show the usefulness of the approach in a rescue simulated domain 
this work present an iterative anytime heuristic search algorithm called anytime window a awa where node expansion is localized within a sliding window comprising of level of the search tree graph the search start in depth first mode and gradually proceeds towards a by incrementing the window size an analysis on a uniform tree model provides some very useful property of this algorithm a modification of awa is presented to guarantee bounded optimal solution at each iteration experimental result on the knapsack problem and tsp demonstrate the efficacy of the proposed technique over some existing anytime search method 
in this paper we propose quantminer a mining quantitative association rule system this system is based on a genetic algorithm that dynamically discovers good interval in association rule by optimizing both the support and the confidence the experiment on real and artificial database have shown the usefulness of quantminer a an interactive data mining tool 
query processing of owl dl ontology is intractable in the worst case but we present a novel technique that in practice allows for efficient querying of ontology with large aboxes in secondary storage we focus on the processing of instance retrieval query i e query that retrieve individual in the abox which are instance of a given concept c our technique us summarization and refinement to reduce instance retrieval to a small relevant subset of the original abox we demonstrate the effectiveness of this technique in aboxes with up to million assertion our result are applicable to the very expressive description logic shin which corresponds to owl dl minus nominal and datatypes 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
many intelligent tutoring system it have been developed deployed assessed and proven to facilitate learning however most of these system do not generally adapt to new circumstance do not self evaluate and self configure their own strategy and do not monitor the usage history of the learning content being delivered or presented to the student these shortcoming force it developer to often spend much development time in manual revision and fine tuning of the learning and instructional content of an it in this paper we describe an intelligent agent that delivers learning material adaptively to different student factoring in the usage history of the learning material and student profile a observed by the agent student tutor interaction includes the activity of going through learning material such a a topical tutorial a set of example and a set of problem our assumption is that our agent will be able to capture and utilize these student activity a the primer to select the appropriate example or problem to administer to the student using an integrated introspective case based reasoning approach our agent further learns from it experience and refines it reasoning process including the instructional strategy to adapt to student need moreover our agent monitor the usage history of the learning material to improve it performance we have built an end to end it using an agent powered by this integrated introspective case based reasoning engine we have deployed the it in a c course result indicate that the it wa able to learn to deliver more appropriate example and problem to the student 
distance measure like the euclidean distance have been the most widely used to measure similarity between feature vector in the content based image retrieval cbir system however in these similarity measure no assumption is made about the probability distribution and the local relevance of the feature vector therefore irrelevant feature might hurt retrieval performance probabilistic approach have proven to be an effective solution to this cbir problem in this paper we use a bayesian logistic regression model in order to compute the weight of a pseudo metric to improve it discriminatory capacity and then to increase image retrieval accuracy the pseudo metric weight were adjusted by the classical logistic regression model in ksantini et al the bayesian logistic regression model wa shown to be a significantly better tool than the classical logistic regression one to improve the retrieval performance the retrieval method is fast and is based on feature selection experimental result are reported on the zubud and wang color image database proposed by deselaers et al 
we introduce a new type of combinatorial auction that allows agent to bid for good to buy for good to sell and for transformation of good one such transformation can be seen a a step in a production process so solving the auction requires choosing the sequence in which the accepted bid should be implemented we introduce a bidding language for this type of auction and analyse the corresponding winner determination problem 
previous work in knowledge transfer in machine learning ha been restricted to task in a single domain however evidence from psychology and neuroscience suggests that human are capable of transferring knowledge across domain we present here a novel learning method based on neuroevolution for transferring knowledge across domain we use many layered sparsely connected neural network in order to learn a structural representation of task then we mine frequent sub graph in order to discover sub network that are useful for multiple task these sub network are then used a primitive for speeding up the learning of subsequent related task which may be in different domain 
testing embedded software system on the control unit of vehicle is a safety relevant task and developing the test suite for performing the test on test bench is time consuming we present the foundation and result of a case study to automate the generation of test for control software of vehicle control unit based on a specification of requirement in term of finite state machine this case study build upon our previous work on generation of test for physical system based on relational behavior model in order to apply the respective algorithm the finite state machine representation is transformed into a relational model we present the transformation the application of the test generation algorithm to a real example and discus the result and some specific challenge regarding software testing 
in this paper we present an approach to representing and managing temporally flexible behavior in the situation calculus based on a model of time and concurrent situation we define a new hybrid framework combining temporal constraint reasoning and reasoning about action we show that the constraint based interval planning approach can be imported into the situation calculus by defining a temporal and concurrent extension of the basic action theory finally we provide a version of the golog interpreter suitable for managing flexible plan on multiple timeline 
we describe how to improve the performance of mdp planning algorithm by modifying them to use the search control mechanism of planner such a tlplan shop and talplanner in our experiment modified version of rtdp lrtdp and value iteration were exponentially faster than the original algorithm on the largest problem the original algorithm could solve the modified one were about time faster on another set of problem whose state space were more than time larger than the original algorithm could solve the modified algorithm took only about second 
the quality of text correction system can be improved when using complex language model and by taking peculiarity of the garbled input text into account we report on a series of experiment where we crawl domain dependent web corpus for a given garbled input text from crawled corpus we derive dictionary and language model which are used to correct the input text we show that correction accuracy is improved when integrating word bigram frequency value from the crawl a a new score into a baseline correction strategy based on word similarity and word unigram frequency in a second series of experiment we compare the quality of distinct language model measuring how closely these model reect the frequency observed in a given input text it is shown that crawled language model are superior to language model obtained from standard corpus 
a prerequisite to efficient behavior by a multi robot team is the ability to accurately perceive the environment in this paper we present an approach to deal with sensing uncertainty at the coordination level specifically robot attach information regarding feature that caused the initiation of a course of action to any coordination message for that activity further information regarding such feature acquired by the team are then combined and the expected utility of the started action is re evaluated accordingly experiment show that the approach allows to coordinate a large group of robot addressing sensing uncertainty in a tractable way 
automatic document summarization is a problem of creating a document surrogate that adequately represents the full document content we aim at a summarization system that can replicate the quality of summary created by human in this paper we investigate the machine learning method for extracting full sentence from document based on the document semantic graph structure in particular we explore how the support vector machine svm learning method is affected by the quality of linguistic analysis and the corresponding semantic graph representation we apply two type of linguistic analysis a simple part of speech tagging of noun phrase and verb and full logical form analysis which identifies subject predicate object triple and then build the semantic graph we train the svm classifier to identify summary node and use these node to extract sentence experiment with the duc and cast datasets show that the svm based extraction of sentence doe not differ significantly for the simple and the sophisticated syntactic analysis in both case the graph attribute used in learning are essential for the classifier performance and the quality of extracted summary 
we investigate search problem under risk in state space graph with the aim of finding optimal path for risk averse agent we consider problem where uncertainty is due to the existence of different scenario of known probability with different impact on cost of solution path we consider various non linear decision criterion eu rdu yaari to express risk averse preference then we provide a general optimization procedure for such criterion based on a path ranking algorithm applied on a scalarized valuation of the graph we also consider partial preference model like second order stochastic dominance ssd and propose a multiobjective search algorithm to determine ssd optimal path finally the numerical performance of our algorithm are presented and discussed 
in this paper we argue that the agglomerative clustering with vector cosine similarity measure performs poorly due to two reason first the nearest neighbor of a document belong to different class in many case since any pair of document share lot of general word second the sparsity of class specific core word lead to grouping document with the same class label into different cluster both problem can be resolved by suitable smoothing of document model and using kullback leibler divergence of two smoothed model a pairwise document distance inspired by the recent work in information retrieval we propose a novel context sensitive semantic smoothing method that can automatically identifies multiword phrase in a document and then statistically map phrase to individual document term we evaluate the new model based similarity measure on three datasets using complete linkage criterion for agglomerative clustering and find out it significantly improves the clustering quality over the traditional vector cosine measure 
we present a novel text to picture system that synthesizes a picture from general unrestricted natural language text the process is analogous to text to speech synthesis but with pictorial output that conveys the gist of the text our system integrates multiple ai component including natural language processing computer vision computer graphic and machine learning we present an integration framework that combine these component by first identifying infonnative and picturable text unit then searching for the most likely image part conditioned on the text and finally optimizing the picture layout conditioned on both the text and image part the effectiveness of our system is assessed in two user study using child s book and news article experiment show that the synthesized picture convey a much infonnation about child s story a the original artist illustration and much more information about news article than their original photo alone these result suggest that text to picture synthesis ha great potential in augmenting human computer and human human communication modality with application in education and health care among others 
while it is commonly agreed that analogy is useful in human problem solving exactly how analogy can and should be used remains an intriguing problem vanlehn for instance argues that there are difference in how novice and expert use analogy but the vanlehn and jones cascade model doe not implement these difference this paper analyzes several variation in strategy for using analogy to explore possible source of novice expert difference we describe a series of ablation experiment on an expert model to examine the effect of strategy variation in using analogy in problem solving we provide evidence that failing to use qualitative reasoning when encoding problem being careless in validating analogical inference and not using multiple retrieval can degrade the efficiency of problem solving 
semantic entailment is the problem of determining if the meaning of a given sentence entail that of another this is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and ha a large number of application this paper present a principled approach to this problem that build on inducing representation of text snippet into a hierarchical knowledge representation along with a sound optimization based inferential mechanism that make use of it to decide semantic entailment a preliminary evaluation on the pascal text collection is presented 
the author present twig a visually grounded word learning system that us it existing knowledge of vocabulary grammar and action schema to help it learn the meaning of new word from it environment most system built to learn word meaning from sensory data focus on the base case of learning word when the robot know nothing and do not incorporate grammatical knowledge to aid the process of inferring meaning the present study show how using existing language knowledge can aid the word learning process in three way first partial par of sentence can focus the robot s attention on the correct item or relation in the environment second grammatical inference can suggest whether a new word refers to a unary or binary relation third the robot s existing predicate schema can suggest possibility for a new predicate the author demonstrate that twig can use it understanding of the phrase got the ball while watching a game of catch to learn that i refers to the speaker you refers to the addressee and the name refer to particular people the robot then us these new word to learn that am and are refer to the identity relation 
the research area of plan recognition and natural language parsing share many common feature and even algorithm however the dialog between these two discipline ha not been effective specifically significant recent result in parsing mildly context sensitive grammar have not been leveraged in the state of the art plan recognition system this paper will outline the relation between natural language processing nlp and plan recognition pr argue that each of them can effectively inform the other and then focus on key recent research result in nlp and argue for their applicability to pr 
search based algorithm like planner scheduler and satisfiability solver are notorious for having numerous parameter with a wide choice of value that can affect their performance drastically a a result the user of these algorithm who may not be search expert spend a significant time in tuning the value of the parameter to get acceptable performance on their particular problem domain in this paper we present a learning based approach for automatic tuning of search based algorithm to help such user the benefit of our methodology is that it handle diverse parameter type performs effectively for a broad range of systematic a well a non systematic search based solver the selected parameter could make the algorithm solve up to problem while the bad parameter would lead to none being solved incorporates user specified performance criterion and is easy to implement moreover the selected parameter will satisfy in the first try or the ranked candidate can be used along with to minimize the number of time the parameter setting need to he adjusted until a problem is solved 
a the complexity of narrative based virtual environment grows the need for effective communication of information to the user of these system increase effective camera control for narrative oriented virtual world involves decision making at three different level choosing cinematic geometric composition choosing the best camera parameter for conveying affective information and choosing camera shot and transition to maintain thetorical coherence we propose a camera planning system that mirror the film production pipeline we describe our formalization of film idiom used to communicate affective information our representation of idiom capture their hierarchical nature represents the causal motivation for selection of shot and provides a way for the system designer to specify the ranking of candidate shot sequence 
the automated planning community ha traditionally focused on the efficient synthesis of plan given a complete domain theory in the past several year this line of work met with significant success and the future course of the community seems to be set on efficient planning with even richer model while this line of research ha it application there are also many domain and scenario where the first bottleneck is getting the domain model at any level of completeness in these scenario the modeling burden automatically render the planning technology unusable to counter this i will motivate model lite planning technology aimed at reducing the domain modeling burden possibly at the expense of reduced functionality and outline the research challenge that need to be addressed to realize it 
this paper proposes and develops a new graph based semi supervised learning method different from previous graph based method that are based on discriminative model our method is essentially a generative model in that the class conditional probability are estimated by graph propagation and the class prior are estimated by linear regression experimental result on various datasets show that the proposed method is superior to existing graph based semi supervised learning method especially when the labeled subset alone prof insufficient to estimate meaningful class prior 
analysis of postgenomic biological data such a microarray and snp data is a subtle art and science and the statistical method most commonly utilized sometimes prove inadequate machine learning technique can provide superior understanding in many 
the paper present a pair of new conformant planner cpapc and cpaph based on recent development in theory of action and change a an input the planner take a domain description d in action language al which allows state constraint non stratified axiom together with a set of cnf formula describing the initial state and a set of literal representing the goal we propose two approximation of the transition diagram t defined by d both approximation are deterministic transition function and can be computed efficiently moreover they are sound and sometimes complete with respect to t in it search for a plan an approximation based planner analysis path of an approximation instead of that of t cpapc and cpaph are forward best first search planner based on this idea we compare them with two state of the art conformant planner kacmbp and conformant ff cff over benchmark in the literature and over two new domain one ha large number of state constraint and another ha a high degree of incompleteness our planner perform reasonably well in benchmark domain and outperform kacmbp and cff in the first domain while still working well with the second one our experimental result show that having an integral part of a conformant planner to deal with state constraint directly can significantly improve it performance extending a similar claim for classical planner in thiebaux hoffmann nebel 
autonomic self managing computing system face the critical problem of resource allocation to different computing element adopting a recent model we view the problem of provisioning resource a involving utility elicitation and optimization to allocate resource given imprecise utility information in this paper we propose a new algorithm for regret based optimization that performs significantly faster than that proposed in earlier work we also explore new regret based elicitation heuristic that are able to find near optimal allocation while requiring a very small amount of utility information from the distributed computing element since regret computation is intensive we compare these to the more tractable nelder mead optimization technique w r t amount of utility information required 
recently social text stream e g blog web forum and email have become ubiquitous with the evolution of the web in some sense social text stream are sensor of the real world often it is desirable to extract real world event from the social text stream however existing event detection research mainly focused only on the stream property of social text stream but ignored the contextual temporal and social information embedded in the stream in this paper we propose to detect event from social text stream by exploring the content a well a the temporal and social dimension we define the term event a the information flow between a group of social actor on a specific topic over a certain time period we represent social text stream a multi graph where each node represents a social actor and each edge represents the information flow between two actor the content and temporal association within the flow of information are embedded in the corresponding edge event are detected by combining text based clustering temporal segmentation and information flow based graph cut of the dual graph of the social network experiment conducted with the enron email dataset and the political blog dataset from dailykos show the proposed event detection approach outperforms the other alternative 
the profile of an individual is a record of the type and area of skill of that individual topical profile plus a description of her collaboration network social profile in this paper we define and formalize the task of automatically determining an expert profile of a person from a heterogeneous corpus made up of a large organization s intranet we propose multiple model for addressing the topical profiling task our main method build on idea from information retrieval while refinement bring in filtering allowing an area into a person s profile only if she is among the top ranking expert in the area an evaluation based on the w c corpus made available by trec show significant improvement of the refined method over the baseline we apply our profiling algorithm to significantly enhance the performance of a state ofthe art expert finding algorithm and to help user of an operational expert search system find the person they would contact given a specific problem topic or information need finally we address the task of determining a social profile for a given person using graph based method 
we look at the problem in belief revision of trying to make inference about what an agent believed or will believe at a given moment based on an observation of how the agent ha responded to some sequence of previous belief revision input over time we adopt a reverse engineering approach to this problem assuming a framework for iterated belief revision which is based on sequence we construct a model of the agent that best explains the observation further consideration on this best explaining model then allow inference about the agent s epistemic behaviour to be made we also provide an algorithm which computes this best explanation 
a drawback of traditional default logic is that there is no general mechanism for preferring one default rule over another to remedy this problem numerous default logic augmented with priority relation have been introduced in this paper we show how trust value derived from web based social network can be used to prioritize default we provide a coupling between the method for computing trust value in social network and the prioritized reiter default of baader hollunder where specificity of terminological concept is used to prioritize default we compare our approach with specificity based prioritization and discus how the two can be combined finally we show how our approach can be applied to other variant of prioritized default logic 
we propose two new online method for estimating the size of a backtracking search tree the first method is based on a weighted sample of the branch visited by chronological backtracking the second is a recursive method based on assuming that the 
in this paper we will provide a fast polynomialtime algorithm for solving simple temporal problem stp with piecewise linear convex preference function and a utilitarian objective function our algorithm is motivated by the study of the linear programming lp dual of a given mincost circulation problem mccp we will also show how this duality relationship between simple temporal problem with preference stpps and mccps lead to fast incremental algorithm for solving the former our algorithm bear important implication in planning scheduling and execution monitoring scenario where partial plan are subject to repeated change and the most preferred solution to the underlying stpps have to be computed and updated fast incrementally 
in a constraint optimization problem cop many feasible valuation lead to the same objective value this often mean a huge search space and poor performance in the propagation between the objective and problem variable in this paper we propose a different modeling and search strategy which focus on the cost function we show that by constructing a dual model on the objective variable we can get strong propagalion between the objective variable and the problem variable which allows search on the objective variable we explain why and when searching on the objective variable can lead to large gain we present a new russian doll search algorithm ords which work on objective variable with dynamic variable ordering finally we demonstrate using the hard still life optimization problem the benefit of changing to the objective function model and ords 
planning under uncertainty ha been well studied but usually the uncertainty is in action outcome this work instead investigates uncertainty in the amount of time that action require to execute in addition to this temporal uncertainty the problem being studied must have robust solution plan that are optimized based on an objective function this thesis summary detail two iterative approach that have been used to solve these type of problem and discus future work including mdp approach 
current approach to solving markov decision process mdps are sensitive to the size of the mdp when applied to real world problem though mdps exhibit considerable implicit redundancy especially in the form of symmetry existing model minimization method do not exploit this redundancy due to symmetry well in this work given such symmetry we present a time efficient algorithm to construct a functionally equivalent reduced model of the mdp further we present a real time dynamic programming rtdp algorithm which obviates an explicit construction of the reduced model by integrating the given symmetry into it the rtdp algorithm solves the reduced model while working with parameter of the original model and the given symmetry a rtdp us it experience to determine which state to backup it focus on part of the reduced state set that are most relevant this result in significantly faster learning and a reduced overall execution time the algorithm proposed are particularly effective in the case of structured automorphisms even when the reduced model doe not have fewer feature we demonstrate the result empirically on several domain 
we have developed a set of microplanning choice rule which are intended to enable natural language generation nlg system to generate appropriate text for reader with below average literacy focusing in particular on choice related to how discourse structure is expressed cue phrase ordering sentence structure evaluation experiment suggest that our rule do enhance the readability of text for low skilled reader although there is still room for improvement 
the problem of efficiently finding similar item in a large corpus of high dimensional data point arises in many real world task such a music image and video retrieval beyond the scaling difficulty that arise with lookup in large data set the complexity in these domain is exacerbated by an imprecise definition of similarity in this paper we describe a method to learn a similarity function from only weakly labeled positive example once learned this similarity function is used a the basis of a hash function to severely constrain the number of point considered for each lookup tested on a large real world audio dataset only a tiny fraction of the point are ever considered for each lookup to increase efficiency no comparison in the original high dimensional space of point are required the performance far surpasses in term of both efficiency and accuracy a state of the art locality sensitive hashing based technique for the same problem and data set 
if neuron are treated a latent variable our visual system are non linear densely connected graphical model containing billion of variable and thousand of billion of parameter current algorithm would have difficulty learning a graphical model of this scale starting with an algorithm that ha difficulty learning more than a few thousand parameter i describe a series of progressively better learning algorithm all of which are designed to run on neuron like hardware the latest member of this series can learn deep multi layer belief net quite rapidly it turn a generic network with three hidden layer and million connection into a very good generative model of handwritten digit after learning the model give classification performance that is comparable to the best discriminative method 
several recent approach for processing graphical model constraint and bayesian network simultaneously exploit graph decomposition and local consistency enforcing graph decomposition exploit the problem structure and offer space and time complexity bound while hard information propagation provides practical improvement of space and time behavior inside these theoretical bound concurrently the extension of local consistency to weighted constraint network ha led to important improvement in branch and bound based solver indeed soft local consistency give incrementally computed strong lower bound providing inexpensive yet powerful pruning and better informed heuristic in this paper we consider combination of tree decomposition based approach and soft local consistency enforcing for solving weighted constraint problem the intricacy of weighted information processing lead to different approach with different theoretical property it appears that the most promising combination sacrifice a bit of theory for improved practical efficiency 
the ksu willie entry in the semantic vision challenge will use a variety of classifier some standard classifier and some newly developed classifier to learn the classification of image downloaded from the web ksu willie will use those classifier to identify object in the environment 
we present an algorithm for learning a model of the effect of action in noisy stochastic world we consider learning in a d simulated block world with realistic physic to model this world we develop a planning representation with explicit mechanism for expressing object reference and noise we then present a learning algorithm that can create rule while also learning derived predicate and evaluate this algorithm in the block world simulator demonstrating that we can learn rule that effectively model the world dynamic 
in the development of discipline addressing dynamic a major role wa played by the assumption that process can be modelled by introducing state property called potentiality anticipating in which respect a next state will be different a second assumption often made is that these state property can be related to other state property called reducer the current paper proposes a philosophical framework in term of potentiality and their reducer to obtain a common philosophical foundation for method in ai and cognitive science to model dynamic this framework provides a unified philosophical foundation for numerical symbolic and hybrid approach 
in recent year the combinatorics of argumentation with argument that can attack each other ha been studied extensively especially attack graph put in the focus of attention by dung s seminal work have proven to be a productive tool of analysis in this paper a new style of algorithm is presented that computes the minimal admissible set containing or attacking the argument it is a breadth first algorithm using labelings the algorithm is applied to the computation of the preferred and stable extension of a given attack graph 
this paper describes aspogamo a visual tracking system that determines the coordinate and trajectory of football player in camera view based on tv broadcast to do so aspogamo solves a complex probabilistic estimation problem that consists of three subproblems that interact in subtle way the estimation of the camera direction and zoom factor the tracking and smoothing of player route and the disambiguation of tracked player after occlusion the paper concentrate on system aspect that make it suitable for operating under unconstrained condition and in almost realtime we report on result obtained in a public demonstration at robocup where we conducted extensive experiment with real data from live coverage of world cup game in germany 
in many practical machine learning and data mining application unlabeled training example are readily available but labeled one are fairly expensive to obtain therefore semi supervised learning algorithm such a co training have attracted much attention previous research mainly focus on semi supervised classification in this paper a co training style semi supervised regression algorithm i e coreg is proposed this algorithm us two k nearest neighbor regressors with different distance metric each of which label the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled example on the labeled one experiment show that coreg can effectively exploit unlabeled data to improve regression estimate 
transfer learning concern applying knowledge learned in one task the source to improve learning another related task the target in this paper we use structure mapping a psychological and computational theory about analogy making to find mapping between the source and target task and thus construct the transfer functional automatically our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and us heuristic search to find the best maximal mapping the algorithm take a input the source and target task specification represented a qualitative dynamic bayes network which do not need probability information we apply this method to the keepaway task from robocup simulated soccer and compare the result from automated transfer to that from handcoded transfer 
active information fusion is to selectively choose the sensor so that the information gain can compensate the cost spent in information gathering however determining the most informative and cost effective sensor requires an evaluation of all possible sensor combination which is computationally intractable particularly when information theoretic criterion is used this paper present a methodology to actively select a sensor subset with the best tradeoff between information gain and sensor cost by exploiting the synergy among sensor our approach includes two aspect a method for efficient mutual information computation and a graph theoretic approach to reduce search space the approach can reduce the time complexity significantly in searching for a near optimal sensor subset 
many mdps exhibit an hierarchical structure where the agent need to perform various subtasks that are coupled only by a small sub set of variable containing notably shared resource previous work ha shown how this hierarchical structure can be exploited by solving several sub mdps representing the different subtasks in different calling context and a root mdp responsible for sequencing and synchronizing the subtasks instead of a huge mdp representing the whole problem another important idea used by efficient algorithm for solving flat mdps such a l ao and l rtdp is to exploit reachability information and an admissible heuristic in order to accelerate the search by pruning state that cannot be reached from a given starting state under an optimal policy in this paper we combine both idea and develop a variant of the ao algorithm for performing forward heuristic search in hierarchical model this algorithm show great performance improvement over hierarchical approach using standard mdp solver such a value iteration a well a with respect to ao applied to a flat representation of the problem moreover it present a general new method for accelerating ao and other forward search algorithm substantial performance gain may be obtained in these algorithm by partitioning the set of search node and solving a subset of node completely before propagating the result to other subset 
nsf and nasa sponsored a workshop to discus harvesting solar power in space one solution considered wa the use of a swarm of robot to form a solar reflector how can these robot organize to form a large parabolic structure and be effectively controlled the approach of this project is to treat the formation a a lattice of cell each cell is in one of a given state governed by a set of mles a command that indicates the geometric formation is sent to a seed robot the formation would then transform a neighbor attain their calculated relationship based on the formation definition 
computational model of grounding are extended to include representation of degree of groundedness these representation are then used for decision making in dialogue management for spoken dialogue system several domain will be explored with this model and an implementation will be tested and evaluated 
this paper report a statistical identification technique that differentiates script and language in degraded and distorted document image we identify script and language through document vectorization which transforms each document image into an electronic document vector that characterizes the shape and frequency of the contained character and word image we first identify script based on the density and distribution of vertical run between character stroke and a vertical scan line latin based language are then differentiated using a set of word shape code constructed using horizontal word run and character extremum point experimental result show that our method is tolerant to noise document degradation and slight document skew and attains an average identification rate over 
this paper introduces a multidimensional semantic framework for adaptive system different plane allow u to represent ontology of user her action context device domain while the intersection between plane allow u to represent the semantic rule for inferring new user feature or adaptation strategy the adoption of ontology based framework aim at creating a server for user modeling and adaptation strategy 
kernel discriminant analysis kda is one of the most effective nonlinear technique for dimensionality reduction and feature extraction it can be applied to a wide range of application involving highdimensional data including image gene expression and text data this paper develops a new algorithm to further improve the overall performance of kda by effectively integrating the boosting and kda technique the proposed method called boosting kernel discriminant analysis bkda posse several appealing property first like all kernel method it handle nonlinearity in a disciplined manner that is also computationally attractive second by introducing pairwise class discriminant information into the discriminant criterion and simultaneously employing boosting to robustly adjust the information it further improves the classification accuracy third by calculating the significant discriminant information in the null space of the within class scatter operator it also effectively deal with the small sample size problem which is widely encountered in real world application for kda fourth by taking advantage of the boosting and kda technique it constitutes a strong ensemblebased kda framework experimental result on gene expression data demonstrate the promising performance of the proposed methodology 
we introduce a novel algorithm toqr for relaxing failed query over database that is over constrained dnf query that return an empty result toqr us a small dataset to discover the implicit relationship among the domain attribute and then it exploit this domain knowledge to relax the failed query toqr start with a relaxed query that doe not include any constraint and it try to add to it a many a possible of the original constraint or their relaxation the order in which the constraint are added is derived from the domain s causal structure which is learned by applying the tan algorithm to the small training dataset our experiment show that toqr clearly outperforms other approach even when trained on a handful of example it successfully relaxes more that of the failed query furthermore toqr s relaxed query are highly similar to the original failed query 
this paper summarizes our research in the area of semantic tagging at the word and sense level and set the ground for a new approach to text level sentiment annotation using a combination of machine learning and linguisticallymotivated technique we describe a system for sentiment tagging of word and sens based on wordnet gloss and advance the treatment of sentiment a a fuzzy category 
in a recent paper we presented a new logic called e for reasoning about the knowledge action and perception of an agent although formulated using modal operator we argued that the language wa in fact a dialect of the situation calculus but with the situation term suppressed this allowed u to develop a clean and workable semantics for the language without piggybacking on the generic tarski semantics for first order logic in this paper we reconsider the relation between e and the situation calculus and show how to map sentence of e into the situation calculus we argue that the fragment of the situation calculus represented by e is rich enough to handle the basic action theory defined by reiter a well a golog finally we show that in the full second order version of e almost all of the situation calculus can be accommodated 
although there ha been significant previous work on semi supervised learning for classification there ha been relatively little in sequence modeling this paper present an approach that leverage recent work in manifold learning on sequence to discover word cluster from language data including both syntactic class and semantic topic from unlabeled data we form a smooth low dimensional feature space where each word token is projected based on it underlying role a a function or content word we then use this projection a additional input feature to a linear chain conditional random field trained on limited labeled training data on standard part of speech tagging and chinese word segmentation data set we show a much a error reduction due to the unlabeled data and also statistically significant improvement over a related semi supervised sequence tagging method due to miller et al 
reinforcement learning ha had spectacular success over the last several decade while meant to require le human input than supervised learning reinforcement learning can be substantially accelerated with a priori available domain expertise the way of providing human knowledge to a reinforcement learning agent vary from crafting state feature to initial policy design to initial value function design we chose the latter and propose a novel approach for acquiring a high quality initial value function via apprenticeship learning this approach work well in domain when a body of expert data are available our apprentice reinforcement learning arl agent us dynamic programming to compute value for the state visited by the expert a laplacian regularizer is then engaged to extrapolate these onto the entire state space the result of this process is a high quality initial value function to be further refined by any value function based reinforcement learning method in a grid world domain arl wa able to speed up td learning method by a factor of two from a single observed expert s trace 
possibilistic logic offer a convenient tool for handling uncertain or prioritized formula and coping with inconsistency propositional logic formula are thus associated with weight belonging to a linearly ordered scale however especially in case of multiple source information only partial knowledge may be available about the relative ordering between weight of formula in order to cope with this problem a two sorted counterpart of possibilistic logic is introduced piece of information are encoded a clause where special literal refer to the weight constraint between weight translate into logical formula of the corresponding sort and are gathered in a distinct auxiliary knowledge base an inference relation which is sound and complete with respect to preferential model semantics enables u to draw plausible conclusion from the two knowledge base the inference process is characterized by using forgetting variable for handling the symbolic weight and hence an inference process is obtained by mean of a dnf compilation of the two knowledge base 
coalition formation is an important capability of automated negotiation among self interested agent in order for coalition to be stable a key question that must be answered is how the gain from cooperation are to be distributed recent research ha revealed that traditional solution concept such a the shapley value core least core and nucleolus are vulnerable to various manipulation in open anonymous environment such a the internet these manipulation include submitting false name collusion and hiding some skill to address this a solution concept called the anonymity proof core which is robust against such manipulation wa developed however the representation size of the outcome function in the anonymity proof core and similar concept requires space exponential in the number of agent skill this paper proposes a compact representation of the outcome function given that the characteristic function is represented using a recently introduced compact language that explicitly specifies only coalition that introduce synergy this compact representation scheme can successfully express the outcome function in the anonymity proof core furthermore this paper develops a new solution concept the anonymity proof nucleolus that is also expressible in this compact representation we show that the anonymity proof nucleolus always exists is unique and is in the anonymity proof core if the latter is nonempty and assigns the same value to symmetric skill 
this paper proposes a novel approach to discover simultaneous time differential law equation having high plausibility to represent first principle underlying objective process the approach ha the power to identify law equation containing hidden state variable and or representing chaotic dynamic without using any detailed domain knowledge 
researcher and practitioner from both the artificial intelligence and pervasive computing community have been paying increasing attention to the task of inferring user high level goal from low level sensor reading a common assumption made by most approach is that a user either ha a single goal in mind or achieves several goal sequentially however in real world environment a user often ha multiple goal that are concurrently carried out and a single action can serve a a common step towards multiple goal in this paper we formulate the multiple goal recognition problem and exemplify it in an indoor environment where an rf based wireless network is available we propose a goal recognition algorithm based on a dynamic model set and show how goal model evolve over time based on pre defined state experiment with real data demonstrate that our method can accurately and efficiently recognize multiple interleaving goal in a user s trace 
the use of ontology in various application domain such a data integration the semantic web or ontology based data management where ontology provide the access to large amount of data is posing challenging requirement w r t a trade off between expressive power of a dl and efficiency of reasoning the logic of the dl lite family were specifically designed to meet such requirement and optimized w r t the data complexity of answering complex type of query in this paper we propose dl litebool an extension of dl lite with full booleans and number restriction and study the complexity of reasoning in dl litebool and it significant sub logic we obtain our result together with useful insight into the property of the studied logic by a novel reduction to the one variable fragment of first order logic we study the computational complexity of satisfiability and subsumption and the data complexity of answering positive existential query which extend union of conjunctive query notably we extend the logspace upper bound for the data complexity of answering union of conjunctive query in dl lite to positive query and to the possibility of expressing also number restriction and hence local functionality in the tbox 
to accomplish a household task an autonomous system need a plan with step it is desirable to derive this plan dynamically instead of pre coding it in the system in this paper we find a plan by using common sense knowledge collected from volunteer over the web through distributed knowledge capture technique this knowledge consists of step for executing common household task we first pre process the data with part of speech po tagging to identify the action and object in the step in all available plan for the task we then determine the order of the step to accomplish the task using discriminative a well a generative model for the discriminative approach we cluster the plan using hierarchical agglomerative clustering and choose a plan from the biggest cluster in the contrasting approach we make use of generative markov chain technique using human judgment we show that the generative model with the first order markov chain ha the best performance we also show that environmental constraint can be incorporated in the generated plan 
