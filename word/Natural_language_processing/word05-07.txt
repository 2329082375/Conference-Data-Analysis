named entity recognition ner is an important part of many natural language processing task current approach often employ machine learning technique and require supervised data however many language lack such resource this paper present an almost unsupervised learning algorithm for automatic discovery of named entity ne in a resource free language given a bilingual corpus in which it is weakly temporally aligned with a resource rich language ne have similar time distribution across such corpus and often some of the token in a multi word ne are transliterated we develop an algorithm that exploit both observation iteratively the algorithm make use of a new frequency based metric for time distribution and a resource free discriminative approach to transliteration seeded with a small number of transliteration pair our algorithm discovers multi word ne and take advantage of a dictionary if one exists to account for translated or partially translated ne we evaluate the algorithm on an english russian corpus and show high level of ne discovery in russian 
we describe how context sensitive user tailored output is specified and produced in the comic multimodal dialogue system at the conference we will demonstrate the user adapted feature of the dialogue manager and text planner 
we present an unsupervised approach to symmetric word alignment in which two simple asymmetric model are trained jointly to maximize a combination of data likelihood and agreement between the model compared to the standard practice of intersecting prediction of independently trained model joint training provides a reduction in aer moreover a simple and efficient pair of hmm aligners provides a reduction in aer over symmetrized ibm model prediction 
in this paper we extend an existing parser to produce richer output annotated with function label we obtain state of the art result both in function labelling and in parsing by automatically relabelling the penn treebank tree in particular we obtain the best published result on semantic function label this suggests that current statistical parsing method are sufficiently general to produce accurate shallow semantic annotation 
we consider the problem of constructing a directed acyclic graph that encodes temporal relation found in a text the unit of our analysis is a temporal segment a fragment of text that maintains temporal coherence the strength of our approach lie in it ability to simultaneously optimize pairwise ordering preference and global constraint on the graph topology our learning method achieves f measure in temporal segmentation and accuracy in inferring temporal relation between two segment 
we present an approach to statistical machine translation that combine idea from phrase based smt and traditional grammar based mt our system incorporates the concept of multi word translation unit into transfer of dependency structure snippet and model and train statistical component according to stateof the art smt system compliant with classical transfer based mt target dependency structure snippet are input to a grammar based generator an experimental evaluation show that the incorporation of a grammar based generator into an smt framework provides improved grammaticality while achieving state of the art quality on in coverage example suggesting a possible hybrid framework 
we present a very efficient statistical incremental parser for ltag spinal a variant of ltag the parser support the full adjoining operation dynamic predicate coordination and non projective dependency with a formalism of provably stronger generative capacity a compared to cfg using gold standard po tag a input on section of the ptb the parser achieves an f score of for syntactic dependency defined on ltag derivation tree which are deeper than the dependency extracted from ptb alone with head rule for example in magerman s style 
we present a probabilistic bilingual capitalization model for capitalizing machine translation output using conditional random field experiment carried out on three language pair and a variety of experiment condition show that our model significantly outperforms a strong monolingual capitalization model baseline especially when working with small datasets and or european language pair 
abstract we present a formalization of dependency labeling with integer linear programming we focus on the integration of subcategorization into the decision making process where the various subcategorization frame of a verb compete with each other a maximum entropy model provides the weight for ilp optimization 
transliteration is the task of converting a word from one alphabetic script to another we present a novel substring based approach to transliteration inspired by phrasebased model of machine translation we investigate two implementation of substringbased transliteration a dynamic programming algorithm and a finite state transducer we show that our substring based transducer not only outperforms a state of the art letterbased approach by a significant margin but is also order of magnitude faster 
reordering is currently one of the most important problem in statistical machine translation system this paper present a novel strategy for dealing with it statistical machine reordering smr it consists in using the powerful technique developed for statistical machine translation smt to translate the source language s into a reordered source language s which allows for an improved translation into the target language t the smt task change from s t to s t which lead to a monotonized word alignment and shorter translation unit in addition the use of class in smr help to infer new word reordering experiment are reported in the esen wmt task and the zhen iwslt task and show signicant improvement in translation quality 
machine transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language it is useful for machine translation cross lingual information retrieval multilingual text and speech processing punjabi machine transliteration pmt is a special case of machine transliteration and is a process of converting a word from shahmukhi based on arabic script to gurmukhi derivation of landa shardha and takri old script of indian subcontinent two script of punjabi irrespective of the type of word the punjabi machine transliteration system us transliteration rule character mapping and dependency rule for transliteration of shahmukhi word into gurmukhi the pmt system can transliterate every word written in shahmukhi 
this paper proposes a generic mathematical formalism for the combination of various structure string tree dag graph and product of them the polarization of the object of the elementary structure control the saturation of the final structure this formalism is both elementary and powerful enough to strongly simulate many grammar formalism such a rewriting system dependency grammar tag hpsg and lfg 
many algorithm have been developed to harvest lexical semantic resource however few have linked the mined knowledge into formal knowledge repository in this paper we propose two algorithm for automatically ontologizing attaching semantic relation into wordnet we present an empirical evaluation on the task of attaching part of and causation relation showing an improvement on f score over a baseline model 
in this work we learn cluster of contextual annotation for non terminal in the penn treebank perhaps the best way to think about this problem is to contrast our work with that of klein and manning that research used tree transformation to create various grammar with different contextual annotation on the non terminal these grammar were then used in conjunction with a cky parser the author explored the space of different annotation combination by hand here we try to automate the process to learn the right combination automatically our result are not quite a good a those carefully created by hand but they are close v 
supposeyouareonamobiledevicewith nokeyboard e g acellorpda how canyouentertextquickly t graffiti thisdemowillshowhowlanguagemodel ingcanbeusedtospeedupdataentry both inthemobilecontext aswellasthedesk top thewildthingencouragesusersto usewildcards alanguagemodelfinds thek bestexpansions usersquicklyfigure outwhentheycangetawaywithwild card generalpurposetrigramlanguage modelsareeffectiveforthegeneralcase unrestrictedtext butthereareimportant specialcaseslikesearchingoverpopular webqueries wheremorerestrictedlan guagemodelsareevenmoreeffective 
historically unsupervised learning technique have lacked a principled technique for selecting the number of unseen component research into non parametric prior such a the dirichlet process ha enabled instead the use of infinite model in which the number of hidden category is not fixed but can grow with the amount of training data here we develop the infinite tree a new infinite model capable of representing recursive branching structure over an arbitrarily large set of hidden category specifically we develop three infinite tree model each of which enforces different independence assumption and for each model we define a simple direct assignmentsampling inference procedure we demonstrate the utility of our model by doing unsupervised learning of part of speech tag from treebank dependency skeleton structure achieving an accuracy of and by doing unsupervised splitting of part of speech tag which increase the accuracy of a generative dependency parser from to 
pipeline computation in which a task is decomposed into several stage that are solved sequentially is a common computational strategy in natural language processing the key problem of this model is that it result in error accumulation and suffers from it inability to correct mistake in previous stage we develop a framework for decision made via in pipeline model which address these difficulty and present and evaluates it in the context of bottom up dependency parsing for english we show improvement in the accuracy of the inferred tree relative to existing model interestingly the proposed algorithm shine especially when evaluated globally at a sentence level where our result are significantly better than those of existing approach 
we have established a phonotactic language model a the solution to spoken language identification lid in this framework we define a single set of acoustic token to represent the acoustic activity in the world s spoken language a voice tokenizer convert a spoken document into a text like document of acoustic token thus a spoken document can be represented by a count vector of acoustic token and token n gram in the vector space we apply latent semantic analysis to the vector in the same way that it is applied in information retrieval in order to capture salient phonotactics present in spoken document the vector space modeling of spoken utterance constitutes a paradigm shift in lid technology and ha proven to be very successful it present a error rate reduction over one of the best reported result on the nist language recognition evaluation database 
word clustering is important for automatic thesaurus construction text classification and word sense disambiguation recently several study have reported using the web a a corpus this paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web count each pair of word is queried to a search engine which produce a co occurrence matrix by calculating the similarity of word a word co occurrence graph is obtained a new kind of graph clustering algorithm called newman clustering is applied for efficiently identifying word cluster evaluation are made on two set of word group derived from a web directory and wordnet 
the amount of document directly published by end user is increasing along with the growth of web such document often contain spoken style expression which are difficult to analyze using conventional parser this paper present dependency parsing whose goal is to analyze japanese semi spoken expression one characteristic of our method is that it can parse self dependent independent segment using sequential labeling 
we examine the effect of contextual and acoustic cue in the disambiguation of three discourse pragmatic function of the word okay result of a perception study show that contextual cue are stronger predictor of discourse function than acoustic cue however acoustic feature capturing the pitch excursion at the right edge of okay feature prominently in disambiguation whether other contextual cue are present or not 
this paper describes a novel instance based sentence boundary determination method for natural language generation that optimizes a set of criterion based on example in a corpus compared to existing sentence boundary determination approach our work offer three significant contribution first our approach provides a general domain independent framework that effectively address sentence boundary determination by balancing a comprehensive set of sentence complexity and quality related constraint second our approach can simulate the characteristic and the style of naturally occurring sentence in an application domain since our solution are optimized based on their similarity to example in a corpus third our approach can adapt easily to suit a natural language generation system s capability by balancing the strength and weakness of it subcomponents e g it aggregation and referring expression generation capability our final evaluation show that the proposed method result in significantly better sentence generation outcome than a widely adopted approach 
i review a number of grammar induction algorithm abl emile adios and test them on the eindhoven corpus resulting in disappointing result compared to the usually tested corpus atis ovis also i show that using neither po tag induced from biemann s unsupervised po tagging algorithm nor hand corrected po tag a input improves this situation last i argue for the development of entirely incremental grammar induction algorithm instead of the approach of the system discussed before 
most current statistical natural language processing model use only local feature so a to permit dynamic programming in inference but this make them unable to fully account for the long distance structure that is prevalent in language use we show how to solve this dilemma with gibbs sampling a simple monte carlo method used to perform approximate inference in factored probabilistic model by using simulated annealing in place of viterbi decoding in sequence model such a hmms cmms and crfs it is possible to incorporate non local structure while preserving tractable inference we use this technique to augment an existing crf based information extraction system with long distance dependency model enforcing label consistency and extraction template consistency constraint this technique result in an error reduction of up to over state of the art system on two established information extraction task 
we apply pattern based method for collecting hypernym relation from the web we compare our approach with hypernym extraction from morphological clue and from large text corpus we show that the abundance of available data on the web enables obtaining good result with relatively unsophisticated technique 
this paper describes an algorithm to automatically generate a list of cognate in a target language by mean of support vector machine while levenshtein distance wa used to align the training file no knowledge repository other than an initial list of cognate used for training purpose wa input into the algorithm evaluation wa set up in a cognate production scenario which mimed a real life situation where no word list were available in the target language delivering the ideal environment to test the feasibility of a more ambitious project that will involve language portability an overall improvement of over the baseline showed promising horizon 
speech recognition in many morphologically rich language suffers from a very high out of vocabulary oov ratio earlier work ha shown that vocabulary decomposition method can practically solve this problem for a subset of these language this paper compare various vocabulary decomposition approach to open vocabulary speech recognition using estonian speech recognition a a benchmark comparison are performed utilizing large model of lexical item and smaller vocabulary of item a large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate while the unsupervised morphology discovery method morfessor baseline give marginally weaker result only the morfessor based approach is shown to adequately scale to smaller vocabulary size 
the psycholinguistic literature provides evidence for syntactic priming i e the tendency to repeat structure this paper describes a method for incorporating priming into an incremental probabilistic parser three model are compared which involve priming of rule between sentence within sentence and within coordinate structure these model simulate the reading time advantage for parallel structure found in human data and also yield a small increase in overall parsing accuracy 
a number of russian verb lack sg nonpast form these paradigmatic gap are puzzling because they seemingly contradict the highly productive nature of inflectional system we model the persistence and spread of russian gap via a multi agent model with bayesian learning we ran three simulation no grammar learning learning with arbitrary analogical pressure and morphophonologically conditioned learning we compare the result to the attested historical development of the gap contradicting previous account we propose that the persistence of gap can be explained in the absence of synchronic competition between form 
this paper present a corrective model for speech recognition of inflected language the model based on a discriminative framework incorporates word n gram feature a well a factored morphological feature providing error reduction over the model based solely on word n gram feature experiment on a large vocabulary task namely the czech portion of the malach corpus demonstrate performance gain of about absolute in word error rate wherein morphological feature contribute about a third of the improvement a simple feature selection mechanism based on x statistic is shown to be effective in reducing the number of feature by about without any loss in performance making it feasible to explore yet larger feature space 
in this paper we develop a methodology for discovering the thematic structure of the qur an based on a fundamental idea in data mining and related discipline that with respect to some collection of text the lexical frequency profile of the individual text are a good indicator of their conceptual content and thus provide a reliable criterion for their classification relative to one another this idea is applied to the discovery of thematic interrelationship among the sura chapter of the qur an by abstracting lexical frequency data from them and then applying hierarchical cluster analysis to that data the result reported here indicate that the proposed methodology yield usable result in understanding the qur an on the basis of it lexical semantics 
live closed caption for deaf and hard of hearing audience are currently produced by stenographer or by voice writer using speech recognition both technique can produce caption with error we are currently developing a correction module that allows a user to intercept the real time caption stream and correct it before it is broadcast we report result of preliminary experiment on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system 
the ami meeting corpus is now publicly available including manual annotation file generated in the nxt xml format but lacking explicit metadata for the meeting of the corpus to increase the usability of this important resource a representation format based on relational database is proposed which maximizes informativeness simplicity and reusability of the metadata and annotation the annotation file are converted to a tabular format using an easily adaptable xslt based mechanism and their consistency is verified in the process metadata file are generated directly in the imdi xml format from implicit information and converted to tabular format using a similar procedure the result and tool will be freely available with the ami corpus sharing the metadata using the open archive network will contribute to increase the visibility of the ami corpus 
in the past year a number of lexical association measure have been studied to help extract new scientific terminology or general language collocation the implicit assumption of this research wa that newly designed term measure involving more sophisticated statistical criterion would outperform simple count of co occurrence frequency we here explicitly test this assumption by way of four qualitative criterion we show that purely statistic based measure reveal virtually no difference compared with frequency of occurrence count while linguistically more informed metric do reveal such a marked difference 
semantic lexical matching is a prominent subtask within text understanding application yet it is rarely evaluated in a direct manner this paper proposes a definition for lexical reference which capture the common goal of lexical matching based on this definition we created and analyzed a test dataset that wa utilized to directly evaluate compare and improve lexical matching model we suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual component 
this paper introduces a novel framework for the accurate retrieval of relational concept from huge text prior to retrieval all sentence are annotated with predicate argument structure and ontological identifier by applying a deep parser and a term recognizer during the run time user request are converted into query of region algebra on these annotation structural matching with pre computed semantic annotation establishes the accurate and efficient retrieval of relational concept this framework wa applied to a text retrieval system for medline experiment on the retrieval of biomedical correlation revealed that the cost is sufficiently small for real time application and that the retrieval precision is significantly improved 
information extraction ie is a fundamental technology for nlp previous method for ie were relying on co occurrence relation soft pattern and property of the target for example syntactic role which result in problem of handling paraphrasing and alignment of instance our system are anchor and relation is based on the dependency relation model and tackle these problem by unifying entity according to their dependency relation which we found to provide more invariant relation between entity in many case in order to exploit the complexity and characteristic of relation path we further classify the relation path into the category of easy average and hard and utilize different extraction strategy based on the characteristic of those category our extraction method lead to improvement in performance by and for muc and muc respectively a compared to the state of art ie system 
this paper proposes a semi supervised boosting approach to improve statistical word alignment with limited labeled data and large amount of unlabeled data the proposed approach modifies the supervised boosting algorithm to a semi supervised learning algorithm by incorporating the unlabeled data in this algorithm we build a word aligner by using both the labeled data and the unlabeled data then we build a pseudo reference set for the unlabeled data and calculate the error rate of each word aligner using only the labeled data based on this semi supervised boosting algorithm we investigate two boosting method for word alignment in addition we improve the word alignment result by combining the result of the two semi supervised boosting method experimental result on word alignment indicate that semi supervised boosting achieves relative error reduction of and a compared with supervised boosting and unsupervised boosting respectively 
we study the issue of porting a known nlp method to a language with little existing nlp resource specifically hebrew svm based chunking we introduce two svm based method model tampering and anchored learning these allow fine grained analysis of the learned svm model which provides guidance to identify error in the training corpus distinguish the role and interaction of lexical feature and eventually construct a model with error reduction the resulting chunker is shown to be robust in the presence of noise in the training corpus relies on le lexical feature than wa previously understood and achieves an f measure performance of on automatically po tagged text the svm analysis method also provide general insight on svm based chunking 
a resource grammar is a standard library for the gf grammar formalism it raise the abstraction level of writing domain specific grammar by taking care of the general grammatical rule of a language gf resource grammar have been built in parallel for eleven language and share a common interface which simplifies multilingual application we reflect on our experience with the russian resource grammar trying to answer the question how well russian fit into the common interface and where the line between language independent and language specific should be drawn 
named entity recognition ner is an important part of many natural language processing task current approach often employ machine learning technique and require supervised data however many language lack such resource this paper present an almost unsupervised learning algorithm for automatic discovery of named entity ne in a resource free language given a bilingual corpus in which it is weakly temporally aligned with a resource rich language ne have similar time distribution across such corpus and often some of the token in a multi word ne are transliterated we develop an algorithm that exploit both observation iteratively the algorithm make use of a new frequency based metric for time distribution and a resource free discriminative approach to transliteration seeded with a small number of transliteration pair our algorithm discovers multi word ne and take advantage of a dictionary if one exists to account for translated or partially translated ne we evaluate the algorithm on an english russian corpus and show high level of ne discovery in russian 
we describe how context sensitive user tailored output is specified and produced in the comic multimodal dialogue system at the conference we will demonstrate the user adapted feature of the dialogue manager and text planner 
we present an unsupervised approach to symmetric word alignment in which two simple asymmetric model are trained jointly to maximize a combination of data likelihood and agreement between the model compared to the standard practice of intersecting prediction of independently trained model joint training provides a reduction in aer moreover a simple and efficient pair of hmm aligners provides a reduction in aer over symmetrized ibm model prediction 
in this paper we extend an existing parser to produce richer output annotated with function label we obtain state of the art result both in function labelling and in parsing by automatically relabelling the penn treebank tree in particular we obtain the best published result on semantic function label this suggests that current statistical parsing method are sufficiently general to produce accurate shallow semantic annotation 
we consider the problem of constructing a directed acyclic graph that encodes temporal relation found in a text the unit of our analysis is a temporal segment a fragment of text that maintains temporal coherence the strength of our approach lie in it ability to simultaneously optimize pairwise ordering preference and global constraint on the graph topology our learning method achieves f measure in temporal segmentation and accuracy in inferring temporal relation between two segment 
we present an approach to statistical machine translation that combine idea from phrase based smt and traditional grammar based mt our system incorporates the concept of multi word translation unit into transfer of dependency structure snippet and model and train statistical component according to stateof the art smt system compliant with classical transfer based mt target dependency structure snippet are input to a grammar based generator an experimental evaluation show that the incorporation of a grammar based generator into an smt framework provides improved grammaticality while achieving state of the art quality on in coverage example suggesting a possible hybrid framework 
we present a very efficient statistical incremental parser for ltag spinal a variant of ltag the parser support the full adjoining operation dynamic predicate coordination and non projective dependency with a formalism of provably stronger generative capacity a compared to cfg using gold standard po tag a input on section of the ptb the parser achieves an f score of for syntactic dependency defined on ltag derivation tree which are deeper than the dependency extracted from ptb alone with head rule for example in magerman s style 
we present a probabilistic bilingual capitalization model for capitalizing machine translation output using conditional random field experiment carried out on three language pair and a variety of experiment condition show that our model significantly outperforms a strong monolingual capitalization model baseline especially when working with small datasets and or european language pair 
abstract we present a formalization of dependency labeling with integer linear programming we focus on the integration of subcategorization into the decision making process where the various subcategorization frame of a verb compete with each other a maximum entropy model provides the weight for ilp optimization 
transliteration is the task of converting a word from one alphabetic script to another we present a novel substring based approach to transliteration inspired by phrasebased model of machine translation we investigate two implementation of substringbased transliteration a dynamic programming algorithm and a finite state transducer we show that our substring based transducer not only outperforms a state of the art letterbased approach by a significant margin but is also order of magnitude faster 
reordering is currently one of the most important problem in statistical machine translation system this paper present a novel strategy for dealing with it statistical machine reordering smr it consists in using the powerful technique developed for statistical machine translation smt to translate the source language s into a reordered source language s which allows for an improved translation into the target language t the smt task change from s t to s t which lead to a monotonized word alignment and shorter translation unit in addition the use of class in smr help to infer new word reordering experiment are reported in the esen wmt task and the zhen iwslt task and show signicant improvement in translation quality 
machine transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language it is useful for machine translation cross lingual information retrieval multilingual text and speech processing punjabi machine transliteration pmt is a special case of machine transliteration and is a process of converting a word from shahmukhi based on arabic script to gurmukhi derivation of landa shardha and takri old script of indian subcontinent two script of punjabi irrespective of the type of word the punjabi machine transliteration system us transliteration rule character mapping and dependency rule for transliteration of shahmukhi word into gurmukhi the pmt system can transliterate every word written in shahmukhi 
this paper proposes a generic mathematical formalism for the combination of various structure string tree dag graph and product of them the polarization of the object of the elementary structure control the saturation of the final structure this formalism is both elementary and powerful enough to strongly simulate many grammar formalism such a rewriting system dependency grammar tag hpsg and lfg 
many algorithm have been developed to harvest lexical semantic resource however few have linked the mined knowledge into formal knowledge repository in this paper we propose two algorithm for automatically ontologizing attaching semantic relation into wordnet we present an empirical evaluation on the task of attaching part of and causation relation showing an improvement on f score over a baseline model 
in this work we learn cluster of contextual annotation for non terminal in the penn treebank perhaps the best way to think about this problem is to contrast our work with that of klein and manning that research used tree transformation to create various grammar with different contextual annotation on the non terminal these grammar were then used in conjunction with a cky parser the author explored the space of different annotation combination by hand here we try to automate the process to learn the right combination automatically our result are not quite a good a those carefully created by hand but they are close v 
supposeyouareonamobiledevicewith nokeyboard e g acellorpda how canyouentertextquickly t graffiti thisdemowillshowhowlanguagemodel ingcanbeusedtospeedupdataentry both inthemobilecontext aswellasthedesk top thewildthingencouragesusersto usewildcards alanguagemodelfinds thek bestexpansions usersquicklyfigure outwhentheycangetawaywithwild card generalpurposetrigramlanguage modelsareeffectiveforthegeneralcase unrestrictedtext butthereareimportant specialcaseslikesearchingoverpopular webqueries wheremorerestrictedlan guagemodelsareevenmoreeffective 
historically unsupervised learning technique have lacked a principled technique for selecting the number of unseen component research into non parametric prior such a the dirichlet process ha enabled instead the use of infinite model in which the number of hidden category is not fixed but can grow with the amount of training data here we develop the infinite tree a new infinite model capable of representing recursive branching structure over an arbitrarily large set of hidden category specifically we develop three infinite tree model each of which enforces different independence assumption and for each model we define a simple direct assignmentsampling inference procedure we demonstrate the utility of our model by doing unsupervised learning of part of speech tag from treebank dependency skeleton structure achieving an accuracy of and by doing unsupervised splitting of part of speech tag which increase the accuracy of a generative dependency parser from to 
pipeline computation in which a task is decomposed into several stage that are solved sequentially is a common computational strategy in natural language processing the key problem of this model is that it result in error accumulation and suffers from it inability to correct mistake in previous stage we develop a framework for decision made via in pipeline model which address these difficulty and present and evaluates it in the context of bottom up dependency parsing for english we show improvement in the accuracy of the inferred tree relative to existing model interestingly the proposed algorithm shine especially when evaluated globally at a sentence level where our result are significantly better than those of existing approach 
we have established a phonotactic language model a the solution to spoken language identification lid in this framework we define a single set of acoustic token to represent the acoustic activity in the world s spoken language a voice tokenizer convert a spoken document into a text like document of acoustic token thus a spoken document can be represented by a count vector of acoustic token and token n gram in the vector space we apply latent semantic analysis to the vector in the same way that it is applied in information retrieval in order to capture salient phonotactics present in spoken document the vector space modeling of spoken utterance constitutes a paradigm shift in lid technology and ha proven to be very successful it present a error rate reduction over one of the best reported result on the nist language recognition evaluation database 
word clustering is important for automatic thesaurus construction text classification and word sense disambiguation recently several study have reported using the web a a corpus this paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web count each pair of word is queried to a search engine which produce a co occurrence matrix by calculating the similarity of word a word co occurrence graph is obtained a new kind of graph clustering algorithm called newman clustering is applied for efficiently identifying word cluster evaluation are made on two set of word group derived from a web directory and wordnet 
the amount of document directly published by end user is increasing along with the growth of web such document often contain spoken style expression which are difficult to analyze using conventional parser this paper present dependency parsing whose goal is to analyze japanese semi spoken expression one characteristic of our method is that it can parse self dependent independent segment using sequential labeling 
we examine the effect of contextual and acoustic cue in the disambiguation of three discourse pragmatic function of the word okay result of a perception study show that contextual cue are stronger predictor of discourse function than acoustic cue however acoustic feature capturing the pitch excursion at the right edge of okay feature prominently in disambiguation whether other contextual cue are present or not 
this paper describes a novel instance based sentence boundary determination method for natural language generation that optimizes a set of criterion based on example in a corpus compared to existing sentence boundary determination approach our work offer three significant contribution first our approach provides a general domain independent framework that effectively address sentence boundary determination by balancing a comprehensive set of sentence complexity and quality related constraint second our approach can simulate the characteristic and the style of naturally occurring sentence in an application domain since our solution are optimized based on their similarity to example in a corpus third our approach can adapt easily to suit a natural language generation system s capability by balancing the strength and weakness of it subcomponents e g it aggregation and referring expression generation capability our final evaluation show that the proposed method result in significantly better sentence generation outcome than a widely adopted approach 
i review a number of grammar induction algorithm abl emile adios and test them on the eindhoven corpus resulting in disappointing result compared to the usually tested corpus atis ovis also i show that using neither po tag induced from biemann s unsupervised po tagging algorithm nor hand corrected po tag a input improves this situation last i argue for the development of entirely incremental grammar induction algorithm instead of the approach of the system discussed before 
most current statistical natural language processing model use only local feature so a to permit dynamic programming in inference but this make them unable to fully account for the long distance structure that is prevalent in language use we show how to solve this dilemma with gibbs sampling a simple monte carlo method used to perform approximate inference in factored probabilistic model by using simulated annealing in place of viterbi decoding in sequence model such a hmms cmms and crfs it is possible to incorporate non local structure while preserving tractable inference we use this technique to augment an existing crf based information extraction system with long distance dependency model enforcing label consistency and extraction template consistency constraint this technique result in an error reduction of up to over state of the art system on two established information extraction task 
we apply pattern based method for collecting hypernym relation from the web we compare our approach with hypernym extraction from morphological clue and from large text corpus we show that the abundance of available data on the web enables obtaining good result with relatively unsophisticated technique 
this paper describes an algorithm to automatically generate a list of cognate in a target language by mean of support vector machine while levenshtein distance wa used to align the training file no knowledge repository other than an initial list of cognate used for training purpose wa input into the algorithm evaluation wa set up in a cognate production scenario which mimed a real life situation where no word list were available in the target language delivering the ideal environment to test the feasibility of a more ambitious project that will involve language portability an overall improvement of over the baseline showed promising horizon 
speech recognition in many morphologically rich language suffers from a very high out of vocabulary oov ratio earlier work ha shown that vocabulary decomposition method can practically solve this problem for a subset of these language this paper compare various vocabulary decomposition approach to open vocabulary speech recognition using estonian speech recognition a a benchmark comparison are performed utilizing large model of lexical item and smaller vocabulary of item a large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate while the unsupervised morphology discovery method morfessor baseline give marginally weaker result only the morfessor based approach is shown to adequately scale to smaller vocabulary size 
the psycholinguistic literature provides evidence for syntactic priming i e the tendency to repeat structure this paper describes a method for incorporating priming into an incremental probabilistic parser three model are compared which involve priming of rule between sentence within sentence and within coordinate structure these model simulate the reading time advantage for parallel structure found in human data and also yield a small increase in overall parsing accuracy 
a number of russian verb lack sg nonpast form these paradigmatic gap are puzzling because they seemingly contradict the highly productive nature of inflectional system we model the persistence and spread of russian gap via a multi agent model with bayesian learning we ran three simulation no grammar learning learning with arbitrary analogical pressure and morphophonologically conditioned learning we compare the result to the attested historical development of the gap contradicting previous account we propose that the persistence of gap can be explained in the absence of synchronic competition between form 
this paper present a corrective model for speech recognition of inflected language the model based on a discriminative framework incorporates word n gram feature a well a factored morphological feature providing error reduction over the model based solely on word n gram feature experiment on a large vocabulary task namely the czech portion of the malach corpus demonstrate performance gain of about absolute in word error rate wherein morphological feature contribute about a third of the improvement a simple feature selection mechanism based on x statistic is shown to be effective in reducing the number of feature by about without any loss in performance making it feasible to explore yet larger feature space 
in this paper we develop a methodology for discovering the thematic structure of the qur an based on a fundamental idea in data mining and related discipline that with respect to some collection of text the lexical frequency profile of the individual text are a good indicator of their conceptual content and thus provide a reliable criterion for their classification relative to one another this idea is applied to the discovery of thematic interrelationship among the sura chapter of the qur an by abstracting lexical frequency data from them and then applying hierarchical cluster analysis to that data the result reported here indicate that the proposed methodology yield usable result in understanding the qur an on the basis of it lexical semantics 
live closed caption for deaf and hard of hearing audience are currently produced by stenographer or by voice writer using speech recognition both technique can produce caption with error we are currently developing a correction module that allows a user to intercept the real time caption stream and correct it before it is broadcast we report result of preliminary experiment on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system 
the ami meeting corpus is now publicly available including manual annotation file generated in the nxt xml format but lacking explicit metadata for the meeting of the corpus to increase the usability of this important resource a representation format based on relational database is proposed which maximizes informativeness simplicity and reusability of the metadata and annotation the annotation file are converted to a tabular format using an easily adaptable xslt based mechanism and their consistency is verified in the process metadata file are generated directly in the imdi xml format from implicit information and converted to tabular format using a similar procedure the result and tool will be freely available with the ami corpus sharing the metadata using the open archive network will contribute to increase the visibility of the ami corpus 
in the past year a number of lexical association measure have been studied to help extract new scientific terminology or general language collocation the implicit assumption of this research wa that newly designed term measure involving more sophisticated statistical criterion would outperform simple count of co occurrence frequency we here explicitly test this assumption by way of four qualitative criterion we show that purely statistic based measure reveal virtually no difference compared with frequency of occurrence count while linguistically more informed metric do reveal such a marked difference 
semantic lexical matching is a prominent subtask within text understanding application yet it is rarely evaluated in a direct manner this paper proposes a definition for lexical reference which capture the common goal of lexical matching based on this definition we created and analyzed a test dataset that wa utilized to directly evaluate compare and improve lexical matching model we suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual component 
this paper introduces a novel framework for the accurate retrieval of relational concept from huge text prior to retrieval all sentence are annotated with predicate argument structure and ontological identifier by applying a deep parser and a term recognizer during the run time user request are converted into query of region algebra on these annotation structural matching with pre computed semantic annotation establishes the accurate and efficient retrieval of relational concept this framework wa applied to a text retrieval system for medline experiment on the retrieval of biomedical correlation revealed that the cost is sufficiently small for real time application and that the retrieval precision is significantly improved 
information extraction ie is a fundamental technology for nlp previous method for ie were relying on co occurrence relation soft pattern and property of the target for example syntactic role which result in problem of handling paraphrasing and alignment of instance our system are anchor and relation is based on the dependency relation model and tackle these problem by unifying entity according to their dependency relation which we found to provide more invariant relation between entity in many case in order to exploit the complexity and characteristic of relation path we further classify the relation path into the category of easy average and hard and utilize different extraction strategy based on the characteristic of those category our extraction method lead to improvement in performance by and for muc and muc respectively a compared to the state of art ie system 
this paper proposes a semi supervised boosting approach to improve statistical word alignment with limited labeled data and large amount of unlabeled data the proposed approach modifies the supervised boosting algorithm to a semi supervised learning algorithm by incorporating the unlabeled data in this algorithm we build a word aligner by using both the labeled data and the unlabeled data then we build a pseudo reference set for the unlabeled data and calculate the error rate of each word aligner using only the labeled data based on this semi supervised boosting algorithm we investigate two boosting method for word alignment in addition we improve the word alignment result by combining the result of the two semi supervised boosting method experimental result on word alignment indicate that semi supervised boosting achieves relative error reduction of and a compared with supervised boosting and unsupervised boosting respectively 
we study the issue of porting a known nlp method to a language with little existing nlp resource specifically hebrew svm based chunking we introduce two svm based method model tampering and anchored learning these allow fine grained analysis of the learned svm model which provides guidance to identify error in the training corpus distinguish the role and interaction of lexical feature and eventually construct a model with error reduction the resulting chunker is shown to be robust in the presence of noise in the training corpus relies on le lexical feature than wa previously understood and achieves an f measure performance of on automatically po tagged text the svm analysis method also provide general insight on svm based chunking 
a resource grammar is a standard library for the gf grammar formalism it raise the abstraction level of writing domain specific grammar by taking care of the general grammatical rule of a language gf resource grammar have been built in parallel for eleven language and share a common interface which simplifies multilingual application we reflect on our experience with the russian resource grammar trying to answer the question how well russian fit into the common interface and where the line between language independent and language specific should be drawn 
the limited coverage of lexical semantic resource is a significant problem for nlp system which can be alleviated by automatically classifying the unknown word supersense tagging assigns unknown noun one of broad semantic category used by lexicographer to organise their manual insertion into wordnet ciaramita and johnson present a tagger which us synonym set gloss a annotated training example we describe an unsupervised approach based on vector space similarity which doe not require annotated example but significantly outperforms their tagger we also demonstrate the use of an extremely large shallow parsed corpus for calculating vector space semantic similarity 
we claim that existing specification language for tree based grammar fail to adequately support identifier managment we then show that xmg extensible meta grammar provides a sophisticated treatment of identifier which is effective in supporting a linguist friendly grammar design 
current parsing model are not immediately applicable for language that exhibit strong interaction between morphology and syntax e g modern hebrew mh arabic and other semitic language this work represents a first attempt at modeling morphological syntactic interaction in a generative probabilistic framework to allow for mh parsing we show that morphological information selected in tandem with syntactic category is instrumental for parsing semitic language we further show that redundant morphological information help syntactic disambiguation 
we present bayesum for bayesian summarization a model for sentence extraction in query focused summarization bayesum leverage the common case in which multiple document are relevant to a single query using these document a reinforcement for query term bayesum is not afflicted by the paucity of information in short query we show that approximate inference in bayesum is possible on large data set and result in a state of the art summarization system furthermore we show how bayesum can be understood a a justified query expansion technique in the language modeling for ir framework 
this paper present the first probabilistic parsing result for french using the recently released french treebank we start with an unlexicalized pcfg a a baseline model which is enriched to the level of collins model by adding lexicalization and subcategorization the lexicalized sister head model and a bigram model are also tested to deal with the flatness of the french treebank the bigram model achieves the best performance constituency f score and dependency accuracy all lexicalized model outperform the unlexicalized baseline consistent with probabilistic parsing result for english but contrary to result for german where lexicalization ha only a limited effect on parsing performance 
this research is aimed at the problem of disambiguating toponym place name in term of a classification derived by merging information from two publicly available gazetteer to establish the difficulty of the problem we measured the degree of ambiguity with respect to a gazetteer for toponym in news we found that of the toponym found in a corpus that were ambiguous in a gazetteer lacked a local discriminator in the text given the scarcity of humanannotated data our method used unsupervised machine learning to develop disambiguation rule toponym were automatically tagged with information about them found in a gazetteer a toponym that wa ambiguous in the gazetteer wa automatically disambiguated based on preference heuristic this automatically tagged data wa used to train a machine learner which disambiguated toponym in a human annotated news corpus at accuracy 
we present two discriminative method for name transliteration the method correspond to local and global modeling approach in modeling structured output space both method do not require alignment of name in different language their feature are computed directly from the name themselves we perform an experimental evaluation of the method for name transliteration from three language arabic korean and russian into english and compare the method experimentally to a state of the art joint probabilistic modeling approach we find that the discriminative method outperform probabilistic modeling with the global discriminative modeling approach achieving the best performance in all language 
extraction of relation between entity is an important part of information extraction on free text previous method are mostly based on statistical correlation and dependency relation between entity this paper re examines the problem at the multiresolution layer of phrase clause and sentence using dependency and discourse relation our multi resolution framework are anchor and relation us clausal relation in way to filter noisy dependency path and to increase reliability of dependency path extraction the resulting system outperforms the previous approach by on muc muc and ace rdc domain respectively 
user supplied review are widely and increasingly used to enhance ecommerce and other website because review can be numerous and varying in quality it is important to ass how helpful each review is while review helpfulness is currently assessed manually in this paper we consider the task of automatically assessing it experiment using svm regression on a variety of feature over amazon com product review show promising result with rank correlation of up to we found that the most useful feature include the length of the review it unigrams and it product rating 
the present work advance the accuracy and training speed of discriminative parsing our discriminative parsing method ha no generative component yet surpasses a generative baseline on constituent parsing and doe so with minimal linguistic cleverness our model can incorporate arbitrary feature of the input and parse state and performs feature selection incrementally over an exponential feature space during training we demonstrate the flexibility of our approach by testing it with several parsing strategy and various feature set our implementation is freely available at 
we report on the development of a new automatic feedback model to improve information retrieval in digital library our hypothesis is that some particular sentence selected based on argumentative criterion can be more useful than others to perform well known feedback information retrieval task the argumentative model we explore is based on four disjunct class which ha been very regularly observed in scientific report purpose method result conclusion to test this hypothesis we use the rocchio algorithm a baseline while rocchio selects the feature to be added to the original query based on statistical evidence we propose to base our feature selection also on argumentative criterion thus we restrict the expansion on feature appearing only in sentence classified into one of our argumentative category our result obtained on the ohsumed collection show a significant improvement when expansion is based on purpose mean average precision and conclusion mean average precision content rather than on other argumentative content these result suggest that argumentation is an important linguistic dimension that could benefit information retrieval 
dependency structure do not have the information of phrase category in phrase structure grammar thus dependency parsing relies heavily on the lexical information of word this paper discus our investigation into the effectiveness of lexicalization in dependency parsing specifically by restricting the degree of lexicalization in the training phase of a parser we examine the change in the accuracy of dependency relation experimental result indicate that minimal or low lexicalization is sufficient for parsing accuracy 
at present adapting an information extraction system to new topic is an expensive and slow process requiring some knowledge engineering for each new topic we propose a new paradigm of information extraction which operates on demand in response to a user s query on demand information extraction odie aim to completely eliminate the customization effort given a user s query the system will automatically create pattern to extract salient relation in the text of the topic and build table from the extracted information using paraphrase discovery technology it relies on recent advance in pattern discovery paraphrase discovery and extended named entity tagging we report on experimental result in which the system created useful table for many topic demonstrating the feasibility of this approach 
suppose we have a large dictionary of string each entry start with a figure of merit popularity we wish to find the kbest match for a substring s in a dictinoary dict that is grep s dict sort n head k but we would like to do this in sublinear time example application web query with popularity product with price and ad with click through rate this paper proposes a novel index k best suffix array based on idea borrowed from suffix array and kdtrees a standard suffix array sort the suffix by a single order lexicographic whereas k best suffix array are sorted by two order lexicographic and popularity lookup time is between log n and sqrt n 
this paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text the parser us a representation for syntactic structure similar to dependency link which is well suited for incremental parsing in contrast to previous unsupervised parser the parser doe not use part of speech tag and both learning and parsing are local and fast requiring no explicit clustering or global optimization the parser is evaluated by converting it output into equivalent bracketing and improves on previously published result for unsupervised parsing from plain text 
unification grammar are widely accepted a an expressive mean for describing the structure of natural language in general the recognition problem is undecidable for unification grammar even with restricted variant of the formalism offline parsable grammar the problem is computationally hard we present two natural constraint on unification grammar which limit their expressivity we first show that non reentrant unification grammar generate exactly the class of contextfree language we then relax the constraint and show that one reentrant unification grammar generate exactly the class of tree adjoining language we thus relate the commonly used and linguistically motivated formalism of unification grammar to more restricted computationally tractable class of language 
we present an api for computing the semantic relatedness of word in wikipedia 
acl is used to systematically study domain whose element can be uniquely factored into product of irreducible element the best known example of such domain are the positive integer which can be factored into product of prime and univariate polynomial with rational coefficient which can be factored into product of irreducible polynomial there are many other such domain euclidean domain are an algebraic abstraction of both the positive integer and the rational polynomial in which the usual proof of unique factorization for both the integer and the polynomial can be generalized 
subjectivity and meaning are both important property of language this paper explores their interaction and brings empirical evidence in support of the hypothesis that subjectivity is a property that can be associated with word sens and word sense disambiguation can directly benefit from subjectivity annotation 
the calo meeting assistant is an integrated multimodal meeting assistant technology that capture speech gesture and multimodal data from multiparty interaction during meeting and us machine learning and robust discourse processing to provide a rich browsable record of a meeting 
surface realisers divide into those used in generation nlg geared realisers and those mirroring the parsing process reversible realisers while the first rely on grammar not easily usable for parsing it is unclear how the second type of realisers could be parameterised to yield from among the set of possible paraphrase the paraphrase appropriate to a given generation context in this paper we present a surface realiser which combine a reversible grammar used for parsing and doing semantic construction with a symbolic mean of selecting paraphrase 
conditional random field crfs have been applied with considerable success to a number of natural language processing task however these task have mostly involved very small label set when deployed on task with larger label set the requirement for computational resource mean that training becomes intractable this paper describes a method for training crfs on such task using error correcting output code ecoc a number of crfs are independently trained on the separate binary labelling task of distinguishing between a subset of the label and it complement during decoding these model are combined to produce a predicted label sequence which is resilient to error by individual model error correcting crf training is much le resource intensive and ha a much faster training time than a standardly formulated crf while decoding performance remains quite comparable this allows u to scale crfs to previously impossible task a demonstrated by our experiment with large label set 
we present a framenet based semantic role labeling system for swedish text a training data for the system we used an annotated corpus that we produced by transferring framenet annotation from the english side to the swedish side in a parallel corpus in addition we describe two frame element bracketing algorithm that are suitable when no robust constituent parser are available we evaluated the system on a part of the framenet example corpus that we translated manually and obtained an accuracy score of on the classification of presegmented frame element and precision and recall score of and for the complete task 
this paper report the development of log linear model for the disambiguation in wide coverage hpsg parsing the estimation of log linear model requires high computational cost especially with wide coverage grammar using technique to reduce the estimation cost we trained the model using section of penn tree bank a series of experiment empirically evaluated the estimation technique and also examined the performance of the disambiguation model on the parsing of real world sentence 
we have extended the acl theorem prover to automatically prove property of vhdl circuit with ibm s internal sixthsense verification system we have used this extension to verify a multiplier used in an industrial floating point unit the property we ultimately verify corresponds to the correctness of the component that produce a pair of bit vector whose summation is equal to the product this property is beyond the scale of the sixthsense system alone in this paper we show how we verified the multiplier by illustrating key acl lemma and theorem and also property checked by sixthsense 
abstract 
we investigate prototype driven learning for primarily unsupervised grammar induction prior knowledge is specified declaratively by providing a few canonical example of each target phrase type this sparse prototype information is then propagated across a corpus using distributional similarity feature which augment an otherwise standard pcfg model we show that distributional feature are effective at distinguishing bracket label but not determining bracket location to improve the quality of the induced tree we combine our pcfg induction with the ccm model of klein and manning which ha complementary stengths it identifies bracket but doe not label them using only a handful of prototype we show substantial improvement over naive pcfg induction for english and chinese grammar induction 
we present a novel parser combination scheme that work by reparsing input sentence once they have already been parsed by several different parser we apply this idea to dependency and constituent parsing generating result that surpass state of theart accuracy level for individual parser 
in this paper we compare the performance of a state of the art statistical parser bikel in parsing written and spoken language and in generating subcategorization cue from written and spoken language although bikel s parser achieves a higher accuracy for parsing written language it achieves a higher accuracy when extracting subcategorization cue from spoken language additionally we explore the utility of punctuation in helping parsing and extraction of subcategorization cue our experiment show that punctuation is of little help in parsing spoken language and extracting subcategorization cue from spoken language this indicates that there is no need to add punctuation in transcribing spoken corpus simply in order to help parser 
task solving in dialogue depends on the linguistic alignment of the interlocutor which pickering garrod have suggested to be based on mechanistic repetition effect in this paper we seek confirmation of this hypothesis by looking at repetition in corpus and whether repetition is correlated with task success we show that the relevant repetition tendency is based on slow adaptation rather than short term priming and demonstrate that lexical and syntactic repetition is a reliable predictor of task success given the first five minute of a taskoriented dialogue 
negative life event play an important role in triggering depressive episode developing psychiatric service that can automatically identify such event is beneficial for mental health care and prevention before these service can be provided some meaningful semantic pattern such a have to be extracted in this work we present a text mining framework capable of inducing variable length semantic pattern from unannotated psychiatry web resource this framework integrates a cognitive motivated model hyperspace analog to language hal to represent word a well a combination of word then a cascaded induction process cip bootstrap with a small set of seed pattern and incorporates relevance feedback to iteratively induce more relevant pattern the experimental result show that by combining the hal model and relevance feedback the cip can induce semantic pattern from the unannotated web corpus so a to reduce the reliance on annotated corpus 
in order to realize the full potential of dependency based syntactic parsing it is desirable to allow non projective dependency structure we show how a datadriven deterministic dependency parser in itself restricted to projective structure can be combined with graph transformation technique to produce non projective structure experiment using data from the prague dependency treebank show that the combined system can handle nonprojective construction with a precision sufficient to yield a significant improvement in overall parsing accuracy this lead to the best reported performance for robust non projective parsing of czech 
in order to build a simulated robot that accepts instruction in unconstrained natural language a corpus of route instruction wa collected from human subject in the office navigation domain the instruction were segmented by the step in the actual route and labeled with the action taken in each step this flat formulation reduced the problem to an ie segmentation task to which we applied conditional random field we compared the performance of crfs with a set of hand written rule the result showed that crfs perform better with a success rate 
when a word sense disambiguation wsd system is trained on one domain but applied to a different domain a drop in accuracy is frequently observed this highlight the importance of domain adaptation for word sense disambiguation in this paper we first show that an active learning approach can be successfully used to perform domain adaptation of wsd system then by using the predominant sense predicted by expectation maximization em and adopting a count merging technique we improve the effectiveness of the original adaptation process achieved by the basic active learning approach 
we present a new semi supervised training procedure for conditional random field crfs that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data our approach is based on extending the minimum entropy regularization framework to the structured prediction case yielding a training objective that combine unlabeled conditional entropy with labeled conditional likelihood although the training objective is no longer concave it can still be used to improve an initial model e g obtained from supervised training by iterative ascent we apply our new training algorithm to the problem of identifying gene and protein mention in biological text and show that incorporating unlabeled data improves the performance of the supervised crf in this case 
we address the issue of human subjectivity when authoring summary aiming at a simple robust evaluation of machine generated summary applying a cross comprehension test on human authored short summary from broadcast news the level of subjectivity is gauged among four author the instruction set is simple thus there is enough room for subjectivity however the approach is robust because the test doe not use the absolute score relying instead on relative comparison effectively alleviating the subjectivity finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summary 
the paper proposes a methodology for dealing with multiword expression in natural language processing application it provides a practically justified taxonomy of such unit and suggests the way in which the individual class can be processed computationally while the study is currently limited to polish and english we believe our finding can be successfully employed in the processing of other language with emphasis on inflectional one 
we propose a supervised two phase framework to address the problem of paraphrase recognition pr unlike most pr system that focus on sentence similarity our framework detects dissimilarity between sentence and make it paraphrase judgment based on the significance of such dissimilarity the ability to differentiate significant dissimilarity not only reveals what make two sentence a non paraphrase but also help to recall additional paraphrase that contain extra but insignificant information experimental result show that while being accurate at discerning non paraphrasing dissimilarity our implemented system is able to achieve higher paraphrase recall at an overall performance comparable to the alternative 
in this paper we investigate unsupervised name transliteration using comparable corpus corpus where text in the two language deal in some of the same topic and therefore share reference to named entity but are not translation of each other we present two distinct method for transliteration one approach using an unsupervised phonetic transliteration method and the other using the temporal distribution of candidate pair each of these approach work quite well but by combining the approach one can achieve even better result we believe that the novelty of our approach lie in the phonetic based scoring method which is based on a combination of carefully crafted phonetic feature and empirical result from the pronunciation error of second language learner of english unlike previous approach to transliteration this method can in principle work with any pair of language in the absence of a training dictionary provided one ha an estimate of the pronunciation of word in text 
we investigate the effect of corpus size in combining supervised and unsupervised learning for two type of attachment decision relative clause attachment and prepositional phrase attachment the supervised component is collins parser trained on the wall street journal the unsupervised component gather lexical statistic from an unannotated corpus of newswire text we find that the combined system only improves the performance of the parser for small training set surprisingly the size of the unannotated corpus ha little effect due to the noisiness of the lexical statistic acquired by unsupervised learning 
in this paper we propose a new context dependent smt model that is tightly coupled with a language model it is designed to decrease the translation ambiguity and efficiently search for an optimal hypothesis by reducing the hypothesis search space it work through reciprocal incorporation between source and target context a source word is determined by the context of previous and corresponding target word and the next target word is predicted by the pair consisting of the previous target word and it corresponding source word in order to alleviate the data sparseness in chunk based translation we take a stepwise back off translation strategy moreover in order to obtain more semantically plausible translation result we use bilingual verb noun collocation these are automatically extracted by using chunk alignment and a monolingual dependency parser a a case study we experimented on the language pair of japanese and korean a a result we could not only reduce the search space but also improve the performance 
various method have been proposed for automatic synonym acquisition a synonym are one of the most fundamental lexical knowledge whereas many method are based on contextual clue of word little attention ha been paid to what kind of category of contextual information are useful for the purpose this study ha experimentally investigated the impact of contextual information selection by extracting three kind of word relationship from corpus dependency sentence co occurrence and proximity the evaluation result show that while dependency and proximity perform relatively well by themselves combination of two or more kind of contextual information give more stable performance we ve further investigated useful selection of dependency relation and modification category and it is found that modification ha the greatest contribution even greater than the widely adopted subject object combination 
thesaurus and ontology provide important value in facilitating access to digital archive by representing underlying principle of organization translation of such resource into multiple language is an important component for providing multilingual access however the specificity of vocabulary term in most ontology precludes fully automated machine translation using general domain lexical resource in this paper we present an efficient process for leveraging human translation when constructing domain specific lexical resource we evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of concept used to catalogue a large archive of oral history our experiment demonstrate a cost effective technique for accurate machine translation of large ontology 
we present a novel classifier based deterministic parser for chinese constituency parsing our parser computes parse tree from bottom up in one pas and us classifier to make shift reduce decision trained and evaluated on the standard training and test set our best model using stacked classifier run in linear time and ha labeled precision and recall above using gold standard part of speech tag surpassing the best published result our svm parser is time faster than state of the art parser while producing more accurate result our maxent and dtree parser run at speed time faster than state of the art parser but with loss in accuracy 
lasso is a regularization method for parameter estimation in linear model it optimizes the model parameter with respect to a loss function subject to model complexity this paper explores the use of lasso for statistical language modeling for text input owing to the very large number of parameter directly optimizing the penalized lasso loss function is impossible therefore we investigate two approximation method the boosted lasso blasso and the forward stagewise linear regression fslr both method when used with the exponential loss function bear strong resemblance to the boosting algorithm which ha been used a a discriminative training method for language modeling evaluation on the task of japanese text input show that blasso is able to produce the best approximation to the lasso solution and lead to a significant improvement in term of character error rate over boosting and the traditional maximum likelihood estimation 
we propose a new method for reformatting web document by extracting semantic structure from web page our approach is to extract tree that describe hierarchical relation in document we developed an algorithm for this task by employing the em algorithm and clustering technique preliminary experiment showed that our approach wa more effective than baseline method 
we present an algorithm for automatically disambiguating noun noun compound by deducing the correct semantic relation between their constituent word this algorithm us a corpus of compound annotated with wordnet sens and covering different semantic relation we make this corpus available online for researcher interested in the semantics of noun noun compound the algorithm take a input the wordnet sens for the noun in a compound find all parent sens hypernym of those sens and search the corpus for other compound containing any pair of those sens the relation with the highest proportional co occurrence with any sense pair is returned a the correct relation for the compound this algorithm wa tested using a leave one out procedure on the corpus of compound the algorithm identified the correct relation for compound with high precision in of case where a relation wa found with a proportional co occurrence of it wa the correct relation for the compound being disambiguated 
human categorization is neither a binary nor a context free process rather some concept are better example of a category than others while the criterion for category membership may be satisfied to different degree by different concept in different context in light of these empirical fact wordnet s static category structure appears both excessively rigid and unduly fragile for processing real text in this paper we describe a syntagmatic corpus based approach to redefining wordnet s category in a functional gradable and context sensitive fashion we describe how the diagnostic property for these definition are automatically acquired from the web and how the increased flexibility in categorization that arises from these redefinition offer a robust account of metaphor comprehension in the mold of glucksberg s theory of category inclusion furthermore we demonstrate how this competence with figurative categorization can effectively be governed by automatically generated ontological constraint also acquired from the web 
this paper describes the latest version of speech to speech translation system developed by the team of nict atr for over twenty year the system is now ready to be deployed for the travel domain a new noise suppression technique notably improves speech recognition performance corpus based approach of recognition translation and synthesis enable coverage of a wide variety of topic and portability to other language 
automatic phrasing is essential to mandarin text to speech synthesis we select word format a target linguistic feature and propose an hmm based approach to this issue then we define four state of prosodic position for each word when employing a discrete hidden markov model the approach achieves high accuracy of roughly which is very close to that from manual labeling our experimental result also demonstrate that this approach ha advantage over those part of speech based one 
in this paper we explore correlation of dependency relation path to rank candidate answer in answer extraction using the correlation measure we compare dependency relation of a candidate answer and mapped question phrase in sentence with the corresponding relation in question different from previous study we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure the correlation are further incorporated into a maximum entropy based ranking model which estimate path weight from training experimental result show that our method significantly outperforms state of the art syntactic relation based method by up to in mrr 
automatically acquired lexicon with subcategorization information have already proved accurate and useful enough for some purpose but their accuracy still show room for improvement by mean of diathesis alternation this paper proposes a new filtering method which improved the performance of korhonen s acquisition system remarkably with the precision increased to and recall unchanged making the acquired lexicon much more practical for further manual proofreading and other nlp us 
previous study in data driven dependency parsing have shown that tree transformation can improve parsing accuracy for specific parser and data set we investigate to what extent this can be generalized across language treebanks and parser focusing on pseudo projective parsing a a way of capturing non projective dependency and transformation used to facilitate parsing of coordinate structure and verb group the result indicate that the beneficial effect of pseudo projective parsing is independent of parsing strategy but sensitive to language or treebank specific property by contrast the construction specific transformation appear to be more sensitive to parsing strategy but have a constant positive effect over several language 
integer linear programming ha recently been used for decoding in a number of probabilistic model in order to enforce global constraint however in certain application such a non projective dependency parsing and machine translation the complete formulation of the decoding problem a an integer linear program render solving intractable we present an approach which solves the problem incrementally thus we avoid creating intractable integer linear program this approach is applied to dutch dependency parsing and we show how the addition of linguistically motivated constraint can yield a significant improvement over state of the art 
data driven technique have been used for many computational linguistics task model derived from data are generally more robust than hand crafted system since they better reect the distribution of the phenomenon being modeled with the availability of large corpus of spoken dialog dialog management is now reaping the benet of data driven technique in this paper we compare two approach to modeling subtask structure in dialog a chunk based model of subdialog sequence and a parse based or hierarchical model we evaluate these model using customer agent dialog from a catalog service domain 
this paper examines two problem in document level sentiment analysis determining whether a given document is a review or not and classifying the polarity of a review a positive or negative we first demonstrate that review identification can be performed with high accuracy using only unigrams a feature we then examine the role of four type of simple linguistic knowledge source in a polarity classification system 
we report initial result on the relatively novel task of automatic classification of author personality using a corpus of personal weblogs or blog we investigate the accuracy that can be achieved when classifying author on four important personality trait we explore both binary and multiple classification using differing set of n gram feature result are promising for all four trait examined 
in this paper we present a tool that us comparable corpus to find appropriate translation equivalent for expression that are considered by translator a difficult for a phrase in the source language the tool identifies a range of possible expression used in similar context in target language corpus and present them to the translator a a list of suggestion in the paper we discus the method and present result of human evaluation of the performance of the tool which highlight it usefulness when dictionary solution are lacking 
we consider the problem of predictive inference for probabilistic binary sequence labeling model under f score a utility for a simple class of model we show that the number of hypothesis whose expected fscore need to be evaluated is linear in the sequence length and present a framework for efficiently evaluating the expectation of many common loss utility function including the f score this framework includes both exact and faster inexact calculation method 
in this paper we will present an efficient method to compute the co occurrence count of any pair of substring in a parallel corpus and an algorithm that make use of these count to create sub sentential alignment on such a corpus this algorithm ha the advantage of being a general a possible regarding the segmentation of text 
co occurrence analysis ha been used to determine related word or term in many nlp related application such a query expansion in information retrieval ir however related word are usually determined with respect to a single word without relevant information for it application context for example the word programming may be considered to be strongly related to java and applied inappropriately to expand a query on java travel to solve this problem we propose to add another context word in the relation to specify the appropriate context of the relation leading to term relation of the form java travel indonesia the extracted relation are used for query expansion in ir our experiment on several trec collection show that this new type of context dependent relation performs much better than the traditional co occurrence relation 
entity annotation involves attaching a label such a name or organization to a sequence of token in a document all the current rule based and machine learning based approach for this task operate at the document level we present a new and generic approach to entity annotation which us the inverse index typically created for rapid key word based searching of a document collection we define a set of operation on the inverse index that allows u to create annotation defined by cascading regular expression the entity annotation for an entire document corpus can be created purely of the index with no need to access the original document experiment on two publicly available data set show very significant performance improvement over the document based annotator 
given a parallel corpus semantic projection attempt to transfer semantic role annotation from one language to another typically by exploiting word alignment in this paper we present an improved method for obtaining constituent alignment between parallel sentence to guide the role projection task our extension are twofold a we model constituent alignment a minimum weight edge cover in a bipartite graph which allows u to find a globally optimal solution efficiently b we propose tree pruning a a promising strategy for reducing alignment noise experimental result on an english german parallel corpus demonstrate improvement over state of the art model 
the paper present the position specific posterior lattice a novel representation of automatic speech recognition lattice that naturally lends itself to efficient indexing of position information and subsequent relevance ranking of spoken document using proximity in experiment performed on a collection of lecture recording mit icampus data the spoken document ranking accuracy wa improved by relative over the commonly used baseline of indexing the best output from an automatic speech recognizer the mean average precision map increased from when using best output to when using the new lattice representation the reference used for evaluation is the output of a standard retrieval engine working on the manual transcription of the speech collection albeit lossy the pspl lattice is also much more compact than the asr gram lattice from which it is computed which translates in reduced inverted index size a well at virtually no degradation in word error rate performance since new path are introduced in the lattice the oracle accuracy increase over the original asr lattice 
on a multi dimensional text categorization task we compare the effectiveness of a feature based approach with the use of a state of the art sequential learning technique that ha proven successful for task such a email act classification our evaluation demonstrates for the three separate dimension of a well established annotation scheme that novel thread based feature have a greater and more consistent impact on classification performance 
this paper present an adaptive learning framework for phonetic similarity modeling psm that support the automatic construction of transliteration lexicon the learning algorithm start with minimum prior knowledge about machine transliteration and acquires knowledge iteratively from the web we study the active learning and the unsupervised learning strategy that minimize human supervision in term of data labeling the learning process refines the psm and construct a transliteration lexicon at the same time we evaluate the proposed psm and it learning algorithm through a series of systematic experiment which show that the proposed framework is reliably effective on two independent database 
discriminative reranking is one method for constructing high performance statistical parser collins a discriminative reranker requires a source of candidate par for each sentence this paper describes a simple yet novel method for constructing set of best par based on a coarse to fine generative parser charniak this method generates best list that are of substantially higher quality than previously obtainable we used these par a the input to a maxent reranker johnson et al riezler et al that selects the best parse from the set of par for each sentence obtaining an f score of on sentence of length or le 
both rhetorical structure and punctuation have been helpful in discourse processing based on a corpus annotation project this paper report the discursive usage of chinese punctuation mark in news commentary text colon dash ellipsis exclamation mark question mark and semicolon the rhetorical pattern of these mark are compared against pattern around cue phrase in general result show that these chinese punctuation mark though fewer in number than cue phrase are easy to identify have strong correlation with certain relation and can be used a distinctive indicator of nuclearity in chinese text 
in this paper we study the effect of different word level preprocessing decision for arabic on smt quality our result show that given large amount of training data splitting off only proclitics performs best however for small amount of training data it is best to apply english like tokenization using part of speech tag and sophisticated morphological analysis and disambiguation moreover choosing the appropriate preprocessing produce a significant increase in bleu score if there is a change in genre between training and test data 
we discus different strategy for smoothing the phrasetable in statistical mt and give result over a range of translation setting we show that any type of smoothing is a better idea than the relative frequency estimate that are often used the best smoothing technique yield consistent gain of approximately absolute according to the bleu metric 
there are some sort of preposition noun combination in farsi that apparently a prepositional phrase almost behaves a compound preposition a they are not completely behaving a compound it is doubtful that the process of word formation is a morphological one the analysis put forward by this paper proposes incorporation by which an n is incorporated to a p constructing a compound preposition in this way tagging preposition and parsing text in natural language processing is defined in a proper manner 
japanese case marker which indicate the grammatical relation of the complement np to the predicate often pose challenge to the generation of japanese text be it done by a foreign language learner or by a machine translation mt system in this paper we describe the task of predicting japanese case marker and propose machine learning method for solving it in two setting i monolingual when given information only from the japanese sentence and ii bilingual when also given information from a corresponding english source sentence in an mt context we formulate the task after the well studied task of english semantic role labelling and explore feature from a syntactic dependency structure of the sentence for the monolingual task we evaluated our model on the kyoto corpus and achieved over accuracy in assigning correct case marker for each phrase for the bilingual task we achieved an accuracy of per phrase using a bilingual dataset from a technical domain we show that in both setting feature that exploit dependency information whether derived from gold standard annotation or automatically assigned contribute significantly to the prediction of case marker 
a web search with double checking model is proposed to explore the web a a live corpus five association measure including variant of dice overlap ratio jaccard and cosine a well a co occurrence double check codc are presented in the experiment on rubenstein goodenough s benchmark data set the codc measure achieves correlation coefficient which competes with the performance of the model using wordnet the experiment on link detection of named entity using the strategy of direct association association matrix and scalar association matrix verify that the double check frequency are reliable further study on named entity clustering show that the five measure are quite useful in particular codc measure is very stable on word word and name name experiment the application of codc measure to expand community chain for personal name disambiguation achieves and increase compared to the system without community expansion all the experiment illustrate that the novel model of web search with double checking is feasible for mining association from the web 
this paper describes a summarization system for technical chat and email on the linux kernel to reflect the complexity and sophistication of the discussion they are clustered according to subtopic structure on the sub message level and immediate responding pair are identified through machine learning method a resulting summary consists of one or more mini summary each on a subtopic from the discussion 
senseclusters is a freely available system that identifies similar context in text it relies on lexical feature to build first and second order representation of context which are then clustered using unsupervised method it wa originally developed to discriminate among context centered around a given target word but can now be applied more generally it also support method that create descriptive and discriminating label for the discovered cluster 
word alignment method can gain valuable guidance by ensuring that their alignment maintain cohesion with respect to the phrase specified by a monolingual dependency tree however this hard constraint can also rule out correct alignment and it utility decrease a alignment model become more complex we use a publicly available structured output svm to create a max margin syntactic aligner with a soft cohesion constraint the resulting aligner is the first to our knowledge to use a discriminative learning method to train an itg bitext parser 
word and character bigram are both used a feature in chinese text processing task but no systematic comparison or analysis of their value a feature for chinese text categorization ha been reported heretofore we carry out here a full performance comparison between them by experiment on various document collection including a manually word segmented corpus a a golden standard and a semi quantitative analysis to elucidate the characteristic of their behavior and try to provide some preliminary clue for feature term choice in most case character bigram are better than word and dimensionality setting in text categorization system 
instant messaging chat session are real time text based conversation which can be analyzed using dialogue act model we describe a statistical approach for modelling and detecting dialogue act in instant messaging dialogue this involved the collection of a small set of task based dialogue and annotating them with a revised tag set we then dealt with segmentation and synchronisation issue which do not arise in spoken dialogue the model we developed combine naive bayes and dialogue act n gram to obtain better than accuracy in our tagging experiment 
large vocabulary continuous speech recognition of inflective language such a czech russian or serbo croatian is heavily deteriorated by excessive out of vocabulary rate in this paper we tackle the problem of vocabulary selection language modeling and pruning for inflective language we show that by explicit reduction of out of vocabulary rate we can achieve significant improvement in recognition accuracy while almost preserving the model size reported result are on czech speech corpus 
the penn treebank doe not annotate within base noun phrase np committing only to flat structure that ignore the complexity of english np this mean that tool trained on treebank data cannot learn the correct internal structure of np this paper detail the process of adding gold standard bracketing within each noun phrase in the penn treebank we then examine the consistency and reliability of our annotation finally we use this resource to determine np structure using several statistical approach thus demonstrating the utility of the corpus this add detail to the penn treebank that is necessary for many nlp application 
unsupervised learning of linguistic structure is a difficult problem a common approach is to define a generative model and maximize the probability of the hidden structure given the observed data typically this is done using maximum likelihood estimation mle of the model parameter we show using part of speech tagging that a fully bayesian approach can greatly improve performance rather than estimating a single set of parameter the bayesian approach integrates over all possible parameter value this difference ensures that the learned structure will have high probability over a range of possible parameter and permit the use of prior favoring the sparse distribution that are typical of natural language our model ha the structure of a standard trigram hmm yet it accuracy is closer to that of a state of the art discriminative model smith and eisner up to percentage point better than mle we find improvement both when training from data alone and using a tagging dictionary 
we present a hierarchical phrase based statistical machine translation in which a target sentence is efficiently generated in left to right order the model is a class of synchronous cfg with a greibach normal form like structure for the projected production rule the paired target side of a production rule take a phrase prefixed form the decoder for the target normalized form is based on an early style top down parser on the source side the target normalized form coupled with our top down parser implies a left to right generation of translation which enables u a straightforward integration with ngram language model our model wa experimented on a japanese to english newswire translation task and showed statistically significant performance improvement against a phrase based translation system 
automatic segmentation is important for making multimedia archive comprehensible and for developing downstream information retrieval and extraction module in this study we explore approach that can segment multiparty conversational speech by integrating various knowledge source e g word audio and video recording speaker intention and context in particular we evaluate the performance of a maximum entropy approach and examine the effectiveness of multimodal feature on the task of dialogue segmentation we also provide a quantitative account of the effect of using asr transcription a opposed to human transcript 
this paper show how to use the unfoldfold transformation to transform projective bilexical dependency grammar pbdgs into ambiguity preserving weakly equivalent context free grammar cfgs these cfgs can be parsed in o n time using a cky algorithm with appropriate indexing rather than the o n time required by a naive encoding informally using the cky algorithm with such a cfg mimic the step of the eisner satta o n pbdg parsing algorithm this transformation make all of the technique developed for cfgs available to pbdgs we demonstrate this by describing a maximum posterior parse decoder for pbdgs 
this paper present the particular use of jibiki papillon s web server development platform for the lexalp project lexalp s goal is to harmonise the terminology on spatial planning and sustainable development used within the alpine convention so that the member state are able to cooperate and communicate efficiently in the four official language french german italian and slovene to this purpose lexalp us the jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal system and four different language in this paper we present how a generic platform like jibiki can cope with a new kind of dictionary 
this paper present the use of support vector machine svm to detect relevant information to be included in a query focused summary several svms are trained using information from pyramid of summary content unit their performance is compared with the best performing system in duc using both rouge and autopan an automatic scoring method for pyramid evaluation 
we develop admissible a search heuristic for synchronous parsing with inversion transduction grammar and present result both for bitext alignment and for machine translation decoding we also combine the dynamic programming hook trick with a search for decoding these technique make it possible to find optimal alignment much more quickly and make it possible to find optimal translation for the first time even in the presence of pruning we are able to achieve higher bleu score with the same amount of computation 
we have constructed a corpus of news article in which event are annotated for estimated bound on their duration here we describe a method for measuring inter annotator agreement for these event duration distribution we then show that machine learning technique applied to this data yield coarse grained event duration information considerably outperforming a baseline and approaching human performance 
in this paper we present a learning approach for coreference resolution of noun phrase in unrestricted text the approach learns from a small annotated corpus and the task includes resolving not just pronoun but rather general noun phrase in contrast to previous work we attempt to evaluate our approach on a common data set the muc coreference corpus we obtained encouraging result indicating that on the general noun phrase coreference task the learning approach hold promise and achieves accuracy comparable to non learning approach 
in this paper we describe an empirical study of chinese chunking on a corpus which is extracted from upenn chinese treebank ctb first we compare the performance of the state of the art machine learning model then we propose two approach in order to improve the performance of chinese chunking we propose an approach to resolve the special problem of chinese chunking this approach extends the chunk tag for every problem by a tag extension function we propose two novel voting method based on the characteristic of chunking task compared with traditional voting method the proposed voting method consider long distance information the experimental result show that the svms model outperforms the other model and that our proposed approach can improve performance significantly 
supervised and semi supervised sense disambiguation method will mi tag the instance of a target word if the sens of these instance are not defined in sense inventory or there are no tagged instance for these sens in training data here we used a model order identification method to avoid the misclassification of the instance with undefined sens by discovering new sens from mixed data tagged and untagged corpus this algorithm try to obtain a natural partition of the mixed data by maximizing a stability criterion defined on the classification result from an extended label propagation algorithm over all the possible value of the number of sens or sense number model order experimental result on senseval data indicate that it outperforms svm a one class partially supervised classification algorithm and a clustering based model order identification algorithm when the tagged data is incomplete 
this paper introduces a method for computational analysis of move structure in abstract of research article in our approach sentence in a given abstract are analyzed and labeled with a specific move in light of various rhetorical function the method involves automatically gathering a large number of abstract from the web and building a language model of abstract move we also present a prototype concordancer care which exploit the move tagged abstract for digital learning this system provides a promising approach to web based computer assisted academic writing 
we present a version of inversion transduction grammar where rule probability are lexicalized throughout the synchronous parse tree along with pruning technique for efficient training alignment result improve over unlexicalized itg on short sentence for which full em is feasible but pruning seems to have a negative impact on longer sentence 
this paper regard question answering qa a question biased term extraction qbte this new qbte approach liberates qa system from the heavy burden imposed by question type or answer type in conventional approach a qa system analyzes a given question and determines the question type and then it selects answer from among answer candidate that match the question type consequently the output of a qa system is restricted by the design of the question type the qbte directly extract answer a term biased by the question to confirm the feasibility of our qbte approach we conducted experiment on the crl qa data based on fold cross validation using maximum entropy model mem a an ml technique experimental result showed that the trained system achieved in mrr and in top accuracy 
lexical class when tailored to the application and domain in question can provide an effective mean to deal with a number of natural language processing nlp task while manual construction of such class is difficult recent research show that it is possible to automatically induce verb class from cross domain corpus with promising accuracy we report a novel experiment where similar technology is applied to the important challenging domain of biomedicine we show that the resulting classification acquired from a corpus of biomedical journal article is highly accurate and strongly domain specific it can be used to aid bio nlp directly or a useful material for investigating the syntax and semantics of verb in biomedical text 
the trend in information retrieval system is from document to sub document retrieval such a sentence in a summarization system and word or phrase in question answering system despite this trend system continue to model language at a document level using the inverse document frequency idf in this paper we compare and contrast idf with inverse sentence frequency isf and inverse term frequency itf a direct comparison reveals that all language model are highly correlated however the average isf and itf value are and higher than idf all language model appeared to follow a power law distribution with a slope coefficient of for document and for sentence and term we conclude with an analysis of idf stability with respect to random journal and section partition of the full text scientific article in our experimental corpus 
the interpretation of temporal expression in text is an important constituent task for many practical natural language processing task including question answering information extraction and text summarisation although temporal expression have long been studied in the research literature it is only more recently with the impetus provided by exercise like the ace program that attention ha been directed to broad coverage implemented system in this paper we describe our approach to intermediate semantic representation in the interpretation of temporal expression 
this paper explores the use of two graph algorithm for unsupervised induction and tagging of nominal word sens based on corpus our main contribution is the optimization of the free parameter of those algorithm and it evaluation against publicly available gold standard we present a thorough evaluation comprising supervised and unsupervised mode and both lexical sample and all word task the result show that in spite of the information loss inherent to mapping the induced sens to the gold standard the optimization of parameter based on a small sample of noun carry over to all noun performing close to supervised system in the lexical sample task and yielding the second best wsd system for the senseval all word task 
we propose a novel reordering model for phrase based statistical machine translation smt that us a maximum entropy maxent model to predicate reordering of neighbor block phrase pair the model provides content dependent hierarchical phrasal reordering with generalization based on feature automatically learned from a real world bitext we present an algorithm to extract all reordering event of neighbor block from bilingual data in our experiment on chinese to english translation this maxent based reordering model obtains significant improvement in bleu score on the nist mt and iwslt task 
we revisit the idea of history based parsing and present a history based parsing framework that strives to be simple general and flexible we also provide a decoder for this probability model that is linear space optimal and anytime a parser based on this framework when evaluated on section of the penn tree bank compare favorably with other state of the art approach in term of both accuracy and speed 
in this paper we present word sense disambiguation wsd experiment on ten highly polysemous verb in chinese where significant performance improvement are achieved using rich linguistic feature our system performs significantly better and in some case substantially better than the baseline on all ten verb our result also demonstrate that feature extracted from the output of an automatic chinese semantic role labeling system in general benefited the wsd system even though the amount of improvement wa not consistent across the verb for a few verb semantic role information actually hurt wsd performance the inconsistency of feature performance is a general characteristic of the wsd task a ha been observed by others we argue that this result can be explained by the fact that word sens are partitioned along different dimension for different verb and the feature therefore need to be tailored to particular verb in order to achieve adequate accuracy on verb sense disambiguation 
sentiment classification seek to identify a piece of text according to it author s general feeling toward their subject be it positive or negative traditional machine learning technique have been applied to this problem with reasonable success but they have been shown to work well only when there is a good match between the training and test data with respect to topic this paper demonstrates that match with respect to domain and time is also important and present preliminary experiment with training data labeled with emoticon which ha the potential of being independent of domain topic and time 
this paper present an unsupervised topic identification method integrating linguistic and visual information based on hidden markov model hmms we employ hmms for topic identification wherein a state corresponds to a topic and various feature including linguistic visual and audio information are observed our experiment on two kind of cooking tv program show the effectiveness of our proposed method 
this paper describes a new method for computing lexical chain these are sequence of semantically related word that reflect a text s cohesive structure in contrast to previous method we are able to select chain based on their cohesive strength this is achieved by analyzing the connectivity in graph representing the lexical chain we show that the generated chain significantly improve performance of automatic text summarization and keyphrase indexing 
non verbal modality such a gesture can improve processing of spontaneous spoken language for example similar hand gesture tend to predict semantic similarity so feature that quantify gestural similarity can improve semantic task such a coreference resolution however not all hand movement are informative gesture psychological research ha shown that speaker are more likely to gesture meaningfully when their speech is ambiguous ideally one would attend to gesture only in such circumstance and ignore other hand movement we present conditional modality fusion which formalizes this intuition by treating the informativeness of gesture a a hidden variable to be learned jointly with the class label applied to coreference resolution conditional modality fusion significantly outperforms both early and late modality fusion which are current technique for modality combination 
this paper describes the first system for large scale acquisition of subcategorization frame scfs from english corpus data which can be used to acquire comprehensive lexicon for verb noun and adjective the system incorporates an extensive rulebased classifier which identifies verbal adjectival and nominal frame from grammatical relation grs output by a robust parser the system achieves state ofthe art performance on all three set 
we present an algorithm which creates a german ccgbank by translating the syntax graph in the german tiger corpus into ccg derivation tree the resulting corpus contains derivation covering of all complete sentence in tiger lexicon extracted from this corpus contain correct lexical entry for of all known token in unseen text 
for transliterating foreign word into chinese the pronunciation of a source word is spelled out with kanji character because kanji comprises ideogram an individual pronunciation may be represented by more than one character however because different kanji character convey different meaning and impression character must be selected carefully in this paper we propose a transliteration method that model both pronunciation and impression whereas existing method do not model impression given a source word and impression keywords related to the source word our method derives possible transliteration candidate and sort them according to their probability we evaluate our method experimentally 
convolution tree kernel ha shown promising result in semantic role classification however it only carry out hard matching which may lead to over fitting and le accurate similarity measure to remove the constraint this paper proposes a grammardriven convolution tree kernel for semantic role classification by introducing more linguistic knowledge into the standard tree kernel the proposed grammar driven tree kernel display two advantage over the previous one grammar driven approximate substructure matching and grammardriven approximate tree node matching the two improvement enable the grammardriven tree kernel explore more linguistically motivated structure feature than the previous one experiment on the conll srl shared task show that the grammardriven tree kernel significantly outperforms the previous non grammar driven one in srl moreover we present a composite kernel to integrate feature based and tree kernel based method experimental result show that the composite kernel outperforms the previously best reported method 
when a machine learning based named entity recognition system is employed in a new domain it performance usually degrades in this paper we provide an empirical study on the impact of training data size and domain information on the performance stability of named entity recognition model we present an informative sample selection method for building high quality and stable named entity recognition model across domain experimental result show that the performance of the named entity recognition model is enhanced significantly after being trained with these informative sample 
recent development in statistical modeling of various linguistic phenomenon have shown that additional feature give consistent performance improvement quite often improvement are limited by the number of feature a system is able to explore this paper describes a novel progressive training algorithm that selects feature from virtually unlimited feature space for conditional maximum entropy cme modeling experimental result in edit region identification demonstrate the benefit of the progressive feature selection pfs algorithm the pfs algorithm maintains the same accuracy performance a previous cme feature selection algorithm e g zhou et al when the same feature space are used when additional feature and their combination are used the pfs give relative improvement over the previously reported best result in edit region identification on switchboard corpus kahn et al which lead to a relative error reduction in parsing the switchboard corpus when gold edits are used a the upper bound 
opinion analysis is an important research topic in recent year however there are no common method to create evaluation corpus this paper introduces a method for developing opinion corpus involving multiple annotator the characteristic of the created corpus are discussed and the methodology to select more consistent testing collection and their corresponding gold standard are proposed under the gold standard an opinion extraction system is evaluated the experiment result show some interesting phenomenon 
we describe a method for incorporating syntactic information in statistical machine translation system the first step of the method is to parse the source language string that is being translated the second step is to apply a series of transformation to the parse tree effectively reordering the surface string on the source language side of the translation system the goal of this step is to recover an underlying word order that is closer to the target language word order than the original string the reordering approach is applied a a pre processing step in both the training and decoding phase of a phrase based statistical mt system we describe experiment on translation from german to english showing an improvement from bleu score for a baseline system to bleu score for the system with reordering a statistically significant improvement 
in this paper we investigate how to automatically determine if two document collection are written from different perspective by perspective we mean a point of view for example from the perspective of democrat or republican we propose a test of different perspective based on distribution divergence between the statistical model of two collection experimental result show that the test can successfully distinguish document collection of different perspective from other type of collection 
we present a perceptron style discriminative approach to machine translation in which large feature set can be exploited unlike discriminative reranking approach our system can take advantage of learned feature in all stage of decoding we first discus several challenge to error driven discriminative approach in particular we explore different way of updating parameter given a training example we find that making frequent but smaller update is preferable to making fewer but larger update then we discus an array of feature and show both how they quantitatively increase bleu score and how they qualitatively interact on specific example one particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process which ha previously been entirely heuristic 
this paper describes the development of questionbank a corpus of parse annotated question for i use in training parser employed in qa and ii evaluation of question parsing we present a series of experiment to investigate the effectiveness of questionbank a both an exclusive and supplementary training resource for a state of the art parser in parsing both question and non question test set we introduce a new method for recovering empty node and their antecedent capturing long distance dependency from parser output in cfg tree using lfg f structure reentrancies our main finding are i using questionbank training data improves parser performance to labelled bracketing f score an increase of almost over the baseline ii back testing experiment on non question data penn ii wsj section show that the retrained parser doe not suffer a performance drop on non question material iii ablation experiment show that the size of training material provided by questionbank is sufficient to achieve optimal result iv our method for recovering empty node capture long distance dependency in question from the atis corpus with high precision and low recall in summary questionbank provides a useful new resource in parser based qa research 
combining fine grained opinion information to produce opinion summary is important for sentiment analysis application toward that end we tackle the problem of source coreference resolution linking together source mention that refer to the same entity the partially supervised nature of the problem lead u to define and approach it a the novel problem of partially supervised clustering we propose and evaluate a new algorithm for the task of source coreference resolution that outperforms competitive baseline 
in this paper a method to incorporate linguistic information regarding single word and compound verb is proposed a a first step towards an smt model based on linguistically classified phrase by substituting these verb structure by the base form of the head verb we achieve a better statistical word alignment performance and are able to better estimate the translation model and generalize to unseen verb form during translation preliminary experiment for the english spanish language pair are performed and future research line are detailed 
this paper present a method for building genetic language taxonomy based on a new approach to comparing lexical form instead of comparing form cross linguistically a matrix of language internal similarity between form is calculated these matrix are then compared to give distance between language we argue that this coheres better with current thinking in linguistics and psycholinguistics an implementation of this approach called philologicon is described along with it application to dyen et al s ninety five wordlists from indo european language 
cl research began experimenting with massive xml tagging of text to answer question in trec in duc the experiment were extended into text summarization based on these experiment the knowledge management system km wa developed to combine these two capability and to serve a a unified basis for other type of document exploration km ha been extended to include web question answering both general and topic based summarization information extraction and document exploration the document exploration functionality includes identification of semantically similar concept and dynamic ontology creation a development of km ha continued user modeling ha become a key research issue how will different user want to use the information they identify 
twicpen is a terminology assistance system for reader of printed ie off line material in foreign language it consists of a hand held scanner and sophisticated parsing and translation software to provide reader a limited number of translation selected on the basis of a linguistic analysis of the whole scanned text fragment a phrase part of the sentence etc the use of a morphological and syntactic parser make it possible i to disambiguate to a large extent the word selected by the user and hence to drastically reduce the noise in the response and ii to handle expression compound collocation idiom often a major source of difficulty for non native reader the system exists for the following language pair english french french english german french and italian french 
we describe a new procedure for verifying acl property about finite state machine fsms using satisfiability sat solving we present an algorithm for converting acl conjecture into conjunctive normal form cnf which we then output and check with an external satisfiability solver the procedure is directly available a an acl proof request when the sat tool is successful a theorem is added to the acl system database a a lemma for use in future proof attempt when the sat tool is unsuccessful we use it output to construct a counter example to the original acl property 
this paper investigates conceptually and empirically the novel sense matching task which requires to recognize whether the sens of two synonymous word match in context we suggest direct approach to the problem which avoid the intermediate step of explicit word sense disambiguation and demonstrate their appealing advantage and stimulating potential for future research 
non sentential utterance e g short answer a in who came to the party peter are pervasive in dialogue a with other form of ellipsis the elided material is typically present in the context e g the question that a short answer answer we present a machine learning approach to the novel task of identifying fragment and their antecedent in multiparty dialogue we compare the performance of several learning algorithm using a mixture of structural and lexical feature and show that the task of identifying antecedent given a fragment can be learnt successfully f we discus why the task of identifying fragment is harder f and finally report on a combined task f 
we present a statistical phrase based translation model that us hierarchical phrase phrase that contain subphrases the model is formally a synchronous context free grammar but is learned from a bitext without any syntactic information thus it can be seen a a shift to the formal machinery of syntax based translation system without any linguistic commitment in our experiment using bleu a a metric the hierarchical phrase based model achieves a relative improvement of over pharaoh a state of the art phrase based system 
we present a search based approach to automatic surface realization given a corpus of domain sentence using heuristic search based on a statistical language model and a structure we introduce called an inheritance table we overgenerate a set of complete syntactic semantic tree that are consistent with the given semantic structure and have high likelihood relative to the language model these tree are then lexicalized linearized scored and ranked this model is being developed to generate real time navigation instruction 
this paper present a word support model wsm the wsm can effectively perform homophone selection and syllable word segmentation to improve chinese input system the experimental result show that the wsm is able to achieve tonal syllable input with four tone and toneless syllable input without four tone syllable to word stw accuracy of and respectively among the converted word and while applying the wsm a an adaptation processing together with the microsoft input method editor msime and an optimized bigram model the average tonal and toneless stw improvement are and respectively 
this paper describes an algorithm for propagating verb argument along lexical chain consisting of wordnet relation the algorithm creates verb argument structure using verbnet syntactic pattern in order to increase the coverage a larger set of verb sens were automatically associated with the existing pattern from verbnet the algorithm is used in an in house question answering system for re ranking the set of candidate answer test on factoid question from trec indicate that the algorithm improved the system performance by 
in this paper we address the issue of automatically assigning information status to discourse entity using an annotated corpus of conversational english and exploiting morpho syntactic and lexical feature we train a decision tree to classify entity introduced by noun phrase a old mediated or new we compare it performance with hand crafted rule that are mainly based on morpho syntactic feature and closely relate to the guideline that had been used for the manual annotation the decision tree model achieves an overall accuracy of significantly outperforming the hand crafted algorithm we also experiment with binary classification by collapsing in turn two of the three target class into one and retraining the model the highest accuracy achieved on binary classification is 
we present magead a morphological analyzer and generator for the arabic language family our work is novel in that it explicitly address the need for processing the morphology of the dialect magead performs an on line analysis to or generation from a root pattern feature representation it ha separate phonological and orthographic representation and it allows for combining morpheme from different dialect we present a detailed evaluation of magead 
we introduce a new multi threaded parsing algorithm on unification grammar designed specifically for multimodal interaction and noisy environment by lifting some traditional constraint namely those related to the ordering of constituent we overcome several difficulty of other system in this domain we also present several criterion used in this model to constrain the search process using dynamically loadable scoring function some early analysis of our implementation are discussed 
example based parsing ha already been proposed in literature in particular attempt are being made to develop technique for language pair where the source and target language are different e g direct projection algorithm hwa et al this enables one to develop parsed corpus for target language having fewer linguistic tool with the help of a resource rich source language the dpa algorithm work on the assumption of direct correspondence which simply mean that the relation between two word of the source language sentence can be projected directly between the corresponding word of the parallel target language sentence however we find that this assumption doe not hold good all the time this lead to wrong parsed structure of the target language sentence a a solution we propose an algorithm called pseudo dpa pdpa that can work even if direct correspondence assumption is not guaranteed the proposed algorithm work in a recursive manner by considering the embedded phrase structure from outermost level to the innermost the present work discus the pdpa algorithm and illustrates it with respect to english hindi language pair link grammar based parsing ha been considered a the underlying parsing scheme for this work 
work on the semantics of question ha argued that the relation between a question and it answer s can be cast in term of logical entailment in this paper we demonstrate how computational system designed to recognize textual entailment can be used to enhance the accuracy of current open domain automatic question answering q a system in our experiment we show that when textual entailment information is used to either filter or rank answer returned by a q a system accuracy can be increased by a much a overall 
this paper proposes a method for detecting error in article usage and singular plural usage based on the mass count distinction first it learns decision list from training data generated automatically to distinguish mass and count noun then in order to improve it performance it is augmented by feedback that is obtained from the writing of learner finally it detects error by applying rule to the mass count distinction experiment show that it achieves a recall of and a precision of and outperforms other method used for comparison when augmented by feedback 
this paper examines whether a learningbased coreference resolver can be improved using semantic class knowledge that is automatically acquired from a version of the penn treebank in which the noun phrase are labeled with their semantic class experiment on the ace test data show that a resolver that employ such induced semantic class knowledge yield a statistically significant improvement of in f measure over one that exploit heuristically computed semantic class knowledge in addition the induced knowledge improves the accuracy of common noun resolution by 
in this paper we describe a coreference resolution method that employ a classification and a clusterization phase in a novel way the clusterization is produced a a graph cutting algorithm in which node of the graph correspond to the mention of the text whereas the edge of the graph constitute the confidence derived from the coreference classification in experiment the graph cutting algorithm for coreference resolution called bestcut achieves state of the art performance 
we present a framework for word alignment based on log linear model all knowledge source are treated a feature function which depend on the source language sentence the target language sentence and possible additional variable log linear model allow statistical alignment model to be easily extended by incorporating syntactic information in this paper we use ibm model alignment probability po correspondence and bilingual dictionary coverage a feature our experiment show that log linear model significantly outperform ibm translation model 
partial cognate are pair of word in two language that have the same meaning in some but not all context detecting the actual meaning of a partial cognate in context can be useful for machine translation tool and for computer assisted language learning tool in this paper we propose a supervised and a semi supervised method to disambiguate partial cognate between two language french and english the method use only automatically labeled data therefore they can be applied for other pair of language a well we also show that our method perform well when using corpus from different domain 
this paper present a study on if and how automatically extracted keywords can be used to improve text categorization in summary we show that a higher performance a measured by micro averaged f measure on a standard text categorization collection is achieved when the full text representation is combined with the automatically extracted keywords the combination is obtained by giving higher weight to word in the full text that are also extracted a keywords we also present result for experiment in which the keywords are the only input to the categorizer either represented a unigrams or intact of these two experiment the unigrams have the best performance although neither performs a well a headline only 
in this paper we discus how to utilize the co occurrence of answer in building and automatic question answering system that answer a series of question on a specific topic in a batch mode experiment show that the answer to the many of the question in the series usually have a high degree of co occurrence in relevant document passage this feature sometimes can t be easily utilized in an automatic qa system which process question independently however it can be utilized in a qa system that process question in a batch mode we have used our pervious trec qa system a baseline and augmented it with new answer clustering and co occurrence maximization component to build the batch qa system the experiment result show that the qa system running under the batch mode get significant performance improvement over our baseline trec qa system 
there is little consensus on a standard experimental design for the compound interpretation task this paper introduces well motivated general desideratum for semantic annotation scheme and describes such a scheme for in context compound annotation accompanied by detailed publicly available guideline classification experiment on an open text dataset compare favourably with previously reported result and provide a solid baseline for future research 
in this paper we investigate chinese english name transliteration using comparable corpus corpus where text in the two language deal in some of the same topic and therefore share reference to named entity but are not translation of each other we present two distinct method for transliteration one approach using phonetic transliteration and the second using the temporal distribution of candidate pair each of these approach work quite well but by combining the approach one can achieve even better result we then propose a novel score propagation method that utilizes the co occurrence of transliteration pair within document pair this propagation method achieves further improvement over the best result from the previous step 
we present a computationally tractable account of the interaction between sentence marker and focus marking in somali somali a a cushitic language ha a basic pattern wherein a small core clause is preceded and in some case followed by a set of topic which provide scene seting information against which the core is interpreted some topic appear to carry a focus marker indicating that they are particularly salient we will outline a computationally tractable grammar for somali in which focus marking emerges naturally from a consideration of the use of a range of sentence marker 
we propose a new language learning model that learns a syntactic semantic grammar from a small number of natural language string annotated with their semantics along with basic assumption about natural language syntax we show that the search space for grammar induction is a complete grammar lattice which guarantee the uniqueness of the learned grammar 
short vowel and other diacritic are not part of written arabic script exception are made for important political and religious text and in script for beginning student of arabic script without diacritic have considerable ambiguity because many word with different diacritic pattern appear identical in a diacritic le setting we propose in this paper a maximum entropy approach for restoring diacritic in a document the approach can easily integrate and make effective use of diverse type of information the model we propose integrates a wide array of lexical segment based and part of speech tag feature the combination of these feature type lead to a state of the art diacritization model using a publicly available corpus ldc s arabic treebank part we achieve a diacritic error rate of a segment error rate and a word error rate of in case ending le setting we obtain a diacritic error rate of a segment error rate and a word error rate of 
this paper present a new approach based on equivalent pseudowords eps to tackle word sense disambiguation wsd in chinese language eps are particular artificial ambiguous word which can be used to realize unsupervised wsd a bayesian classifier is implemented to test the efficacy of the ep solution on senseval chinese test set the performance is better than state of the art result with an average f measure of the experiment verifies the value of ep for unsupervised wsd 
though both document summarization and keyword extraction aim to extract concise representation from document these two task have usually been investigated independently this paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document under the assumption that the summary and keywords of a document can be mutually boosted the approach can naturally make full use of the reinforcement between sentence and keywords by fusing three kind of relationship between sentence and word either homogeneous or heterogeneous experimental result show the effectiveness of the proposed approach for both task the corpus based approach is validated to work almost a well a the knowledge based approach for computing word semantics 
we describe an algorithm for a novel task disambiguating the pronoun you in conversation you can be generic or referential finding referential you is important for task such a addressee identification or extracting owner of action item our classifier achieves accuracy in two person conversation an initial study show promising performance even on more complex multi party meeting 
we present an unsupervised system that exploit linguistic knowledge resource namely english and german lexical database and the world wide web to identify english inclusion in german text we describe experiment with this system and the corpus which wa developed for this task we report the classification result of our system and compare them to the performance of a trained machine learner in a series of inand cross domain experiment 
in this paper we view coreference resolution a a problem of ranking candidate partition generated by different coreference system we propose a set of partition based feature to learn a ranking model for distinguishing good and bad partition our approach compare favorably to two state of the art coreference system when evaluated on three standard coreference data set 
this paper introduces a method for learning to find translation of a given source term on the web in the approach the source term is used a query and part of pattern to retrieve and extract translation in web page the method involves using a bilingual term list to learn source target surface pattern at runtime the given term is submitted to a search engine then the candidate translation are extracted from the returned summary and subsequently ranked based on the surface pattern occurrence count and transliteration knowledge we present a prototype called termmine that applies the method to translate term evaluation on a set of encyclopedia term show that the method significantly outperforms the state of the art online machine translation system 
short messaging service sm text behave quite differently from normal written text and have some very special phenomenon to translate sm text traditional approach model such irregularity directly in machine translation mt however such approach suffer from customization problem a tremendous effort is required to adapt the language model of the existing translation system to handle sm text style we offer an alternative approach to resolve such irregularity by normalizing sm text before mt in this paper we view the task of sm normalization a a translation problem from the sm language to the english language and we propose to adapt a phrase based statistical mt model for the task evaluation by fold cross validation on a parallel sm normalized corpus of sentence show that our method can achieve in bleu score against the baseline bleu score another experiment of translating sm text from english to chinese on a separate sm text corpus show that using sm normalization a mt preprocessing can largely boost sm translation performance from to in bleu score 
we present an implemented system for the resolution of it this and that in transcribed multi party dialog the system handle np anaphoric a well a discoursedeictic anaphor i e pronoun with vp antecedent selectional preference for np or vp antecedent are determined on the basis of corpus count our result show that the system performs significantly better than a recency based baseline 
in this paper we introduce an empirical approach to the semantic interpretation of superlative adjective we present a corpus annotated for superlative and propose an interpretation algorithm that us a wide coverage parser and produce semantic representation we achieve f score between and for detecting attributive superlative and an accuracy in the range of for determining the correct comparison set a far a we are aware this is the first automated approach to superlative for open domain text and question 
one of acl s most interesting feature is that it is executable so user can run the program that they verify and debug them during verification in fact the acl implementors have gone well out of their way to make sure acl program can be executed efficiently nevertheless acl doe not provide a framework for reasoning about the cost of function invocation this paper describes how such a framework can be added to acl by using acl macro and supporting code to access the prover state the approach is illustrated with a cost analysis of red black tree operation 
accurately representing synonymy using distributional similarity requires large volume of data to reliably represent infrequent word however the na ve nearest neighbour approach to comparing context vector extracted from large corpus scale poorly o n in the vocabulary size in this paper we compare several existing approach to approximating the nearest neighbour search for distributional similarity we investigate the trade off between efficiency and accuracy and find that sash houle and sakuma provides the best balance 
in this paper we describe a novel data structure for phrase based statistical machine translation which allows for the retrieval of arbitrarily long phrase while simultaneously using le memory than is required by current decoder implementation we detail the computational complexity and average retrieval time for looking up phrase translation in our suffix array based data structure we show how sampling can be used to reduce the retrieval time by order of magnitude with no loss in translation quality 
there are two decoding algorithm essential to the area of natural language processing one is the viterbi algorithm for linear chain model such a hmms or crfs the other is the cky algorithm for probabilistic context free grammar however task such a noun phrase chunking and relation extraction seem to fall between the two neither of them being the best fit ideally we would like to model entity and relation with two layer of label we present a tractable algorithm for exact inference over two layer of label and chunk with time complexity o n and provide empirical result comparing our model with linear chain model 
this paper present a novel application of alternating structure optimization aso to the task of semantic role labeling srl of noun predicate in nombank aso is a recently proposed linear multi task learning algorithm which extract the common structure of multiple task to improve accuracy via the use of auxiliary problem in this paper we explore a number of different auxiliary problem and we are able to significantly improve the accuracy of the nombank srl task using this approach to our knowledge our proposed approach achieves the highest accuracy published to date on the english nombank srl task 
this paper present a corpus study that explores the extent to which caption contribute to recognizing the intended message of an information graphic it then present an implemented graphic interpretation system that take into account a variety of communicative signal and an evaluation study showing that evidence obtained from shallow processing of the graphic s caption ha a significant impact on the system s success this work is part of a larger project whose goal is to provide sight impaired user with effective access to information graphic 
we describe a new sentence realization framework for text to text application this framework us idl expression a a representation formalism and a generation mechanism based on algorithm for intersecting idl expression with probabilistic language model we present both theoretical and empirical result concerning the correctness and efficiency of these algorithm 
we describe a new approach for synthetically combining the output of several different machine translation mt engine operating on the same input the goal is to produce a synthetic combination that surpasses all of the original system in translation quality our approach us the individual mt engine a black box and doe not require any explicit cooperation from the original mt system a decoding algorithm us explicit word match in conjunction with confidence estimate for the various engine and a trigram language model in order to score and rank a collection of sentence hypothesis that are synthetic combination of word from the various original engine the highest scoring sentence hypothesis is selected a the final output of our system experiment using several arabic to english system of similar quality show a substantial improvement in the quality of the translation output 
this paper describes ferret an interactive question answering q a system designed to address the challenge of integrating automatic q a application into real world environment ferret utilizes a novel approach to q a known a predictive questioning which attempt to identify the question and answer that user need by analyzing how a user interacts with a system while gathering information related to a particular scenario 
we are presenting a new hybrid alignment architecture for aligning bilingual linguistically annotated parallel corpus it is able to align simultaneously at paragraph sentence phrase and word level using statistical and heuristic cue along with linguistics based rule the system currently aligns english and german text and the linguistic annotation used cover po tag lemma and syntactic constitutents however a the system is highly modular we can easily adapt it to new language pair and other type of annotation the hybrid nature of the system allows experiment with a variety of alignment cue to find solution to word alignment problem like the correct alignment of rare word and multiwords or how to align despite syntactic difference between two language first performance test are promising and we are setting up a gold standard for a thorough evaluation of the system 
in order to effectively access the rapidly increasing range of medium content available in the home new kind of more natural interface are needed in this paper we explore the application of multimodal interface technology to searching and browsing a database of movie the resulting system allows user to access movie using speech pen remote control and dynamic combination of these modality an experimental evaluation with more than user is presented contrasting two variant of the system one combining speech with traditional remote control input and a second where the user ha a tablet display supporting speech and pen input 
this paper proposes an approach to improve word alignment for language with scarce resource using bilingual corpus of other language pair to perform word alignment between language l and l we introduce a third language l although only small amount of bilingual data are available for the desired language pair l l large scale bilingual corpus in l l and l l are available based on these two additional corpus and with l a the pivot language we build a word alignment model for l and l this approach can build a word alignment model for two language even if no bilingual corpus is available in this language pair in addition we build another word alignment model for l and l using the small l l bilingual corpus then we interpolate the above two model to further improve word alignment between l and l experimental result indicate a relative error rate reduction of a compared with the method only using the small bilingual corpus in l and l 
this paper investigates the use of sublexical unit a a solution to handling the complex morphology with productive derivational process in the development of a lexical functional grammar for turkish such sublexical unit make it possible to expose the internal structure of word with multiple derivation to the grammar rule in a uniform manner this in turn lead to more succinct and manageable rule further the semantics of the derivation can also be systematically reflected in a compositional way by constructing pred value on the fly we illustrate how we use sublexical unit for handling simple productive derivational morphology and more interesting case such a causativization etc which change verb valency our priority is to handle several linguistic phenomenon in order to observe the effect of our approach on both the c structure and the f structure representation and grammar writing leaving the coverage and evaluation issue aside for the moment 
event based summarization attempt to select and organize the sentence in a summary with respect to the event or the sub event that the sentence describe each event ha it own internal structure and meanwhile often relates to other event semantically temporally spatially causally or conditionally in this paper we define an event a one or more event term along with the named entity associated and present a novel approach to derive intraand interevent relevance using the information of internal association semantic relatedness distributional similarity and named entity clustering we then apply pagerank ranking algorithm to estimate the significance of an event for inclusion in a summary from the event relevance derived experiment on the duc test data show that the relevance of the named entity involved in event achieves better result when their relevance is derived from the event term they associate it also reveals that the topic specific relevance from document themselves outperforms the semantic relevance from a general purpose knowledge base like word net 
in this paper we propose a small set of lexical conceptual relation which allow to encode adjective in computational relational lexica in a principled and integrated way our main motivation come from the fact that adjective and certain class of verb related in a way or another with adjective do not have a satisfactory representation in this kind of lexica this is due to a great extent to the heterogeneity of their semantic and syntactic property we sustain that such property are mostly derived from the relation holding between adjective and other po accordingly our proposal is mainly concerned with the specification of appropriate cross po relation to encode adjective in lexica of the type considered here 
we have written a new record library for modelling fixed size array and linear memory our implementation provides fixnum optimized o log n read and writes from address n space is not allocated until location are used so large address space can be represented we do not use single threaded object or acl array which free the user from syntactic restriction and slow array warning finally we can prove the same hypothesis free rewrite rule found in misc record for efficient rewriting during theorem proving 
recent work by nerbonne and wiersma ha provided a foundation for measuring syntactic difference between corpus it us part of speech trigram a an approximation to syntactic structure comparing the trigram of two corpus for statistically significant difference this paper extends the method and it application it extends the method by using leaf path ancestor of sampson instead of trigram which capture internal syntactic structure every leaf in a parse tree record the path back to the root the corpus used for testing is the international corpus of english great britain nelson et al which contains syntactically annotated speech of great britain the speaker are grouped into geographical region based on place of birth this is different in both nature and number than previous experiment which found difference between two group of norwegian l learner of english we show that dialectal variation in eleven british region from the ice gb is detectable by our algorithm using both leaf ancestor path and trigram 
this paper describes adaptation of unsupervised word sense discrimination technique to the problem of name discrimination these method cluster the context containing an ambiguous name such that each cluster refers to a unique underlying person or place we also present new technique to assign meaningful label to the discovered cluster 
we propose a computational model of text reuse tailored for ancient literary text available to u often only in small and noisy sample the model take into account source alternation pattern so a to be able to align even sentence with low surface similarity we demonstrate it ability to characterize text reuse in the greek new testament 
abstract in this paper we propose a novel discriminative language model which can be applied quite generally compared to the well known n gram language model discriminative language model can achieve more accurate discrimination because they can employ overlapping feature and nonlocal information however discriminative language model have been used only for re ranking in specific application because negative example are not available we propose sampling pseudo negative example taken from probabilistic language model however this approach requires prohibitive computational cost if we are dealing with quite a few feature and training sample we tackle the problem by estimating the latent information in sentence using a semimarkov class model and then extracting feature from them we also use an online margin based algorithm with efficient kernel computation experimental result 
in this paper we describe our experience in extending a standard cross language information retrieval clir approach which us parallel aligned corpus and latent semantic indexing most if not all previous work which follows this approach ha focused on bilingual retrieval two example involve the use of frenchenglish or english greek parallel corpus our extension to the approach is massively parallel in two sens one linguistic and the other computational first we make use of a parallel aligned corpus consisting of almost parallel translation in over distinct language each in over document given the size of this dataset a massively parallel approach wa also necessitated in the more usual computational sense our result indicate that far from adding more noise more linguistic parallelism is better when it come to cross language retrieval precision in addition to the self evident benefit that clir can be performed on more language 
we developed a new method of transforming japanese case particle when transforming japanese passive sentence into active sentence it separate training data into each input particle and us machine learning for each particle we also used numerous rich feature for learning our method obtained a high rate of accuracy in contrast a method that did not separate training data for any input particle obtained a lower rate of accuracy in addition a method that did not have many rich feature for learning used in a previous study murata and isahara obtained a much lower accuracy rate we confirmed that these improvement were significant through a statistical test we also conducted experiment utilizing traditional method using verb dictionary and manually prepared heuristic rule and confirmed that our method obtained much higher accuracy rate than traditional method 
in this paper we present a novel training method for a localized phrase based prediction model for statistical machine translation smt the model predicts block with orientation to handle local phrase re ordering we use a maximum likelihood criterion to train a log linear block bigram model which us real valued feature e g a language model score a well a binary feature based on the block identity themselves e g block bigram feature our training algorithm can easily handle million of feature the best system obtains a improvement over the baseline on a standard arabic english translation task 
most current machine transliteration system employ a corpus of known sourcetarget word pair to train their system and typically evaluate their system on a similar corpus in this paper we explore the performance of transliteration system on corpus that are varied in a controlled way in particular we control the number and prior language knowledge of human transliterators used to construct the corpus and the origin of the source word that make up the corpus we find that the word accuracy of automated transliteration system can vary by up to in absolute term depending on the corpus on which they are run we conclude that at least four human transliterators should be used to construct corpus for evaluating automated transliteration system and that although absolute word accuracy metric may not translate across corpus the relative ranking of system performance remains stable across differing corpus 
this paper proposes an unsupervised lexicon building method for the detection of polar clause which convey positive or negative aspect in a specific domain the lexical entry to be acquired are called polar atom the minimum human understandable syntactic structure that specify the polarity of clause a a clue to obtain candidate polar atom we use context coherency the tendency for same polarity to appear successively in context using the overall density and precision of coherency in the corpus the statistical estimation pick up appropriate polar atom among candidate without any manual tuning of the threshold value the experimental result show that the precision of polarity assignment with the automatically acquired lexicon wa on average and our method is robust for corpus in diverse domain and for the size of the initial lexicon 
we present a new approach for mapping natural language sentence to their formal meaning representation using string kernel based classifier our system learns these classifier for every production in the formal language grammar meaning representation for novel natural language sentence are obtained by finding the most probable semantic parse using these string classifier our experiment on two real world data set show that this approach compare favorably to other existing system and is particularly robust to noise 
sentence compression is the task of producing a summary at the sentence level this paper focus on three aspect of this task which have not received detailed treatment in the literature training requirement scalability and automatic evaluation we provide a novel comparison between a supervised constituent based and an weakly supervised word based compression algorithm and examine how these model port to different domain written v spoken text to achieve this a human authored compression corpus ha been created and our study highlight potential problem with the automatically gathered compression corpus currently used finally we ass whether automatic evaluation measure can be used to determine compression quality 
this paper present a pilot study of the use of phrasal statistical machine translation smt technique to identify and correct writing error made by learner of english a a second language esl using example of mass noun error found in the chinese learner error corpus clec to guide creation of an engineered training set we show that application of the smt paradigm can capture error not well addressed by widely used proofing tool designed for native speaker our system wa able to correct of mistake in a set of naturally occurring example of mass noun error found on the world wide web suggesting that effort to collect alignable corpus of preand post editing esl writing sample offer can enable the development of smt based writing assistance tool capable of repairing many of the complex syntactic and lexical problem found in the writing of esl learner 
with performance above accuracy for newspaper text part of speech po tagging might be considered a solved problem previous study have shown that allowing the parser to resolve po tag ambiguity doe not improve performance however for grammar formalism which use more fine grained grammatical category for example tag and ccg tagging accuracy is much lower in fact for these formalism premature ambiguity resolution make parsing infeasible we describe a multi tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing we extend this multi tagging approach to the po level to overcome error introduced by automatically assigned po tag although po tagging accuracy seems high maintaining some po tag ambiguity in the language processing pipeline result in more accurate ccg supertagging 
we discus image sense discrimination isd and apply a method based on spectral clustering using multimodal feature from the image and text of the embedding web page we evaluate our method on a new data set of annotated web image retrieved with ambiguous query term experiment investigate different level of sense granularity a well a the impact of text and image feature and global versus local text feature 
we present a minimum bayes risk mbr decoder for statistical machine translation the approach aim to minimize the expected loss of translation error with regard to the bleu score we show that mbr decoding on n best list lead to an improvement of translation quality we report the performance of the mbr decoder on four different task the tc star epps spanish english task the nist chinese english task and the gale arabic english and chinese english task the absolute improvement of the bleu score is between for the tc star task and for the gale chinese english task 
this paper present a corpus based account of structural priming in human sentence processing focusing on the role that syntactic representation play in such an account we estimate the strength of structural priming effect from a corpus of spontaneous spoken dialogue annotated syntactically with combinatory categorial grammar ccg derivation this methodology allows u to test a range of prediction that ccg make about priming in particular we present evidence for priming between lexical and syntactic category encoding partially satisfied sub categorization frame and we show that priming effect exist both for incremental and normal form ccg derivation 
this paper is devoted to the expression for a formal theory of communication network in the acl logic more precisely we have developed a generic model called genoc in a general mathematical notation with the use of quantification over variable a well a over function we present here it expression in the first order quantifier free logic of the acl theorem prover we describe our systematic approach to cast it into acl especially how we use the encapsulation principle to obtain a systematic methodology to specify and validate on chip communication architecture we summarize the different instance of genoc developed so far in acl some come from industrial design we illustrate our approach on an xy routing algorithm 
this paper describes a novel system for acquiring adjectival subcategorization frame scfs and associated frequency information from english corpus data the system incorporates a decision tree classifier for scf type which test for the presence of grammatical relation grs in the output of a robust statistical parser it us a powerful pattern matching language to classify grs into frame hierarchically in a way that mirror inheritance based lexica the experiment show that the system is able to detect scf type with precision and recall rate a new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition 
the ability to compress sentence while preserving their grammaticality and most of their meaning ha recently received much attention our work view sentence compression a an optimisation problem we develop an integer programming formulation and infer globally optimal compression in the face of linguistically motivated constraint we show that such a formulation allows for relatively simple and knowledge lean compression model that do not require parallel corpus or large scale resource the proposed approach yield result comparable and in some case superior to state of the art 
this paper present the result of automatically inducing a combinatory categorial grammar ccg lexicon from a turkish dependency treebank the fact that turkish is an agglutinating free wordorder language present a challenge for language theory we explored possible way to obtain a compact lexicon consistent with ccg principle from a treebank which is an order of magnitude smaller than penn wsj 
processing discourse connective is important for task such a discourse parsing and generation for these task it is useful to know which connective can signal the same coherence relation this paper present experiment into modelling the substitutability of discourse connective it show that substitutability effect distributional similarity a novel variance based function for comparing probability distribution is found to assist in predicting substitutability 
shortage of manually labeled data is an obstacle to supervised relation extraction method in this paper we investigate a graph based semi supervised learning algorithm a label propagation lp algorithm for relation extraction it represents labeled and unlabeled example and their distance a the node and the weight of edge of a graph and try to obtain a labeling function to satisfy two constraint it should be fixed on the labeled node it should be smooth on the whole graph experiment result on the ace corpus showed that this lp algorithm achieves better performance than svm when only very few labeled example are available and it also performs better than bootstrapping for the relation extraction task 
in this paper we present how the automatic extraction of event from text can be used to both classify narrative text according to plot quality and produce advice in an interactive learning environment intended to help student with story writing we focus on the story rewriting task in which an exemplar story is read to the student and the student rewrite the story in their own word the system automatically extract event from the raw text formalized a a sequence of temporally ordered predicate argument these event are given to a machine learner that produce a coarse grained rating of the story the result of the machine learner and the extracted event are then used to generate fine grained advice for the student 
a hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structure for semantic role labeling srl the hybrid kernel consists of two individual convolution kernel a path kernel which capture predicate argument link feature and a constituent structure kernel which capture the syntactic structure feature of argument evaluation on the datasets of conll srl shared task show that the novel hybrid convolution tree kernel out performs the previous tree kernel we also combine our new hybrid tree kernel based method with the standard rich flat feature based method the experimental result show that the combinational method can get better performance than each of them individually 
this paper explores method for generating subjectivity analysis resource in a new language by leveraging on the tool and resource available in english given a bridge between english and the selected target language e g a bilingual dictionary or a parallel corpus the method can be used to rapidly create tool for subjectivity analysis in the new language 
we investigate the utility of supertag information for guiding an existing dependency parser of german using weighted constraint to integrate the additionally available information the decision process of the parser is influenced by changing it preference without excluding alternative structural interpretation from being considered the paper report on a series of experiment using varying model of supertags that significantly increase the parsing accuracy in addition an upper bound on the accuracy that can be achieved with perfect supertags is estimated 
developing better method for segmenting continuous text into word is important for improving the processing of asian language and may shed light on how human learn to segment speech we propose two new bayesian word segmentation method that assume unigram and bigram model of word dependency respectively the bigram model greatly outperforms the unigram model and previous probabilistic model demonstrating the importance of such dependency for word segmentation we also show that previous probabilistic model rely crucially on sub optimal search procedure 
we describe a dialogue system that work with it interlocutor to identify object our contribution include a concise modular architecture with reversible process of understanding and generation an information state model of reference and flexible link between semantics and collaborative problem solving 
this paper present a hybrid approach to question answering in the clinical domain that combine technique from summarization and information retrieval we tackle a frequently occurring class of question that take the form what is the best drug treatment for x starting from an initial set of medline citation our system first identifies the drug under study abstract are then clustered using semantic class from the umls ontology finally a short extractive summary is generated for each abstract to populate the cluster two evaluation a manual one focused on short answer and an automatic one focused on the supporting abstract demonstrate that our system compare favorably to pubmed the search system most widely used by physician today 
we describe an embedding of the acl logic into higher order logic an implementation of this embedding allows acl to be used a an oracle for higher order logic provers 
efficient decoding ha been a fundamental problem in machine translation especially with an integrated language model which is essential for achieving good translation quality we develop faster approach for this problem based on k best parsing algorithm and demonstrate their effectiveness on both phrase based and syntax based mt system in both case our method achieve significant speed improvement often by more than a factor of ten over the conventional beam search method at the same level of search error and translation accuracy 
this paper present an empirical evaluation of the quality of publicly available large scale knowledge resource the study includes a wide range of manually and automatically derived large scale knowledge resource in order to establish a fair and neutral comparison the quality of each knowledge resource is indirectly evaluated using the same method on a word sense disambiguation task the evaluation framework selected ha been the senseval english lexical sample task the study empirically demonstrates that automatically acquired knowledge resource surpass both in term of precision and recall the knowledge resource derived manually and that the combination of the knowledge contained in these resource is very close to the most frequent sense classifier a far a we know this is the first time that such a quality assessment ha been performed showing a clear picture of the current state of the art of publicly available wide coverage semantic resource 
we present an api developed to access germanet a lexical semantic database for german represented in xml the api provides a set of software function for parsing and retrieving information from germanet then we present a case study which build upon the germanet api and implement an application for computing semantic relatedness according to five different metric the package can again serve a a software library to be deployed in natural language processing application a graphical user interface allows to interactively experiment with the system 
creating large amount of annotated data to train statistical pcfg parser is expensive and the performance of such parser decline when training and test data are taken from different domain in this paper we use selftraining in order to improve the quality of a parser and to adapt it to a different domain using only small amount of manually annotated seed data we report significant improvement both when the seed and test data are in the same domain and in the outof domain adaptation scenario in particular we achieve reduction in annotation cost for the in domain case yielding an improvement of over previous work and a reduction for the domain adaptation case this is the first time that self training with small labeled datasets is applied successfully to these task we were also able to formulate a characterization of when selftraining is valuable 
a distributional method for part of speech induction is presented which in contrast to most previous work determines the part of speech distribution of syntactically ambiguous word without explicitly tagging the underlying text corpus this is achieved by assuming that the word pair consisting of the left and right neighbor of a particular token is characteristic of the part of speech at this position and by clustering the neighbor pair on the basis of their middle word a observed in a large corpus the result obtained in this way are evaluated by comparing them to the part of speech distribution a found in the manually tagged brown corpus 
name tagging is a critical early stage in many natural language processing pipeline in this paper we analyze the type of error produced by a tagger distinguishing name classification and various type of name identification error we present a joint inference model to improve chinese name tagging by incorporating feedback from subsequent stage in an information extraction pipeline name structure parsing cross document coreference semantic relation extraction and event extraction we show through example and performance measurement how different stage can correct different type of error the resulting accuracy approach that of individual human annotator 
syntax based statistical machine translation mt aim at applying statistical model to structured data in this paper we present a syntax based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar synchronous dependency insertion grammar are a version of synchronous grammar defined on dependency tree we first introduce our approach to inducing such a grammar from parallel corpus second we describe the graphical model for the machine translation task which can also be viewed a a stochastic tree to tree transducer we introduce a polynomial time decoding algorithm for the model we evaluate the output of our mt system using the nist and bleu automatic mt evaluation software the result show that our system outperforms the baseline system based on the ibm model in both translation speed and quality 
in this paper we investigate a novel method to detect asymmetric entailment relation between verb our starting point is the idea that some point wise verb selectional preference carry relevant semantic information experiment using word net a a gold standard show promising result where applicable our method used in combination with other approach significantly increase the performance of entailment detection a combined approach including our model improves the aroc of absolute point with respect to standard model 
topic segmentation and identification are often tackled a separate problem whereas they are both part of topic analysis in this article we study how topic identification can help to improve a topic segmenter based on word reiteration we first present an unsupervised method for discovering the topic of a text then we detail how these topic are used by segmentation for finding topical similarity between text segment finally we show through the result of an evaluation done both for french and english the interest of the method we propose 
we exploit the resource in the arabic treebank atb and arabic gigaword ag to determine the best feature for the novel task of automatically creating lexical semantic verb class for modern standard arabic msa the verb are classified into group that share semantic element of meaning a they exhibit similar syntactic behavior the result of the clustering experiment are compared with a gold standard set of class which is approximated by using the noisy english translation provided in the atb to create levin like class for msa the quality of the cluster is found to be sensitive to the inclusion of syntactic frame lsa vector morphological pattern and subject animacy the best set of parameter yield an f score of compared to a random baseline of an f score of 
we describe an automatic word sense disambiguation wsd system that disambiguates verb sens using syntactic and semantic feature that encode information about predicate argument and semantic class our system performs at the best published accuracy on the english verb of senseval we also experiment with using the gold standard predicate argument label from propbank for disambiguating fine grained wordnet sens and course grained propbank framesets and show that disambiguation of verb sens can be further improved with better extraction of semantic role 
this paper describes a reader based experiment on lexical cohesion detailing the task given to reader and the analysis of the experimental data we conclude with discussion of the usefulness of the data in future research on lexical cohesion 
clarification request cr in conversation ensure and maintain mutual understanding and thus play a crucial role in robust dialogue interaction in this paper we describe a corpus study of cr in task oriented dialogue and compare our finding to those reported in two prior study we find that cr behavior in task oriented dialogue differs significantly from that in everyday conversation in a number of way moreover the dialogue type the modality and the channel quality all influence the decision of when to clarify and at which level of the grounding process finally we identify form function correlation which can inform the generation of cr 
this paper present a status quo of an ongoing research study of collocation an essential linguistic phenomenon having a wide spectrum of application in the field of natural language processing the core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction method using precision recall measure and a proposal of a new approach integrating multiple basic method and statistical classification we demonstrate that combining multiple independent technique lead to a significant performance improvement in comparison with individual basic method 
this article describes a robust semantic parser that us a broad knowledge base created by interconnecting three major resource framenet verbnet and propbank the framenet corpus contains the example annotated with semantic role whereas the verbnet lexicon provides the knowledge about the syntactic behavior of the verb we connect verbnet and framenet by mapping the framenet frame to the verbnet intersective levin class the propbank corpus which is tightly connected to the verbnet lexicon is used to increase the verb coverage and also to test the effectiveness of our approach the result indicate that our model is an interesting step towards the design of more robust semantic parser 
we demonstrate a system for flexible querying against text that ha been annotated with the result of nlp processing the system support self overlapping and parallel layer integration of syntactic and ontological hierarchy flexibility in the format of returned result and tight integration with sql we present a query language and it use on example taken from the nlp literature 
we present an approach to using a morphological analyzer for tokenizing and morphologically tagging including part of speech tagging arabic word in one process we learn classifier for individual morphological feature a well a way of using these classifier to choose among entry from the output of the analyzer we obtain accuracy rate on all task in the high ninety 
in evaluating the output of language technology application mt natural language generation summarisation automatic evaluation technique generally conflate measurement of faithfulness to source content with fluency of the resulting text in this paper we develop an automatic evaluation metric to estimate fluency alone by examining the use of parser output a metric and show that they correlate with human judgement of generated text fluency we then develop a machine learner based on these and show that this performs better than the individual parser metric approaching a lower bound on human performance we finally look at different language model for generating sentence and show that while individual parser metric can be fooled depending on generation method the machine learner provides a consistent estimator of fluency 
we introduce a framework for syntactic parsing with latent variable based on a form of dynamic sigmoid belief network called incremental sigmoid belief network we demonstrate that a previous feed forward neural network parsing model can be viewed a a coarse approximation to inference with this class of graphical model by constructing a more accurate but still tractable approximation we significantly improve parsing accuracy suggesting that isbns provide a good idealization for parsing this generative model of parsing achieves state of theart result on wsj text and error reduction over the baseline neural network parser 
the end to end performance of natural language processing system for compound task such a question answering and textual entailment is often hampered by use of a greedy best pipeline architecture which cause error to propagate and compound at each stage we present a novel architecture which model these pipeline a bayesian network with each low level task corresponding to a variable in the network and then we perform approximate inference to find the best labeling our approach is extremely simple to apply but gain the benefit of sampling the entire distribution over label at each stage in the pipeline we apply our method to two task semantic role labeling and recognizing textual entailment and achieve useful performance gain from the superior pipeline architecture 
the linguist s search engine lse wa designed to provide an intuitive easy to use interface that enables language researcher to seek linguistically interesting example on the web based on syntactic and lexical criterion we briefly describe it user interface and architecture a well a recent development that include lse search capability for chinese 
the integration of sophisticated inference based technique into natural language processing application first requires a reliable method of encoding the predicate argument structure of the propositional content of text recent statistical approach to automated predicate argument annotation have utilized parse tree path a predictive feature which encode the path between a verb predicate and a node in the parse tree that governs it argument in this paper we explore a number of alternative for how these parse tree path are encoded focusing on the difference between automatically generated constituency par and dependency par after describing five alternative for encoding parse tree path we investigate how well each can be aligned with the argument substring in annotated text corpus their relative precision and recall performance and their comparative learning curve result indicate that constituency parser produce parse tree path that can more easily be aligned to argument substring perform better in precision and recall and have more favorable learning curve than those produced by a dependency parser 
statistical machine translation system are usually trained on large amount of bilingual text and monolingual text in the target language in this paper we explore the use of transductive semi supervised method for the effective use of monolingual data from the source language in order to improve translation quality we propose several algorithm with this aim and present the strength and weakness of each one we present detailed experimental evaluation on the french english europarl data set and on data from the nist chinese english largedata track we show a significant improvement in translation quality on both task 
statistical parser trained and tested on the penn wall street journal wsj treebank have shown vast improvement over the last year much of this improvement however is based upon an ever increasing number of feature to be trained on typically the wsj treebank data this ha led to concern that such parser may be too finely tuned to this corpus at the expense of portability to other genre such worry have merit the standard charniak parser check in at a labeled precision recall f measure of on the penn wsj test set but only on the test set from the brown treebank corpus this paper should allay these fear in particular we show that the reranking parser described in charniak and johnson improves performance of the parser on brown to furthermore use of the self training technique described in mcclosky et al raise this to an error reduction of again without any use of labeled brown data this is remarkable since training the parser and reranker on labeled brown data achieves only 
we present a method for unsupervised topic modelling which adapts method used in document classification blei et al griffith and steyvers to unsegmented multi party discourse transcript we show how bayesian inference in this generative model can be used to simultaneously address the problem of topic segmentation and topic identification automatically segmenting multi party meeting into topically coherent segment with performance which compare well with previous unsupervised segmentation only method galley et al while simultaneously extracting topic which rate highly when assessed for coherence by human judge we also show that this method appears robust in the face of off topic dialogue and speech recognition error 
this paper compare different measure of graphemic similarity applied to the task of bilingual lexicon induction between a swiss german dialect and standard german the measure have been adapted to this particular language pair by training stochastic transducer with the expectation maximisation algorithm or by using handmade transduction rule these adaptive metric show up to f measure improvement over a static metric like levenshtein distance 
this paper address two remaining challenge in chinese word segmentation the challenge in hlt is to find a robust segmentation method that requires no prior lexical knowledge and no extensive training to adapt to new type of data the challenge in modelling human cognition and acquisition it to segment word efficiently without using knowledge of wordhood we propose a radical method of word segmentation to meet both challenge the most critical concept that we introduce is that chinese word segmentation is the classification of a string of character boundary cb s into either word boundary wb s and non word boundary in chinese cb s are delimited and distributed in between two character hence we can use the distributional property of cb among the background character string to predict which cb s are wb s 
most information extraction system either use hand written extraction pattern or use a machine learning algorithm that is trained on a manually annotated corpus both of these approach require massive human effort and hence prevent information extraction from becoming more widely applicable in this paper we present ures unsupervised relation extraction system which extract relation from the web in a totally unsupervised way it take a input the description of the target relation which include the name of the predicate the type of their attribute and several seed instance of the relation then the system downloads from the web a large collection of page that are likely to contain instance of the target relation from those page utilizing the known seed instance the system learns the relation pattern which are then used for extraction we present several experiment in which we learn pattern and extract instance of a set of several common ie relation comparing several pattern learning and filtering setup we demonstrate that using simple noun phrase tagger is sufficient a a base for accurate pattern however having a named entity recognizer which is able to recognize the type of the relation attribute significantly enhances the extraction performance we also compare our approach with knowitall s fixed generic pattern 
a bloom filter bf is a randomised data structure for set membership query it space requirement are significantly below lossless information theoretic lower bound but it produce false positive with some quantifiable probability here we explore the use of bfs for language modelling in statistical machine translation we show how a bf containing n gram can enable u to use much larger corpus and higher order model complementing a conventional n gram lm within an smt system we also consider i how to include approximate frequency information efficiently within a bf and ii how to reduce the error rate of these model by first checking for lower order sub sequence in candidate ngrams our solution in both case retain the one sided error guarantee of the bf while takingadvantageof thezipf likedistribution of word frequency to reduce the space requirement 
statistical ranking method based on centroid vector profile extracted from external knowledge have become widely adopted in the top definitional qa system in trec and in these approach term in the centroid vector are treated a a bag of word based on the independent assumption to relax this assumption this paper proposes a novel language model based answer reranking method to improve the existing bag of word model approach by considering the dependence of the word in the centroid vector experiment have been conducted to evaluate the different dependence model the result on the trec test set show that the reranking approach with biterm language model significantly outperforms the one with the bag of word model and unigram language model by and respectively in f measure 
we describe a probabilistic approach to content selection for meeting summarization we use skipchain conditional random field crf to model non local pragmatic dependency between paired utterance such a question answer that typically appear together in summary and show that these model outperform linear chain crfs and bayesian model in the task we also discus different approach for ranking all utterance in a sequence using crfs our best performing system achieves of human performance when evaluated with the pyramid evaluation metric which represents a absolute increase compared to our most competitive non sequential classifier 
a complex relation is any n ary relation in which some of the argument may be be unspecified we present here a simple two stage method for extracting complex relation between named entity in text the first stage creates a graph from pair of entity that are likely to be related and the second stage score maximal clique in that graph a potential complex relation instance we evaluate the new method against a standard baseline for extracting genomic variation relation from biomedical text 
sitting at the intersection between statistic and machine learning dynamic bayesian network have been applied with much success in many domain such a speech recognition vision and computational biology while natural language processing increasingly relies on statistical method we think they have yet to use graphical model to their full potential in this paper we report on experiment in learning edit distance cost using dynamic bayesian network and present result on a pronunciation classification task by exploiting the ability within the dbn framework to rapidly explore a large model space we obtain a reduction in error rate compared to a previous transducer based method of learning edit distance 
this paper present a probabilistic framework qarla for the evaluation of text summarisation system the input of the framework is a set of manual reference summary a set of baseline automatic summary and a set of similarity metric between summary it provides i a measure to evaluate the quality of any set of similarity metric ii a measure to evaluate the quality of a summary using an optimal set of similarity metric and iii a measure to evaluate whether the set of baseline summary is reliable or may produce biased result compared to previous approach our framework is able to combine different metric and evaluate the quality of a set of metric without any a priori weighting of their relative importance we provide quantitative evidence about the effectiveness of the approach to improve the automatic evaluation of text summarisation system by combining several similarity metric 
situation entity s are the event state generic statement and embedded fact and proposition introduced to a discourse by clause of text we report on the first datadriven model for labeling clause according to the type of se they introduce se classification is important for discourse mode identification and for tracking the temporal progression of a discourse we show that a linguistically motivated cooccurrence feature and grammatical relation information from deep syntactic analysis improve classification accuracy and b using a sequencing model provides improvement over assigning label based on the utterance alone we report on genre effect which support the analysis of discourse mode having characteristic distribution and sequence of s 
in this paper we explore the power of randomized algorithm to address the challenge of working with very large amount of data we apply these algorithm to generate noun similarity list from million page we reduce the running time from quadratic to practically linear in the number of element to be computed 
we investigate generalization of the all subtrees dop approach to unsupervised parsing unsupervised dop model assign all possible binary tree to a set of sentence and next use a large random subset of all subtrees from these binary tree to compute the most probable parse tree we will test both a relative frequency estimator for unsupervised dop and a maximum likelihood estimator which is known to be statistically consistent we report state of the art result on english wsj german negra and chinese ctb data to the best of our knowledge this is the first paper which test a maximum likelihood estimator for dop on the wall street journal leading to the surprising result that an unsupervised parsing model beat a widely used supervised model a treebank pcfg 
instance of a word drawn from different domain may have different sense prior the proportion of the different sens of a word this in turn affect the accuracy of word sense disambiguation wsd system trained and applied on different domain this paper present a method to estimate the sense prior of word drawn from a new domain and highlight the importance of using well calibrated probability when performing these estimation by using well calibrated probability we are able to estimate the sense prior effectively to achieve significant improvement in wsd accuracy 
extracting semantic relationship between entity is challenging this paper investigates the incorporation of diverse lexical syntactic and semantic knowledge in feature based relation extraction using svm our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing give limited further enhancement this suggests that most of useful information in full parse tree for relation extraction is shallow and can be captured by chunking we also demonstrate how semantic information such a wordnet and name list can be used in feature based relation extraction to further improve the performance evaluation on the ace corpus show that effective incorporation of diverse feature enables our system outperform previously best reported system on the ace relation subtypes and significantly outperforms tree kernel based system by over in f measure on the ace relation type 
most machine transliteration system transliterate out of vocabulary oov word through intermediate phonemic mapping a framework ha been presented that allows direct orthographical mapping between two language that are of different origin employing different alphabet set a modified joint source channel model along with a number of alternative have been proposed aligned transliteration unit along with their context are automatically derived from a bilingual training corpus to generate the collocational statistic the transliteration unit in bengali word take the pattern c m where c represents a vowel or a consonant or a conjunct and m represents the vowel modifier or matra the english transliteration unit are of the form c v where c represents a consonant and v represents a vowel a bengali english machine transliteration system ha been developed based on the proposed model the system ha been trained to transliterate person name from bengali to english it us the linguistic knowledge of possible conjuncts and diphthong in bengali and their equivalent in english the system ha been evaluated and it ha been observed that the modified joint source channel model performs best with a word agreement ratio of and a transliteration unit agreement ratio of 
even in a massive corpus such a the web a substantial fraction of extraction appear infrequently this paper show how to ass the correctness of sparse extraction by utilizing unsupervised language model the realm system which combine hmmbased and n gram based language model rank candidate extraction by the likelihood that they are correct our experiment show that realm reduces extraction error by on average when compared with previous work because realm pre computes language model based on it corpus and doe not require any hand tagged seed it is far more scalable than approach that learn model for each individual relation from handtagged data thus realm is ideally suited for open information extraction where the relation of interest are not specified in advance and their number is potentially vast 
this paper address the issue of text normalization an important yet often overlooked problem in natural language processing by text normalization we mean converting informally inputted text into the canonical form by eliminating noise in the text and detecting paragraph and sentence boundary in the text previously text normalization issue were often undertaken in an ad hoc fashion or studied separately this paper first give a formalization of the entire problem it then proposes a unified tagging approach to perform the task using conditional random field crf the paper show that with the introduction of a small set of tag most of the text normalization task can be performed within the approach the accuracy of the proposed method is high because the subtasks of normalization are interdependent and should be performed together experimental result on email data cleaning show that the proposed method significantly outperforms the approach of using cascaded model and that of employing independent model 
we have developed an extension of acl that includes the implementation of hash based association list and function memoization this make some algorithm execute more quickly this extension enabled partially by the implementation of hash con represents acl data object in a canonical way thus the comparison of any two such object can be determined without the cost of descending through their con structure a restricted set of acl user defined function may be memoized the underlying implementation may conditionally retain the value of such function call so that if a repeated function application is requested a previously computed value may instead be returned we have defined a fast association list access and update function using hash table we provide a file reader that identifies and eliminates duplicate representation of repeated object and a file printer that produce output with no duplicate subexpressions 
information extraction system incorporate multiple stage of linguistic analysis although error are typically compounded from stage to stage it is possible to reduce the error in one stage by harnessing the result of the other stage we demonstrate this by using the result of coreference analysis and relation extraction to reduce the error produced by a chinese name tagger we use an n best approach to generate multiple hypothesis and have them re ranked by subsequent stage of processing we obtained thereby a reduction of in spurious and incorrect name tag and a reduction of in missed tag 
in this paper we present a quantitative and qualitative analysis of annotation in the hinoki treebank of japanese and investigate a method of speeding annotation by using part of speech tag the hinoki treebank is a redwood style treebank of japanese dictionary definition sentence sentence are annotated by three different annotator and the agreement evaluated an average agreement of wa found using strict agreement and using labeled precision exploiting po tag allowed the annotator to choose the best parse with fewer decision 
to facilitate the use of syntactic information in the study of child language acquisition a coding scheme for grammatical relation grs in transcript of parent child dialog ha been proposed by sagae macwhinney and lavie we discus the use of current nlp technique to produce the grs in this annotation scheme by using a statistical parser charniak and memory based learning tool for classification daelemans et al we obtain high precision and recall of several grs we demonstrate the usefulness of this approach by performing automatic measurement of syntactic development with the index of productive syntax scarborough at similar level to what child language researcher compute manually 
we show that the problem of parsing and surface realization for grammar formalism with context free derivation coupled with montague semantics under a certain restriction can be reduced in a uniform way to datalog query evaluation a well a giving a polynomialtime algorithm for computing all derivation tree in the form of a shared forest from an input string or input logical form this reduction ha the following complexity theoretic consequence for all such formalism i the decision problem of recognizing grammaticality surface realizability of an input string logical form is in logcfl and ii the search problem of finding one logical form surface string from an input string logical form is in functional logcfl moreover the generalized supplementary magic set rewriting of the datalog program resulting from the reduction yield efficient earley style algorithm for both parsing and generation 
this paper design a novel lexical hub to disambiguate word sense using both syntagmatic and paradigmatic relation of word it only employ the semantic network of wordnet to calculate word similarity and the edinburgh association thesaurus eat to transform contextual space for computing syntagmatic and other domain relation with the target word without any back off policy the result on the english lexical sample of senseval show that lexical cohesion based on edge counting technique is a good way of unsupervisedly disambiguating sens 
the statistical modelling of language together with advance in wide coverage grammar development have led to high level of robustness and efficiency in nlp system and made linguistically motivated large scale language processing a possibility matsuzaki et al kaplan et al this paper describes an nlp system which is based on syntactic and semantic formalism from theoretical linguistics and which we have used to analyse the entire gigaword corpus billion word in le than day using only processor this combination of detail and speed of analysis represents a break through in nlp technology 
stochastic optimality theory boersma is a widely used model in linguistics that did not have a theoretically sound learning method previously in this paper a markov chain monte carlo method is proposed for learning stochastic ot grammar following a bayesian framework the goal is finding the posterior distribution of the grammar given the relative frequency of input output pair the data augmentation algorithm allows one to simulate a joint posterior distribution by iterating two conditional sampling step this gibbs sampler construct a markov chain that converges to the joint distribution and the target posterior can be derived a it marginal distribution 
chatting is a popular communication medium on the internet via icq chat room etc chat language is different from natural language due to it anomalous and dynamic nature which render conventional nlp tool inapplicable the dynamic problem is enormously troublesome because it make static chat language corpus outdated quickly in representing contemporary chat language to address the dynamic problem we propose the phonetic mapping model to present mapping between chat term and standard word via phonetic transcription i e chinese pinyin in our case different from character mapping the phonetic mapping can be constructed from available standard chinese corpus to perform the task of dynamic chat language term normalization we extend the source channel model by incorporating the phonetic mapping model experimental result show that this method is effective and stable in normalizing dynamic chat language term 
this paper investigates the use of machine learning algorithm to label modifier noun compound with a semantic relation the attribute used a input to the learning algorithm are the web frequency for phrase containing the modifier noun and a prepositional joining term we compare and evaluate different algorithm and different joining phrase on nastase and szpakowicz s dataset of modifier noun compound we find that by using a support vector machine classifier we can obtain better performance on this dataset than a current state of the art system even with a relatively small set of prepositional joining term 
in this interactive presentation a chinese named entity and relation identification system is demonstrated the domain specific system ha a three stage pipeline architecture which includes word segmentation and part of speech po tagging named entity recognition and named entity relation identitfication the experimental result have shown that the average f measure for word segmentation and po tagging after correcting error achieves and separately moreover the overall average f measure for kind of name entity and kind of named entity relation is and respectively 
in a new approach to large scale extraction of fact from unstructured text distributional similarity become an integral part of both the iterative acquisition of high coverage contextual extraction pattern and the validation and ranking of candidate fact the evaluation measure the quality and coverage of fact extracted from one hundred million web document starting from ten seed fact and using no additional knowledge lexicon or complex tool 
the applicability of many current information extraction technique is severely limited by the need for supervised training data we demonstrate that for certain field structured extraction task such a classified advertisement and bibliographic citation small amount of prior knowledge can be used to learn effective model in a primarily unsupervised fashion although hidden markov model hmms provide a suitable generative model for field structured text general unsupervised hmm learning fails to learn useful structure in either of our domain however one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solution in both domain we found that unsupervised method can attain accuracy with unlabeled example comparable to those attained by supervised method on labeled example and that semi supervised method can make good use of small amount of labeled data 
in this paper we investigate the benefit of stochastic predictor component for the parsing quality which can be obtained with a rule based dependency grammar by including a chunker a supertagger a pp attacher and a fast probabilistic parser we were able to improve upon the baseline by bringing the overall labelled accuracy to on the german negra corpus we attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner thus avoiding the problem of error propagation 
we present an investigation of recently proposed character and word sequence kernel for the task of authorship attribution based on relatively short text performance is compared with two corresponding probabilistic approach based on markov chain several configuration of the sequence kernel are studied on a relatively large dataset author where each author covered several topic utilising moffat smoothing the two probabilistic approach obtain similar performance which in turn is comparable to that of character sequence kernel and is better than that of word sequence kernel the result further suggest that when using a realistic setup that take into account the case of text which are not written by any hypothesised author the amount of training material ha more influence on discrimination performance than the amount of test material moreover we show that the recently proposed author unmasking approach is le useful when dealing with short text 
successful participation in dialogue a well a understanding written text requires among others interpretation of specification implicitly conveyed through parallel structure while those whose reconstruction requires insertion of a missing element such a gapping and ellipsis have been addressed to a certain extent by computational approach there is virtually no work addressing parallel structure headed by vice versa like operator whose reconstruction requires transformation in this paper we address the meaning reconstruction of such construct by an informed reasoning process the applied technique include building deep semantic representation application of category of pattern underlying a formal reconstruction and using pragmatically motivated and empirically justified preference we present an evaluation of our algorithm conducted on a uniform collection of text containing the phrase in question 
we present tutorial dialogue system in two different domain that demonstrate the use of dialogue management and deep natural language processing technique generation technique are used to produce natural sounding feedback adapted to student performance and the dialogue history and context is used to interpret tentative answer phrased a question 
this paper address the problem of acquiring lexical semantic relationship applied to the lexical entailment relation our main contribution is a novel conceptual integration between the two distinct acquisition paradigm for lexical relation the pattern based and the distributional similarity approach the integrated method exploit mutual complementary information of the two approach to obtain candidate relation and informative characterizing feature then a small size training set is used to construct a more accurate supervised classifier showing significant increase in both recall and precision over the original approach 
several user have had problem using equivalence based rewriting in acl because the acl rewriter cache it result we describe this problem in some detail together with a partial solution first implemented in acl version this partial solution consists of a new primitive double rewrite together with a new warning to suggest possible use of this primitive 
we address the task of unsupervised topic segmentation of speech data operating over raw acoustic information in contrast to existing algorithm for topic segmentation of speech our approach doe not require input transcript our method predicts topic change by analyzing the distribution of reoccurring acoustic pattern in the speech signal corresponding to a single speaker the algorithm robustly handle noise inherent in acoustic matching by intelligently aggregating information about the similarity profile from multiple local comparison our experiment show that audio based segmentation compare favorably with transcriptbased segmentation computed over noisy transcript these result demonstrate the desirability of our method for application where a speech recognizer is not available or it output ha a high word error rate 
recent work on conditional random field crfs ha demonstrated the need for regularisation to counter the tendency of these model to overfit the standard approach to regularising crfs involves a prior distribution over the model parameter typically requiring search over a hyperparameter space in this paper we address the overfitting problem from a different perspective by factoring the crf distribution into a weighted product of individual expert crf distribution we call this model a logarithmic opinion pool lop of crfs lop crfs we apply the lop crf to two sequencing task our result show that unregularised expert crfs with an unregularised crf under a lop can outperform the unregularised crf and attain a performance level close to the regularised crf lop crfs therefore provide a viable alternative to crf regularisation without the need for hyperparameter search 
lexical feature are key to many approach to sentiment analysis and opinion detection a variety of representation have been used including single word multi word ngrams phrase and lexico syntactic pattern in this paper we use a subsumption hierarchy to formally define different type of lexical feature and their relationship to one another both in term of representational coverage and performance we use the subsumption hierarchy in two way a an analytic tool to automatically identify complex feature that outperform simpler feature and to reduce a feature set by removing unnecessary feature we show that reducing the feature set improves performance on three opinion classification task especially when combined with traditional feature selection 
synchronous context free grammar scfgs have been successfully exploited a translation model in machine translation application when parsing with an scfg computational complexity grows exponentially with the length of the rule in the worst case in this paper we examine the problem of factorizing each rule of an input scfg to a generatively equivalent set of rule each having the smallest possible length our algorithm work in time o n log n for each rule of length n this improves upon previous result and solves an open problem about recognizing permutation that can be factored 
ordering information is a difficult but important task for application generating natural language text we present a bottom up approach to arranging sentence extracted for multi document summarization to capture the association and order of two textual segment eg sentence we define four criterion chronology topical closeness precedence and succession these criterion are integrated into a criterion by a supervised learning approach we repeatedly concatenate two textual segment into one segment based on the criterion until we obtain the overall segment with all sentence arranged our experimental result show a significant improvement over existing sentence ordering strategy 
this investigation proposes an approach to modeling the discourse of spoken dialogue using semantic dependency graph by characterizing the discourse a a sequence of speech act discourse modeling becomes the identification of the speech act sequence a statistical approach is adopted to model the relation between word in the user s utterance using the semantic dependency graph dependency relation between the headword and other word in a sentence is detected using the semantic dependency grammar in order to evaluate the proposed method a dialogue system for medical service is developed experimental result show that the rate for speech act detection and task completion are and respectively and the average number of turn of each dialogue is compared with the bayes classifier and the partial pattern tree based approach we obtain and improvement in accuracy for speech act identification respectively 
we show that we can automatically classify semantically related phrase into class classification robustness is improved by training with multiple source of evidence including within document cooccurrence html markup syntactic relationship in sentence substitutability in query log and string similarity our work provides a benchmark for automatic n way classification into wordnet s semantic class both on a trec news corpus and on a corpus of substitutable search query phrase 
we present new statistical model for jointly labeling multiple sequence and apply them to the combined task of part of speech tagging and noun phrase chunking the model is based on the factorial hidden markov model fhmm with distributed hidden state representing part of speech and noun phrase sequence we demonstrate that this joint labeling approach by enabling information sharing between tagging chunking subtasks out performs the traditional method of tagging and chunking in succession further we extend this into a novel model switching fhmm to allow for explicit modeling of cross sequence dependency based on linguistic knowledge we report tagging chunking accuracy for varying dataset size and show that our approach is relatively robust to data sparsity 
the problem of part of speech induction from text involves two aspect firstly a set of word class is to be derived automatically secondly each word of a vocabulary is to be assigned to one or several of these word class in this paper we present a method that solves both problem with good accuracy our approach adopts a mixture of statistical method that have been successfully applied in word sense induction it main advantage over previous attempt is that it reduces the syntactic space to only the most important dimension thereby almost eliminating the otherwise omnipresent problem of data sparseness 
this paper present a method for the automatic generation of a table of content this type of summary could serve a an effective navigation tool for accessing information in long text such a book to generate a coherent table of content we need to capture both global dependency across different title in the table and local constraint within section our algorithm effectively handle these complex dependency by factoring the model into local and global component and incrementally constructing the model s output the result of automatic evaluation and manual assessment confirm the benefit of this design our system is consistently ranked higher than nonhierarchical baseline 
in contrast to the latest progress in speech recognition the state of the art in natural language generation for spoken language dialog system is lagging behind the core dialog manager are now more sophisticated and natural sounding and flexible output is expected but not achieved with current simple technique such a template based system portability of system across subject domain and language is another increasingly important requirement in dialog system this paper present an outline of legend a system that is both portable and generates natural sounding output this goal is achieved through the novel use of existing lexical resource such a framenet and wordnet 
this paper describes a novel framework for interactive question answering q a based on predictive questioning generated off line from topic representation of complex scenario predictive question represent request for information that capture the most salient and diverse aspect of a topic we present experimental result from large user study featuring a fully implemented interactive q a system named ferret that demonstrates that surprising performance is achieved by integrating predictive question into the context of a q a dialogue 
random indexing is a vector space technique that provides an efficient and scalable approximation to distributional similarity problem we present experiment showing random indexing to be poor at handling large volume of data and evaluate the use of weighting function for improving the performance of random indexing we find that random index is robust for small data set but performance degrades because of the influence high frequency attribute in large data set the use of appropriate weight function improves this significantly 
conditional random field lafferty et al are quite effective at sequence labeling task like shallow parsing sha and pereira and named entity extraction mccallum and li crfs are log linear allowing the incorporation of arbitrary feature into the model to train on unlabeled data we require unsupervised estimation method for log linear model few exist we describe a novel approach contrastive estimation we show that the new technique can be intuitively understood a exploiting implicit negative evidence and is computationally efficient applied to a sequence labeling problem po tagging given a tagging dictionary and unlabeled text contrastive estimation outperforms em with the same feature set is more robust to degradation of the dictionary and can largely recover by modeling additional feature 
we introduce an error mining technique for automatically detecting error in resource that are used in parsing system we applied this technique on parsing result produced on several million word by two distinct parsing system which share the syntactic lexicon and the pre parsing processing chain we were thus able to identify missing and erroneous information in these resource 
combination method are an effective way of improving system performance this paper examines the benefit of system combination for unsupervised wsd we investigate several votingand arbiter based combination strategy over a diverse pool of unsupervised wsd system our combination method rely on predominant sens which are derived automatically from raw text experiment using the semcor and senseval data set demonstrate that our ensemble yield significantly better result when compared with state of the art 
the aim of this paper is to develop animated agent that can control multimodal instruction dialogue by monitoring user s behavior first this paper report on our wizard of oz experiment and then using the collected corpus proposes a probabilistic model of fine grained timing dependency among multimodal communication behavior speech gesture and mouse manipulation a preliminary evaluation revealed that our model can predict a instructor s grounding judgment and a listener s successful mouse manipulation quite accurately suggesting that the model is useful in estimating the user s understanding and can be applied to determining the agent s next action 
inspired by previous preprocessing approach to smt this paper proposes a novel probabilistic approach to reordering which combine the merit of syntax and phrase based smt given a source sentence and it parse tree our method generates by tree operation an n best list of reordered input which are then fed to standard phrase based decoder to produce the optimal translation experiment show that for the nist mt task of chinese toenglish translation the proposal lead to bleu improvement of 
assessing the quality of user generated content is an important problem for many web forum while quality is currently assessed manually we propose an algorithm to ass the quality of forum post automatically and test it on data provided by nabble com we use state of the art classification technique and experiment with five feature class surface lexical syntactic forum specific and similarity feature we achieve an accuracy of on the task of automatically assessing post quality in the software domain using forum specific feature without forum specific feature we achieve an accuracy of 
current phrase based smt system perform poorly when using small training set this is a consequence of unreliable translation estimate and low coverage over source and target phrase this paper present a method which alleviates this problem by exploiting multiple translation of the same source phrase central to our approach is triangulation the process of translating from a source to a target language via an intermediate third language this allows the use of a much wider range of parallel corpus for training and can be combined with a standard phrase table using conventional smoothing method experimental result demonstrate bleu improvement for triangulated model over a standard phrase based system 
in this paper we provide a formalization of a set of default rule that we claim are required for the transfer of information such a causation event rate and duration in the interpretation of metaphor such rule are domain independent and are identified a invariant adjunct to any conceptual metaphor we also show a way of embedding the invariant mapping in a semantic framework 
in this paper we present a system that automatically extract the pro and con from online review although many approach have been developed for extracting opinion from text our focus here is on extracting the reason of the opinion which may themselves be in the form of either fact or opinion leveraging online review site with author generated pro and con we propose a system for aligning the pro and con to their sentence in review text a maximum entropy model is then trained on the resulting labeled set to subsequently extract pro and con from online review site that do not explicitly provide them our experimental result show that our resulting system identifies pro and con with precision and recall 
we present a novel pcfg based architecture for robust probabilistic generation based on wide coverage lfg approximation cahill et al automatically extracted from treebanks maximising the probability of a tree given an f structure we evaluate our approach using string based evaluation we currently achieve coverage of a bleu score of and string accuracy of on the penn ii wsj section sentence of length 
kernel method such a support vector machine svms have attracted a great deal of popularity in the machine learning and natural language processing nlp community polynomial kernel svms showed very competitive accuracy in many nlp problem like part of speech tagging and chunking however these method are usually too inefficient to be applied to large dataset and real time purpose in this paper we propose an approximate method to analogy polynomial kernel with efficient data mining approach to prevent exponential scaled testing time complexity we also present a new method for speeding up svm classifying which doe independent to the polynomial degree d the experimental result showed that our method is and time faster than traditional polynomial kernel in term of training and testing respectively 
this paper describes our work on building part of speech po tagger for bengali we have use hidden markov model hmm and maximum entropy me based stochastic tagger bengali is a morphologically rich language and our tagger make use of morphological and contextual information of the word since only a small labeled training set is available word simple stochastic approach doe not yield very good result in this work we have studied the effect of using a morphological analyzer to improve the performance of the tagger we find that the use of morphology help improve the accuracy of the tagger especially when le amount of tagged corpus are available 
for many year statistical machine translation relied on generative model to provide bilingual word alignment in several independent effort showed that discriminative model could be used to enhance or replace the standard generative approach building on this work we demonstrate substantial improvement in word alignment accuracy partly though improved training method but predominantly through selection of more and better feature our best model produce the lowest alignment error rate yet reported on canadian hansard bilingual data 
this paper present recent extension to poliqarp an open source tool for indexing and searching morphosyntactically annotated corpus which turn it into a tool for indexing and searching certain kind of treebanks complementary to existing treebank search engine in particular the paper discus the motivation for such a new tool the extended query syntax of poliqarp and implementation and efficiency issue 
we present a user requirement study for question answering on meeting record that ass the difficulty of user question in term of what type of knowledge is required in order to provide the correct answer we grounded our work on the empirical analysis of elicited user query we found that the majority of elicited query around pertain to argumentative process and outcome our analysis also suggests that standard keyword based information retrieval can only deal successfully with le than of the query and that it must be complemented with other type of metadata and inference 
certain distinction made in the lexicon of one language may be redundant when translating into another language we quantify redundancy among source type by the similarity of their distribution over target type we propose a language independent framework for minimising lexical redundancy that can be optimised directly from parallel text optimisation of the source lexicon for a given target language is viewed a model selection over a set of cluster based translation model redundant distinction between type may exhibit monolingual regularity for example inflexion pattern we define a prior over model structure using a markov random field and learn feature over set of monolingual type that are predictive of bilingual redundancy the prior make model selection more robust without the need for language specific assumption regarding redundancy using these model in a phrase based smt system we show significant improvement in translation quality for certain language pair 
this paper present a speech understanding component for enabling robust situated human robot communication the aim is to gain semantic interpretation of utterance that serve a a basis for multi modal dialog management also in case where the recognized word stream is not grammatically correct for the understanding process we designed semantic processable unit which are adapted to the domain of situated communication our framework support the specic characteristic of spontaneous speech used in combination with gesture in a real world scenario it also provides information about the dialog act finally we present a processing mechanism using these concept structure to generate the most likely semantic interpretation of the utterance and to evaluate the interpretation with respect to semantic coherence 
we first show how a structural locality bias can improve the accuracy of state of the art dependency grammar induction model trained by em from unannotated example klein and manning next by annealing the free parameter that control this bias we achieve further improvement we then describe an alternative kind of structural bias toward broken hypothesis consisting of partial structure over segmented sentence and show a similar pattern of improvement we relate this approach to contrastive estimation smith and eisner a apply the latter to grammar induction in six language and show that our new approach improves accuracy by absolute over ce and over em achieving to our knowledge the best result on this task to date our method structural annealing is a general technique with broad applicability to hidden structure discovery problem 
it ha previously been assumed in the psycholinguistic literature that finite state model of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model we show that a simple computational model a bigram part of speech tagger based on the design used by corley and crocker make correct prediction on processing difficulty observed in a wide range of empirical sentence processing data we use two mode of evaluation one that relies on comparison with a control sentence paralleling practice in human study another that measure probability drop in the disambiguating region of the sentence both are surprisingly good indicator of the processing difficulty of garden path sentence the sentence tested are drawn from published source and systematically explore five different type of ambiguity previous study have been narrower in scope and smaller in scale we do not deny the limitation of finite state model but argue that our result show that their usefulness ha been underestimated 
standard approach to chinese word segmentation treat the problem a a tagging task assigning label to the character in the sequence indicating whether the character mark a word boundary discriminatively trained model based on local character feature are used to make the tagging decision with viterbi decoding finding the highest scoring segmentation in this paper we propose an alternative word based segmentor which us feature based on complete word and word sequence the generalized perceptron algorithm is used for discriminative training and we use a beamsearch decoder closed test on the first and second sighan bakeoffs show that our system is competitive with the best in the literature achieving the highest reported f score for a number of corpus 
in this paper we present an unsupervised methodology for propagating lexical cooccurrence vector into an ontology such a wordnet we evaluate the framework on the task of automatically attaching new concept into the ontology experimental result show attachment accuracy in the first position and accuracy in the top position this framework could potentially serve a a foundation for ontologizing lexical semantic resource and assist the development of other largescale and internally consistent collection of semantic information 
we investigate independent and relevant event based extractive mutli document summarization approach in this paper event are defined a event term and associated event element with independent approach we identify important content by frequency of event with relevant approach we identify important content by pagerank algorithm on the event map constructed from document experimental result are encouraging 
the logon mt demonstrator assembles independently valuable general purpose nlp component into a machine translation pipeline that capitalizes on output quality the demonstrator embodies an interesting combination of hand built symbolic resource and stochastic process 
this work provides the essential foundation for modular construction of typed unification grammar for natural language much of the information in such grammar is encoded in the signature and hence the key is facilitating a modularized development of type signature we introduce a definition of signature module and show how two module combine our definition are motivated by the actual need of grammar developer obtained through a careful examination of large scale grammar we show that our definition meet these need by conforming to a detailed set of desideratum 
computational humor will be needed in interface no le than other cognitive capability there are many practical setting where computational humor will add value among them there are business world application such a advertisement e commerce etc general computer mediated communication and human computer interaction increase in the friendliness of natural language interface educational and edutainment system in particular in the educational field it is an important resource for getting selective attention help in memorizing name and situation etc and we all know how well it work with child automated humor production in general is a very difficult task but we wanted to prove that some result can be achieved even in short time we have worked at a concrete limited problem a the core of the european project hahacronym the main goal of hahacronym ha been the realization of an acronym ironic re analyzer and generator a a proof of concept in a focalized but non restricted context to implement this system some general tool have been adapted or developed for the humorous context system output ha been submitted to evaluation by human subject with a very positive result 
this paper focus on the exploration of term dependence in the application of sentence retrieval the adjacent term appearing in query are assumed to be related with each other these assumed dependence among query term will be further validated for each sentence and sentence which present strong syntactic relationship among query term are considered more relevant experimental result have fully demonstrated the promising of the proposed model in improving sentence retrieval effectiveness 
we investigate different feature set for performing automatic sentence level discourse segmentation within a general machine learning approach including feature derived from either finite state or contextfree annotation we achieve the best reported performance on this task and demonstrate that our spade inspired context free feature are critical to achieving this level of accuracy this counter recent result suggesting that purely finite state approach can perform competitively 
inflected language in a low resource setting present a data sparsity problem for statistical machine translation in this paper we present a minimally supervised algorithm for morpheme segmentation on arabic dialect which reduces unknown word at translation time by over total vocabulary size by over and yield a significant increase in bleu score over a previous state of theart phrase based statistical mt system 
current research in text mining favour the quantity of text over their quality but for bilingual terminology mining and for many language pair large comparable corpus are not available more importantly a term are defined vi vi a specific domain with a restricted register it is expected that the quality rather than the quantity of the corpus matter more in terminology mining our hypothesis therefore is that the quality of the corpus is more important than the quantity and ensures the quality of the acquired terminological resource we show how important the type of discourse is a a characteristic of the comparable corpus 
grapheme to phoneme conversion g p is a core component of any text to speech system we show that adding simple syllabification and stress assignment constraint namely one nucleus per syllable and one main stress per word to a joint n gram model for g p conversion lead to a dramatic improvement in conversion accuracy secondly we assessed morphological preprocessing for g p conversion while morphological information ha been incorporated in some past system it contribution ha never been quantitatively assessed for german we compare the relevance of morphological preprocessing with respect to the morphological segmentation method training set size the g p conversion algorithm and two language english and german 
in statistic based summarization step one sentence compression knight and marcu knight and marcu k m present a noisy channel model for sentence compression the main difficulty in using this method is the lack of data knight and marcu use a corpus of training sentence more data is not easily available so in addition to improving the original k m noisy channel model we create unsupervised and semi supervised model of the task finally we point out problem with modeling the task in this way they suggest area for future research 
a an area of great linguistic and cultural diversity asian language resource have received much le attention than their western counterpart creating a common standard for asian language resource that is compatible with an international standard ha at least three strong advantage to increase the competitive edge of asian country to bring asian country to closer to their western counterpart and to bring more cohesion among asian country to achieve this goal we have launched a two year project to create a common standard for asian language resource the project is comprised of four research item building a description framework of lexical entry building sample lexicon building an upper layer ontology and evaluating the proposed framework through an application this paper outline the project in term of it aim and approach 
this paper describes the main feature of our tool called legal taxonomy syllabus the system is an ontology based tool designed to annotate and recover multi lingua legal information and build conceptual dictionary on european directive 
we propose a method of organizing reading material for vocabulary learning it enables u to select a concise set of reading text from a target corpus that contains all the target vocabulary to be learned we used a specialized vocabulary for an english certification test a the target vocabulary and used english wikipedia a free content encyclopedia a the target corpus the organized reading material would enable learner not only to study the target vocabulary efficiently but also to gain a variety of knowledge through reading the reading material are available on our web site 
we present a novel framework that combine strength from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy specifically by combining dependency and hpsg parsing we show that by using surface dependency to constrain the application of wide coverage hpsg rule we can benefit from a number of parsing technique designed for highaccuracy dependency parsing while actually performing deep syntactic analysis our framework result in a absolute improvement over a state of the art approach for wide coverage hpsg parsing 
we propose a conditional random field based method for supertagging and apply it to the task of learning new lexical item for hpsg based precision grammar of english and japanese using a pseudo likelihood approximation we are able to scale our model to hundred of supertags and ten of thousand of training sentence we show that it is possible to achieve start of the art result for both language using maximally language independent lexical feature further we explore the performance of the model at the typeand token level demonstrating their superior performance when compared to a unigram based baseline and a transformation based learning approach 
the paper present a bayesian model for text summarization which explicitly encodes and exploit information on how human judgment are distributed over the text comparison is made against non bayesian summarizers using test data from japanese news text it is found that the bayesian approach generally leverage performance of a summarizer at time giving it a significant lead over nonbayesian model 
deriving the polarity and strength of opinion is an important research topic attracting significant attention over the last few year in this work to measure the strength and polarity of an opinion we consider the economic context in which the opinion is evaluated instead of using human annotator or linguistic resource we rely on the fact that text in on line system influence the behavior of human and this effect can be observed using some easy to measure economic variable such a revenue or product price by reversing the logic we infer the semantic orientation and strength of an opinion by tracing the change in the associated economic variable in effect we use econometrics to identify the economic value of text and assign a dollar value to each opinion phrase measuring sentiment effectively and without the need for manual labeling we argue that by interpreting opinion using econometrics we have the first objective quantifiable and contextsensitive evaluation of opinion we make the discussion concrete by presenting result on the reputation system of amazon com we show that user feedback affect the pricing power of merchant and by measuring their pricing power we can infer the polarity and strength of the underlying feedback posting 
despite much recent progress on accurate semantic role labeling previous work ha largely used independent classifier possibly combined with separate label sequence model via viterbi decoding this stand in stark contrast to the linguistic observation that a core argument frame is a joint structure with strong dependency between argument we show how to build a joint model of argument frame incorporating novel feature that model these interaction into discriminative log linear model this system achieves an error reduction of on all argument and on core argument over a state of the art independent classifier for gold standard parse tree on propbank 
effectively identifying event in unstructured text is a very difficult task this is largely due to the fact that an individual event can be expressed by several sentence in this paper we investigate the use of clustering method for the task of grouping the text span in a news article that refer to the same event the key idea is to cluster the sentence using a novel distance metric that exploit regularity in the sequential structure of event within a document when this approach is compared to a simple bag of word baseline a statistically significant increase in performance is observed 
we introduce the possibility of combining lexical association measure and present empirical result of several method employed in automatic collocation extraction first we present a comprehensive summary overview of association measure and their performance on manually annotated data evaluated by precision recall graph and mean average precision second we describe several classification method for combining association measure followed by their evaluation and comparison with individual measure finally we propose a feature selection algorithm significantly reducing the number of combined measure with only a small performance degradation 
we propose a framework to derive the distance between concept from distributional measure of word co occurrence we use the category in a published thesaurus a coarse grained concept allowing all possible distance value to be stored in a concept concept matrix roughly the size of that created by existing measure we show that the newly proposed concept distance measure outperform traditional distributional word distance measure in the task of ranking word pair in order of semantic distance and correcting real word spelling error in the latter task of all the wordnet based measure only that proposed by jiang and conrath outperforms the best distributional concept distance measure 
in this paper we explore the utility of the navigation map nm a graphical representation of the discourse structure we run a user study to investigate if user perceive the nm a helpful in a tutoring spoken dialogue system from the user perspective our result show that the nm presence allows them to better identify and follow the tutoring plan and to better integrate the instruction it wa also easier for user to concentrate and to learn from the system if the nm wa present our preliminary analysis on objective metric further strengthens these finding 
we present an approach to pronoun resolution based on syntactic path through a simple bootstrapping procedure we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entity this path information enables u to handle previously challenging resolution instance and also robustly address traditional syntactic coreference constraint highly coreferent path also allow mining of precise probabilistic gender number information we combine statistical knowledge with well known feature in a support vector machine pronoun resolution classifier significant gain in performance are observed on several datasets 
semantic role labeling is the process of annotating the predicate argument structure in text with semantic label in this paper we present a state of the art baseline semantic role labeling system based on support vector machine classifier we show improvement on this system by i adding new feature including feature extracted from dependency par ii performing feature selection and calibration and iii combining par obtained from semantic parser trained using different syntactic view error analysis of the baseline system showed that approximately half of the argument identification error resulted from parse error in which there wa no syntactic constituent that aligned with the correct argument in order to address this problem we combined semantic par from a minipar syntactic parse and from a chunked syntactic representation with our original baseline system which wa based on charniak par all of the reported technique resulted in performance improvement 
due to the historical and cultural reason english phase especially the proper noun and new word frequently appear in web page written primarily in asian language such a chinese and korean although these english term and their equivalence in the asian language refer to the same concept they are erroneously treated a independent index unit in traditional information retrieval ir this paper describes the degree to which the problem arises in ir and suggests a novel technique to solve it our method firstly extract an english phrase from asian language web page and then unifies the extracted phrase and it equivalence s in the language a one index unit experimental result show that the high precision of our conceptual unification approach greatly improves the ir performance 
we investigate the problem of learning a part of speech po lexicon for a resource poor language dialectal arabic developing a high quality lexicon is often the first step towards building a po tagger which is in turn the front end to many nlp system we frame the lexicon acquisition problem a a transductive learning problem and perform comparison on three transductive algorithm transductive svms spectral graph transducer and a novel transductive clustering method we demonstrate that lexicon learning is an important task in resource poor domain and lead to significant improvement in tagging accuracy for dialectal arabic 
this paper present a new web mining scheme for parallel data acquisition based on the document object model dom a web page is represented a a dom tree then a dom tree alignment model is proposed to identify the translationally equivalent text and hyperlink between two parallel dom tree by tracing the identified parallel hyperlink parallel web document are recursively mined compared with previous mining scheme the benchmark show that this new mining scheme improves the mining coverage reduces mining bandwidth and enhances the quality of mined parallel sentence 
we have developed an automated japanese essay scoring system called jess the system need expert writing rather than expert raters to build the evaluation model by detecting statistical outlier of predetermined aimed essay feature compared with many professional writing for each prompt our system can evaluate essay the following three feature are examined rhetoric syntactic variety or the use of various structure in the arrangement of phase clause and sentence organization characteristic associated with the orderly presentation of idea such a rhetorical feature and linguistic cue and content vocabulary related to the topic such a relevant information and precise or specialized vocabulary the final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorial and column from the mainichi daily news newspaper a diagnosis for the essay is also given 
this paper demonstrates two method to improve the performance of instance based learning ibl algorithm for the problem of semantic role labeling srl two ibl algorithm are utilized k nearest neighbor knn and priority maximum likelihood pml with a modified back off combination method the experimental data are the wsj and brown corpus test set from the conll shared task it is shown that applying the tree based predicate argument recognition algorithm para to the data a a preprocessing stage allows knn and pml to deliver f and respectively on the wsj and f and on the brown corpus an increase of in f measurement over the most recent published pml result for this problem palmer et al training time for ibl algorithm are very much faster than for other widely used technique for srl e g parsing support vector machine perceptrons etc and the feature reduction effect of para yield testing and processing speed of around second per sentence for knn and second per sentence for pml respectively suggesting that ibl could be a more practical way to perform srl for nlp application where it is employed such a realtime machine translation or automatic speech recognition 
this paper describes an architecture to convert sinhala unicode text into phonemic specification of pronunciation the study wa mainly focused on disambiguating schwa and a vowel epenthesis for consonant which is one of the significant problem found in sinhala this problem ha been addressed by formulating a set of rule the proposed set of rule wa tested using distinct word obtained from a corpus and compared with the same word manually transcribed to phoneme by an expert the grapheme to phoneme g p conversion model achieves accuracy 
in this paper we present a novel approach for inducing word alignment from sentence aligned data we use a conditional random field crf a discriminative model which is estimated on a small supervised training set the crf is conditioned on both the source and target text and thus allows for the use of arbitrary and overlapping feature over these data moreover the crf ha efficient training and decoding process which both find globally optimal solution we apply this alignment model to both french english and romanian english language pair we show how a large number of highly predictive feature can be easily incorporated into the crf and demonstrate that even with only a few hundred word aligned training sentence our model improves over the current state of the art with alignment error rate of and for the two task respectively 
recently many natural language processing nlp application have improved the quality of their output by using various machine learning technique to mine information extraction ie pattern for capturing information from the input text currently to mine ie pattern one should know in advance the type of the information that should be captured by these pattern in this work we propose a novel methodology for corpus analysis based on cross examination of several document collection representing different instance of the same domain we show that this methodology can be used for automatic domain template creation a the problem of automatic domain template creation is rather new there is no well defined procedure for the evaluation of the domain template quality thus we propose a methodology for identifying what information should be present in the template using this information we evaluate the automatically created domain template through the text snippet retrieved according to the created template 
citation function is dened a the author s reason for citing a given paper e g acknowledgement of the use of the cited method the automatic recognition of the rhetorical function of citation in scientic text ha many application from improvement of impact factor calculation to text summarisation and more informative citation indexer we show that our annotation scheme for citation function is reliable and present a supervised machine learning framework to automatically classify citation function using both shallow and linguistically inspired feature we nd amongst other thing a strong relationship between citation function and sentiment classication 
this paper address the automatic classification of semantic relation in noun phrase based on cross linguistic evidence from a set of five romance language a set of novel semantic and contextual englishromance np feature is derived based on empirical observation on the distribution of the syntax and meaning of noun phrase on two corpus of different genre europarl and cluvi the feature were employed in a support vector machine algorithm which achieved an accuracy of europarl and cluvi an improvement compared with two state of the art model reported in the literature 
we have implemented parallelism primitive that permit an acl programmer to parallelize execution of acl function we introduce logical definition for these primitive explain the feature of our extension give an evaluation strategy for our implementation and use the parallelism primitive in example to show speedup 
the paper present an owl ontology for hpsg the hpsg ontology is integrated with an existing owl ontology gold a a community of practice extension the basic idea are illustrated by visualization of type hierarchy for part of speech 
this is a proposal for a an xml mark up of argumentation the annotation can be used to help the reader e g by mean of selective highlighting or diagramming and for further processing summarization critique use in information retrieval the article proposes a set of marker derived from manual corpus annotation exemplifies their use describes a way to assign them using surface cue and limited syntax for scoping and suggests further direction including an acquisition tool the application of machine learning and a collaborative dtd definition task 
nlp system for task such a question answering and information extraction typically rely on statistical parser but the efficacy of such parser can be surprisingly low particularly for sentence drawn from heterogeneous corpus such a the web we have observed that incorrect par often result in wildly implausible semantic interpretation of sentence which can be detected automatically using semantic information obtained from the web based on this observation we introduce web based semantic filtering a novel domain independent method for automatically detecting and discarding incorrect par we measure the effectiveness of our filtering system called woodward on two test collection on a set of trec question it reduces error by on a set of more complex penn treebank sentence the reduction in error rate wa 
this paper explores technique to take advantage of the fundamental difference in structure between hidden markov model hmm and hierarchical hidden markov model hhmm the hhmm structure allows repeated part of the model to be merged together a merged model take advantage of the recurring pattern within the hierarchy and the cluster that exist in some sequence of observation in order to increase the extraction accuracy this paper also present a new technique for reconstructing grammar rule automatically this work build on the idea of combining a phrase extraction method with hhmm to expose pattern within english text the reconstruction is then used to simplify the complex structure of an hhmm the model discussed here are evaluated by applying them to natural language task based on conll and a sub corpus of the lancaster treebank 
cross language text categorization is the task of assigning semantic class to document written in a target language e g english while the system is trained using labeled document in a source language e g italian in this work we present many solution according to the availability of bilingual resource and we show that it is possible to deal with the problem even when no such resource are accessible the core technique relies on the automatic acquisition of multilingual domain model from comparable corpus experiment show the effectiveness of our approach providing a low cost solution for the cross language text categorization task in particular when bilingual dictionary are available the performance of the categorization get close to that of monolingual text categorization 
speech recognition problem are a reality in current spoken dialogue system in order to better understand these phenomenon we study dependency between speech recognition problem and several higher level dialogue factor that define our notion of student state frustration anger certainty and correctness we apply chi square x analysis to a corpus of speech based computer tutoring dialogue to discover these dependency both within and across turn significant dependency are combined to produce interesting insight regarding speech recognition problem and to propose new strategy for handling these problem we also find that tutoring a a new domain for speech application exhibit interesting tradeoff and new factor to consider for spoken dialogue design 
in this work we investigate the use of error correcting output code ecoc for boosting centroid text classifier the implementation framework is to decompose one multi class problem into multiple binary problem and then learn the individual binary classification problem by centroid classifier however this kind of decomposition incurs considerable bias for centroid classifier which result in noticeable degradation of performance for centroid classifier in order to address this issue we use model refinement to adjust this so called bias the basic idea is to take advantage of misclassified example in the training data to iteratively refine and adjust the centroid of text data the experimental result reveal that model refinement can dramatically decrease the bias introduced by ecoc and the combined classifier is comparable to or even better than svm classifier in performance 
we present an overview of tarsqi a modular system for automatic temporal annotation that add time expression event and temporal relation to news text 
text is not unadulterated fact a text can make you laugh or cry but can it also make you short sell your stock in company a and buy up option in company b research in the domain of finance strongly suggests that it can study have shown that both the informational and affective aspect of news text affect the market in profound way impacting on volume of trade stock price volatility and even future firm earnings this paper aim to explore a computable metric of positive or negative polarity in financial news text which is consistent with human judgment and can be used in a quantitative analysis of news sentiment impact on financial market result from a preliminary evaluation are presented and discussed 
extracting tree transducer rule for syntactic mt system can be hindered by word alignment error that violate syntactic correspondence we propose a novel model for unsupervised word alignment which explicitly take into account target language constituent structure while retaining the robustness and efficiency of the hmm alignment model our model s prediction improve the yield of a tree transducer extraction system without sacrificing alignment quality we also discus the impact of various posteriorbased method of reconciling bidirectional alignment 
recently there is a need for a qa system to answer not only factoid question but also descriptive question descriptive question are question which need answer that contain definitional information about the search term or describe some special event we have proposed a new descriptive qa model and presented the result of a system which we have built to answer descriptive question we defined descriptive answer type dat s a answer type for descriptive question we discussed how our proposed model wa applied to the descriptive question with some experiment 
semantic parsing is the task of mapping natural language sentence to complete formal meaning representation the performance of semantic parsing can be potentially improved by using discriminative reranking which explores arbitrary global feature in this paper we investigate discriminative reranking upon a baseline semantic parser scissor where the composition of meaning representation is guided by syntax we examine if feature used for syntactic parsing can be adapted for semantic parsing by creating similar semantic feature based on the mapping between syntax and semantics we report experimental result on two real application an interpreter for coaching instruction in robotic soccer and a natural language database interface the result show that reranking can improve the performance on the coaching interpreter but not on the database interface 
we present an effective training algorithm for linearly scored dependency parser that implement online large margin multi class training crammer and singer crammer et al on top of efficient parsing technique for dependency tree eisner the trained parser achieve a competitive dependency accuracy for both english and czech with no language specific enhancement 
to study pp attachment disambiguation a a benchmark for empirical method in natural language processing it ha often been reduced to a binary decision problem between verb or noun attachment in a particular syntactic configuration a parser however must solve the more general task of deciding between more than two alternative in many different context we combine the attachment prediction made by a simple model of lexical attraction with a full fledged parser of german to determine the actual benefit of the subtask to parsing we show that the combination of data driven and rule based component can reduce the number of all parsing error by and raise the attachment accuracy for dependency parsing of german to an unprecedented 
transforming syntactic representation in order to improve parsing accuracy ha been exploited successfully in statistical parsing system using constituency based representation in this paper we show that similar transformation can give substantial improvement also in data driven dependency parsing experiment on the prague dependency treebank show that systematic transformation of coordinate structure and verb group result in a error reduction for a deterministic data driven dependency parser combining these transformation with previously proposed technique for recovering non projective dependency lead to state of the art accuracy for the given data set 
we evaluate the accuracy of an unlexicalized statistical parser trained on k treebanked sentence from balanced data and tested on the parc depbank we demonstrate that a parser which is competitive in accuracy without sacrificing processing speed can be quickly tuned without reliance on large in domain manually constructed treebanks this make it more practical to use statistical parser in application that need access to aspect of predicate argument structure the comparison of system using depbank is not straightforward so we extend and validate depbank and highlight a number of representation and scoring issue for relational evaluation scheme 
this paper show that a simple two stage approach to handle non local dependency in named entity recognition ner can outperform existing approach that handle non local dependency while being much more computationally efficient ner system typically use sequence model for tractable inference but this make them unable to capture the long distance structure present in text we use a conditional random field crf based ner system using local feature to make prediction and then train another crf which us both local information and feature extracted from the output of the first crf using feature capturing non local dependency from the same document our approach yield a relative error reduction on the f score over state of the art ner system using local information alone when compared to the relative error reduction offered by the best system that exploit non local information our approach also make it easy to incorporate non local information from other document in the test corpus and this give u a error reduction over ner system using local information alone additionally our running time for inference is just the inference time of two sequential crfs which is much le than that directly model the dependency and do approximate inference 
we present a novel method for extracting parallel sub sentential fragment from comparable non parallel bilingual corpus by analyzing potentially similar sentence pair using a signal processing inspired approach we detect which segment of the source sentence are translated into segment in the target sentence and which are not this method enables u to extract useful machine translation training data even from very non parallel corpus which contain no parallel sentence pair we evaluate the quality of the extracted data by showing that it improves the performance of a state of the art statistical machine translation system 
a minimally supervised machine learning framework is described for extracting relation of various complexity bootstrapping start from a small set of n ary relation instance a seed in order to automatically learn pattern rule from parsed data which then can extract new instance of the relation and it projection we propose a novel rule representation enabling the composition of n ary relation rule on top of the rule for projection of the relation the compositional approach to rule construction is supported by a bottom up pattern extraction method in comparison to other automatic approach our rule cannot only localize relation argument but also assign their exact target argument role the method is evaluated in two task the extraction of nobel prize award and management succession event performance for the new nobel prize task is strong for the management succession task the result compare favorably with those of existing pattern acquisition approach 
identification of transliterated name is a particularly difficult task of named entity recognition ner especially in the chinese context of all possible variation of transliterated named entity the difference between prc and taiwan is the most prevalent and most challenging in this paper we introduce a novel approach to the automatic extraction of diverging transliteration of foreign named entity by bootstrapping co occurrence statistic from tagged and segmented chinese corpus preliminary experiment yield promising result and show it potential in nlp application 
this paper present a novel algorithm for the acquisition of information extraction pattern the approach make the assumption that useful pattern will have similar meaning to those already identified a relevant pattern are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity evaluation show this algorithm performs well when compared with a previously reported document centric approach 
in this paper we study the utility of discourse structure for spoken dialogue performance modeling we experiment with various way of exploiting the discourse structure in isolation a context information for other factor correctness and certainty and through trajectory in the discourse structure hierarchy our correlation and paradise result show that while the discourse structure is not useful in isolation using the discourse structure a context information for other factor or via trajectory produce highly predictive parameter for performance analysis 
we describe a novel approach to statistical machine translation that combine syntactic information in the source language with recent advance in phrasal translation this method requires a source language dependency parser target language word segmentation and an unsupervised word alignment component we align a parallel corpus project the source dependency parse onto the target sentence extract dependency treelet translation pair and train a tree based ordering model we describe an efficient decoder and show that using these tree based model in combination with conventional smt model provides a promising approach that incorporates the power of phrasal smt with the linguistic generality available in a parser 
the language modeling community is showing a growing interest in using large collection of text mined from the world wide web www to supplement sparse in domain text resource however in most case the style and content of the text harvested from these corpus differs significantly from the specific nature of these domain in this paper we present a relative entropy r e based method to select relevant subset of sentence whose distribution in an n gram sense match the domain of interest using simulation we provide an analysis of how the proposed scheme outperforms filtering technique proposed in recent language modeling literature on mining text from the web a comparative study is presented using a text collection of over m word collected from the www experimental result show that by using the proposed subset selection scheme we can get performance improvement in both word error rate wer and perplexity ppl over the model built from the entire collection by using just of the data improvement in data selection also translated to a significant reduction in the vocabulary size a well a the number of estimated parameter in the adapted language model 
this paper describes a fully automatic two stage machine learning architecture that learns temporal relation between pair of event the first stage learns the temporal attribute of single event description such a tense grammatical aspect and aspectual class these imperfect guess combined with other linguistic feature are then used in a second stage to classify the temporal relationship between two event we present both an analysis of our new feature and result on the timebank corpus that is higher than previous work that used perfect human tagged feature 
existing named entity ne transliteration approach often exploit a general model to transliterate ne regardless of their origin a a result both a chinese name and a french name assuming it is already translated into chinese will be translated into english using the same model which often lead to unsatisfactory performance in this paper we propose a cluster specific ne transliteration framework we group name origin into a smaller number of cluster then train transliteration and language model for each cluster under a statistical machine translation framework given a source ne we first select appropriate model by classifying it into the most likely cluster then we transliterate this ne with the corresponding model we also propose a phrasebased name transliteration model which effectively combine context information for transliteration our experiment showed substantial improvement on the transliteration accuracy over a state of the art baseline system significantly reducing the transliteration character error rate from to 
obtaining high quality machine translation is still a long way off a post editing phase is required to improve the output of a machine translation system an alternative is the so called computer assisted translation in this framework a human translator interacts with the system in order to obtain high quality translation a statistical phrase based approach to computer assisted translation is described in this article a new decoder algorithm for interactive search is also presented that combine monotone and non monotone search the system ha been assessed in the trans type project for the translation of several printer manual from to english to from spanish german and french 
we present an approach for the joint extraction of entity and relation in the context of opinion recognition and analysis we identify two type of opinion related entity expression of opinion and source of opinion along with the linking relation that exists between them inspired by roth and yih we employ an integer linear programming approach to solve the joint opinion recognition task and show that global constraint based inference can significantly boost the performance of both relation extraction and the extraction of opinion related entity performance further improves when a semantic role labeling system is incorporated the resulting system achieves f measure of and for entity and relation extraction respectively improving substantially over prior result in the area 
semantic relatedness is a very important factor for the coreference resolution task to obtain this semantic information corpusbased approach commonly leverage pattern that can express a specific semantic relation the pattern however are designed manually and thus are not necessarily the most effective one in term of accuracy and breadth to deal with this problem in this paper we propose an approach that can automatically find the effective pattern for coreference resolution we explore how to automatically discover and evaluate pattern and how to exploit the pattern to obtain the semantic relatedness information the evaluation on ace data set show that the pattern based semantic information is helpful for coreference resolution 
we present a czech english statistical machine translation system which performs tree to tree translation of dependency structure the only bilingual resource required is a sentence aligned parallel corpus all other resource are monolingual we also refer to an evaluation method and plan to compare our system s output with a benchmark system 
morphological segmentation ha been shown to be beneficial to a range of nlp task such a machine translation speech recognition speech synthesis and information retrieval recently a number of approach to unsupervised morphological segmentation have been proposed this paper describes an algorithm that draw from previous approach and combine them into a simple model for morphological segmentation that outperforms other approach on english and german and also yield good result on agglutinative language such a finnish and turkish we also propose a method for detecting variation within stem in an unsupervised fashion the segmentation quality reached with the new algorithm is good enough to improve grapheme to phoneme conversion 
parsing is a computationally intensive task due to the combinatorial explosion seen in chart parsing algorithm that explore possible parse tree in this paper we propose a method to limit the combinatorial explosion by restricting the cyk chart parsing algorithm based on the output of a chunk parser when tested on the three parser presented in collins we observed an approximate three fold speedup with only an average decrease of in both precision and recall 
in this paper we propose a sentence ordering algorithm using a semi supervised sentence classification and historical ordering strategy the classification is based on the manifold structure underlying sentence addressing the problem of limited labeled data the historical ordering help to ensure topic continuity and avoid topic bias experiment demonstrate that the method is effective 
to solve a problem of how to evaluate computer produced summary a number of automatic and manual method have been proposed manual method evaluate summary correctly because human evaluate them but are costly on the other hand automatic method which use evaluation tool or program are low cost although these method cannot evaluate summary a accurately a manual method in this paper we investigate an automatic evaluation method that can reduce the error of traditional automatic method by using several evaluation result obtained manually we conducted some experiment using the data of the text summarization challenge tsc a comparison with conventional automatic method show that our method outperforms other method usually used 
we describe the semantic enrichment of journal article with chemical structure and biomedical ontology term using oscar a program for chemical named entity recognition ner we describe how oscar work and how it can been adapted for general ner we discus it implementation in a real publishing workflow and possible application for enriched article 
this paper study the problem of identifying erroneous correct sentence the problem ha important application e g providing feedback for writer of english a a second language controlling the quality of parallel bilingual sentence mined from the web and evaluating machine translation result in this paper we propose a new approach to detecting erroneous sentence by integrating pattern discovery with supervised learning model experimental result show that our technique are promising 
we present a study aimed at investigating the use of semantic information in a novel nlp application electronic career guidance ecg in german ecg is formulated a an information retrieval ir task whereby textual description of profession document are ranked for their relevance to natural language description of a person s professional interest the topic we compare the performance of two semantic ir model ir utilizing semantic relatedness sr measure based on either wordnet or wikipedia and a set of heuristic and ir measuring the similarity between the topic and document based on explicit semantic analysis esa gabrilovich and markovitch we evaluate the performance of sr measure intrinsically on the task of t computing sr and t solving reader s digest word power rdwp question 
consistency of corpus annotation is an essential property for the many us of annotated corpus in computational and theoretical linguistics while some research address the detection of inconsistency in positional annotation e g part of speech and continuous structural annotation e g syntactic constituency no approach ha yet been developed for automatically detecting annotation error in discontinuous structural annotation this is significant since the annotation of potentially discontinuous stretch of material is increasingly relevant from tree bank for free word order language to semantic and discourse annotation in this paper we discus how the variation n gram error detection approach dickinson and meurers a can be extended to discontinuous structural annotation we exemplify the approach by showing how it successfully detects error in the syntactic annotation of the german tiger corpus brant et al 
this paper present an approach for multilingual document clustering in comparable corpus the algorithm is of heuristic nature and it us a unique evidence for clustering the identification of cognate named entity between both side of the comparable corpus one of the main advantage of this approach is that it doe not depend on bilingual or multilingual resource however it depends on the possibility of identifying cognate named entity between the language used in the corpus an additional advantage of the approach is that it doe not need any information about the right number of cluster the algorithm calculates it we have tested this approach with a comparable corpus of news written in english and spanish in addition we have compared the result with a system which translates selected document feature the obtained result are encouraging 
dependency analysis of natural language ha gained importance for it applicability to nlp task non projective structure are common in dependency analysis therefore we need fine grained mean of describing them especially for the purpose of machine learning oriented approach like parsing we present an evaluation on twelve language which explores several constraint and measure on non projective structure we pursue an edge based approach concentrating on property of individual edge a opposed to property of whole tree in our evaluation we include previously unreported measure taking into account level of node in dependency tree our empirical result corroborate theoretical result and show that an edge based approach using level of node provides an accurate and at the same time expressive mean for capturing non projective structure in natural language 
previous work ha used monolingual parallel corpus to extract and generate paraphrase we show that this task can be done using bilingual parallel corpus a much more commonly available resource using alignment technique from phrase based statistical machine translation we show how paraphrase in one language can be identified using a phrase in another language a a pivot we define a paraphrase probability that allows paraphrase extracted from a bilingual parallel corpus to be ranked using translation probability and show how it can be refined to take contextual information into account we evaluate our paraphrase extraction and ranking method using a set of manual word alignment and contrast the quality with paraphrase extracted from automatic alignment 
software to translate english text into american sign language asl animation can improve information accessibility for the majority of deaf adult with limited english literacy asl natural language generation nlg is a special form of multimodal nlg that us multiple linguistic output channel asl nlg technology ha application for the generation of gesture animation and other communication signal that are not easily encoded a text string 
in this paper we present a new approach to controlling the behaviour of a natural language generation system by correlating internal decision taken during free generation of a wide range of text with the surface stylistic characteristic of the resulting output and using the correlation to control the generator this contrast with the generate and test architecture adopted by most previous empirically based generation approach offering a more efficient generic and holistic method of generator control we illustrate the approach by describing a system in which stylistic variation in the sense of biber can be effectively controlled during the generation of short medical information text 
complex task like question answering need to be able to identify event in text and the relation among those event we show that this event identification task and a related task identifying the semantic class of these event can both be formulated a classification problem in a word chunking paradigm we introduce a variety of linguistically motivated feature for this task and then train a system that is able to identify event with a precision of and a recall of we then show a variety of analysis of this model and their implication for the event identification task 
this paper demonstrates the usefulness of summary in an extrinsic task of relevance judgment based on a new method for measuring agreement relevance prediction which compare subject judgment on summary with their own judgment on full text document we demonstrate that because this measure is more reliable than previous gold standard measure we are able to make stronger statistical statement about the benefit of summarization we found positive correlation between rouge score and two different summary type where only weak or negative correlation were found using other agreement measure however we show that rouge may be sensitive to the choice of summarization style we discus the importance of these result and the implication for future summarization evaluation 
we consider the task of unsupervised lecture segmentation we formalize segmentation a a graph partitioning task that optimizes the normalized cut criterion our approach move beyond localized comparison and take into account long range cohesion dependency our result demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition error 
we extended language modeling approach in information retrieval ir to combine collaborative filtering cf and content based filtering cbf our approach is based on the analogy between ir and cf especially between cf and relevance feedback rf both cf and rf exploit user preference relevance judgment to recommend item we first introduce a multinomial model that combine cf and cbf in a language modeling framework we then generalize the model to another multinomial model that approximates the polya distribution this generalized model outperforms the multinomial model by for cbf and for cf in recommending english wikipedia article the performance of the generalized model for three different datasets wa comparable to that of a state of the art item based cf method 
in this paper we describe the current state of a new japanese lexical resource the hinoki treebank the treebank is built from dictionary definition sentence and us an hpsg based japanese grammar to encode both syntactic and semantic information it is combined with an ontology based on the definition sentence to give a detailed sense level description of the most familiar word of japanese 
this paper describes a method of interactively visualizing and directing the process of translating a sentence the method allows a user to explore a model of syntax based statistical machine translation mt to understand the model s strength and weakness and to compare it to other mt system using this visualization method we can find and address conceptual and practical problem in an mt system in our demonstration at acl new user of our tool will drive a syntax based decoder for themselves 
this paper describes mimus a multimodal and multilingual dialogue system for the in home scenario which allows user to control some home device by voice and or click it design relies on wizard of oz experiment and is targeted at disabled user mimus follows the information state update approach to dialogue management and support english german and spanish with the possibility of changing language on the fly mimus includes a gesture enabled talking head which endows the system with a human like personality 
at least two kind of relation exist among related word taxonomical relation and thematic relation both relation identify related word useful to language understanding and generation information retrieval and so on however although word with taxonomical relation are easy to identify from linguistic resource such a dictionary and thesaurus word with thematic relation are difficult to identify because they are rarely maintained in linguistic resource in this paper we sought to extract thematically non taxonomically related word set among word in document by employing case marking particle derived from syntactic analysis we then verified the usefulness of word set with non taxonomical relation that seems to be a thematic relation for information retrieval 
we propose an unsupervised segmentation method based on an assumption about language data that the increasing point of entropy of successive character is the location of a word boundary a large scale experiment wa conducted by using mb of unsegmented training data and mb of test data and precision of wa attained with recall being around moreover we found that the precision wa stable at around independently of the learning data size 
obtaining large volume of inference knowledge such a entailment rule ha become a major factor in achieving robust semantic processing while there ha been substantial research on learning algorithm for such knowledge their evaluation methodology ha been problematic hindering further research we propose a novel evaluation methodology for entailment rule which explicitly address their semantic property and yield satisfactory human agreement level the methodology is used to compare two state of the art learning algorithm exposing critical issue for future progress 
we investigate the connection between part of speech po distribution and content in language we define po block to be group of part of speech we hypothesise that there exists a directly proportional relation between the frequency of po block and their content salience we also hypothesise that the class membership of the part of speech within such block reflects the content load of the block on the basis that open class part of speech are more content bearing than closed class part of speech we test these hypothesis in the context of information retrieval by syntactically representing query and removing from them content poor block in line with the aforementioned hypothesis for our first hypothesis we induce po distribution information from a corpus and approximate the probability of occurrence of po block a per two statistical estimator separately for our second hypothesis we use simple heuristic to estimate the content load within po block we use the text retrieval conference trec query of and to retrieve document from the wt g and wt g test collection with five different retrieval strategy experimental outcome confirm that our hypothesis hold in the context of information retrieval 
we propose a bootstrapping approach to training a memoriless stochastic transducer for the task of extracting transliteration from an english arabic bitext the transducer learns it similarity metric from the data in the bitext and thus can function directly on string written in different writing script without any additional language knowledge we show that this bootstrapped transducer performs a well or better than a model designed specifically to detect arabic english transliteration 
deterministic parsing guided by treebank induced classifier ha emerged a a simple and efficient alternative to more complex model for data driven parsing we present a systematic comparison of memory based learning mbl and support vector machine svm for inducing classifier for deterministic dependency parsing using data from chinese english and swedish together with a variety of different feature model the comparison show that svm give higher accuracy for richly articulated feature model across all language albeit with considerably longer training time the result also confirm that classifier based deterministic parsing can achieve parsing accuracy very close to the best result reported for more complex parsing model 
this paper present a language independent probabilistic answer ranking framework for question answering the framework estimate the probability of an individual answer candidate given the degree of answer relevance and the amount of supporting evidence provided in the set of answer candidate for the question our approach wa evaluated by comparing the candidate answer set generated by chinese and japanese answer extractor with the re ranked answer set produced by the answer ranking framework empirical result from testing on ntcir factoid question show a performance improvement in chinese answer selection and a improvement in japanese answer selection 
this paper describes a minimal topology driven parsing algorithm for topological grammar that synchronizes a rewriting grammar and a dependency grammar obtaining two linguistically motivated syntactic structure the use of non local slash and visitor feature can be restricted to obtain a cky type analysis in polynomial time german long distance phenomenon illustrate the algorithm bringing to the fore the procedural need of the analysis of syntax topology mismatch in constraint based approach like for example hpsg 
gorman and curran argue that thesaurus generation for billion word corpus is problematic a the full computation take many day we present an algorithm with which the computation take under two hour we have created and made publicly available thesaurus based on large corpus for at time of writing seven major world language the development is implemented in the sketch engine kilgarriff et al another innovative development in the same tool is the presentation of the grammatical behaviour of a word against the background of how all other word of the same word class behave thus the english noun constraint occurs in the plural is this a salient lexical fact to form a judgement we need to know the distribution for all noun we use histogram to present the distribution in a way that is easy to grasp 
the natural language toolkit is a suite of program module data set and tutorial supporting research and teaching in computational linguistics and natural language processing nltk is written in python and distributed under the gpl open source license over the past year the toolkit ha been rewritten simplifying many linguistic data structure and taking advantage of recent enhancement in the python language this paper report on the simplified toolkit and explains how it is used in teaching nlp 
we present a new approach to relation extraction that requires only a handful of training example given a few pair of named entity known to exhibit or not exhibit a particular relation bag of sentence containing the pair are extracted from the web we extend an existing relation extraction method to handle this weaker form of supervision and present experimental result demonstrating that our approach can reliably extract relation from web document 
this paper present a novel training algorithm for a linearly scored block sequence translation model the key component is a new procedure to directly optimize the global scoring function used by a smt decoder no translation language or distortion model probability are used a in earlier work on smt therefore our method which employ le domain specific knowledge is both simpler and more extensible than previous approach moreover the training procedure treat the decoder a a black box and thus can be used to optimize any decoding scheme the training algorithm is evaluated on a standard arabic english translation task 
this paper defines a generative probabilistic model of parse tree which we call pcfg la this model is an extension of pcfg in which non terminal symbol are augmented with latent variable fine grained cfg rule are automatically induced from a parsed corpus by training a pcfg la model using an em algorithm because exact parsing with a pcfg la is np hard several approximation are described and empirically compared in experiment using the penn wsj corpus our automatically trained model gave a performance of f sentence word which is comparable to that of an unlexicalized pcfg parser created using extensive manual feature selection 
this paper compare different bilexical tree based model for bilingual alignment em training for the new model benefit from the dynamic programming hook trick the model produce improved dependency structure for both language 
we directly investigate a subject of much recent debate do word sense disambiguation model help statistical machine translation quality we present empirical result casting doubt on this common but unproved assumption using a state of the art chinese word sense disambiguation model to choose translation candidate for a typical ibm statistical mt system we find that word sense disambiguation doe not yield significantly better translation quality than the statistical machine translation system alone error analysis suggests several key factor behind this surprising finding including inherent limitation of current statistical mt architecture 
how can protein fold so quickly into their unique native structure we show here that there is a natural analogy between parsing and the protein folding problem and demonstrate that cky can find the native structure of a simplified lattice model of protein with high accuracy 
there have been many proposal to extract semantically related word using measure of distributional similarity but these typically are not able to distinguish between synonym and other type of semantically related word such a antonym co hyponym and hypernym we present a method based on automatic word alignment of parallel corpus consisting of document translated into multiple language and compare our method with a monolingual syntax based method the approach that us aligned multilingual data to extract synonym show much higher precision and recall score for the task of synonym extraction than the monolingual syntax based approach 
this paper introduces a new method for identifying candidate phrasal term also known a multiword unit which applies a nonparametric rank based heuristic measure evaluation of this measure the mutual rank ratio metric show that it produce better result than standard statistical measure when applied to this task 
over the last fifty year the big five model of personality trait ha become a standard in psychology and research ha systematically documented correlation between a wide range of linguistic variable and the big five trait a distinct line of research ha explored method for automatically generating language that varies along personality dimension we present personage personality generator the first highly parametrizable language generator for extraversion an important aspect of personality we evaluate two personality generation method direct generation with particular parameter setting suggested by the psychology literature and overgeneration and selection using statistical model trained from judge s rating result show that both method reliably generate utterance that vary along the extraversion dimension according to human judge 
this paper present a function word centered syntax based fws solution to address phrase ordering in the context of statistical machine translation smt motivated by the observation that function word often encode grammatical relationship among phrase within a sentence we propose a probabilistic synchronous grammar to model the ordering of function word and their left and right argument we improve phrase ordering performance by lexicalizing the resulting rule in a small number of case corresponding to function word the experiment show that the fws approach consistently outperforms the baseline system in ordering function word argument and improving translation quality in both perfect and noisy word alignment scenario 
this paper present the result of experiment in which we tested different kind of feature for retrieval of chinese opinionated text we assume that the task of retrieval of opinionated text oir can be regarded a a subtask of general ir but with some distinct feature the experiment showed that the best result were obtained from the combination of character based processing dictionary look up maximum matching and a negation check 
the aim of this paper is to present a simple yet efficient implementation of a tool for simultaneous rule based morphosyntactic tagging and partial parsing formalism the parser is currently used for creating a tree bank of partial par in a valency acquisition project over the ipi pan corpus of polish 
adapting language model across style and topic such a for lecture transcription involves combining generic style model with topic specific content relevant to the target document in this work we investigate the use of the hidden markov model with latent dirichlet allocation hmm lda to obtain syntactic state and semantic topic assignment to word instance in the training corpus from these context dependent label we construct style and topic model that better model the target document and extend the traditional bag of word topic model to n gram experiment with static model interpolation yielded a perplexity and relative word error rate wer reduction of and respectively over an adapted trigram baseline adaptive interpolation of mixture component further reduced perplexity by and wer by a modest 
recent research present conflicting evidence on whether word sense disambiguation wsd system can help to improve the performance of statistical machine translation mt system in this paper we successfully integrate a state of the art wsd system into a state of the art hierarchical phrase based mt system hiero we show for the first time that integrating a wsd system improves the performance of a state ofthe art statistical mt system on an actual translation task furthermore the improvement is statistically significant 
this paper describes an extremely lexicalized probabilistic model for fast and accurate hpsg parsing in this model the probability of parse tree are defined with only the probability of selecting lexical entry the proposed model is very simple and experiment revealed that the implemented parser run around four time faster than the previous model and that the proposed model ha a high accuracy comparable to that of the previous model for probabilistic hpsg which is defined over phrase structure we also developed a hybrid of our probabilistic model and the conventional phrase structure based model the hybrid model is not only significantly faster but also significantly more accurate by two point of precision and recall compared to the previous model 
we propose a method for extracting semantic orientation of word desirable or undesirable regarding semantic orientation a spin of electron we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function we also propose a criterion for parameter selection on the basis of magnetization given only a small number of seed word the proposed method extract semantic orientation with high accuracy in the experiment on english lexicon the result is comparable to the best value ever reported 
general information retrieval system are designed to serve all user without considering individual need in this paper we propose a novel approach to personalized search it can in a unified way exploit and utilize implicit feedback information such a query log and immediately viewed document moreover our approach can implement result re ranking and query expansion simultaneously and collaboratively based on this approach we develop a client side personalized web search agent pair personalized assistant for information retrieval which support both english and chinese our experiment on trec and htrdp collection clearly show that the new approach is both effective and efficient 
dependency based representation of natural language syntax require a fine balance between structural flexibility and computational complexity in previous work several constraint have been proposed to identify class of dependency structure that are wellbalanced in this sense the best known but also most restrictive of these is projectivity most constraint are formulated on fully specified structure which make them hard to integrate into model where structure are composed from lexical information in this paper we show how two empirically relevant relaxation of projectivity can be lexicalized and how combining the resulting lexicon with a regular mean of syntactic composition give rise to a hierarchy of mildly context sensitive dependency language 
this paper proposes an efficient method of sentence retrieval based on syntactic structure collins proposed tree kernel to calculate structural similarity however structual retrieval based on tree kernel is not practicable because the size of the index table by tree kernel becomes impractical we propose more efficient algorithm approximating tree kernel tree overlapping and subpath set these algorithm are more efficient than tree kernel because indexing is possible with practical computation resource the result of the experiment comparing these three algorithm showed that structural retrieval with tree overlapping and subpath set were faster than that with tree kernel by time and time respectively 
we analyze humorous spoken conversation from a classic comedy television show friend by examining acoustic prosodic and linguistic feature and their utility in automatic humor recognition using a simple annotation scheme we automatically label speaker turn in our corpus that are followed by laugh a humorous and the rest a non humorous our humor prosody analysis reveals significant difference in prosodic characteristic such a pitch tempo energy etc of humorous and non humorous speech even when accounted for the gender and speaker difference humor recognition wa carried out using standard supervised learning classifier and show promising result significantly above the baseline 
in this paper we present valido a tool that support the difficult task of validating sense choice produced by a set of annotator the validator can analyse the semantic graph resulting from each sense choice and decide which sense is more coherent with respect to the structure of the adopted lexicon we describe the interface and report an evaluation of the tool in the validation of manual sense annotation 
abstract we demonstrate one aspect of an affectextraction system for use in intelligent conversational agent this aspect performs a degree of affective interpretation of some type of metaphorical utterance 
this paper describes an ongoing effort to parse the hebrew bible the parser consults the bracketing information extracted from the cantillation mark of the masoetic text we first constructed a cantillation treebank which encodes the prosodic structure of the text it wa found that many of the prosodic boundary in the cantillation tree correspond directly or indirectly to the phrase boundary of the syntactic tree we are trying to build all the useful boundary information wa then extracted to help the parser make syntactic decision either serving a hard constraint in rule application or used probabilistically in tree ranking this ha greatly improved the accuracy and efficiency of the parser and reduced the amount of manual work in building a hebrew treebank 
we propose a general method for reranker construction which target choosing the candidate with the least expected loss rather than the most probable candidate different approach to expected loss approximation are considered including estimating from the probabilistic model used to generate the candidate estimating from a discriminative model trained to rerank the candidate and learning to approximate the expected loss the proposed method are applied to the parse reranking task with various baseline model achieving significant improvement both over the probabilistic model and the discriminative rerankers when a neural network parser is used a the probabilistic model and the voted perceptron algorithm with data defined kernel a the learning algorithm the loss minimization model achieves labeled constituent f score on the standard wsj parsing task 
we present a method for automatic determiner selection based on an existing language model we train on the penn treebank and also use additional data from the north american news text corpus our result are a significant improvement over previous best 
the part whole relation is of special importance in biomedicine structure and process are organised along partitive ax anatomy for example is rich in part whole relation this paper report preliminary experiment on part whole extraction from a corpus of anatomy definition using a fully automatic iterative algorithm to learn simple lexico syntactic pattern from multiword term the experiment show that meronym can be extracted using these pattern a failure analysis point out factor that could contribute to improvement in both precision and recall including pattern generalisation pattern pruning and term matching the analysis give insight into the relationship between domain terminology and lexical relation and into evaluation strategy for relation learning 
in this paper we introduce semtag a free and open software architecture for the development of tree adjoining grammar integrating a compositional semantics semtag differs from xtag in two main way first it provides an expressive grammar formalism and compiler for factorising and specifying tag second it support semantic construction 
we proposed a subword based tagging for chinese word segmentation to improve the existing character based tagging the subword based tagging wa implemented using the maximum entropy maxent and the conditional random field crf method we found that the proposed subword based tagging outperformed the character based tagging in all comparative experiment in addition we proposed a confidence measure approach to combine the result of a dictionary based and a subword tagging based segmentation this approach can produce an ideal tradeoff between the in vocaulary rate and out of vocabulary rate our technique were evaluated using the test data from sighan bakeoff we achieved higher f score than the best result in three of the four corpus pku cityu and msr 
semantic relation between text concept denote the core element of lexical semantics this paper present a model for the automatic detection of intention semantic relation our approach first identifies the syntactic pattern that encode intention then we select syntactic and semantic feature for a svm learning classifier in conclusion we discus the application of intention relation to q a 
in this paper we study different centrality measure being used in predicting noun phrase appearing in the abstract of scientific article our experimental result show that centrality measure improve the accuracy of the prediction in term of both precision and recall we also found that the method of constructing noun phrase network significantly influence the accuracy when using the centrality heuristic itself but is negligible when it is used together with other text feature in decision tree 
this paper explores the use of a character segment based character correction model language modeling and shallow morphology for arabic ocr error correction experimentation show that character segment based correction is superior to single character correction and that language modeling boost correction by improving the ranking of candidate correction while shallow morphology had a small adverse effect further given sufficiently large corpus to extract a dictionary and to train a language model word based correction work well for a morphologically rich language such a arabic 
psychiatric document retrieval attempt to help people to efficiently and effectively locate the consultation document relevant to their depressive problem individual can understand how to alleviate their symptom according to recommendation in the relevant document this work proposes the use of high level topic information extracted from consultation document to improve the precision of retrieval result the topic information adopted herein includes negative life event depressive symptom and semantic relation between symptom which are beneficial for better understanding of user query experimental result show that the proposed approach achieves higher precision than the word based retrieval model namely the vector space model vsm and okapi model adopting word level information alone 
to improve the interaction between student and an intelligent tutoring system we developed two natural language generator that we systematically evaluated in a three way comparison that included the original system a well we found that the generator which intuitively produce the best language doe engender the most learning specifically it appears that functional aggregation is responsible for the improvement 
we present a novel approach to automatically annotate image using associated text we detect and classify all entity person and object in the text after which we determine the salience the importance of an entity in a text and visualness the extent to which an entity can be perceived visually of these entity we combine these measure to compute the probability that an entity is present in the image the suitability of our approach wa successfully tested on image text pair of yahoo news 
the paper present the position specific posterior lattice pspl a novel lossy representation of automatic speech recognition lattice that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken document in experiment performed on a collection of lecture recording mit icampus data the spoken document ranking accuracy wa improved by relative over the commonly used baseline of indexing the best output from an automatic speech recognizer the inverted index built from pspl lattice is compact about of the size of gram asr lattice and of the size of the uncompressed speech and it allows for extremely fast retrieval furthermore little degradation in performance is observed when pruning pspl lattice resulting in even smaller index of the size of gram asr lattice 
we present an approach to mt between turkic language and present result from an implementation of a mt system from turkmen to turkish our approach relies on ambiguous lexical and morphological transfer augmented with target side rule based repair and rescoring with statistical language model 
i propose a computational treatment of superlative starting with superlative construction and the main challenge in automatically recognising and extracting their component initial experimental evidence is provided for the value of the proposed work for question answering i also briefly discus it potential value for sentiment detection and opinion extraction 
in this paper we describe the research using machine learning technique to build a comma checker to be integrated in a grammar checker for basque after several experiment and trained with a little corpus of word the system guess correctly not placing comma with a precision of and a recall of it also get a precision of and a recall of in the task of placing comma finally we have shown that these result can be improved using a bigger and a more homogeneous corpus to train that is a bigger corpus written by one unique author 
discriminative learning method are widely used in natural language processing these method work best when their training and test data are drawn from the same distribution for many nlp task however we are confronted with new domain in which labeled data is scarce or non existent in such case we seek to adapt existing model from a resource rich source domain to a resource poor target domain we introduce structural correspondence learning to automatically induce correspondence among feature from different domain we test our technique on part of speech tagging and show performance gain for varying amount of source and target training data a well a improvement in target domain parsing accuracy using our improved tagger 
this paper explores technique for reducing the effectiveness of standard authorship attribution technique so that an author a can preserve anonymity for a particular document d we discus feature selection and adjustment and show how this information can be fed back to the author to create a new document d for which the calculated attribution move away from a since it can be labor intensive to adjust the document in this fashion we attempt to quantify the amount of effort required to produce the anonymized document and introduce two level of anonymization shallow and deep in our test set we show that shallow anonymization can be achieved by making change per word to reduce the likelihood of identifying a a the author by an average of more than for deep anonymization we adapt the unmasking work of koppel and schler to provide feedback that allows the author to choose the level of anonymization 
this paper describes an on going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on chinese character the ultimate goal of this project is set to construct a cognitively sound and computationally effective character grounded machine understandable resource philosophically chinese ideogram ha it ontological status but it applicability to the nlp task ha not been expressed explicitly in term of language resource we thus propose the first attempt to locate chinese character within the context of ontology having the primary success in applying it to some nlp task we believe that the construction of this knowledge resource will shed new light on theoretical setting a well a the construction of chinese lexical semantic resource 
syntactic parsing requires a fine balance between expressivity and complexity so that naturally occurring structure can be accurately parsed without compromising efficiency in dependency based parsing several constraint have been proposed that restrict the class of permissible structure such a projectivity planarity multi planarity well nestedness gap degree and edge degree while projectivity is generally taken to be too restrictive for natural language syntax it is not clear which of the other proposal strike the best balance between expressivity and complexity in this paper we review and compare the different constraint theoretically and provide an experimental evaluation using data from two treebanks investigating how large a proportion of the structure found in the treebanks are permitted under different constraint the result indicate that a combination of the well nestedness constraint and a parametric constraint on discontinuity give a very good fit with the linguistic data 
this paper investigates a machine learning approach for temporally ordering and anchoring event in natural language text to address data sparseness we used temporal reasoning a an oversampling method to dramatically expand the amount of training data resulting in predictive accuracy on link labeling a high a using a maximum entropy classifier on human annotated data this method compared favorably against a series of increasingly sophisticated baseline involving expansion of rule derived from human intuition 
we study the impact of syntactic and shallow semantic information in automatic classification of question and answer and answer re ranking we define a new tree structure based on shallow semantics encoded in predicate argument structure pas and b new kernel function to exploit the representational power of such structure with support vector machine our experiment suggest that syntactic information help task such a question answer classification and that shallow semantics give remarkable contribution when a reliable set of pas can be extracted e g from answer 
named entity translation is indispensable in cross language information retrieval nowadays we propose an approach of combining lexical information web statistic and inverse search based on google to backward translate a chinese named entity ne into english our system achieves a high top accuracy of which is a relatively good performance reported in this area until present 
many learning task have subtasks for which much training data exists therefore we want to transfer learning from the old generalpurpose subtask to a more specific new task for which there is often le data while work in transfer learning often considers how the old task should affect learning on the new task in this paper we show that it help to take into account how the new task affect the old specifically we perform joint decoding of separately trained sequence model preserving uncertainty between the task and allowing information from the new task to affect prediction on the old task on two standard text data set we show that joint decoding outperforms cascaded decoding 
our paper report an attempt to apply an unsupervised clustering algorithm to a hungarian treebank in order to obtain semantic verb class starting from the hypothesis that semantic metapredicates underlie verb syntactic realization we investigate how one can obtain semantically motivated verb class by automatic mean the most frequent hungarian verb were clustered on the basis of their complementation pattern yielding a set of basic class and hint about the feature that determine verbal subcategorization the resulting class serve a a basis for the subsequent analysis of their alternation behavior 
this paper proposes a knowledge representation model and a logic proving setting with axiom on demand successfully used for recognizing textual entailment it also detail a lexical inference system which boost the performance of the deep semantic oriented approach on the rte data the linear combination of two slightly different logical system with the third lexical inference system achieves accuracy on the rte data 
entity relation detection is a form of information extraction that find predefined relation between pair of entity in text this paper describes a relation detection approach that combine clue from different level of syntactic processing using kernel method information from three different level of processing is considered tokenization sentence parsing and deep dependency analysis each source of information is represented by kernel function then composite kernel are developed to integrate and extend individual kernel so that processing error occurring at one level can be overcome by information from other level we present an evaluation of these method on the ace relation detection task using support vector machine and show that each level of syntactic processing contributes useful information for this task when evaluated on the official test data our approach produced very competitive ace value score we also compare the svm with knn on different kernel 
in this paper we present paraeval an automatic evaluation framework that us paraphrase to improve the quality of machine translation evaluation previous work ha focused on fixed n gram evaluation metric coupled with lexical identity matching paraeval address three important issue support for paraphrase synonym matching recall measurement and correlation with human judgment we show that paraeval correlate significantly better than bleu with human assessment in measurement for both fluency and adequacy 
we address the problem dealing with skewed data and propose a method for estimating effective training story for the topic tracking task for a small number of labelled positive story we extract story pair which consist of positive and it associated story from bilingual comparable corpus to overcome the problem of a large number of labelled negative story we classify them into some cluster this is done by using k mean with em the result on the tdt corpus show the effectiveness of the method 
this paper investigates the automatic identification of aspect of information structure is in text the experiment use the prague dependency treebank which is annotated with is following the praguian approach of topic focus articulation we automatically detect t opic and f ocus using node attribute from the treebank a basic feature and derived feature inspired by the annotation guideline we show the performance of c bagging and ripper classifier on several class of instance such a noun and pronoun only noun only pronoun a baseline system assigning always f ocus ha an f score of our best system obtains 
we propose widl expression a a flexible formalism that facilitates the integration of a generic sentence realization system within end to end language processing application widl expression represent compactly probability distribution over finite set of candidate realization and have optimal algorithm for realization via interpolation with language model probability distribution we show the effectiveness of a widl based nlg system in two sentence realization task automatic translation and headline generation 
information extraction ie is the task of extracting knowledge from unstructured text we present a novel unsupervised approach for information extraction based on graph mutual reinforcement the proposed approach doe not require any seed pattern or example instead it depends on redundancy in large data set and graph based mutual reinforcement to induce generalized extraction pattern the proposed approach ha been used to acquire extraction pattern for the ace automatic content extraction relation detection and characterization rdc task ace rdc is considered a hard task in information extraction due to the absence of large amount of training data and inconsistency in the available data the proposed approach achieves superior performance which could be compared to supervised technique with reasonable training data 
domain adaptation is an important problem in natural language processing nlp due to the lack of labeled data in novel domain in this paper we study the domain adaptation problem from the instance weighting perspective we formally analyze and characterize the domain adaptation problem from a distributional view and show that there are two distinct need for adaptation corresponding to the different distribution of instance and classification function in the source and the target domain we then propose a general instance weighting framework for domain adaptation our empirical result on three nlp task show that incorporating and exploiting more information from the target domain through instance weighting is effective 
statistical machine translation system are based on one or more translation model and a language model of the target language while many different translation model and phrase extraction algorithm have been proposed a standard word n gram back off language model is used in most system in this work we propose to use a new statistical language model that is based on a continuous representation of the word in the vocabulary a neural network is used to perform the projection and the probability estimation we consider the translation of european parliament speech this task is part of an international evaluation organized by the tc star project in the proposed method achieves consistent improvement in the bleu score on the development and test data we also present algorithm to improve the estimation of the language model probability when splitting long sentence into shorter chunk 
in this paper we describe a novel distributed language model for n best list re ranking the model is based on the client server paradigm where each server host a portion of the data and provides information to the client this model allows for using an arbitrarily large corpus in a very efficient way it also provides a natural platform for relevance weighting and selection we applied this model on a billion word corpus and re ranked the n best list from hiero a state of the art phrase based system using bleu a a metric the re ranked translation achieves a relative improvement of significantly better than the model best translation 
we present an automatic approach to tree annotation in which basic nonterminal symbol are alternately split and merged to maximize the likelihood of a training treebank starting with a simple x bar grammar we learn a new grammar whose nonterminals are subsymbols of the original nonterminals in contrast with previous work we are able to split various terminal to different degree a appropriate to the actual complexity in the data our grammar automatically learn the kind of linguistic distinction exhibited in previous work on manual tree annotation on the other hand our grammar are much more compact and substantially more accurate than previous work on automatic annotation despite it simplicity our best grammar achieves an f of on the penn treebank higher than fully lexicalized system 
this paper demonstrates how unsupervised technique can be used to learn model of deep linguistic structure determining the semantic role of a verb s dependent is an important step in natural language understanding we present a method for learning model of verb argument pattern directly from unannotated text the learned model are similar to existing verb lexicon such a verbnet and propbank but additionally include statistic about the linkings used by each verb the method is based on a structured probabilistic model of the domain and unsupervised learning is performed with the em algorithm the learned model can also be used discriminatively a semantic role labelers and when evaluated relative to the propbank annotation the best learned model reduces of the error between an informed baseline and an oracle upper bound 
over the last few year two of the main research direction in machine learning of natural language processing have been the study of semi supervised learning algorithm a a way to train classiers when the labeled data is scarce and the study of way to exploit knowledge and global information in structured learning task in this paper we suggest a method for incorporating domain knowledge in semi supervised learning algorithm our novel framework unies and can exploit several kind of task specic constraint the experimental result presented in the information extraction domain demonstrate that applying constraint help the model to generate better feedback during learning and hence the framework allows for high performance learning with significantly le training data than wa possible before on these task 
we introduce a simple method to pack word for statistical word alignment our goal is to simplify the task of automatic word alignment by packing several consecutive word together when we believe they correspond to a single word in the opposite language this is done using the word aligner itself i e by bootstrapping on it output we evaluate the performance of our approach on a chinese to english machine translation task and report a relative increase in bleu score over a state of the art phrasebased smt system 
we analyze the concept of focus in speech and the relationship between focus and speech act for prosodic generation we determine how the speaker s utterance are influenced by speaker s intention the relationship between speech act and focus information is used to define which part of the sentence serve a the focus part we propose the focus to emphasize tone fet structure to analyze the focus component we also design the fet grammar to analyze the intonation pattern and produce tone mark a a result of our analysis we present a proof of the concept working example to validate our proposal more comprehensive evaluation are part of our current work 
this paper introduces a maximum entropy dependency parser based on an efficient kbest maximum spanning tree mst algorithm although recent work suggests that the edge factored constraint of the mst algorithm significantly inhibit parsing accuracy we show that generating the best par according to an edge factored model ha an oracle performance well above the best performance of the best dependency parser this motivates our parsing approach which is based on reranking the kbest par generated by an edge factored model oracle parse accuracy result are presented for the edge factored model and best result for the reranker on eight language seven from conll x and english 
fine grained sense distinction are one of the major obstacle to successful word sense disambiguation in this paper we present a method for reducing the granularity of the wordnet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchy namely the oxford dictionary of english we ass the quality of the mapping and the induced clustering and evaluate the performance of coarse wsd system in the senseval english all word task 
in this paper we show that generative model are competitive with and sometimes superior to discriminative model when both kind of model are allowed to learn structure that are optimal for discrimination in particular we compare bayesian network and conditional loglinear model on two nlp task we observe that when the structure of the generative model encodes very strong independence assumption a la naive bayes a discriminative model is superior but when the generative model is allowed to weaken these independence assumption via learning a more complex structure it can achieve very similar or better performance than a corresponding discriminative model in addition a structure learning for generative model is far more efficient they may be preferable for some task 
an unsupervised part of speech po tagging system that relies on graph clustering method is described unlike in current state of the art approach the kind and number of different tag is generated by the method itself we compute and merge two partitioning of word graph one based on context similarity of high frequency word another on log likelihood statistic for word of lower frequency using the resulting word cluster a a lexicon a viterbi po tagger is trained which is refined by a morphological component the approach is evaluated on three different language by measuring agreement with existing tagger 
statistical machine translation is quite robust when it come to the choice of input representation it only requires consistency between training and testing a a result there is a wide range of possible preprocessing choice for data used in statistical machine translation this is even more so for morphologically rich language such a arabic in this paper we study the effect of different word level preprocessing scheme for arabic on the quality of phrase based statistical machine translation we also present and evaluate different method for combining preprocessing scheme resulting in improved translation quality 
word sense disambiguation suffers from a long standing problem of knowledge acquisition bottleneck although state of the art supervised system report good accuracy for selected word they have not been shown to be promising in term of scalability in this paper we present an approach for learning coarser and more general set of concept from a sense tagged corpus in order to alleviate the knowledge acquisition bottleneck we show that these general concept can be transformed to fine grained word sens using simple heuristic and applying the technique for recent senseval data set show that our approach can yield state of the art performance 
the present paper proposes a method by which to translate output of a robust hpsg parser into semantic representation of typed dynamic logic tdl a dynamic plural semantics defined in typed lambda calculus with it higher order representation of context tdl analyzes and describes the inherently inter sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner the present study show that the proposed translation method successfully combine robustness and descriptive adequacy of contemporary semantics the present implementation achieves high coverage approximately for the real text of the penn treebank corpus 
the paper present a new model for context dependent interpretation of linguistic expression about spatial proximity between object in a natural scene the paper discus novel psycholinguistic experimental data that test and verifies the model the model ha been implemented and enables a conversational robot to identify object in a scene through topological spatial relation e g x near y the model can help motivate the choice between topological and projective preposition 
the aim of this paper is to present a new method for identifying linguistic structure in the aggregate analysis of the language variation the method consists of extracting the most frequent sound correspondence from the aligned transcription of word based on the extracted correspondence every site is compared to all other site and a correspondence index is calculated for each site this method enables u to identify sound alternation responsible for dialect division and to measure the extent to which each alternation is responsible for the division obtained by the aggregate analysis 
this paper present noisy channel based korean preprocessor system which corrects word spacing and typographical error the proposed algorithm corrects both error simultaneously using eojeol transition pattern dictionary and statistical data such a eumjeol n gram and jaso transition probability the algorithm minimizes the usage of huge word dictionary 
this demo present lexflow a work flow management system for cross fertilization of computational lexicon borrowing from technique used in the domain of document workflow we model the activity of lexicon management a a set of workflow type where lexical entry move across agent in the process of being dynamically updated a prototype of lexflow ha been implemented with extensive use of xml technology xslt xpath xforms svg and open source tool cocoon tomcat mysql lexflow is a web based application that enables the cooperative and distributed management of computational lexicon 
this paper present a method for adapting a language generator to the strength and weakness of a synthetic voice thereby improving the naturalness of synthetic speech in a spoken language dialogue system the method train a discriminative reranker to select paraphrase that are predicted to sound natural when synthesized the ranker is trained on realizer and synthesizer feature in supervised fashion using human judgement of synthetic voice quality on a sample of the paraphrase representative of the generator s capability result from a cross validation study indicate that discriminative paraphrase reranking can achieve substantial improvement in naturalness on average ameliorating the problem of highly variable synthesis quality typically encountered with today s unit selection synthesizer 
in this paper we propose forest to string rule to enhance the expressive power of tree to string translation model a forestto string rule is capable of capturing nonsyntactic phrase pair by describing the correspondence between multiple parse tree and one string to integrate these rule into tree to string translation model auxiliary rule are introduced to provide a generalization level experimental result show that on the nist chinese english test set the tree to string model augmented with forest to string rule achieves a relative improvement of in term of bleu score over the original model which allows treeto string rule only 
spoken language generation for dialogue system requires a dictionary of mapping between semantic representation of concept the system want to express and realization of those concept dictionary creation is a costly process it is currently done by hand for each dialogue domain we propose a novel unsupervised method for learning such mapping from user review in the target domain and test it on restaurant review we test the hypothesis that user review that provide individual rating for distinguished attribute of the domain entity make it possible to map review sentence to their semantic representation with high precision experimental analysis show that the mapping learned cover most of the domain ontology and provide good linguistic variation a subjective user evaluation show that the consistency between the semantic representation and the learned realization is high and that the naturalness of the realization is higher than a hand crafted baseline 
this paper describes recent progress on the trip architecture for developing spoken language dialogue system the interactive poster session will include demonstration of two system built using trip a computer purchasing assistant and an object placement and manipulation task 
this paper study the enrichment of spanish wordnet with synset gloss automatically obtained from the english word net gloss using a phrase based statistical machine translation system we construct the english spanish translation system from a parallel corpus of proceeding of the european parliament and study how to adapt statistical model to the domain of dictionary definition we build specialized language and translation model from a small set of parallel definition and experiment with robust manner to combine them a statistically significant increase in performance is obtained the best system is finally used to generate a definition for all spanish synset which are currently ready for a manual revision a a complementary issue we analyze the impact of the amount of in domain data needed to improve a system trained entirely on out of domain data 
we describe a generic framework for integrating various stochastic model of discourse coherence in a manner that take advantage of their individual strength an integral part of this framework are algorithm for searching and training these stochastic coherence model we evaluate the performance of our model and algorithm and show empirically that utility trained log linear coherence model outperform each of the individual coherence model considered 
we propose in this paper a method for quantifying sentence grammaticality the approach based on property grammar a constraint based syntactic formalism make it possible to evaluate a grammaticality index for any kind of sentence including ill formed one we compare on a sample of sentence the grammaticality index obtained from pg formalism and the acceptability judgement measured by mean of a psycholinguistic analysis the result show that the derived grammaticality index is a fairly good tracer of acceptability score 
a grammatical method of combining two kind of speech repair cue is presented one cue prosodic disjuncture is detected by a decision tree based ensemble classifier that us acoustic cue to identify where normal prosody seems to be interrupted lickley the other cue syntactic parallelism codifies the expectation that repair continue a syntactic category that wa left unfinished in the reparandum levelt the two cue are combined in a treebank pcfg whose state are split using a few simple tree transformation parsing performance on the switchboard and fisher corpus suggests that these two cue help to locate speech repair in a synergistic way 
in this paper we report our work on building a po tagger for a morphologically rich languagehindi the theme of the research is to vindicate the stand thatif morphology is strong and harnessable then lack of training corpus is not debilitating we establish a methodology of po tagging which the resource disadvantaged lacking annotated corpus language can make use of the methodology make use of locally annotated modestly sized corpus word exhaustive morpohological analysis backed by high coverage lexicon and a decision tree based learning algorithm cn the evaluation of the system wa done with fold cross validation of the corpus in the news domain www bbc co uk hindi the current accuracy of po tagging is and can be further improved 
we propose a novel algorithm for english to persian transliteration previous method proposed for this language pair apply a word alignment tool for training by contrast we introduce an alignment algorithm particularly designed for transliteration our new model improves the english to persian transliteration accuracy by over an n gram baseline we also propose a novel back transliteration method for this language pair a previously unstudied problem experimental result demonstrate that our algorithm lead to an absolute improvement of over standard transliteration approach 
this paper present a detailed study of the integration of knowledge from both dependency par and hierarchical word ontology into a maximum entropy based tagging model that simultaneously label word with both syntax and semantics our finding show that information from both these source can lead to strong improvement in overall system accuracy dependency knowledge improved performance over all class of word and knowledge of the position of a word in an on tological hierarchy increased accuracy for word not seen in the training data the resulting tagger offer the highest reported tagging accuracy on this tagset to date 
hidden markov model hmms are powerful statistical model that have found successful application in information extraction ie in current approach to applying hmms to ie an hmm is used to model text at the document level this modelling might cause undesired redundancy in extraction in the sense that more than one filler is identified and extracted we propose to use hmms to model text at the segment level in which the extraction process consists of two step a segment retrieval step followed by an extraction step in order to retrieve extraction relevant segment from document we introduce a method to use hmms to model and retrieve segment our experimental result show that the resulting segment hmm ie system not only achieves near zero extraction redundancy but also ha better overall extraction performance than traditional document hmm ie system 
the main aim of the mima mining information for management and acquisition search system is to achieve structuring knowledge to accelerate knowledge exploitation in the domain of science and technology this system integrates natural language processing including ontology development information retrieval visualization and database technology the structuring knowledge that we define indicates knowledge storage hierarchical classification of knowledge analysis of knowledge visualization of knowledge we aim at integrating different type of database paper and patent technology and innovation and knowledge domain and simultaneously retrieving different type of knowledge application for the several target such a syllabus structuring will also be mentioned 
we report on an investigation of the pragmatic category of topic in danish dialog and it correlation to surface feature of np using a corpus of utterance we trained a decision tree system on feature the system achieved near human performance with success rate of and f score of in fold cross validation test human performance and the most important feature turned out to be preverbal position definiteness pronominalisation and non subordination we discovered that np in epistemic matrix clause e g i think were seldom topic and we suspect that this hold for other interpersonal matrix clause a well 
in this paper we present a weakly supervised learning approach for spoken language understanding in domain specific dialogue system we model the task of spoken language understanding a a successive classification problem the first classifier topic classifier is used to identify the topic of an input utterance with the restriction of the recognized target topic the second classifier semantic classifier is trained to extract the corresponding slot value pair it is mainly data driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language most importantly it allows the employment of weakly supervised strategy for training the two classifier we first apply the training strategy of combining active learning and self training tur et al for topic classifier also we propose a practical method for bootstrapping the topic dependent semantic classifier from a small amount of labeled sentence experiment have been conducted in the context of chinese public transportation information inquiry domain the experimental result demonstrate the effectiveness of our proposed slu framework and show the possibility to reduce human labeling effort significantly 
multiple sequence alignment technique have recently gained popularity in the natural language community especially for task such a machine translation text generation and paraphrase identification prior work fall into two category depending on the type of input used a parallel corpus e g multiple translation of the same text or b comparable text non parallel but on the same topic so far only technique based on parallel text have successfully used syntactic information to guide alignment in this paper we describe an algorithm for incorporating syntactic feature in the alignment process for non parallel text with the goal of generating novel paraphrase of existing text our method us dynamic programming with alignment decision based on the local syntactic similarity between two sentence our result show that syntactic alignment outrivals syntax free method by in both grammaticality and fidelity when computed over the novel sentence generated by alignment induced finite state automaton 
this paper explores the relationship between the translation quality and the retrieval effectiveness in machine translation mt based cross language information retrieval clir to obtain mt system of different translation quality we degrade a rule based mt system by decreasing the size of the rule base and the size of the dictionary we use the degraded mt system to translate query and submit the translated query of varying quality to the ir system retrieval effectiveness is found to correlate highly with the translation quality of the query we further analyze the factor that affect the retrieval effectiveness title query are found to be preferred in mt based clir in addition dictionary based degradation is shown to have stronger impact than rule based degradation in mt based clir 
the sammie system is an in car multi modal dialogue system for an mp application it is used a a testing environment for our research in natural intuitive mixed initiative interaction with particular emphasis on multimodal output planning and realization aimed to produce output adapted to the context including the driver s attention state w r t the primary driving task 
this paper present a method of automatically constructing information extraction pattern on predicate argument structure pas obtained by full parsing from a smaller training corpus because pas represent generalized structure for syntactical variant pattern on pas are expected to be more generalized than those on surface word in addition pattern are divided into component to improve recall and we introduce a support vector machine to learn a prediction model using pattern matching result in this paper we present experimental result and analyze them on how well protein protein interaction were extracted from medline abstract the result demonstrated that our method improved accuracy compared to a machine learning approach using surface word part of speech pattern 
call center handle customer query from various domain such a computer sale and support mobile phone car rental etc each such domain generally ha a domain model which is essential to handle customer complaint these model contain common problem category typical customer issue and their solution greeting style currently these model are manually created over time towards this we propose an unsupervised technique to generate domain model automatically from call transcription we use a state of the art automatic speech recognition system to transcribe the call between agent and customer which still result in high word error rate and show that even from these noisy transcription of call we can automatically build a domain model the domain model is comprised of primarily a topic taxonomy where every node is characterized by topic s typical question answer q a typical action and call statistic we show how such a domain model can be used for topic identification of unseen call we also propose application for aiding agent while handling call and for agent monitoring based on the domain model 
annotated corpus are valuable resource for developing natural language processing application this work focus on acquiring annotated data for multilingual processing application we present an annotation environment that support a web based user interface for acquiring word alignment between english and chinese a well a a visualization tool for researcher to explore the annotated data 
we describe the new release of the rasp robust accurate statistical parsing system designed for syntactic annotation of free text the new version includes a revised and more semantically motivated output representation an enhanced grammar and part of speech tagger lexicon and a more flexible and semi supervised training method for the structural parse ranking model we evaluate the released version on the wsj using a relational evaluation scheme and describe how the new release allows user to enhance performance using in domain lexical information 
state of the art computer assisted translation engine are based on a statistical prediction engine which interactively provides completion to what a human translator type the integration of human speech into a computer assisted system is also a challenging area and is the aim of this paper so far only a few method for integrating statistical machine translation mt model with automatic speech recognition asr model have been studied they were mainly based on n best rescoring approach n best rescoring is not an appropriate search method for building a real time prediction engine in this paper we study the incorporation of mt model and asr model using finite state automaton we also propose some transducer based on mt model for rescoring the asr word graph 
this paper present technique to apply semi crfs to named entity recognition task with a tractable computational cost our framework can handle an ner task that ha long named entity and many label which increase the computational cost to reduce the computational cost we propose two technique the first is the use of feature forest which enables u to pack feature equivalent state and the second is the introduction of a filtering process which significantly reduces the number of candidate state this framework allows u to use a rich set of feature extracted from the chunk based representation that can capture informative characteristic of entity we also introduce a simple trick to transfer information about distant entity by embedding label information into non entity label experimental result show that our model achieves an f score of on the jnlpba shared task without using any external resource or post processing technique 
this paper introduces a new application of boosting for parse reranking several parser have been proposed that utilize the all subtrees representation e g tree kernel and data oriented parsing this paper argues that such an all subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees we show how the boosting algorithm can be applied to the all subtrees representation and how it selects a small and relevant feature set efficiently two experiment on parse reranking show that our method achieves comparable or even better performance than kernel method and also improves the testing efficiency 
in this paper we argue that n gram language model are not sufficient to address word reordering required for machine translation we propose a new distortion model that can be used with existing phrase based smt decoder to address those n gram language model limitation we present empirical result in arabic to english machine translation that show statistically significant improvement when our proposed model is used we also propose a novel metric to measure word order similarity or difference between any pair of language based on word alignment 
it ha previously been assumed in the psycholinguistic literature that finite state model of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model we show that a simple computational model a bigram part of speech tagger based on the design used by corley and crocker make correct prediction on processing difficulty observed in a wide range of empirical sentence processing data we use two mode of evaluation one that relies on comparison with a control sentence paralleling practice in human study another that measure probability drop in the disambiguating region of the sentence both are surprisingly good indicator of the processing difficulty of garden path sentence the sentence tested are drawn from published source and systematically explore five different type of ambiguity previous study have been narrower in scope and smaller in scale we do not deny the limitation of finite state model but argue that our result show that their usefulness ha been underestimated 
machine translation of a source language sentence involves selecting appropriate target language word and ordering the selected word to form a well formed target language sentence most of the previous work on statistical machine translation relies on local association of target word phrase with source word phrase for lexical selection in contrast in this paper we present a novel approach to lexical selection where the target word are associated with the entire source sentence global without the need to compute local association further we present a technique for reconstructing the target language sentence from the selected word we compare the result of this approach against those obtained from a finite state based statistical machine translation system which relies on local lexical association 
how far can we get with unsupervised parsing if we make our training corpus several order of magnitude larger than ha hitherto be attempted we present a new algorithm for unsupervised parsing using an all subtrees model termed u dop which par directly with packed forest of all binary tree we train both on penn s wsj data and on the much larger nanc corpus showing that u dop outperforms a treebank pcfg on the standard wsj test set while u dop performs worse than state of the art supervised parser on handannotated sentence we show that the model outperforms supervised parser when evaluated a a language model in syntax based machine translation on europarl we argue that supervised parser miss the fluidity between constituent and non constituent and that in the field of syntax based language modeling the end of supervised parsing ha come in sight 
recently proposed deterministic classifier based parser nivre and scholz sagae and lavie yamada and mat sumoto offer attractive alternative to generative statistical parser deterministic parser are fast efficient and simple to implement but generally le accurate than optimal or nearly optimal statistical parser we present a statistical shift reduce parser that bridge the gap between deterministic and probabilistic parser the parsing model is essentially the same a one previously used for deterministic parsing but the parser performs a best first search instead of a greedy search using the standard section of the wsj corpus of the penn treebank for training and testing our parser ha precision and recall using automatically assigned part of speech tag perhaps more interestingly the parsing model is significantly different from the generative model used by other well known accurate parser allowing for a simple combination that produce precision and recall of and respectively 
we investigate the impact of parse quality on a syntactically informed statistical machine translation system applied to technical text we vary parse quality by varying the amount of data used to train the parser a the amount of data increase parse quality improves leading to improvement in machine translation output and result that significantly outperform a state of the art phrasal baseline 
we propose a novel bilingual topical admixture bitam formalism for word alignment in statistical machine translation under this formalism the parallel sentence pair within a document pair are assumed to constitute a mixture of hidden topic each word pair follows a topic specific bilingual translation model three bitam model are proposed to capture topic sharing at different level of linguistic granularity i e at the sentence or word level these model enable word alignment process to leverage topical content of document pair efficient variational approximation algorithm are designed for inference and parameter estimation with the inferred latent topic bitam model facilitate coherent pairing of bilingual linguistic entity that share common topical aspect our preliminary experiment show that the proposed model improve word alignment accuracy and lead to better translation quality 
this paper deal with multilingual database generation from parallel corpus the idea is to contribute to the enrichment of lexical database for language with few linguistic resource our approach is endogenous it relies on the raw text only it doe not require external linguistic resource such a stemmer or tagger the system produce alignment for the european language of the acquis communautaire corpus 
in this paper we present a hybrid method for word segmentation and po tagging the target language are those in which word boundary are ambiguous such a chinese and japanese in the method word based and character based processing is combined and word segmentation and po tagging are conducted simultaneously experimental result on multiple corpus show that the integrated method ha high accuracy 
we present a novel hybrid approach for word sense disambiguation wsd which make use of a relational formalism to represent instance and background knowledge it is built using inductive logic programming technique to combine evidence coming from both source during the learning process producing a rule based wsd model we experimented with this approach to disambiguate highly ambiguous verb in english portuguese translation result showed that the approach is promising achieving an average accuracy of which outperforms the other machine learning technique investigated 
this paper proposes a novel composite kernel for relation extraction the composite kernel consists of two individual kernel an entity kernel that allows for entity related feature and a convolution parse tree kernel that model syntactic information of relation example the motivation of our method is to fully utilize the nice property of kernel method to explore diverse knowledge for relation extraction our study illustrates that the composite kernel can effectively capture both flat and structured feature without the need for extensive feature engineering and can also easily scale to include more feature evaluation on the ace corpus show that our method outperforms the previous best reported method and significantly out performs previous two dependency tree kernel for relation extraction 
web extraction system attempt to use the immense amount of unlabeled text in the web in order to create large list of entity and relation unlike traditional ie method the web extraction system do not label every mention of the target entity or relation instead focusing on extracting a many different instance a possible while keeping the precision of the resulting list reasonably high ures is a web relation extraction system that learns powerful extraction pattern from unlabeled text using short description of the target relation and their attribute the performance of ures is further enhanced by classifying it output instance using the property of the extracted pattern the feature we use for classification and the trained classification model are independent from the target relation which we demonstrate in a series of experiment in this paper we show how the introduction of a simple rule based ner can boost the performance of ures on a variety of relation we also compare the performance of ures to the performance of the state of the art knowitall system and to the performance of it pattern learning component which us a simpler and le powerful pattern language than ures 
named entity recognition system extract entity such a people organization and location from unstructured text rather than extract these mention in isolation this paper present a record extraction system that assembles mention into record i e database tuples we construct a probabilistic model of the compatibility between field value then employ graph partitioning algorithm to cluster field into cohesive record we also investigate compatibility function over set of field rather than simply pair of field to examine how higher representational power can impact performance we apply our technique to the task of extracting contact record from faculty and student homepage demonstrating a error reduction over baseline approach 
in sayeed and szpakowicz we proposed a parser inspired by some aspect of the minimalist program this incremental parser wa designed specifically to handle discontinuous constituency phenomenon for np in latin we take a look at the application of this parser to a specific kind of apparent island violation in latin involving the extraction of constituent including subject from tensed embedded clause we make use of idea about the left periphery from rizzi to modify our parser in order to handle apparently violated subject island and similar phenomenon 
we report an empirical study on the role of syntactic feature in building a semi supervised named entity ne tagger our study address two question what type of syntactic feature are suitable for extracting potential ne to train a classifier in a semi supervised setting how good is the resulting ne classifier on testing instance dissimilar from it training data our study show that constituency and dependency parsing constraint are both suitable feature to extract ne and train the classifier moreover the classifier showed significant accuracy improvement when constituency feature are combined with new dependency feature furthermore the degradation in accuracy on unfamiliar test case is low suggesting that the trained classifier generalizes well 
in this paper we present espresso a weakly supervised general purpose and accurate algorithm for harvesting semantic relation the main contribution are i a method for exploiting generic pattern by filtering incorrect instance using the web and ii a principled measure of pattern and instance reliability enabling the filtering algorithm we present an empirical comparison of espresso with various state of the art system on different size and genre corpus on extracting various general and specific relation experimental result show that our exploitation of generic pattern substantially increase system recall with small effect on overall precision 
large corpus of parsed sentence with semantic role label e g propbank provide training data for use in the creation of high performance automatic semantic role labeling system despite the size of these corpus individual verb or rolesets often have only a handful of instance in these corpus and only a fraction of english verb have even a single annotation in this paper we describe an approach for dealing with this sparse data problem enabling accurate semantic role labeling for novel verb rolesets with only a single training example our approach involves the identification of syntactically similar verb found in propbank the alignment of argument in their corresponding rolesets and the use of their corresponding annotation in propbank a surrogate training data 
in this paper we define a novel similarity measure between example of textual entailment and we use it a a kernel function in support vector machine svms this allows u to automatically learn the rewrite rule that describe a non trivial set of entailment case the experiment with the data set of the rte challenge show an improvement of over the state of the art method 
we propose a new simple model for the automatic induction of selectional preference using corpus based semantic similarity metric focusing on the task of semantic role labeling we compute selectional preference for semantic role in evaluation the similarity based model show lower error rate than both resnik s wordnet based model and the em based clustering model but ha coverage problem 
clarissa an experimental voice enabled procedure browser that ha recently been deployed on the international space station i is to the best of our knowledge the first spoken dialog system in space this paper give background on the system and the i procedure then discus the research developed to address three key problem grammar based speech recognition using the regulus toolkit svm based method for open microphone speech recognition and robust side effect free dialogue management for handling undos correction and confirmation 
in machine learning whether one can build a more accurate classifier by using unlabeled data semi supervised learning is an important issue although a number of semi supervised method have been proposed their effectiveness on nlp task is not always clear this paper present a novel semi supervised method that employ a learning paradigm which we call structural learning the idea is to find what good classifier are like by learning from thousand of automatically generated auxiliary classification problem on unlabeled data by doing so the common predictive structure shared by the multiple classification problem can be discovered which can then be used to improve performance on the target problem the method produce performance higher than the previous best result on conll syntactic chunking and conll named entity chunking english and german 
in machine learning whether one can build a more accurate classifier by using unlabeled data semi supervised learning is an important issue although a number of semi supervised method have been proposed their effectiveness on nlp task is not always clear this paper present a novel semi supervised method that employ a learning paradigm which we call structural learning the idea is to find what good classifier are like by learning from thousand of automatically generated auxiliary classification problem on unlabeled data by doing so the common predictive structure shared by the multiple classification problem can be discovered which can then be used to improve performance on the target problem the method produce performance higher than the previous best result on conll syntactic chunking and conll named entity chunking english and german 
in this paper we present a supervised word sense disambiguation methodology that exploit kernel method to model sense distinction in particular a combination of kernel function is adopted to estimate independently both syntagmatic and domain similarity we defined a kernel function namely the domain kernel that allowed u to plug external knowledge into the supervised learning process external knowledge is acquired from unlabeled data in a totally unsupervised way and it is represented by mean of domain model we evaluated our methodology on several lexical sample task in different language outperforming significantly the state of the art for each of them while reducing the amount of labeled training data required for learning 
to make it practical to mechanize proof in programming language metatheory several capability are required of the theorem proving framework one must be able to represent and efficiently reason about complex recursively defined expression define arbitrary induction scheme including mutual induction over several object and induction over derivation and reason about variable binding with minimal overhead we introduce a method for performing these proof in acl including a macro which automates the process of defining function and theorem to facilitate reasoning about recursive data type to illustrate this method we present a proof in acl of the soundness of the simply typed calculus 
this paper present a comparative study of five parameter estimation algorithm on four nlp task three of the five algorithm are well known in the computational linguistics community maximum entropy me estimation with l regularization the averaged perceptron ap and boosting we also investigate me estimation with l regularization using a novel optimization algorithm and blasso which is a version of boosting with lasso l regularization we first investigate all of our estimator on two re ranking task a parse selection task and a language model lm adaptation task then we apply the best of these estimator to two additional task involving conditional sequence model a conditional markov model cmm for part of speech tagging and a conditional random field crf for chinese word segmentation our experiment show that across task three of the estimator me estimation with l or l regularization and ap are in a near statistical tie for first place 
the ability to detect similarity in conjunct head is potentially a useful tool in helping to disambiguate coordination structure a difficult task for parser we propose a distributional measure of similarity designed for such a task we then compare several different measure of word similarity by testing whether they can empirically detect similarity in the head noun of noun phrase conjuncts in the wall street journal wsj treebank we demonstrate that several measure of word similarity can successfully detect conjunct head similarity and suggest that the measure proposed in this paper is the most appropriate for this task 
we present a general framework to incorporate prior knowledge such a heuristic or linguistic feature in statistical generative word alignment model prior knowledge play a role of probabilistic soft constraint between bilingual word pair that shall be used to guide word alignment model training we investigate knowledge that can be derived automatically from entropy principle and bilingual latent semantic analysis and show how they can be applied to improve translation performance 
in this paper we put forward an information theoretic definition of the redundancythat is observed across the sound inventory of the world s language through rigorous statistical analysis we find that this redundancy is an invariant property of the consonant inventory the statistical analysis further unfolds that the vowel inventory do not exhibit any such property which in turn point to the fact that the organizing principle of the vowel and the consonant inventory are quite different in nature 
we propose a novel method to expand a small existing translation dictionary to a large translation dictionary using a pivot language our method depends on the assumption that it is possible to find a pivot language for a given language pair on condition that there are both a large translation dictionary from the source language to the pivot language and a large translation dictionary from the pivot language to the destination language experiment that expands the indonesian japanese dictionary using the english language a a pivot language show that the proposed method can improve performance of a real clir system 
this paper describes senselearner a minimally supervised word sense disambiguation system that attempt to disambiguate all content word in a text using wordnet sens we evaluate the accuracy of senselearner on several standard sense annotated data set and show that it compare favorably with the best result reported during the recent senseval evaluation 
we present an elegant and extensible model that is capable of providing semantic interpretation for an unusually wide range of textual table in document unlike the few existing table analysis model which largely rely on relatively ad hoc heuristic our linguistically oriented approach is systematic and grammar based which allows our model to be concise and yet recognize a wider range of data model than others and disambiguate to a significantly finer extent the underlying semantic interpretation of the table in term of data model drawn from relation database theory to accomplish this the model introduces viterbi parsing under two dimensional stochastic cfgs the cleaner grammatical approach facilitates not only greater coverage but also grammar extension and maintenance a well a a more direct and declarative link to semantic interpretation for which we also introduce a new cleaner data model in disambiguation experiment on recognizing relevant data model of unseen web table from different domain a blind evaluation of the model showed precision and recall 
the noisy channel model approach is successfully applied to various natural language processing task currently the main research focus of this approach is adaptation method how to capture characteristic of word and expression in a target domain given example sentence in that domain a a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information especially the new method is suitable for language in which word are not delimited by whitespace we applied our method to a phoneme to text transcription task in japanese and reduced about of the error in the result of an existing method 
in this paper we discus the current method in the representation of corpus annotated at multiple level of linguistic organization so called multi level or multi layer corpus taking five approach which are representative of the current practice in this area we discus the commonality and difference between them focusing on the underlying data model the goal of the paper is to identify the common concern in multi layer corpus representation and processing so a to lay a foundation for a unifying modular data model 
this paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related class for each class in the hierarchy either manually predefined or automatically clustered a linear discriminative function is determined in a top down way using a perceptron algorithm with the lower level weight vector derived from the upper level weight vector a the upper level class normally ha much more positive training example than the lower level class the corresponding linear discriminative function can be determined more reliably the upper level discriminative function then can effectively guide the discriminative function learning in the lower level which otherwise might suffer from limited training data evaluation on the ace rdc corpus show that the hierarchical strategy much improves the performance by and in f measure on leastand mediumfrequent relation respectively it also show that our system outperforms the previous best reported system by in f measure on the subtypes using the same feature set 
we describe our entry into the multilingual summarization evaluation mse competition for evaluating generic multidocument summarization system where document are drawn both from english data and english translation of arabic data our system is based on a bayesian query focused summarization model adapted to the generic multidocument setting and tuned against the rouge evaluation metric in the human pyramid based evaluation our system scored an average of approximately better than the next best system which scored in the automatic evaluation our system scored behind four other site with the skip bigram evaluation and behind two other site with the standard bigram evaluation 
in this paper we describe and evaluate several statistical model for the task of realization ranking i e the problem of discriminating between competing surface realization generated for a given input semantics three model and several variant are trained and tested an n gram language model a discriminative maximum entropy model using structural information and incorporating the language model a a separate feature and finally an svm ranker trained on the same feature set the resulting hybrid tactical generator is part of a larger semantic transfer mt system 
in this paper we present an unlexicalized parser for german which employ smoothing and suffix analysis to achieve a labelled bracket f score of higher than previously reported result on the negra corpus in addition to the high accuracy of the model the use of smoothing in an unlexicalized parser allows u to better examine the interplay between smoothing and parsing result 
word of foreign origin are referred to a borrowed word or loanword a loanword is usually imported to chinese by phonetic transliteration if a translation is not easily available semantic transliteration is seen a a good tradition in introducing foreign word to chinese not only doe it preserve how a word sound in the source language it also carry forward the word s original semantic attribute this paper attempt to automate the semantic transliteration process for the first time we conduct an inquiry into the feasibility of semantic transliteration and propose a probabilistic model for transliterating personal name in latin script into chinese the result show that semantic transliteration substantially and consistently improves accuracy over phonetic transliteration in all the experiment 
in this paper we approach word sense disambiguation and information extraction a a unified tagging problem the task consists of annotating text with the tagset defined by the wordnet supersense class for noun and verb since the tagset is directly related to wordnet synset the tagger return partial word sense disambiguation furthermore since the noun tag include the standard named entity detection class person location organization time etc the tagger a a by product return extended named entity information we cast the problem of supersense tagging a a sequential labeling task and investigate it empirically with a discriminatively trained hidden markov model experimental evaluation on the main sense annotated datasets available i e semcor and senseval show considerable improvement over the best known first sense baseline 
ranking document or sentence according to both topic and sentiment relevance should serve a critical function in helping user when topic and sentiment polarity of the targeted text are not explicitly given a is often the case on the web in this paper we propose several sentiment information retrieval model in the framework of probabilistic language model assuming that a user both input query term expressing a certain topic and also specifies a sentiment polarity of interest in some manner we combine sentiment relevance model and topic relevance model with model parameter estimated from training data considering the topic dependence of the sentiment our experiment prove that our model are effective 
we present a novel translation model based on tree to string alignment template tat which describes the alignment between a source parse tree and a target string a tat is capable of generating both terminal and non terminal and performing reordering at both low and high level the model is linguistically syntax based because tat are extracted automatically from word aligned source side parsed parallel text to translate a source sentence we first employ a parser to produce a source parse tree and then apply tat to transform the tree into a target string our experiment show that the tat based model significantly outperforms pharaoh a state of the art decoder for phrase based model 
this paper present the application of wordnet based semantic relatedness measure to automatic speech recognition asr in multi party meeting different word utterance context relatedness measure and utterance coherence measure are defined and applied to the rescoring of n best list no significant improvement in term of word error rate wer are achieved compared to a large word based n gram baseline model we discus our result and the relation to other work that achieved an improvement with such model for simpler task 
text adaptation is a teacher practice used to help with reading comprehension and english language skill development for english language learner ell carlo august mclaughlin snow dressler lippman lively white echevarria vogt and short yano long and ross the practice of text adaptation involves a teacher s modification of text to make them more understandable given a student s reading level teacher adaptation include text summary vocabulary support e g providing synonym and translation it is a time consuming but critical practice for k teacher who teach ell since reading level appropriate text are often hard to find to this end we have implemented the automated text adaptation tool v ata v an innovative educational tool that automatically generates text adaptation similar to those teacher might create we have also completed a teacher pilot study schwarm and ostendorf and heilman collins thompson callan and eskenazi describe related research addressing the development of nlp based reading support tool 
we study a number of natural language decipherment problem using unsupervised learning these include letter substitution cipher character code conversion phonetic decipherment and word based cipher with relevance to machine translation straightforward unsupervised learning technique most often fail on the first try so we describe technique for understanding error and significantly increasing performance 
we examine the problem of choosing word order for a set of dependency tree so a to minimize total dependency length we present an algorithm for computing the optimal layout of a single tree a well a a numerical method for optimizing a grammar of ordering over a set of dependency type a grammar generated by minimizing dependency length in unordered tree from the penn treebank is found to agree surprisingly well with english word order suggesting that dependency length minimization ha influenced the evolution of english 
discriminative probabilistic model are very popular in nlp because of the latitude they afford in designing feature but training involves complex trade offs among weight which can be dangerous a few highlyindicative feature can swamp the contribution of many individually weaker feature causing their weight to be undertrained such a model is le robust for the highly indicative feature may be noisy or missing in the test data to ameliorate this weight undertraining we introduce several new feature bagging method in which separate model are trained on subset of the original feature and combined using a mixture model or a product of expert these method include the logarithmic opinion pool used by smith et al we evaluate feature bagging on linear chain conditional random field for two natural language task on both task the feature bagged crf performs better than simply training a single crf on all the feature 
markov order conditional random field crfs and semi markov crfs are two popular model for sequence segmentation and labeling both model have advantage in term of the type of feature they most naturally represent we propose a hybrid model that is capable of representing both type of feature and describe efficient algorithm for it training and inference we demonstrate that our hybrid model achieves error reduction of and over a standard order crf and a semi markov crf resp on the task of chinese word segmentation we also propose the use of a powerful feature for the semi markov crf the log conditional odds that a given token sequence constitutes a chunk according to a generative model which reduces error by an additional our best system achieves f measure the highest reported score on this test set 
most of the work on treebank based statistical parsing exclusively us the wall street journal part of the penn treebank for evaluation purpose due to the presence of this quasi standard the question of to which degree parsing result depend on the property of treebanks wa often ignored in this paper we use two similar german treebanks t ba d z and negra and investigate the role that different annotation decision play for parsing for these purpose we approximate the two treebanks by gradually taking out or inserting the corresponding annotation component and test the performance of a standard pcfg parser on all treebank version our result give an indication of which structure are favorable for parsing and which one are not 
sentence boundary detection in speech is important for enriching speech recognition output making it easier for human to read and downstream module to process in previous work we have developed hidden markov model hmm and maximum entropy maxent classifier that integrate textual and prosodic knowledge source for detecting sentence boundary in this paper we evaluate the use of a conditional random field crf for this task and relate result with this model to our prior work we evaluate across two corpus conversational telephone speech and broadcast news speech on both human transcription and speech recognition output in general our crf model yield a lower error rate than the hmm and maxent model on the nist sentence boundary detection task in speech although it is interesting to note that the best result are achieved by three way voting among the classifier this probably occurs because each model ha different strength and weakness for modeling the knowledge source 
a query speller is crucial to search engine in improving web search relevance this paper describes novel method for use of distributional similarity estimated from query log in learning improved query spelling correction model the key to our method is the property of distributional similarity between two term it is high between a frequently occurring misspelling and it correction and low between two irrelevant term only with similar spelling we present two model that are able to take advantage of this property experimental result demonstrate that the distributional similarity based model can significantly outperform their baseline system in the web query spelling correction task 
recently there ha been a rise of interest in unsupervised detection of highlevel semantic relation involving complex unit such a phrase and whole sentence typically such approach are faced with two main obstacle data sparseness and correctly generalizing from the example in this work we describe the clustered clause representation which utilizes information based clustering and inter sentence dependency to create a simplified and generalized representation of the grammatical clause we implement an algorithm which us this representation to detect a predefined set of high level relation and demonstrate our model s eectiveness in overcoming both the problem mentioned 
a key question facing the parsing community is how to compare parser which use different grammar formalism and produce different output evaluating a parser on the same resource used to create it can lead to non comparable accuracy score and an over optimistic view of parser performance in this paper we evaluate a ccg parser on depbank and demonstrate the difficulty in converting the parser output into depbank grammatical relation in addition we present a method for measuring the effectiveness of the conversion which provides an upper bound on parsing accuracy the ccg parser obtains an f score of on labelled dependency against an upper bound of we compare the ccg parser against the rasp parser outperforming rasp by over overall and on the majority of dependency type 
this paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrase database a trivial dialogue phrase is defined a an expression used by a chatbot program a the answer of a user input a transfer like genetic algorithm ga method is used to generating the trivial dialogue phrase for the creation of a natural language generation nlg knowledge base the automatic evaluation of a generated phrase is performed by producing n gram and retrieving their frequency from the world wide web www preliminary experiment show very positive result 
this paper proposes a framework for training conditional random field crfs to optimize multivariate evaluation measure including non linear measure such a f score our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure specifically focusing on sequential segmentation task i e text chunking and named entity recognition we introduce a loss function that closely reflects the target evaluation measure for these task namely segmentation f score our experiment show that our method performs better than standard crf training 
cross linguistic similarity are reflected by the speech sound system of language all over the world in this work we try to model such similarity observed in the consonant inventory through a complex bipartite network we present a systematic study of some of the appealing feature of these inventory with the help of the bipartite network an important observation is that the occurrence of consonant follows a two regime power law distribution we find that the consonant inventory size distribution together with the principle of preferential attachment are the main reason behind the emergence of such a two regime behavior in order to further support our explanation we present a synthesis model for this network based on the general theory of preferential attachment 
the paper considers how to scale up dialogue protocol to multilogue setting with multiple conversationalist we extract two benchmark to evaluate scaled up protocol based on the long distance resolution possibility of non sentential utterance in dialogue and multilogue in the british national corpus in light of these benchmark we then consider three possible transformation to dialogue protocol formulated within an issue based approach to dialogue management we show that one such transformation yield protocol for querying and assertion that fulfill these benchmark 
we present outilex a generalist linguistic platform for text processing the platform includes several module implementing the main operation for text processing and is designed to use large coverage language resource these resource dictionary grammar annotated text are formatted into xml in accordance with current standard evaluation on efficiency are given 
this paper present the result of the development of a high throughput real time modularized text analysis and information retrieval system that identifies clinically relevant entity in clinical note map the entity to several standardized nomenclature and make them available for subsequent information retrieval and data mining the performance of the system wa validated on a small collection of document partitioned into query topic and manually examined by physician and nurse abstractor for relevance to the query topic we find that simple key phrase searching result in recall and precision a combination of nlp approach to indexing improve the recall to while lowering the precision to 
in this paper we propose an implementable characterization of genre suitable for automatic genre identification of web page this characterization is implemented a an inferential model based on a modified version of bayes theorem such a model can deal with genre hybridism and individualization two important force behind genre evolution result show that this approach is effective and is worth further research 
morphological disambiguation is the process of assigning one set of morphological feature to each individual word in a text when the word is ambiguous there are several possible analysis for the word a disambiguation procedure based on the word context must be applied this paper deal with morphological disambiguation of the hebrew language which combine morpheme into a word in both agglutinative and fusional way we present an un supervised stochastic model the only resource we use is a morphological analyzer which deal with the data sparseness problem caused by the affixational morphology of the hebrew language we present a text encoding method for language with affixational morphology in which the knowledge of word formation rule which are quite restricted in hebrew help in the disambiguation we adapt hmm algorithm for learning and searching this text representation in such a way that segmentation and tagging can be learned in parallel in one step result on a large scale evaluation indicate that this learning improves disambiguation for complex tag set our method is applicable to other language with affix morphology 
an automatic word spacing is one of the important task in korean language processing and information retrieval since there are a number of confusing case in word spacing of korean there are some mistake in many text including news article this paper present a high accurate method for automatic word spacing based on self organizing gram model this method is basically a variant of gram model but achieves high accuracy by automatically adapting context size in order to find the optimal context size the proposed method automatically increase the context size when the contextual distribution after increasing it dose not agree with that of the current context it also decrease the context size when the distribution of reduced context is similar to that of the current context this approach achieves high accuracy by considering higher dimensional data in case of necessity and the increased computational cost are compensated by the reduced context size the experimental result show that the self organizing structure of gram model enhances the basic model 
sentence compression is a task of creating a short grammatical sentence by removing extraneous word or phrase from an original sentence while preserving it meaning existing method learn statistic on trimming context free grammar cfg rule however these method sometimes eliminate the original meaning by incorrectly removing important part of sentence because trimming probability only depend on parent and daughter non terminal in applied cfg rule we apply a maximum entropy model to the above method our method can easily include various feature for example other part of a parse tree or word the sentence contain we evaluated the method using manually compressed sentence and human judgment we found that our method produced more grammatical and informative compressed sentence than other method 
we present a method for noun phrase chunking in hebrew we show that the traditional definition of base np a non recursive noun phrase doe not apply in hebrew and propose an alternative definition of simple np we review syntactic property of hebrew related to noun phrase which indicate that the task of hebrew simplenp chunking is harder than base np chunking in english a a confirmation we apply method known to work well for english to hebrew data these method give low result f from to in hebrew we then discus our method which applies svm induction over lexical and morphological feature morphological feature improve the average precision by recall by and f measure by resulting in a system with average performance of precision recall and f measure 
this paper describes a hybrid model that combine a rule based model with two statistical model for the task of po guessing of chinese unknown word the rule based model is sensitive to the type length and internal structure of unknown word and the two statistical model utilize contextual information and the likelihood for a character to appear in a particular position of word of a particular length and po category by combining model that use different source of information the hybrid model achieves a precision of a significant improvement over the best result reported in previous study which wa 
we briefly describe a two way speech to speech english farsi translation system prototype developed for use in doctor patient interaction the overarching philosophy of the developer ha been to create a system that enables effective communication rather than focusing on maximizing component level performance the discussion focus on the general approach and evaluation of the system by an independent government evaluation team 
we briefly describe a two way speech tospeech english farsi translation system prototype developed for use in doctorpatient interaction the overarching philosophy of the developer ha been to create a system that enables effective communication rather than focusing on maximizing component level performance the discussion focus on the general approach and evaluation of the system by an independent government evaluation team 
with the overwhelming amount of biological knowledge stored in free text natural language processing nlp ha received much attention recently to make the task of managing information recorded in free text more feasible one requirement for most nlp system is the ability to accurately recognize biological entity term in free text and the ability to map these term to corresponding record in database such task is called biological named entity tagging in this paper we present a system that automatically construct a protein entity dictionary which contains gene or protein name associated with uniprot identifier using online resource the system can run periodically to always keep up to date with these online resource using online resource that were available on dec we obtained term for entity the dictionary can be accessed from the following website http biocreative ifsm umbc edu biothesaurus 
many automatic evaluation metric for machine translation mt rely on making comparison to human translation a resource that may not always be available we present a method for developing sentence level mt evaluation metric that do not directly rely on human reference translation our metric are developed using regression learning and are based on a set of weaker indicator of fluency and adequacy pseudo reference experimental result suggest that they rival standard reference based metric in term of correlation with human judgment on new test instance 
in this paper we investigate named entity transliteration based on a phonetic scoring method the phonetic method is computed using phonetic feature and carefully designed pseudo feature the proposed method is tested with four language arabic chinese hindi and korean and one source language english using comparable corpus the proposed method is developed from the phonetic method originally proposed in tao et al in contrast to the phonetic method in tao et al constructed on the basis of pure linguistic knowledge the method in this study is trained using the winnow machine learning algorithm there is salient improvement in hindi and arabic compared to the previous study moreover we demonstrate that the method can also achieve comparable result when it is trained on language data different from the target language the method can be applied both with minimal data and without target language data for various language 
this paper focus on the use of advanced technique of text analysis a support for collocation extraction a hybrid system is presented that combine statistical method and multilingual parsing for detecting accurate collocational information from english french spanish and italian corpus the advantage of relying on full parsing over using a traditional window method which ignores the syntactic information is first theoretically motivated then empirically validated by a comparative evaluation experiment 
this paper present the first empirical result to our knowledge on learning synchronous grammar that generate logical form using statistical machine translation technique a semantic parser based on a synchronous context free grammar augmented with operator is learned given a set of training sentence and their correct logical form the resulting parser is shown to be the bestperforming system so far in a database query domain 
we present a korean question answering framework for restricted domain called k qard k qard is developed to achieve domain portability and robustness and the framework is successfully applied to build question answering system for several domain 
this paper describes a system which generates animation for cooking action in recipe to help people understand recipe written in japanese the major goal of this research is to increase the scalability of the system i e to develop a system which can handle various kind of cooking action we designed and compiled the lexicon of cooking action required for the animation generation system the lexicon includes the action plan used for animation generation and the information about ingredient upon which the cooking action is taken preliminary evaluation show that our lexicon contains most of the cooking action that appear in japanese recipe we also discus how to handle linguistic expression in recipe which are not included in the lexicon in order to generate animation for them 
a number of metric for automatic evaluation of machine translation have been proposed in recent year with some metric focusing on measuring the adequacy of mt output and other metric focusing on fluency adequacy oriented metric such a bleu measure gram overlap of mt output and their reference but do not represent sentence level information in contrast fluency oriented metric such a rouge w compute longest common subsequence but ignore word not aligned by the lcs we propose a metric based on stochastic iterative string alignment sia which aim to combine the strength of both approach we compare sia with existing metric and find that it outperforms them in overall evaluation and work specially well in fluency evaluation 
in this paper we present several extension of marie a freely available n gram based statistical machine translation smt decoder the extension mainly consist of the ability to accept and generate word graph and the introduction of two new n gram model in the loglinear combination of feature function the decoder implement additionally the decoder is enhanced with a caching strategy that reduces the number of n gram call improving the overall search efficiency experiment are carried out over the eurpoean parliament spanish english translation task 
a verifying compiler is one that emits both object code and a proof of correspondence between object and source code we report the use of acl in building a verifying compiler for cryptol a stream based language for encryption algorithm specification that target rockwell collins aamp microprocessor and is designed to compile efficiently to hardware too this paper report on our success in verifying the core transformation of the compiler those transformation over the sub language of cryptol that begin after higher order aspect of the language are compiled away and finish just before hardware or software specific transformation are exercised the core transformation are responsible for aggressive optimization we have written an acl macro that automatically generates both the correspondence theorem and their proof the compiler also supply measure function that acl us to automatically prove termination of cryptol program including program with mutually recursive clique of stream our verifying compiler ha proved the correctness of it core transformation for multiple algorithm including tea rc and aes finally we describe an acl book of primitive operation for the general specification and verification of encryption algorithm 
this paper present a discriminative pruning method of n gram language model for chinese word segmentation to reduce the size of the language model that is used in a chinese word segmentation system importance of each bigram is computed in term of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram then we propose a step by step growing algorithm to build the language model of desired size experimental result show that the discriminative pruning method lead to a much smaller model compared with the model pruned using the state of the art method at the same chinese word segmentation f measure the number of bigram in the model can be reduced by up to correlation between language model perplexity and word segmentation performance is also discussed 
language model lm adaptation is important for both speech and language processing it is often achieved by combining a generic lm with a topic specific model that is more relevant to the target document unlike previous work on unsupervised lm adaptation this paper investigates how effectively using named entity ne information instead of considering all the word help lm adaptation we evaluate two latent topic analysis approach in this paper namely clustering and latent dirichlet allocation lda in addition a new dynamically adapted weighting scheme for topic mixture model is proposed based on lda topic analysis our experimental result show that the ne driven lm adaptation framework outperforms the baseline generic lm the best result is obtained using the lda based approach by expanding the named entity with syntactically filtered word together with using a large number of topic which yield a perplexity reduction of compared to the baseline generic lm 
this paper proposes a method for incrementally translating english spoken language into japanese to realize simultaneous translation between language with different word order such a english and japanese our method utilizes the feature that the word order of a target language is flexible to resolve the problem of generating a grammatically incorrect sentence our method us dependency structure and japanese dependency constraint to determine the word order of a translation moreover by considering the fact that the inversion of predicate expression occurs more frequently in japanese spoken language our method take advantage of a predicate inversion to resolve the problem that japanese ha the predicate at the end of a sentence furthermore our method includes the function of canceling an inversion by restating a predicate when the translation is incomprehensible due to the inversion we implement a prototype translation system and conduct an experiment with all sentence in the atis corpus the result indicate improvement in comparison to two other method 
a multimedia blog creation system is described that us japanese dialogue with an intelligent robot although multimedia blog are increasing in popularity creating blog is not easy for user who lack high level information literacy skill even skilled user have to waste time creating and assigning text description to their blog and searching related multimedia such a image music and illustration to enable effortless and enjoyable creation of multimedia blog we developed the system on a prototype robot called papero video message are recorded and converted into text description by papero using continuous speech recognition papero then search for suitable multimedia content on the internet and database and then based on the search result chooses appropriate sympathetic comment by using natural language text retrieval the retrieved content papero s comment and the video recording on the user s blog is automatically uploaded and edited the system wa evaluated by user for creating travel blog and proved to be helpful for both inexperienced and experienced user the system enabled easy multimedia rich blog creation and even provided user the pleasure of chatting with papero 
this paper introduces conceptual framework of an ontology for describing linguistic service on network based language infrastructure the ontology defines a taxonomy of processing resource and the associated static language resource it also develops a sub ontology for abstract linguistic object such a expression meaning and description these help define functionality of a linguistic service the proposed ontology is expected to serve a a solid basis for the interoperability of technical element in language infrastructure 
we propose a collaborative framework for collecting thai unknown word found on web page over the internet our main goal is to design and construct a web based system which allows a group of interested user to participate in constructing a thai unknown word open dictionary the proposed framework provides supporting algorithm and tool for automatically identifying and extracting unknown word from web page of given url the system yield the result of unknown word candidate which are presented to the user for verification the approved unknown word could be combined with the set of existing word in the lexicon to improve the performance of many nlp task such a word segmentation information retrieval and machine translation our framework includes word segmentation and morphological analysis module for handling the non segmenting characteristic of thai written language to take advantage of large available text resource on the web our unknown word boundary identification approach is based on the statistical string pattern matching algorithm 
we translate sentence generation from tag grammar with semantic and pragmatic information into a planning problem by encoding the contribution of each word declaratively and explicitly this allows u to exploit the performance of off the shelf planner it also open up new perspective on referring expression generation and the relationship between language and action 
we describe a novel neural network architecture for the problem of semantic role labeling many current solution are complicated consist of several stage and handbuilt feature and are too slow to be applied a part of real application that require such semantic label partly because of their use of a syntactic parser pradhan et al gildea and jurafsky our method instead learns a direct mapping from source sentence to semantic tag for a given predicate without the aid of a parser or a chunker our resulting system obtains accuracy comparable to the current state of the art at a fraction of the computational cost 
we present a learning framework for structured support vector model in which boosting and bagging method are used to construct ensemble model we also propose a selection method which is based on a switching model among a set of output of individual classifier when dealing with natural language parsing problem the switching model us subtrees mined from the corpus and a boosting based algorithm to select the most appropriate output the application of the proposed framework on the domain of semantic parsing show advantage in comparison with the original large margin method 
syntactic knowledge is important for pronoun resolution traditionally the syntactic information for pronoun resolution is represented in term of feature that have to be selected and defined heuristically in the paper we propose a kernel based method that can automatically mine the syntactic information from the parse tree for pronoun resolution specifically we utilize the parse tree directly a a structured feature and apply kernel function to this feature a well a other normal feature to learn the resolution classifier in this way our approach avoids the effort of decoding the parse tree into the set of flat syntactic feature the experimental result show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task 
this paper describes a method for automatically learning effective dialogue strategy generated from a library of dialogue content using reinforcement learning from user feedback this library includes greeting social dialogue chit chat joke and relationship building a well a the more usual clarification and verification component of dialogue we tested the method through a motivational dialogue system that encourages take up of exercise and show that it can be used to construct good dialogue strategy with little effort 
we present an implemented xml data model and a new simplified query language for multi level annotated corpus the new query language involves automatic conversion of query into the underlying more complicated mmaxql query language it support query for sequential and hierarchical but also associative e g coreferential relation the simplified query language ha been designed with non expert user in mind 
this paper describes our attempt at nombank based automatic semantic role labeling srl nombank is a project at new york university to annotate the argument structure for common noun in the penn treebank ii corpus we treat the nombank srl task a a classification problem and explore the possibility of adapting feature previously shown useful in propbank based srl system various nombank specific feature are explored on test section our best system achieves f score of when correct automatic syntactic parse tree are used to our knowledge this is the first reported automatic nombank srl system 
this paper suggests refinement for the distributional similarity hypothesis our proposed hypothesis relate the distributional behavior of pair of word to lexical entailment a tighter notion of semantic similarity that is required by many nlp application to automatically explore the validity of the defined hypothesis we developed an inclusion testing algorithm for characteristic feature of two word which incorporates corpus and web based feature sampling to overcome data sparseness the degree of hypothesis validity wa then empirically tested and manually analyzed with respect to the word sense level in addition the above testing algorithm wa exploited to improve lexical entailment acquisition 
we propose a novel approach to crosslingual language model lm adaptation based on bilingual latent semantic analysis blsa a blsa model is introduced which enables latent topic distribution to be efficiently transferred across language by enforcing a one to one topic correspondence during training using the proposed blsa framework crosslingual lm adaptation can be performed by first inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language n gram lm via marginal adaptation the proposed framework also enables rapid bootstrapping of lsa model for new language based on a source lsa model from another language on chinese to english speech and text translation the proposed blsa framework successfully reduced word perplexity of the english lm by over for a unigram lm and up to for a gram lm furthermore the proposed approach consistently improved machine translation quality on both speech and text based adaptation 
dialogue system are one of the most challenging application of natural language processing in recent year some statistical dialogue model have been proposed to cope with the dialogue problem the evaluation of these model is usually performed by using them a annotation model many of the work on annotation use information such a the complete sequence of dialogue turn or the correct segmentation of the dialogue this information is not usually available for dialogue system in this work we propose a statistical model that us only the information that is usually available and performs the segmentation and annotation at the same time the result of this model reveal the great influence that the availability of a correct segmentation ha in obtaining an accurate annotation of the dialogue 
we investigate the logical issue behind axiomatizing equation that contain both recursive call and quantifier in acl we identify a class of such equation named extended tail recursive equation that can be uniformly introduced in the logic we point out some potential benefit of this axiomatization and discus the logical impediment behind introducing more general quantified formula 
we describe an open source toolkit for statistical machine translation whose novel contribution are a support for linguistically motivated factor b confusion network decoding and c efficient data format for translation model and language model in addition to the smt decoder the toolkit also includes a wide variety of tool for training tuning and applying the system to many translation task 
data sparseness is one of the factor that degrade statistical machine translation smt existing work ha shown that using morpho syntactic information is an effective solution to data sparseness however fewer effort have been made for chinese to english smt with using english morpho syntactic analysis we found that while english is a language with le inflection using english lemma in training can significantly improve the quality of word alignment that lead to yield better translation performance we carried out comprehensive experiment on multiple training data of varied size to prove this we also proposed a new effective linear interpolation method to integrate multiple homologous feature of translation model 
the increasing complexity of summarization system make it difficult to analyze exactly which module make a difference in performance we carried out a principled comparison between the two most commonly used scheme for assigning importance to word in the context of query focused multi document summarization raw frequency word probability and log likelihood ratio we demonstrate that the advantage of log likelihood ratio come from it known distributional property which allow for the identification of a set of word that in it entirety defines the aboutness of the input we also find that llr is more suitable for query focused summarization since unlike raw frequency it is more sensitive to the integration of the information need defined by the user 
we present the currently most efficient solver for scope underspecification it also convert between different underspecification formalism and count reading our tool make the practical use of large scale grammar with underspecified semantic output more feasible and can be used in grammar debugging 
we present an approach to query expansion in answer retrieval that us statistical machine translation smt technique to bridge the lexical gap between question and answer smt based query expansion is done by i using a full sentence paraphraser to introduce synonym in context of the entire query and ii by translating query term into answer term using a full sentence smt model trained on question answer pair we evaluate these global context aware query expansion technique on tfidf retrieval from million question answer pair extracted from faq page experimental result show that smtbased expansion improves retrieval performance over local expansion and over retrieval without expansion 
we describe a new loss function due to jeon and lin for estimating structured log linear model on arbitrary feature the loss function can be seen a a generative alternative to maximum likelihood estimation with an interesting information theoretic interpretation and it is statistically consistent it is substantially faster than maximum conditional likelihood estimation of conditional random field lafferty et al an order of magnitude or more we compare it performance and training time to an hmm a crf an memm and pseudolikelihood on a shallow parsing task these experiment help tease apart the contribution of rich feature and discriminative training which are shown to be more than additive 
this paper considers the problem of automatic assessment of local coherence we present a novel entity based representation of discourse which is inspired by centering theory and can be computed automatically from raw text we view coherence assessment a a ranking learning problem and show that the proposed discourse representation support the effective learning of a ranking function our experiment demonstrate that the induced model achieves significantly higher accuracy than a state of the art coherence model 
this paper explores the role of information retrieval in answering relationship question a new class complex information need formally introduced in trec since information retrieval is often an integral component of many question answering strategy it is important to understand the impact of different term based technique within a framework of sentence retrieval we examine three factor that contribute to question answering performance the use of different retrieval engine relevance both at the document and sentence level and redundancy result point out the limitation of purely term based method to this challenging task nevertheless ir based technique provide a strong baseline on top of which more sophisticated language processing technique can be deployed 
japanese dependency structure is usually represented by relationship between phrasal unit called bunsetsus one of the biggest problem with dependency structure analysis in spontaneous speech is that clause boundary are ambiguous this paper describes a method for detecting the boundary of quotation and inserted clause and that for improving the dependency accuracy by applying the detected boundary to dependency structure analysis the quotation and inserted clause are determined by using an svm based text chunking method that considers information on morpheme pause filler etc the information on automatically analyzed dependency structure is also used to detect the beginning of the clause our evaluation experiment using corpus of spontaneous japanese csj showed that the automatically estimated boundary of quotation and inserted clause helped to improve the accuracy of dependency structure analysis 
previous research applying kernel method to natural language parsing have focussed on proposing kernel over parse tree which are hand crafted based on domain knowledge and computational consideration in this paper we propose a method for defining kernel in term of a probabilistic model of parsing this model is then trained so that the parameter of the probabilistic model reflect the generalization in the training data the method we propose then us these trained parameter to define a kernel for reranking parse tree in experiment we use a neural network based statistical parser a the probabilistic model and use the resulting kernel with the voted perceptron algorithm to rerank the top par from the probabilistic model this method achieves a significant improvement over the accuracy of the probabilistic model 
many error produced by unsupervised and semi supervised relation extraction re system occur because of wrong recognition of entity that participate in the relation this is especially true for system that do not use separate named entity recognition component instead relying on general purpose shallow parsing such system have greater applicability because they are able to extract relation that contain attribute of unknown type however this generality come with the cost in accuracy in this paper we show how to use corpus statistic to validate and correct the argument of extracted relation instance improving the overall re performance we test the method on sres a self supervised web relation extraction system we also compare the performance of corpus based method to the performance of validation and correction method based on supervised ner component 
an open domain spoken dialog system ha to deal with the challenge of lacking lexical a well a conceptual knowledge a the real world is constantly changing it is not possible to store all necessary knowledge beforehand therefore this knowledge ha to be acquired during the run time of the system with the help of the out of vocabulary information of a speech recognizer a every word can have various meaning depending on the context in which it is uttered additional context information is taken into account when searching for the meaning of such a word in this paper i will present the incremental ontology learning framework on l the defined task for the framework are the hypernym extraction from internet text for unknown term delivered by the speech recognizer the mapping of those and their hypernym into ontological concept and instance and the following integration of them into the system s ontology 
we investigate the factor which determine constituent order in german clause and propose an algorithm which performs the task in two step first the best candidate for the initial sentence position is chosen then the order for the remaining constituent is determined the rst task is more difcult than the second one because of property of the german sentence initial position experiment show a signicant improvement over competing approach our algorithm is also more efcient than these 
we present a novel method for predicting in flected word form for generating morpho logically rich language in machine trans lation we utilize a rich set of syntactic and morphological knowledge source from both source and target sentence in a prob abilistic model and evaluate their contribu tion in generating russian and arabic sen tences our result show that the proposed model substantially outperforms the com monly used baseline of a trigram target lan guage model in particular the use of mor phological and syntactic feature lead to large gain in prediction accuracy we also show that the proposed method is effective with a relatively small amount of data 
a natural language understanding research advance towards deeper knowledge modeling the task become more and more complex we are interested in more nuanced word characteristic more linguistic property deeper semantic and syntactic feature one such example explored in this article is the mention detection and recognition task in the automatic content extraction project with the goal of identifying named nominal or pronominal reference to real world entity mention and labeling them with three type of information entity type entity subtype and mention type in this article we investigate three method of assigning these related tag and compare them on several data set a system based on the method presented in this article participated and ranked very competitively in the ace evaluation 
an emotion lexicon is an indispensable resource for emotion analysis this paper aim to mine the relationship between word and emotion using weblog corpus a collocation model is proposed to learn emotion lexicon from weblog article emotion classification at sentence level is experimented by using the mined lexicon to demonstrate their usefulness 
this paper proposes a novel method of building polarity tagged corpus from html document the characteristic of this method is that it is fully automatic and can be applied to arbitrary html document the idea behind our method is to utilize certain layout structure and linguistic pattern by using them we can automatically extract such sentence that express opinion in our experiment the method could construct a corpus consisting of sentence 
we investigate the use of machine learning in combination with feature engineering technique to explore human multimodal clarification strategy and the use of those strategy for dialogue system we learn from data collected in a wizard of oz study where different wizard could decide whether to ask a clarification request in a multimodal manner or else use speech alone we show that there is a uniform strategy across wizard which is based on multiple feature in the context these are generic runtime feature which can be implemented in dialogue system our prediction model achieve a weighted f score of which is a improvement over a one rule baseline to ass the effect of model feature discretisation and selection we also conduct a regression analysis we then interpret and discus the use of the learnt strategy for dialogue system throughout the investigation we discus the issue arising from using small initial wizard of oz data set and we show that feature engineering is an essential step when learning from such limited data 
when training the parameter for a natural language system one would prefer to minimize best loss error on an evaluation set since the error surface for many natural language problem is piecewise constant and riddled with local minimum many system instead optimize log likelihood which is conveniently differentiable and convex we propose training instead to minimize the expected loss or risk we define this expectation using a probability distribution over hypothesis that we gradually sharpen anneal to focus on the best hypothesis besides the linear loss function used in previous work we also describe technique for optimizing nonlinear function such a precision or the bleu metric we present experiment training log linear combination of model for dependency parsing and for machine translation in machine translation annealed minimum risk training achieves significant improvement in bleu over standard minimum error training we also show improvement in labeled dependency parsing 
shortage of manually sense tagged data is an obstacle to supervised word sense disambiguation method in this paper we investigate a label propagation based semi supervised learning algorithm for wsd which combine labeled and unlabeled data in learning process to fully realize a global consistency assumption similar example should have similar label our experimental result on benchmark corpus indicate that it consistently outperforms svm when only very few labeled example are available and it performance is also better than monolingual bootstrapping and comparable to bilingual bootstrapping 
we are currently developing a translation aid system specially designed for english to japanese volunteer translator working mainly online in this paper we introduce the stratified reference lookup interface that ha been incorporated into the source text area of the system which distinguishes three user awareness level depending on the type and nature of the reference unit the different awareness level are assigned to reference unit from a variety of reference source according to the criterion of composition difficulty speciality and resource type 
we approach the zero anaphora resolution problem by decomposing it into intra sentential and inter sentential zero anaphora resolution for the former problem syntactic pattern of the appearance of zero pronoun and their antecedent are useful clue taking japanese a a target language we empirically demonstrate that incorporating rich syntactic pattern feature in a state of the art learning based anaphora resolution model dramatically improves the accuracy of intra sentential zero anaphora which consequently improves the overall performance of zero anaphora resolution 
we present a novel approach for discovering word category set of word sharing a significant aspect of their meaning we utilize meta pattern of high frequency word and content word in order to discover pattern candidate symmetric pattern are then identified using graph based measure and word category are created based on graph clique set our method is the first pattern based method that requires no corpus annotation or manually provided seed pattern or word we evaluate our algorithm on very large corpus in two language using both human judgment and wordnet based evaluation our fully unsupervised result are superior to previous work that used a po tagged corpus and computation time for huge corpus are order of magnitude faster than previously reported 
in this paper we propose a machine learning approach to paragraph boundary identification which utilizes linguistically motivated feature we investigate the relation between paragraph boundary and discourse cue pronominalization and information structure we test our algorithm on german data and report improvement over three baseline including a reimplementation of sporleder lapata s work on paragraph segmentation an analysis of the feature contribution suggests an interpretation of what paragraph boundary indicate and what they depend on 
we present a portable translator that recognizes and translates phrase on signboard and menu a captured by a builtin camera this system can be used on pda or mobile phone and resolve the difficulty of inputting some character set such a japanese and chinese if the user doesn t know their reading through the high speed mobile network small image of signboard can be quickly sent to the recognition and translation server since the server run state of the art recognition and translation technology and huge dictionary the proposed system offer more accurate character recognition and machine translation 
this paper present an unsupervised learning approach to disambiguate various relation between name entity by use of various lexical and syntactic feature from the context it work by calculating eigen vector of an adjacency graph s laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors experiment result on ace corpus show that this spectral clustering based approach outperforms the other clustering method 
we present a global discriminative statistical word order model for machine translation our model combine syntactic movement and surface movement information and is discriminatively trained to choose among possible word order we show that combining discriminative training with feature to detect these two different kind of movement phenomenon lead to substantial improvement in word ordering performance over strong baseline integrating this word order model in a baseline mt system result in a point improvement in bleu for english to japanese translation 
this paper demonstrates a conceptually simple but effective method of increasing the accuracy of qa system on factoid style question we define the notion of an inverted question and show that by requiring that the answer to the original and inverted question be mutually consistent incorrect answer get demoted in confidence and correct one promoted additionally we show that lack of validation can be used to assert no answer nil condition we demonstrate increase of performance on trec and other question set and discus the kind of future activity that can be particularly beneficial to approach such a ours 
natural language generation nlg is a way to automatically realize a correct expression in response to a communicative goal this technology is mainly explored in the field of machine translation report generation dialog system etc in this paper we have explored the nlg technique for another novel application assisting disabled child to take part in conversation the limited physical ability and mental maturity of our intended user made the nlg approach different from others we have taken a flexible approach where main emphasis is given on flexibility and usability of the system the evaluation result show this technique can increase the communication rate of user during a conversation 
we present a term recognition approach to extract acronym and their definition from a large text collection parenthetical expression appearing in a text collection are identified a potential acronym assuming term appearing frequently in the proximity of an acronym to be the expanded form definition of the acronym we apply a term recognition method to enumerate such candidate and to measure the likelihood score of the expanded form based on the list of the expanded form and their likelihood score the proposed algorithm determines the final acronym definition pair the proposed method combined with a letter matching algorithm achieved precision and recall on an evaluation corpus with acronym definition pair 
event based summarization extract and organizes summary sentence in term of the event that the sentence describe in this work we focus on semantic relation among event term by connecting term with relation we build up event term graph upon which relevant term are grouped into cluster we assume that each cluster represents a topic of document then two summarization strategy are investigated i e selecting one term a the representative of each topic so a to cover all the topic or selecting all term in one most significant topic so a to highlight the relevant information related to this topic the selected term are then responsible to pick out the most appropriate sentence describing them the evaluation of clustering based summarization on duc document set show encouraging improvement over the well known pagerank based summarization 
spoken monologue feature greater sentence length and structural complexity than do spoken dialogue to achieve high parsing performance for spoken monologue it could prove effective to simplify the structure by dividing a sentence into suitable language unit this paper proposes a method for dependency parsing of japanese monologue based on sentence segmentation in this method the dependency parsing is executed in two stage at the clause level and the sentence level first the dependency within a clause are identified by dividing a sentence into clause and executing stochastic dependency parsing for each clause next the dependency over clause boundary are identified stochastically and the dependency structure of the entire sentence is thus completed an experiment using a spoken monologue corpus show this method to be effective for efficient dependency parsing of japanese monologue sentence 
this paper examines what kind of similarity between word can be represented by what kind of word vector in the vector space model through two experiment three method for constructing word vector i e lsa based cooccurrence based and dictionary based method were compared in term of the ability to represent two kind of similarity i e taxonomic similarity and associative similarity the result of the comparison wa that the dictionary based word vector better reflect taxonomic similarity while the lsa based and the cooccurrence based word vector better reflect associative similarity 
this paper proposes a novel method for phrase based statistical machine translation by using pivot language to conduct translation between language lf and le with a small bilingual corpus we bring in a third language lp which is named the pivot language for lf lp and lp le there exist large bilingual corpus using only lf lp and lp le bilingual corpus we can build a translation model for lf le the advantage of this method lie in that we can perform translation between lf and le even if there is no bilingual corpus available for this language pair using bleu a a metric our pivot language method achieves an absolute improvement of relative a compared with the model directly trained with lf le sentence pair for french spanish translation moreover with a small lf le bilingual corpus available our method can further improve the translation quality by using the additional lf lp and lp le bilingual corpus 
in this paper we exploit non local feature a an estimate of long distance dependency to improve performance on the statistical spoken language understanding slu problem the statistical natural language parser trained on text perform unreliably to encode non local information on spoken language an alternative method we propose is to use trigger pair that are automatically extracted by a feature induction algorithm we describe a light version of the inducer in which a simple modification is efficient and successful we evaluate our method on an slu task and show an error reduction of up to over the base local model 
we present a general framework for automatically extracting social network and biographical fact from conversational speech our approach relies on fusing the output produced by multiple information extraction module including entity recognition and detection relation detection and event detection module we describe the specific feature and algorithmic refinement effective for conversational speech these cumulatively increase the performance of social network extraction from to for the development set and from to for the test set a measured by f measure on the tie within a network the same framework can be applied to other genre of text we have built an automatic biography generation system for general domain text using the same approach 
in this paper we present a method for guessing po tag of unknown word using local and global information although many existing method use only local information i e limited window size or intra sentential feature global information extra sentential feature provides valuable clue for predicting po tag of unknown word we propose a probabilistic model for po guessing of unknown word using global information a well a local information and estimate it parameter using gibbs sampling we also attempt to apply the model to semi supervised learning and conduct experiment on multiple corpus 
using abundant web resource to mine chinese term translation can be applied in many field such a reading writing assistant machine translation and cross language information retrieval in mining english translation of chinese term how to obtain effective web page and evaluate translation candidate are two challenging issue in this paper the approach based on semantic prediction is first proposed to obtain effective web page the proposed method predicts possible english meaning according to each constituent unit of chinese term and expands these english item using semantically relevant knowledge for searching the refined related term are extracted from top retrieved document through feedback learning to construct a new query expansion for acquiring more effective web page for obtaining a correct translation list a translation evaluation method in the weighted sum of multi feature is presented to rank these candidate estimated from effective web page experimental result demonstrate that the proposed method ha good performance in chinese english term translation acquisition and achieves accuracy 
this paper describes the largest scale annotation project involving the enron email corpus to date over email were classified by human into the category business and personal and then sub categorised by type within these category the paper quantifies how well human perform on this task evaluated by inter annotator agreement it present the problem experienced with the separation of these language type a a final section the paper present preliminary result using a machine to perform this classification task 
we investigate automatic classification of speculative language hedging in biomedical text using weakly supervised machine learning our contribution include a precise description of the task with annotation guideline analysis and discussion a probabilistic weakly supervised learning model and experimental evaluation of the method presented we show that hedge classification is feasible using weakly supervised ml and point toward avenue for future research 
we explore the use of restricted dialogue context in reinforcement learning rl of effective dialogue strategy for information seeking spoken dialogue system e g communicator walker et al the context we use are richer than previous research in this area e g levin and pieraccini scheffler and young singh et al pietquin which use only slot based information but are much le complex than the full dialogue information state explored in henderson et al for which tractabe learning is an issue we explore how incrementally adding richer feature allows learning of more effective dialogue strategy we use user simulation learned from communicator data walker et al georgila et al b to explore the effect of different feature on learned dialogue strategy our result show that adding the dialogue move of the last system and user turn increase the average reward of the automatically learned strategy by over the original hand coded communicator system and by over a baseline rl policy that us only slot status feature we show that the learned strategy exhibit an emergent focus switching strategy and effective use of the give help action 
we propose a novel method for automatically interpreting compound noun based on a predefined set of semantic relation first we map verb token in sentential context to a fixed set of seed verb using wordnet similarity and moby s thesaurus we then match the sentence with semantic relation based on the semantics of the seed verb and grammatical role of the head noun and modifier based on the semantics of the matched sentence we then build a classifier using timbl the performance of our final system at interpreting nc is 
this paper proposes a new approach for multi word expression mwe extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis theory of longest common subsequence lcs originates from computer science and ha been established a affine gap model in bioinformatics we perform this developed lcs technique combined with linguistic criterion in mwe extraction in comparison with traditional n gram method which is the major technique for mwe extraction lcs approach is applied with great efficiency and performance guarantee experimental result show that lcs based approach achieves better result than n gram 
we study unsupervised method for learning refinement of the nonterminals in a treebank following matsuzaki et al and prescher we may for example split np without supervision into np and np which behave differently we first propose to learn a pcfg that add such feature to nonterminals in such a way that they respect pattern of linguistic feature passing each node s nonterminal feature are either identical to or independent of those of it parent this linguistic constraint reduces runtime and the number of parameter to be learned however it did not yield improvement when training on the penn treebank an orthogonal strategy wa more successful to improve the performance of the em learner by treebank preprocessing and by annealing method that split nonterminals selectively using these method we can maintain high parsing accuracy while dramatically reducing the model size 
most approach to event extraction focus on mention anchored in verb however many mention of event surface a noun phrase detecting them can increase the recall of event extraction and provide the foundation for detecting relation between event this paper describes a weakly supervised method for detecting nominal event mention that combine technique from word sense disambiguation wsd and lexical acquisition to create a classifier that label noun phrase a denoting event or non event the classifier us boot strapped probabilistic generative model of the context of event and non event the context are the lexically anchored semantic dependency relation that the np appear in our method dramatically improves with bootstrapping and comfortably outperforms lexical lookup method which are based on very much larger hand crafted resource 
security critical application at the highest evaluation assurance level eal require formal proof of correctness in order to achieve certification to support secure application development at the highest eals we have developed technique to largely automate the process of producing proof of correctness of machine code a part of the secure high assurance development environment program we have produced in acl an executable formal model of the rockwell collins aamp g microprocessor at the instruction set level in order to facilitate proof of correctness about that processor s machine code the aamp g currently in use in rockwell collins secure system product support strict time and space partitioning in hardware and ha received a u s national security agency nsa multiple independent level of security mil certificate based in part on a formal proof of correctness of it separation kernel microcode proof of correctness of aamp g machine code are accomplished using the method of compositional cutpoints which requires neither traditional clock function nor a verification condition generator vcg in this paper we will summarize the aamp g architecture detail our acl model of the processor and describe our development of the compositional cutpoint method into a robust machine code proof framework 
in this paper we focus on how to improve pronoun resolution using the statistic based semantic compatibility information we investigate two unexplored issue that influence the effectiveness of such information statistic source and learning framework specifically we for the first time propose to utilize the web and the twin candidate model in addition to the previous combination of the corpus and the single candidate model to compute and apply the semantic information our study show that the semantic compatibility obtained from the web can be effectively incorporated in the twin candidate learning model and significantly improve the resolution of neutral pronoun 
this paper present an effective approach for resume information extraction to support automatic resume management and routing a cascaded information extraction ie framework is designed in the first pas a resume is segmented into a consecutive block attached with label indicating the information type then in the second pas the detailed information such a name and address are identified in certain block e g block labelled with personal information instead of searching globally in the entire resume the most appropriate model is selected through experiment for each ie task in different pass the experimental result show that this cascaded hybrid model achieves better f score than flat model that do not apply the hierarchical structure of resume it also show that applying different ie model in different pass according to the contextual structure is effective 
in this work we provide an empirical analysis of difference in word use between gender in telephone conversation which complement the considerable body of work in sociolinguistics concerned with gender linguistic difference experiment are performed on a large speech corpus of roughly conversation we employ machine learning technique to automatically categorize the gender of each speaker given only the transcript of his her speech achieving accuracy an analysis of the most characteristic word for each gender is also presented experiment reveal that the gender of one conversation side influence lexical use of the other side a surprising result is that we were able to classify male only v female only conversation with almost perfect accuracy 
in this work we propose a translation model for monolingual sentence retrieval we propose four method for constructing a parallel corpus of the four method proposed a lexicon learned from a bilingual arabicenglish corpus aligned at the sentence level performs best significantly improving result over the query likelihood baseline further we demonstrate that smoothing from the local context of the sentence improves retrieval over the query likelihood baseline 
we present the design and evaluation of a translator s amenuensis that us comparable corpus to propose and rank nonliteral solution to the translation of expression from the general lexicon using distributional similarity and bilingual dictionary the method outperforms established technique for extracting translation equivalent from parallel corpus the interface to the system is available at http corpus leeds ac uk assist v 
the research below explores scheme for evaluating automatic summary of business meeting using the icsi meeting corpus janin et al both automatic and subjective evaluation were carried out with a central interest being whether or not the two type of evaluation correlate with each other the evaluation metric were used to compare and contrast differing approach to automatic summarization the deterioration of summary quality on asr output versus manual transcript and to determine whether manual extract are rated significantly higher than automatic extract 
motivated by psycholinguistic finding that eye gaze is tightly linked to human language production we developed an unsupervised approach based on translation model to automatically learn the mapping between word and object on a graphic display during human machine conversation the experimental result indicate that user eye gaze can provide useful information to establish such mapping which have important implication in automatically acquiring and interpreting user vocabulary for conversational system 
in this paper we present a novel global reordering model that can be incorporated into standard phrase based statistical machine translation unlike previous local reordering model that emphasize the reordering of adjacent phrase pair till mann and zhang our model explicitly model the reordering of long distance by directly estimating the parameter from the phrase alignment of bilingual training sentence in principle the global phrase reordering model is conditioned on the source and target phrase that are currently being translated and the previously translated source and target phrase to cope with sparseness we use n best phrase alignment and bilingual phrase clustering and investigate a variety of combination of conditioning factor through experiment we show that the global reordering model significantly improves the translation accuracy of a standard japanese english translation task 
this paper discus sampling strategy for building a dependency analyzed corpus and analyzes them with different kind of corpus we used the kyoto text corpus a dependency analyzed corpus of newspaper article and prepared the ipal corpus a dependency analyzed corpus of example sentence in dictionary a a new and different kind of corpus the experimental result revealed that the length of the test set controlled the accuracy and that the longest first strategy wa good for an expanding corpus but this wa not the case when constructing a corpus from scratch 
countability of english noun is important in various natural language processing task it especially play an important role in machine translation since it determines the range of possible determiner this paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse it claim that when a noun appears more than once in a discourse they will all share the same countability in the discourse the basic idea of the proposed method is that mispredictions can be correctly overridden using efficiently the one countability per discourse property experiment show that the proposed method successfully reinforces countability prediction and outperforms other method used for comparison 
we present a novel approach to the word sense disambiguation problem which make use of corpus based evidence combined with background knowledge employing an inductive logic programming algorithm the approach generates expressive disambiguation rule which exploit several knowledge source and can also model relation between them the approach is evaluated in two task identification of the correct translation for a set of highly ambiguous verb in englishportuguese translation and disambiguation of verb from the senseval lexical sample task the average accuracy obtained for the multilingual task outperforms the other machine learning technique investigated in the monolingual task the approach performs a well a the state of the art system which reported result for the same set of verb 
in this paper we investigate a structured model for jointly classifying the sentiment of text at varying level of granularity inference in the model is based on standard sequence classification technique using constrained viterbi to ensure consistent solution the primary advantage of such a model is that it allows classification decision from one level in the text to influence decision at another experiment show that this method can significantly reduce classification error relative to model trained in isolation 
we have previously introduced a method of word sense disambiguation that computes the intended sense of a target word using wordnet based measure of semantic relatedness patwardhan et al senserelate targetword is a perl package that implement this algorithm the disambiguation process is carried out by selecting that sense of the target word which is most related to the context word relatedness between word sens is measured using the wordnet similarity perl module 
we consider the problem of producing a multi document summary given a collection of document since most successful method of multi document summarization are still largely extractive in this paper we explore just how well an extractive method can perform we introduce an oracle score based on the probability distribution of unigrams in human summary we then demonstrate that with the oracle score we can generate extract which score on average better than the human summary when evaluated with rouge in addition we introduce an approximation to the oracle score which produce a system with the best known performance for the document understanding conference duc evaluation 
this paper proposes the idea of ranking definition of a person a set of biographical fact to automatically generate who is this quiz the definition are ordered according to how difficult they make it to name the person such ranking would enable user to interactively learn about a person through dialogue with a system with improved understanding and lasting motivation which is useful for educational system in our approach we train a ranker that learns from data the appropriate ranking of definition based on feature that encode the importance of keywords in a definition a well a it content experimental result show that our approach is significantly better in ranking definition than baseline that use conventional information retrieval measure such a tf idf and pointwise mutual information pmi 
this paper proposes a supervised learning method for detecting a semantic relation between a given pair of named entity which may be located in different sentence the method employ newly introduced contextual feature based on centering theory a well a conventional syntactic and word based feature these feature are organized a a tree structure and are fed into a boosting based classification algorithm experimental result show the proposed method outperformed prior method and increased precision and recall by and 
various kind of scored dependency graph are proposed a packed shared data structure in combination with optimum dependency tree search algorithm this paper classifies the scored dependency graph and discus the specific feature of the dependency forest df which is the packed shared data structure adopted in the preference dependency grammar pdg and proposes the graph branch algorithm for computing the optimum dependency tree from a df this paper also report the experiment showing the computational amount and behavior of the graph branch algorithm 
until quite recently extending phrase based statistical machine translation pbsmt with syntactic structure caused system performance to deteriorate in this work we show that incorporating lexical syntactic description in the form of supertags can yield significantly better pbsmt system we describe a novel pbsmt model that integrates supertags into the target language model and the target side of the translation model two kind of supertags are employed those from lexicalized tree adjoining grammar and combinatory categorial grammar despite the difference between these two approach the supertaggers give similar improvement in addition to supertagging we also explore the utility of a surface global grammaticality measure based on combinatory operator we perform various experiment on the arabic to english nist test set addressing issue such a sparseness scalability and the utility of system subcomponents our best result bleu improves by relative to a state of theart pbsmt model which compare very favourably with the leading system on the nist task 
this paper present a comparative study of probabilistic treebank parsing of german using the negra and t ba d z tree bank experiment with the stanford parser which us a factored pcfg and dependency model show that contrary to previous claim for other parser lexicalization of pcfg model boost parsing performance for both treebanks the experiment also show that there is a big difference in parsing performance when trained on the negra and on the t ba d z treebanks parser performance for the model trained on t ba d z are comparable to parsing result for english with the stanford parser when trained on the penn treebank this comparison at least suggests that german is not harder to parse than it west germanic neighbor language english 
we investigate the unsupervised detection of semi fixed cue phrase such a this paper proposes a novel approach from unseen text on the basis of only a handful of seed cue phrase with the desired semantics the problem in contrast to bootstrapping approach for question answering and information extraction is that it is hard to find a constraining context for occurrence of semi fixed cue phrase our method us component of the cue phrase itself rather than external context to bootstrap it successfully excludes phrase which are different from the target semantics but which look superficially similar the method achieves accuracy outperforming standard bootstrapping approach 
this interactive presentation describes lexnet a graphical environment for graph based nlp developed at the university of michigan lexnet includes lexrank for text summarization biased lexrank for passage retrieval and tumbl for binary classification all tool in the collection are based on random walk on lexical graph that is graph where different nlp object e g sentence or phrase are represented a node linked by edge proportional to the lexical similarity between the two node we will demonstrate these tool on a variety of nlp task including summarization question answering and prepositional phrase attachment 
this paper present an extensive evaluation of five different alignment and investigates their impact on the corresponding mt system output we introduce new measure for intrinsic evaluation and examine the distribution of phrase and untranslated word during decoding to identify which characteristic of different alignment affect translation we show that precision oriented alignment yield better mt output translating more word and using longer phrase than recall oriented alignment 
this paper present archivus a multi modal language enabled meeting browsing and retrieval system the prototype is in an early stage of development and we are currently exploring the role of natural language for interacting in this relatively unfamiliar and complex domain we briefly describe the design and implementation status of the system and then focus on how this system is used to elicit useful data for supporting hypothesis about multimodal interaction in the domain of meeting retrieval and for developing nlp module for this specific domain 
in this paper we propose a new ensemble document clustering method the novelty of our method is the use of non negative matrix factorization nmf in the generation phase and a weighted hypergraph in the integration phase in our experiment we compared our method with some clustering method our method achieved the best result 
data driven grammatical function tag assignment ha been studied for english using the penn ii treebank data in this paper we address the question of whether such method can be applied successfully to other language and treebank resource in addition to tag assignment accuracy and f score we also present result of a task based evaluation we use three machine learning method to assign cast lb function tag to sentence parsed with bikel s parser trained on the cast lb treebank the best performing method svm achieves an f score of on gold standard tree and on parser output a statistically significant improvement of over the baseline in a task based evaluation we generate lfg functional structure from the function tag enriched tree on this task we achive an f score of a statistically significant improvement over the baseline 
recent study suggest that machine learning can be applied to develop good automatic evaluation metric for machine translated sentence this paper further analyzes aspect of learning that impact performance we argue that previously proposed approach of training a humanlikeness classifier is not a well correlated with human judgment of translation quality but that regression based learning produce more reliable metric we demonstrate the feasibility of regression based metric through empirical analysis of learning curve and generalization study and show that they can achieve higher correlation with human judgment than standard automatic metric 
spoken language understanding slu address the problem of extracting semantic meaning conveyed in an utterance the traditional knowledge based approach to this problem is very expensive it requires joint expertise in natural language processing and speech recognition and best practice in language engineering for every new domain on the other hand a statistical learning approach need a large amount of annotated data for model training which is seldom available in practical application outside of large research lab a generative hmm cfg composite model which integrates easy to obtain domain knowledge into a data driven statistical learning framework ha previously been introduced to reduce data requirement the major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework we also study and compare conditional random field crfs with perceptron learning for slu experimental result show that the conditional model achieve more than relative reduction in slot error rate over the hmm cfg model which had already achieved an slu accuracy at the same level a the best result reported on the atis data 
this paper present an approach for the automatic acquisition of qualia structure for noun from the web and thus open the possibility to explore the impact of qualia structure for natural language processing at a larger scale the approach build on earlier work based on the idea of matching specific lexico syntactic pattern conveying a certain semantic relation on the world wide web using standard search engine in our approach the qualia element are actually ranked for each qualia role with respect to some measure the specific contribution of the paper lie in the extensive analysis and quantitative comparison of different measure for ranking the qualia element further for the first time we present a quantitative evaluation of such an approach for learning qualia structure with respect to a handcrafted gold standard 
we present a comparative study on machine translation evaluation according to two different criterion human likeness and human acceptability we provide empirical evidence that there is a relationship between these two kind of evaluation human likeness implies human acceptability but the reverse is not true from the point of view of automatic evaluation this implies that metric based on human likeness are more reliable for system tuning our result also show that current evaluation metric are not always able to distinguish between automatic and human translation in order to improve the descriptive power of current metric we propose the use of additional syntax based metric and metric combination inside the qarla framework 
alvis research the design use and interoperability of topic specic search engine with the goal of developing an open source prototype of a peer to peer semantic based search engine our approach is not the traditional semantic web approach with coded meta data but rather an engine that can build on content through semi automatic analysis this paper describes the alvis document processing architecture the rst part of the alvis pipeline 
one of the challenge in the automatic generation of referring expression is to identify a set of domain entity coherently that is from the same conceptual perspective we describe and evaluate an algorithm that generates a conceptually coherent description of a target set the design of the algorithm is motivated by the result of psycholinguistic experiment 
we propose a robust method of automatically constructing a bilingual word sense dictionary from readily available monolingual ontology by using estimation maximization without any annotated training data or manual tuning we demonstrate our method on the english framenet and chinese hownet structure owing to the robustness of em iteration in improving translation likelihood our word sense translation accuracy are very high at on average for the most ambiguous word in the english framenet with sens or more we also carried out a pilot study on using this automatically generated bilingual word sense dictionary to choose the best translation candidate and show the first significant evidence that frame semantics are useful for translation disambiguation translation disambiguation accuracy using frame semantics is compared to by using dictionary glossing only these result demonstrate the great potential for future application of bilingual frame semantics to machine translation task 
the idea of nugget pyramid ha recently been introduced a a refinement to the nugget based methodology used to evaluate answer to complex question in the trec qa track this paper examines data from the evaluation the first large scale deployment of the nugget pyramid scheme we show that this method of combining judgment of nugget importance from multiple assessor increase the stability and discriminative power of the evaluation while introducing only a small additional burden in term of manual assessment we also consider an alternative method for combining assessor opinion which yield a distinction similar to microand macro averaging in the context of classification task while the two approach differ in term of underlying assumption their result are nevertheless highly correlated 
the increasing flow of information between language ha led to a rise in the frequency of non native or loan word where term of one language appear transliterated in another dealing with such out of vocabulary word is essential for successful cross lingual information retrieval for example technique such a stemming should not be applied indiscriminately to all word in a collection and so before any stemming foreign word need to be identified in this paper we investigate three approach for the identification of foreign word in arabic text lexicon language pattern and n gram and present that result show that lexicon based approach outperform the other technique 
in this paper we will describe odie the on demand information extraction system given a user s query the system will produce table of the salient information about the topic in structured form it produce the table in le than one minute without any knowledge engineering by hand i e pattern creation or paraphrase knowledge creation which wa the largest obstacle in traditional ie this demonstration is based on the idea and technology reported in sekine a substantial speed up over the previous system which required about minute to analyze one year of newspaper wa achieved through a new approach to handling pattern candidate now le than one minute is required when using year of newspaper corpus in addition functionality wa added to facilitate investigation of the extracted information 
we present an unsupervised learning approach to disambiguate various relation between name entity by use of various lexical and syntactic feature from the context it work by calculating eigen vector of an adjacency graph s laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors this method can address two difficulty encoutered in hasegawa et al s hierarchical clustering no consideration of manifold structure in data and requirement to provide cluster number by user experiment result on ace corpus show that this spectral clustering based approach outperforms hasegawa et al s hierarchical clustering method and a plain k mean clustering method 
statistical mt ha made great progress in the last few year but current translation model are weak on re ordering and target language fluency syntactic approach seek to remedy these problem in this paper we take the framework for acquiring multi level syntactic translation rule of galley et al from aligned tree string pair and present two main extension of their approach first instead of merely computing a single derivation that minimally explains a sentence pair we construct a large number of derivation that include contextually richer rule and account for multiple interpretation of unaligned word second we propose probability estimate and a training procedure for weighting these rule we contrast different approach on real example show that our estimate based on multiple derivation favor phrasal re ordering that are linguistically better motivated and establish that our larger rule provide a bleu point increase over minimal rule 
this study aim at identifying when an event written in text occurs in particular we classify a sentence for an event into four time slot morning daytime evening and night to realize our goal we focus on expression associated with time slot time associated word however listing up all the time associated word is impractical because there are numerous time associated expression we therefore use a semi supervised learning method the na ve bayes classifier backed up with the expectation maximization algorithm in order to iteratively extract time associated word while improving the classifier we also propose to use support vector machine to filter out noisy instance that indicates no specific time period a a result of experiment the proposed method achieved of accuracy and outperformed other method 
this paper introduces a method for the semi automatic generation of grammar test item by applying natural language processing nlp technique based on manually designed pattern sentence gathered from the web are transformed into test on grammaticality the method involves representing test writing knowledge a test pattern acquiring authentic sentence on the web and applying generation strategy to transform sentence into item at runtime sentence are converted into two type of toefl style question multiple choice and error detection we also describe a prototype system fast free assessment of structural test evaluation on a set of generated question indicates that the proposed method performs satisfactory quality our methodology provides a promising approach and offer significant potential for computer assisted language learning and assessment 
we introduce a semi supervised approach to training for statistical machine translation that alternate the traditional expectation maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word alignment quality on a small manually word aligned sub corpus we show that our algorithm lead not only to improved alignment but also to machine translation output of higher quality 
we propose a new hierarchical bayesian n gram model of natural language our model make use of a generalization of the commonly used dirichlet distribution called pitman yor process which produce power law distribution more closely resembling those in natural language we show that an approximation to the hierarchical pitman yor language model recovers the exact formulation of interpolated kneser ney one of the best smoothing method for n gram language model experiment verify that our model give cross entropy result superior to interpolated kneser ney and comparable to modified kneser ney 
recently confusion network decoding ha been applied in machine translation system combination due to error in the hypothesis alignment decoding may result in ungrammatical combination output this paper describes an improved confusion network based method to combine output from multiple mt system in this approach arbitrary feature may be added log linearly into the objective function thus allowing language model expansion and re scoring also a novel method to automatically select the hypothesis which other hypothesis are aligned against is proposed a generic weight tuning algorithm may be used to optimize various automatic evaluation metric including ter bleu and meteor the experiment using the arabic to english and chinese to english nist mt evaluation task show significant improvement in bleu score compared to earlier confusion network decoding based method 
abstract the goal of this task is to allow for comparison across sense induction and discrimination system and also to compare these system to other supervised and knowledgebased system in total there were participating system we reused the semeval english lexical sample subtask of task and set up both clustering style unsupervised evaluation using ontonotes sens a gold standard and a supervised evaluation using the part of the dataset for mapping we provide a comparison to the result of the system participating in the lexical sample subtask of task 
we investigate a family of update method for online machine learning algorithm for cost sensitive multiclass and structured classification problem the update rule are based on multinomial logistic model the most interesting question for such an approach is how to integrate the cost function into the learning paradigm we propose a number of solution to this problem to demonstrate the applicability of the algorithm we evaluated them on a number of classification task related to incremental dependency parsing these task were conventional multiclass classification hiearchical classification and a structured classification task complete labeled dependency tree prediction the performance figure of the logistic algorithm range from slightly lower to slightly higher than margin based online algorithm 
for natural language understanding it is essential to reveal semantic relation between word to date only the is a relation ha been publicly available toward deeper natural language understanding we semi automatically constructed the domain dictionary that represents the domain relation between japanese fundamental word this is the first japanese domain resource that is fully available besides our method doe not require a document collection which is indispensable for keyword extraction technique but is hard to obtain a a task based evaluation we performed blog categorization also we developed a technique for estimating the domain of unknown word 
this paper describes a study of the pattern of translational equivalence exhibited by a variety of bitexts the study found that the complexity of these pattern in every bitext wa higher than suggested in the literature these finding shed new light on why syntactic constraint have not helped to improve statistical translation model including finite state phrase based model tree to string model and tree to tree model the paper also present evidence that inversion transduction grammar cannot generate some translational equivalence relation even in relatively simple real bitexts in syntactically similar language with rigid word order instruction for replicating our experiment are at http nip c nyu edu genpar acl 
this paper proposes a novel approach to the induction of combinatory categorial grammar ccgs by their potential affinity with the genetic algorithm gas specifically ccgs utilize a rich yet compact notation for lexical category which combine with relatively few grammatical rule presumed universal thus the search for a ccg consists in large part in a search for the appropriate category for the data set s lexical item we present and evaluates a system utilizing a simple ga to successively search and improve on such assignment the fitness of categorial assignment is approximated by the coverage of the resulting grammar on the data set itself and candidate solution are updated via the standard ga technique of reproduction crossover and mutation 
we present an efficient algorithm for the redundancy elimination problem given an underspecified semantic representation usr of a scope ambiguity compute an usr with fewer mutually equivalent reading the algorithm operates on underspecified chart representation which are derived from dominance graph it can be applied to the usrs computed by large scale grammar we evaluate the algorithm on a corpus and show that it reduces the degree of ambiguity significantly while taking negligible runtime 
several nlp task are characterized by asymmetric data where one class label none signifying the absence of any structure named entity coreference relation etc dominates all other class classifier built on such data typically have a higher precision and a lower recall and tend to overproduce the none class we present a novel scheme for voting among a committee of classifier that can significantly boost the recall in such situation we demonstrate result showing up to a relative improvement in ace value for the ace relation extraction task for english arabic and chinese 
this paper report the corpus oriented development of a wide coverage japanese hpsg parser we first created an hpsg treebank from the edr corpus by using heuristic conversion rule and then extracted lexical entry from the treebank the grammar developed using this method attained wide coverage that could hardly be obtained by conventional manual development we also trained a statistical parser for the grammar on the treebank and evaluated the parser in term of the accuracy of semantic role identification and dependency analysis 
in this paper we propose guided learning a new learning framework for bidirectional sequence classification the task of learning the order of inference and training the local classifier are dynamically incorporated into a single perceptron like learning algorithm we apply this novel learning algorithm to po tagging it obtains an error rate of on the standard ptb test set which represents relative error reduction over the previous best result on the same data set while using fewer feature 
statistical language model should improve a the size of the n gram increase from to or higher however the number of parameter and calculation and the storage requirement increase very rapidly if we attempt to store all possible combination of n gram to avoid these problem the reduced n gram approach previously developed by o boyle can be applied a reduced n gram language model can store an entire corpus s phrase history length within feasible storage limit another theoretical advantage of reduced n gram is that they are closer to being semantically complete than traditional model which include all n gram in our experiment the reduced n gram zipf curve are first presented and compared with previously obtained conventional n gram for both english and chinese the reduced n gram model is then applied to large english and chinese corpus for english we can reduce the model size compared to gram traditional model size with factor of for a million word corpus and for a million word corpus while obtaining and improvement in perplexity for chinese we gain a perplexity reduction and we reduce the model size by a factor larger than this paper is a step towards the modeling of english and chinese using semantically complete phrase in an n gram model 
this paper proposes method for extracting loanword from cyrillic mongolian corpus and producing a japanese mongolian bilingual dictionary we extract loanword from mongolian corpus using our own handcrafted rule to complement the rule based extraction we also extract word in mongolian corpus that are phonetically similar to japanese katakana word a loanword in addition we correspond the extracted loanword to japanese word and produce a bilingual dictionary we propose a stemming method for mongolian to extract loanword correctly we verify the effectiveness of our method experimentally 
in this paper we describe a rote extractor that learns pattern for finding semantic relationship in unrestricted text with new procedure for pattern generalization and scoring these include the use of part of speech tag to guide the generalization named entity category inside the pattern an edit distance based pattern generalization algorithm and a pattern accuracy calculation procedure based on evaluating the pattern on several test corpus in an evaluation with entity the system attains a precision higher than for half of the relationship considered 
this paper describes a work in progress aiming at linking the two largest italian lexical semantic database italwordnet and parole simple clip the adopted linking methodology the software tool devised and implemented for this purpose and the result of the first mapping phase regarding storderentities are illustrated here 
recognizing idiom in a sentence is important to sentence understanding this paper discus the lexical knowledge of idiom for idiom recognition the challenge are that idiom can be ambiguous between literal and idiomatic meaning and that they can be transformed when expressed in a sentence however there ha been little research on japanese idiom recognition with it ambiguity and transformation taken into account we propose a set of lexical knowledge for idiom recognition we evaluated the knowledge by measuring the performance of an idiom recognizer that exploit the knowledge a a result more than of the idiom in a corpus are recognized with accuracy 
to enable conversational qa it is important to examine key issue addressed in conversational system in the context of question answering in conversational system understanding user intent is critical to the success of interaction recent study have also shown that the capability to automatically identify problematic situation during interaction can significantly improve the system performance therefore this paper investigates the new implication of user intent and problematic situation in the context of question answering our study indicate that in basic interactive qa there are different type of user intent that are tied to different kind of system performance e g problematic error free situation once user are motivated to find specific information related to their information goal the interaction context can provide useful cue for the system to automatically identify problematic situation and user intent 
this paper proposes a named entity recognition ner method for speech recognition result that us confidence on automatic speech recognition asr a a feature the asr confidence feature indicates whether each word ha been correctly recognized the ner model is trained using asr result with named entity ne label a well a the corresponding transcription with ne label in experiment using support vector machine svms and speech data from japanese newspaper article the proposed method outperformed a simple application of text based ner to asr result in ner f measure by improving precision these result show that the proposed method is effective in ner for noisy input 
we present a web mining method for discovering and enhancing relationship in which a specified concept word class participates we discover a whole range of relationship focused on the given concept rather than generic known relationship a in most previous work our method is based on clustering pattern that contain concept word and other word related to them we evaluate the method on three different rich concept and find that in each case the method generates a broad variety of relationship with good precision 
this paper describes a word and phrase alignment approach based on a dependency analysis of french english parallel corpus referred to a alignment by syntax based propagation both corpus are analysed with a deep and robust dependency parser starting with an anchor pair consisting of two word that are translation of one another within aligned sentence the alignment link is propagated to syntactically connected word 
we propose a novel algorithm for inducing semantic taxonomy previous algorithm for taxonomy induction have typically focused on independent classifier for discovering new single relationship based on hand constructed or automatically discovered textual pattern by contrast our algorithm flexibly incorporates evidence from multiple classifier over heterogenous relationship to optimize the entire structure of the taxonomy using knowledge of a word s coordinate term to help in determining it hypernym and vice versa we apply our algorithm on the problem of sense disambiguated noun hyponym acquisition where we combine the prediction of hypernym and coordinate term classifier with the knowledge in a preexisting semantic taxonomy wordnet we add novel synset to wordnet at precision a relative error reduction of over a non joint algorithm using the same component classifier finally we show that a taxonomy built using our algorithm show a relative f score improvement over wordnet on an independent testset of hypernym pair 
this paper describes a parser which generates parse tree with empty element in which trace and filler are co indexed the parser is an unlexicalized pcfg parser which is guaranteed to return the most probable parse the grammar is extracted from a version of the penn treebank which wa automatically annotated with feature in the style of klein and manning the annotation includes gpsg style slash feature which link trace and filler and other feature which improve the general parsing accuracy in an evaluation on the penn treebank marcus et al the parser outperformed other unlexicalized pcfg parser in term of labeled bracketing f score it result for the empty category prediction task and the trace filler co indexation task exceed all previously reported result with and f score respectively 
a good dictionary contains not only many entry and a lot of information concerning each one of them but also adequate mean to reveal the stored information information access depends crucially on the quality of the index we will present here some idea of how a dictionary could be enhanced to support a speaker writer to find the word s he is looking for to this end we suggest to add to an existing electronic resource an index based on the notion of association we will also present preliminary work of how a subset of such association for example topical association can be acquired by filtering a network of lexical co occurrence extracted from a corpus 
in this paper we examine the task of extracting a set of biographic fact about target individual from a collection of web page we automatically annotate training text with positive and negative example of fact extraction and train rote na ve bayes and conditional random field extraction model for fact extraction from individual web page we then propose and evaluate method for fusing the extracted information across document to return a consensus answer a novel cross field bootstrapping method leverage data interdependency to yield improved performance 
word alignment using recency vector based approach ha recently become popular one major advantage of these technique is that unlike other approach they perform well even if the size of the parallel corpus is small this make these algorithm worth studying for language where resource are scarce in this work we studied the performance of two very popular recency vector based approach proposed in fung and mckeown and somers respectively for word alignment in english hindi parallel corpus but performance of the above algorithm wa not found to be satisfactory however subsequent addition of some new constraint improved the performance of the recency vector based alignment technique significantly for the said corpus the present paper discus the new version of the algorithm and it performance in detail 
this paper proposes a statistical tree to tree model for producing translation two main contribution are a follows a method for the extraction of syntactic structure with alignment information from a parallel corpus of translation and use of a discriminative feature based model for prediction of these target language syntactic structure which we call aligned extended projection or aeps an evaluation of the method on translation from german to english show similar performance to the phrase based model of koehn et al 
in this paper we present method for improving the disambiguation of noun phrase np coordination within the framework of a lexicalised history based parsing model a well a reducing noise in the data we look at modelling two main source of information for disambiguation symmetry in conjunct structure and the dependency between conjunct lexical head our change to the baseline model result in an increase in np coordination dependency f score from to which represents a relative reduction in f score error of 
in this paper we present a method that improves japanese dependency parsing by using large scale statistical information it take into account two kind of information not considered in previous statistical machine learning based parsing method information about dependency relation among the case element of a verb and information about co occurrence relation between a verb and it case element this information can be collected from the result of automatic dependency parsing of large scale corpus the result of an experiment in which our method wa used to rerank the result obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the result obtained using the existing method 
this paper present an approach to incrementally generating locative expression it address the issue of combinatorial explosion inherent in the construction of relational context model by a contextually defining the set of object in the context that may function a a landmark and b sequencing the order in which spatial relation are considered using a cognitively motivated hierarchy of relation and visual and discourse salience 
we describe a method for discriminative training of a language model that make use of syntactic feature we follow a reranking approach where a baseline recogniser is used to produce best output for each acoustic input and a second reranking model is then used to choose an utterance from these best list the reranking model make use of syntactic feature together with a parameter estimation method that is based on the perception algorithm we describe experiment on the switchboard speech recognition task the syntactic feature provide an additional reduction in test set error rate beyond the model of roark et al a roark et al b significant at p 
reading proficiency is a fundamental component of language competency however finding topical text at an appropriate reading level for foreign and second language learner is a challenge for teacher this task can be addressed with natural language processing technology to ass reading level existing measure of reading level are not well suited to this task but previous work and our own pilot experiment have shown the benefit of using statistical language model in this paper we also use support vector machine to combine feature from traditional reading level measure statistical language model and other language processing tool to produce a better method of assessing reading level 
we introduce spmt a new class of statistical translation model that use syntactified target language phrase the spmt model outperform a state of the art phrase based baseline model by bleu point on the nist chinese english test corpus and point on a human based quality metric that rank translation on a scale from to 
we present a practical hpsg parser for english an intelligent search engine to retrieve medline abstract that represent biomedical event and an efficient medline search tool helping user to find information about biomedical entity such a gene protein and the interaction between them 
in this work we present a method for named entity recognition ner our method doe not rely on complex linguistic resource and apart from a hand coded system we do not use any language dependent tool the only information we use is automatically extracted from the document without human intervention moreover the method performs well even without the use of the hand coded system the experimental result are very encouraging our approach even outperformed the hand coded system on ner in spanish and it achieved high accuracy in portuguese 
