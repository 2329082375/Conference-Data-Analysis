holographic recurrent network hrns are recurrent network which incorporate associative memory technique for storing seq uential structure hrns can be easily and quickly trained using gradient descent technique to generate sequence of discrete output and trajec tory through continuous space the performance of hrns is found to be superior to that of ordinary recurrent network on these sequence gener ation task 
one way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolution in space and time this paper show how to create a learning managerial hierarchy in which high level manager learn how to set task to their sub manager who in turn learn how to satisfy them sub manager need not initially understand their manager command they simply learn to maximise their reinforcement in the context of the current command we illustrate the system using a simple maze task a the system learns how to get around satisfying command at the multiple level it explores more efficiently than standard flat learning and build a more comprehensive map 
automatic recognition of spoken letter is one of the most challenging task in the field of computer speech recognition the difficulty of the task is due to the acous tic similarity of many of the letter accurate recognition requires the system to perform fine phonetic distinction such a s b v d b v p d v t t v s g c v z v v z m v n and j v k the ability to per form fine phonetic distinction to discriminate among the minimal sound unit of the language is a fundamen tal unsolved problem in computer speech recognition 
recently linsker and mackay and miller have analysed hebbian correlational rule for synaptic development in the visual system and miller ha studied such rule in the case of two population of fibre particularly two eye miller s analysis ha so far assumed that each of the two population ha exactly the same correlational structure relaxing this constraint by considering the effect of small perturbative correlation within and between eye permit study of the stability of the solution we predict circumstance in which qualitative change are seen including the production of binocularly rather than monocularly driven unit 
recent research on reinforcement learning ha focused on algorithmsbased on the principle of dynamic programming dp one of the most promising area of application for these algorithmsis the control of dynamical system and some impressiveresults have been achieved however there are significant gapsbetween practice and theory in particular there are no convergenceproofs for problem with continuous state and action space or for system involving non linear function 
vector quantization is useful for data compression competitive learning which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data vector quantizati on of labelled data for classification ha a different objective to minimize the number of misclassifications and a different algorithm is appropriate we show that a variant of kohonen s lvq algorithm can be seen a a multiclass extension of an algorithm which in a restricted class case can be proven to converge to the bayes optimal classification boundary we compare the performance of the lvq algorithm to that of a modified version having a decreasing window and normalized step size on a ten class vowel classification problem 
a novel method for feature extraction ha been applied to a problem of three dimensionalobject recognition intrator and gold the method is related to recent statistical theory huber friedman and is derived from a biologically motivated computationaltheory bienenstock et al result of an initial study replicating recent psychophysicalexperiments bulthoff and edelman demonstrated the utility of the proposed methodfor feature extraction we describe 
holographic recurrent network hrns are recurrent network which incorporate associative memory technique for storing seq uential structure hrns can be easily and quickly trained using gradient descent technique to generate sequence of discrete output and trajec tory through continuous space the performance of hrns is found to be superior to that of ordinary recurrent network on these sequence gener ation task 
one way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolution in space and time this paper show how to create a learning managerial hierarchy in which high level manager learn how to set task to their sub manager who in turn learn how to satisfy them sub manager need not initially understand their manager command they simply learn to maximise their reinforcement in the context of the current command we illustrate the system using a simple maze task a the system learns how to get around satisfying command at the multiple level it explores more efficiently than standard flat learning and build a more comprehensive map 
automatic recognition of spoken letter is one of the most challenging task in the field of computer speech recognition the difficulty of the task is due to the acous tic similarity of many of the letter accurate recognition requires the system to perform fine phonetic distinction such a s b v d b v p d v t t v s g c v z v v z m v n and j v k the ability to per form fine phonetic distinction to discriminate among the minimal sound unit of the language is a fundamen tal unsolved problem in computer speech recognition 
recently linsker and mackay and miller have analysed hebbian correlational rule for synaptic development in the visual system and miller ha studied such rule in the case of two population of fibre particularly two eye miller s analysis ha so far assumed that each of the two population ha exactly the same correlational structure relaxing this constraint by considering the effect of small perturbative correlation within and between eye permit study of the stability of the solution we predict circumstance in which qualitative change are seen including the production of binocularly rather than monocularly driven unit 
recent research on reinforcement learning ha focused on algorithmsbased on the principle of dynamic programming dp one of the most promising area of application for these algorithmsis the control of dynamical system and some impressiveresults have been achieved however there are significant gapsbetween practice and theory in particular there are no convergenceproofs for problem with continuous state and action space or for system involving non linear function 
vector quantization is useful for data compression competitive learning which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data vector quantizati on of labelled data for classification ha a different objective to minimize the number of misclassifications and a different algorithm is appropriate we show that a variant of kohonen s lvq algorithm can be seen a a multiclass extension of an algorithm which in a restricted class case can be proven to converge to the bayes optimal classification boundary we compare the performance of the lvq algorithm to that of a modified version having a decreasing window and normalized step size on a ten class vowel classification problem 
a novel method for feature extraction ha been applied to a problem of three dimensionalobject recognition intrator and gold the method is related to recent statistical theory huber friedman and is derived from a biologically motivated computationaltheory bienenstock et al result of an initial study replicating recent psychophysicalexperiments bulthoff and edelman demonstrated the utility of the proposed methodfor feature extraction we describe 
the ability to identify and reason about novel aspect of their input would greatly enhance the capability of artificial neural network the extent of the novelty could be used to judge the appropriateness of individual network for the task to be performed the location and shape of novel feature could be employed to identify the unusual component of the input and to choose an appropriate response 
traditional approach to neural coding characterize the encoding of known stimulus in average neural response organism face nearly the opposite task extracting information about an unknown time dependent stimulus from short segment of a spike train here the neural code wa characterized from the point of view of the organism culminating in algorithm for real time stimulus estimation based on a single example of the spike train these method were applied to an identified movement sensitive neuron in the fly visual system such decoding experiment determined the effective noise level and fault tolerance of neural computation and the structure of the decoding algorithm suggested a simple model for real time analog signal processing with spiking neuron 
the inverse kinematics problem for redundant manipulator is ill posed and nonlinear there are two fundamentally different issue which result in the need for some form of regularization the existence of multiple s olution branch global ill posedness and the existence of excess degree of freedom local illposedness for certain class of manipulator learning method applied to input output data generated from the forward function can be used to globally regularize the problem by partitioning the domain of the for ward mapping into a finite set of region over which the inverse problem is well posed local regularization can be accomplished by an appropriate parameterization of the redundancy consistently over each region a a result the i ll posed problem can be transformed into a finite set of well posed problem eachcan then be solved separately to construct approximate direct inverse functi ons 
biological retina extract spatial and temporal feature in an attempt to reduce the complexity of performing visual task we have built and tested a silicon retina which encodes several useful temporal feature found in vertebrate retina the cell in our silicon retina are selective to direction highly sensitive to positive contrast change around an ambient light level and tuned to a particular velocity inhibitory connection in the null direction perform the direction selectivity we desire this silicon retina is on a mm die and consists of a array of photoreceptors 
quot trajectory extension learning quot is a new technique for learningcontrol in robot which assumes that there exists some parameterof the desired trajectory that can be smoothly varied from a regionof easy solvability of the dynamic to a region of desired behaviorwhich may have more difficult dynamic by gradually varying theparameter practice movement remain near the desired path whilea neural network learns to approximate the inverse dynamic forexample the average speed of motion 
this work address three problem with reinforcement learning and adaptiveneuro control non markovian interface between learner and environment on line learning based on system realization vectorvaluedadaptive critic an algorithm is described which is based on systemrealization and on two interacting fully recurrent continually running networkswhich may learn in parallel problem with parallel learning areattacked by adaptive randomness it is also described how 
batch gradient descent deltaw t gammajde dw t converges to a minimumof quadratic form with a time constant no better than max min where min and max are the minimum and maximum eigenvalue of the hessianmatrix of e with respect to w it wa recently shown that adding amomentum term deltaw t gammajde dw t ff deltaw t gamma improves this to pmax min although only in the batch case here we show that secondordermomentum deltaw t gammajde dw t 
in this paper we show that the lvq learning algorithm converges to locally asymptotic stable equilibrium of an ordinary differential equation we show that the learning algorithm performs stochastic approximation convergence of the voronoi vector is guaranteed under the appropriate condition on the underlying statistic of the classification problem we also present a modification to the learning algorithm which we argue result in convergence of the lvq for a larger set of initial condition finally we show that lvq is a general histogram classifier and that it risk converges to the bayesian optimal risk a the appropriate parameter go to infinity with the number of past observation 
this paper deal with a neural network model in which each neuron performs a threshold logic function an important property of the model is that it always converges to a stable state when operating in a serial mode this property is the basis of the potential application of the model such a associative memory device and combinatorial optimization one of the motivation for use of the model for solving hard combinatorial problem is the fact that it can be implemented by optical device and thus operate at a higher speed than conventional electronics the main theme in this work is to investigate the power of the model for solving np hard problem and to understand the relation between speed of operation and the size of a neural network in particular it will be shown that for any np hard problem the existence of a polynomial size network that solves it implies that np co np also for the traveling salesman problem tsp even a polynomial size network that get an e approximate solution doe not exist unless p np the above result are of great practical interest because right now it is possible to build neural network which will operate fast but are limited in the number of neuron they contain 
many auditory theorist consider the temporal adaptation of theauditory nerve a key aspect of speech coding in the auditory periphery experiment with model of auditory localization and pitchperception also suggest temporal adaptation is an important elementof practical auditory processing i have designed fabricated and successfully tested an analog integrated circuit that modelsmany aspect of auditory nerve response including temporal adaptation introductionwe are modeling 
the goal of perception is to extract invariant property of the underlyingworld by computing contrast at edge the retina reduces incidentlight intensity spanning twelve decade to a twentyfold variation in onestroke it solves the dynamic range problem and extract relative reflectivity bringing u a step closer to the goal we have built a contrast sensitive silicon retina that model all major synaptic interaction in theouter plexiform layer of the vertebrate retina using 
the backpropagation algorithm can be used for both recognition and generationof time trajectory when used a a recognizer it ha been shownthat the performance of a network can be greatly improved by addingstructure to the architecture the same is true in trajectory generation in particular a new architecture corresponding to a quot reversed quot tdnn isproposed result show dramatic improvement of performance in the generationof hand written character acombination of tdnn and 
the algorithm presented performs gradient descent on the weight spaceof an artificial neural network ann using a finite difference toapproximate the gradient the method is novel in that it achieves a computationalcomplexity similar to that of node perturbation o n butdoes not require access to the activity of hidden or internal neuron this is possible due to a stochastic relation between perturbation at theweights and the neuron of an ann the algorithm is also similar to 
the focused gamma network is proposed a one of the possible implementation of the gamma neural model the focused gamma network is compared with the focused backpropagation network and tdnn for a time series prediction problem and with adaline in a system identification problem 
i present a modular network architecture and a learning algorithm basedon incremental dynamic programming that allows a single learning agentto learn to solve multiple markovian decision task mdts with significanttransfer of learning across the task i consider a class of mdts called composite task formed by temporally concatenating a number ofsimpler elemental mdts the architecture is trained on a set of compositeand elemental mdts the temporal structure of a composite task is 
we describe a neural network called rulenet that learns explicit symbolic condition action rule in a formal string manipulation domain rulenet discovers functional category over element of the domain and at various point during learning extract rule that operate on these category the rule are then injected back into rulenet and training continues in a process called iterative projection by incorporating rule in this way rulenet exhibit enhanced learning and generalization performance over alternative neural net approach by integrating symbolic rule learning and subsymbolic category learning rulenet ha capability that go beyond a purely symbolic system we show how this architecture can be applied to the problem of case role assignment in natural language processing yielding a novel rule based solution 
abstract we identify the three principle factor affecting the performance oflearning by network with localized unit unit noise sample density and the structure of the target function we then analyze the effect of unit receptive field parameter on these factor and use this analysis to propose a new learning algorithm which dynamically alters receptive field property during learning how receptive field parameter affect neural learning bartlett w mel stephen m omohundro 
animal locomotion pattern are controlled by recurrent neural networkscalled central pattern generator cpgs although a cpg can oscillateautonomously it rhythm and phase must be well coordinated with thestate of the physical system using sensory input in this paper we proposea learning algorithm for synchronizing neural and physical oscillator withspecific phase relationship sensory input connection are modified by thecorrelation between cellular activity and input signal 
a board is described that contains the anna neural network chip anda dsp c digital signal processor the anna analog neural networkarithmetic unit chip performs mixed analog digital processing the combination of anna with the dsp allows high speed end toendexecution of numerous signal processing application includingthe preprocessing the neural net calculation and the postprocessingsteps the anna board evaluates neural network to time faster than the dsp alone 
neuron in area mt of primate visual cortex encode the velocity of moving object we present a model of how mt cell aggregate response from v to form such a velocity representation two different set of unit with local receptive field receive input from motion energy filter one set of unit form estimate of local motion while the second set computes the utility of these estimate output from this second set of unit gate the output from the first set through a gain control mechanism this active process for selecting only a subset of local motion response to integrate into more global response distinguishes our model from previous model of velocity estimation the model yield accurate velocity estimate in synthetic image containing multiple moving target of varying size luminance and spatial frequency profile and deal well with a number of transparency phenomenon 
this paper explores the effect of initial weight selection on feed forward network learning simple function with the back propagation technique we first demonstrate through the use of monte carlo technique that the magnitude of the initial condition vector in weight space is a very significant parameter in convergence time variability in order to further understand this result additional deterministic experiment were performed the result of these experiment demonstrate the extreme sensitivity of back propagation to initial weight configuration 
in a bayesian framework we give a principled account of how domainspecificprior knowledge such a imperfect analytic domain theory can beoptimally incorporated into network of locally tuned unit by choosinga specific architecture and by applying a specific training regimen ourmethod proved successful in overcoming the data deficiency problem ina large scale application to devise a neural control for a hot line rollingmill it achieves in this application significantly higher 
simple second order recurrent network are shown to readily learn small knownregular grammar when trained with positive and negative string example weshow that similar method are appropriate for learning unknown grammar fromexamples of their string the training algorithm is an incremental real time recurrentlearning rtrl method that computes the complete gradient and updatesthe weight at the end of each string after or during training a dynamic clusteringalgorithm extract 
we compare two strategy for training connectionist a well a non connectionist model for statistical pattern recognition the probabilistic strategyis based on the notion that bayesian discrimination i e o ptimal classification is achieved when the classifier learns thea posterioriclass distribution of the random feature vector the differential strategy is based on the notion that the identity of the largest class a posteriori probability of the feature vector is all that is needed to achieve bayesian discrim ination each strategy is directly linked to a family of objective function that can be used in the supervised train ing procedure we prove that the probabilistic strategy linked with error measure objective function such a mean squared error and cross entropy typically used to train classifier necessarily requires larger training set and more complex classifier architecture than those needed to approximate the bayesian discriminant function in contrast we prove that the differential strategy linked with classification figure of merit objective function cfmmono requires the minimum classifier functional complexity and the fewest training example necessary to approximate the bayesian discriminant function with specified precision measured in probability of error we present our proof in the context of a game of chance in which an unfair sided die is tossed repeatedly we show that this rigged game of dice is a paradigm at the root of all statistical pattern recognition task and demonstrate how a simple extension of the concept lead u to a general information theoretic model of sample complexity for statistical pattern recognition 
we present a methodological framework enabling a detailed descriptionof the performance of hopfield like attractor neural network ann in the first two iteration using the bayesian approach wefind that performance is improved when a history based term is includedin the neuron s dynamic a further enhancement of the network s performance is achieved by judiciously choosing the censoredneurons those which become active in a given iteration on the basisof the magnitude of 
although the detection of invariant structure in a given set of input patternsis vital to many recognition task connectionist learning rule tend to focus ondirections of high variance principal component the prediction paradigm isoften used to reconcile this dichotomy here we suggest a more direct approach toinvariant learning based on an anti hebbian learning rule an unsupervised twolayernetwork implementing this method in a competitive setting learns to extractcoherent depth 
best first model merging is a general technique for dynamically choosing the structure of a neural or related architecture while avoiding overfitting it is applicable to both learning and recognition task and often generalizes significantly better than fixed structure w e demonstrate the approach applied to the task of choosing radial basis function for function learning choosing local af fine model for curve and constraint surface modelling and choosing the structure of a balltree or bumptree to maximize efficiency of access 
we analyse the effect of analog noise on the synaptic arithmeticduring multilayer perceptron training by expanding the cost functionto include noise mediated penalty term prediction are madein the light of these calculation which suggest that fault tolerance generalisation ability and learning trajectory should be improvedby such noise injection extensive simulation experiment on twodistinct classification problem substantiate the claim the resultsappear to be perfectly 
the multi state time delay neural network m tdnn integratesa nonlinear time alignment procedure dtw and the highaccuracyphoneme spotting capability of a tdnn into a connectionistspeech recognition system with word level classification anderror backpropagation we present an m tdnn for recognizingcontinuously spelled letter a task characterized by a small buthighly confusable vocabulary our m tdnn achieves word accuracy on speaker dependent independent task 
abstract the probabilistic neural network pnn algorithm represents the likelihood function of a given class a the sum of identical isotropic gaussians in practice pnn is often an excellent pattern classier outperforming other classiers including backpropagation however it is not robust with respect to ane transformation of feature space and this can lead to poor performance on certain data we have derived an extension of pnn called weighted pnn wpnn which compensates for this aw by allowing anisotropic gaussians i e gaussians whose covariance is not a multiple of the identity matrix the covariance is optimized using a genetic algorithm some interesting feature of which are it redundant logarithmic encoding and large population size experimental result validate our claim 
in visual processing the ability to deal with missing and noisy information is crucial occlusion and unreliable feature detector often lead to situation where little or no direct information about feature is available however the available information is usually sufficient to highly constrain the output we discus bayesian technique for extracting class probability given partial data the optimal solution involves integrating over the missing dimension weighted by the local probability density the framework extends naturally to the case of noisy information we show how to obtain closed form approximation to the bayesian solution using gaussian basis function network simulation on a complex task d hand gesture recognition validate the theory when both integration and weighting by input density are used performance decrease gracefully with the number of missing or noisy feature performance is substantially degraded if either step is omitted 
existing metric for the learning performance of feed forward neural network do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the result of the comparison i propose new metric which have the desirable property of being independent of the training epoch limit the efficiency measure the yield of correct network in proportion to the training effort expended the optimal epoch limit provides the greatest efficiency the learning performance is modelled statistically and asymptotic performance is estimated implementation detail may be found in hamey 
three method for improving the performance of gaussian radial basisfunction rbf network were tested on the nettalk task in rbf anew example is classified by computing it euclidean distance to a set ofcenters chosen by unsupervised method the application of supervisedlearning to learn a non euclidean distance metric wa found to reduce theerror rate of rbf network while supervised learning of each center s varianceresulted in inferior performance the best improvement in 
the kbann knowledge based artificial neural network approach us neural network to refine knowledge that can be written in the form of simple propositional rule we extend this idea further by presenting the manncon multivariable artificial neural network control algorithm by which the mathematical equation governing a pid proportional integral derivative controller determine the topology and initial weight of a network which is further trained using backpropagation we apply this method to the task of controlling the outflow and temperature of a water tank producing statistically significant gain in accuracy over both a standard neural network approach and a nonlearning pid controller furthermore using the pid knowledge to initialize the weight of the network produce statistically le variation in test set accuracy when compared to network initialized with small random number 
despite the fact that complex visual scene contain multiple overlapping object peopleperform object recognition with ease and accuracy one operation that facilitates recognitionis an early segmentation process in which feature of object are grouped and labeled accordingto which object they belong current computational system that perform this operation arebased on predefined grouping heuristic we describe a system called magic that learns howto group feature based on a set 
we present a feed forward network architecture for recognizing an unconstrainedhandwritten multi digit string this is an extension of previouswork on recognizing isolated digit in this architecture a single digit recognizeris replicated over the input the output layer of the network iscoupled to a viterbi alignment module that chooses the best interpretationof the input training error are propagated through the viterbi module the novelty in this procedure is that segmentation is 
this paper describes a technique for learning both the number of state and the topology of hidden markov model from example the induction process start with the most specific model consistent with the training data and generalizes by successively merging state both the choice of state to merge and the stopping criterion are guided by the bayesian posterior pro bability we compare our algorithm with the baum welch method of estimating fixed size model and find that it can induce minimal hmms from data in case where fix ed estimation doe not converge or requires redundant parameter to converge 
neural network model have been criticized for their inability to make useofcompositional representation in this paper we describe a series of psychological phenomenon that demonstrate the role of structured representation in cognition these finding suggest that people compare relational representation via a process of structural alignment this process will have to be captured by any model of cognition symbolic or subsymbolic 
we investigate a model in which excitatory neuron have dynamical thresholdswhich display both fatigue and potentiation the fatigue propertyleads to oscillatory behavior it is responsible for the ability of the modelto perform segmentation i e decompose a mixed input into staggeredoscillations of the activity of the cell assembly memory affected byit potentiation is responsible for sustaining these staggered oscillationsafter the input is turned off i e the system 
ionpaul r cooperinstitute for the learning sciencesnorthwestern universityevanston ilcooper il nwu edupeter n prokopowiczinstitute for the learning sciencesnorthwestern universityevanston ilprokopowicz il nwu eduabstractnetwork vision system must make inference from evidential informationacross level of representational abstraction from low level invariant through intermediate scene segment to high level behaviorally relevantobject description this paper show 
you want your neural net algorithm to learn sequence do not just use conventionalgradient descent or approximation thereof in recurrent net time delay net etc instead use your sequence learning algorithm to implement the following method no matter whatyour final goal are train a network to predict it next input from the previous one sinceonly unpredictable input convey new information ignore all predictable input but let allunexpected input plus information about the 
we introduce and demonstrate a bootstrap method for construction of an inverse function for the robot kinematic mapping using only sample configurationspace workspace data unsupervised learning clustering technique are used on pre image neighborhood in order to learn to partition the configuration space into subset over which the kinematic mapping is invertible supervised learning is then used separately on each of the partition to appro ximate the inverse function the ill posed inverse kinematics function is thereby regularized and a global inverse kinematics solution for the wristless puma manipulator is developed 
a peg in hole insertion task is used a an example to illustratethe utility of direct associative reinforcement learning method forlearning control under real world condition of uncertainty andnoise task complexity due to the use of an unchamfered holeand a clearance of le than mm is compounded by the presenceof positional uncertainty of magnitude exceeding to time theclearance despite this extreme degree of uncertainty our resultsindicate that direct 
the planar thallium tl myocardial perfusion scintigram is a widely used diagnostic technique for detecting and estimating the risk of coronary artery disease interpretation is currently based on visual scoring of myocardial defect combined with image quantitation and is known to have a significant subjective component neural network learned to interpret thallium scintigrams a determined by both individual and multiple consensus expert rating four different type of network were explored single layer two layer backpropagation bp bp with weight smoothing and two layer radial basis function rbf the rbf network wa found to yield the best performance generalization by region and compare favorably with human expert we conclude that this network is a valuable clinical tool that can be used a a reference diagnostic support system to help reduce inter and intraobserver variability this system is now being further developed to include other variable that are expected to improve the final clinical diagnosis 
previously we have introduced the idea of neural network transfer where learning on a target problem is sped up by using the weightsobtained from a network trained for a related source task here we present a new algorithm called discriminability based transfer dbt which us an information measure to estimate the utilityof hyperplanes defined by source weight in the target network and rescales transferred weight magnitude accordingly severalexperiments demonstrate that target 
although considerable interest ha been shown in language inferenceand automaton induction using recurrent neural network success ofthese model ha mostly been limited to regular language we havepreviously demonstrated that neural network pushdown automaton nnpda model is capable of learning deterministic context free language e g anbnand parenthesis language from example however the learning task is computationally intensive in this paper wediscuss some way in 
we introduce a framework for training architecture composed of several module this framework which us a statistical formulation of learning system provides a unique formalism for describing many classical connectionist algorithm a well a complex system where several algorithm interact it allows to design hybrid system which combine the advantage of connectionist algorithm a well a other learning algorithm 
the self organization of recurrent feature discovery network isstudied from the perspective of dynamical system bifurcationtheory reveals parameter regime in which multiple equilibrium orlimit cycle coexist with the equilibrium at which the networksperform principal component analysis introductionoja made the remarkable observation that a simple model neuron with anhebbian adaptation rule develops into a filter for the first principal component ofthe input distribution 
abstract inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl we have designed a chip that employ a correlation model to report the one dimensional field motion of a scene in real time using subthreshold analog vlsi technique we have fabricated and successfully tested a transistor chip using a standard mosis process 
neural network with binary weight are very important from both the theoretical and practical point of view in this paper we investigate the learnability of single binary perceptrons and union of binary perceptron network i e an or of binary perceptrons where each input unit is connected to one and only one perceptron we give a polynomial time algorithm that pac learns these network under the uniform distribution the algorithm is able to identify both the network connectivity and the weight value necessary to represent the target function these result suggest that under reasonable distribution perceptron network may be easier to learn than fully connected network 
standard value function approach to finding policy for partially observablemarkov decision process pomdps are intractable for large model the intractabilityof these algorithm is due to a great extent to their generating an optimalpolicy over the entire belief space however in real pomdp problem most beliefstates are unlikely and there is a structured low dimensional manifold of plausiblebeliefs embedded in the high dimensional belief space 
abstract we present the information theoretic derivation of a learn ing algorithm that cluster unlabelled data with linear discriminants i n contrast to method that try to preserve information about the input pat tern we maximize the information gained from observing the output of robust binary discriminator implemented with sigmoid node we derive a local weight adaptation rule via gradient ascent in this objectiv e demonstrate it dynamic on some simple data set relate our approach to previous work and suggest direction in which it may be extended 
we use connectionist modeling to develop an analysis of stress system in term of ease of learnability in traditional linguistic analysis learnability argument determine default parameter setting based on the feasibilty of logically deducing correct setting from an initial state our approach provides an empirical alternative to such argument based on perceptron learning experiment using data from nineteen human language we develop a novel characterization of stress pattern in term of six parameter these provide both a partial description of the stress pattern itself and a prediction of it learnability without invoking abstract theoretical construct such a metrical foot this work demonstrates that machine learning method can provide a fresh approach to understanding linguistic phenomenon 
a computer model of the musculoskeletal system of the lobstergastric mill wa constructed in order to provide a behavioral interpretationof the rhythmic pattern obtained from isolated stomatogastricganglion the model wa based on hill s muscle modeland quasi static approximation of the skeletal dynamic and couldsimulate the change of chewing pattern by the effect of neuromodulators the stomatogastric nervous systemthe crustacean stomatogastric ganglion stg is a circuit of 
abstract the bayesian model comparison framework is reviewed and the bayesianoccam s razor is explained this framework can be applied to feedforwardnetworks making possible objective comparison between solutionsusing alternative network architecture objective choice of magnitudeand type of weight decay term quantied estimate of the error barson network parameter and on network output the framework also generatesa measure of the eective number of parameter 
abstract recurrent cascade correlation rcc is a recurrent versio n of the cascade correlation learning architecture of fahlman and lebiere fahlman rcc can learn from example to map a sequence of input into a desired sequence of output new hidden unit with recurrent connection are added to the network one at a time a they are needed during training in effect the n etwork build up a finite state machine tailored specifically for the current problem rcc retains the advantage of cascade correlation fast learning good generalization automatic construction of a near min imal multi layered network and the ability to learn complex behavior through a sequence of simple lesson the power of rcc is demonstrated on two task learning a finite state grammar from example of legal string and learning to recognize character in morse code this research wa sponsored in part by the national science foundation contract eet the view and conclusion 
cascade correlation is a new architecture and supervised l earning algorithm for artificial neural network instead of just adjusting the weight in a network of fixed topology cascade correlation begin with a minimal network then automatically train and add new hidden unit one by one creating a multi layer structure once a new hidden unit ha been added to the network it input side weight are frozen this unit then becomes a permanent feature detector in the network available for producing output or for creating other more complex feature detector the cascade correlation architecture ha several advantage over existing algorithm it learns very quickly the network de termines it own size and topology it retains the structure it ha built even if the training set change and it requires no back propagation of error signal through the connection of the network 
cascade correlation is a new architecture and supervised learning algorithm for artificial neural network instead of just adjusting the weight in a network of fixed topology cascade correlation begin with aminimal network then automatically train and add new hidden unit one by one creating a multi layerstructure once a new hidden unit ha been added to the network it input side weight are frozen this unitthen becomes a permanent feature detector in the network available for 
the output of a typical multi output classification network do not satisfy theaxioms of probability probability should be positive and sum to one this problem canbe solved by treating the trained network a a preprocessor that produce a feature vectorthat can be further processed for instance by classical statistical estimation technique we find that in case of interest neural network are and should be somewhat underdetermined because the training data is always 
platt s resource allocation network ran platt a b is modified for a reinforcement learning paradigm and to quot restart quot existing hidden unit rather than adding new unit after restarting unit continue to learn via back propagation the resultingrestart algorithm is tested in a q learning network that learns tosolve an inverted pendulum problem solution are found faster onaverage with the restart algorithm than without it introductionthe goal of supervised learning is 
large vc dimension classifier can learn difficult task but are usuallyimpractical because they generalize well only if they are trained with hugequantities of data in this paper we show that even very high order polynomialclassifiers can be trained with a small amount of training data andyet generalize better than classifier with a smaller vc dimension thisis achieved with a maximum margin algorithm the generalized portrait 
we demonstrate in this paper how certain form of rule based knowledge can be used to prestructure a neural network of normalized basis function and give a probabilistic interpretation of the network architecture we describe several way to assure that rule based knowledge is preserved during training and present a method for complexity reduction that try to minimize the number of rule and the number of conjuncts after training the re ned rule are extracted and analyzed 
learning an input output mapping from a set of example of the type that many neural network have been constructed to perform can be regarded a synthesizing an approximation of a multi dimensional function from this point of view this form of learning is closely related to regularization theory the theory developed in poggio and girosi show the equivalence between regularization and a class of threelayer network that we call regularization network or hyper basis function 
it is known from biological data that the response pattern of interneurons in the olfactory macroglomerulus mgc of insect are of central importance for the coding of the olfactory signal we propose an analytically tractable model of the mgc which allows u to relate the distribution of response pattern to the architecture of the network 
in a new incremental cascade network architecture ha beenpresented this paper discus the property of such cascadenetworks and investigates their generalization ability under theparticular constraint of small data set the evaluation is done forcascade network consisting of local linear map using the mackeyglasstime series prediction task a a benchmark our result indicatethat to bring the potential of large network to bear on theproblem of extracting information from 
a recognition system is reported which recognizes name spelled over thetelephone with brief pause between letter the system us separateneural network to locate segment boundary and classify letter theletter score are then used to search a database of name to find the bestscoring name the speaker independent classification rate for spoken lettersis the system retrieves the correct name spelled with pausesbetween letter of the time from a database of name 
we investigate the use of information from all second order derivative of the error function to perform network pruning i e removing unimportant weight from a trained network in order to improve generalization and increase the speed of further training our method optimal brain surgeon ob is significantly better than magnitude based method which can often remove the wrong weight ob also represents a major improvement over other method such a optimal brain damage le cun denker and solla because ours us the full off diagonal information of the hessian matrix h crucial to ob is a recursion relation for calculating h from training data and structural information of the net we illustrate ob on standard benchmark problem the monk s problem the most successful method in a recent competition in machine learning thrun et al wa backpropagation using weight decay which yielded a network with weight for one monk s problem ob requires only weight for the same performance accuracy on two other monk s problem our method required only and of the weight found by magnitude based pruning 
a new class of data structure called bumptrees is described these structure are useful for ef ficiently implementing a number of neural network related operation an empirical comparison with radial basis function is presented on a robot arm mapping learning task application to density estimation classification and constraint representation and learning are also outlined what is a bumptree a bumptree is a new geometric data structure which is useful for efficiently learning representing and evaluating geometric relationship in a variety of context they are a natural generalization of several hierarchical geometric data structure including oct tree k d tree balltrees and boxtrees they are useful for many geometric learning task including approximating function constraint surface classification region and probability density from sample in the function approximation case the approach is related to radial basis function neural network but support faster construction faster access and more flexible modification we provide empirical data comparing bumptrees with radial basis function in section 
previous work m i sereno cf m e sereno showed that a feedforward network with area v like input layer unit and a hebb rule can develop area mt like second layer unit that solve the aperture problem for pattern motion the present study extends this earlier work to more complex motion saito et al showed that neuron with large receptive field in macaque visual area mst are sensitive to different sens of rotation and dilation irrespective of the receptive field location of the movement singularity a network with an mt like second layer wa trained and tested on combination of rotating dilating and translating pattern third layer unit learn to detect specific sens of rotation or dilation in a position independent fashion despite having position dependent direction selectivity within their receptive field 
a performance comparison of two self organizing network the kohonenfeature map and the recently proposed growing cell structuresis made for this purpose several performance criterion forself organizing network are proposed and motivated the modelsare tested with three example problem of increasing difficulty thekohonen feature map demonstrates slightly superior result onlyfor the simplest problem for the other more difficult and also morerealistic problem the growing cell 
given a set of training example determining the appropriate numberof free parameter is a challenging problem constructivelearning algorithm attempt to solve this problem automatically byadding hidden unit and therefore free parameter during learning we explore an alternative class of algorithm called metamorphosis algorithm in which the number of unit is fixed butthe number of free parameter gradually increase during learning the architecture we investigate is composed 
abstract how can artificial neural net generalize better from fewer example in order to generalize successfully neural network learning method typically require large training data set we introduce a neural network learning method that generalizes rationally from many fewer data point relying instead on prior knowledge encoded in previously learned neural network for example in robot control learning task reported here previously learned network that model the effect of robot action are used to guide subsequent learning of robot control function for each observed training example of the target function e g the robot control policy the learner explains the observed example in term of it prior knowledge then analyzes this explanation to infer additional information about the shape or slope of the target function this shape knowledge is used to bias generalization in the learned target function result are presented applying this approach to a simulated robot task based on reinforcement learning 
in stochastic learning weight are random variable whose timeevolution is governed by a markov process at each time step n the weight can be described by a probability density functionp n we summarize the theory of the time evolution of p andgive graphical example of the time evolution that contrast thebehavior of stochastic learning with true gradient descent batchlearning finally we use the formalism to obtain prediction of thetime required for noise induced 
the real time computation of motion from real imagesusing a single chip with integrated sensor is a hard problem we present two analog vlsi scheme that use pulsedomain neuromorphic circuit to compute motion pulsesof variable width rather than graded potential representa natural medium for evaluating temporal relationship both algorithm measure speed by timing a moving edgein the image our first model is inspired by reichardt salgorithm in the fly and yield a non monotonic 
