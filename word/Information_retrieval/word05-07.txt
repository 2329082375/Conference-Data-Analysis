pagerank ha been known to be a successful algorithm in ranking web source in order to avoid the rank sink problem pagerank assumes that a surfer being in a page jump to a random page with a certain probability in the standard pagerank algorithm the jumping probability are assumed to be the same for all the page regardless of the page property this is not the case in the real world since presumably a surfer would more likely follow the out link of a high quality hub page than follow the link of a low quality one in this poster we propose a novel algorithm dirichlet pagerank to address this problem by adapting exible jumping probability based on the number of out link in a page empirical result on trec data show that our method outperforms the standard pagerank algorithm 
much interesting text on the web consists largely of opinionated or evaluative text a opposed to directly informative text the new eld of sentiment analysis seek to characterize such aspect of natural language text a opposed to just the bare fact we suggest that appraisal expression extraction should be viewed a a fundamental task for sentiment analysis we dene an appraisal expression to be a piece of text expressing some evaluative stance towards a particular object the task is to nd these element and characterize the type and orientation positive or negative of the evaluative stance a well a it target and possibly it source potential application of these method include new approach to the now traditional task of sentiment classication and opinion mining a well a possibly for adversarial textual analysis and intention detection for intelligence application 
user searching for information in hypermedia environment often perform querying followed by manual navigation yet the conventional text hypertext retrieval paradigm doe not explicity take post query navigation into account this paper proposes a new retrieval paradigm called navigation aided retrieval nar which treat both querying and navigation a first class activity in the nar paradigm querying is seen a a mean to identify starting point for navigation and navigation is guided based on information supplied in the query nar is a generalization of the conventional probabilistic information retrieval paradigm which implicitly assumes no navigation take place this paper present a formal model for navigation aided retrieval and report empirical result that point to the real world applicability of the model the experiment were performed over a large web corpus provided by trec using human judgment on a new rating scale developed for navigation aided retrieval in the case of ambiguous query the new retrieval model identifies good starting point for post query navigation for le ambiguous query that need not be paired with navigation the output closely match that of a conventional retrieval system 
richard granger will be providing an update on the deployment of information technology at a national scale in the nh in england particular topic that will be covered include variability of performance and user organization and supplier access channel strategy for nh user and member of the public take up rate for new technology including internet adoption data on number of user and transaction to date will also be provided 
this acceptance talk is a curious mixture of personal history and developing idea in the context of the growing field of ir covering several decade i want to concentrate on model and theory interpreted loosely and try and give an insight into where i have got to in my thinking where the idea came from and where i believe we are going in the last few year i have been working on the development of what might be coined a a design language for ir it take it inspiration from quantum mechanic but by analogy only the mathematical object represent document these object might be vector or density operator in an n dimensional vector space usually a hilbert space a request for information or a query is taken a an observable and is represented a a linear operator on the space linear operator can be expressed a matrix such an operator hermitian ha a set of eigenvectors forming a basis for the space which we interpret a a point of view or perspective from which to understand the space thus any document vector can be located with respect to the basis and we can calculate an inner product between such a vector and any basis vector which may be interpreted a a probability of relevance the probability of observing any given eigenvector is now given by the square of that inner product assuming all vector are normalised hence we connect the probability of observation to the geometry of the space furthermore the subspace of the space make up a lattice structure which is equivalent to a logic this make up the entire mathematical structure and the language for handling this structure is linear algebra vector matrix projection inner product neatly captured by the dirac notation used in quantum mechanic our probability is slightly different from classical probability the same for logic we end up with quantum logic and quantum probability a commitment to this kind of mathematical structure with which to model object and process in ir depends on two critical assumption the distance in the space between object are a source of important relationship with respect to relevance and aboutness the observation of a property such a relevance or aboutness is user dependent in the sense that a potential interaction is specified by a user through an operator which when measured achieves outcome with a probability determined by the geometry of the space the geometry of this mathematical structure and the probability defined on it are closely connected by the following theorem due to gleason one may summarise this theorem by saying that the probability of a subspace is given by a simple algorithm derived from a projection onto the subspace and a special kind of operator namely a statistical operator or density matrix and conversely that given a probability measure on the subspace then we can encode that measure uniquely through such an algorithm this is a very powerful theorem and it consequence remain to be explored so how did i get to this point and form of abstraction most of my research work can be divided into contribution to the following area clusteringevaluationprobabilistic modelslogic modelsgeometry in all these area i have attempted to search for underlying mathematical structure that would lead to computation these topic have in common that they depend on the construction of measure on a space which in some sense determines the usefulness or effectiveness of the structure for clustering one considers mapping from metric space to ultrametic space and measure the closeness of fit in the case of evaluation one start with a relational conjoint structure and imposes some constraint given by what is to be measured one then construct a numerical representation of this structure leading to such measure a f or e for probabilistic model the main difficulty is concerned with deciding on an appropriate event space on which to define the right probability measure for me the most significant example in this context wa the attempt to construct a logical uncertainty principle which formulated a measure of uncertainty on incomplete logical construct this attempt left unspecified the exact form of the measure in the geometry of ir i finally managed to formulate that measure a a projection valued measure this way of thinking did not appear out of nowhere it wa heavily influenced by the work of fairthorne whose work on brouwerian logic an intuitionistic logic wa picked up by salton in his early book on ir at an earlier stage mackay wrote a paper that opened with this paper relates to the borderline linking experimental and theoretical physic with mathematical logic and cover at several point ground which is common to the theory of communication he go on to define an information operator which is very similar in scope and intent to the hermitian operator above maron who collaborated with mackay stated in his paper therefore it can be argued that index description should not be viewed a property of document they function to relate document and user one can see that the development of these early idea wa continued to the construction of the geometry of ir what doe it leave to be done an attempt should be made to use this design language to build an ir system on the theoretical front it is worth considering whether it would be better to start with a transition probability space rather than a hilbert space a von neumann did in translated in the assumption that closed linear subspace will be the element of our logic can be challenged a perhaps a construction with different element is possible it is not obvious what the best form of conditional probability might be in these space agreeing on a form of conditionalisation is intimately tied up with how to model contextuality there is some evidence to suggest that contextuality play a role in modelling the conjuncton of concept widdows such context have been modelled in quantum theory almost from the beginning for example gleason s theorem precludes noncontextual hidden variable theory 
in the last few year internet user have seen the rapid expansion of phishing the use of spoofed e mail and fraudulent website designed to trick user into divulging sensitive data more recently we have seen the growth of pharming the use of malware or dns based attack to misdirect user to rogue website in this panel we will examine the state of the art in anti phishing solution and explore promising direction for future research 
we propose a new photo search method that us three dimensional d viewpoint a query d viewpoint based image retrieval is especially useful for searching collection of archaeological photograph which contain many different image of the same object our method is designed to enable user to retrieve image that contain the same object but show a different view and to browse group of image taken from a similar viewpoint we also propose using d scene to query by example which mean that user do not have the problem of trying to formulate appropriate query this combination give user an easy way of accessing not only photograph but also archived information 
similarity measure for text have historically been an important tool for solving information retrieval problem in many interesting setting however document are often closely connected to other document a well a other non textual object for instance email message are connected to other message via header information in this paper we consider extended similarity metric for document and other object embedded in graph facilitated via a lazy graph walk we provide a detailed instantiation of this framework for email data where content social network and a timeline are integrated in a structural graph the suggested framework is evaluated for two email related problem disambiguating name in email document and threading we show that reranking scheme based on the graph walk similarity measure often outperform baseline method and that further improvement can be obtained by use of appropriate learning method 
given a large online network of online auction user and their history of transaction how can we spot anomaly and auction fraud this paper describes the design and implementation of netprobe a system that we propose for solving this problem netprobe model auction user and transaction a a markov random field tuned to detect the suspicious pattern that fraudsters create and employ a belief propagation mechanism to detect likely fraudsters our experiment show that netprobe is both efficient and effective for fraud detection we report experiment on synthetic graph with a many a node and edge where netprobe wa able to spot fraudulent node with over precision and recall within a matter of second we also report experiment on a real dataset crawled from ebay with nearly transaction between more than user where netprobe wa highly effective at unearthing hidden network of fraudsters within a realistic response time of about minute for scenario where the underlying data is dynamic in nature we propose incrementalnetprobe which is an approximate but fast variant of netprobe our experiment prove that incremental netprobe executes nearly doubly fast a compared to netprobe while retaining over of it accuracy 
recently a number of trec track have adopted a retrieval effectiveness metric called bpref which ha been designed for evaluation environment with incomplete relevance data a graded relevance version of this metric called rpref ha also been proposed however we show that the application of q measure normalised discounted cumulative gain ndcg or average precision avep to condensed list obtained by ltering out all unjudged document from the original ranked list is actually a better solution to the incompleteness problem than bpref furthermore we show that the use of graded relevance boost the robustness of ir evaluation to incompleteness and therefore that q measure and ndcg based on condensed list are the best choice to this end we use four graded relevance test collection from ntcir to compare ten different ir metric in term of system ranking stability and pairwise discriminative power 
we introduce the notion of query substitution that is generating a new query to replace a user s original search query our technique us modications based on typical substitution web searcher make to their query in this way the new query is strongly related to the original query containing term closely related to all of the original term this contrast with query expansion through pseudo relevance feedback which is costly and can lead to query drift this also contrast with query relaxation through boolean or tfidf retrieval which reduces the specicit y of the query we dene a scale for evaluating query substitution and show that our method performs well at generating new query related to the original query we build a model for selecting between candidate by using a number of feature relating the query candidate pair and by tting the model to human judgment of relevance of query suggestion this further improves the quality of the candidate generated experiment show that our technique signican tly increase coverage and eectiv ene in the setting of sponsored search 
the task of finding people who are expert on a topic ha recently received increased attention we introduce a different expert finding task for which a small number of example expert is given instead of a natural language query and the system s task is to return similar expert we define compare and evaluate a number of way of representing expert and investigate how the size of the initial example set affect performance we show that more finegrained representation of candidate result in higher performance and larger sample set a input lead to improved precision 
in this paper we present a novel method for the classification of web site this method exploit both structure and content of web site in order to discern their functionality it allows for distinguishing between eight of the most relevant functional class of web site we show that a pre classification of web site utilizing structural property considerably improves a subsequent textual classification with standard technique we evaluate this approach on a dataset comprising more than web site with about million crawled and million known web page our approach achieves an accuracy of for the coarse grained classification of these web site category and subject descriptor h information storage and retrieval information search and retrieval information filtering selection process h database management database application data mining 
recently manning et al resorted the permuterm indexof garfield a a time efficient and elegant solution to the string dictionary problem in which pattern query may possibly include one wild card symbol called tolerant retrieval problem unfortunately the permuterm index is space inefficient because it quadruple the dictionary size in this paper we propose the compressed permuterm index which solves the tolerant retrieval problem in optimal query time i e time proportional to the length of the searched pattern and space close to the k th order empirical entropy of the indexed dictionary our index can be used to solve also more sophisticated query which involve several wild card symbol or require to prefix match multiple field in a database of record the result is based on an elegant variant of the burrow wheeler transform defined on a dictionary of string of variable length which allows to easily adapt known compressed index makinen navarro to solve the tolerant retrieval problem experiment show that our index support fast query within a space occupancy that is close to the one achievable by compressing the string dictionary via gzip bzip or ppmdi this improves known approach based on front coding by more than in absolute space occupancy still guaranteeing comparable query time 
a an increasingly large number of owl ontology become available on the semantic web and the description in the ontology become more complicated finding the cause of error becomes an extremely hard task even for expert existing ontology development environment provide some limited support in conjunction with a reasoner for detecting and diagnosing error in owl ontology typically these are restricted to the mere detection of for example unsatisfiable concept we have integrated a number of simple debugging cue generated from our description logic reasoner pellet in our hypertextual ontology development environment swoop these cue in conjunction with extensive undo redo and annotea based collaboration support in swoop significantly improve the owl debugging experience and point the way to more general improvement in the presentation of an ontology to new user 
we describe a task based evaluation to determine whether multi document summary measurably improve user performance whe using online news browsing system for directed research we evaluated the multi document summary generated by newsblaster a robust news browsing system that cluster online news article and summarizes multiple article on each event four group of subject were asked to perform the same time restricted fact gathering task reading news under different condition no summary at all single sentence summary drawn from one of the article newsblaster multi document summary and human summary our result show that in comparison to source document only the quality of report assembled using newsblaster summary wa significantly better and user satisfaction wa higher with both newsblaster and human summary 
this paper proposes a random web crawl model a web crawl is a biased and partial image of the web this paper deal with the hyperlink structure i e a web crawl is a graph whose vertex are the page and whose edge are the hypertextual link of course a web crawl ha a very special structure we recall some known result about it we then propose a model generating similar structure our model simply simulates a crawling i e build and crawl the graph at the same time the graph generated have lot of known property of web crawl our model is simpler than most random web graph model but capture the same property notice that it model the crawling process instead of the page writing process of web graph model 
it is now a common practice for e commerce web site to enable their customer to write review of product that they have purchased such review provide valuable source of information on these product they are used by potential customer to find opinion of existing user before deciding to purchase a product they are also used by product manufacturer to identify problem of their product and to find competitive intelligence information about their competitor unfortunately this importance of review also give good incentive for spam which contains false positive or malicious negative opinion in this paper we make an attempt to study review spam and spam detection to the best of our knowledge there is still no reported study on this problem 
in strategic management there ha been a debate over many year already in alfred chandler had stated structure follows strategy in the nineteen eighty michael porter modified chandler s dictum about structure following strategy by introducing a second level of structure organizational structure follows strategy which in turn follows structure so the question became what is leading what technology ha in this debate been seen a a part of either the structure of the organisation itself or part of the development of the environment in which the organisation try to survive by adapting the notion that technological advancement can also change the paradigmas of organisational strategy development is new this ha mainly to do with the impact of the technological change on the workflow and procedure of organisation never before they were so profound a in our day technological change affect u on different level of our strategic development i will give three example of change that are occurring or have occurred in sound and vision the first is the introduction of rfid transmitter in admission ring for the sound and vision experience the second is the setup of a back office medium asset management storage and distribution structure for the public broadcaster the third is the development of the archive towards becoming a medium application service provider 
path expression are the principal mean of locating data in a hierarchical model but path expression are brittle because they often depend on the structure of data and break if the data is structured differently the structure of data could be unfamiliar to a user may differ within a data collection or may change over time a the schema evolves this paper proposes a novel construct that locates related node in an instance of an xml data model independent of a specific structure it can augment many xpath expression and can be seamlessly incorporated in xquery or xslt 
this paper is concerned with rank aggregation the task of combining the ranking result of individual ranker at meta search previously rank aggregation wa performed mainly by mean of unsupervised learning to further enhance ranking accuracy we propose employing supervised learning to perform the task using labeled data we refer to the approach a supervised rank aggregation we set up a general framework for conducting supervised rank aggregation in which learning is formalized an optimization which minimizes disagreement between ranking result and the labeled data a case study we focus on markov chain based rank aggregation in this paper the optimization for markov chain based method is not a convex optimization problem however and thus is hard to solve we prove that we can transform the optimization problem into that of semidefinite programming and solve it efficiently experimental result on meta search show that supervised rank aggregation can significantly outperform existing unsupervised method 
namespaces are a central building block of xml technology today they provide the identification mechanism for many xml related vocabulary despite their ubiquity there is no established mechanism for describing namespaces and in particular for describing the dependency of namespaces we propose a simple model for describing namespaces and their dependency using these description it is possible to compile directory of namespaces providing searchable and browsable namespace description 
abstract this paper focus on a method for the stylistic segmentation of text document our technique involves mapping the change in a feature throughout a text we use the linguistic feature of conjunction and modality through taxonomy from systemic functional linguistics this segmentation ha application in automated summarization particularly of large document category and subject descriptor 
contextual search refers to proactively capturing the information need of a user by automatically augmenting the user query with information extracted from the search context for example by using term from the web page the user is currently browsing or a file the user is currently editing we present three different algorithm to implement contextual search for the web the first query rewriting qr augments each query with appropriate term from the search context and us an off the shelf web search engine to answer this augmented query the second rank biasing rb generates a representation of the context and answer query using a custom built search engine that exploit this representation the third iterative filtering meta search ifm generates multiple subqueries based on the user query and appropriate term from the search context us an off theshelf search engine to answer these subqueries and re rank the result of the subqueries using rank aggregation method we extensively evaluate the three method using context and over human relevance judgment of search result we show that while qr work surprisingly well the relevance and recall can be improved using rb and substantially more using ifm thus qr rb and ifm represent a cost effective design spectrum for contextual search 
content targeted advertising the task of automatically associating ad to a web page constitutes a key web monetization strategy nowadays further it introduces new challenging technical problem and raise interesting question for instance how to design ranking function able to satisfy conflicting goal such a selecting advertisement ad that are relevant to the user and suitable and profitable to the publisher and advertiser in this paper we propose a new framework for associating ad with web page based on genetic programming gp our gp method aim at learning function that select the most appropriate ad given the content of a web page these ranking function are designed to optimize overall precision and minimize the number of misplacement by using a real ad collection and web page from a newspaper we obtained a gain over a stateof the art baseline method of in average precision further by evolving individual to provide good ranking estimation gp wa able to discover ranking function that are very eective in placing ad in web page while avoiding irrelevant one 
we propose a simple method for converting many standard measure of retrieval performance into metasearch algorithm our focus is both on the analysis of retrieval measure themselves and on the development of new metasearch algorithm given the conversion method proposed our experimental result using trec data indicate that system oriented measure of overall retrieval performance such a average precision yield good metasearch algorithm whose performance equal or exceeds that of benchmark technique such a combmnz and condorcet 
we present a language for specifying web service interface a web service interface put three kind of constraint on the user of the service first the interface specifies the method that can be called by a client together with type of input and output parameter these are called signature constraint second the interface may specify propositional constraint on method call and output value that may occur in a web service conversation these are called consistency constraint third the interface may specify temporal constraint on the ordering of method call these are called protocol constraint the interface can be used to check first if two or more web service are compatible and second if a web service a can be safely substituted for a web service b the algorithm for compatibility checking verifies that two or more interface fulfill each others constraint the algorithm for substitutivity checking verifies that service a demand fewer and fulfills more constraint than service b 
the primary function of current web search engine is essentially relevance ranking at the document level however myriad structured information about real world object is embedded in static web page and online web database document level information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering object oriented query in this paper we propose a paradigm shift to enable searching at the object level in traditional information retrieval model document are taken a the retrieval unit and the content of a document is considered reliable however this reliability assumption is no longer valid in the object retrieval context when multiple copy of information about the same object typically exist these copy may be inconsistent because of diversity of web site quality and the limited performance of current information extraction technique if we simply combine the noisy and inaccurate attribute information extracted from different source we may not be able to achieve satisfactory retrieval performance in this paper we propose several language model for web object retrieval namely an unstructured object retrieval model a structured object retrieval model and a hybrid model with both structured and unstructured retrieval feature we test these model on a paper search engine and compare their performance we conclude that the hybrid model is the superior by taking into account the extraction error at varying level 
information graphic are non pictorial graphic such a bar chart and line graph that depict attribute of entity and relation among entity most information graphic appearing in popular medium have a communicative goal or intended message consequently information graphic constitute a form of language this paper argues that information graphic are a valuable knowledge resource that should be retrievable from a digital library and that such graphic should be taken into account when summarizing a multimodal document for subsequent indexing and retrieval but to accomplish this the information graphic must be understood and it message recognized the paper present our bayesian system for recognizing the primary message of one kind of information graphic simple bar chart and discus the potential role of an information graphic s message in indexing graphic and summarizing multimodal document 
existing retrieval model generally do not offer any guarantee for optimal retrieval performance indeed it is even difficult if not impossible to predict a model s empirical performance analytically this limitation is at least partly caused by the way existing retrieval model are developed where relevance is only coarsely modeled at the level of document and query a opposed to a finer granularity level of term in this paper we present a new axiomatic approach to developing retrieval model based on direct modeling of relevance with formalized retrieval constraint defined at the level of term the basic idea of this axiomatic approach is to search in a space of candidate retrieval function for one that can satisfy a set of reasonable retrieval constraint to constrain the search space we propose to define a retrieval function inductively and decompose a retrieval function into three component function inspired by the analysis of the existing retrieval function with the inductive definition we derive several new retrieval function using the axiomatic retrieval framework experiment result show that the derived new retrieval function are more robust and le sensitive to parameter setting than the existing retrieval function with comparable optimal performance 
federated search is the task of retrieving relevant document from different information resource one of the main research problem in federated search is to combine the result from different source into a single ranked list recent work proposed a regression based method to download some document from each ranked list of the different source calculated comparable score for the document and estimated mapping function that transform source specific score into comparable score experiment have shown that downloading more document improves the accuracy of result merging however downloading more document increase the computation and communication cost this paper proposes a utility based optimization method that enables the system to automatically decide on the desired number of training document to download according to the user s need for effectiveness and efficiency 
in this paper we proposed an online algorithm called fqt stream frequent query tree of stream to mine the set of all frequent tree pattern over a continuous xml data stream a new numbering method is proposed to represent the tree structure of a xml query tree an effective sub tree numeration approach is developed to extract the essential information from the xml data stream the extracted information is stored in an effective summary data structure frequent query tree are mined from the current summary data structure by a depth first search manner 
it is extremely hard for a global organization with service over multiple channel to capture a consistent and unified view of it data service and interaction while soa and web service are addressing integration and interoperability problem it is painful for an operational organization with legacy system to quickly switch to service based method we need method to combine advantage of traditional i e web desktop or mobile application development environment and service based deployment in this paper we focus on the design and implementation of session management a a core service to support business process and go beyond application specific session and web session we develop local session component for different platform and complement them with a remote session service that is independent of application and platform we aim to close the gap between the two world by combining their performance availability and interoperability advantage 
relevance feedback is a powerful technique to enhance content based image retrieval cbir performance it solicits the user s relevance judgment on the retrieved image returned by the cbir system the user s labeling is then used to learn a classifier to distinguish between relevant and irrelevant image however the top returnedimages may not be the most informative one the challenge is thus to determine which unlabeled image would be the most informative i e improve the classifier the most if they were labeled and used a training sample in this paper we propose a novel active learning algorithm called laplacian optimal design lod for relevance feedback image retrieval our algorithm is based on aregression model which minimizes the least square error on the measured or labeled image and simultaneously preserve the local geometrical structure of the image space specifically we assume that if two image are sufficiently close to each other then their measurement or label are close a well by constructing a nearest neighbor graph the geometrical structure of the image space can be described by the graph laplacian we discus how result from the field of optimal experimental design may be used to guide our selection of a subset of image which give u the most amount of information experimental result on corel database suggest that theproposed approach achieves higher precision in relevance feedback image retrieval 
this paper discus generating document structure from annotated medium repository in a domain independent manner this approach the vision of a universal rdf browser we start by applying the search and browse paradigm established for the www to rdf presentation furthermore this paper add to this paradigm the clustering based derivation of document structure from search return providing simple but domain independent hypermedia generation from rdf store while such generated presentation hardly meet the standard of those written by human they provide quick access to medium repository when the required document ha not yet been written the resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any rdf repository the impact for content provider is that a soon a one add new medium item and their annotation to a repository they become immediately available for automatic integration into subsequently requested presentation 
in this paper we revisit the dependence language modelfor information retrieval proposed in and show that thismodel is deficient from a theoretical point of view we thenpropose a new model well founded theoretically for integratingdependencies between term in the language model this new model is simpler yet more general than the oneproposed in and yield similar result in our experiment on both syntactic and semantic dependency 
to facilitate the search for relevant information across a setof online distributed collection a federated information retrieval system typically represents each collection centrally by a set of vocabulary or sampled document accurate retrieval is therefore related to how precise each representation reflects the underlying content stored in that collection a collection evolve over time collection representation should also be updated to reflect any change however a current solution ha not yet been proposed in this study we examine both the implication of out of date representation set on retrieval accuracy a well a proposing three different policy for managing necessary update each policyis evaluated on a testbed of forty four dynamic collection over an eight week period our finding show that out of date representation significantly degrade performance overtime however adopting a suitable update policy can minimise this problem 
co occurrence data is quite common in many real application latent semantic analysis lsa ha been successfully used to identify semantic relation in such data however lsa can only handle a single co occurrence relationship between two type of object in practical application there are many case where multiple type of object exist and any pair of these object could have a pairwise co occurrence relation all these co occurrence relation can be exploited to alleviate data sparseness or to represent object more meaningfully in this paper we propose a novel algorithm m lsa which conduct latent semantic analysis by incorporating all pairwise co occurrence among multiple type of object based on the mutual reinforcement principle m lsa identifies the most salient concept among the co occurrence data and represents all the object in a unified semantic space m lsa is general and we show that several variant of lsa are special case of our algorithm experiment result show that m lsa outperforms lsa on multiple application including collaborative filtering text clustering and text categorization 
a common limitation of many retrieval model including the recently proposed axiomatic approach is that retrieval score are solely based on exact i e syntactic matching of term in the query and document without allowing distinct but semantically related term to match each other and contribute to the retrieval score in this paper we show that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of term we define several desirable retrieval constraint for semantic term matching and use such constraint to extend the axiomatic model to directly support semantic term matching based on the mutual information of term computed on some document set we show that such extension can be efficiently implemented a query expansion experiment result on several representative data set show that with mutual information computed over the document in either the target collection for retrieval or an external collection such a the web our semantic expansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model a a pseudo feedback method our method also outperforms a state of the art language modeling feedback method 
there ha been increased interest in the use of simulated query for evaluation and estimation purpose in information retrieval however there are still many unaddressed issue regarding their usage and impact on evaluation because their quality in term of retrieval performance is unlike real query in this paper wefocus on method for building simulated known item topic and explore their quality against real known item topic using existing generation model a our starting point we explore factor which may influence the generation of the known item topic informed by this detailed analysis on six european language we propose a model with improved document and term selection property showing that simulated known item topic can be generated that are comparable to real known item topic this is a significant step towards validating the potential usefulness of simulated query for evaluation purpose and becausebuilding model of querying behavior provides a deeper insight into the querying process so that better retrieval mechanism can be developed to support the user 
this paper describes autotag a tool which suggests tag for weblog post using collaborative filtering method an evaluation of autotag on a large collection of post show good accuracy coupled with the blogger s final quality control autotag assist both in simplifying the tagging process and in improving it quality 
this paper address the problem of learning to classify textsby exploiting information derived from clustering both training and testing set the incorporation of knowledge resulting from clustering into the feature space representation of the text is expected to boost the performance of a classifier experiment conducted on several widely used datasets demonstrate the effectiveness of the proposed algorithm especially for small training set 
the trec gov collection make a valuable web testbed for distributed information retrieval method because it is naturally partitioned and includes web oriented query with judged answer it can usefully model aspect of government and large corporate portal analysis of the gov data show that a purely distributed approach would not be feasible for providing search on a gov portal because of the large number of web site and the high proportion that do not provide a search interface an alternative hybrid approach combining both distributed and centralized technique is proposed and server selection method are evaluated within this framework using web oriented evaluation methodology a number of well known algorithm are compared against representative highest anchor ranked page harp and anchor weighted sum awsum of a family of new selection method which use link anchortext extracted from an auxiliary crawl to provide description of site which are not themselves crawled of the previously published method redde substantially outperformed three variant of cori and also outperformed a method based on kullback leibler divergence extended except on topic distillation harp and awsum performed best overall but were outperformed on the topic distillation task by extended kl divergence 
low cost method for acquiring relevance judgment can be a boon to researcher who need to evaluate new retrieval task or topic but do not have the resource to make thousand of judgment while these judgment are very useful for a one time evaluation it is not clear that they can be trusted when re used to evaluate new system in this work we formally define what it mean for judgment to be reusable the confidence in an evaluation of new system can be accurately assessed from an existing set of relevance judgment we then present a method for augmenting a set of relevance judgment with relevance estimate that require no additional assessor effort using this method practically guarantee reusability with a few a five judgment per topic taken from only two system we can reliably evaluate a larger set of ten system even the smallest set of judgment can be useful for evaluation of new system 
discovering mapping between concept hierarchy is widely regarded a one of the hardest and most urgent problem facing the semantic web the problem is even harder in domain where concept are inherently vague and ill defined and cannot be given a crisp definition a notion of approximate concept mapping is required in such domain but until now no such notion is vailable the first contribution of this paper is a definition for approximate mapping between concept roughly a mapping between two concept is decomposed into a number of submappings and a sloppiness value determines the fraction of these submappings that can be ignored when establishing the mapping a potential problem of such a definition is that with an increasing sloppiness value it will gradually allow mapping between any two arbitrary concept to improve on this trivial behaviour we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable match but at the same time maximises the sloppiness required to conclude undesirable match the second contribution of this paper is to show that a google based similarity measure ha exactly these desirable property we establish these result by experimental validation in the domain of musical genre we show that this domain doe suffer from ill defined concept we take two real life genre hierarchy from the web we compute approximate mapping between them at varying level of sloppiness and we validate our result against a handcrafted gold standard our method make use of the huge amount of knowledge that is implicit in the current web and exploit this knowledge a a heuristic for establishing approximate mapping between ill defined concept 
in this article we present novel learning method for estimating the quality of result returned by a search engine in response to a query estimation is based on the agreement between the top result of the full query and the top result of it sub query we demonstrate the usefulness of quality estimation for several application among them improvement of retrieval detecting query for which no relevant content exists in the document collection and distributed information retrieval experiment on trec data demonstrate the robustness and the effectiveness of our learning algorithm 
the work presented in this paper is motivated by the practical need for content extraction and the available data source and evaluation benchmark from the ace program the chinese entity detection and recognition edr task is of particular interest to u this task present u several language independent and language dependent challenge e g rising from the complication of extraction target and the problem of word segmentation etc in this paper we propose a novel solution to alleviate the problem special in the task mention detection take advantage of machine learning approach and character based model it manipulates different type of entity being mentioned and different constitution unit i e extent and head separately mention referring to the same entity are linked together by integrating most specific first and closest first rule based pairwise clustering algorithm type of mention and entity are determined by head driven classification approach the implemented system achieves ace value of when evaluated on the edr chinese corpus which ha been one of the top tier result alternative approach to mention detection and clustering are also discussed and analyzed 
fitting enough information from webpage to make browsing on small screen compelling is a challenging task one approach is to present the user with a thumbnail image of the full web page and allow the user to simply press a single key to zoom into a region which may then be transcoded into wml xhtml summarized etc however if region for zooming are presented naively this yield a frustrating experience because of the number of coherent region sentence image and word that may be inadvertently separated here we cast the web page segmentation problem into a machine learning framework where we re examine this task through the lens of entropy reduction and decision tree learning this yield an efficient and effective page segmentation algorithm we demonstrate how simple technique from computer vision can be used to fine tune the result the resulting segmentation keep coherent region together when tested on a broad set of complex webpage 
these are exciting time for information retrieval web search engine have brought ir to the mass it now affect the life of hundred of million of people and growing a internet search company launch ever more product based on technique developed in these are exciting time for information retrieval web search engine have brought ir to the mass it now affect the life of hundred of million of people and growing a internet search company launch ever more product based on technique developed in ir research the real world pose unique challenge for search algorithm they operate at unprecedented scale and over a wide diversity of information in addition we have entered an unprecedented world of adversarial information retrieval the lure of billion of dollar of commerce guided by search engine motivates all kind of people to try all kind of trick to get their site to the top of the search result what technique do people use to defeat ir algorithm what are the evaluation challenge for a web search engine how much impact ha ir had on search engine how doe google serve over million query a day often with sub second response time this talk will show that the world of algorithm and system design for commercial search engine can be described by two of murphy s law a if anything can go wrong it will and b if anything cannot go wrong it will anyway 
mailing list archive in an enterprise are a valuable source for employee to dig into the past proceeding of the organization that could be relevant to their present task going through the proceeding of discussion about certain topic might be cumbersome and regular search technique might not work in this context due to the genre that the document belong to in this paper we propose method based on theory of subjectivity to retrieve email message that could contain argumentative discussion about the topic that the user is interested in 
we describe an adaptive method for extracting record from web page our algorithm combine a weighted tree matching metric with clustering for obtaining data extraction pattern we compare our method experimentally to the state of the art and show that our approach is very competitive for rigidly structured record such a product description and far superior for loosely structured record such a entrieson blog 
in this paper we describe research into the use of ontology to integrate access to cultural heritage and photographic archive the use of the cidoc crm and crm core ontology are described together with the metadata mapping methodology a system integrating data from four content provider will be demonstrated 
while the pagerank algorithm ha proven to be very effective for ranking web page the rank score of web page can be manipulated to handle the manipulation problem and to cast a new insight on the web structure we propose a ranking algorithm called diffusionrank diffusionrank is motivated by the heat diffusion phenomenon which can be connected to web ranking because the activity flow on the web can be imagined a heat flow the link from a page to another can be treated a the pipe of an air conditioner and heat flow can embody the structure of the underlying web graph theoretically we show that diffusionrank can serve a a generalization of pagerank when the heat diffusion co efficient tends to infinity in such a case diffusionrank pagerank ha low ability of anti manipulation when diffusionrank obtains the highest ability of anti manipulation but in such a case the web structure is completely ignored consequently is an interesting factor that can control the balance between the ability of preserving the original web and the ability of reducing the effect of manipulation it is found empirically that when diffusionrank ha a penicillin like effect on the link manipulation moreover diffusionrank can be employed to find group to group relation on the web to divide the web graph into several part and to find link community experimental result show that the diffusionrank algorithm achieves the above mentioned advantage a expected 
text categorization is an important research area in many information retrieval ir application to save the storage space and computation time in text categorization efficient and effective algorithm for reducing the data before analysis are highly desired traditional technique for this purpose can generally be classified into feature extraction and feature selection because of efficiency the latter is more suitable for text data such a web document however many popular feature selection technique such a information gain ig and test chi are all greedy in nature and thus may not be optimal according to some criterion moreover the performance of these greedy method may be deteriorated when the reserved data dimension is extremely low in this paper we propose an efficient optimal feature selection algorithm by optimizing the objective function of orthogonal centroid oc subspace learning algorithm in a discrete solution space called orthogonal centroid feature selection ocfs experiment on newsgroups ng reuters corpus volume rcv and open directory project odp data show that ocfs is consistently better than ig and chi with smaller computation time especially when the reduced dimension is extremely small 
in recent year different commercial weblog subscribing system have been proposed to return story from user subscribed feed in this paper we propose a novel clustering based r aggregator called a r clusgator system rcs for weblog reading note that an r feed may have several different topic a user may only be interested in a subset of these topic in addition there could be many different story from multiple r feed which discus similar topic from different perspective a user may be interested in this topic but do not know how to collect all feed related to this topic in contrast to many previous work we cluster all story in r feed into hierarchical structure to better serve the reader through this way user can easily find all their interested story to make the system current we propose a flexible time window for incremental clustering rcs utilizes both link information and content information for efficient clustering experiment show the effectiveness of rcs 
in this paper we describe an application pubcloud that us tagclouds for the summarization of result from query over thepubmed database of biomedical literature pubcloud responds toqueries of this database with tag cloud generated from wordsextracted from the abstract returned by the query the result ofa user study comparing the pubcloud tag cloud summarization ofquery result with the standard result list provided by pubmedindicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustrationbut that it is le effective at the task of enabling user to discoverrelations between concept 
moore s law and the wave of technology it enabled have led to tremendous improvement in productivity and the quality of life in the industrialized world yet technology ha had almost no effect on the four billion people that make le u day in this talk i argue that the decreasing cost of computing and wireless networking make this the right time to spread the benefit of technology and that the biggest missing piece is a lack of focus on the problem that matter including health education and government after covering some example application that have shown very high impact i take an early look at the research agenda for developing region finally i examine some of the pragmatic issue required to make progress on these very challenging problem my goal is to convince high tech researcher that technology for developing region is an important and viable research topic 
in this work we propose a method that retrieves a list of related query given an initial input query the related query are based on the query log of previously issued query by human user which can be discovered using our improved association rule mining model user can use the suggested related query to tune or redirect the search process our method not only discovers the related query but also rank them according to the degree of their relatedness unlike many other rival technique it exploit only limited query log information and performs relatively better on query in all frequency division 
we investigate the diverse goal that people have when they issue the same query to a search engine and the ability of current search engine to address such diversity we quantify the potential value of personalizing search result based on this analysis great variance wa found in the result that different individual rated a relevant for the same query even when the same information goal wa expressed our analysis suggests that while search engine do a good job of ranking result to maximize global happiness they do not do a very good job for specific individual 
the result of the web query log analysis may be significantly shifted depending on the fraction of agent non human client which are not excluded from the log to detect and exclude agent the web log study use threshold value for a number of request submitted by a client during the observation period however different study use different observation period and a threshold assigned to one period is usually incomparable with the threshold assigned to the other period we propose the uniform method equally working on the different observation period the method base on the sliding window technique a threshold is assigned to the sliding window rather than to the whole observation period besides we determine the sub optimal value of the parameter of the method a window size and a threshold and recommend unique query a an upper bound of the threshold for hour sliding window 
the primary aim of xml element retrieval is to return to user xml element rather than whole document this poster describes a small study in which we elicited user expectation i e their anticipated experience when interacting with an xml retrieval system a compared to a traditional flat document retrieval system 
performance evaluation is an important issue in web search engine research traditional evaluation method rely on much human effort and are therefore quite time consuming with click through data analysis we proposed an automatic search engine performance evaluation method this method generates navigational type query topic and answer automatically based on search user querying and clicking behavior experimental result based on a commercial chinese search engine s user log show that the automatically method get a similar evaluation result with traditional assessor based one 
peer to peer p p web search ha gained a lot of interest lately due to the salient characteristic of p p system namely scalability fault tolerance and load balancing however the lack of global knowledge in a vast and dynamically evolving environment like the web present a grand challenge for organizing content and providing efficient searching semantic overlay network son have been proposed a an approach to reduce cost and increase quality of result and in this paper we present an unsupervised approach for distributed and decentralized son construction aiming to support efficient search mechanism in unstructured p p system 
on an abstract level xml schema increase the limited expressive power of document type definition dtds by extending them with a recursive typing mechanism however an investigation of the xml schema definition xsds occurring in practice reveals that the vast majority of them are structurally equivalent to dtds this might be due to the complexity of the xml schema specification and the difficulty to understand the effect of constraint on typing and validation of schema to shed some light on the actual expressive power of xsds this paper study the impact of the element declaration consistent edc and the unique particle attribution upa rule an equivalent formalism based on contextual pattern rather than on recursive type is proposed which might serve a a light weight front end for xml schema finally the effect of edc and upa on the way xml document can be typed is discussed it is argued that a cleaner more robust stronger but equally efficient class is obtained by replacing edc and upa with the notion of pas preorder typing schema that allow to determine the type of an element of a streaming document when it opening tag is met this notion can be defined in term of restrained competition regular expression and there is again an equivalent syntactical formalism based on contextual pattern 
query by semantic description qbsd is a natural paradigm for retrieving content from large database of music a major impediment to the development of good qbsd system for music information retrieval ha been the lack of a cleanly labeled publicly available heterogeneous data set of song and associated annotation we have collected the computer audition lab song cal data set by having human listen to and annotate song using a survey designed to capture semantic association between music and word we adapt the supervised multi class labeling sml model which ha shown good performance on the task of image retrieval and use the cal data to learn a model for music retrieval the model parameter are estimated using the weighted mixture hierarchy expectation maximization algorithm which ha been specifically designed to handle real valued semantic association between word and song rather than binary class label the output of the sml model a vector of class conditional probability can be interpreted a a semantic multinomial distribution over a vocabulary by also representing a semantic query a a query multinomial distribution we can quickly rank order the song in a database based on the kullback leibler divergence between the query multinomial and each song s semantic multinomial qualitative and quantitative result demonstrate that our sml model can both annotate a novel song with meaningful word and retrieve relevant song given a multi word text based query 
often scientist seek to search for article on the web related to a particular chemical when a scientist search for a chemical formula using a search engine today she get article where the exact keyword string expressing the chemical formula is found searching for the exact occurrence of keywords during searching result in two problem for this domain a if the author search for ch and the article ha h c the article is not returned and b ambiguous search like he return all document where helium is mentioned a well a document where the pronoun he occurs to remedy these deficiency we propose a chemical formula search engine to build a chemical formula search engine we must solve the following problem extract chemical formula from text document index chemical formula and designranking function for the chemical formula furthermore query model are introduced for formula search and for each a scoring scheme based on feature of partial formula is proposed tomeasure the relevance of chemical formula and query we evaluate algorithm for identifying chemical formula in document using classification method based on support vector machine svm and a probabilistic model based on conditional random field crf different method for svm and crf to tune the trade off between recall and precision forim balanced data are proposed to improve the overall performance a feature selection method based on frequency and discrimination isused to remove uninformative and redundant feature experiment show that our approach to chemical formula extraction work well especially after trade off tuning the result also demonstrate that feature selection can reduce the index size without changing ranked query result much 
in this paper we present a novel multi webpage summarization algorithm it add the graph based ranking algorithm into the framework of maximum marginal relevance mmr method to not only capture the main topic of the web page but also eliminate the redundancy existing in the sentence of the summary result the experiment result indicates that the new approach ha the better performance than the previous method 
pagerank ha been known to be a successful algorithm in ranking web source in order to avoid the rank sink problem pagerank assumes that a surfer being in a page jump to a random page with a certain probability in the standard pagerank algorithm the jumping probability are assumed to be the same for all the page regardless of the page property this is not the case in the real world since presumably a surfer would more likely follow the out link of a high quality hub page than follow the link of a low quality one in this poster we propose a novel algorithm dirichlet pagerank to address this problem by adapting exible jumping probability based on the number of out link in a page empirical result on trec data show that our method outperforms the standard pagerank algorithm 
much interesting text on the web consists largely of opinionated or evaluative text a opposed to directly informative text the new eld of sentiment analysis seek to characterize such aspect of natural language text a opposed to just the bare fact we suggest that appraisal expression extraction should be viewed a a fundamental task for sentiment analysis we dene an appraisal expression to be a piece of text expressing some evaluative stance towards a particular object the task is to nd these element and characterize the type and orientation positive or negative of the evaluative stance a well a it target and possibly it source potential application of these method include new approach to the now traditional task of sentiment classication and opinion mining a well a possibly for adversarial textual analysis and intention detection for intelligence application 
user searching for information in hypermedia environment often perform querying followed by manual navigation yet the conventional text hypertext retrieval paradigm doe not explicity take post query navigation into account this paper proposes a new retrieval paradigm called navigation aided retrieval nar which treat both querying and navigation a first class activity in the nar paradigm querying is seen a a mean to identify starting point for navigation and navigation is guided based on information supplied in the query nar is a generalization of the conventional probabilistic information retrieval paradigm which implicitly assumes no navigation take place this paper present a formal model for navigation aided retrieval and report empirical result that point to the real world applicability of the model the experiment were performed over a large web corpus provided by trec using human judgment on a new rating scale developed for navigation aided retrieval in the case of ambiguous query the new retrieval model identifies good starting point for post query navigation for le ambiguous query that need not be paired with navigation the output closely match that of a conventional retrieval system 
richard granger will be providing an update on the deployment of information technology at a national scale in the nh in england particular topic that will be covered include variability of performance and user organization and supplier access channel strategy for nh user and member of the public take up rate for new technology including internet adoption data on number of user and transaction to date will also be provided 
this acceptance talk is a curious mixture of personal history and developing idea in the context of the growing field of ir covering several decade i want to concentrate on model and theory interpreted loosely and try and give an insight into where i have got to in my thinking where the idea came from and where i believe we are going in the last few year i have been working on the development of what might be coined a a design language for ir it take it inspiration from quantum mechanic but by analogy only the mathematical object represent document these object might be vector or density operator in an n dimensional vector space usually a hilbert space a request for information or a query is taken a an observable and is represented a a linear operator on the space linear operator can be expressed a matrix such an operator hermitian ha a set of eigenvectors forming a basis for the space which we interpret a a point of view or perspective from which to understand the space thus any document vector can be located with respect to the basis and we can calculate an inner product between such a vector and any basis vector which may be interpreted a a probability of relevance the probability of observing any given eigenvector is now given by the square of that inner product assuming all vector are normalised hence we connect the probability of observation to the geometry of the space furthermore the subspace of the space make up a lattice structure which is equivalent to a logic this make up the entire mathematical structure and the language for handling this structure is linear algebra vector matrix projection inner product neatly captured by the dirac notation used in quantum mechanic our probability is slightly different from classical probability the same for logic we end up with quantum logic and quantum probability a commitment to this kind of mathematical structure with which to model object and process in ir depends on two critical assumption the distance in the space between object are a source of important relationship with respect to relevance and aboutness the observation of a property such a relevance or aboutness is user dependent in the sense that a potential interaction is specified by a user through an operator which when measured achieves outcome with a probability determined by the geometry of the space the geometry of this mathematical structure and the probability defined on it are closely connected by the following theorem due to gleason one may summarise this theorem by saying that the probability of a subspace is given by a simple algorithm derived from a projection onto the subspace and a special kind of operator namely a statistical operator or density matrix and conversely that given a probability measure on the subspace then we can encode that measure uniquely through such an algorithm this is a very powerful theorem and it consequence remain to be explored so how did i get to this point and form of abstraction most of my research work can be divided into contribution to the following area clusteringevaluationprobabilistic modelslogic modelsgeometry in all these area i have attempted to search for underlying mathematical structure that would lead to computation these topic have in common that they depend on the construction of measure on a space which in some sense determines the usefulness or effectiveness of the structure for clustering one considers mapping from metric space to ultrametic space and measure the closeness of fit in the case of evaluation one start with a relational conjoint structure and imposes some constraint given by what is to be measured one then construct a numerical representation of this structure leading to such measure a f or e for probabilistic model the main difficulty is concerned with deciding on an appropriate event space on which to define the right probability measure for me the most significant example in this context wa the attempt to construct a logical uncertainty principle which formulated a measure of uncertainty on incomplete logical construct this attempt left unspecified the exact form of the measure in the geometry of ir i finally managed to formulate that measure a a projection valued measure this way of thinking did not appear out of nowhere it wa heavily influenced by the work of fairthorne whose work on brouwerian logic an intuitionistic logic wa picked up by salton in his early book on ir at an earlier stage mackay wrote a paper that opened with this paper relates to the borderline linking experimental and theoretical physic with mathematical logic and cover at several point ground which is common to the theory of communication he go on to define an information operator which is very similar in scope and intent to the hermitian operator above maron who collaborated with mackay stated in his paper therefore it can be argued that index description should not be viewed a property of document they function to relate document and user one can see that the development of these early idea wa continued to the construction of the geometry of ir what doe it leave to be done an attempt should be made to use this design language to build an ir system on the theoretical front it is worth considering whether it would be better to start with a transition probability space rather than a hilbert space a von neumann did in translated in the assumption that closed linear subspace will be the element of our logic can be challenged a perhaps a construction with different element is possible it is not obvious what the best form of conditional probability might be in these space agreeing on a form of conditionalisation is intimately tied up with how to model contextuality there is some evidence to suggest that contextuality play a role in modelling the conjuncton of concept widdows such context have been modelled in quantum theory almost from the beginning for example gleason s theorem precludes noncontextual hidden variable theory 
in the last few year internet user have seen the rapid expansion of phishing the use of spoofed e mail and fraudulent website designed to trick user into divulging sensitive data more recently we have seen the growth of pharming the use of malware or dns based attack to misdirect user to rogue website in this panel we will examine the state of the art in anti phishing solution and explore promising direction for future research 
we propose a new photo search method that us three dimensional d viewpoint a query d viewpoint based image retrieval is especially useful for searching collection of archaeological photograph which contain many different image of the same object our method is designed to enable user to retrieve image that contain the same object but show a different view and to browse group of image taken from a similar viewpoint we also propose using d scene to query by example which mean that user do not have the problem of trying to formulate appropriate query this combination give user an easy way of accessing not only photograph but also archived information 
similarity measure for text have historically been an important tool for solving information retrieval problem in many interesting setting however document are often closely connected to other document a well a other non textual object for instance email message are connected to other message via header information in this paper we consider extended similarity metric for document and other object embedded in graph facilitated via a lazy graph walk we provide a detailed instantiation of this framework for email data where content social network and a timeline are integrated in a structural graph the suggested framework is evaluated for two email related problem disambiguating name in email document and threading we show that reranking scheme based on the graph walk similarity measure often outperform baseline method and that further improvement can be obtained by use of appropriate learning method 
given a large online network of online auction user and their history of transaction how can we spot anomaly and auction fraud this paper describes the design and implementation of netprobe a system that we propose for solving this problem netprobe model auction user and transaction a a markov random field tuned to detect the suspicious pattern that fraudsters create and employ a belief propagation mechanism to detect likely fraudsters our experiment show that netprobe is both efficient and effective for fraud detection we report experiment on synthetic graph with a many a node and edge where netprobe wa able to spot fraudulent node with over precision and recall within a matter of second we also report experiment on a real dataset crawled from ebay with nearly transaction between more than user where netprobe wa highly effective at unearthing hidden network of fraudsters within a realistic response time of about minute for scenario where the underlying data is dynamic in nature we propose incrementalnetprobe which is an approximate but fast variant of netprobe our experiment prove that incremental netprobe executes nearly doubly fast a compared to netprobe while retaining over of it accuracy 
recently a number of trec track have adopted a retrieval effectiveness metric called bpref which ha been designed for evaluation environment with incomplete relevance data a graded relevance version of this metric called rpref ha also been proposed however we show that the application of q measure normalised discounted cumulative gain ndcg or average precision avep to condensed list obtained by ltering out all unjudged document from the original ranked list is actually a better solution to the incompleteness problem than bpref furthermore we show that the use of graded relevance boost the robustness of ir evaluation to incompleteness and therefore that q measure and ndcg based on condensed list are the best choice to this end we use four graded relevance test collection from ntcir to compare ten different ir metric in term of system ranking stability and pairwise discriminative power 
we introduce the notion of query substitution that is generating a new query to replace a user s original search query our technique us modications based on typical substitution web searcher make to their query in this way the new query is strongly related to the original query containing term closely related to all of the original term this contrast with query expansion through pseudo relevance feedback which is costly and can lead to query drift this also contrast with query relaxation through boolean or tfidf retrieval which reduces the specicit y of the query we dene a scale for evaluating query substitution and show that our method performs well at generating new query related to the original query we build a model for selecting between candidate by using a number of feature relating the query candidate pair and by tting the model to human judgment of relevance of query suggestion this further improves the quality of the candidate generated experiment show that our technique signican tly increase coverage and eectiv ene in the setting of sponsored search 
the task of finding people who are expert on a topic ha recently received increased attention we introduce a different expert finding task for which a small number of example expert is given instead of a natural language query and the system s task is to return similar expert we define compare and evaluate a number of way of representing expert and investigate how the size of the initial example set affect performance we show that more finegrained representation of candidate result in higher performance and larger sample set a input lead to improved precision 
in this paper we present a novel method for the classification of web site this method exploit both structure and content of web site in order to discern their functionality it allows for distinguishing between eight of the most relevant functional class of web site we show that a pre classification of web site utilizing structural property considerably improves a subsequent textual classification with standard technique we evaluate this approach on a dataset comprising more than web site with about million crawled and million known web page our approach achieves an accuracy of for the coarse grained classification of these web site category and subject descriptor h information storage and retrieval information search and retrieval information filtering selection process h database management database application data mining 
recently manning et al resorted the permuterm indexof garfield a a time efficient and elegant solution to the string dictionary problem in which pattern query may possibly include one wild card symbol called tolerant retrieval problem unfortunately the permuterm index is space inefficient because it quadruple the dictionary size in this paper we propose the compressed permuterm index which solves the tolerant retrieval problem in optimal query time i e time proportional to the length of the searched pattern and space close to the k th order empirical entropy of the indexed dictionary our index can be used to solve also more sophisticated query which involve several wild card symbol or require to prefix match multiple field in a database of record the result is based on an elegant variant of the burrow wheeler transform defined on a dictionary of string of variable length which allows to easily adapt known compressed index makinen navarro to solve the tolerant retrieval problem experiment show that our index support fast query within a space occupancy that is close to the one achievable by compressing the string dictionary via gzip bzip or ppmdi this improves known approach based on front coding by more than in absolute space occupancy still guaranteeing comparable query time 
a an increasingly large number of owl ontology become available on the semantic web and the description in the ontology become more complicated finding the cause of error becomes an extremely hard task even for expert existing ontology development environment provide some limited support in conjunction with a reasoner for detecting and diagnosing error in owl ontology typically these are restricted to the mere detection of for example unsatisfiable concept we have integrated a number of simple debugging cue generated from our description logic reasoner pellet in our hypertextual ontology development environment swoop these cue in conjunction with extensive undo redo and annotea based collaboration support in swoop significantly improve the owl debugging experience and point the way to more general improvement in the presentation of an ontology to new user 
we describe a task based evaluation to determine whether multi document summary measurably improve user performance whe using online news browsing system for directed research we evaluated the multi document summary generated by newsblaster a robust news browsing system that cluster online news article and summarizes multiple article on each event four group of subject were asked to perform the same time restricted fact gathering task reading news under different condition no summary at all single sentence summary drawn from one of the article newsblaster multi document summary and human summary our result show that in comparison to source document only the quality of report assembled using newsblaster summary wa significantly better and user satisfaction wa higher with both newsblaster and human summary 
this paper proposes a random web crawl model a web crawl is a biased and partial image of the web this paper deal with the hyperlink structure i e a web crawl is a graph whose vertex are the page and whose edge are the hypertextual link of course a web crawl ha a very special structure we recall some known result about it we then propose a model generating similar structure our model simply simulates a crawling i e build and crawl the graph at the same time the graph generated have lot of known property of web crawl our model is simpler than most random web graph model but capture the same property notice that it model the crawling process instead of the page writing process of web graph model 
it is now a common practice for e commerce web site to enable their customer to write review of product that they have purchased such review provide valuable source of information on these product they are used by potential customer to find opinion of existing user before deciding to purchase a product they are also used by product manufacturer to identify problem of their product and to find competitive intelligence information about their competitor unfortunately this importance of review also give good incentive for spam which contains false positive or malicious negative opinion in this paper we make an attempt to study review spam and spam detection to the best of our knowledge there is still no reported study on this problem 
in strategic management there ha been a debate over many year already in alfred chandler had stated structure follows strategy in the nineteen eighty michael porter modified chandler s dictum about structure following strategy by introducing a second level of structure organizational structure follows strategy which in turn follows structure so the question became what is leading what technology ha in this debate been seen a a part of either the structure of the organisation itself or part of the development of the environment in which the organisation try to survive by adapting the notion that technological advancement can also change the paradigmas of organisational strategy development is new this ha mainly to do with the impact of the technological change on the workflow and procedure of organisation never before they were so profound a in our day technological change affect u on different level of our strategic development i will give three example of change that are occurring or have occurred in sound and vision the first is the introduction of rfid transmitter in admission ring for the sound and vision experience the second is the setup of a back office medium asset management storage and distribution structure for the public broadcaster the third is the development of the archive towards becoming a medium application service provider 
path expression are the principal mean of locating data in a hierarchical model but path expression are brittle because they often depend on the structure of data and break if the data is structured differently the structure of data could be unfamiliar to a user may differ within a data collection or may change over time a the schema evolves this paper proposes a novel construct that locates related node in an instance of an xml data model independent of a specific structure it can augment many xpath expression and can be seamlessly incorporated in xquery or xslt 
this paper is concerned with rank aggregation the task of combining the ranking result of individual ranker at meta search previously rank aggregation wa performed mainly by mean of unsupervised learning to further enhance ranking accuracy we propose employing supervised learning to perform the task using labeled data we refer to the approach a supervised rank aggregation we set up a general framework for conducting supervised rank aggregation in which learning is formalized an optimization which minimizes disagreement between ranking result and the labeled data a case study we focus on markov chain based rank aggregation in this paper the optimization for markov chain based method is not a convex optimization problem however and thus is hard to solve we prove that we can transform the optimization problem into that of semidefinite programming and solve it efficiently experimental result on meta search show that supervised rank aggregation can significantly outperform existing unsupervised method 
namespaces are a central building block of xml technology today they provide the identification mechanism for many xml related vocabulary despite their ubiquity there is no established mechanism for describing namespaces and in particular for describing the dependency of namespaces we propose a simple model for describing namespaces and their dependency using these description it is possible to compile directory of namespaces providing searchable and browsable namespace description 
abstract this paper focus on a method for the stylistic segmentation of text document our technique involves mapping the change in a feature throughout a text we use the linguistic feature of conjunction and modality through taxonomy from systemic functional linguistics this segmentation ha application in automated summarization particularly of large document category and subject descriptor 
contextual search refers to proactively capturing the information need of a user by automatically augmenting the user query with information extracted from the search context for example by using term from the web page the user is currently browsing or a file the user is currently editing we present three different algorithm to implement contextual search for the web the first query rewriting qr augments each query with appropriate term from the search context and us an off the shelf web search engine to answer this augmented query the second rank biasing rb generates a representation of the context and answer query using a custom built search engine that exploit this representation the third iterative filtering meta search ifm generates multiple subqueries based on the user query and appropriate term from the search context us an off theshelf search engine to answer these subqueries and re rank the result of the subqueries using rank aggregation method we extensively evaluate the three method using context and over human relevance judgment of search result we show that while qr work surprisingly well the relevance and recall can be improved using rb and substantially more using ifm thus qr rb and ifm represent a cost effective design spectrum for contextual search 
content targeted advertising the task of automatically associating ad to a web page constitutes a key web monetization strategy nowadays further it introduces new challenging technical problem and raise interesting question for instance how to design ranking function able to satisfy conflicting goal such a selecting advertisement ad that are relevant to the user and suitable and profitable to the publisher and advertiser in this paper we propose a new framework for associating ad with web page based on genetic programming gp our gp method aim at learning function that select the most appropriate ad given the content of a web page these ranking function are designed to optimize overall precision and minimize the number of misplacement by using a real ad collection and web page from a newspaper we obtained a gain over a stateof the art baseline method of in average precision further by evolving individual to provide good ranking estimation gp wa able to discover ranking function that are very eective in placing ad in web page while avoiding irrelevant one 
we propose a simple method for converting many standard measure of retrieval performance into metasearch algorithm our focus is both on the analysis of retrieval measure themselves and on the development of new metasearch algorithm given the conversion method proposed our experimental result using trec data indicate that system oriented measure of overall retrieval performance such a average precision yield good metasearch algorithm whose performance equal or exceeds that of benchmark technique such a combmnz and condorcet 
we present a language for specifying web service interface a web service interface put three kind of constraint on the user of the service first the interface specifies the method that can be called by a client together with type of input and output parameter these are called signature constraint second the interface may specify propositional constraint on method call and output value that may occur in a web service conversation these are called consistency constraint third the interface may specify temporal constraint on the ordering of method call these are called protocol constraint the interface can be used to check first if two or more web service are compatible and second if a web service a can be safely substituted for a web service b the algorithm for compatibility checking verifies that two or more interface fulfill each others constraint the algorithm for substitutivity checking verifies that service a demand fewer and fulfills more constraint than service b 
the primary function of current web search engine is essentially relevance ranking at the document level however myriad structured information about real world object is embedded in static web page and online web database document level information retrieval can unfortunately lead to highly inaccurate relevance ranking in answering object oriented query in this paper we propose a paradigm shift to enable searching at the object level in traditional information retrieval model document are taken a the retrieval unit and the content of a document is considered reliable however this reliability assumption is no longer valid in the object retrieval context when multiple copy of information about the same object typically exist these copy may be inconsistent because of diversity of web site quality and the limited performance of current information extraction technique if we simply combine the noisy and inaccurate attribute information extracted from different source we may not be able to achieve satisfactory retrieval performance in this paper we propose several language model for web object retrieval namely an unstructured object retrieval model a structured object retrieval model and a hybrid model with both structured and unstructured retrieval feature we test these model on a paper search engine and compare their performance we conclude that the hybrid model is the superior by taking into account the extraction error at varying level 
information graphic are non pictorial graphic such a bar chart and line graph that depict attribute of entity and relation among entity most information graphic appearing in popular medium have a communicative goal or intended message consequently information graphic constitute a form of language this paper argues that information graphic are a valuable knowledge resource that should be retrievable from a digital library and that such graphic should be taken into account when summarizing a multimodal document for subsequent indexing and retrieval but to accomplish this the information graphic must be understood and it message recognized the paper present our bayesian system for recognizing the primary message of one kind of information graphic simple bar chart and discus the potential role of an information graphic s message in indexing graphic and summarizing multimodal document 
existing retrieval model generally do not offer any guarantee for optimal retrieval performance indeed it is even difficult if not impossible to predict a model s empirical performance analytically this limitation is at least partly caused by the way existing retrieval model are developed where relevance is only coarsely modeled at the level of document and query a opposed to a finer granularity level of term in this paper we present a new axiomatic approach to developing retrieval model based on direct modeling of relevance with formalized retrieval constraint defined at the level of term the basic idea of this axiomatic approach is to search in a space of candidate retrieval function for one that can satisfy a set of reasonable retrieval constraint to constrain the search space we propose to define a retrieval function inductively and decompose a retrieval function into three component function inspired by the analysis of the existing retrieval function with the inductive definition we derive several new retrieval function using the axiomatic retrieval framework experiment result show that the derived new retrieval function are more robust and le sensitive to parameter setting than the existing retrieval function with comparable optimal performance 
federated search is the task of retrieving relevant document from different information resource one of the main research problem in federated search is to combine the result from different source into a single ranked list recent work proposed a regression based method to download some document from each ranked list of the different source calculated comparable score for the document and estimated mapping function that transform source specific score into comparable score experiment have shown that downloading more document improves the accuracy of result merging however downloading more document increase the computation and communication cost this paper proposes a utility based optimization method that enables the system to automatically decide on the desired number of training document to download according to the user s need for effectiveness and efficiency 
in this paper we proposed an online algorithm called fqt stream frequent query tree of stream to mine the set of all frequent tree pattern over a continuous xml data stream a new numbering method is proposed to represent the tree structure of a xml query tree an effective sub tree numeration approach is developed to extract the essential information from the xml data stream the extracted information is stored in an effective summary data structure frequent query tree are mined from the current summary data structure by a depth first search manner 
it is extremely hard for a global organization with service over multiple channel to capture a consistent and unified view of it data service and interaction while soa and web service are addressing integration and interoperability problem it is painful for an operational organization with legacy system to quickly switch to service based method we need method to combine advantage of traditional i e web desktop or mobile application development environment and service based deployment in this paper we focus on the design and implementation of session management a a core service to support business process and go beyond application specific session and web session we develop local session component for different platform and complement them with a remote session service that is independent of application and platform we aim to close the gap between the two world by combining their performance availability and interoperability advantage 
relevance feedback is a powerful technique to enhance content based image retrieval cbir performance it solicits the user s relevance judgment on the retrieved image returned by the cbir system the user s labeling is then used to learn a classifier to distinguish between relevant and irrelevant image however the top returnedimages may not be the most informative one the challenge is thus to determine which unlabeled image would be the most informative i e improve the classifier the most if they were labeled and used a training sample in this paper we propose a novel active learning algorithm called laplacian optimal design lod for relevance feedback image retrieval our algorithm is based on aregression model which minimizes the least square error on the measured or labeled image and simultaneously preserve the local geometrical structure of the image space specifically we assume that if two image are sufficiently close to each other then their measurement or label are close a well by constructing a nearest neighbor graph the geometrical structure of the image space can be described by the graph laplacian we discus how result from the field of optimal experimental design may be used to guide our selection of a subset of image which give u the most amount of information experimental result on corel database suggest that theproposed approach achieves higher precision in relevance feedback image retrieval 
this paper discus generating document structure from annotated medium repository in a domain independent manner this approach the vision of a universal rdf browser we start by applying the search and browse paradigm established for the www to rdf presentation furthermore this paper add to this paradigm the clustering based derivation of document structure from search return providing simple but domain independent hypermedia generation from rdf store while such generated presentation hardly meet the standard of those written by human they provide quick access to medium repository when the required document ha not yet been written the resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any rdf repository the impact for content provider is that a soon a one add new medium item and their annotation to a repository they become immediately available for automatic integration into subsequently requested presentation 
in this paper we revisit the dependence language modelfor information retrieval proposed in and show that thismodel is deficient from a theoretical point of view we thenpropose a new model well founded theoretically for integratingdependencies between term in the language model this new model is simpler yet more general than the oneproposed in and yield similar result in our experiment on both syntactic and semantic dependency 
to facilitate the search for relevant information across a setof online distributed collection a federated information retrieval system typically represents each collection centrally by a set of vocabulary or sampled document accurate retrieval is therefore related to how precise each representation reflects the underlying content stored in that collection a collection evolve over time collection representation should also be updated to reflect any change however a current solution ha not yet been proposed in this study we examine both the implication of out of date representation set on retrieval accuracy a well a proposing three different policy for managing necessary update each policyis evaluated on a testbed of forty four dynamic collection over an eight week period our finding show that out of date representation significantly degrade performance overtime however adopting a suitable update policy can minimise this problem 
co occurrence data is quite common in many real application latent semantic analysis lsa ha been successfully used to identify semantic relation in such data however lsa can only handle a single co occurrence relationship between two type of object in practical application there are many case where multiple type of object exist and any pair of these object could have a pairwise co occurrence relation all these co occurrence relation can be exploited to alleviate data sparseness or to represent object more meaningfully in this paper we propose a novel algorithm m lsa which conduct latent semantic analysis by incorporating all pairwise co occurrence among multiple type of object based on the mutual reinforcement principle m lsa identifies the most salient concept among the co occurrence data and represents all the object in a unified semantic space m lsa is general and we show that several variant of lsa are special case of our algorithm experiment result show that m lsa outperforms lsa on multiple application including collaborative filtering text clustering and text categorization 
a common limitation of many retrieval model including the recently proposed axiomatic approach is that retrieval score are solely based on exact i e syntactic matching of term in the query and document without allowing distinct but semantically related term to match each other and contribute to the retrieval score in this paper we show that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of term we define several desirable retrieval constraint for semantic term matching and use such constraint to extend the axiomatic model to directly support semantic term matching based on the mutual information of term computed on some document set we show that such extension can be efficiently implemented a query expansion experiment result on several representative data set show that with mutual information computed over the document in either the target collection for retrieval or an external collection such a the web our semantic expansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model a a pseudo feedback method our method also outperforms a state of the art language modeling feedback method 
there ha been increased interest in the use of simulated query for evaluation and estimation purpose in information retrieval however there are still many unaddressed issue regarding their usage and impact on evaluation because their quality in term of retrieval performance is unlike real query in this paper wefocus on method for building simulated known item topic and explore their quality against real known item topic using existing generation model a our starting point we explore factor which may influence the generation of the known item topic informed by this detailed analysis on six european language we propose a model with improved document and term selection property showing that simulated known item topic can be generated that are comparable to real known item topic this is a significant step towards validating the potential usefulness of simulated query for evaluation purpose and becausebuilding model of querying behavior provides a deeper insight into the querying process so that better retrieval mechanism can be developed to support the user 
this paper describes autotag a tool which suggests tag for weblog post using collaborative filtering method an evaluation of autotag on a large collection of post show good accuracy coupled with the blogger s final quality control autotag assist both in simplifying the tagging process and in improving it quality 
this paper address the problem of learning to classify textsby exploiting information derived from clustering both training and testing set the incorporation of knowledge resulting from clustering into the feature space representation of the text is expected to boost the performance of a classifier experiment conducted on several widely used datasets demonstrate the effectiveness of the proposed algorithm especially for small training set 
the trec gov collection make a valuable web testbed for distributed information retrieval method because it is naturally partitioned and includes web oriented query with judged answer it can usefully model aspect of government and large corporate portal analysis of the gov data show that a purely distributed approach would not be feasible for providing search on a gov portal because of the large number of web site and the high proportion that do not provide a search interface an alternative hybrid approach combining both distributed and centralized technique is proposed and server selection method are evaluated within this framework using web oriented evaluation methodology a number of well known algorithm are compared against representative highest anchor ranked page harp and anchor weighted sum awsum of a family of new selection method which use link anchortext extracted from an auxiliary crawl to provide description of site which are not themselves crawled of the previously published method redde substantially outperformed three variant of cori and also outperformed a method based on kullback leibler divergence extended except on topic distillation harp and awsum performed best overall but were outperformed on the topic distillation task by extended kl divergence 
low cost method for acquiring relevance judgment can be a boon to researcher who need to evaluate new retrieval task or topic but do not have the resource to make thousand of judgment while these judgment are very useful for a one time evaluation it is not clear that they can be trusted when re used to evaluate new system in this work we formally define what it mean for judgment to be reusable the confidence in an evaluation of new system can be accurately assessed from an existing set of relevance judgment we then present a method for augmenting a set of relevance judgment with relevance estimate that require no additional assessor effort using this method practically guarantee reusability with a few a five judgment per topic taken from only two system we can reliably evaluate a larger set of ten system even the smallest set of judgment can be useful for evaluation of new system 
discovering mapping between concept hierarchy is widely regarded a one of the hardest and most urgent problem facing the semantic web the problem is even harder in domain where concept are inherently vague and ill defined and cannot be given a crisp definition a notion of approximate concept mapping is required in such domain but until now no such notion is vailable the first contribution of this paper is a definition for approximate mapping between concept roughly a mapping between two concept is decomposed into a number of submappings and a sloppiness value determines the fraction of these submappings that can be ignored when establishing the mapping a potential problem of such a definition is that with an increasing sloppiness value it will gradually allow mapping between any two arbitrary concept to improve on this trivial behaviour we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable match but at the same time maximises the sloppiness required to conclude undesirable match the second contribution of this paper is to show that a google based similarity measure ha exactly these desirable property we establish these result by experimental validation in the domain of musical genre we show that this domain doe suffer from ill defined concept we take two real life genre hierarchy from the web we compute approximate mapping between them at varying level of sloppiness and we validate our result against a handcrafted gold standard our method make use of the huge amount of knowledge that is implicit in the current web and exploit this knowledge a a heuristic for establishing approximate mapping between ill defined concept 
in this article we present novel learning method for estimating the quality of result returned by a search engine in response to a query estimation is based on the agreement between the top result of the full query and the top result of it sub query we demonstrate the usefulness of quality estimation for several application among them improvement of retrieval detecting query for which no relevant content exists in the document collection and distributed information retrieval experiment on trec data demonstrate the robustness and the effectiveness of our learning algorithm 
the work presented in this paper is motivated by the practical need for content extraction and the available data source and evaluation benchmark from the ace program the chinese entity detection and recognition edr task is of particular interest to u this task present u several language independent and language dependent challenge e g rising from the complication of extraction target and the problem of word segmentation etc in this paper we propose a novel solution to alleviate the problem special in the task mention detection take advantage of machine learning approach and character based model it manipulates different type of entity being mentioned and different constitution unit i e extent and head separately mention referring to the same entity are linked together by integrating most specific first and closest first rule based pairwise clustering algorithm type of mention and entity are determined by head driven classification approach the implemented system achieves ace value of when evaluated on the edr chinese corpus which ha been one of the top tier result alternative approach to mention detection and clustering are also discussed and analyzed 
fitting enough information from webpage to make browsing on small screen compelling is a challenging task one approach is to present the user with a thumbnail image of the full web page and allow the user to simply press a single key to zoom into a region which may then be transcoded into wml xhtml summarized etc however if region for zooming are presented naively this yield a frustrating experience because of the number of coherent region sentence image and word that may be inadvertently separated here we cast the web page segmentation problem into a machine learning framework where we re examine this task through the lens of entropy reduction and decision tree learning this yield an efficient and effective page segmentation algorithm we demonstrate how simple technique from computer vision can be used to fine tune the result the resulting segmentation keep coherent region together when tested on a broad set of complex webpage 
these are exciting time for information retrieval web search engine have brought ir to the mass it now affect the life of hundred of million of people and growing a internet search company launch ever more product based on technique developed in these are exciting time for information retrieval web search engine have brought ir to the mass it now affect the life of hundred of million of people and growing a internet search company launch ever more product based on technique developed in ir research the real world pose unique challenge for search algorithm they operate at unprecedented scale and over a wide diversity of information in addition we have entered an unprecedented world of adversarial information retrieval the lure of billion of dollar of commerce guided by search engine motivates all kind of people to try all kind of trick to get their site to the top of the search result what technique do people use to defeat ir algorithm what are the evaluation challenge for a web search engine how much impact ha ir had on search engine how doe google serve over million query a day often with sub second response time this talk will show that the world of algorithm and system design for commercial search engine can be described by two of murphy s law a if anything can go wrong it will and b if anything cannot go wrong it will anyway 
mailing list archive in an enterprise are a valuable source for employee to dig into the past proceeding of the organization that could be relevant to their present task going through the proceeding of discussion about certain topic might be cumbersome and regular search technique might not work in this context due to the genre that the document belong to in this paper we propose method based on theory of subjectivity to retrieve email message that could contain argumentative discussion about the topic that the user is interested in 
we describe an adaptive method for extracting record from web page our algorithm combine a weighted tree matching metric with clustering for obtaining data extraction pattern we compare our method experimentally to the state of the art and show that our approach is very competitive for rigidly structured record such a product description and far superior for loosely structured record such a entrieson blog 
in this paper we describe research into the use of ontology to integrate access to cultural heritage and photographic archive the use of the cidoc crm and crm core ontology are described together with the metadata mapping methodology a system integrating data from four content provider will be demonstrated 
while the pagerank algorithm ha proven to be very effective for ranking web page the rank score of web page can be manipulated to handle the manipulation problem and to cast a new insight on the web structure we propose a ranking algorithm called diffusionrank diffusionrank is motivated by the heat diffusion phenomenon which can be connected to web ranking because the activity flow on the web can be imagined a heat flow the link from a page to another can be treated a the pipe of an air conditioner and heat flow can embody the structure of the underlying web graph theoretically we show that diffusionrank can serve a a generalization of pagerank when the heat diffusion co efficient tends to infinity in such a case diffusionrank pagerank ha low ability of anti manipulation when diffusionrank obtains the highest ability of anti manipulation but in such a case the web structure is completely ignored consequently is an interesting factor that can control the balance between the ability of preserving the original web and the ability of reducing the effect of manipulation it is found empirically that when diffusionrank ha a penicillin like effect on the link manipulation moreover diffusionrank can be employed to find group to group relation on the web to divide the web graph into several part and to find link community experimental result show that the diffusionrank algorithm achieves the above mentioned advantage a expected 
text categorization is an important research area in many information retrieval ir application to save the storage space and computation time in text categorization efficient and effective algorithm for reducing the data before analysis are highly desired traditional technique for this purpose can generally be classified into feature extraction and feature selection because of efficiency the latter is more suitable for text data such a web document however many popular feature selection technique such a information gain ig and test chi are all greedy in nature and thus may not be optimal according to some criterion moreover the performance of these greedy method may be deteriorated when the reserved data dimension is extremely low in this paper we propose an efficient optimal feature selection algorithm by optimizing the objective function of orthogonal centroid oc subspace learning algorithm in a discrete solution space called orthogonal centroid feature selection ocfs experiment on newsgroups ng reuters corpus volume rcv and open directory project odp data show that ocfs is consistently better than ig and chi with smaller computation time especially when the reduced dimension is extremely small 
in recent year different commercial weblog subscribing system have been proposed to return story from user subscribed feed in this paper we propose a novel clustering based r aggregator called a r clusgator system rcs for weblog reading note that an r feed may have several different topic a user may only be interested in a subset of these topic in addition there could be many different story from multiple r feed which discus similar topic from different perspective a user may be interested in this topic but do not know how to collect all feed related to this topic in contrast to many previous work we cluster all story in r feed into hierarchical structure to better serve the reader through this way user can easily find all their interested story to make the system current we propose a flexible time window for incremental clustering rcs utilizes both link information and content information for efficient clustering experiment show the effectiveness of rcs 
in this paper we describe an application pubcloud that us tagclouds for the summarization of result from query over thepubmed database of biomedical literature pubcloud responds toqueries of this database with tag cloud generated from wordsextracted from the abstract returned by the query the result ofa user study comparing the pubcloud tag cloud summarization ofquery result with the standard result list provided by pubmedindicated that the tag cloud interface is advantageous in presenting descriptive information and in reducing user frustrationbut that it is le effective at the task of enabling user to discoverrelations between concept 
moore s law and the wave of technology it enabled have led to tremendous improvement in productivity and the quality of life in the industrialized world yet technology ha had almost no effect on the four billion people that make le u day in this talk i argue that the decreasing cost of computing and wireless networking make this the right time to spread the benefit of technology and that the biggest missing piece is a lack of focus on the problem that matter including health education and government after covering some example application that have shown very high impact i take an early look at the research agenda for developing region finally i examine some of the pragmatic issue required to make progress on these very challenging problem my goal is to convince high tech researcher that technology for developing region is an important and viable research topic 
in this work we propose a method that retrieves a list of related query given an initial input query the related query are based on the query log of previously issued query by human user which can be discovered using our improved association rule mining model user can use the suggested related query to tune or redirect the search process our method not only discovers the related query but also rank them according to the degree of their relatedness unlike many other rival technique it exploit only limited query log information and performs relatively better on query in all frequency division 
we investigate the diverse goal that people have when they issue the same query to a search engine and the ability of current search engine to address such diversity we quantify the potential value of personalizing search result based on this analysis great variance wa found in the result that different individual rated a relevant for the same query even when the same information goal wa expressed our analysis suggests that while search engine do a good job of ranking result to maximize global happiness they do not do a very good job for specific individual 
the result of the web query log analysis may be significantly shifted depending on the fraction of agent non human client which are not excluded from the log to detect and exclude agent the web log study use threshold value for a number of request submitted by a client during the observation period however different study use different observation period and a threshold assigned to one period is usually incomparable with the threshold assigned to the other period we propose the uniform method equally working on the different observation period the method base on the sliding window technique a threshold is assigned to the sliding window rather than to the whole observation period besides we determine the sub optimal value of the parameter of the method a window size and a threshold and recommend unique query a an upper bound of the threshold for hour sliding window 
the primary aim of xml element retrieval is to return to user xml element rather than whole document this poster describes a small study in which we elicited user expectation i e their anticipated experience when interacting with an xml retrieval system a compared to a traditional flat document retrieval system 
performance evaluation is an important issue in web search engine research traditional evaluation method rely on much human effort and are therefore quite time consuming with click through data analysis we proposed an automatic search engine performance evaluation method this method generates navigational type query topic and answer automatically based on search user querying and clicking behavior experimental result based on a commercial chinese search engine s user log show that the automatically method get a similar evaluation result with traditional assessor based one 
peer to peer p p web search ha gained a lot of interest lately due to the salient characteristic of p p system namely scalability fault tolerance and load balancing however the lack of global knowledge in a vast and dynamically evolving environment like the web present a grand challenge for organizing content and providing efficient searching semantic overlay network son have been proposed a an approach to reduce cost and increase quality of result and in this paper we present an unsupervised approach for distributed and decentralized son construction aiming to support efficient search mechanism in unstructured p p system 
on an abstract level xml schema increase the limited expressive power of document type definition dtds by extending them with a recursive typing mechanism however an investigation of the xml schema definition xsds occurring in practice reveals that the vast majority of them are structurally equivalent to dtds this might be due to the complexity of the xml schema specification and the difficulty to understand the effect of constraint on typing and validation of schema to shed some light on the actual expressive power of xsds this paper study the impact of the element declaration consistent edc and the unique particle attribution upa rule an equivalent formalism based on contextual pattern rather than on recursive type is proposed which might serve a a light weight front end for xml schema finally the effect of edc and upa on the way xml document can be typed is discussed it is argued that a cleaner more robust stronger but equally efficient class is obtained by replacing edc and upa with the notion of pas preorder typing schema that allow to determine the type of an element of a streaming document when it opening tag is met this notion can be defined in term of restrained competition regular expression and there is again an equivalent syntactical formalism based on contextual pattern 
query by semantic description qbsd is a natural paradigm for retrieving content from large database of music a major impediment to the development of good qbsd system for music information retrieval ha been the lack of a cleanly labeled publicly available heterogeneous data set of song and associated annotation we have collected the computer audition lab song cal data set by having human listen to and annotate song using a survey designed to capture semantic association between music and word we adapt the supervised multi class labeling sml model which ha shown good performance on the task of image retrieval and use the cal data to learn a model for music retrieval the model parameter are estimated using the weighted mixture hierarchy expectation maximization algorithm which ha been specifically designed to handle real valued semantic association between word and song rather than binary class label the output of the sml model a vector of class conditional probability can be interpreted a a semantic multinomial distribution over a vocabulary by also representing a semantic query a a query multinomial distribution we can quickly rank order the song in a database based on the kullback leibler divergence between the query multinomial and each song s semantic multinomial qualitative and quantitative result demonstrate that our sml model can both annotate a novel song with meaningful word and retrieve relevant song given a multi word text based query 
often scientist seek to search for article on the web related to a particular chemical when a scientist search for a chemical formula using a search engine today she get article where the exact keyword string expressing the chemical formula is found searching for the exact occurrence of keywords during searching result in two problem for this domain a if the author search for ch and the article ha h c the article is not returned and b ambiguous search like he return all document where helium is mentioned a well a document where the pronoun he occurs to remedy these deficiency we propose a chemical formula search engine to build a chemical formula search engine we must solve the following problem extract chemical formula from text document index chemical formula and designranking function for the chemical formula furthermore query model are introduced for formula search and for each a scoring scheme based on feature of partial formula is proposed tomeasure the relevance of chemical formula and query we evaluate algorithm for identifying chemical formula in document using classification method based on support vector machine svm and a probabilistic model based on conditional random field crf different method for svm and crf to tune the trade off between recall and precision forim balanced data are proposed to improve the overall performance a feature selection method based on frequency and discrimination isused to remove uninformative and redundant feature experiment show that our approach to chemical formula extraction work well especially after trade off tuning the result also demonstrate that feature selection can reduce the index size without changing ranked query result much 
in this paper we present a novel multi webpage summarization algorithm it add the graph based ranking algorithm into the framework of maximum marginal relevance mmr method to not only capture the main topic of the web page but also eliminate the redundancy existing in the sentence of the summary result the experiment result indicates that the new approach ha the better performance than the previous method 
in this paper we introduce our multi metamodel process ontology m po which is based on various existing reference model and language from the workflow and choreography domain this ontology allows the extraction of arbitrary choreography interface description from arbitrary internal workflow model we also report on an initial validation we translate an ibm websphere mq workflow model into the m po ontology and then extract an abstract bpel model from the ontology 
expanding a seed set into a larger community is a common procedure in link based analysis we show how to adapt recent result from theoretical computer science to expand a seed set into a community with small conductance and a strong relationship to the seed while examining only a small neighborhood of the entire graph we extend existing result to give theoretical guarantee that apply to a variety of seed set from specied community we also describe simple and exible heuristic for applying these method in practice and present early experiment showing that these method compare favorably with existing approach 
in this paper we present an analysis of http traffic captured from internet caf s and kiosk from two different developing country cambodia and ghana this paper ha two main contribution the first contribution is a analysis of the characteristic of the web trace including the distribution and classification of the web object requested by the user we outline notable feature of the data set which effect the performance of the web for user in developing region using the trace data we also perform several simulation analysis of cache performance including both traditional caching and more novel off line caching proposal the second contribution is a set of suggestion on mechanism to improve the user experience of the web in these region these mechanism include both application of well known research technique a well a offering some le well studied suggestion based on intermittent connectivity 
we address the problem of measuring global quality metric of search engine like corpus size index freshness and density of duplicate in the corpus the recently proposed estimator for such metric suffer from significant bias and or poor performance due to inaccurate approximation of the so called document degree we present two new estimator that are able to overcome the bias introduced by approximate degree our estimator are based on a careful implementation of an approximate importance sampling procedure comprehensive theoretical and empirical analysis of the estimator demonstrates that they have essentially no bias even in situation where document degree are poorly approximated building on an idea from we discus rao blackwellization a a generic method for reducing variance in search engine estimator we show that rao blackwellizing our estimator result in significant performance improvement while not compromising accuracy 
requirement engineering re is emerging a an increasingly important discipline for supporting web application development a these are designed to satisfy diverse stakeholder need additional functional information multimedia and usability requirement a compared to traditional software application moreover when considering innovative e commerce application value based re is an extremely relevant methodology which exploit the concept of economic value during the re activity in contrast most of the methodology proposed for the development of web application primarily focus on the system design and paying le attention to the re and specifically to value based re focusing this aspect the paper present integration of value based re model to webml model using our recently proposed vip business modeling framework we also analyze the framework s potential in linking other modeling approach and argue about it significant integration potential with various e r oo based process aware web modeling approach 
this paper present an experimental study of user assessing the quality of google web search result in particular we look at how user satisfaction correlate with the effectiveness of google a quantified by ir measure such a precision and the suite of cumulative gain measure cg dcg ndcg result indicate strong correlation between user satisfaction cg and precision moderate correlation with dcg with perhaps surprisingly negligible correlation with ndcg the reason for the low correlation with ndcg are examined 
we present a load generator and performance measurement tool autoperf which requires minimal input and configuration from the user and produce a comprehensive capacity analysis a well a server side resource usage profile of a web based distributed system in an automated fashion the tool requires only the workload and deployment description of the distributed system and automatically set typical parameter that load generator program need such a maximum number of user to be emulated number of user for each experiment warm up time etc the tool also doe all the co ordination required to generate a critical type of measure namely resource usage per transaction or per user for each software server this is a necessary input for creating a performance model of a software system 
we present a family of measure of proximity of an arbitrary node in a directed graph to a pre specified subset of node called the anchor our measure are based on three dierent propagation scheme and two dierent us of the connectivity structure of the graph we consider a web specific application of the above measure with two disjoint anchor good and bad web page and study the accuracy of these measure in this context 
in the proposed phd thesis it will be examined how attention data from the user can be exploited in order to enhance and personalize information retrieval up to now nearly all implicit feedback source that are used for information retrieval are based on mouse and keyboard input like clickthrough scrolling and annotation behavior in this work an unobtrusive eye tracker will be used a an attention evidence source being able to precisely detect read or skimmed document passage this information will be stored in attention annotated document e g containing read skimmed highlighted commented passage based on such annotated document the user s current thematic context will be estimated first this attention based context will be utilized for attention based change concerning the index of vector space based ir method it is intended to regard the context a virtual document which will be included in the index in this way local desktop or enterprisewide search engine could be enhanced by allowing new type of query e g find a set of document on my computer concerning the current topic that i have formerly used in a similar context second the thematic context will be utilized for preand postprocessing step concerning the retrieval process e g for query expansion and result reranking therefore attention enhanced keyword extraction technique will be developed that take the degree of attention from the user into account 
today the web is increasingly used a a platform for distributed service which transcend organizational boundary to form federated application consequently there is a growing interest in the architectural aspect of web based system i e the composition of the overall solution into individual web application and web service from different party the design and evolution of federated system call for model that give an overview of the structural a well a trust specific composition and reflect the technical detail of the various access we introduce the webcomposition architecture model wam a an overall modeling approach tailored to aspect of highly distributed system with federation a an integral factor 
clio is an existing schema mapping tool that provides user friendly mean to manage and facilitate the complex task of transformation and integration of heterogeneous data such a xml over the web or in xml database by mean of mapping from source to target schema clio can help user conveniently establish the precise semantics of data transformation and integration in this paper we study the problem of how to efficiently implement such data transformation i e generating target data from the source data based on schema mapping we present a three phase framework for high performance xml to xml transformation based on schema mapping and discus methodology and algorithm for implementing these phase in particular we elaborate on novel technique such a streamed extraction of mapped source value and scalable disk based merging of overlapping data including duplicate elimination we compare our transformation framework with alternative method such a using xquery or sql xml provided by current commercial database the result demonstrate that the three phase framework although a simple a it is is highly scalable and outperforms the alternative method by order of magnitude 
on the investigation of linguistic technique used in ontology matching we propose a new idea of virtual document to pursue a cost effective approach to linguistic matching in this paper basically a a collection of weighted word the virtual document of a uriref declared in an ontology contains not only the local description but also the neighboring information to reflect the intended meaning of the uriref document similarity can be computed by traditional vector space technique and then be used in the similarity based approach to ontology matching in particular the rdf graph structure is exploited to define the description formulation and the neighboring operation experimental result show that linguistic matching based on the virtual document is dominant in average f measure a compared to other three approach it is also demonstrated by our experiment that the virtual document approach is cost effective a compared to other linguistic matching approach 
expertise retrieval ha been largely unexplored on data other than the w c collection at the same time many intranet of university and other knowledge intensive organisation offer example of relatively small but clean multilingual expertise data covering broad range of expertise area we first present two main expertise retrieval task along with a set of baseline approach based on generative language modeling aimed at finding expertise relation between topic and people for our experimental evaluation we introduce and release a new test set based on a crawl of a university site using this test set we conduct two series of experiment the first is aimed at determining the effectiveness of baseline expertise retrieval method applied to the new test set the second is aimed at assessing refined model that exploit characteristic feature of the new test set such a the organizational structure of the university and the hierarchical structure of the topic in the test set expertise retrieval model are shown to be robust with respect to environment smaller than the w c collection and current technique appear to be generalizable to other setting 
collection selection ranking collection according to user query is crucial in distributed search however few feature are used to rank collection in the current collection selection method while hundred of feature are exploited to rank web page in web search the lack of feature affect the efficiency of collection selection in distributed search in this paper we exploit some new feature and learn to rank collection with them through svm and rankingsvm respectively experimental result show that our feature are beneficial to collection selection and the learned ranking function outperform the classical cori algorithm 
the availability of map interface and location aware device make a growing amount of unstructured geo referenced information available on the web in particular over twelve million geo referenced photo are now available on flickr a popular photo sharing website we show a method to analyze the flickr data and generate aggregate knowledge in the form of representative tag for arbitrary area in the world we display these tag on a map interface in an interactive web application along with image associated with each tag we then use the implicit feedback of the aggregate user interaction with the tag and image to learn which image best describe the area shown on the map 
a collaborative crawler is a group of crawling node in which each crawling node is responsible for a specic portion of the web we study the problem of collecting geographically aware page using collaborative crawling strategy we rst propose several collaborative crawling strategy for the geographically focused crawling whose goal is to collect web page about specied geographic location by considering feature like url address of page content of page extended anchor text of link and others later we propose various evaluation criterion to qualify the performance of such crawling strategy finally we experimentally study our crawling strategy by crawling the real web data showing that some of our crawling strategy greatly outperform the simple url hash based partition collaborative crawling in which the crawling assignment are determined according to the hash value computation over url more precisely feature like url address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling 
this paper report a new general framework of focused web crawling based on relational subgroup discovery predicate are used explicitly to represent the relevance clue of those unvisited page in the crawl frontier and then firstorder classification rule are induced using subgroup discovery technique the learned relational rule with sufficient support and confidence will guide the crawling process afterwards we present the many interesting feature of our proposed first order focused crawler together with preliminary promising experimental result category and subject descriptor h information interface and presentation hypertext hypermedia i artificial intelligence learning 
ranking is a very important topic in information retrieval while algorithm for learning ranking model have been intensively studied this is not the case for feature selection despite of it importance the reality is that many feature selection method used in classification are directly applied to ranking we argue that because of the striking difference between ranking and classification it is better to develop different feature selection method for ranking to this end we propose a new feature selection method in this paper specifically for each feature we use it value to rank the training instance and define the ranking accuracy in term of a performance measure or a loss function a the importance of the feature we also define the correlation between the ranking result of two feature a the similarity between them based on the definition we formulate the feature selection issue a an optimization problem for which it is to find the feature with maximum total importance score and minimum total similarity score we also demonstrate how to solve the optimization problem in an efficient way we have tested the effectiveness of our feature selection method on two information retrieval datasets and with two ranking model experimental result show that our method can outperform traditional feature selection method for the ranking task 
abstract we consider the problem of visualizing the evolution of tag within the flickr flickr com online image sharing community any user of the flickr service may append a tag to any photo in the system over the past year user have on average added over a million tag each week understanding the evolution of these tag over time is therefore a challenging task we present a new approach based on a characterization of the most interesting tag associated with a sliding interval of time an animation provided via flash in a web browser allows the user to observe and interact with the interesting tag a they evolve over time new algorithm and data structure are required to support the ecient generation of this visualization we combine a novel solution to an interval covering problem with extension to previous work on score aggregation in order to create an ecient backend system capable of producing visualization at arbitrary scale on this large dataset in real time category and subject descriptor 
we present a method for acquiring ontological knowledge using search query log we first use query log to identify important context associated with term belonging to a semantic category we then use these context to harvest new word belonging to this category our evaluation on selected category indicates that the method work very well to help harvesting term achieving to accuracy in categorizing newly acquired term 
a common perception is that there are two competing vision for the future evolution of the web the semantic web and web a closer look though reveals that the core technology and concern of these two approach are complementary and that each field can and must draw from the other s strength we believe that future web application will retain the web focus on community and usability while drawing on semantic web infrastructure to facilitate mashup like information sharing however there are several open issue that must be addressed before such application can become commonplace in this paper we outline a semantic weblogs scenario that illustrates the potential for combining web and semantic web technology while highlighting the unresolved issue that impede it realization nevertheless we believe that the scenario can be realized in the short term we point to recent progress made in resolving each of the issue a well a future research direction for each of the community 
the web mashup scripting language wmsl enables an end user you working from his browser e g not needing any other infrastructure to quickly write mashups that integrate any two or more web service on the web the end user accomplishes this by writing a web page that combine html metadata in the form of mapping relation and small piece of code or script the mapping relation enable not only the discovery and retrieval of the wmsl page but also affect a new programming paradigm that abstract many programming complexity from the script writer furthermore the wmsl web page or script that disparate end user you write can be harvested by crawler to automatically generate the concept needed to build lightweight ontology containing local semantics of a web service and it data model to extend context ontology or middle ontology and to develop link or mapping between these ontology this enables an open source model of building ontology based on the wmsl web page or script that end user you write 
we argue that the ability to identify pair of related term is at the heart of what make spectral retrieval work in practice scheme such a latent semantic indexing lsi and it descendant have this ability in the sense that they can be viewed a computing a matrix of term term relatedness score which is then used to expand the given document not the query for almost all existing spectral retrieval scheme this matrix of relatedness score depends on a fixed low dimensional subspace of the original term space we instead vary the dimension and study for each term pair the resultin curve of relatedness score we find that it is actually the shape of this curve which is indicative for the term pair relatedness and not any of the individual relatedness score on the curve we derive two simple parameterless algorithm that detect this shape and that consistently outperform previous method on a number of test collection our curve also shed light on the effectiveness of three fundamental type of variation of the basic lsi scheme 
we consider the problem of efficiently sampling web search engine query result in turn using a small random sample instead of the full set of result lead to efficient approximate algorithm for several application such a determining the set of category in a given taxonomy spanned by the search result finding the range of metadata value associated to the result set in order to enable multi faceted search estimating the size of the result set data mining association to the query term we present and analyze an efficient algorithm for obtaining uniform random sample applicable to any search engine based on posting list and document at a time evaluation to our knowledge all popular web search engine e g google inktomi altavista alltheweb belong to this class furthermore our algorithm can be modified to follow the modern object oriented approach whereby posting list are viewed a stream equipped with a next method and the next method for boolean and other complex query is built from the next method for primitive term in our case we show how to construct a basic next p method that sample term posting list with probability p and show how to construct next p method for boolean operator and or wand from primitive method finally we test the efficiency and quality of our approach on both synthetic and real world data 
a video blogging system ha been developed for easily producing your own video program that can be made available to the public in much the same way that blog are created the user merely type a program script on a webpage the same a creating a blog selects a direction style and paste in some additional material content to create a cg based video program that can be openly distributed to the general public the script direction style and material content are automatically combined to create a movie file on the server side the movie file can then be accessed by referring to an r feed and viewed on the screen of various device 
we describe a novel system for evaluating and performing stream based text categorization stream based text categorization considers the text being categorized a a stream of symbol which differs from the traditional feature based approach which relies on extracting feature from the text the system implement character based language model specifically model based on the ppm text compression scheme a well a count based measure such a r measure and c measure use of the system demonstrates that all of these technique outperform svm a feature based classifier at stream related classification task such a authorship ascription 
translation for cross language information retrieval need not be word based we show that character n gram in one language can be translated into character n gram of another language we demonstrate that such translation produce retrieval result on par with and often exceeding those of word based and stem based translation 
search engine are the primary gateway of information access on the web today behind the scene search engine crawl the web to populate a local indexed repository of web page used to answer user search query in an aggregate sense the web is very dynamic causing any repository of web page to become out of date over time which in turn cause query answer quality to degrade given the considerable size dynamicity and degree of autonomy of the web a a whole it is not feasible for a search engine to maintain it repository exactly synchronized with the web in this paper we study how to schedule web page for selective re downloading into a search engine repository the scheduling objective is to maximize the quality of the user experience for those who query the search engine we begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live web impact the quality of the user experience this characterization lead to a user centric metric of the quality of a search engine s local repository we use this metric to derive a policy for scheduling web page re downloading that is driven by search engine usage and free of exterior tuning parameter we then focus on the important subproblem of scheduling refreshing of web page already present in the repository and show how to compute the priority efficiently we provide extensive empirical comparison of our user centric method against prior web page refresh strategy using real web data our result demonstrate that our method requires far fewer resource to maintain same search engine quality level for user leaving substantially more resource available for incorporating new web page into the search repository 
in recent year document clustering ha been receiving more and more attention a an important and fundamental technique for unsupervised document organization automatictopic extraction and fast information retrieval or filtering in this paper we propose a novel method for clustering document using regularization unlike traditional globally regularized clustering method our method first construct a local regularized linear label predictor for each document vector and then combine all those local regularizers with a global smoothness regularizer so we call our algorithm clustering with local and global regularization clgr we will show that the cluster membership of the document can be achieved by eigenvalue decomposition of a sparse symmetric matrix which can be efficiently solved by iterative method finally our experimental evaluation on several datasets are presented to show the superiority of clgr over traditional document clustering method 
a web page may be relevant to multiple topic even when nominally on a single topic the page may attract attention and thus link from multiple community instead of indiscriminately summing the authority provided by all page we decompose a web page into separate subnodes with respect to each community pointing to it by considering the relevance of these community we are able to better model the query specific reputation for each p otential result we apply a total of query to the trec gov dataset to demonstrate how the use of community relevance can improve ranking performance 
we present an empirical evaluation and comparison of two content extraction method in html absolute xpath expression and relative xpath expression we argue that the relative xpath expression although not widely used should be used in preference to absolute xpath expression in extracting content from human created web document evaluation of robustness cover four thousand query executed on several hundred webpage we show that in referencing part of real world dynamic html document relative xpath expression are on average significantly more robust than absolute xpath one 
the central problem for many application in information retrieval is ranking and learning to rank is considered a a promising approach for addressing the issue ranking svm for example is a state of the art method for learning to rank and ha been empirically demonstrated to be effective in this paper we study the issue of learning to rank particularly the approach of using svm technique to perform the task we point out that although ranking svm is advantageous it still ha shortcoming ranking svm employ a single hyperplane in the feature space a the model for ranking which is too simple to tackle complex ranking problem furthermore the training of ranking svm is also computationally costly in this paper we look at an alternative approach to ranking svm which we call multiple hyperplane ranker mhr and make comparison between the two approach mhr take the divide and conquer strategy it employ multiple hyperplanes to rank instance and finally aggregate the ranking by the hyperplanes mhr contains ranking svm a a special case and mhr can overcome the shortcoming which ranking svm ha experimental result on two information retrieval datasets show that mhr can outperform ranking svm in ranking 
object oriented programming is the current mainstream programming paradigm but existing rdf apis are mostly triple oriented traditional technique for bridging a similar gap between relational database and object oriented program cannot be applied directly given the different nature of semantic web data for example in the semantics of class membership inheritance relation and object conformance to schema we present activerdf an object oriented api for managing rdf data that offer full manipulation and querying of rdf data doe not rely on a schema and fully conforms to rdf s semantics activerdf can be used with different rdf data store adapter have been implemented to generic sparql endpoint sesame jena redland and yars and new adapter can be added easily in addition integration with the popular ruby on rail framework enables rapid development of semantic web application 
while many work have been devoted to service matchmaking and modeling nonfunctional property the problem of matching service request to offer in an optimal way ha not yet been extensively studied in this paper we formalize three kind of optimal service selection problem based on different criterion then we study their complexity and implement solution we prove that one time cost make the optimal selection problem computationally hard in the absence of these cost the problem can be solved in polynomial time we designed and implemented both exact and heuristic suboptimal algorithm for the hard case and carried out a preliminary experimental evaluation with interesting result 
in xml database new schema version may be released a frequently a once every two week this poster describes a taxonomy of change for xml schema evolution it examines the impact of those change on schema validation and query evaluation based on that study it proposes guideline for xml schema evolution and for writing query in such a way that they continue to operate a expected across evolving schema 
we consider the problem of document indexing and representation recently locality preserving indexing lpi wa proposed for learning a compact document subspace different from latent semantic indexing which is optimal in the sense of global euclidean structure lpi is optimal in the sense of local manifold structure however lpi is extremely sensitive to the number of dimension this make it difficult to estimate the intrinsic dimensionality while inaccurately estimated dimensionality would drastically degrade it performance one reason leading to this problem is that lpi is non orthogonal non orthogonality distorts the metric structure of the document space in this paper we propose a new algorithm called orthogonal lpi orthogonal lpi iteratively computes the mutually orthogonal basis function which respect the local geometrical structure moreover our empirical study show that olpi can have more locality preserving power than lpi we compare the new algorithm to lsi and lpi extensive experimental result show that orthogonal lpi obtains better performance than both lsi and lpi more crucially it is insensitive to the number of dimension which make it an efficient data preprocessing method for text clustering classification retrieval etc 
in this research we focus on tracking topic that originate and evolve from a specific event intuitively a few key element of a target event such a date location and person involved would be enough for making a decision on whether a test story is on topic consequently a profile based event tracking method is proposed we attempt to build an event profile from the given on topic story by robust information retrieval technology a feature selection metric and a recognized event clause are utilized to determine most if not all key semantic element of the target event preliminary experiment on the tdt mandarin corpus show that this profile based event tracking method is promising 
content personalization is a very important aspect in the field of e learning although current standard do not fully support it in this paper we outline an extension to the adl scorm sharable content object reference model standard in an effort to permit a suitable adaptivity based on user s characteristic applying this extension we can create adaptable course which should be personalized before shown to the student 
the relevant in context retrieval task is document or article retrieval with a twist where not only the relevant article should be retrieved but also the relevant information within each article captured by a set of xml element should be correctly identified our main research question is how to evaluate the relevant in context task we propose a generalized average precision measure that meet two main requirement i the score reflects the ranked list of article inherent in the result list and at the same time ii the score also reflects how well the retrieved information per article i e the set of element corresponds to the relevant information the resulting measure wa used at inex 
in this research we investigate whether one can model online searching a a learning paradigm we examined the searching characteristic of participant engaged in searching task we classified the searching task according to anderson and krathwohl s taxonomy an updated version of bloom s taxonomy anderson and krathwohl is a six level categorization of cognitive learning research result show that applying take the most searching effort a measured by query per session and specific topic searched per session the category of remembering and understanding which are lower order learning level exhibit searching characteristic similar to the higher order category of evaluating and creating it seems that searcher rely primarily on their internal knowledge and use searching primarily a fact checking and verification when engaged in evaluating and creating implication are that the commonly held notion of web searcher having simple information goal may not be correct we discus the implication for web searching including designing interface to support exploration and alternate view 
many application in analytical domain often have the need to connect the dot i e query about the structure of data in bioinformatics for example it is typical to want to query about interaction between protein the aim of such query is to extract relationship between entity i e path from a data graph often such query will specify certain constraint that qualifying result must satisfy e g path involving a set of mandatory node unfortunately most present day semantic web query language including the current draft of the anticipated recommendation sparql lack the ability to express query about arbitrary path structure in data in addition many system that support some limited form of path query rely on main memory graph algorithm limiting their applicability to very large scale graph in this paper we present an approach for supporting path extraction query our proposal comprises i a query language sparq l which extends sparql with path variable and path variable constraint expression and ii a novel query evaluation framework based on efficient algebraic technique for solving path problem which allows for path query to be efficiently evaluated on disk resident rdf graph the effectiveness of our proposal is demonstrated by a performance evaluation of our approach on both real world based and synthetic dataset 
mobile spam in an increasing threat that may be addressed using filtering system like those employed against email spam we believe that email filtering technique require some adaptation to reach good level of performance on sm spam especially regarding message representation in order to test this assumption we have performed experiment on sm filtering using top performing email spam filter on mobile spam message using a suitable feature representation with result supporting our hypothesis 
large scale web and text retrieval system deal with amount of data that greatly exceed the capacity of any single machine to handle the necessary data volume and query throughput rate parallel system are used in which the document and index data are split across tightly clustered distributed computing system the index data can be distributed either by document or by term in this paper we examine method for load balancing in term distributed parallel architecture and propose a suite of technique for reducing net querying cost in combination the technique we describe allow a improvement in query throughput when tested on an eight node parallel computer system 
in interactive question answering qa user and system take turn to ask question and provide answer in such an interactive setting user question largely depend on the answer provided by the system one question is whether user follow up question can provide feedback for the system to automatically ass it performance e g ass whether a correct answer is delivered this self awareness can make qa system more intelligent for information seeking for example by adapting better strategy to cope with problematic situation therefore this paper describes our initial investigation in addressing this problem our result indicate that interaction context can provide useful cue for automated performance assessment in interactive qa 
exhaustive evaluation of ranked query can be expensive particularly when only a small subset of the overall ranking is required or when query contain common term this concern give rise to technique for dynamic query pruning that is method for eliminating redundant part of the usual exhaustive evaluation yet still generating a demonstrably good enough set of answer to the query in this work we propose new pruning method that make use of impact sorted index compared to exhaustive evaluation the new method reduce the amount of computation performed reduce the amount of memory required for accumulator reduce the amount of data transferred from disk and at the same time allow performance guarantee in term of precision and mean average precision these strong claim are backed by experiment using the trec terabyte collection and query 
an xml range query may impose predicate on the numerical or textual content of the element and or their respective path structure in order to handle content and structure range query efficiently an xml query processing engine need to incorporate effective indexing and summarization technique to efficiently partition the xml document and locate the result in this paper we propose a dynamic summarization and indexing method flux based on bloom filter and b tree to tackle these problem the result of our extensive experimental evaluation indicated the efficiency of the proposed system 
phishing is a significant problem involving fraudulent email and web site that trick unsuspecting user into revealing private information in this paper we present the design implementation and evaluation of cantina a novel content based approach to detecting phishing web site based on the tf idf information retrieval algorithm we also discus the design and evaluation of several heuristic we developed to reduce false positive our experiment show that cantina is good at detecting phishing site correctly labeling approximately of phishing site 
the class imbalance problem ha been known to hinder the learning performance of classification algorithm various real world classification task such a text categorization suffer from this phenomenon we demonstrate that active learning is capable of solving the problem 
search engine can record which document were clicked for which query and use these query document pair a soft relevance judgment however compared to the true judgment click log give noisy and sparse relevance information we apply a markov random walk model to a large click log producing a probabilistic ranking of document for a given query a key advantage of the model is it ability to retrieve relevant document that have not yet been clicked for that query and rank those effectively we conduct experiment on click log from image search comparing our backward random walk model to a different forward random walk varying parameter such a walk length and self transition probability the most effective combination is a long backward walk with high self transition probability 
a minimal perfect function map a static set of n key on to the range of integer n we present a scalable high performance algorithm based on random graph for constructing minimal perfect hash function mphfs for a set of n key our algorithm output a description of h in expected time o n the evaluation of h x requires three memory access for any key x and the description of h take up n byte n bit this is the best most space efficient known result to date using a simple heuristic and huffman coding the space requirement is further reduced to n byte n bit we present a high performance architecture that is easy to parallelize and scale well to very large data set encountered in internet search application experimental result on a one billion url dataset obtained from live search crawl data show that the proposed algorithm a find an mphf for one billion url in le than minute and b requires only bit key for the description of h 
keyword search for smallest lowest common ancestor slcas in xml data ha recently been proposed a a meaningful way to identify interesting data node inxml data where their subtrees contain an input set of keywords in this paper we generalize this useful search paradigm to support keyword search beyond the traditional and semantics to include both and and or boolean operator a well we first analyze property of the lca computation and propose improved algorithm to solve the traditional keyword search problem with only and semantics we then extend our approach to handle general keyword search involving combination of and and or boolean operator the effectiveness of our new algorithm is demonstrated with a comprehensive experimental performance study 
in this paper we study the medium workload collected from a large number of commercial web site hosted by a major isp and that collected from a large group of home user connected to the internet via a well known cable company some of our key finding are surprisingly the majority of medium content are still delivered via downloading from web server a substantial percentage of medium downloading connection are aborted before completion due to the long waiting time a hybrid approach pseudo streaming is used by client to imitate real streaming the mismatch between the downloading rate and the client playback speed in pseudo streaming is common which either cause frequent playback delay to the client or unnecessary traffic to the internet compared with streaming downloading and pseudo streaming are neither bandwidth efficient nor performance effective to address this problem we propose the design of autostream an innovative system that can provide additional previewing and streaming service automatically for medium object hosted on standard web site in server farm at the client s will 
in this note we consider a simple reformulation of the traditional power iteration algorithm for computing the stationary distribution of a markov chain rather than communicate their current probability value to their neighbor at each step node instead communicate only change in probability value this reformulation enables a large degree of flexibility in the manner in which node update their value leading to an array of optimization and feature including faster convergence efficient incremental updating and a robust distributed implementation while the spirit of many of these optimization appear in previous literature we observe several case where this unification simplifies previous work removing technical complication and extending their range of applicability we implement and measure the performance of several optimization on a sizable m node web subgraph seeing significant composite performance gain especially for the case of incremental recomputation after change to the web graph 
we propose that the information access behavior of a group of people can be modeled a an information flow issue in which people intentionally or unintentionally influence and inspire each other thus creating an interest in retrieving or getting a specific kind of information or product information flow model how information is propagated in a social network it can be a real social network where interaction between people reside it can be moreover a virtual social network in that people only influence each other unintentionally for instance through collaborative filtering we leverage user access pattern to model information flow and generate effective personalized recommendation first an early adoption based information flow eabif network describes the influential relationship between people second based on the fact that adoption is typically category specific we propose a topic sensitive eabif teabif network in which access pattern are clustered with respect to the category once an item ha been accessed by early adopter personalized recommendation are achieved by estimating whom the information will be propagated to with high probability in our experiment with an online document recommendation system the result demonstrate that the eabif and the teabif can respectively achieve an improved precision recall of and compared to traditional collaborative filtering given an early adopter exists 
we present a novel framework for answering complex question that relies on question decomposition complex question are decomposed by a procedure that operates on a markov chain by following a random walk on a bipartite graph of relation established between concept related to the topic of a complex question and subquestions derived from topic relevant passage that manifest these relation decomposed question discovered during this random walk are then submitted to a state of the art question answering q a system in order to retrieve a set of passage that can later be merged into a comprehensive answer by a multi document summarization md system in our evaluation we show that access to the decomposition generated using this method can signicantly enhance the relevance and comprehensiveness of summarylength answer to complex question 
this paper introduces a family of link based ranking algorithm that propagate page importance through link in these algorithm there is a damping function that decrease with distance so a direct link implies more endorsement than a link through a long path pagerank is the most widely known ranking function of this family the main objective of this paper is to determine whether this family of ranking technique ha some interest per se and how different choice for the damping function impact on rank quality and on convergence speed even though our result suggest that pagerank can be approximated with other simpler form of ranking that may be computed more efficiently our focus is of more speculative nature in that it aim at separating the kernel of pagerank that is link based importance propagation from the way propagation decay over path we focus on three damping function having linear exponential and hyperbolic decay on the length of the path the exponential decay corresponds to pagerank and the other function are new our presentation includes algorithm analysis comparison and experiment that study their behavior under different parameter in real web graph data among other result we show how to calculate a linear approximation that induces a page ordering that is almost identical to pagerank s using a fixed small number of iteration comparison were performed using kendall s on large domain datasets 
collection size is an important feature to represent the content summary of a collection and play a vital role in collection selection for distributed search in uncooperative environment collection size estimation algorithm are adopted to estimate the size of collection with their search interface this paper proposes heterogeneous capture hc algorithm in which the capture probability of document are modeled with logistic regression with heterogeneous capture probability hc algorithm estimate collection size through conditional maximum likelihood experimental result on real web data show that our hc algorithm outperforms both multiple capture recapture and capture history algorithm 
research and development of information access technology for scanned paper document ha been hampered by the lack of public test collection of realistic scope and complexity a part of a project to create a prototype system for search and mining of mass of document image we are assembling a terabyte dataset to support evaluation of both end to end complex document information processing cdip task e g text retrieval and data mining a well a component technology such a optical character recognition ocr document structure analysis signature matching and authorship attribution 
one way to help all user of commercial web search engine be more successful in their search is to better understand what those user with greater search expertise are doing and use this knowledge to benefit everyone in this paper we study the interaction log of advanced search engine user and those not so advanced to better understand how these user group search the result show that there are marked difference in the query result click post query browsing and search success of user we classify a advanced based on their use of query operator relative to those classified a non advanced our finding have implication for how advanced user should be supported during their search and how their interaction could be used to help searcher of all experience level find more relevant information and learn improved searching strategy 
large scale learning is often realistic only in a semi supervised setting where a small set of labeled example is available together with a large collection of unlabeled data in many information retrieval and data mining application linear classifier are strongly preferred because of their ease of implementation interpretability and empirical performance in this work we present a family of semi supervised linear support vector classifier that are designed to handle partially labeled sparse datasets with possibly very large number of example and feature at their core our algorithm employ recently developed modified finite newton technique our contribution in this paper are a follows a we provide an implementation of transductive svm tsvm that is significantly more efficient and scalable than currently used dual technique for linear classification problem involving large sparse datasets b we propose a variant of tsvm that involves multiple switching of label experimental result show that this variant provides an order of magnitude further improvement in training efficiency c we present a new algorithm for semi supervised learning based on a deterministic annealing da approach this algorithm alleviates the problem of local minimum in the tsvm optimization procedure while also being computationally attractive we conduct an empirical study on several document classification task which confirms the value of our method in large scale semi supervised setting 
the netherlands had parliamentary election on november we built a system which helped voter to make an informed choice among the many participating party one of the most important piece of information in the dutch election and subsequent coalition government formation is the party program a text document with an average length of page our system provides the voter with focused access to party program enabling her to make a topic wise comparison of party viewpoint we complemented this type of access what do the party promise with access to news what happens around these topic and blog what do people say about them we describe the system including design technical detail and user statistic 
we present rpref our generalization of the bpref evaluation metric for assessing the quality of search engine result given graded rather than binary user relevance judgment 
since the debut of pagerank and hit hyperlink induced web document ranking ha come a long way the web ha become increasingly vast and topically diverse such vastness ha led many into the area of topic sensitive ranking and it variant we address the high dimensionality of the web by providing tool for focused search a focused search engine is one which seek coverage over a subset of topic of the web and present user with relevant search result in a known domain this demonstration will introduce reader to the genieknows com vertical search engine 
a query independent feature relating perhaps to document content linkage or usage can be transformed into a static per document relevance weight for use in ranking the challenge is to find a good function to transform feature value into relevance score this paper present floe a simple density analysis method for modelling the shape of the transformation required based on training data and without assuming independence between feature and baseline for a new query independent feature it address the question is it required for ranking what sort of transformation is appropriate and after adding it how successful wa the chosen transformation based on this we apply sigmoid transformation to pagerank indegree url length and clickdistance tested in combination with a bm baseline 
automated singer identification is important in organising browsing and retrieving data in large music database in this paper we propose a novel scheme called hybrid singer identifier hsi for automated singer recognition hsi can effectively use multiple low level feature extracted from both vocal and non vocal music segment to enhance the identification process with a hybrid architecture and build profile of individual singer characteristic based on statistical mixture model extensive experimental result conducted on a large music database demonstrate the superiority of our method over state of the art approach 
different from traditional information retrieval both content and structure are critical to the success of web information retrieval in recent year many relevance propagation technique have been proposed to propagate content information between web page through web structure to improve the performance of web search in this paper we first propose a generic relevance propagation framework and then provide a comparison study on the effectiveness and efficiency of various representative propagation model that can be derived from this generic framework we come to many conclusion that are useful for selecting a propagation model in real world search application including sitemap based propagation model outperform hyperlink based model in sense of both effectiveness and efficiency and sitemap based term propagation is easier to be integrated into real world search engine because of it parallel offline implementation and acceptable complexity some other more detailed study result are also reported in the paper 
this paper introduces a novel method for automatic annotation of image with keywords from a generic vocabulary of concept or object for the purpose of content based image retrieval an image represented a sequence of feature vector characterizing low level visual feature such a color texture or oriented edge is modeled a having been stochastically generated by a hidden markov model whose state represent concept the parameter of the model are estimated from a set of manually annotated training image each image in a large test collection is then automatically annotated with the a posteriori probability of concept present in it this annotation support content based search of the image collection via keywords various aspect of model parameterization parameter estimation and image annotation are discussed empirical retrieval result are presented on two image collection corel and key frame from trecvid comparison are made with two other recently developed technique on the same datasets 
in this paper we formulate image retrieval by text query a a vector space classification problem this is achieved by creating a high dimensional visual vocabulary that represents the image document in great detail we show how the representation of these image document enables the application of well known text retrieval technique such a rocchio tf idf and na ve bayes to the semantic image retrieval problem we tested these method on a corel image subset and achieve state of the art retrieval performance using the proposed method 
several recent study have demonstrated that the type of improvement in information retrieval system effectiveness reported in forum such a sigir and trec do not translate into a benefit for user two of the study used an instance recall task and a third used a question answering task so perhaps it is unsurprising that the precision based measure of ir system effectiveness on one shot query evaluation do not correlate with user performance on these task in this study we evaluate two different information retrieval task on trec web track data a precision based user task measured by the length of time that user need to find a single document that is relevant to a trec topic and a simple recall based task represented by the total number of relevant document that user can identify within five minute user employ search engine with controlled mean average precision map of between and our result show that there is no significant relationship between system effectiveness measured by map and the precision based task a significant but weak relationship is present for the precision at one document returned metric a weak relationship is present between map and the simple recall based task 
in this poster we describe the study of an interface technique that provides a list of suggested additional query term a a searcher type a search query in effect offering interactive query expansion iqe option while the query is formulated analysis of the result show that offering iqe during query formulation lead to better quality initial query and an increased uptake of query expansion these finding have implication for how iqe should be offered in retrieval interface 
retrospective news event detection red is defined a the discovery of previously unidentified event in historical news corpus although both the content and time information of news article are helpful to red most research focus on the utilization of the content of news article few research work have been carried out on finding better usage of time information in this paper we do some exploration on both direction based on the following two characteristic of news article on the one hand news article are always aroused by event on the other hand similar article reporting the same event often redundantly appear on many news source the former hint a generative model of news article and the latter provides data enriched environment to perform red with consideration of these characteristic we propose a probabilistic model to incorporate both content and time information in a unified framework this model give new representation of both news article and news event furthermore based on this approach we build an interactive red system hiscovery which provides additional function to present event photo story and chronicle 
in this paper we address the issue of learning to rank for document retrieval in the task a model is automatically created with some training data and then is utilized for ranking of document the goodness of a model is usually evaluated with performance measure such a map mean average precision and ndcg normalized discounted cumulative gain ideally a learning algorithm would train a ranking model that could directly optimize the performance measure with respect to the training data existing method however are only able to train ranking model by minimizing loss function loosely related to the performance measure for example ranking svm and rankboost train ranking model by minimizing classification error on instance pair to deal with the problem we propose a novel learning algorithm within the framework of boosting which can minimize a loss function directly defined on the performance measure our algorithm referred to a adarank repeatedly construct weak ranker on the basis of reweighted training data and finally linearly combine the weak ranker for making ranking prediction we prove that the training process of adarank is exactly that of enhancing the performance measure used experimental result on four benchmark datasets show that adarank significantly outperforms the baseline method of bm ranking svm and rankboost 
the inherent ambiguity of short keyword query demand for enhanced method for web retrieval in this paper we propose to improve such web query by expanding them with term collected from each user s personal information repository thus implicitly personalizing the search output we introduce five broad technique for generating the additional query keywords by analyzing user data at increasing granularity level ranging from term and compound level analysis up to global co occurrence statistic a well a to using external thesaurus our extensive empirical analysis under four different scenario show some of these approach to perform very well especially on ambiguous query producing a very strong increase in the quality of the output ranking subsequently we move this personalized search framework one step further and propose to make the expansion process adaptive to various feature of each query a separate set of experiment indicates the adaptive algorithm to bring an additional statistically significant improvement over the best static expansion approach 
mirroring web site is a well known technique commonly used in the web community a mirror site should be updated frequently to ensure that it reflects the content of the original site existing mirroring tool apply page level strategy to check each page of a site which is inefficient and expensive in this paper we propose a novel site level mirror maintenance strategy our approach study the evolution of web directorystructures and mine association rule between ancestor descendant web directory discovered rule indicate the evolution correlation between web directory thus when maintaining the mirror of a web site directory we can optimally skipsubdirectories which are negatively correlated with it in undergoing significant change the preliminary experimental result show that our approach improves the efficiency of the mirror maintenance process significantly while sacrificing slightly in keeping the freshness of the mirror 
disk access performance is a major bottleneck in traditional information retrieval system compared to system memory disk bandwidth is poor and seek time are worse we circumvent this problem by considering query evaluation strategy in main memory we show how new accumulator trimming technique combined with inverted list skipping can produce extremely high performance retrieval system without resorting to method that may harm effectiveness we evaluate our technique using galago a new retrieval system designed for efficient query processing our system achieves a improvement in query throughput over previous method 
we present webpod a portable system that enables mobile user to use the same persistent personalized web browsing session on any internet enabled device no matter what computer is being used webpod provides a consistent browsing session maintaining all of a user s plugins bookmark browser web content open browser window and browser configuration option and preference this is achieved by leveraging rapid improvement in capacity cost and size of portable storage device webpod provides a virtualization and checkpoint restart mechanism that decouples the browsing environment from the host enabling web browsing session to be suspended to portable storage carried around and resumed from the storage device on another computer webpod virtualization also isolates web browsing session from the host protecting the browsing privacy of the user and preventing malicious web content from damaging the host we have implemented a linux webpod prototype and demonstrate it ability to quickly suspend and resume web browsing session enabling a seamless web browsing experience for mobile user a they move among computer 
rdf is increasingly being used to represent metadata rdf site summary r is an application of rdf on the web that ha considerably grown in popularity however the way r system operate today doe not scale well in this paper we introduce g top a scalable publish subscribe system for selective information dissemination g top is particularly well suited for application that deal with large volume content distribution from diverse source r is an instance of the content distribution problem g top allows use of ontology a a way to provide additional information about the data furthermore in this paper we show how g top can support rdfs class taxonomy we have implemented and experimentally evaluated g top and we provide result in the paper demonstrating it scalability compared to alternative 
accurate topical categorization of user query allows for increased effectiveness efficiency and revenue potential in general purpose web search system such categorization becomes critical if the system is to return result not just from a general web collection but from topic specific database a well maintaining sufficient categorization recall is very difficult a web query are typically short yielding few feature per query we examine three approach to topical categorization of general web query matching against a list of manually labeled query supervised learning of classifier and mining of selectional preference rule from large unlabeled query log each approach ha it advantage in tackling the web query classification recall problem and combining the three technique allows u to classify a substantially larger proportion of query than any of the individual technique we examine the performance of each approach on a real web query stream and show that our combined method accurately classifies of query outperforming the recall of the best single approach by nearly with a improvement in overall effectiveness 
in structured information retrieval the aim is to exploit document structure to retrieve relevant component allowing the user to go straight to the relevant material this paper look at the so called best entry point beps which are intended to give the user the best starting point to access the relevant information in the document we examine the relationship between beps and relevant component in the inex ad hoc assessment our main finding are the following first although document are short assessor often choose the best entry point some distance from the start of the document second many of the best entry point coincide with the first relevant character in relevant document showing a strong relation between the bep and relevant text third we find browsing beps in article with a single relevant passage and container beps or context beps in article with more relevant passage 
the web play a critical role in hosting web community their content and interaction a prime example is the open source software os community whose member including software developer and user interact almost exclusively over the web constantly generating sharing and refining content in the form of software code through active interaction over the web on code design and bug resolution process the semantic web is an envisaged extension of the current web in which content is given a well defined meaning through the specification of metadata and ontology increasing the utility of the content and enabling information from heterogeneous source to be integrated we developed a prototype semantic web system for os community dhruv dhruv provides an enhanced semantic interface to bug resolution message and recommends related software object and artifact dhruv us an integrated model of the openacs community the software and the web interaction which is semi automatically populated from the existing artifact of the community 
in this paper we describe new adaptive crawling strategy to e ciently locate the entry point to hidden web source the fact that hidden web source are very sparsely dis tributed make the problem of locating them especially chal lenging we deal with this problem by using the content of page to focus the crawl on a topic by prioritizing promising link within the topic and by also following link that may not lead to immediate benefit we propose a new framework whereby crawler automatically learn pattern of promising link and adapt their focus a the crawl progress thus greatly reducing the amount of required manual setup and tuning our experiment over real web page in a repre sentative set of domain indicate that online learning lead to significant gain in harvest rate the adaptive crawler retrieve up to three time a many form a crawler that use a fixed focus strategy 
in this paper we present an application framework that leverage geospatial content on the world wide web by enabling innovative mode of interaction and novel type of user interface on advanced mobile phone and pda we discus the current development step involved in building mobile geospatial web application and derive three technological pre requisite for our framework spatial query operation based on visibility and field of view a d environment model and a presentationindependent data exchange format for geospatial query result we propose the local visibility model a a suitable xml based candidate and present a prototype implementation 
the success of the semantic web depends on the availability of web page annotated with metadata free form metadata or tag a used in social bookmarking and folksonomies have become more and more popular and successful such tag are relevant keywords associated with or assigned to a piece of information e g a web page describing the item and enabling keyword based classification in this paper we propose p tag a method which automatically generates personalized tag for web page upon browsing a web page p tag produce keywords relevant both to it textual content but also to the data residing on the surfer s desktop thus expressing a personalized viewpoint empirical evaluation with several algorithm pursuing this approach showed very promising result we are therefore very confident that such a user oriented automatic tagging approach can provide large scale personalized metadata annotation a an important step towards realizing the semantic web 
we develop a novel framework for the page level template detection problem our framework is built on two main idea the first is theautomatic generation of training data for a classifier that given apage assigns a templateness score to every dom node of the page the second is the global smoothing of these per node classifier score bysolving a regularized isotonic regression problem the latter follows from a simple yet powerful abstraction of templateness on a page our extensive experiment on human labeled test data show that our approachdetects template effectively 
pagerank is known to be an efficient metric for computing general document importance in the web while commonly used a a one size fit all measure the ability to produce topically biased rank ha not yet been fully explored in detail in particular it wa still unclear to what granularity of topic the computation of biased page rank make sense in this paper we present the result of a thorough quantitative and qualitative analysis of biasing pagerank on open directory category we show that the map quality of biased pagerank generally increase with the odp level up to a certain point thus sustaining the usage of more specialized category to bias pagerank on in order to improve topic specific search 
social network play important role in the semantic web knowledge management information retrieval ubiquitous computing and so on we propose a social network extraction system called polyphonet which employ several advanced technique to extract relation of person detect group of person and obtain keywords for a person search engine especially google are used to measure co occurrence of information and obtain web document several study have used search engine to extract social network from the web but our research advance the following point first we reduce the related method into simple pseudocodes using google so that we can build up integrated system second we develop several new algorithm for social networking mining such a those to classify relation into category to make extraction scalable and to obtain and utilize person to word relation third every module is implemented in polyphonet which ha been used at four academic conference each with more than participant we overview that system finally a novel architecture called super social network mining is proposed it utilizes simple module using google and is characterized by scalability and relate identify process identification of each entity and extraction of relation are repeated to obtain a more precise social network 
automated matching of semantic service description is the key to automatic service discovery and binding but when trying to find a match for a certain request it may often happen that the request cannot be serviced by a single offer but could be handled by combining existing offer in this case automatic service composition is needed although automatic composition is an active field of research it is mainly viewed a a planning problem and treated separatedly from service discovery in this paper we argue that an integrated approach to the problem is better than seperating these issue a is usually done we propose an approach that integrates service composition into service discovery and matchmaking to match service request that ask for multiple connected effect discus general issue involved in describing and matching such service and present an efficient algorithm implementing our idea 
traditionally information retrieval system aim to maximize thenumber of relevant document returned to a user within some windowof the top for that goal the probability ranking principle whichranks document in decreasing order of probability of relevance isprovably optimal however there are many scenario in which thatranking doe not optimize for the user information need oneexample is when the user would be satisfied with some limitednumber of relevant document rather than needing all relevantdocuments we show that in such a scenario an attempt to returnmany relevant document can actually reduce the chance of findingany relevant document we consider a number of information retrieval metric from theliterature including the rank of the first relevant result the no metric that penalizes a system only for retrieving no relevantresults near the top and the diversity of retrieved result whenqueries have multiple interpretation we observe that given aprobabilistic model of relevance it is appropriate to rank so asto directly optimize these metric in expectation while doing somay be computationally intractable we show that a simple greedyoptimization algorithm that approximately optimizes the givenobjectives produce ranking for trec query that outperform thestandard approach based on the probability ranking principle 
machine learning is commonly used to improve ranked retrieval system due to computational difficulty few learning technique have been developed to directly optimize for mean average precision map despite it widespread use in evaluating such system existing approach optimizing map either do not find a globally optimal solution or are computationally expensive in contrast we present a general svm learning algorithm that efficiently find a globally optimal solution to a straightforward relaxation of map we evaluate our approach using the trec and trec web track corpus wt g comparing against svms optimized for accuracy and rocarea in most case we show our method to produce statistically significant improvement in map score 
we address the problem of adaptive compression of natural language text focusing on the case where low bandwidth is available and the receiver ha little processing power a in mobile application our technique achieves compression ratio around and requires very little effort from the receiver this tradeoff not previously achieved with alternative technique is obtained by breaking the usual symmetry between sender and receiver dominant in statistical adaptive compression moreover we show that our technique can be adapted to avoid decompression at all in case where the receiver only want to detect the presence of some keywords in the document this is useful in scenario such a selective dissemination of information news clipping alert system text categorization and clustering thanks to the asymmetry we introduce the receiver can search the compressed text much faster than the plain text this wa previously achieved only in semistatic compression scenario 
this paper provides a novel approach to use uri fragment identifier to enable http client to address and process content independent of it original representation 
this paper present adaptive web search aws a novel search technique that combine personalized social and real time collaborative search preliminary empirical result from a small sample suggest that an aws prototype built on wamp platform using yahoo web search api generates more relevant result and allows faster discovery of information 
xml is the predominant format for representing structured information inside document but it stop at the level of file this make it hard to use xml oriented tool to process information which is scattered over multiple document within a file system file system xml fsx and it content integration provides a unified view of file system structure and content fsx s adaptor map file content to xml which mean that any file format can be integrated with an xml view in the integrated view of the file system 
we propose a methodology for building a practical robust query classification system that can identify thousand of query class with reasonable accuracy while dealing in real time with the query volume of a commercial web search engine we use a blind feedback technique given a query we determine it topic by classifying the web search result retrieved by the query motivated by the need of search advertising we primarily focus on rare query which are the hardest from the point of view of machine learning yet in aggregation account for a considerable fraction of search engine traffic empirical evaluation confirms that our methodology yield a considerably higher classification accuracy than previously reported we believe that the proposed methodology will lead to better matching of online ad to rare query and overall to a better user experience 
this paper present an algorithm for unsupervised noun sense induction based on clustering of web search result the algorithm doe not utilize labeled training instance or any other external knowledge source preliminary result on a small dataset show that this technique provides two advantage over other technique in the literature it detects real world sens not found in dictionary or other lexical resource and it doe not require that the number of word sens be specified in advance 
many information source contain information that can only be accessed through search specific search engine federated search provides search solution of this type of hidden information that cannot be searched by conventional search engine in many scenario of federated search such a the search among health care provider or among intelligence agency an individual information source doe not want to disclose the source of the search result to user or other source therefore this paper proposes a two step federated search protocol that protects the privacy of information source a far a we know this is the first attempt to address the research problem of protecting source privacy in federated text search 
service level agreement slas establish a contract between service providersand client concerning quality of service qos parameter without properpenalties service provider have strong incentive to deviate from theadvertised qos causing loss to the client reliable qos monitoring andproper penalty computed on the basis of delivered qos are thereforeessential for the trustworthiness of a service oriented environment in thispaper we present a novel qos monitoring mechanism based on quality rating from theclients a reputation mechanism collect the rating and computes theactual quality delivered to the client the mechanism provides incentive forthe client to report honestly and pay special attention to minimizing costand overhead 
typical cross language retrieval requires special linguistic resource such a bilingual dictionary and parallel corpus in this study we focus on the cross lingual retrieval problem that only us online translation system we compare two approach a translation based approach that directly translates query into the language of document and then applies traditional information retrieval technique and a model based approach that first learns a statistical translation model from the translation acquired from an online translation system and then applies the learned statistical model to cross lingual information retrieval our empirical study with imageclef ha shown the model based approach performs significantly better than the translation based approach 
the aggregation and comparison of behavioral pattern on the www represent a tremendous opportunity for understanding past behavior and predicting future behavior in this paper we take a first step at achieving this goal we present a large scale study correlating the behavior of internet user on multiple system ranging in size from million query to million blog post to news article we formalize a model for event in these time varying datasets and study their correlation we have created an interface for analyzing the datasets which includes a novel visual artifact the dtwradar for summarizing difference between time series using our tool we identify a number of behavioral property that allow u to understand the predictive power of pattern of use 
a part of a large eort to acquire large repository of fact from unstructured text on the web a seed based framework for textual information extraction allows for weakly supervised extraction of class attribute e g side ee ct and generic equivalent for drug from anonymized query log the extraction is guided by a small set of seed attribute without any need for handcrafted extraction pattern or further domain specic knowledge the attribute of class pertaining to various domain of interest to web search user have accuracy level signican tly exceeding current state of the art inherently noisy search query are shown to be a highly valuable albeit unexplored resource for web based information extraction in particular for the task of class attribute extraction 
in this work we study similarity measure for text centric xml document based on an extended vector space model which considers both document content and structure experimental result based on a benchmark showed superior performance of the proposed measure over the baseline which ignores structural knowledge of xml document 
determining the user intent of web search is a difficult problem due to the sparse data available concerning the searcher in this paper we examine a method to determine the user intent underlying web search engine query we qualitatively analyze sample of query from seven transaction log from three different web search engine containing more than five million query from this analysis we identified characteristic of user query based on three broad classification of user intent the classification of informational navigational and transactional represent the type of content destination the searcher desired a expressed by their query we implemented our classification algorithm and automatically classified a separate web search engine transaction log of over a million query submitted by several hundred thousand user our finding show that more than of web query are informational in nature with about each being navigational and transactional in order to validate the accuracy of our algorithm we manually coded query and compared the classification to the result from our algorithm this comparison showed that our automatic classification ha an accuracy of of the remaining of the query the user intent is generally vague or multi faceted pointing to the need to for probabilistic classification we illustrate how knowledge of searcher intent might be used to enhance future web search engine 
the unarguably fast and continuous growth of the volume of indexed and indexable document on the web pose a great challenge for search engine this is true regarding not only search effectiveness but also time and space efficiency in this paper we present an index pruning technique targeted for search engine that address the latter issue without disconsidering the former to this effect we adopt a new pruning strategy capable of greatly reducing the size of search engine index experiment using a real search engine show that our technique can reduce the index storage cost by up to over traditional lossless compression method while keeping the loss in retrieval precision to a minimum when compared to the index size with no compression at all the compression rate is higher than i e le than one eighth of the original size more importantly our result indicate that due to the reduction in storage overhead query processing time can be reduced to nearly of the original time with no loss in average precision the new method yield significative improvement when compared against the best known static pruning method for search engine index in addition since our technique is orthogonal to the underlying search algorithm it can be adopted by virtually any search engine 
while overall bandwidth in the internet ha grown rapidly over the last few year and an increasing number of client enjoy broadband connectivity many others still access the internet over much slower dialup or wireless link to address this issue a number of technique for optimized delivery of web and multimedia content over slow link have been proposed including protocol optimization caching compression and multimedia transcoding and several large isps have recently begun to widely promote dialup acceleration service based on such technique a recent paper by rhea liang and brewer proposed an elegant technique called value based caching that cache substring of file rather than entire file and thus avoids repeated transmission of substring common to several page or page version we propose and study a hierarchical substring caching technique that provides significant saving over this basic approach we describe several additional technique for minimizing overhead and perform an evaluation on a large set of real web access trace that we collected in the second part of our work we compare our approach to a widely studied alternative approach based on delta compression and show how to integrate the two for best overall performance the studied technique are typically employed in a client proxy environment with each proxy serving a large number of client and an important aspect is how to conserve resource on the proxy while exploiting the significant memory and cpu power available on current client 
a search engine that can handle tv program and web content in an integrated way is proposed conventional search engine have been able to handle web content and or data stored in a pc desktop a target information in the future however the target information is expected to be stored in various place such a in hard disk hd dvd recorder digital camera mobile device and even in real space a ubiquitous content and a search engine that can search across such heterogeneous resource will become essential therefore a a first step towards developing such next generation search engine a prototype search system for web and tv program is developed that performs integrated search of those content and that allows chain search where related content can be accessed from each search result the integrated search is achieved by generating integrated index for web and tv content based on vector space model and by computing similarity between the query and all the content described by the index the chain search of related content is done by computing similarity between the selected result and all other content based on the integrated index also the zoom based display of the search result enables to control medium transition and level of detail of the content to acquire information efficiently in this paper testing of a prototype of the integrated search engine validated the approach taken by the proposed method 
single iteration clarification dialog a implemented in the trec hard track represent an attempt to introduce interaction into ad hoc retrieval while preserving the many benefit of large scale evaluation although previous experiment have not conclusively demonstrated performance gain resulting from such interaction it is unclear whether these finding speak to the nature of clarification dialog or simply the limitation of current system to probe the limit of such interaction we employed a human intermediary to formulate clarification question and exploit user response in addition to establishing a plausible upper bound on performance we were also able to induce an ontology of clarification to characterize human behavior this ontology in turn serf a the input to a regression model that attempt to determine which type of clarification question are most helpful our work can serve to inform the design of interactive system that initiate user dialog 
the asymmetry of activity in virtual community is of great interest while participation in the activity of virtual community is crucial for a community s survival and development many people prefer lurking that is passive attention over active participation lurking can be measured and perhaps affected by both dispositional and situational variable this work investigates the concept of cultural capital a situational antecedent of lurking and de lurking the decision to start posting after a certain amount of lurking time cultural capital is defined a the knowledge that enables an individual to interpret various cultural code the main hypothesis state that a user s cultural capital affect her level of activity in a community and her decision to de lurk and cease to exist in very active community because of information overload this hypothesis is analyzed by mathematically defining a social communication network scn of activity in authenticated discussion forum we validate this model by examining the scn using data collected in a sample of online forum in open university in israel and work based community from ibm the hypothesis verified here make it clear that fostering receptive participation may be a important and constructive a encouraging active contribution in online community 
computer user are asked to generate keep secret and recall an increasing number of password for us including host account email server e commerce site and online financial service unfortunately the password entropy that user can comfortably memorize seems insufficient to store unique secure password for all these account and it is likely to remain constant a the number of password and the adversary s computational power increase into the future in this paper we propose a technique that us a strengthened cryptographic hash function to compute secure password for arbitrarily many account while requiring the user to memorize only a single short password this mechanism function entirely on the client no server side change are needed unlike previous approach our design is both highly resistant to brute force attack and nearly stateless allowing user to retrieve their password from any location so long a they can execute our program and remember a short secret this combination of security and convenience will we believe entice user to adopt our scheme we discus the construction of our algorithm in detail compare it strength and weakness to those of related approach and present password multiplier an implementation in the form of an extension to the mozilla firefox web browser 
today web search engine provide the easiest way to reach information on the web in this scenario more than of indian language content on the web is not searchable due to multiple encoding of web page most of these encoding are proprietary and hence need some kind of standardization for making the content accessible via a search engine in this paper we present a search engine called webkhoj which is capable of searching multi script and multi encoded indian language content on the web we describe a language focused crawler and the transcoding process involved to achieve accessibility of indian langauge content in the end we report some of the experiment that were conducted along with result on indian language web content 
this paper describes an experimental system in which customized high performance xml parser are prepared using parser generation and compilation technique parsing is integrated with schema based validation and deserialization and the resulting validating processor are shown to be a fast a or in many case significantly faster than traditional nonvalidating parser high performance is achieved by integration across layer of software that are traditionally separate by avoiding unnecessary data copying and transformation and by careful attention to detail in the generated code the effect of api design on xml performance is also briefly discussed 
pseudo relevance feedback ha proven to be an effective strategy for improving retrieval accuracy in all retrieval model however the performance of existing pseudo feedback method is often affected significantly by some parameter such a the number of feedback document to use and the relative weight of original query term these parameter generally have to be set by trial and error without any guidance in this paper we present a more robust method for pseudo feedback based on statistical language model our main idea is to integrate the original query with feedback document in a single probabilistic mixture model and regularize the estimation of the language model parameter in the model so that the information in the feedback document can be gradually added to the original query unlike most existing feedback method our new method ha no parameter to tune experiment result on two representative data set show that the new method is significantly more robust than a state of the art baseline language modeling approach for feedback with comparable or better retrieval accuracy 
this paper present a study of incorporating domain specific knowledge i e information about concept and relationship between concept in a certain domain in an information retrieval ir system to improve it effectiveness in retrieving biomedical literature the effect of different type of domain specific knowledge in performance contribution are examined based on the trec platform we show that appropriate use of domain specific knowledge in a proposed conceptual retrieval model yield about improvement over the best reported result in passage retrieval in the genomics track of trec 
historian and scholar can better understand historic event by studying the geographic and chronological activity of individual who witnessed them a lack of adequate tool to help user study these activity can hinder the process of learning and discovery in this paper we present an interface to address this problem that contains three component a map a timeline and a text representation of a survivor s movement these component simultaneously provide query input where user can specify their need and dynamic result display where user can immediately see the effect of their decision the result of a pilot study show that user reacted positively to the interface 
recognition and retrieval of historical handwritten material is an unsolved problem we propose a novel approach to recognizing and retrieving handwritten manuscript based upon word image classification a a key step decision tree with normalized pixel a feature form the basis of a highly accurate adaboost classifier trained on a corpus of word image that have been resized and sampled at a pyramid of resolution to stem problem from the highly skewed distribution of class frequency word class with very few training sample are augmented with stochastically altered version of the original this increase recognition performance substantially on a standard corpus of page of handwritten material from the george washington collection the recognition performance show a substantial improvement in performance over previous published result v following word recognition retrieval is done using a language model over the recognized word retrieval performance also show substantially improved result over previously published result on this database recognition retrieval result on a more challenging database of page from the george washington collection are also presented 
information retrieval evaluation based on the pooling method is inherently biased against system that did not contribute to the pool of judged document this may distort the result obtained about the relative quality of the system evaluated and thus lead to incorrect conclusion about the performance of a particular ranking technique we examine the magnitude of this effect and explore how it can be countered by automatically building an unbiased set of judgement from the original biased judgement obtained through pooling we compare the performance of this method with other approach to the problem of incomplete judgement such a bpref and show that the proposed method lead to higher evaluation accuracy especially if the set of manual judgement is rich in document but highly biased against some system 
structural summary are data structure that preserve all structural feature of xml document in a compact form we investigate the applicability of the most popular summary a textitaccess method within xml query processing in this context issue like space and false positive introduced by the summary need to be examined our evaluation reveals that the additional space required by the more precise structure is usually small and justified by the considerable performance gain that they achieve 
the poster describes a fast simple yet accurate method to associate large amount of web resource stored in a search engine database with geographic location the method us location by ip data domain name and content related feature zip and area code the novelty of the approach lie in building location by ip database by using continuous ip block method another contribution is domain name analysis the method us search engine infrastructure and make it possible to effectively associate large amount of search engine data with geography on a regular basis experiment ran on yandex search engine index evaluation ha proved the efficacy of the approach 
are sponsored link the primary business model for web search engine providing web consumer with relevant result this research address this issue by investigating the relevance of sponsored and non sponsored link for ecommerce query from the major search engine the result show that average relevance rating for sponsored and non sponsored link are virtually the same although the relevance rating for sponsored link are statistically higher we used ecommerce query and retrieved link for these query from three major web search engine google msn and yahoo we present the implication for web search engine and sponsored search a a long term business model a well a a mechanism for finding relevant information for searcher 
it is well known that web page classification can be enhanced by using hyperlink that provide linkage between web page however in the web space hyperlink are usually sparse noisy and thus in many situation can only provide limited help in classification in this paper we extend the concept of linkage from explicit hyperlink to implicit link built between web page by observing that people who search the web with the same query often click on different but related document together we draw implicit link between web page that are clicked after the same query those page are implicitly linked we provide an approach for automatically building the implicit link between web page using web query log together with a thorough comparison between the us of implicit and explicit link in web page classification our experimental result on a large dataset confirm that the use of the implicit link is better than using explicit link in classification performance with an increase of more than in term of the macro f measurement 
we present cluster based retrieval cbr experiment on the largest available turkish document collection our experiment evaluate retrieval effectiveness and efficiency on both an automatically generated clustering structure and a manual classification of document in particular we compare cbr effectiveness with full text search f and evaluate several implementation alternative for cbr our finding reveal that cbr yield comparable effectiveness figure with f furthermore by using a specifically tailored cluster skipping inverted index we significantly improve in memory query processing efficiency of cbr in comparison to other traditional cbr technique and even f 
it ha become a promising direction to measure similarity of web search query by mining the increasing amount of click through data logged by web search engine which record the interaction between user and the search engine most existing approach employ the click through data for similarity measure of query with little consideration of the temporal factor while the click through data is often dynamic and contains rich temporal information in this paper we present a new framework of time dependent query semantic similarity model on exploiting the temporal characteristic of historical click through data the intuition is that more accurate semantic similarity value between query can be obtained by taking into account the timestamps of the log data with a set of user defined calendar schema and calendar pattern our time dependent query similarity model is constructed using the marginalized kernel technique which can exploit both explicit similarity and implicit semantics from the click through data effectively experimental result on a large set of click through data acquired from a commercial search engine show that our time dependent query similarity model is more accurate than the existing approach moreover we observe that our time dependent query similarity model can to some extent reflect real world semantics such a real world event that are happening over time 
relevance judgment are used to compare text retrieval system given a collection of document and query and a set of system being compared a standard approach to forming judgment is to manually examine all document that are highly ranked by any of the system however not all of these relevance judgment provide the same benefit to the final result particularly if the aim is to identify which system are best rather than to fully order them in this paper we propose new experimental methodology that can significantly reduce the volume of judgment required in system comparison using rank biased precision a recently proposed effectiveness measure we show that judging around document for each of query in a trec scale system evaluation containing over run is sufficient to identify the best system 
we present an analysis of the structured relationship observed in a randomly sampled set of question like query submitted to a search engine for a popular online encyclopedic document collection our study show that a relatively small number of binary relationship account for most of the query in the sample this empirically validates an approach of analyzing query log to identify the relationship most relevant to user need and populating corresponding fact table from the collection for factoid question answering our analysis show that such an approach can lead to substantial coverage of user question 
demographic information play an important role in personalized web application however it is usually not easy to obtain this kind of personal data such a age and gender in this paper we made a first approach to predict user gender and age from their web browsing behavior in which the webpage view information is treated a a hidden variable to propagate demographic information between different user there are three main step in our approach first learning from the webpage click though data webpage are associated with user known age and gender tendency through a discriminative model second user unknown age and gender are predicted from the demographic information of the associated webpage through a bayesian framework third based on the fact that webpage visited by similar user may be associated with similar demographic tendency and user with similar demographic information would visit similar webpage a smoothing component is employed to overcome the data sparseness of web click though log experiment are conducted on a real web click through log to demonstrate the effectiveness of the proposed approach the experimental result show that the proposed algorithm can achieve up to improvement on gender prediction and on age prediction in term of macro f compared to baseline algorithm 
we present ditabbu digital talking book builder a framework for automatic production of time based hypermedia for the web focusing on the digital talking book domain delivering digital talking book collection to a wide range of user is an expensive task a it must take into account each user profile s different need therefore authoring should be dismissed in favour of automation with ditabbu we enable automated content delivery in several playback platform targeted to specific user need featuring powerful navigation capability over the content ditabbu can also be used a testbed for prototyping novel capability through it flexible extension mechanism 
we show that the time web site take to respond to http request can leak private information using two different type of attack the first direct timing directly measure response time from a web site to expose private information such a validity of an username at a secured site or the number of private photo in a publicly viewable gallery the second cross site timing enables a malicious web site to obtain information from the user s perspective at another site for example a malicious site can learn if the user is currently logged in at a victim site and in some case the number of object in the user s shopping cart our experiment suggest that these timing vulnerability are wide spread we explain in detail how and why these attack work and discus method for writing web application code that resists these attack 
this paper proposes a probabilistic model for definitional question answering qa that reflects the characteristic of the definitional question the intention of the definitional question is to request the definition about the question target therefore an answer for the definitional question should contain the content relevant to the topic of the target and have a representation form of the definition style modeling the problem of definitional qa from both the topic and definition viewpoint the proposed probabilistic model convert the task of answering the definitional question into that of estimating the three language model topic language model definition language model and general language model the proposed model systematically combine several evidence in a probabilistic framework experimental result show that a definitional qa system based on the proposed probabilistic model is comparable to state of the art system 
the increasing amount of communication between individual in e format e g email instant messaging and the web ha motivated computational research in social network analysis sna previous work in sna ha emphasized the social network sn topology measured by communication frequency while ignoring the semantic information in sn in this paper we propose two generative bayesian model for semantic community discovery in sn combining probabilistic modeling with community detection in sn to simulate the generative model an enf gibbs sampling algorithm is proposed to address the efficiency and performance problem of traditional method experimental study on enron email corpus show that our approach successfully detects the community of individual and in addition provides semantic topic description of these community 
we present a system to automatically generate r feed from html document that consist of time series item with date expression e g archive of weblogs bbs chat mailing list site update description and event announcement our system extract date expression performs structure analysis of a html document and detects or generates title from the document 
in this paper we propose the use of record management principle to identify and manage web site resource with enduring value a record current web archiving activity collaborative or organisational whilst extremely valuable in their own right often do not and cannot incorporate requirement for proper record management material collected under such initiative therefore may not be reliable or authentic from a legal or archival perspective with insufficient metadata collected about the object during it active life and valuable material destroyed whilst ephemeral item are maintained education training and collaboration between stakeholder are integral to avoiding these risk and successfully preserving valuable web based material 
technical professional spend of their time at work searching for information and have specialized information need that are not well served by generic enterprise search tool in this study we investigated how a group of software engineer use a workplace search system we identify pattern of search behaviour specific to this group and distinct from general web and intranet search pattern and make design recommendation for search system that will better serve the need of this group 
pagerank is the best known technique for link based importance ranking the computed importance score however are not directly comparable across different snapshot of an evolving graph we present an efficiently computable normalization for pagerank score that make them comparable across graph furthermore we show that the normalized pagerank score are robust to non local change in the graph unlike the standard pagerank measure 
many server selection method suitable for distributed information retrieval application rely in the absence of cooperation on the availability of unbiased sample of document from the constituent collection we describe a number of sampling method which depend only on the normal query response mechanism of the applicable search facility we evaluate these method on a number of collection typical of a personal metasearch application result demonstrate that bias exist for all method particularly toward longer document and that in some case these bias can be reduced but not eliminated by choice of parameter we also introduce a new sampling technique multiple query which produce sample of similar quality to the best current technique but with significantly reduced cost 
the interoperability among distributed and autonomous system is the ultimate challenge facing the semantic web heterogeneity of data representation is the main source of problem this paper proposes an innovative solution that combine lexical approach and language game the benefit for distributed annotation system on the web are twofold firstly it will reduce the complexity of the semantic problem by moving the focus from the full featured ontology level to the simpler lexicon level secondly it will avoid the drawback of a centralized third party mediator that may become a single point of failure the main contribution of this work are concerned with providing a proof of concept that language game can be an effective solution to creating and managing a distributed process of agreement on a shared lexicon describing a fully distributed service oriented architecture for language game providing empirical evidence on a real world case study in the domain of ski mountaineering 
adaptive web site have been proposed to enhance ease of navigation and information retrieval a variety of approach are described in the literature but consideration of interface presentation issue and realistic user study are generally lacking we report here a large scale study of site with dynamic information collection and user interest where adaptation is based on an ant colony optimization technique we find that most user were able to locate information effectively without needing to perform explicit search the behavior of user who did search wa similar to that on internet search engine simulation based on site and user model give insight into the adaptive behavior and correspond to observation 
cryptographic protocol are useful for trust engineering in web transaction the cryptographic protocol programming language cppl provides a model wherein trust management annotation are attached to protocol action and are used to constrain the behavior of a protocol participant to be compatible with it own trust policy the first implementation of cppl generated stand alone single session server making it unsuitable for deploying protocol on the web we describe a new compiler that us a constraint based analysis to produce multi session server program the resulting program run without persistent tcp connection for deployment on traditional web server most importantly the compiler preserve existing proof about the protocol we present an enhanced version of the cppl language discus the generation and use of constraint show their use in the compiler formalize the preservation of property present subtlety and outline implementation detail 
this study investigates the effectiveness of retrieval system and human user in generating term for query expansion we compare three source of term system generated term term user select from top ranked sentence and user generated term result demonstrate that overall the system generated more effective expansion term than user but that user selection of term improved precision at the top of the retrieved document list 
in this paper we study the privacy preservation property of aspecific technique for query log anonymization token based hashing in this approach each query is tokenized and then a secure hash function is applied to each token we show that statistical technique may be applied to partially compromise the anonymization we then analyze the specific risk that arise from these partial compromise focused on revelation of identity from unambiguous name address and so forth and the revelation of fact associated with an identity that are deemed to be highly sensitive our goal in this work is two fold to show that token based hashing is unsuitable for anonymization and to present a concrete analysis of specific technique that may be effective in breaching privacy against which other anonymization scheme should be measured 
wrapping is the process of navigating a data source semi automatically extracting data and transforming it into a form suitable for data processing application there are currently a number of established product on the market for wrapping data from web page one such approach is lixto a product of research performed at our institute our work is concerned with extending the wrapping functionality of lixto to pdf document a the pdf format is relatively unstructured this is a challenging task we have developed a method to segment the page into block which are represented a node in a relational graph this paper describes our current research in the use of relational matching technique on this graph to locate wrapping instance 
collaborative filtering technique have become popular in the past decade a an effective way to help people deal with information overload recent research ha identified significant vulnerability in collaborative filtering technique shilling attack in which attacker introduce biased rating to influence recommendation system have been shown to be effective against memory based collaborative filtering algorithm we examine the effectiveness of two popular shilling attack the random attack and the average attack on a model based algorithm that us singular value decomposition svd to learn a low dimensional linear model our result show that the svd based algorithm is much more resistant to shilling attack than memory based algorithm furthermore we develop an attack detection method directly built on the svd based algorithm and show that this method detects random shilling attack with high detection rate and very low false alarm rate 
we present a metadata free system for the interaction with massive collection of music the musicsurfer musicsurfer automatically extract description related to instrumentation rhythm and harmony from music audio signal together with efficient similarity metric the description allow navigation of multimillion track music collection in a flexible and efficient way without the need of metadata or human rating 
query log and pseudo relevance feedback prf offer way in which term to refine web searcher query can be selected offered to searcher and used to improve search effectiveness in this poster we present a study of these technique that aim to characterize the degree of similarity between them across a set of test query and the same set broken out by query type the result suggest that i similarity increase with the amount of evidence provided to the prf algorithm ii similarity is higherwhen title snippet are used for prf than full text and iii similarity is higher for navigational than informational query the finding have implication for the combined usage of query log and prf in generating query refinement alternative 
the semantic web language rdfs and owl have been around for some time now however the presence of these language ha not brought the breakthrough of the semantic web the creator of the language had hoped for owl ha a number of problem in the area of interoperability and usability in the context of many practical application scenario which impede the connection to the software engineering and database community in this paper we present owl flight which is loosely based on owl but the semantics is grounded in logic programming rather than description logic and it borrows the constraint based modeling style common in database this result in different type of modeling primitive and enforces a different style of ontology modeling we analyze the modeling paradigm of owl dl and owl flight a well a reasoning task supported by both language we argue that different application on the semantic web require different style of modeling and thus both type of language are required for the semantic web 
there have been several authoring method proposed in the literature that are model based essentially following the model driven design philosophy while useful such method need an effective way to allow the application designer to somehow synthesize the actual running application from the specification in this paper we describe hyperde an environment that combine model driven design and domain specific language we show the advantage of this combination to enable rapid authoring and prototyping of web application 
web spam is behavior that attempt to deceive search engine ranking algorithm trustrank is a recent algorithm that can combat web spam however trustrank is vulnerable in the sense that the seed set used by trustrank may not be sufficiently representative to cover well the different topic on the web also for a given seed set trustrank ha a bias towards larger community we propose the use of topical information to partition the seed set and calculate trust score for each topic separately to address the above issue a combination of these trust score for a page is used to determine it ranking experimental result on two large datasets show that our topical trustrank ha a better performance than trustrank in demoting spam site or page compared to trustrank our best technique can decrease spam from the top ranked site by a much a 
semantic portal is the next generation of web portal that are powered by semantic web technology for improved information sharing and exchange for a community of user current method of searching in semantic portal are limited to keyword based search using information retrieval ir technique ontology based formal query and reasoning or a simple combination of the two in this paper we propose an enhanced model that tightly integrates ir with formal query and reasoning to fully utilize both textual and semantic information for searching in semantic portal the model extends the search capability of existing method and can answer more complex search request the idea in a fuzzy description logic dl ir model and a formal dl query method are employed and combined in our model based on the model a semantic search service is implemented and evaluated the evaluation show very large improvement over existing method 
we consider the problem of analyzing word trajectory in both time and frequency domain with the specific goal of identifying important and le reported periodic and aperiodic word a set of word with identical trend can be grouped together to reconstruct an event in a completely un supervised manner the document frequency of each word across time is treated like a time series where each element is the document frequency inverse document frequency dfidf score at one time point in this paper we first applied spectral analysis to categorize feature for different event characteristic important and le reported periodic and aperiodic modeled aperiodic feature with gaussian density and periodic feature with gaussian mixture density and subsequently detected each feature s burst by the truncated gaussian approach proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic event all of the above method can be applied to time series data in general we extensively evaluated our method on the year reuters news corpus and showed that they were able to uncover meaningful aperiodic and periodic event 
the paper describes a user oriented performance evaluation measure for text segmentation experiment show that the proposed measure differentiates well between error distribution with varying user impact 
readability assessment is a method to measure the difficulty of a piece of text material and it is widely used in educational field to assist instructor to prepare appropriate material for student in this paper we investigate the application of readability assessment in web development such that user can retrieve information which is appropriate to their level we propose a bilingual english and chinese assessment scheme for web page and web site readability based on textual feature and conduct a series of experiment with real web data to evaluate our scheme experimental result show that apart from just indicating the readability level the estimated score act a a good heuristic to figure out page with low textual content furthermore we can obtain the overall content distribution in a web site by studying the variation of it readability 
a common motivation for personalised search system is the ability to disambiguate query based on some knowledge of a user s interest an analysis of log file from three search provider covering a range of scenario suggests that this sort of disambiguation would be of marginal use for more specialised provider but may be of use for whole of web search 
we are developing support for creativity in learning through information discovery and exploratory search user engage in creative task such a inventing new product and service the system support evolving information need it gather and present relevant information visually using image and text user are able to search browse and explore result from multiple query and interact with information element by manipulating design and expressing interest a field study wa conducted to evaluate the system in an undergraduate class the result demonstrated the efficacy of our system for developing creative idea exposure to diverse information in visual and interactive form is shown to support student engaged in invention task 
we describe thresher a system that let non technical user teach their browser how to extract semantic web content from html document on the world wide web user specify example of semantic content by highlighting them in a web browser and describing their meaning we then use the tree edit distance between the dom subtrees of these example to create a general pattern or wrapper for the content and allow the user to bind rdf class and predicate to the node of these wrapper by overlaying match to these pattern on standard document inside the haystack semantic web browser we enable a rich semantic interaction with existing web page unwrapping semantic data buried in the page html by allowing end user to create modify and utilize their own pattern we hope to speed adoption and use of the semantic web and it application 
in this paper we propose a new strategy with time granularity reasoning for utilizing temporal information in topic tracking compared with previous one our work ha four distinguished characteristic firstly we try to determine a set of topic time for a target topic from the given on topic story it help to avoid the negative influence from other irrelevant time secondly we take into account time granularity variance when deciding whether a coreference relationship exists between two time thirdly both publication time and time presented in text are considered finally a time is only one attribute of a topic we increase the similarity between a story and a target topic only when they are related not only temporally but also semantically experiment on two tdt corpus show that our method make good use of temporal information in news story 
web authoring environment enable end user to create application that integrate information from other web source user can create web site that include built in component to dynamically incorporate for example weather information stock quote or the latest news from different web source recent survey conducted among end user have indicated an increasing interest in creating such application unfortunately web authoring environment do not provide support beyond a limited set of built in component this work address this limitation by providing end user support for clipping information from a target web site to incorporate it into the end user site the support consists of a mechanism to identify the target clipping with multiple marker to increase robustness and a dynamic assessment of the retrieved information to quantify it reliability the clipping approach ha been integrated a a feature into a popular web authoring tool on which we present the result of two preliminary study 
in this poster we present a method for extracting query related to real life event or news related query from large web query log the method employ query frequency and search over a collection of recent news news related query can be helpful for disambiguating user information need a well a for effective online news processing the performed evaluation prof that the method yield good precision 
locating file based on file system structure file property and maybe even file content is a core task of the user interface of operating system by adapting xpath s power to the environment of a unix shell it is possible to greatly increase the expressive power of the command line language we present a concept for integrating an xpath view of the file system into a shell the emphxpath shell xpsh which can be used to find file based on file attribute and content in a very flexible way the syntax of the command line language is backwards compatible with traditional shell and the new xpath based expression can be easily mastered with a little bit of xpath knowledge 
hash based similarity search reduces a continuous similarity relation to the binary concept similar or not similar two feature vector are considered a similar if they are mapped on the same hash key from it runtime performance this principle is unequaled while being unaffected by dimensionality concern at the same time similarity hashing is applied with great success for near similarity search in large document collection and it is considered a a key technology for near duplicate detection and plagiarism analysis this paper reveals the design principle behind hash based search method and present them in a unified way we introduce new stress statistic that are suited to analyze the performance of hash based search method and we explain the rationale of their effectiveness based on these insight we show how optimum hash function for similarity search can be derived we also present new result of a comparative study between different hash based search method 
we propose a language model based approach for addressing the performance robustness problem with respect to free parameter value of pseudo feedback based query expansion method given a query we create a set of language model representing different form of it expansion by varying the parameter value of some expansion method then we select a single model using criterion originally proposed for evaluating the performance of using the original query or for deciding whether to employ expansion at all experimental result show that these criterion are highly effective in selecting relevance language model that are not only significantly more effective than poor performing one but that also yield performance that is almost indistinguishable from that of manually optimized relevance model 
the aim of web page visualization is to present in a very informative and interactive way a set of web document to the user in order to let him or her navigate through these document in the web context this may correspond to several user s task displaying the result of a search engine or visualizing a graph of page such a a hypertext or a surf map in addition to web page visualization web page clustering also greatly improves the amount of information presented to the user by highlighting the similarity between the document in this paper we explore the use of a cellular automaton ca to generate such map of web page 
the open directory project is clearly one of the largest collaborative effort to manually annotate web page this effort involves over editor and resulted in metadata specifying topic and importance for more than million web page still given that this number is just about percent of the web page indexed by google is this effort enough to make a difference in this paper we discus how these metadata can be exploited to achieve high quality personalized web search first we address this by introducing an additional criterion for web page ranking namely the distance between a user profile defined using odp topic and the set of odp topic covered by each url returned in regular web search we empirically show that this enhancement yield better result than current web search using google then in the second part of the paper we investigate the boundary of biasing pagerank on subtopics of the odp in order to automatically extend these metadata to the whole web 
in this paper we investigate how detailed tracking of user interaction can be monitored using standard web technology our motivation is to enable implicit interaction and to ease usability evaluation of web application outside the lab to obtain meaningful statement on how user interact with a web application the collected information need to be more detailed and fine grained than that provided by classical log file we focus on task such a classifying the user with regard to computer usage proficiency or making a detailed assessment of how long it took user to fill in field of a form additionally it is important in the context of our work that usage tracking should not alter the user s experience and that it should work with existing server and browser setup we present an implementation for detailed tracking of user action on web page an http proxy modifies html page by adding javascript code before delivering them to the client this javascript tracking code collect data about mouse movement keyboard input and more we demonstrate the usefulness of our approach in a case study 
web page include extraneous material that may be viewed a undesirable by a user increasingly many web site also require user to register to access either all or portion of the site such tension between content owner and user ha resulted in a cat and mouse game between content provided and how user access it we carried out a measurement based study to understand the nature of extraneous content and it impact on performance a perceived by user we characterize how this content is distributed and the effectiveness of blocking mechanism to stop it a well a countermeasure taken by content owner to negate such mechanism we also examine site that require some form of registration to control access and the attempt made to circumvent it result from our study show that extraneous content exists on a majority of popular page and that a reduction in downloaded object and byte with corresponding latency reduction can be attained by blocking such content the top ten advertisement delivering company delivered of all url matched a ad in our study both the server name and the remainder of the url are important in matching a url a an ad a majority of popular site require some form of registration and for such site user can obtain an account from a shared public database we discus future measure and countermeasure on the part of each side 
topic detection and tracking and topic segmentation play an important role in capturing the local and sequential information of document previous work in this area usually focus on single document although similar multiple document are available in many domain in this paper we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar document based on mutual information mi and weighted mutual information wmi that is a combination of mi and term weight the basic idea is that the optimal segmentation maximizes mi or wmi our approach can detect shared topic among document it can find the optimal boundary in a document and align segment among document at the same time it also can handle single document segmentation a a special case of the multi document segmentation and alignment our method can identify and strengthen cue term that can be used for segmentation and partially remove stop word by using term weight based on entropy learned from multiple document our experimental result show that our algorithm work well for the task of single document segmentation shared topic detection and multi document segmentation utilizing information from multiple document can tremendously improve the performance of topic segmentation and using wmi is even better than using mi for the multi document segmentation 
we conducted a series of experiment in which surveyed web search user answered question about the quality of search result on the basis of the result summary summary shown to different group of user were editorially constructed so that they differed in only one attribute such a length some attribute had no effect on user quality judgment while in other case changing an attribute had a halo effect which caused seemingly unrelated dimension of result quality to be rated higher by user 
in today s web many functionality wise similar web service are offered through heterogeneous interface operation definition and business protocol ordering constraint defined on legal operation invocation sequence the typical approach to enable interoperation in such a heterogeneous setting is through developing adapter there have been approach for classifying possible mismatch between service interface and business protocol to facilitate adapter development however the hard job is that of identifying given two service specification the actual mismatch between their interface and business protocol in this paper we present novel technique and a tool that provides semi automated support for identifying and resolution of mismatch between service interface and protocol and for generating adapter specification we make the following main contribution i we identify mismatch between service interface which lead to finding mismatch of type of signature merge split and extra missing message ii we identify all ordering mismatch between service protocol and generate a tree called mismatch tree for mismatch that require developer input for their resolution in addition we provide semi automated support in analyzing the mismatch tree to help in resolving such mismatch we have implemented the approach in a tool inside ibm wid websphere integration developer our experiment with some real world case study show the viability of the proposed approach the method and tool are significant in that they considerably simplify the problem of adapting service so that interoperation is possible 
the performance of document clustering system depends on employing optimal text representation which are not only difficult to determine beforehand but also may vary from one clustering problem to another a a first step towards building robust document clusterers a strategy based on feature diversity and cluster ensemble is presented in this work experiment conducted on a binary clustering problem show that our method is robust to near optimal model order selection and able to detect constructive interaction between different document representation in the test bed 
to find similar web page to a query page on the web this paper introduces a novel link based similarity measure called pagesim contrast to simrank a recursive refinement of cocitation pagesim can measure similarity between any two web page whereas simrank cannot in some case we give some intuition to the pagesim model and outline the model with mathematical definition finally we give an example to illustrate it effectiveness 
identifying redundant information in sentence is useful for several application such a summarization document provenance detecting text reuse and novelty detection the task of identifying redundant information in sentence is defined a follows given a query sentence the task is to retrieve sentence from a given collection that express all or some subset of the information present in the query sentence sentence retrieval technique rank sentence based on some measure of their similarity to a query the effectiveness of such technique depends on the similarity measure used to rank sentence an effective retrieval model should be able to handle low word overlap between query and candidate sentence and go beyond just word overlap simple language modeling technique like query likelihood retrieval have outperformed tf idf and word overlap based method for ranking sentence in this paper we compare the performance of sentence retrieval using different language modeling technique for the problem of identifying redundant information 
in this paper we present egoir an approach for retrieving legal information based on ontology this approach ha been developed with legal ontology to be deployed within the e government context 
we developed a prototype for integrated retrieval and aggregation of diverse information contained in scanned paper document such complex document information processing combine several form of image processing together with textual linguistic processing to enable effective analysis of complex document collection a necessity for a wide range of application this is the first system to attempt integrated retrieval from complex document we report it current capability 
the web search multitasking study based on automatic task session detection procedure is described the result of the study multitasking is very rare it usually cover only task session it is frequently formed into a temporal inclusion of an interrupting task session into the interrupted session the quantitative characteristic of multitasking greatly differ from the characteristic of sequential execution of one and several task a searcher minimizes task switching cost he avoids multitasking and while multitasking he us cheapest manner of task switching 
a service oriented system emerges from composition of service dynamically composed reactive web service form a special class of service oriented system where the delay associated with communication unreliability and unavailability of service and competition for resource from multiple service requester are dominant concern a complexity of service increase an abstract design language for the specification of service and interaction between them is desired in this paper we present asdl abstract service design language a wide spectrum language for modelling web service we initially provide an informal description of our computational model for service oriented system we then present asdl along with it specification oriented semantics defined in interval temporal logic itl a sound formalism for specifying and reasoning about temporal property of system the objective of asdl is to provide a notation for the design of service composition and interaction protocol at an abstract level 
web search engine present list of caption comprising title snippet and url to help user decide which search result to visit understanding the influence of feature of these caption on web search behavior may help validate algorithm and guideline for their improved generation in this paper we develop a methodology to use clickthrough log from a commercial search engine to study user behavior when interacting with search result caption the finding of our study suggest that relatively simple caption feature such a the presence of all term query term the readability of the snippet and the length of the url shown in the caption can significantly influence user web search behavior 
a major limitation of most existing retrieval model and system is that the retrieval decision is made based solely on the query and document collection information about the actual user and search context is largely ignored in this paper we study how to exploit implicit feedback information including previous query and clickthrough information to improve retrieval accuracy in an interactive information retrieval setting we propose several context sensitive retrieval algorithm based on statistical language model to combine the preceding query and clicked document summary with the current query for better ranking of document we use the trec ap data to create a test collection with search context information and quantitatively evaluate our model using this test set experiment result show that using implicit feedback especially the clicked document summary can improve retrieval performance substantially 
investigating whether one can view web searching a a learning process we examined the searching characteristic of participant engaged in searching task we classified the searching task according an updated version of bloom s taxonomy a six level categorization of cognitive learning result show that applying take the most searching effort a measured by query per session and specific topic searched per session the lower level category of remembering and understanding exhibit searching characteristic similar to the higher order learning of evaluating and creating it appears that searcher rely primarily on their internal knowledge for evaluating and creating using searching primarily a fact checking and verification implication are that the commonly held notion that web searcher have simple information need may not be correct we discus the implication for web searching including designing interface to support exploration 
we present a novel web search interaction feature which for a given query provides link to website frequently visited by other user with similar information need these popular destination complement traditional search result allowing direct navigation to authoritative resource for the query topic destination are identified using the history of search and browsing behavior of many user over an extended time period whose collective behavior provides a basis for computing source authority we describe a user study which compared the suggestion of destination with the previously proposed suggestion of related query a well a with traditional unaided web search result show that search enhanced by destination suggestion outperforms other system for exploratory task with best performance obtained from mining past user behavior at query level granularity 
search query applied to extract relevant information from the world wide web over a period of time may be denoted a continuous search query the improvement of continuous search query may concern not only the quality of retrieved result but also the freshness of result i e the time between the availability of a respective data object on the web and the notification of a user by the search engine in some case a user should be notified immediately since the value of the respective information decrease quickly a e g news about company that affect the value of respective stock or sale offer for product that may no longer be available after a short period of time in the document filtering literature the optimization of such query is usually based on threshold classification document above a quality threshold are returned to a user the threshold is tuned in order to optimize the quality of retrieved result the disadvantage of such approach is that the amount of information returned to a user may hardly be controlled without further user interaction in this paper we consider the optimization of bounded continuous search query where only the estimated best k element are returned to a user we present a new optimization method for bounded continuous search query based on the optimal stopping theory and compare the new method to method currently applied by web search system the new method provides result of significantly higher quality for the case where very fresh result have to be delivered 
when investigating alternative estimate for term discriminativeness we discovered that relevance information and idf are much closer related than formulated in classical literature therefore we revisited the justification of idf a it follows from the binary independent retrieval bir model the main result is a formal framework uncovering the close relationship of a generalised idf and the bir model the framework make explicit how to incorporate relevance information into any retrieval function that involves an idf component in addition to the idf based formulation of the bir model we propose poisson based estimate a an alternative to the classical estimate this being motivated by the superiority of poisson based estimate for the within document term frequency the main experimental finding is that a poisson based idf is superior to the classical idf where the superiority is particularly evident for long query 
we investigate the effect of search engine brand i e the identifying name or logo that distinguishes a product from it competitor on evaluation of system performance this research is motivated by the large amount of search traffic directed to a handful of web search engine even though most are of equal technical quality with similar interface we conducted a laboratory study with participant to measure the effect of four search engine brand while controlling for the quality of search engine result there wa a difference between the most highly rated search engine and the lowest using average relevance rating even though search engine result were identical in both content and presentation qualitative analysis suggests branding affect user view of popularity trust and specialization we discus implication for search engine marketing and the design of search engine quality study 
while the idea that querying mechanism for complex relationship otherwise known a semantic association should be integral to semantic web search technology ha recently gained some ground the issue of how search result will be ranked remains largely unaddressed since it is expected that the number of relationship between entity in a knowledge base will be much larger than the number of entity themselves the likelihood that semantic association search would result in an overwhelming number of result for user is increased therefore elevating the need for appropriate ranking scheme furthermore it is unlikely that ranking scheme for ranking entity document resource etc may be applied to complex structure such a semantic association in this paper we present an approach that rank result based on how predictable a result might be for user it is based on a relevance model semrank which is a rich blend of semantic and information theoretic technique with heuristic that support the novel idea of modulative search where user may vary their search mode to effect change in the ordering of result depending on their need we also present the infrastructure used in the ssark system to support the computation of semrank value for resulting semantic association and their ordering 
we introduce a new relevance scoring technique that enhances existing relevance scoring scheme with term position information this technique us chronological term rank ctr which capture the position of term a they occur in the sequence of word in a document ctr is both conceptually and computationally simple when compared to other approach that use document structure information such a term proximity term order and document feature ctr work well when paired with okapi bm we evaluate the performance of various combination of ctr with okapi bm in order to identify the most effective formula we then compare the performance of the selected approach against the performance of existing method such a okapi bm pivoted length normalization and language model significant improvement are seen consistently across a variety of trec data and topic set measured by the major retrieval performance metric this seems to be the first use of this statistic for relevance scoring there is likely to be greater retrieval improvement possible using chronological term rank enhanced method in future work 
finding a proper distribution of translation probability is one of the most important factor impacting the effectiveness of a cross language information retrieval system in this paper we present a new approach that computes translation probability for a given query by using only a bilingual dictionary and a monolingual corpus in the target language the algorithm combine term association measure with an iterative machine learning approach based on expectation maximization our approach considers only pair of translation candidate and is therefore le sensitive to data sparseness issue than approach using higher n gram the learned translation probability are used a query term weight and integrated into a vector space retrieval system result for english german cross lingual retrieval show substantial improvement over a baseline using dictionary lookup without term weighting 
researcher of commercial search engine often collect datausing the application programming interface api or by scraping result from the web user interface wui butanecdotal evidence suggests the interface produce differentresults we provide the first in depth quantitative analysisof the result produced by the google msn and yahoo apiand wui interface after submitting a variety of queriesto the interface for month we found significant discrepanciesin several category our finding suggest that theapi index are not older but they are probably smaller for google and yahoo researcher may use our finding tobetter understand the difference between the interface andchoose the best api for their particular type of query 
focused structured document retrieval employ the concept of best entry point bep which is intended to provide optimal starting point from which user can browse to relevant document component in this paper we describe and evaluate a method for finding beps in xml document experiment conducted within the framework of inex evaluation campaign on the wikipedia xml collection shown the effectiveness of the proposed approach 
this paper describes a large scale evaluation of theeffectiveness of hit in comparison with other link based rankingalgorithms when used in combination with a state of the art textretrieval algorithm exploiting anchor text we quantified theireffectiveness using three common performance measure the meanreciprocal rank the mean average precision and the normalizeddiscounted cumulative gain measurement the evaluation is based ontwo large data set a breadth first search crawl of millionweb page containing billion hyperlink and referencing billion distinct url and a set of query sampled from aquery log each query having on average result about ofwhich were labeled by judge we found that hit outperformspagerank but is about a effective a web page in degree the sameholds true when any of the link based feature are combined withthe text retrieval algorithm finally we studied the relationshipbetween query specificity and the effectiveness of selectedfeatures and found that link based feature perform better forgeneral query whereas bm f performs better for specificqueries 
we introduce an evaluation metric called p measure for the task of retrieving 
the key property of the wwww is it universality one must be able to access it whatever the hardware device software platform and network one is using and despite the disability one might have and whether oner is in a developed or developing country it must support information of any language culture quality medium and field without discrimination so that a hypertext link can go anywhere it must support information intended for people and that intended for machine processing the web architecture incorporated various choice which support these ax of universality currently the architecture and the principle are being exploited in the recent mobile web initiative in w c to promote content which can be accessed optimally from conventional computer and mobile device new exciting area arise every few month a possible semantic web flagship application a new area burst forth the fundamental principle remain important and are extended and adjusted at the same time the principle of openness and consensus among international stakeholder which the www consortium employ for new technology are adjusted but ever important 
in this work we explore the use of error correcting output code ecoc to enhance the performance of centroid text classifier the framework is to decompose one multi class problem into multiple binary problem and then learn the individual binary classification problem by centroid classifier however this kind of decomposition incurs considerable bias for centroid classifier which result in noticeable degradation of performance to address this issue we use model refinement to adjust this so called bias 
traditionally stemming ha been applied to information retrieval task by transforming word in document to the their root form before indexing and applying a similar transformation to query term although it increase recall this naive strategy doe not work well for web search since it lower precision and requires a significant amount of additional computation in this paper we propose a context sensitive stemming method that address these two issue two unique property make our approach feasible for web search first based on statistical language modeling we perform context sensitive analysis on the query side we accurately predict which of it morphological variant is useful to expand a query term with before submitting the query to the search engine this dramatically reduces the number of bad expansion which in turn reduces the cost of additional computation and improves the precision at the same time second our approach performs a context sensitive document matching for those expanded variant this conservative strategy serf a a safeguard against spurious stemming and it turn out to be very important for improving precision using word pluralization handling a an example of our stemming approach our experiment on a major web search engine show that stemming only of the query traffic we can improve relevance a measured by average discounted cumulative gain dcg by on these queriesand over all query traffic 
in the domain of video content retrieval we present an approach for selecting word and phrase from highly imperfect automatically generated transcript extracted term are ranked according to their descriptiveness and presented to the user in a multimedia browser interface we use sense querying from the wordnet lexical database for our method of text selection and ranking evaluation of video summarization task from user show that the method of ranking and emphasizing term according to descriptiveness result in higher accuracy response in le time compared to the baseline of no ranking 
we present research leading toward an understanding of the optimal audio visual representation for illustrating concept for illiterate and semi literate user of computer in our user study which to our knowledge is the first of it kind we presented to illiterate subject each of different health symptom in one representation randomly selected among the following ten text static drawing staticphotographs hand drawn animation and video each with and without voice annotation the goal wa to see how comprehensible these representation type were for an illiterate audience we used a methodology for generating each of the representation tested in a way that fairly stack one representational type against the others our main result are that voice annotation generally help in speed of comprehension but bimodal audio visual information can be confusing for the target population richer information is not necessarily better understood overall the relative value of dynamic imagery versus static imagery depends on various factor analysis of these statistically significant result and additional detailed result are also provided 
we present an incremental algorithm for building a neighborhood graph from a set of document this algorithm is based on a population of artificial agent that imitate the way real ant build structure with self assembly behavior we show that our method outperforms standard algorithm for building such neighborhood graph up to time faster on the tested database with equal quality and how the user may interactively explore the graph 
automatic image annotation is the task of automatically assigning word to an image that describe the content of the image machine learning approach have been explored to model the association between word and image from an annotated set of image and generate annotation for a test image the paper proposes method to use a hierarchy defined on the annotation word derived from a text ontology to improve automatic image annotation and retrieval specifically the hierarchy is used in the context of generating a visual vocabulary for representing image and a a framework for the proposed hierarchical classification approach for automatic image annotation the effect of using the hierarchy in generating the visual vocabulary is demonstrated by improvement in the annotation performance of translation model in addition to performance improvement hierarchical classification approach yield well to constructing multimedia ontology 
successful navigation from a relevant web page to other relevant page depends on the page linking to other relevant page we measured the distance to travel from relevant page to relevant page and found a bimodal distribution of distance peaking at and hop in an attempt to make it easier to navigate among relevant page we added content similarity link to page with these additional link significantly more relevant document were close to each other a browser plug in or other tool that provides link to page similar to a given page should increase the ability of web user to find relevant page via navigation 
we consider the following full text search autocompletion feature imagine a user of a search engine typing a query then with every letter being typed we would like an instant display of completion of the last query word which would lead to good hit at the same time the best hit for any of these completion should be displayed known indexing data structure that apply to this problem either incur large processing time for a substantial class of query or they use a lot of space we present a new indexing data structure that us no more space than a state of the art compressed inverted index but with time faster query processing time even on the large trec terabyte collection which comprises over million document we achieve on a single machine and with the index on disk average response time of one tenth of a second we have built a full fledged interactive search engine that realizes the proposed autocompletion feature combined with support for proximity search semi structured xml text subword and phrase completion and semantic tag 
this paper describes personalization metadata standard that can be used to enable individual to access and use resource based on a user s particular requirement the paper describes two approach which are being developed in the library and web world and highlight some of the potential challenge which will need to be addressed in order to maximise interoperability the paper concludes by arguing the need for greater dialogue across these two community 
context influence the search process but to date research ha not definitively identified which aspect of context are the most influential for information retrieval and thus are worthy of integration in today s retrieval system in this research we isolated for examination two aspect of context task and document genre and examined the relationship between them within a software engineering work domain in this domain the nature of the task ha an impact on decision of relevance and usefulness and the document collection contains a distinctive set of genre our data set wa a document repository created and used by our target population the document surrogate were meta tagged by purpose and document type correspondence analysis of this categorical data identified some specific relationship between genre and task a well a four broad dimension of variability underlying these relationship these result have the potential to inform the design of a contextual retrieval system by refining search result for this domain 
per query relevance measure provide standardized repeatable measurement of search result quality but they ignore much of what user actually experience in a full search session this paper examines how well we can approximate a user s ultimate session level satisfaction using a simple relevance metric we find that thisrelationship is surprisingly strong by incorporating additional property of the query itself we construct a model which predicts user satisfaction even more accurately than relevance alone 
standard machine learning technique typically require ample training data in the form of labeled instance in many situation it may be too tedious or costly to obtain sufficient labeled data for adequate classifier performance however in text classification human can easily guess the relevance of feature that is word that are indicative of a topic thereby enabling the classifier to focus it feature weight more appropriately in the absence of sufficient labeled data we will describe an algorithm for tandem learning that begin with a couple of labeled instance and then at each iteration recommends feature and instance for a human to label tandem learning using an oracle result in much better performance than learning on only feature or only instance we find that human can emulate the oracle to an extent that result in performance accuracy comparable to that of the oracle our unique experimental design help factor out system error from human error leading to a better understanding of when and why interactive feature selection work 
many web ir and digital library application require a crawling process to collect page with the ultimate goal of taking advantage of useful information available on web site for some of these application the criterion to determine when a page is to be present in a collection are related to the page content however there are situation in which the inner structure of the page provides a better criterion to guide the crawling process than their content in this paper we present a structure driven approach for generating web crawler that requires a minimum effort from user the idea is to take a input a sample page and an entry point to a web site and generate a structure driven crawler based on navigation pattern sequence of pattern for the link a crawler ha to follow to reach the page structurally similar to the sample page in the experiment we have carried out structure driven crawler generated by our new approach were able to collect all page that match the sample given including those page added after their generation 
in this paper we propose a novel approach of image annotation byconstructing a hierarchical mapping between low level visualfeatures and text feature utilizing the relation within and acrossboth visual feature and text feature moreover we propose a novelannotation strategy that maximizes both the accuracy and thediversity of the generated annotation by generalizing or specifyingthe annotation in the corresponding annotation hierarchy experiment with scientific image from royal society ofchemistry journal show that the proposed annotation approachproduces satisfactory result at different level of annotation 
the web today includes many page intended to deceive search engine and attain an unwarranted result ranking since the link among web page are used to calculate authority ranking system would benefit from knowing which page contain content to be trusted and which do not we propose and compare various trust propagation method to estimate the trustworthiness of each page we find that a non trust preserving propagation method is able to achieve close to a fifty percent improvement over trustrank in separating spam from non spam page 
the currently booming search engine industry ha determined many online organization to attempt to artificially increase their ranking in order to attract more visitor to their web site at the same time the growth of the web ha also inherently generated several navigational hyperlink structure that have a negative impact on the importance measure employed by current search engine in this paper we propose and evaluate algorithm for identifying all these noisy link on the web graph may them be spam or simple relationship between real world entity represented by site replication of content etc unlike prior work we target a different type of noisy link structure residing at the site level instead of the page level we thus investigate and annihilate site level mutual reinforcement relationship abnormal support coming from one site towards another a well a complex link alliance between web site our experiment with the link database of the todobr search engine show a very strong increase in the quality of the output ranking after having applied our technique 
this paper describes a kernel based web service abbreviated a service matching mechanism for service discovery and integration the matching mechanism try to exploit the latent semantics by the structure of service using textual similarity and n spectrum kernel value a feature of low level and mid level we build up a model to estimate the functional similarity between service whose parameter are learned by a ranking svm the experiment result showed that several metric for the retrieval of service have been improved by our approach 
since the website is one of the most important organizational structure of the web how to effectively rank website ha been essential to many web application such a web search and crawling in order to get the rank of website researcher used to describe the inter connectivity among website with a so called hostgraph in which the node denote website and the edge denote linkage between website if and only if there are hyperlink from the page in one website to the page in the other there will be an edge between these two website and then adopted the random walk model in the hostgraph however a pointed in this paper the random walk over such a hostgraph is not reasonable because it is not in accordance with the browsing behavior of web surfer therefore the derivate rank cannot represent the true probability of visiting the corresponding website in this work we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the pagerank value of the page inside that website nevertheless since the number of web page is much larger than that of website it is not feasible to base the calculation of the rank of website on the calculation of pagerank to tackle this problem we proposed a novel method named aggregaterank rooted in the theory of stochastic complement which cannot only approximate the sum of pagerank accurately but also have a lower computational complexity than pagerank both theoretical analysis and experimental evaluation show that aggregaterank is a better method for ranking website than previous method 
in most existing retrieval model document are scored primarily based on various kind of term statistic such a within document frequency inverse document frequency and document length intuitively the proximity of matched query term in a document can also be exploited to promote score of document in which the matched query term are close to each other such a proximity heuristic however ha been largely under explored in the literature it is unclear how we can model proximity and incorporate a proximity measure into an existing retrieval model in this paper we systematically explore the query term proximity heuristic specifically we propose and study the effectiveness of five different proximity measure each modeling proximity from a different perspective we then design two heuristic constraint and use them to guide u in incorporating the proposed proximity measure into an existing retrieval model experiment on five standard trec test collection show that one of the proposed proximity measure is indeed highly correlated with document relevance and by incorporating it into the kl divergence language model and the okapi bm model we can significantly improve retrieval performance 
this paper discus the importance of part order preservation in shape matching a part descriptor is introduced that support both preserving and abandoning the order of part the evaluation show that retrieval result are improved by almost if the original ordering is preserved 
information retrieval algorithm leverage various collection statistic to improve performance because these statistic are often computed on a relatively small evaluation corpus we believe using larger non evaluation corpus should improve performance specifically we advocate incorporating external corpus based on language modeling we refer to this process a external expansion when compared to traditional pseudo relevance feedback technique external expansion is more stable across topic and up to more effective in term of mean average precision our result show that using a high quality corpus that is comparable to the evaluation corpus can be a if not more effective than using the web our result also show that external expansion outperforms simulated relevance feedback in addition we propose a method for predicting the extent to which external expansion will improve retrieval performance our new measure demonstrates positive correlation with improvement in mean average precision 
we hypothesized that language modeling retrieval would improve if we reduced the need for document smoothing to provide an inverse document frequency idf like effect we created inverse collection frequency icf weighted query model a a tool to partially separate the idf like role from document smoothing compared to maximum likelihood estimated mle query the icf weighted query achieved a improvement in mean average precision on description query the icf weighted query performed better with le document smoothing than that required by mle query language modeling retrieval may benefit from a mean to separately incorporate an idf like behavior outside of document smoothing 
accessing an ever increasing number of email possibly on small mobile device ha become a major problem for many user email summarization is a promising way to solve this problem in this paper we propose a new framework for email summarization one novelty is to use a fragment quotation graph to try to capture an email conversation the second novelty is to use clue word to measure the importance of sentence in conversation summarization based on clue word and their score we propose a method called cws which is capable of producing a summary of any length a requested by the user we provide a comprehensive comparison of cws with various existing method on the enron data set preliminary result suggest that cws provides better summary than existing method 
with the growth of social bookmarking a new approach for metadata creation called tagging ha emerged in this paper we evaluate the use of tag presentation technique the main goal of our evaluation is to investigate the effect of some of the different property that can be utilized in presenting tag e g alphabetization using larger font etc we show that a number of these factor can affect the ease with which user can find tag and use the tool for presenting tag to user 
mining subtopics from weblogs and analyzing their spatiotemporal pattern have application in multiple domain in this paper we define the novel problem of mining spatiotemporal theme pattern from weblogs and propose a novel probabilistic approach to model the subtopic theme and spatiotemporal theme pattern simultaneously the proposed model discovers spatiotemporal theme pattern by extracting common theme from weblogs generating theme life cycle for each given location and generating theme snapshot for each given time period evolution of pattern can be discovered by comparative analysis of theme life cycle and theme snapshot experiment on three different data set show that the proposed approach can discover interesting spatiotemporal theme pattern effectively the proposed probabilistic model is general and can be used for spatiotemporal text mining on any domain with time and location information 
this paper proposes a method to combine text based and citation based retrieval method in the invalidity patent search using the ntcir test collection including eight year of uspto patent we show the effectiveness of our method experimentally 
figure in digital document contain important information current digital library do not summarize and index information available within figure for document retrieval we present our system on automatic categorization of figure and extraction of data from d plot a machine learning based method is used to categorize figure into a set of predefined type based on image feature an automated algorithm is designed to extract data value from solid line curve in d plot the semantic type of figure and extracted data value from d plot can be integrated with textual information within document to provide more effective document retrieval service for digital library user experimental evaluation ha demonstrated that our system can produce result suitable for real world use 
this paper describes the creation of a testbed to evaluate people searching strategy on the world wide web this task involves resolving person name ambiguity and locating relevant information characterising every individual under the same name 
web performance measurement and availability test have been carried out using a variety of infrastructure over the last several year disruption in the internet can lead to web site being unavailable or increase user perceived latency the unavailability could be due to dns failure in segment of the physical network cutting off thousand of user or attack prompt reaction to network wide event can be facilitated by local or remote measurement and monitoring better yet a distributed set of intercommunicating measurement and monitoring entity that react to event dynamically could go a long way to handle disruption we have designed and built atmen a triggered measurement infrastructure to communicate and coordinate across various administrative entity atmen node can trigger new measurement query ongoing passive measurement or historical measurement stored on remote node and coordinate the response to make local decision atmen reduces wasted measurement by judiciously reusing measurement along three ax spatial temporal and application we describe the use of atmen for key web application such a performance based ranking of popular web site and availability of dns server on which most web transaction are dependent the evaluation of atmen is done using multiple network monitoring entity called gigascopes installed across the usa measurement data of a popular network application involving million of user distributed across the internet and score of client to aid in gathering measurement information upon demand our result show that such a system can be built in a scalable fashion 
we propose a novel method of analysing data gathered fromtrec or similar information retrieval evaluation experiment we define two normalized version of average precision that we use to construct a weighted bipartite graph of trec system and topic we analyze the meaning of well known and somewhat generalized indicator fromsocial network analysis on the system topic graph we apply this method to an analysis of trec data amongthe result we find that authority measure system performance that hubness of topic reveals that some topic are better than others at distinguishing more or le effective system that with current measure a system that want to be effective in trec need to be effective on easy topic and that by using different effectiveness measure this is no longer the case 
the personal project planner prototype work a an extension to the file manager to provide people with rich text overlay to their information folder file and also email web page note rich text document like project plan can be created which then provide a context in which to create or reference the email message electronic document web page etc that are needed to complete the plan the user can later locate an information item such a an email message with reference to the plan e g a an alternative to a mostly context free search through the inbox or sent mail the planner explores a possibility that an effective organization of project related information can emerge a a natural by product of effort to plan and structure the project 
this paper present a new management method for morphological variation of keywords the method is called fcg frequent case generation it is based on the skewed distribution of word form in natural language and is suitable for language that have either fair amount of morphological variation or are morphologically very rich the proposed method ha been evaluated so far with four language finnish swedish german and russian which show varying degree of morphological complexity 
this poster investigates a novel query suggestion technique that selects query refinement through a combination of many user post query navigation pattern and the query log of a large search engine we compare this technique which us the query that retrieve in the top ranked search result place where searcher end up after post query browsing i e the landing page with an approach based on query refinement from user search session extracted from query log our finding demonstrate the effectiveness of using landing page for the direct generation of query suggestion a well a the complementary nature of the suggestion it generates with regard to traditional query log based refinement methodology 
this paper proposes a novel framework for music content indexing and retrieval the music structure information i e timing harmony and music region content is represented by the layer of the music structure pyramid we begin by extracting this layered structure information we analyze the rhythm of the music and then segment the signal proportional to the inter beat interval thus the timing information is incorporated in the segmentation process which we call beat space segmentation to describe harmony event we propose a two layer hierarchical approach to model the music chord we also model the progression of instrumental and vocal content a acoustic event after information extraction we propose a vector space modeling approach which us these event a the indexing term in query by example music retrieval a query is represented by a vector of the statistic of the n gram event we then propose two effective retrieval model a hard indexing scheme and a soft indexing scheme experiment show that the vector space modeling is effective in representing the layered music information achieving top retrieval accuracy using sec music clip a the query the soft indexing outperforms hard indexing in general 
given a huge online social network how do we retrieve information from it through crawling even better how do we improve the crawling performance by using parallel crawler that work independently in this paper we present the framework of parallel crawler for online social network utilizing a centralized queue to show how this work in practice we describe our implementation of the crawler for an online auction website the crawler work independently therefore the failing of one crawler doe not affect the others at all the framework ensures that no redundant crawling would occur using the crawler that we built we visited a total of approximately million auction user about of which were completely crawled 
term dependency or co occurrence ha been studied in language modelling for instance by metzler croft who showed that retrieval performance could be significantlyenhanced using term dependency information in this work weshow how term dependency can be modelled within the divergence from randomness dfr framework we evaluate our term dependency model on the two adhoc retrieval task using the trec gov terabyte collection furthermore we examine the effect of varying the term dependency window size on the retrieval performance of the proposed model our experiment show that term dependency can indeed besuccessfully incorporated within the dfr framework 
new type of document collection are being developed by various web service the service provider keep track of non textual feature such a click count in this paper we present a framework to use non textual feature to predict the quality of document we also show our quality measure can be successfully incorporated into the language modeling based retrieval model we test our approach on a collection of question and answer pair gathered from a community based question answering service where people ask and answer question experimental result using our quality measure show a significant improvement over our baseline 
in this paper we describe an approach for the verification of web service composition defined by set of bpel process the key aspect of such a verification is the model adopted for representing the communication among the service participating in the composition indeed these communication are asynchronous and buffered in the existing execution framework while most verification approach assume a synchronous communication model for efficiency reason in our approach we develop a parametric model for describing web service composition which allows u to capture a hierarchy of communication model ranging from synchronous communication to asynchronous communication with complex buffer structure moreover we develop a technique to associate with a web service composition the most adequate communication model i e the simplest model that is sufficient to capture all the behavior of the composition this way we can provide an accurate model of a wider class of service composition scenario while preserving a much a possible an efficient performance in verification 
the effectiveness of information retrieval system is measured by comparing performance on a common set of query and document significance test are often used to evaluate the reliability of such comparison previous work ha examined such test but produced result with limited application other work established an alternative benchmark for significance but the resulting test wa too stringent in this paper we revisit the question of how such test should be used we find that the t test is highly reliable more so than the sign or wilcoxon test and is far more reliable than simply showing a large percentage difference in effectiveness measure between ir system our result show that past empirical work on significance test over estimated the error of such test we also re consider comparison between the reliability of precision at rank and mean average precision arguing that past comparison did not consider the assessor effort required to compute such measure this investigation show that assessor effort would be better spent building test collection with more topic each assessed in le detail 
we present globedb a system for hosting web application that performs autonomic replication of application data globedb offer data intensive web application the benefit of low access latency and reduced update traffic the major distinction in our system compared to existing edge computing infrastructure is that the process of distribution and replication of application data is handled by the system automatically with very little manual administration we show that significant performance gain can be obtained this way performance evaluation with the tpc w benchmark over an emulated wide area network show that globedb reduces latency by a factor of compared to non replicated system and reduces update traffic by a factor of compared to fully replicated system 
a xml database size grow the amount of space used for storing the data and auxiliary data structure becomes a major factor in query and update performance this paper present a new storage scheme for xml data that support all navigational operation in near constant time in addition to supporting efficient query the space requirement of the proposed scheme is within a constant factor of the information theoretic minimum while insertion and deletion can be performed in near constant time a well a a result the proposed structure feature a small memory footprint that increase cache locality whilst still supporting standard apis such a dom and necessary database operation such a query and update efficiently analysis and experiment show that the proposed structure is space and time efficient 
this paper present a contribution to image indexing applied to the document creation task the presented method rank a set of photograph based on how well they aesthetically work within a predefined document color harmony document visual balance and image quality are taken into consideration a user study conducted on people with a range of expertise in document creation helped gather the right visual feature to consider by the algorithm this show some benefit for the traditional document creation task a well a for the case of ever changing web page banner color and layout 
in this paper a hierarchical classification framework ha been proposed for bridging the semantic gap effectively and achieving multi level image annotation automatically first the semantic gap between the low level computable visual feature and user real information need is partitioned into four smaller gap and multiple approachesallare proposed to bridge these smaller gap more effectively to learn more reliable contextual relationship between the atomic image concept and the co appearance of salient object a multi modal boosting algorithm is proposed to enable hierarchical image classification and avoid inter level error transmission a hierarchical boosting algorithm is proposed by incorporating concept ontology and multi task learning to achieve hierarchical image classifier training with automatic error recovery to bridge the gap between the computable image concept and the user real information need a novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the user with a good global view of large scale image collection our experiment on large scale image database have also obtained very positive result 
support vector machine svms have been very successful in text classification however the intrinsic geometric structure of text data ha been ignored by standard kernel commonly used in svms it is natural to assume that the document are on the multinomial manifold which is the simplex of multinomial model furnished with the riemannian structure induced by the fisher information metric we prove that the negative geodesic distance ngd on the multinomial manifold is conditionally positive definite cpd thus can be used a a kernel in svms experiment show the ngd kernel on the multinomial manifold to be effective for text classification significantly outperforming standard kernel on the ambient euclidean space 
in this paper we propose a dual index model for contextual ir for each query we search against both document level and passage level index and use the corresponding merge function to update the weight for both document and paragraph by combining the result from both index according to the granularity information in metadata experiment on trec data show that a significant improvement can be made by using the dual index model 
web process must often operate in volatile environment where the quality of service parameter of the participating service provider change during the life time of the process in order to remain optimal the web process must adapt to these change adaptation requires knowledge about the parameter change of each of the service provider and using this knowledge to determine whether the web process should make a different more optimal decision previously we defined a mechanism called the value of changed information which measure the impact of expected change in the service parameter on the web process thereby offering a way to query and incorporate those change that are useful and cost efficient however computing the value of changed information incurs a substantial computational overhead in this paper we use service expiration time obtained from pre defined service level agreement to reduce the computational overhead of adaptation we formalize the intuition that service whose parameter have not expired need not be considered for querying for revised information using two realistic scenario we illustrate our approach and demonstrate the associated computational saving 
sponsored search is a major revenue source for search company web searcher can issue any query while advertisement keywords are limited query rewriting technique effectively match user query with relevant advertisement keywords thus increase the amount of web advertisement available the match relevance is critical for click in this study we aim to improve query rewriting relevance for this purpose we use an active learning algorithm called transductive experimental design to select the most informative sample to train the query rewriting relevance model experiment show that this approach significantly improves model accuracy and rewriting relevance 
accurate estimation of information retrieval evaluation metric such a average precision require large set of relevance judgment building set large enough for evaluation of real world implementation is at best inefficient at worst infeasible in this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation a new way of looking at average precision lead to a natural algorithm for selecting document to judge and allows u to estimate the degree of confidence by defining a distribution over possible document judgment a study with annotator show that this method can be used by a small group of researcher to rank a set of system in under three hour with confidence information retrieval metric such a average precision require large set of relevance judgment to be accurately estimated building these set is infeasible and often inefficient for many real world retrieval implementation we present a new way of looking at average precision that allows u to estimate the confidence in an evaluation based on the size of the test collection we use this to build an algorithm for selecting the best document to judge to have maximum confidence in an evaluation with a minimal number of relevance judgment a study with annotator show how the algorithm can be used by a small group of researcher to quickly rank a set of system with confidence 
in this paper we show that plsi and nmf optimize the same objective function although plsi and nmf are different algorithm a verified by experiment in addition we also propose a new hybrid method that run plsi and nmf alternatively to achieve better solution 
supervised learning approach to text classification are in practice often required to work with small and unsystematically collected training set the alternative to supervised learning is usually viewed to be building classifier by hand using a domain expert s understanding of which feature of the text are related to the class of interest this is expensive requires a degree of sophistication about linguistics and classification and make it difficult to use combination of weak predictor we propose instead combining domain knowledge with training example in a bayesian framework domain knowledge is used to specify a prior distribution for the parameter of a logistic regression model and labeled training data is used to produce a posterior distribution whose mode we take a the final classifier we show on three text categorization data set that this approach can rescue what would otherwise be disastrously bad training situation producing much more effective classifier 
server side programming is one of the key technology that support today s www environment it make it possible to generate web page dynamically according to a user s request and to customize page for each user however the flexibility obtained by server side programming make it much harder to guarantee validity and security of dynamically generated page to check statically the property of web page generated dynamically by a server side program we develop a static program analysis that approximates the string output of a program with a context free grammar the approximation obtained by the analyzer can be used to check various property of a server side program and the page it generates to demonstrate the effectiveness of the analysis we have implemented a string analyzer for the server side scripting language php the analyzer is successfully applied to publicly available php program to detect cross site scripting vulnerability and to validate page they generate dynamically 
we develop and implement a new indexing technology which allows u to use complete and possibly very large document a query while having a retrieval performance comparable to a standard term query our approach aim at retrieval task such a near duplicate detection and high similarity search to demonstrate the performance of our technology we have compiled the search index wikipedia in the pocket which contains about million english and german wikipedia article this index along with a search interface fit on a conventional cd gigabyte the ingredient of our indexing technology are similarity hashing and minimal perfect hashing 
graphical model have been applied to various information retrieval and natural language processing task in the recent literature in this paper we apply a probabilistic graphical model for answer ranking in question answering this model estimate the joint probability of correctness of all answer candidate from which the probability of correctness of an individual candidate can be inferred the joint prediction model can estimate both the correctness of individual answer a well a their correlation which enables a list of accurate and comprehensive answer this model wa compared with a logistic regression model which directly estimate the probability of correctness of each individual answer candidate an extensive set of empirical result based on trec question demonstrates the effectiveness of the joint model for answer ranking furthermore we combine the joint model with the logistic regression model to improve the efficiency and accuracy of answer ranking 
many online news blog aggregator like google yahoo and msn allow user to browse search many hundred of news source this result in dozen often hundred of story about the same event while the news aggregator cluster these story allowing the user to efficiently scan the major news item at any given time they do not currently allow alternative browsing mechanism within the cluster furthermore their intra cluster ranking mechanism are often based on a notion of authority popularity of the source in many case this lead to the classic power law phenomenon the popular story source are the one that are already popular authoritative thus reinforcing one dominant viewpoint ideally these aggregator would exploit the availability of the tremendous number of source to identify the various dominant thread or viewpoint about a story and highlight these thread for the user this paper present an initial limited approach to such an interface it classifies article into two category fact and opinion we show that the combination of i a classifier trained on a small k training set of editorial report and ii an interactive user interface that ameliorates classification error by re ordering the presentation can be effective in highlighting different underlying viewpoint in a story cluster we briefly discus the classifier used here the training set and the ui and report on some initial anecdotal user feedback and evaluation 
method for detecting sentence in an input document set which are both relevant and novel with respect to an information need would be of direct benefit to many system such a extractive text summarizers however satisfactory level of agreement between judge performing this task manually have yet to demonstrated leaving researcher to conclude that the task is too subjective in previous experiment judge were asked to first identify sentence that are relevant to a general topic and then to eliminate sentence from the list that do not contain new information currently a new task is proposed in which annotator perform the same procedure but within the context of a specific factual information need in the experiment satisfactory level of agreement between independent annotator were achieved on the first step of identifying sentence containing relevant information relevant however the result indicate that judge do not agree on which sentence contain novel information 
the emergence of personalized homepage service e g personalized google homepage and microsoft window live ha enabled web user to select web content of interest and to aggregate them in a single web page the web content are often predefined content block provided by the service provider however it involves intensive manual effort to define the content block and maintain the information in it in this paper we propose a novel personalized homepage system called homepage live to allow end user to use drag and drop action to collect their favorite web content block from existing web page and organize them in a single page moreover homepage live automatically trace the change of block with the evolvement of the container page by measuring the tree edit distance of the selected block by exploiting the immutable element of web page the tracing algorithm performance is significantly improved the experimental result demonstrate the effectiveness and efficiency of our algorithm 
we evaluated the interactive retrieval functionality of the concentric ring view according to a series of usability study this is a ring structure based graphical user interface like a planisphere for image retrieval with multi faceted metadata attribute value for each facet are arranged on a ring and retrieved image are displayed inside using search key derived from the attribute value on the bottom part of the ring by rotating the ring user can browse retrieved image while adjusting search key the first usability test conducted with thirty six participant confirmed that i novice user even junior high school student could use this interface ii user could find image better than anticipated and iii the interface wa good at choosing the first relevant image but user could not refine retrieval because they were unable to reuse retrieved result to solve this problem we added two functionality personal history for reuse and relevance feedback with these improvement we named the new version of the interface concentric ring view f a second usability test with seven participant confirmed the effectiveness of this newer interface 
i present analysis examining some of the cause of poor connectivity in developing country based on a techno economic analysis and design i show that technical limitation per se are not the bottleneck for widespread connectivity rather design policy and regulatory challenge dominate 
summative evaluation method for supervised adaptive topic tracking system convolve the effect of system decision on present utility with the effect on future utility this paper describes a new formative evaluation approach that focus on future utility for use in the design stage of adaptive system topic model quality is assessed at a predefined set of point using a fixed document set to enhance comparability experiment using a vector space topic tracking system illustrate the utility of this approach to formative evaluation 
comparing and contrasting is an important strategy people employ to understand new situation and create solution for new problem similar event can provide hint for problem solving a well a larger context for understanding the specific circumstance of an event lesson can leaned from past experience insight can be gained about the new situation from familiar example and trend can be discovered among similar event a the largest knowledge base for human being the web provides both an opportunity and a challenge to discover comparable case in order to facilitate situation analysis and problem solving in this paper we present compare contrast a system that us the web to discover comparable case for news story document about similar situation but involving distinct entity the system analyzes a news story given by the user and build a model of the story with the story model the system dynamically discovers entity comparable to the main entity in the original story and us these comparable entity a seed to retrieve web page about comparable case the system is domain independent doe not require any domain specific knowledge engineering effort and deal with the complexity of unstructured text and noise on the web in a robust way we evaluated the system with an experiment on a collection of news article and a user study 
this poster focus on the study of term context dependence in the application of sentence retrieval based on markov random field mrf three form of dependence among query term are considered under different assumption of term dependence relationship three feature function are defined with the purpose to utilize association feature between query term in sentence to evaluate the relevance of sentence experimental result have proven the efficiency of the proposed retrieval model in improving the performance of sentence retrieval 
weblog ha quickly evolved into a new information and knowledge dissemination channel yet it is not easy to discover weblog community through keyword search the main contribution of this paper is the study of weblog community from the perspective of social network analysis we proposed a new way of collecting and preparing data for weblog community discovery the data collection stage focus on gaining knowledge of the strength of social tie between weblogs the strength of social tie and the clustering feature of social network guided the discovery of weblog community 
this paper examines a new approach to information distillation over temporally ordered document and proposes a novel evaluation scheme for such a framework it combine the strength of and extends beyond conventional adaptive filtering novelty detection and non redundant passage ranking with respect to long lasting information need task with multiple query our approach support fine grained user feedback via highlighting of arbitrary span of text and leverage such information for utility optimization in adaptive setting for our experiment we defined hypothetical task based on news event in the tdt corpus with multiple query per task answer key nugget were generated for each query and a semi automatic procedure wa used for acquiring rule that allow automatically matching nugget against system response we also propose an extension of the ndcg metric for assessing the utility of ranked passage a a combination of relevance and novelty our result show encouraging utility enhancement using the new approach compared to the baseline system without incremental learning or the novelty detection component 
in real world use of test collection method it is essential that the query test set be representative of the work load expected in the actual application using a random sample of query from a medium company s query log a a gold standard test set we demonstrate that bias in sitemap derived and top n query set can lead to significant perturbation in engine ranking and big difference in estimated performance level 
we address the problem of identifying the domain of onlinedatabases more precisely given a set f of web form automaticallygathered by a focused crawler and an online databasedomain d our goal is to select from f only the formsthat are entry point to database in d having a set ofwebforms that serve a entry point to similar online databasesis a requirement for many application and technique thataim to extract and integrate hidden web information suchas meta searcher online database directory hidden webcrawlers and form schema matching and merging we propose a new strategy that automatically and accuratelyclassifies online database based on feature that canbe easily extracted from web form by judiciously partitioningthe space of form feature this strategy allows theuse of simpler classifier that can be constructed using learningtechniques that are better suited for the feature of eachpartition experiment using real web data in a representativeset of domain show that the use of different classifiersleads to high accuracy precision and recall this indicatesthat our modular classifier composition provides an effectiveand scalable solution for classifying online database 
since the publication of brin and page s paper on pagerank many in the web community have depended on pagerank for the static query independent ordering of web page we show that we can significantly outperform pagerank using feature that are independent of the link structure of the web we gain a further boost in accuracy by using data on the frequency at which user visit web page we use ranknet a ranking machine learning algorithm to combine these and other static feature based on anchor text and domain characteristic the resulting model achieves a static ranking pairwise accuracy of v for pagerank or for random 
vector space model vsm ha been at the core of information retrieval for the past decade vsm considers the document a vector in high dimensional space in such a vector space technique like latent semantic indexing lsi support vector machine svm naive bayes etc can be then applied for indexing and classification however in some case the dimensionality of the document space might be extremely large which make these technique infeasible due to the curse of dimensionality in this paper we propose a novel tensor space model for document analysis we represent document a the second order tensor or matrix correspondingly a novel indexing algorithm called tensor latent semantic indexing tensorlsi is developed in the tensor space our theoretical analysis show that tensorlsi is much more computationally efficient than the conventional latent semantic indexing which make it applicable for extremely large scale data set several experimental result on standard document data set demonstrate the efficiency and effectiveness of our algorithm 
in this paper we investigate method for improving the performance of morph based spoken document retrieval in finnish by extracting relevant index term from confusion network our approach us morpheme like subword unit morphs for recognition and indexing this alleviates the problem of out of vocabulary word especially with inflectional language like finnish confusion network offer a convenient representation of alternative recognition candidate by aligning mutually exclusive term and by giving the posterior probability of each term the rank of the competing term and their posterior probability is used to estimate term frequency for indexing comparing against best recognizer transcript we show that retrieval effectiveness is significantly improved finally the effect of pruning in recognition is analyzed showing that when recognition speed is increased the reduction in retrieval performance due to the increase in the best error rate can be compensated by using confusion network 
we describe a framework for automatically selecting a summary set of photograph from a large collection of geo referenced photo the summary algorithm is based on spatial pattern in photo set but can be expanded to support social temporal a well a textual topical factor of the photo set the summary set can be biased by the user the content of the user s query and the context in which the query is made an initial evaluation on a set of geo referenced photo show that our algorithm performs well producing result that are highly rated by user 
intersecting inverted index is a fundamental operation for many application in information retrieval and database efficient indexing for this operation is known to be a hard problem for arbitrary data distribution however text corpus used in information retrieval application often have convenient power law constraint also known a zipf s law and long tail that allow u to materialize carefully chosen combination of multi keyword index which significantly improve worst case performance without requiring excessive storage these multi keyword index limit the number of posting accessed when computing arbitrary index intersection our evaluation on an e commerce collection of million product show that the index of up to four arbitrary keywords can be intersected while accessing le than of the posting in the largest single keyword index 
focused web browsing activity such a periodically looking up headline news weather report etc which require only selective fragment of particular web page can be made more efficient for user of limited display size handheld mobile device by delivering only the target fragment semantic bookmark provide a robust conceptual framework for recording and retrieving such targeted content not only from the specific page used in creating the bookmark but also from any user specified page with similar content semantics this paper describes a technique for realizing semantic bookmark by coupling machine learning with web page segmentation to create a statistical model of the bookmarked content these model are used to identify and retrieve the bookmarked content from web page that share a common content domain in contrast to ontology based approach where semantic bookmark are limited to available concept in the ontology the learning based approach allows user to bookmark ad hoc personalized semantic concept to effectively target content that fit the limited display of handhelds user evaluation measuring the effectiveness of a prototype implementation of learning based semantic bookmarking at reducing browsing fatigue in handhelds is provided 
we describe and evaluate an approach to personalizing web search that involves post processing the result returned by some underlying search engine so that they re ect the interest of a community of like minded searcher to do this we leverage the search experience of the community by mining the title and snippet text of result that have been selected by community member in response to their query our approach seek to build a community based snippet index that re ect the evolving interest of a group of searcher this index is then sed to re rank the result returned by the underlying search engine by boosting the ranking of key result that have been freq ently selected for similar q erie by community member in the past 
ranking problem is becoming important in many field especially in information retrieval ir many machine learning technique have been proposed for ranking problem such a ranksvm rankboost and ranknet among them ranknet which is based on a probabilistic ranking framework is leading to promising result and ha been applied to a commercial web search engine in this paper we conduct further study on the probabilistic ranking framework and provide a novel loss function named fidelity loss for measuring loss of ranking the fidelity loss notonly inherits effective property of the probabilistic ranking framework in ranknet but posse new property that are helpful for ranking this includes the fidelity loss obtaining zero for each document pair and having a finite upper bound that is necessary for conducting query level normalization we also propose an algorithm named frank based on a generalized additive model for the sake of minimizing the fedelity loss and learning an effective ranking function we evaluated the proposed algorithm for two datasets trec dataset and real web search dataset the experimental result show that the proposed frank algorithm outperforms other learning based ranking method on both conventional ir problem and web search 
table are ubiquitous unfortunately no search engine supportstable search in this paper we propose a novel table specificsearching engine tableseer to facilitate the table extracting indexing searching and sharing in addition wepropose an extensive set of medium independent metadata to precisely present table given a query tableseer rank the returned result using an innovative ranking algorithm tablerank with a tailored vector space model and a novel term weightingscheme experimental result show that tableseer outperforms existing search engine on table search in addition incorporating multiple weighting factor can significantly improve the ranking result 
the success of many innovative web application is not based on the content they produce but on how they combine and link existing content older web engineering method lack flexibility in a sense that they rely strongly on a priori knowledge of existing content structure and do not take into account initially unknown content source we propose the adoption of principle that are also found in component based software engineering to assemble highly extensible solution from reusable artifact the main contribution of our work is a support system consisting of a central service that manages n m relationship between arbitrary web resource and of web application component that realize navigation presentation and interaction for the linked content 
experienced web user have strategy for information search and re access that are not directly supported by web browser or search engine we studied how prevalent these strategy are and whether even experienced user have problem with searching and re accessing information with this aim we conducted a survey with experienced web user the result showed that this group ha frequently used key strategy e g using several browser window in parallel that they find important whereas some of the strategy that have been suggested in previous study are clearly le important for them e g including url on a webpage in some aspect such a query formulation this group resembles le experienced web user for instance we found that most of the respondent had misconception about how their search engine handle query a well a other problem with information search and re access in addition to presenting the prevalence of the strategy and rationale for their use we present concrete design solution and idea for making the key strategy also available to le experienced user 
many system use past behavior preference and environmental factor to attempt to predict user navigation on the internet however we believe that many of these model have shortcoming in that they do not take into account that user may have many different set of preference here we investigate an environmental factor namely time in making prediction about user navigation we present method for creating temporal rule that describe user navigation pattern we also show the benefit of using these rule to predict user navigation and also show the benefit of these model over traditional method an analysis is carried out on a sample of usage log for wireless application protocol wap browsing and the result of this analysis verify our hypothesis 
we report on observation on web characterization study that suggest that the amount of web link among site under different country code top level domain is related to the amount of trade between the corresponding country 
we present an evaluation of a novel hierarchical text summarization method that allows user to view summary of web document from small mobile device unlike previous approach ours doe not require the document to be in html since it infers a hierarchical structure automatically currently the method is used to summarize news article sent to a web mail account in plain text format subject used a web enabled mobile phone emulator to access the account s inbox and view the summarized news article they then used the summary to complete several information seeking task which involved answering factual question about the story in comparing the hierarchical text summary setting to that in which subject were given the full text article there wa no significant difference in task accuracy or the time taken to complete the task however in the hierarchical summarization setting the number of byte transferred per user request is le than half that of the full text case finally in comparing the new method to three other summarization method subject achieved significantly better accuracy on the task when using hierarchical summary 
generic database replication algorithm do not scale linearly in throughput a all update deletion and insertion udi query must be applied to every database replica the throughput is therefore limited to the point where the number of udi query alone is sufficient to overload one server in such scenario partial replication of a database can help a udi query are executed only by a subset of all server in this paper we propose globetp a system that employ partial replication to improve database throughput globetp exploit the fact that a web application s query workload is composed of a small set of read and write template using knowledge of these template and their respective execution cost globetp provides database table placement that produce significant improvement in database throughput we demonstrate the efficiency of this technique using two different industry standard benchmark in our experiment globetp increase the throughput by to compared to full replication while using identical hardware configuration furthermore adding a single query cache improves the throughput by another to 
we formulate and study search algorithm that consider a user s prior interaction with a wide variety of content to personalize that user s current web search rather than relying on the unrealistic assumption that people will precisely specify their intent when searching we pursue technique that leverage implicit information about the user s interest this information is used to re rank web search result within a relevance feedback framework we explore rich model of user interest built from both search related information such a previously issued query and previously visited web page and other information about the user such a document and email the user ha read and created our research suggests that rich representation of the user and the corpus are important for personalization but that it is possible to approximate these representation and provide efficient client side algorithm for personalizing search we show that such personalization algorithm can significantly improve on current web search 
web site that accept and display content such a wiki article or comment typically filter the content to prevent injected script code from running in browser that view the site the diversity of browser rendering algorithm and the desire to allow rich content make filtering quite difficult however and attack such a the samy and yamanner worm have exploited filtering weakness this paper proposes a simple alternative mechanism for preventing script injection called browser enforced embedded policy beep the idea is that a web site can embed a policy in it page that specifies which script are allowed to run the browser which know exactly when it will run a script can enforce this policy perfectly we have added beep support to several browser and built tool to simplify adding policy to web application we found that supporting beep in browser requires only small and localized modification modifying web application requires minimal effort and enforcing policy is generally lightweight 
link analysis algorithm have been extensively used in web information retrieval however current link analysis algorithm generally work on a flat link graph ignoring the hierarchal structure of the web graph they often suffer from two problem the sparsity of link graph and biased ranking of newly emerging page in this paper we propose a novel ranking algorithm called hierarchical rank a a solution to these two problem which considers both the hierarchical structure and the link structure of the web in this algorithm web page are first aggregated based on their hierarchical structure at directory host or domain level and link analysis is performed on the aggregated graph then the importance of each node on the aggregated graph is distributed to individual page belong to the node based on the hierarchical structure this algorithm allows the importance of linked web page to be distributed in the web page space even when the space is sparse and contains new page experimental result on the gov collection of trec and show that hierarchical ranking algorithm consistently outperforms other well known ranking algorithm including the pagerank blockrank and layerrank in addition experimental result show that link aggregation at the host level is much better than link aggregation at either the domain or directory level 
traditional web link based ranking scheme use a single score to measure a page s authority without concern of the community from which that authority is derived a a result a resource that is highly popular for one topic may dominate the result of another topic in which it is le authoritative to address this problem we suggest calculating a score vector for each page to distinguish the contribution from different topic using a random walk model that probabilistically combine page topic distribution and link structure we show how to incorporate the topical model within both pagerank and hit without affecting the overall property and still render insight into topic level transition experiment on multiple datasets indicate that our technique outperforms other ranking approach that incorporate textual analysis 
modern distributed information retrieval technique require accurate knowledge of collection size in non cooperative environment where detailed collection statistic are not available the size of the underlying collection must be estimated while several approach for the estimation of collection size have been proposed their accuracy ha not been thoroughly evaluated an empirical analysis of past estimation approach across a variety of collection demonstrates that their prediction accuracy is low motivated by ecological technique for the estimation of animal population we propose two new approach for the estimation of collection size we show that our approach are significantly more accurate that previous method and are more efficient in use of resource required to perform the estimation 
this paper investigates in a stringent athematical formalism the parallel derivation of three grand probabilistic retrieval model binary independent retrieval bir poisson model pm and language modelling lm the investigation ha been motivated by a number of question firstly though sharing the same origin namely the probability of relevance the model differ with respect to event space how can this be captured in a consistent notation and can we relate the event space secondly bir and pm are closely related but how doe lm fit in thirdly how are tf idf and probabilistic model related the parallel investigation of the model lead to a number of formalised result bir and pm assume the collection to be a set of non relevant document whereas lm assumes the collection to be a set of term from relevant document pm can be viewed a a bridge connecting bir and lm a bir lm equivalence explains bir a a special lm case pm explains tf idf and both bir and lm probability express tf idf in a dual way 
this paper develops a general formal framework for modeling term dependency via markov random field the model allows for arbitrary text feature to be incorporated a evidence in particular we make use of feature based on occurrence of single term ordered phrase and unordered phrase we explore full independence sequential dependence and full dependence variant of the model a novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data ad hoc retrieval experiment are presented on several newswire and web collection including the gov collection used at the trec terabyte track the result show significant improvement are possible by modeling dependency especially on the larger web collection 
extractor and tagger turn unstructured text into entity relation er graph where node are entity email paper person conference company and edge are relation wrote cited work for typed proximity search of the form type personnear company ibm paper xml is an increasingly usefulsearch paradigm in er graph proximity search implementation either perform a pagerank like computation at query time which is slow or precompute store and combine per word pageranks which can be very expensive in term of preprocessing time and space we present hubrank a new system for fast dynamic space efficient proximity search in er graph during preprocessing hubrank computesand index certain sketchy random walk fingerprint for a small fraction of node carefully chosen using query log statistic at query time a small active subgraph is identified bordered bynodes with indexed fingerprint these fingerprint are adaptively loaded to various resolution to form approximate personalized pagerank vector ppvs ppvs at remaining active node are now computed iteratively we report on experiment with citeseer s er graph and million of real cite seer query some representative number follow on our testbed hubrank preprocesses and index time faster than whole vocabulary ppv computation a text index occupies mb whole vocabulary ppvs would consume gb if ppvs are truncated to mb precision compared to true pagerank drop to incontrast hubrank ha precision at mb hubrank s average querytime is millisecond query time pagerank computation take second on average 
in this paper we propose a novel ranking scheme named affinity ranking ar to re rank search result by optimizing two metric diversity which indicates the variance of topic in a group of document information richness which measure the coverage of a single document to it topic both of the two metric are calculated from a directed link graph named affinity graph ag ag model the structure of a group of document based on the asymmetric content similarity between each pair of document experimental result in yahoo directory odp data and newsgroup data demonstrate that our proposed ranking algorithm significantly improves the search performance specifically the algorithm achieves improvement in diversity and improvement in information richness relatively within the top search result 
weblogs have become a prevalent source of information for people to express themselves in general there are two genre of content in weblogs the first kind is about the webloggers personal feeling thought or emotion we call this kind of weblogs affective article the second kind of weblogs is about technology and different kind of informative news in this paper we present a machine learning method for classifying informative and affective article among weblogs we consider this problem a a binary classification problem by using machine learning approach we achieve about on information retrieval performance measure including precision recall and f we set up three study on the application of above classification approach in both research and industrial field the above classification approach is used to improve the performance of classification of emotion from weblog article we also develop an intent driven weblog search engine based on the classification technique to improve the satisfaction of web user finally our approach is applied to search for weblogs with a great deal of informative article 
modern information retrieval ir test collection violate the completeness assumption of the cranfield paradigm in order to maximise the available resource only a sample of document i e the pool are judged for relevance by a human assessor s the subsequent evaluation protocol doe not make any distinction between assessed or unassesseddocuments a document that are not in the pool are assumedto be not relevant for the topic this is beneficial from a practical point of view a the relative performance can be compared with confidence if the experimental condition are fair for all system however given the incompleteness of relevance assessment two form of uncertainty emerge during evaluation the first is aleatory uncertainty which refers to variation in system performance across the topic set which is often addressed through the use of statistical significance test the second form of uncertainty is epistemic which refers to the amount of knowledge or ignorance we have about the estimate of a system s performance epistemic uncertainty is a consequence of incompleteness and is not addressed by the current evaluation protocol in this study we present a first attempt at modelling both aleatory and epistemic uncertainty associatedwith ir evaluation we aim to account for both the variability associated with system performance and the amount of knowledge known about the performance estimate 
scholarly entity such a article journal author and institution are now mostly ranked according to expert opinion and citation data the andrew w mellon foundation funded mesur project at the los alamo national laboratory is developing metric of scholarly impact that can rank a wide range of scholarly entity on the basis of their usage the mesur project start with the creation of a semantic network model of the scholarly community that integrates bibliographic citation and usage data collected from publisher and repository world wide it is estimated that this scholarly semantic network will include approximately million article million author journal and conference proceeding million citation and billion usage related event the largest scholarly semantic network ever created the developed scholarly semantic network will then serve a a standardized platform for the definition and validation of new metric of scholarly impact this poster describes the mesur project s data aggregation and processing technique including the owl scholarly ontology that wa developed to model the scholarly communication process 
xml delivers key advantage in interoperability due to it flexibility expressiveness and platform neutrality a xml ha become a performance critical aspect of the next generation of business computing infrastructure however it ha become increasingly clear that xml parsing often carry a heavy performance penalty and that current widely used parsing technology are unable to meet the performance demand of an xml based computing infrastructure several effort have been made to address this performance gap through the use of grammar based parser generation while the performance of generated parser ha been significantly improved adoption of the technology ha been hindered by the complexity of compiling and deploying the generated parser through careful analysis of the operation required for parsing and validation we have devised a set of specialized byte code designed for the task of xml parsing and validation these byte code are designed to engender the benefit of fine grained composition of parsing and validation that make existing compiled parser fast while being coarse grained enough to minimize interpreter overhead this technique of using an interpretive validating parser balance the need for performance against the requirement of simple tooling and robust scalable infrastructure our approach is demonstrated with a specialized schema compiler used to generate byte code which in turn drive an interpretive parser with almost a little tooling and deployment complexity a a traditional interpretive parser the byte code driven parser usually demonstrates performance within of the fastest fully compiled solution 
we present a design for verification approach to developing reliable web service we focus on composite web service which consist of asynchronously communicating peer our goal is to automatically verify property of interaction among such peer we propose a design pattern that eas the development of such web service and enables a modular assume guarantee style verification strategy in the proposed design pattern each peer is associated with a behavioral interface description which specifies how that peer will interact with other peer using these peer interface we automatically generate bpel specification to publish for interoperability assuming that the participating peer behave according to their interface we verify safety and liveness property about the global behavior of the composite web service during behavior verification during interface verification we check that each peer implementation conforms to it interface using the modularity in the proposed design pattern we are able to perform the interface verification of each peer and the behavior verification a separate step our experiment show that using this modular approach one can automatically and efficiently verify web service implementation 
the rich news system that can automatically annotate radio and television news with the aid of resource retrieved from the world wide web is described automatic speech recognition give a temporally precise but conceptually inaccurate annotation model information extraction from related web news site give the opposite conceptual accuracy but no temporal data our approach combine the two for temporally accurate conceptual semantic annotation of broadcast news first low quality transcript of the broadcast are produced using speech recognition and these are then automatically divided into section corresponding to individual news story a key phrase extraction component find key phrase for each story and us these to search for web page reporting the same event the text and meta data of the web page is then used to create index document for the story in the original broadcast which are semantically annotated using the kim knowledge management platform a web interface then allows conceptual search and browsing of news story and playing of the part of the medium file corresponding to each news story the use of material from the world wide web allows much higher quality textual description and semantic annotation to be produced than would have been possible using the asr transcript directly the semantic annotation can form a part of the semantic web and an evaluation show that the system operates with high precision and with a moderate level of recall 
in the spread of internet internet based information service business ha started to become profitable one of the key technology is personalization successful internet information service must realize personalized information delivery by which the user can automatically receive highly tuned information according to their personal need and preference in order to realize such personalized information service we have developed an automatic user preference capture and an automatic information clipping function based on a personalized information access technique in this paper those technique will be demonstrated by showing a deployed personalized webpage service application 
web clustering is useful for several activity in the www from automatically building web directory to improve retrieval performance nevertheless due to the huge size of the web a linear mechanism must be employed to cluster web document the k mean is one classic algorithm used in this problem we present a variant of the vector model to be used with the k mean algorithm our representation us symbolic object for clustering web document some experiment were done with positive result and future work is optimistic 
a social network can become base for information infrastructure in the future it is important to extract social network that are not biased providing a simple mean for user to register their social relation is also important we propose a method that combine various approach to extract social network especially three kind of network are extracted user registered know link network web mined web link network and face to face touch link network in this paper the combination of social network extraction for community is described and the analysis on the extracted social network is shown 
practical constrains of user interface make the user s judgment during the feedback loop deviate from real thought when the full document is read this is often overlooked in evaluation of relevance feedback this paper quantitatively analyze the impact of judging inconsistency on the performance of relevance feedback 
this poster present ongoing effort to enrich the rdf based semantic web with the tool of the historical event markup and linking project heml an experimental rdf vocabulary for heml data is illustrated a well a it use in storing and querying encoded historical event finally the practical use of heml rdf is illustrated with a toolkit for the piggy bank semantic browser plugin 
we propose measuring visualness of concept with image on the web that is what extent concept have visual characteristic this is a new application of web image mining to know which concept ha visually discriminative power is important for image recognition since not all concept are related to visual content mining image data on the web with our method enables it our method performs probabilistic region selection for image and computes an entropy measure which represents visualness of concept in the experiment we collected about forty thousand image from the web for concept we examined which concept are suitable for annotation of image content 
to alleviate the vocabulary problem this paper investigates the role of user term feedback in interactive text based image retrieval term feedback refers to the feedback from a user on specific term regarding their relevance to a target image previous study have indicated the effectiveness of term feedback in interactive text retrieval however the term feedback ha not shown to be effective in our experiment on text based image retrieval our result indicate that although term feedback ha a positive effect by allowing user to identify more relevant term it also ha a strong negative effect by providing more opportunity for user to specify irrelevant term to understand these different effect and their implication on the potential of term feedback this paper further present analysis of important factor that contribute to the utility of term feedback and discus the outlook of term feedback in interactive text based image retrieval 
despite it intuitive appeal the hypothesis that retrieval at the level of concept should outperform purely term based approach remains unverified empirically in addition the use of knowledge ha not consistently resulted in performance gain after identifying possible reason for previous negative result we present a novel framework for conceptual retrieval that articulates the type of knowledge that are important for information seeking we instantiate this general framework in the domain of clinical medicine based on the principle of evidence based medicine ebm experiment show that an ebm based scoring algorithm dramatically outperforms a state of the art baseline that employ only term statistic ablation study further yield a better understanding of the performance contribution of different component finally we discus how other domain can benefit from knowledge based approach 
one of the main hurdle towards a wide endorsement of ontology is the high cost of constructing them reuse of existing ontology offer a much cheaper alternative than building new one from scratch yet tool to support such reuse are still in their infancy however more ontology are becoming available on the web and online library for storing and indexing ontology are increasing in number and demand search engine have also started to appear to facilitate search and retrieval of online ontology this paper present a fresh view on constructing ontology automatically by identifying ranking and merging fragment of online ontology 
continuous query are used to monitor change to time varying data and to provide result useful for online decision making typically a user desire to obtain the value of some function over distributed data item for example to determine when and whether a the traffic entering a highway from multiple feed road will result in congestion in a thoroughfare or b the value of a stock portfolio exceeds a threshold using the standard web infrastructure for these application will increase the reach of the underlying information but since these query involve data from multiple source with source supporting standard http pull based interface special query processing technique are needed also these application often have the flexibility to tolerate some incoherency i e some difference between the result reported to the user and that produced from the virtual database made up of the distributed data source in this paper we develop and evaluate client pull based technique for refreshing data so that the result of the query over distributed data can be correctly reported conforming to the limited incoherency acceptable to the user we model a well a estimate the dynamic of the data item using a probabilistic approach based on markov chain depending on the dynamic of data we adapt the data refresh time to deliver query result with the desired coherency the commonality of data need of multiple query is exploited to further reduce refresh overhead effectiveness of our approach is demonstrated using live source of dynamic data the number of refreshes it requires is a an order of magnitude le than what we would need if every potential update is pulled from the source and b comparable to the number of message needed by an ideal algorithm one that know how to optimally refresh the data from distributed data source our evaluation also bring out a very practical and attractive tradeoff property of pull based approach e g a small increase in tolerable incoherency lead to a large decrease in message overhead 
indian business cluster have contributed immensely to the country s industrial output poverty alleviation and employment generation however with recent globalization these cluster can loose out to international competitor if they do not continuously innovate and take advantage of the new opportunity that are available through economic liberalization in this paper we discus how information and communication technology ict can help in improving the productivity and growth of these cluster 
the aspect model and the latent dirichlet allocation model are latent generative model proposed with the objective of modeling discrete data such a text though it is not explicitly published to the best of our knowledge it is reasonably well known in there search community that the aspect model doe not perform very well in supervised setting and also that latent model are frequently not identifiable i e their optimal parameter are not unique in this paper we make a much stronger claim about the pitfall of commonly used latent model by constructing a small synthetic but by no mean unrealistic corpus we show that latent model have inherent limitation that prevent them from recovering semantically meaningful parameter from data generated from a reasonable generative distribution in fact our experiment with supervised classification using the aspect model showed that it performance wa rather poor even worse than naive bayes leading u to the synthetic study we also analyze the scenario of using tempered em and show that it would not plug the above shortcoming our analysis suggests that there is also some scope for improvement in the latent dirichlet allocation model lda we then use our insight into the shortcoming of these model to come up with a promising variant of the lda that doe not suffer from the aforesaid drawback this could potentially lead to much better performance and model fit in the supervised scenario 
the success of the semantic web crucially depends on the existence of web page that provide machine understandable meta data this meta data is typically added in the semantic annotation process which is currently not part of the web engineering process web engineering however proposes methodology to design implement and maintain web application but lack the generation of meta data in this paper we introduce a technique to extend existing web engineering methodology to develop semantically annotated web page the novelty of this approach is the definition of a mapping from xml schema to ontology called weesa that can be used to automatically generate rdf meta data from xml content document we further show how we integrated the weesa mapping into an apache cocoon transformer to easily extend xml based web application to semantically annotated web application 
we propose a method to rate the credibility of news article using three clue commonality of the content of article among different news publisher numerical agreement versus contradiction of numerical value reported in the article and objectivity based on subjective speculative phrase and news source we tested this method on news story taken from seven different news site on the web the average agreement between the system produced credibility and the manual judgment of three human assessor on the sample article wa the limitation of the current approach and future direction are discussed 
xml is increasingly being used a a typed data format and therefore it becomes more important to gain access to the type system very often this is an xml schema the xml schema path language spath presented in this paper provides access to xml schema component by extending the well known xpath language to also include the domain of xml schema using spath xml developer gain access to xml schema and thus can more easily develop software which is typeor schema aware and thus more robust 
this paper describes how the bootstrap approach to statistic can be applied to the evaluation of ir effectiveness metric first we argue that bootstrap hypothesis test deserve more attention from the ir community a they are based on fewer assumption than traditional statistical significance test we then describe straightforward method for comparing the sensitivity of ir metric based on bootstrap hypothesis test unlike the heuristic based swap method proposed by voorhees and buckley our method estimate the performance difference required to achieve a given significance level directly from bootstrap hypothesis test result in addition we describe a simple way of examining the accuracy of rank correlation between two metric based on the bootstrap estimate of standard error we demonstrate the usefulness of our method using test collection and run from the ntcir clir track for comparing seven ir metric including those that can handle graded relevance and those based on the geometric mean 
this research examines the privacy comfort level of participant if others can view trace of their web browsing activity during a week long field study participant used an electronic diary daily to annotate each web page visited with a privacy level content category were used by participant to theoretically specify their privacy comfort for each category and by researcher to partition participant actual browsing the content category were clustered into group based on the dominant privacy level applied to the page inconsistency between participant in their privacy rating of category suggest that a general privacy management scheme is inappropriate participant consistency within category suggests that a personalized scheme may be feasible however a more fine grained approach to classification is required to improve result for site that tend to be general of multiple task purpose or dynamic in content 
searching an organization s document repository for expert provides a cost effective solution for the task of expert finding we present two general strategy to expert searching given a document collection which are formalized using generative probabilistic model the first of these directly model an expert s knowledge based on the document that they are associated with whilst the second locates document on topic and then find the associated expert forming reliable association is crucial to the performance of expert finding system consequently in our evaluation we compare the different approach exploring a variety of association along with other operational parameter such a topicality using the trec enterprise corpus we show that the second strategy consistently outperforms the first a comparison against other unsupervised technique reveals that our second model delivers excellent performance 
this research explores the relationship between information retrieval ir system effectiveness and user performance accuracy and speed and their satisfaction with the retrieved result precision of the result completeness of the result and overall system success previous study have concluded that improvement in ir system based on increase in ir effectiveness measure do not reflect on improvement in user performance this work aim at exploiting factor that can possibly be considered a confounding variable in interactive information retrieval iir evaluation in this research we look at substantive approach to evaluate iir system we aim to build an interactive evaluation framework that brings together aspect of system effectiveness and user performance and satisfaction this research also involves developing method for capturing user satisfaction with the retrieved result of ir system a well a examination how user ass their own performance in task completion furthermore we are also interested in identifying evaluation measure which are used in batch mode non interactive experiment but correlate well in interactive ir system thus by the end of this research we hope to develop a valid and reliable metric for iir evaluation a first study wa set up to explore the relationship between system effectiveness a quantified by traditional measure such a precision and recall and user effectiveness and satisfaction of the result though this study wa limited to few user the task involve finding image for recall based task it wa concluded that no direct relationship between system effectiveness and user performance people learn to adapt to a system regardless to it effectiveness this study recommends that a combination of measure e g system effectiveness user performance and satisfaction to be used to evaluate iir system based on our observation from this study we found that user familiarity of the search topic ha increased their performance thus we set up a second experiment to investigate how user satisfaction correlate with some ir effectiveness measure such a precision and the suite of cumulative gain measure cg dcg ndcg in simple web searching task result from this study have shown that cg and precision are better than ndcg at reflecting user satisfaction with the result of an ir system we have also concluded that user of web search engine in the context of simple search task are more concerned with precision than completeness of the search this stemmed from the stronger correlation between user satisfaction with the success of overall search and their satisfaction with the accuracy of the search result than with their satisfaction with the completeness of the search many scholar such a and have recommended considering perception of the user a important a ir effectiveness measure and both should be interpreted a measure of effectiveness therefore the issue in iir evaluation should not be focusing on maximizing the retrieval performance by refining ir technique alone but also understanding user satisfaction behavior and information need this raise the need for more investigation on measure that translate user performance and satisfaction a the criterion of a system future plan are to incorporate variable domain knowledge motivation task complexity and search behaviour on user performance and user evaluation of ir system performance when evaluating interactive ir system this is in an attempt to explore the suitability of different measure in iir evaluation thus the proposed approach adopts a systematic and multidimensional approach to evaluation including not only classical traditional evaluation measure such a precision and recall but also interactive non traditional measure such a user characteristic and their satisfaction 
although personalized search ha been proposed for many year and many personalization strategy have been investigated it is still unclear whether personalization is consistently effective on different query for different user and under different search context in this paper we study this problem and get some preliminary conclusion we present a large scale evaluation framework for personalized search based on query log and then evaluate five personalized search strategy including two click based and three profile based one using day msn query log by analyzing the result we reveal that personalized search ha significant improvement over common web search on some query but it also ha little effect on other query e g query with small click entropy it even harm search accuracy under some situation furthermore we show that straightforward click based personalization strategy perform consistently and considerably well while profile based one are unstable in our experiment we also reveal that both long term and short term context are very important in improving search performance for profile based personalized search strategy 
for many user with a disability it can be difficult or impossible to use a computer mouse to navigate the web an alternative way to select element on a web page is the label typing approach in which user select element by typing part of the label in most case these label are specified by the page author but some selectable element do not have an obvious textual description thus requiring that a label be generated the set of element label on a web page must be both efficient to select by text input and meaningful to the user this paper discus our approach to this problem using page structural analysis and user history to determine important element of a page and then matching this information with the efficiency model of the input device 
hierarchical topic detection is a new task in the tdt evaluation program which aim to organize an unstructured news collection in a directed acyclic graph dag structure reflecting the topic discussed we present a scalable architecture for htd and compare several alternative choice for agglomerative clustering and dag optimization in order to minimize the htd cost metric 
this paper present the architecture and the preliminary evaluation ofa request routing dns server that decouples server selectionfrom the rest of dns functionality our dns server which we refer toas myxdns expose well defined apis for uploading an externallycomputed server selection policy and for interacting with an external networkproximity service with myxdns researcher can explore their ownnetwork proximity metric and request routing algorithm withouthaving to worry about dns internals furthermore myxdns is based onopen source mydns and is available to public stress testing of myxdnsindicated that it achieves it flexibility at an acceptable cost asingle myxdns running on a low level server can process req secwith sub millisecond response even in the presence of continuousupdates to server selection policy 
test collection are most useful when they are reusable that is when they can be reliably used to rank system that did not contribute to the pool pooled relevance judgment for very large collection may not be reusable for two easons they will be very sparse and not sufficiently complete and they may be biased in the sense that theywill unfairly rank some class of system the trec terabyte track judged both a pool and a deep random sample in order to measure the effect of sparseness and bias 
collection selection ha been a research issue for year typically in related work precomputed statistic are employed in order to estimate the expected result quality of each collection and subsequently the collection are ranked accordingly our thesis is that this simple approach is insufficient for several application in which the collection typically overlap this is the case for example for the collection built by autonomous peer crawling the web we argue for the extension of existing quality measure using estimator of mutual overlap among collection and present experiment in which this combination outperforms cori a popular approach based on quality estimation we outline our prototype implementation of a p p web search engine coined minerva that allows handling large amount of data in a distributed and self organizing manner we conduct experiment which show that taking overlap into account during collection selection can drastically decrease the number of collection that have to be contacted in order to reach a satisfactory level of recall which is a great step toward the feasibility of distributed web search 
the world wide web ha been revolutionary in term of impact scale and outreach at every level society ha been changed in some way by the web this panel will consider likely development in this extraordinary human construct a we attempt to realise the next wave of the web a semantic web nigel shadbolt will chair a discussion that will focus on the prospect for the semantic web it likely form and the challenge it face can we achieve the necessary agreement on shared meaning for the semantic web can we achieve a critical mass of semantically annotated data and content how are we to trust such content do the scientific and commercial driver really demand a semantic web how will the move to a mobile and ubiquitous web affect the semantic web how doe web relate to the semantic web 
when a query is submitted to a search engine the search engine return a dynamically generated result page containing the result record each of which usually consists of a link to and or snippet of a retrieved web page in addition such a result page often also contains information irrelevant to the query such a information related to the hosting site of the search engine and advertisement in this paper we present a technique for automatically producing wrapper that can be used to extract search result record from dynamically generated result page returned by search engine automatic search result record extraction is very important for many application that need to interact with search engine such a automatic construction and maintenance of metasearch engine and deep web crawling the novel aspect of the proposed technique is that it utilizes both the visual content feature on the result page a displayed on a browser and the html tag structure of the html source file of the result page experimental result indicate that this technique can achieve very high extraction accuracy 
application and service that access web data are becoming increasingly more useful and wide spread current main stream web query language such a xquery xslt or sparql however focus only on one of the different data format available on the web in contrast xcerpt is a emphversatile semi structured query language i e a query language able to access all kind of web data such a xml and rdf in the same language reusing common concept and language construct to integrate heterogeneous data and a a foundation for semantic web reasoning xcerpt also provides rule xcerpt ha a visual companion language visxcerpt that is conceived a a mere rendering of the textual query language xcerpt using a slightly extended cs both language are demonstrated along a realistic use case integrating xml and rdf data highlighting interesting and unique feature novel language construct and optimization technique are currently under investigation in the xcerpt project cf urlhttp xcerpt org 
temporal and spatial information in text document is often expressed in a qualitative way moreover both are frequently affected by vagueness calling for appropriate extension of traditional framework for qualitative reasoning about time and space our research aim at defining such extension based on fuzzy set theory and applying the resulting framework to two important kind of intelligent information retrieval viz temporal question answering and geographic information retrieval 
this project develops tool to manage personal memory that include a multimedia retrieval system and user interface for different device this paper and demonstration present the mobile interface which allows browsing retrieving and taking picture that are automatically annotated with gps data and audio information the multimedia retrieval system us multimodal information visual content gps metadata and audio information the interface wa evaluated in a cultural heritage site 
this paper proposes a novel approach to measuring xml document similarity by taking into account the semantics between xml element the motivation of the proposed approach is to overcome the problem of under contribution and over contribution existing in previous work the element semantics are learned in an unsupervised way and the proportional transportation similarity is proposed to evaluate xml document similarity by modeling the similarity calculation a a transportation problem experiment of clustering are performed on three acm sigmod data set and result show the favorable performance of the proposed approach 
the web is rapidly moving towards a platform for mass collaboration in content production and consumption from three screen computer mobile phone and tv while there ha been a surge of interest in making web content accessible from mobile device there is a significant lack of progress when it come to making the web experience suitable for viewing on a television towards this end we describe a novel concept namely geotv where we explore a framework by which web content can be presented or pushed in a meaningful manner to create an entertainment experience for the tv audience fresh content on a variety of topic people and place is being created and made available on the web at breathtaking speed navigating fresh content effectively on tv demand a new browsing paradigm that requires fewer mouse click or user interaction from the remote control novel geospatial and temporal browsing technique are provided in geotv that allow user the capability of aggregating and navigating r enabled content in a timely personalized and automatic manner for viewing in an iptv environment this poster is an extension of our previous work on geotracker that utilizes both a geospatial representation and a temporal chronological presentation to help user spot the most relevant update quickly within the context of a web enabled environment we demonstrate the usability of such a tool that greatly enhances a user s ability in locating and browsing video based on his or her geographical interest and various innovative interface design for showing r enabled information in an iptv environment 
the high availability of video stream is making necessary mechanism for indexing such content in the web world in this paper we focus on news program and we propose a mechanism that integrates low and high level video feature to provide a high level semantic description a color luminance analysis is coupled with audio analysis to provide a better identification of all the video segment that compose the video stream each video segment is subject to speech detection and is described through mpeg so that the resulting metadata description can be used to index the video stream an experimental evaluation show the benefit of integrating audio and video analysis 
in this paper we study three basic and key issue related to web query processing load balance broker behavior and performance by individual index server our study while preliminary doe reveal interesting tradeoff load unbalance at low query arrival rate can be controlled with a simple measure of randomizing the distribution of document among the index server the broker is not a bottleneck and disk utilization is higher than cpu utilization 
we offer the first large scale analysis of web traffic based on network flow data using data collected on the internet network we constructed a weighted bipartite client server host graph containing more than x vertex and x edge valued by relative traffic flow when considered a a traffic map of the world wide web the generated graph provides valuable information on the statistical pattern that characterize the global information flow on the web statistical analysis show that client server connection and traffic flow exhibit heavy tailed probability distribution lacking any typical scale in particular the absence of an intrinsic average in some of the distribution implies the absence of a prototypical scale appropriate for server design web centric network design or traffic modeling the inspection of the amount of traffic handled by client and server and their number of connection highlight non trivial correlation between information flow and pattern of connectivity a well a the presence of anomalous statistical pattern related to the behavior of user on the web the result presented here may impact considerably the modeling scalability analysis and behavioral study of web application 
classical logic and datalog related logic have both been proposed a underlying formalism for the semantic web although these two different formalism group have some commonality and look similar in the context of expressively impoverished language like rdf their difference become apparent at more expressive language level after considering some of these difference we argue that although some of the characteristic of datalog have their utility the open environment of the semantic web is better served by standard logic 
the cctld country code top level domain in a url doe not necessarily point to the geographic location of the server concerned the author have surveyed sample server belonging to cctlds in africa with regard to the number of hop required to reach the target site from japan the response time and the nic registration information of each domain the survey ha revealed the geographical distribution of server site a well a their connection environment it ha been found that the percentage of offshore out of home country server is a high a and more than half of these are located in europe offshore server not only provide little benefit to the people of the country to which each cctld rightly belongs but their existence also heightens the risk of a country being unable to control them with it own policy and regulation offshore server constitute a significant aspect of the digital divide problem 
name ambiguity is a special case of identity uncertainty where one person can be referenced by multiple name variation in different situation or evenshare the same name with other people in this paper we present an efficient framework by using two novel topic based model extended from probabilistic latent semantic analysis plsa and latent dirichlet allocation lda our model explicitly introduce a new variable for person and learn the distribution of topic with regard to person and word experiment indicate that our approach consistently outperforms other unsupervised method including spectral and dbscan clustering scalability is addressed by disambiguating author in over paper from the entire citeseer dataset 
this paper describes the ebag infrastructure which is a generic infrastructure inspired from work with school child who could benefit from a electronic schoolbag for collaborative handling of their digital material the ebag infrastructure is utilizing the context aware hycon framework and collaborative web service based on webdav a ubiquitous login and logout mechanism ha been built based on bluetooth sensor network the ebag infrastructure ha been tried out in field test with school kid in this paper we discus experience and design issue for ubiquitous web integration in interactive school environment with multiple interactive whiteboards and workstation this includes proposal for specialized and adaptive xlink structure for organizing school material a well a issue in login logout based on proximity of different display surface 
in this paper we present a query driven indexing retrieval strategy for efficient full text retrieval from large document collection distributed within a structured p p network our indexing strategy is based on two important property the generated distributed index store posting list for carefully chosen indexing term combination and the posting list containing too many document reference are truncated to a bounded number of their top ranked element these two property guarantee acceptable storage and bandwidth requirement essentially because the number of indexing term combination remains scalable and the transmitted posting list never exceed a constant size however a the number of generated term combination can still become quite large we also use term statistic extracted from available query log to index only such combination that are frequently present in user query thus by avoiding the generation of superfluous indexing term combination we achieve an additional substantial reduction in bandwidth and storage consumption a a result the generated distributed index corresponds to a constantly evolving query driven indexing structure that efficiently follows current information need of the user more precisely our theoretical analysis and experimental result indicate that at the price of a marginal loss in retrieval quality for rare query the generated index size and network traffic remain manageable even for web size document collection furthermore our experiment show that at the same time the achieved retrieval quality is fully comparable to the one obtained with a state of the art centralized query engine 
citeseer is a scientific literature digital library and search engine which automatically crawl and index scientific document in the field of computer and information science after serving a a public search engine for nearly ten year citeseer is starting to have scaling problem for handling of more document adding new feature and more user it monolithic architecture design prevents it from effectively making use of new web technology and providing new service after analyzing the current system problem we propose a new architecture and data model citeseerx citeseerx that will overcome the existing problem a well a provide scalability and better performance plus new service and system feature 
we examine the validity and power of the t test wilcoxon test and sign test in determining whether or not the difference in performance between two ir system is significant empirical test conducted on subset of the trec robust retrieval collection indicate that the p value computed by these test for the difference in mean average precision map between two system are very accurate forum wide range of sample size and significance estimate similarly these test have good power with the t test proving superior overall the t test is also valid for comparing geometric mean average precision gmap exhibiting slightly superior accuracy and slightly inferior power than for mapcomparison 
master data refers to core business entity a company us repeatedly across many business process and system such a list or hierarchy of customer supplier account product or organizational unit product information is the most important kind of master data and product information management pim is becoming critical for modern enterprise because it provides a rich business context for various application existing pim system are le flexible and scalable for on demand business a well a too weak to completely capture and use the semantics of master data this paper explores how to use semantic web technology to enhance a collaborative pim system by simplifying modeling and representation while preserving enough dynamic flexibility furthermore we build a semantic pim system using one of the state of art ontology repository and summarize the challenge we encountered based on our experimental result especially on performance and scalability we believe that our study and experience are valuable for both semantic web community and master data management community 
we describe a preliminary analysis of query created by user for topic from the trec robust track our goal wa to explore the potential benefit of using query created by multiple user on retrieval performance for difficult topic we first examine the overlap in user query and the overlap in result with respect to different query for the same topic we then explore the potential benefit of combining user query in various way our result provide some evidence that having access to multiple user query can improve retrieval for individual searcher and for difficult topic 
we use a combination of text analysis and external knowledge source to estimate the commercial taste of blogger from their text our method are evaluated using product wishlists found in the blog initial result are promising showing that valuable insight can be mined from blog not just at the aggregate but also at the individual blog level 
web spam can significantly deteriorate the quality of search engine result thus there is a large incentive for commercial search engine to detect spam page efficiently and accurately in this paper we present a spam detection system that us the topology of the web graph by exploiting the link dependency among the web page and the content of the page themselves we find that linked host tend to belong to the same class either both are spam or both are non spam we demonstrate three method of incorporating the web graph topology into the prediction obtained by our base classifier i clustering the host graph and assigning the label of all host in the cluster by majority vote ii propagating the predicted label to neighboring host and iii using the predicted label of neighboring host a new feature and retraining the classifier the result is an accurate system for detecting web spam that can be applied in practice to large scale web data 
a xquery is gathering momentum a the standard query language for xml there is a growing interest in using it a an integral part of the xml application development infrastructure in that context one question which is often raised is how well xquery interoperates with other xml language and notably with xslt xquery and xslt share a lot in common they share xpath a a common sub language and have the same expressiveness however they are based on fairly different programming paradigm while xslt ha adopted a highly declarative template based approach xquery relies on a simpler and more operational functional approach in this paper we present an approach to compile xslt into xquery and a working implementation of that approach the compilation rule explain how xslt s template based approach can be implemented using the functional approach of xquery and underpins the tight connection between the two language the resulting compiler can be used to migrate a xslt code base to xquery or to enable the use of xquery runtimes e g a will soon be provided by most relational database management system for xslt user we also identify a number of area where compatibility between the two language could be improved finally we show experiment on actual xslt stylesheets demonstrating the applicability of the approach in practice 
available methodology for developing sematic web application do not fully exploit the whole potential deriving from interaction with ontological data source here we introduce an extension of the webml modeling framework to fulfill most of the design requirement emerging for the new area of semantic web we generalize the development process to support semantic web application and we introduce a set of new primitive for ontology importing and querying 
social networking service are a fast growing business in the internet however it is unknown if online relationship and their growth pattern are the same a in real life social network in this paper we compare the structure of three online social networking service cyworld myspace and orkut each with more than million user respectively we have access to complete data of cyworld s ilchon friend relationship and analyze it degree distribution clustering property degree correlation and evolution over time we also use cyworld data to evaluate the validity of snowball sampling method which we use to crawl and obtain partial network topology of myspace and orkut cyworld the oldest of the three demonstrates a changing scaling behavior over time in degree distribution the latest cyworld data s degree distribution exhibit a multi scaling behavior while those of myspace and orkut have simple scaling behavior with different exponent very interestingly each of the two e ponents corresponds to the different segment in cyworld s degree distribution certain online social networking service encourage online activity that cannot be easily copied in real life we show that they deviate from close knit online social network which show a similar degree correlation pattern to real life social network 
the early web wa hailed for giving individual the same publishing power a large content provider but over time large content provider learned to exploit the structure in their data leveraging database and server side technology to provide rich browsing and visualization individual author fall behind once more neither old fashioned static page nor domain specific publishing framework supporting limited customization can match custom database backed web application in this paper we propose exhibit a lightweight framework for publishing structured data on standard web server that requires no installation database administration or programming exhibit let author with relatively limited skill those same enthusiast who could write html page for the early web publish richly interactive page that exploit the structure of their data for better browsing and visualization such structured publishing in turn make that data more useful to all of it consumer individual reader get more powerful interface mashup creator can more easily repurpose the data and semantic web enthusiast can feed the data to the nascent semantic web 
most rdf query language allow for graph structure search through a conjunction of triple which is typically processed using join operation a key factor in optimizing join is determining the join order which depends on the expected cardinality of intermediate result this work proposes a pattern based summarization framework for estimating the cardinality of rdf graph pattern we present experiment on real world and synthetic datasets which confirm the feasibility of our approach 
in this paper we use a blog corpus to demonstrate that we can often identify the author of an anonymous text even where there are many thousand of candidate author our approach combine standard information retrieval method with a text categorization meta learning scheme that determines when to even venture a guess 
distributed ir system query a large number of ir server merge the retrieved result and display them to user since different server handle collection of different size have different processing and bandwidth capacity there can be considerable heterogeneity in their response time the broker in the distributed ir system thus ha to make decision regarding terminating search based on perceived value of waiting retrieving more document and the cost imposed on user by waiting for more response in this paper we apply utility theory to formulate the broker s decision problem the problem is a stochastic nonlinear program we use monte carlo simulation to demonstrate how the optimal wait time may be determined in the context of a comparison shopping engine that query multiple store website for price and product information we use data gathered from store for a set of book our research demonstrates how a broker can leverage information about past retrieval regarding distribution of server response time and relevance score to optimize it performance our main contribution is the formulation of the decision model for optimal wait time and proposal of a solution method our result suggest that the optimal wait time is highly sensitive to the manner in which user value from a set of retrieved result differs from the sum of user value from each result evaluated independently we also find that the optimal wait time increase with the size of the distributed collection but only if user utility from a set of result is nearly equal to the sum of utility from each result 
this talk present ntt s approach for realizing a human centered network last november we announced the ntt group s medium term management strategy which consists of three management objective building the ubiquitous broadband market and helping achieve the e japan strategy and the u japan initiative building a safe secure and convenient communication network environment and broadband access infrastructure while achieving a seamless migration from the legacy telephone network to the next generation network and striving to increase corporate value and achieve sustainable growth since the management strategy take account of japan s future social issue such a declining birthrate and aging population the need to reduce the environmental load etc we believe that the r d activity directed towards accomplishing these objective consequently lead to the realization of a human centered network 
in may the new york time published the article brazil gather archive on it painter portinari the author warren hoge narrates the late candido portinari is consideredhere to be the greatest artist brazil ha ever produced yet allbut a few of his painting are out of public view they havebecome dispersed in private collection in so many place that hisbiographer compared their fate to that of brazil s th centuryrevolutionary hero tiradentes whose body wa dismembered andstrewn along a mile turnpike the inaccessibility ofportinari s work is particularly vexing to his enthusiast becausehis own dedication to producing an epic view of brazil for hiscountrymen wa such that he continued painting even after doctorswarned that exposure to paint wa killing him he died of leadpoisoning at the age of now in a pioneering effort for latinamerica a team of expert in rio is busy assembling the far flungpieces of portinari s obra into an exhaustive computerized archive we are trying to rescue what is authentically ours saidjo o candido portinari the painter s year old son whois the coordinator of the group of researcher who make theirheadquarters on the leafy campus of rio s pontifical catholicuniversity a telecommunication engineer with a ph d from themassachusetts institute of technology and a former chairman of theuniversity s mathematics department mr portinari ha broughtexacting technical standard to the task now four year into theproject the member team ha compiled photographic technical this presentation aim at describing the year effortundertaken by the portinari project to locate document and recordall print drawing and painting created by the brazilian artistcandido portinari a well a all document it is important to highlite the role of science and technologyin this endeavour it wa fascinating to watch over the year howmuch a the portinari project evolved the advance of science andtechnology and especially those related to ir were able toaddress important challenge encountered in it development inthis presentation a few example of this process shall bedescribed such a the problem of identifying false painting thedigital preservation of color image the building of a complexmultimedia knowledge database etc we also present the social work of art education developed inmaking all this material available in loco to a wide audience inbrazil and abroad including school child especially childrenfrom poverty stricken family this social inclusion action stemsfrom the fact that portinari wa deeply concerned to devoting hislife a an artist and also a a political person to social andhuman value in his speech art in the united nation at the palais de nation geneve switzerland von lauestein massaraniobserved that portinari bequeathed to his native brazil painting of greatpoetical intensity he grew up on the vast coffee plantation ofbrodosqui in the state of sao paulo and it is this social settingwhich provided the inspiration for his work all it human cultural and religious aspect captured by portinari s brush reveal him a a chronicler of the concern of twentieth centurybrazil a rich plastic quality and variety of expression arequalitites which give singular appeal to this deeply inspired work his painting are the successful achievement of his objectivethroughout his life to arouse a feeling of the dignity of man offraternity and community spirit from these concern faithfully reflected in portinari sartistic legacy we have been able to build an exceedingly powerfultool for developing a social acton that ha already involved morethan child for all over brazil a shall be demonstratedin our talk and finally we present one of the most important result of theproject candido portinari catalogue raisonn published in september in volume containing work with all their image and their technical bibliographic andhistorical data an example of the complexity of this undertakingis the work involved in dating portinari s oeuvre half of theworks were undated and for each of these work it wasnecessary to cross it with document to search if in aletter in a periodical clipping in a historicalphotograph in an oral history recording interviewed hour total recording etc one could find information thatwould eventually lead to the date the work wa created theportinari project web site http www portinari org br presentsall painting drawing and print and all entering aword contained in the work s description the catalogue raisonn also comprises a detailed chrono biography of theartist s life and time compiled from the document thatform the portinari project archive to complete this year task the team visited almost all brazilian state and more than country in the three america europe and the middle orient 
opinion holder extraction research is important for discriminating between opinion that are viewed from different perspective in this paper we describe our experience of participation in the ntcir opinion analysis pilot task by focusing on opinion holder extraction result in japanese and english our approach to opinion holder extraction wa based on the discrimination between author and authority viewpoint in opinionated sentence and the evaluation result were fair with respect to the japanese document 
in contrast with the current web search method that essentially do document level ranking and retrieval we are exploring a new paradigm to enable web search at the object level we collect web information for object relevant for a specific application domain and rank these object in term of their relevance and popularity to answer user query traditional pagerank model is no longer valid for object popularity calculation because of the existence of heterogeneous relationship between object this paper introduces poprank a domain independent object level link analysis model to rank the object within a specific domain specifically we assign a popularity propagation factor to each type of object relationship study how different popularity propagation factor for these heterogeneous relationship could affect the popularity ranking and propose efficient approach to automatically decide these factor our experiment are done using million c paper and the experimental result show that poprank can achieve significantly better ranking result than naively applying pagerank on the object graph 
sponsored search auction hosted by major search engine allow advertiser to select relevant keywords allocate budget to those term and bid on different advertising position for each keyword in a real time auction against other advertiser this dynamic and competitive process creates the significant problem of optimal bid management especially for large advertiser who need to manage thousand of keywords and spend ten of million on such advertising algorithm for efficient competitive bid optimization are therefore highly desirable we approach this problem by casting it a an online multiple choice knapsack problem and design algorithm for the online knapsack problem achieving a provably optimal competitive ratio this allows for the automation of the bidding process while optimizing bid to best achieve the goal of the program to maximize revenue from sponsored search advertising our bidding strategy can be oblivious i e without knowledge of other bidder price and or click through rate for those position we evaluate our bidding algorithm using both synthetic data and real bidding data scraped from the overture website and also discus a sniping heuristic that strictly improves bidding performance 
memory based method for collaborative filtering predict new rating by averaging weighted rating between respectively pair of similar user or item in practice a large number of rating from similar user or similar item are not available due to the sparsity inherent to rating data consequently prediction quality can be poor this paper re formulates the memory based collaborative filtering problem in a generative probabilistic framework treating individual user item rating a predictor of missing rating the final rating is estimated by fusing prediction from three source prediction based on rating of the same item by other user prediction based on different item rating made by the same user and third rating predicted based on data from other but similar user rating other but similar item existing user based and item based approach correspond to the two simple case of our framework the complete model is however more robust to data sparsity because the different type of rating are used in concert while additional rating from similar user towards similar item are employed a a background model to smooth the prediction experiment demonstrate that the proposed method are indeed more robust against data sparsity and give better recommendation 
for many year it ha been commonly held that a user who add structural hint to a query will improve precision in an element retrieval search at inex we conducted an experiment to test this assumption we present the unexpected result that structural hint in query do not improve precision an analysis of the topic and the judgment suggests that this is because user are particularly bad at giving structural hint 
we describe the detrimental effect of browser cache history sniffing in the context of phishing attack and detail an ap proach that neutralizes the threat by mean of url person alization we report on an implementation performing such personalization on the fly and analyze the cost of and se curity property of our proposed solution 
the biased minimax probability machine bmpm construct a classifier which deal with the imbalanced learning task in this paper we propose a second order cone programming socp based algorithm to train the model we outline the theoretical derivative of the biased classification model and address the text classification task where negative training document significantly outnumber the positive one using the proposed strategy we evaluated the learning scheme in comparison with traditional solution on three different datasets empirical result have shown that our method is more effective and robust to handle imbalanced text classification problem 
the demand for quickly delivering new application is increasingly becoming a business imperative today application development is often done in an ad hoc manner without standard framework or library thus resulting in poor reuse of software asset web service have received much interest in industry due to their potential in facilitating seamless business to business or enterprise application integration a web service composition tool can help automate the process from creating business process functionality to developing executable workflow to deploying them on an execution environment however we find that the main approach taken thus far to standardize and compose web service are piecemeal and insufficient the business world ha adopted a distributed programming approach in which web service instance are described using wsdl composed into flow with a language like bpel and invoked with the soap protocol academia ha propounded the ai approach of formally representing web service capability in ontology and reasoning about their composition using goal oriented inferencing technique from planning we present the first integrated work in composing web service end to end from specification to deployment by synergistically combining the strength of the above approach we describe a prototype service creation environment along with a use case scenario and demonstrate how it can significantly speed up the time to market for new service 
we present a new approach for propagating spam score in web graph in order to combat link spam the resulting spam rating is then used for propagating popularity score like pagerank both propagation work even in presence of censure link that represent distrust initial testing using a c prototype on small example show more reasonable result than other published approach 
we study the effect of user supplied relevance feedback in improving web search result rather than using query refinement or document similarity measure to rerank result we show that the web graph distance between two document is a robust measure of their relative relevancy we demonstrate how the use of this metric can improve the ranking of result url even when the user only rate one document in the dataset our research suggests that such interactive system can significantly improve search result 
federated search link multiple search engine into a single virtual search system most prior research of federated search focused on selecting search engine that have the most relevant content but ignored the retrieval effectiveness of individual search engine this omission can cause serious problem when federating search engine of different quality this paper proposes a federated search technique that us utility maximization to model the retrieval effectiveness of each search engine in a federated search environment the new algorithm rank the available resource by explicitly estimating the amount of relevant material that each resource can return instead of the amount of relevant material that each resource contains an extensive set of experiment demonstrates the effectiveness of the new algorithm 
we present the result of the first large scale turkish information retrieval experiment performed on a trec like test collection the test bed which ha been created for this study contains million word document ad hoc query and ha a size of about mb all document come from the turkish newspaper milliyet we implement and apply simple to sophisticated stemmer and various query document matching function and show that truncating word at a prefix length of creates an effective retrieval environment in turkish however a lemmatizer based stemmer provides significantly better effectiveness over a variety of matching function 
we present the result of a research project focus on improving the usability of web search tool for blind user who interact via screen reader and voice synthesizer in the first stage of our study we proposed eight specific guideline for simplifying this interaction with search engine next we evaluated these criterion by applying them to google uis re implementing the simple search and the result page finally we prepared the environment for a remote test with totally blind user the result highlight how google interface could be improved in order to simplify interaction for the blind 
we present method for finding expert and their contact detail using e mail message we locate message on a topic and then find the associated expert our approach is unsupervised both the list of potential expert and their personal detail are obtained automatically from e mail message header and signature respectively evaluation is done using the e mail list in the w c corpus 
in this paper we describe preliminary work that examines whether statistical property of the structure of website can be an informative measure of their quality we aim to develop a new method for evaluating e government e government website are evaluated regularly by consulting company international organization and academic researcher using a variety of subjective measure we aim to improve on these evaluation using a range of technique from webmetric and social network analysis to pilot our methodology we examine the structure of government audit office site in canada the usa the uk new zealand and the czech republic we report experimental value for a variety of characteristic including the connected component the average distance between node the distribution of path length and the indegree and outdegree these measure are expected to correlate with i the navigability of a website and ii with it nodality which is a combination of hubness and authority comparison of website based on these characteristic raised a number of issue related to the proportion of non hyperlinked content e g pdf and doc file within a site and both the very significant difference in the size of the website and their respective national population method to account for these issue are proposed and discussed there appears to be some correlation between the value measured and the league table reported in the literature however this multi dimensional analysis provides a richer source of evaluative technique than previous work our analysis indicates that the u and canada provide better navigability much better than the uk however the uk site is shown to have the strongest nodality on the web 
we investigate the robustness of three widely used ir relevance measure for large data collection with incomplete judgment the relevance measure we consider are the bpref measure introduced by buckley and voorhees the inferred average precision infap introduced by aslam and yilmaz and the normalized discounted cumulative gain ndcg measure introduced by j rvelin and kek l inen our main result show that ndcg consistently performs better than both bpref and infap the experiment are performed on standard trec datasets under different level of incompleteness of judgment and using two different evaluation method namely the kendall correlation measure order between system ranking and pairwise statistical significance testing the latter may be of independent interest 
this paper employ conceptnet which cover a rich set of commonsense concept to retrieve image with text description by focusing on spatial relationship evaluation on test data of the imageclef show that integrating commonsense knowledge in information retrieval is feasible 
in this short note we demonstrate the applicability of hyperlink downweighting by mean of language model disagreement the method filter out hyperlink with no relevance to the target page without the need of white and blacklist or human interaction we fight various form of nepotism such a common maintainer ad link exchange or misused affiliate program our method is tested on a m page crawl of the de domain with a manually classified page random sample 
the web search engine maintain large scale inverted index which are queried thousand of time per second by user eager for information in order to cope with the vast amount of query load search engine prune their index to keep document that are likely to be returned a top result and use this pruned index to compute the first batch of result while this approach can improve performance by reducing the size of the index if we compute the top result only from the pruned index we may notice a significant degradation in the result quality if a document should be in the top result but wa not included in the pruned index it will be placed behind the result computed from the pruned index given the fierce competition in the online search market this phenomenon is clearly undesirable in this paper we study how we can avoid any degradation of result quality due to the pruning based performance optimization while still realizing most of it benefit our contribution is a number of modification in the pruning technique for creating the pruned index and a new result computation algorithm that guarantee that the top matching page are always placed at the top search result even though we are computing the first batch from the pruned index most of the time we also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithm on a collection of million web page 
latent semantic indexing lsi is a well known unsupervised approach for dimensionality reduction in information retrieval however if the output information i e category label is available it is often beneficial to derive the indexing not only based on the input but also on the target value in the training data set this is of particular importance in application with multiple label in which each document can belong to several category simultaneously in this paper we introduce the multi label informed latent semantic indexing mlsi algorithm which preserve the information of input and meanwhile capture the correlation between the multiple output the recovered latent semantics thus incorporate the human annotated category information and can be used to greatly improve the prediction accuracy empirical study based on two data set reuters and rcv demonstrates very encouraging result 
we consider the problem of dust different url with similar text such duplicate url are prevalent in web site a web server software often us alias and redirections translates url to some canonical form and dynamically generates the same page from various different url request we present a novel algorithm dustbuster for uncovering dust that is for discovering rule for transforming a given url to others that are likely to have similar content dustbuster is able to detect dust effectively from previous crawl log or web server log without examining page content verifying these rule via sampling requires fetching few actual web page search engine can benefit from this information to increase the effectiveness of crawling reduce indexing overhead a well a improve the quality of popularity statistic such a pagerank 
we consider the problem of dust different url with similar text such duplicate url are prevalent in web site a web server software often us alias and redirections and dynamically generates the same page from various different url request we present a novel algorithm dustbuster for uncovering dust that is for discovering rule that transform a given url to others that are likely to have similar content dustbuster mine dust effectively from previous crawl log or web server log without examining page content verifying these rule via sampling requires fetching few actual web page search engine can benefit from information about dust to increase the effectiveness of crawling reduce indexing overhead and improve the quality of popularity statistic such a pagerank category and subject descriptor h information search and retrieval 
we describe a method for predicting query difficulty in a precision oriented web search task our approach us visual feature from retrieved surrogate document representation title snippet etc to predict retrieval effectiveness for a query by training a supervised machine learning algorithm with manually evaluated query visual clue indicative of relevance are discovered we show that this approach ha a moderate correlation of with precision at score from manual relevance judgment of the top ten document retrieved by ten web search engine over query our finding indicate that difficulty predictor which have been successful in recall oriented ad hoc search such a clarity metric are not nearly a correlated with engine performance in precision oriented task such a this yielding a maximum correlation of additionally relying only on visual clue avoids the need for collection statistic that are required by these prior approach this enables our approach to be employed in environment where these statistic are unavailable or costly to retrieve such a metasearch 
large scale text categorization is an important research topic for web data mining one of the challenge in large scale text categorization is how to reduce the human effort in labeling text document for building reliable classification model in the past there have been many study on applying active learning method to automatic text categorization which try to select the most informative document for labeling manually most of these study focused on selecting a single unlabeled document in each iteration a a result the text categorization model ha to be retrained after each labeled document is solicited in this paper we present a novel active learning algorithm that selects a batch of text document for labeling manually in each iteration the key of the batch mode active learning is how to reduce the redundancy among the selected example such that each example provides unique information for model updating to this end we use the fisher information matrix a the measurement of model uncertainty and choose the set of document to effectively maximize the fisher information of a classification model extensive experiment with three different datasets have shown that our algorithm is more effective than the state of the art active learning technique for text categorization and can be a promising tool toward large scale text categorization for world wide web document 
recently interest is growing in non topical text classification task such a genre classification sentiment analysis and authorship profiling we study to what extent ocr error affect stylistic text classification from scanned document we find that even a relatively high level of error in the ocred document doe not substantially affect stylistic classification accuracy 
in this paper we present the design and implementation of our expertise oriented search eos system eos is a researcher social network system it ha gathered information about a halfmillion computer science researcher from the web and constructed a social network among the researcher through their co authorship the relationship in the social network information is used in both ranking expert on a given topic and searching for association between researcher 
in this poster we discus a graphical notation for representing query for semistructured data we try to strike a balance between expressiveness of the query language and simplicity and understandability of the graphical notation we present the primitive of the notation by mean of example 
consensus clustering is the task of deriving a single labeling by applying a consensus function on a cluster ensemble this work introduces bordaconsensus a new consensus function for soft cluster ensemble based on the borda voting scheme in contrast to classic hard consensus function that operate on labelings our proposal considers cluster membership information thus being able to tackle multiclass clustering problem initial small scale experiment reveal that compared to state of the art consensus function bordaconsensus constitutes a good performance v complexity trade off 
we present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph based framework the main idea is to perform reranking based on centrality within bipartite graph of document on one side and cluster on the other side on the premise that these are mutually reinforcing entity link between entity are created via consideration of language model induced from them we find that our cluster document graph give rise to much better retrieval performance than previously proposed document only graph do for example authority based reranking of document via a hit style cluster based approach outperforms a previously proposed pagerank inspired algorithm applied to solely document graph moreover we also show that computing authority score for cluster constitutes an effective method for identifying cluster containing a large percentage of relevant document 
wikis are popular collaborative hypertext authoring environment but they neither support structured access nor information reuse adding semantic annotation help to address these limitation we present an architecture for semantic wikis and discus design decision including structured access view and annotation language we present our prototype semperwiki that implement this architecture 
iskodor integrates personal collection peer search and centralized search service user modeling in iskodor fill three role discovery of site with suitable information store context based query interpretation and automatic profile based filtering of new information explanation and control are achieved through graphical depiction of source explicit feedback regard ingutility and explicit control over peer association behavior and information sharing 
this paper explores probabilistic lexico syntactic pattern matching also known a soft pattern matching while previous method in soft pattern matching are ad hoc in computing the degree of match we propose two formal matching model one based on bigram and the other on the profile hidden markov model phmm both model provide a theoretically sound method to model pattern matching a a probabilistic process that generates token sequence we demonstrate the effectiveness of these model on definition sentence retrieval for definitional question answering we show that both model significantly outperform state of the art manually constructed pattern a critical difference between the two model is that the phmm technique handle language variation more effectively but requires more training data to converge we believe that both model can be extended to other area where lexico syntactic pattern matching can be applied 
we derive a tractable and exact computation for the expectation of f measure we also demonstrate the non convexity of this expectation and investigate error of approximating the expectation under different setting 
in information retrieval relevance ranking of the result is one of the most important single task there are there are many diffierent ranking algorithm based on the content of the document or on some external property e g link structure of html document we present a temporally adaptive content based relevance ranking algorithm that explicitly take into account the temporal behavior of the underlying statistical property of the document in the form of a statistical topic model more we state that our algorithm can be used on top of any ranking algorithm 
this paper describes our effort to investigate factor in user s browsing behavior to automatically evaluate web page that the user show interest in to evaluate web page automatically we developed a client side logging analyzing tool the ginis framework this work focus primarily on client side user behavior using a customized web browser and ajax technology first ginis unobtrusively gather log of user behavior through the user s natural interaction with the web browser then it analysis the log and extract effective rule to evaluate web page using c machine learning system eventually ginis becomes able to automatically evaluate web page using these learned rule 
this paper report a safe regression test selection rts approach that is designed for verifying web service in an end to end manner the safe rts technique ha been integrated into a systematic method that monitor distributed code modification and automates the rts and rt process 
information retrieval is in general an iterative search process in which the user often ha several interaction with a retrieval system for an information need the retrieval system can actively probe a user with question to clarify the information need instead of just passively responding to user query a basic question is thus how a retrieval system should propose question to the user so that it can obtain maximum benefit from the feedback on these question in this paper we study how a retrieval system can perform active feedback i e how to choose document for relevance feedback so that the system can learn most from the feedback information we present a general framework for such an active feedback problem and derive several practical algorithm a special case empirical evaluation of these algorithm show that the performance of traditional relevance feedback presenting the top k document is consistently worse than that of presenting document with more diversity with a diversity based selection algorithm we obtain fewer relevant document however these fewer document have more learning benefit 
a large and growing number of web page display contextual advertising based on keywords automatically extracted from the text of the page and this is a substantial source of revenue supporting the web today despite the importance of this area little formal published research exists we describe a system that learns how to extract keywords from web page for advertisement targeting the system us a number of feature such a term frequency of each potential keyword inverse document frequency presence in meta data and how often the term occurs in search query log the system is trained with a set of example page that have been hand labeled with relevant keywords based on this training it can then extract new keywords from previously unseen page accuracy is substantially better than several baseline system 
a number of existing information retrieval system propose the notion of query context to combine the knowledge of query and user into retrieval to reveal the most exact description of user s information need in this paper we interpret query context a a document consisting of sentence related to the current query this kind of query context is used to re estimate the relevance probability of top ranked document and then re rank top ranked document the experiment show that the proposed context based approach for information retrieval can greatly improved relevance of search result 
tag have recently become popular a a mean of annotating and organizing web page and blog entry advocate of tagging argue that the use of tag produce a folksonomy a system in which the meaning of a tag is determined by it use among the community a a whole we analyze the effectiveness of tag for classifying blog entry by gathering the top tag from technorati and measuring the similarity of all article that share a tag we find that tag are useful for grouping article into broad category but le effective in indicating the particular content of an article we then show that automatically extracting word deemed to be highly relevant can produce a more focused categorization of article we also show that clustering algorithm can be used to reconstruct a topical hierarchy among tag and suggest that these approach may be used to address some of the weakness in current tagging system 
in this paper we define and study a novel search problem comparative web search cws the task of cws is to seek relevant and comparative information from the web to help user conduct comparison among a set of topic a system called cws is developed to effectively facilitate web user comparison need given a set of query which represent the topic that a user want to compare the system is characterized by automatic retrieval and ranking of web page by incorporating both their relevance to the query and the comparative content they contain automatic clustering of the comparative content into semantically meaningful theme extraction of representative keyphrases to summarize the commonness and difference of the comparative content in each theme we developed a novel interface which support two type of view mode a pair view which display the result in the page level and a cluster view which organizes the comparative page into the theme and display the extracted phrase to facilitate user comparison experiment result show the cws system is effective and efficient 
in this paper we describe a solution for incorporating background knowledge into the ontogen system for semi automatic ontology construction this make it easier for different user to construct different and more personalized ontology for the same domain to achieve this we introduce a word weighting schema to be used in the document representation the weighting schema is learned based on the background knowledge provided by user it is than used by ontogen s machine learning and text mining algorithm 
this paper show how different measure of similarity derived from the citation information and the structural content e g title abstract of the collection can be fused to improve classification effectiveness to discover the best fusion framework we apply genetic programming gp technique our experiment with the acm computing classification scheme using document from the acm digital library indicate that gp can discover similarity function superior to those based solely on a single type of evidence effectiveness of the similarity function discovered through simple majority voting is better than that of content based a well a combination based support vector machine classifier experiment also were conducted to compare the performance between gp technique and other fusion technique such a genetic algorithm ga and linear fusion empirical result show that gp wa able to discover better similarity function than other fusion technique 
internet user now rely on a whole arsenal of tool to protect their security and privacy expert recommend that computer user install personal firewall anti virus software spyware blocker spam filter cookie manager and a variety of other tool to keep themselves safe user are told to pick hard to guess password use a different password at every web site and not to write any of their password down they are told to read privacy policy before providing personal information to web site look for lock icon before typing in a credit card number refrain from opening email attachment from people they don t know and even to think twice about opening email attachment from people they do know with so many do s and don t it is not surprising that much of this advice is ignored in this talk i will highlight usability problem that make it difficult for people to protect their privacy and security on the web and i will discus a number of approach to addressing these problem 
most previous web page summarization method treat a web page a plain text however such method fail to uncover the full knowledge associated with a web page needed in building a high quality summary because many of these method do not consider the hidden relationship in the web uncovering the hidden knowledge is important in building good web page summarizers in this paper we extract the extra knowledge from the clickthrough data of a web search engine to improve web page summarization wefirst analyze the feasibility in utilizing the clickthrough data to enhance web page summarization and then propose two adapted summarization method that take advantage of the relationship discovered from the clickthrough data for those page that are not covered by the clickthrough data we design a thematic lexicon approach to generate implicit knowledge for them our method are evaluated on a dataset consisting of manually annotated page a well a a large dataset that is crawled from the open directory project website the experimental result indicate that significant improvement can be achieved through our proposed summarizer a compared to the summarizers that do not use the clickthrough data 
combining data and code from third party source ha enabled a new wave of web mashups that add creativity and functionality to web application however browser are poorly designed to pas data between domain often forcing web developer to abandon security in the name of functionality to address this deficiency we developed subspace a cross domain communication mechanism that allows efficient communication across domain without sacrificing security our prototype requires only a small javascript library and work across all major browser we believe subspace can serve a a new secure communication primitive for web mashups 
spam is a key problem in electronic communication including large scale email system and the growing number of blog content based filtering is one reliable method of combating this threat in it various form but some academic researcher and industrial practitioner disagree on how best to filter spam the former have advocated the use of support vector machine svms for content based filtering a this machine learning methodology give state of the art performance for text classification however similar performance gain have yet to be demonstrated for online spam filtering additionally practitioner cite the high cost of svms a reason to prefer faster if le statistically robust bayesian method in this paper we offer a resolution to this controversy first we show that online svms indeed give state of the art classification performance on online spam filtering on large benchmark data set second we show that nearly equivalent performance may be achieved by a relaxed online svm rosvm at greatly reduced computational cost our result are experimentally verified on email spam blog spam and splog detection task 
community analysis algorithm proposed by clauset newman and moore cnm algorithm find community structure in social network unfortunately cnm algorithm doe not scale well and it use is practically limited to network whose size are up to node we show that this inefficiency is caused from merging community in unbalanced manner and that a simple heuristic that attempt to merge community structure in a balanced manner can dramatically improve community structure analysis the proposed technique are tested using data set obtained from existing social networking service that host million user we have tested three three variation of the heuristic the fastest method process a sn friendship network with million user in minute time faster than cnm and another friendship network with million user in minute respectively another one process a network with node in minute time faster than cnm find community structure that ha improved modularity and scale to a network with million 
we present an approach in which the semantics of an xml language is defined by mean of a transformation from an xml document model an xml schema to an application specific model the application specific model implement the intended behavior of document written in the language a transformation is specified in a model transformation language used in the model driven architecture mda approach for software development our approach provides a better separation of three concern found in xml application syntax syntax processing logic and intended meaning of the syntax it free the developer of low level syntactical detail and improves the adaptability and reusability of xml application declarative transformation rule and the explicit application model provide a finer control over the application part affected by adaptation transformation rule and the application model for an xml language may be composed with the corresponding rule and application model defined for other xml language in that way we achieve reuse and composition of xml application 
existing pseudo relevance feedback method typically perform averaging over the top retrieved document but ignore an important statistical dimension the risk or variance associated with either the individual document model or their combination treating the baseline feedback method a a black box and the output feedback model a a random variable we estimate a posterior distribution for the feed back model by resampling a given query s top retrieved document using the posterior mean or mode a the enhanced feedback model we then perform model combination over several enhanced model each based on a slightly modified query sampled from the original query we find that resampling document help increase individual feedback model precision by removing noise term while sampling from the query improves robustness worst case performance by emphasizing term related to multiple query aspect the result is a meta feedback algorithm that is both more robust and more precise than the original strong baseline method 
this poster overview the main characteristic of a flexible retrieval system of shape present in binary image and discus some evaluation result the system applies multiple indexing criterion of the shape synthesizing distinct characteristic such a global feature of the object contour fourier coefficient boundary irregularity multifractal spectrum presence of concavity and convexity on the boundary contour scale space distribution the system is flexible since it allows customizing the retrieval function to fit an application need the query is a binary image containing the desired shape and a set of parameter specifying the distinct importance of the shape characteristic that must be taken into account to evaluate the relevance of the retrieved shape the retrieval function is then defined a a flexible multicriteria fusion function producing ranked result the evaluation experiment showed that this system can be suited to different retrieval purpose and that generally the combination of the distinct shape indexing criterion increase both recall and precision with respect to the application of any single indexing criterion alone 
finding significant contextual feature is a challenging task in the development of interactive information retrieval ir system this paper investigated a simple method to facilitate such a task by looking at aggregated relevance judgement of retrieved document our study suggested that the agreement on relevance judgement can indicate the effectiveness of retrieved document a the source of significant feature the effect of highly agreed document give u practical implication for the design of adaptive search model in interactive ir system 
many modern natural language processing application utilize search engine to locate large number of web document or to compute statistic over the web corpus yet web search engine are designed and optimized for simple human query they are not well suited to support such application a a result these application are forced to issue million of successive query resulting in unnecessary search engine load and in slow application with limited scalability in response this paper introduces the binding engine be which support query containing typed variable and string processing function for example in response to the query powerful noun be will return all the noun in it index that immediately follow the word powerful sorted by frequency in response to the query city such a propernoun head nounphrase be will return a list of proper noun likely to be city name be s novel neighborhood index enables it to do so with o k random disk seek and o k serial disk read where k is the number of non variable term in it query a a result be can yield several order of magnitude speedup for large scale language processing application the main cost is a modest increase in space to store the index we report on experiment validating these claim and analyze how be s space time tradeoff scale with the size of it index and the number of variable type finally we describe how a be based application extract thousand of fact from the web at interactive speed in response to simple user query 
this paper examines the reliability of implicit feedback generated from clickthrough data in www search analyzing the user decision process using eyetracking and comparing implicit feedback against manual relevance judgment we conclude that click are informative but biased while this make the interpretation of click a absolute relevance judgment difficult we show that relative preference derived from click are reasonably accurate on average 
sequential pattern of d gap exist pervasively in inverted list of web document collection index due to the cluster property in this paper the information of d gap sequential pattern is used a a new dimension for improving inverted index compression we first detect d gap sequential pattern using a novel data structure updown tree based on the detected pattern we further substitute each pattern with it pattern id in the inverted list that contain it the resulted inverted list are then coded with an existing coding scheme experiment show that this approach can effectively improve the compression ratio of existing code 
controlled and reproducible laboratory experiment enabled by reusable test collection represent a well established methodology in modern information retrieval research in order to confidently draw conclusion about the performance of different retrieval method using test collection their reliability and trustworthiness must first be established although such study have been performed for ad hoc test collection currently available resource for evaluating question answering system have not been similarly analyzed this study evaluates the quality of answer pattern and list of relevant document currently employed in automatic question answering evaluation and concludes that they are not suitable for post hoc experimentation these resource created from run submitted by trec qa track participant do not produce fair and reliable assessment of system that did not participate in the original evaluation potential solution for addressing this evaluation gap and their shortcoming are discussed 
the demand of browsing information from general web page using a mobile phone is increasing however since the majority of web page on the internet are optimized for browsing from pc it is difficult for mobile phone user to obtain sufficient information from the web therefore a method to reconstruct pc optimized web page for mobile phone user is essential an example approach is to segment the web page based on it structure and utilize the hierarchy of the content element to regenerate a page suitable for mobile phone browsing in our previous work we have examined a robust automatic web page segmentation scheme which us the distance between content element based on the relative html tag hierarchy i e the number and depth of html tag in web page however this scheme ha a problem that the content distance based on the order of html tag doe not always correspond to the intuitional distance between content element on the actual layout of a web page in this paper we propose a hybrid segmentation method which segment web page based on both the content distance calculated by the previous scheme and a novel approach which utilizes web page layout information experiment conducted to evaluate the accuracy of web page segmentation result prove that the proposed method can segment web page more accurately than conventional method furthermore implementation and evaluation of our system on the mobile phone prove that our method can realize superior usability compared to commercial web browser 
in this paper we present the concept of federated information sharing community fisc which leverage organisational and social relationship with document content to provide community centred information sharing and communication environment prominence is given to capability that go beyond the generic retrieval of document to include the ability to retrieve people their interest and inter relationship we focus on providing social awareness in the large to help user understand the member within community and the relationship between them within the fisc framework we provide viewpoint retrieval to enable a user to construct member specific view s of the community based on their various topic interest a proof of concept we present the first fisc prototype based on the twenty five year sigir collection and example of operational result 
state of the art question answering qa system employ term density ranking to retrieve answer passage such method often retrieve incorrect passage a relationship among question term are not considered previous study attempted to address this problem by matching dependency relation between question and answer they used strict matching which fails when semantically equivalent relationship are phrased differently we propose fuzzy relation matching based on statistical model we present two method for learning relation mapping score from past qa pair one based on mutual information and the other on expectation maximization experimental result show that our method significantly outperforms state of the art density based passage retrieval method by up to in mean reciprocal rank relation matching also brings about a improvement in a system enhanced by query expansion 
automatic extraction of semantic information from text and link in web page is key to improving the quality of search result however the assessment of automatic semantic measure is limited by the coverage of user study which do not scale with the size heterogeneity and growth of the web here we propose to leverage human generated metadata namely topical directory to measure semantic relationship among massive number of pair of web page or topic the open directory project classifies million of url in a topical ontology providing a rich source from which semantic relationship between web page can be derived while semantic similarity measure based on taxonomy tree are well studied the design of well founded similarity measure for object stored in the node of arbitrary ontology graph is an open problem this paper defines an information theoretic measure of semantic similarity that exploit both the hierarchical and non hierarchical structure of an ontology an experimental study show that this measure improves significantly on the traditional taxonomy based approach this novel measure allows u to address the general question of how text and link analysis can be combined to derive measure of relevance that are in good agreement with semantic similarity surprisingly the traditional use of text similarity turn out to be ineffective for relevance ranking 
how good is an ir test collection a series of paper in recent year ha addressed the question by empirically enumerating the consistency of performance comparison using alternate subset of the collection in this paper we propose using test theory which is based on analysis of variance and is specifically designed to ass test collection using the method we not only can measure test reliability after the fact but we can estimate the test collection s reliability before it is even built or used we can also determine an optimal allocation of resource before the fact e g whether to invest in more judge or query the method which is in widespread use in the field of educational testing complement data driven approach to assessing test collection whereas the data driven method focus on test result test theory focus on test design it offer unique practical result a well a insight about the variety and implication of alternative test design 
information technology ha become infrastructure technology a most sector of critical infrastructure rest on an it backbone yet it system are not yet designed to be a safe secure and reliable a physical infrastructure improving the security worthiness of commercial software requires a significant change in the development and product delivery process across the board the security worthiness of all commercial software from all vendor demand that assurance became a critical focus for both provider and customer of it during oracle s long history of building and delivering secure software we continue to invest heavily in building security into each component of the product lifecycle this is also an organic process which is regularly being enhanced to improve overall security practice our effort have evolved from a formal development process to now additionally include secure coding standard intensive developer training innovative bug finding tool and working with leading vendor to raise the bar for all of industry a it pertains to security 
with the growth of interest on the web service people pay increasinglyattention to the choreography that is to describe collaboration ofparticipants in accomplishing a common business goal from a globalviewpoint in this paper based on a simple choreography language and arole oriented process language we study some fundamental issue relatedto choreography especially those related to implementation includingsemantics projection and natural projection dominant role in choice anditerations etc we propose the concept of dominant role and somenovel language structure related to it the study reveals some cluesabout the language the semantics the specification and theimplementation of choreography 
this work proposes a novel cautious surfer to incorporate trust into the process of calculating authority for web page we evaluate a total of sixty query over two large real world data set to demonstrate that incorporating trust can improve pagerank s performance 
this paper is based on our exploratory study of a south indian village in chamrajanagar district of karnataka the study wa to understand the rural communication environment and villager communication preference we examined people s lifestyle working condition and their communication eco system our study revealed that villager unlike urban inhabitant interacted with people outside the village only for specific rather than casual purpose another interesting aspect of rural communication wa the marginal use of the postal system and the ubiquitous use of pay phone apart from word of mouth and face to face interaction in fact personal face to face interaction wa usually preferred among village in this region over other kind of communication despite infrastructural constraint like poor transport service we also observed that communication frequency increased when status quo changed to one that required immediate attention during the analysis we identified certain social economic and cultural communication gap or problem however these problem were clear opportunity to connect the unconnected rural user by deploying new communication system and feature here we have highlighted some of our finding and possible design avenue based on these finding 
currently the vast majority of web site do not support accessibility for visually impaired user usually these user have to rely on screen reader application that sequentially read the content of a web page in audio unfortunately screen reader are not able to detect the meaning of the different page object and thus the implicit semantic knowledge conveyed in the presentation of the page is lost one approach described in literature to tackle this problem is the dante approach which allows semantic annotation of web page to provide screen reader with extra semantic knowledge to better facilitate the audio presentation of a web page until now such annotation were done manually and failed for dynamic page in this paper we combine the dante approach with a web design method wsdm to fully automate the generation of the semantic annotation for visually impaired user to do so the semantic knowledge gathered during the design process is exploited and the annotation are generated a a by product of the design process requiring no extra effort from the designer 
graphical relationship among web page have been exploited inmethods for ranking search result to date specific graphicalproperties have been used in these analysis we introduce a webprojection methodology that generalizes prior effort of graphicalrelationships of the web in several way with the approach wecreate subgraphs by projecting set of page and domain onto thelarger web graph and then use machine learning to constructpredictive model that consider graphical property a evidence wedescribe the method and then present experiment that illustrate theconstruction of predictive model of search result quality and userquery reformulation 
the problem of using topic representation for multi document summarization md ha received considerable attention recently in this paper we describe five different topic representation and introduce a novel representation of topic based on topic theme we present eight different method of generating md and evaluate each of these method on a large set of topic used in past duc workshop our evaluation result show a significant improvement in the quality of summary based on topic theme over md method that use other alternative topic representation 
web service help in achieving increased automation across organizational boundary in this paper we present an approach for annotating wsdl document with semantically rich description we also present an algorithm that considers such annotation in addition to justthetypesofinputandoutputparameters ourmatchmaking algorithm not only return match no match answer but in case of a match a set of condition under which a web service oers the desired functionality 
machine learning is the mainstay for text classification however even the most successful technique are defeated by many real world application that have a strong time varying component to advance research on this challenging but important problem we promote a natural experimental framework the daily classification task which can be applied to large time based datasets such a reuters rcv in this paper we dissect concept drift into three main subtypes we demonstrate via a novel visualization that the recurrent theme subtype is present in rcv this understanding led u to develop a new learning model that transfer induced knowledge through time to benefit future classifier learning task the method avoids two main problem with existing work in inductive transfer scalability and the risk of negative transfer in empirical test it consistently showed more than point f measure improvement for each of four reuters category tested 
traditional ranking scheme of the relevance of a web page to a user query in a search engine are le appropriate when the search term contains geographic information often geographic entity such a address city name and location name appear only once or twice in a web page and are typically not in a heading or larger font consequently an alternative ranking approach to the traditional weighted tf idf relevance ranking is need further if a web site contains a geographic entity it is often the case that it inand out neighbour do not refer to the same entity although they may refer to other geographic entity we present a local search engine that applies a novel ranking algorithm suitable for ranking web page with geographic content we describe it major component geographic ranking focused crawling geographic extractor and the related web site feature 
query expansion in the form of pseudo relevance feedback or relevance feedback is a common technique used to improve retrieval effectiveness most previous approach have ignored important issue such a the role of feature and the importance of modeling term dependency in this paper we propose a robust query expansion technique based onthe markov random field model for information retrieval the technique called latent concept expansion provides a mechanism for modeling term dependency during expansion furthermore the use of arbitrary feature within the model provides a powerful framework for going beyond simple term occurrence feature that are implicitly used by most other expansion technique we evaluate our technique against relevance model a state of the art language modeling query expansion technique our model demonstrates consistent and significant improvement in retrieval effectiveness across several trec data set we also describe how our technique can be used to generate meaningful multi term concept for task such a query suggestion reformulation 
in this paper we describe a capture recapture experiment conducted on google s and msn s cached directory the anticipated outcome of this work wa to monitor evolution rate in these web search service a well a measure their ability to index and maintain fresh and up to date result in their cached directory 
we demonstrate the alexa web mining platform a data mining and web service publication platform designed to enable analysis of alexa s massive web data store the system provides researcher and developer high speed access to our web crawl crawl metadata long term storage and data publication utility we demonstrate the system s capability and user interface 
in this poster we describe alternative inverted index structure that reduce the time required to process query produce a higher query throughput and still return high quality result to the end user we give result based upon the trec terabyte dataset showing improvement that these index give in term of effectiveness and efficiency 
xacml ha emerged a a popular access control language on the web but because of it rich expressiveness it ha proved difficult to analyze in an automated fashion in this paper we present a formalization of xacml using description logic dl which are a decidable fragment of first order logic this formalization allows u to cover a more expressive subset of xacml than propositional logic based analysis tool and in addition we provide a new analysis service policy redundancy also mapping xacml to description logic allows u to use off the shelf dl reasoner for analysis task such a policy comparison verification and querying we provide empirical evaluation of a policy analysis tool that wa implemented on top of open source dl reasoner pellet 
the use of semantic web service sw technology have been suggested to enable more dynamic b b integration of heterogeneous system and partner we present how we add semantics to rosettanet specification to enable the wsmx sw environment to automate mediation of message the benefit of applying sw technology include flexibility in accepting heterogeneity in b b integration category and subject descriptor k electronic commerce electronic data interchange general term management design 
we consider the problem of finding duplicate in data stream duplicate detection in data stream is utilized in various application including fraud detection we develop a solution based on bloom filter and discus the space and time requirement for running the proposed algorithm in both the context of sliding and landmark stream window we run a comprehensive set of experiment using both real and synthetic click stream to evaluate the performance of the proposed solution the result demonstrate that the proposed solution yield extremely low error rate 
continuous query are used to monitor change to time varying data and to provide result useful for online decision making typically a user desire to obtain the value of some aggregation function over distributed data item for example to know a the average of temperature sensed by a set of sensor b the value of index of mid cap stock in these query a client specifies a coherency requirement a part of the query in this paper we present a low cost scalable technique to answer continuous aggregation query using a content distribution network of dynamic data item in such a network of data aggregator each data aggregator serf a set of data item at specific coherency just a various fragment of a dynamic web page are served by one or more node of a content distribution network our technique involves decomposing a client query into sub query and executing sub query on judiciously chosen data aggregator with their individual sub query incoherency bound we provide a technique of getting the optimal query plan i e set of sub query and their chosen data aggregator which satisfies client query s coherency requirement with least cost measured in term of the number of refresh message sent from aggregator to the client for estimating query execution cost we build a continuous query cost model which can be used to estimate the number of message required to satisfy the client specified incoherency bound performance result using real world trace show that our cost based query planning lead to query being executed using le than one third the number of message required by existing scheme 
we present a new family of hybrid index maintenance strategy to be used in on line index construction for monotonically growing text collection these new strategy improve upon recent result for hybrid index maintenance in dynamic text retrieval system like previous technique our new method distinguishes between short and long posting list while short list are maintained using a merge strategy long list are kept separate and are updated in place this way costly relocation of long posting list are avoided we discus the shortcoming of previous hybrid method and give an experimental evaluation of the new technique showing that it index maintenance performance is superior to that of the earlier method especially when the amount of main memory available to the indexing system is small we also present a complexity analysis which prof that under a zipfian term distribution the asymptotical number of disk access performed by the best hybrid maintenance strategy is linear in the size of the text collection implying the asymptotical optimality of the proposed strategy 
an axiom of every good investor is not to buy share when the goodness of them is already into newspaper before the information wa circulating in some form for example from mouth to mouth closed circle or newsletter nowadays the news can also occur in blog that point to public or private community that discus topic that traditional medium do not carry or even hide nowadays standard communication medium are trapped in a cartesian or platonic correspondence assumption they want to tell u how thing really are how they have occurred and how they will happen disregarding a concrete world of problem where opportunity and threat live in real time for people searching and exploring the world of blog can create an acceleration of innovation and a dissolution of the previous status quo here the search unit is not a word but action worry opportunity threat etc that is people living and pursuing shared goal with others which new searching tool can help to find trend innovation and idea taking consciousness in the context described above can ir help to end with this illusion of pseudo objectivity and manipulation of passive individual 
understanding goal and preference behind a user s online activity can greatly help information provider such a search engine and e commerce web site to personalize content and thus improve user satisfaction understanding a user s intention could also provide other business advantage to information provider for example information provider can decide whether to display commercial content based on user s intent to purchase previous work on web search defines three major type of user search goal for search query navigational informational and transactional or resource in this paper we focus our attention on capturing commercial intention from search query and web page i e when a user submits the query or browse a web page whether he she is about to commit or in the middle of a commercial activity such a purchase auction selling paid service etc we call the commercial intention behind a user s online activity a oci online commercial intention we also propose the notion of commercial activity phase cap which identifies in which phase a user is in his her commercial activity research or commit we present the framework of building machine learning model to learn oci based on any web page content based on that framework we build model to detect oci from search query and web page we train machine learning model from two type of data source for a given search query content of algorithmic search result page s and content of top site returned by a search engine our experiment show that the model based on the first data source achieved better performance we also discover that frequent query are more likely to have commercial intention finally we propose our future work in learning richer commercial intention behind user online activity 
the effectiveness of information retrieval ir system is influenced by the degree of term overlap between user query and relevant document query document term mismatch whether partial or total is a fact that must be dealt with by ir system query expansion qe is one method for dealing with term mismatch ir system implementing query expansion are typically evaluated by executing each query twice with and without query expansion and then comparing the two result set while this measure an overall change in performance it doe not directly measure the effectiveness of ir system in overcoming the inherent issue of term mismatch between the query and relevant document nor doe it provide any insight into how such system would behave in the presence of query document term mismatch in this paper we propose a new approach for evaluating query expansion technique the proposed approach is attractive because it provides an estimate of system performance under varying degree of query document term mismatch it make use of readily available test collection and it doe not require any additional relevance judgment or any form of manual processing 
a most blog and traditional medium support r or atom feed the news feed technology becomes increasingly prevalent taking advantage of ubiquitous news feed we design feedex a news feed exchange system forming a distribution overlay network node in feedex not only fetch feed document from the server but also exchange them with neighbor among many benefit of collaborative feed exchange we focus on the low overhead scalable delivery mechanism that increase the availability of news feed our design of feedex is incentive compatible so that node are encouraged into cooperating rather than free riding in addition for a better design of feedex we analyze the data collected from feed for day and present relevant statistic about news feed publishing including the distribution of feed size entry lifetime and publishing rate our experimental evaluation using planetlab machine which fetch from real world feed server show that feedex is an efficient system in many respect even when a node fetch feed document a infrequently a every hour it capture more than of the total entry published and those captured entry are available within minute on average after published at the server by contrast stand alone application in the same condition show of entry coverage and hour of time lag the efficient delivery of feedex is achieved with low communication overhead a each node receives only document exchange call and document checking call per minute on average 
major research challenge in discovering web service include provisioning of service across multiple or heterogeneous registry differentiating between service that share similar functionality improving end to end quality of service qos and enabling client to customize the discovery process proliferation and interoperability of this multitude of web service have lead to the emergence of new standard on how service can be published discovered or used i e uddi wsdl soap such standard can potentially provide many of these feature and much more however there are technical challenge associated with existing standard one of these challenge is the client s ability to control the discovery process across accessible service registry for finding service of interest this work proposes a solution to this problem and introduces the web service relevancy function wsrf used for measuring the relevancy ranking of a particular web service based on qos metric and client preference we present experimental validation result and analysis of the presented idea 
this paper describes the development of a semantic web and ontology based local search system that can be used in wireless mobile communication service 
label propagation exploit the structure of the unlabeled document by propagating the label information of the training document to the unlabeled document the limitation with the existing label propagation approach is that they can only deal with a single type of object we propose a framework named relation propagation that allows for information propagated among multiple type of object empirical study with multi label text categorization showed that the proposed algorithm is more effective than several semi supervised learning algorithm in that it is capable of exploring the correlation among different category and the structure of unlabeled document simultaneously 
in this paper we consider the type of community network that are most often codified within the semantic web we propose the recognition of a new structure which fulfils the definition of community used outside the semantic web we argue that the property inherent in a community allow additional processing to be done with the described relationship existing between entity within the community network taking an existing online community a a case study we describe the ontology and application that we developed to support this community in the semantic web environment and discus what lesson can be learnt from this exercise and applied in more general setting 
this paper explores the use of social annotation to improve websearch nowadays many service e g del icio u have been developed for web user to organize and share their favorite webpage on line by using social annotation we observe that the social annotation can benefit web search in two aspect the annotation are usually good summary of corresponding webpage the count of annotation indicates the popularity of webpage two novel algorithm are proposed to incorporate the above information into page ranking socialsimrank ssr calculates the similarity between social annotation and webqueries socialpagerank spr capture the popularity of webpage preliminary experimental result show that ssr can find the latent semantic association between query and annotation while spr successfully measure the quality popularity of a webpage from the web user perspective we further evaluate the proposed method empirically with manually constructed query and auto generated query on a dataset crawledfrom delicious experiment show that both ssr and sprbenefit web search significantly 
we introduce a new model for semantic annotation and retrieval from image database the new model is based on a probabilistic formulation that pose annotation and retrieval a classification problem and produce solution that are optimal in the minimum probability of error sense it is also database centric by establishing a one to one mapping between semantic class and the group of database image that share the associated semantic label in this work we show that under the database centric probabilistic model optimal annotation and retrieval can be implemented with algorithm that are conceptually simple computationally efficient and do not require prior semantic segmentation of training image due to it simplicity the annotation and retrieval architecture is also amenable to sophisticated parameter tuning a property that is exploited to investigate the role of feature selection in the design of optimal annotation and retrieval system finally we demonstrate the benefit of simply establishing a one to one mapping between keywords and the state of the semantic classification problem over the more complex and currently popular joint modeling of keyword and visual feature distribution the database centric probabilistic retrieval model is compared to existing semantic labeling and retrieval method and shown to achieve higher accuracy than the previously best published result at a fraction of their computational cost 
in this paper we study term based feedback for information retrieval in the language modeling approach with term feedback a user directly judge the relevance of individual term without interaction with feedback document taking full control of the query expansion process we propose a cluster based method for selecting term to present to the user for judgment a well a effective algorithm for constructing refined query language model from user term feedback our algorithm are shown to bring significant improvement in retrieval accuracy over a non feedback baseline and achieve comparable performance to relevance feedback they are helpful even when there are no relevant document in the top 
large web search engine have to answer thousand of query per second with interactive response time due to the size of the data set involved often in the range of multiple terabyte a single query may require the processing of hundred of megabyte or more of index data to keep up with this immense workload large search engine employ cluster of hundred or thousand of machine and a number of technique such a caching index compression and index and query pruning are used to improve scalability in particular two level caching technique cache result of repeated identical query at the frontend while index data for frequently used query term are cached in each node at a lower level we propose and evaluate a three level caching scheme that add an intermediate level of caching for additional performance gain this intermediate level attempt to exploit frequently occurring pair of term by caching intersection or projection of the corresponding inverted list we propose and study several offline and online algorithm for the resulting weighted caching problem which turn out to be surprisingly rich in structure our experimental evaluation based on a large web crawl and real search engine query log show significant performance gain for the best scheme both in isolation and in combination with the other caching level we also observe that a careful selection of cache admission and eviction policy is crucial for best overall performance 
one challenging problem for biomedical text retrieval is to find accurate synonym or name variant for biomedical entity in this paper we propose a new concept based approach to tackle this problem in this approach a set of concept instead of keywords will be extracted from a query first then these concept will be used for retrieval purpose the experiment result show that the proposed approach can boost the retrieval performance and it generates very good result on trec genomics data set 
we study the problem of offering publish subscribe functionality on top of structured overlay network using data model and language from ir we show how to achieve this by extending the distributed hash table chord and present a detailed experimental evaluation of our proposal 
in corporate application vast amount of data are often stored in database system such a oracle apart from structured information this can include text document which cannot easily be retrieved using traditional sql query oracle includes mean to deal with full text document retrieval called oracle text that offer special query operator for search inside text field we have explored the effect of these different operator for query derived from natural language query this article compare the retrieval performance achieved with different automatic reformulations from natural language to oracle sql query 
memory based approach for collaborative filtering identify the similarity between two user by comparing their rating on a set of item in the past the memory based approach ha been shown to suffer from two fundamental problem data sparsity and difficulty in scalability alternatively the model based approach ha been proposed to alleviate these problem but this approach tends to limit the range of user in this paper we present a novel approach that combine the advantage of these two approach by introducing a smoothing based method in our approach cluster generated from the training data provide the basis for data smoothing and neighborhood selection a a result we provide higher accuracy a well a increased efficiency in recommendation empirical study on two datasets eachmovie and movielens show that our new proposed approach consistently outperforms other state of art collaborative filtering algorithm 
people are thirsty for medical information existing web search engine cannot handle medical search well because they do not consider it special requirement often a medical information searcher is uncertain about his exact question and unfamiliar with medical terminology therefore he prefers to pose long query describing his symptom and situation in plain english and receive comprehensive relevant information from search result this paper present medsearch a specialized medical web search engine to address these challenge medsearch can assist ordinary internet user to search for medical information by accepting query of extended length providing diversified search result and suggesting related medical phrase 
structural hint in xml retrieval query can be used to specify both the granularity of the search result the target element and where in a document to search support element these hint might be interpreted either strictly or vaguely but doe it matter if an xml search engine interprets these in one way and the user in another the performance of all run submitted to inex content and structure ca task were measured for each of four different interpretation of ca run that perform well for one interpretation of target element do so regardless of the interpretation of support element but how to interpret the target element doe matter this suggests that to perform well on all ca query it is necessary to know how the target structure specification should be interpreted we extend the nexi query language to include this and hypothesize that using this will increase the overall performance of search engine 
we present confluence an enhancement to a desktop file search tool called confluence which extract conceptual relationship between file by their temporal access pattern in the file system a limitation of a purely file based approach is that a file operation are increasingly abstracted by application their correlation to a user s activity weakens and thereby reduces the applicability of their temporal pattern to deal with this problem we augment the file event stream with a stream of window focus event from the ui layer we present algorithm that analyze this new stream extracting the user s task information which informs the existing confluence algorithm we present result and conclusion from a preliminary user study on confluence 
semantic web technology are bring increasingly employed to solve knowledge management issue in traditional web technology this paper follows that trend and proposes using semantic rule language to construct rule for defining access control rule for web service using these rule a system will be able to manage access to web service and also the information accessed via these service 
it is a truism that a the web grows in size and scope it becomes harder to find what we want to identify like minded people and community to find the best ad to offer and to have application work together smoothly service don t interoperate query yield long list of result most of which seem to miss the point if the web were a person we would expect richer and more successful interaction with it interaction that were quite literally more meaningful that s because in human discourse it is shared meaning that give u real communication yet with the current web meaning cannot be found much recent work ha aspired to change this both for human machine interchange and machine machine synchronization certainly the semantic web look to add meaning to our current simplistic matching of mere string of character against mere bag of word but can we legislate meaning from on high isn t meaning organic and determined by use a moving and context dependent target but if meaning is an evolving organic soup how are human able to get anything done with one another don t we love to define our term but then again is real definition even possible these question have daunted philosopher for year and we probably won t solve them here but we ll try to understand what s at the root of our own current religious debate should meaning on the web be evolutionary driven organically through the bottom up human assignment of tag or doe it need to be carefully crafted and managed by a higher authority using structured representation with defined semantics without picket sign or violence we hope our panelist will explore the two extreme end of the spectrum and several point in between 
current approach to identifying definitional sentence in the context of question answering mainly involve the use of linguistic or syntactic pattern to identify informative nugget this is insufficient a they do not address the novelty factor that a definitional nugget must also posse this paper proposes to address the deficiency by building a human interest model from external knowledge it is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic we compare and contrast our model with current definitional question answering model to show that interestingness play an important factor in definitional question answering 
standard information retrieval ir metric assume a simple model where document are understood a independent unit such an assumption is not adapted to new paradigm like xml or web ir where retrievable information are part of document or set of related document moreover classical hypothesis assumes that the user ignores the structural or logical context of document element and hence the possibility of navigation between unit eprum is a generalisation of precision recall pr that aim at allowing the user to navigate or browse in the corpus structure like the cumulated gain metric it is able to handle continuous valued relevance we apply and compare eprum in the context of xml retrieval a very active field for evaluation metric we also explain how eprum can be used in other ir paradigm 
we study the applicability of xml path summary in the context of current day xml database we find that summary provide an excellent basis for optimizin g data access method which furthermore mix very well with path partitioned store and with effic ient technique common in today s xml query processor such a smart node label also known a structural identifier and structural join we provide practical algorithm for building and exploiting summary and prove it benefit alone or in conjunction with a path partitioned store through ext ensive experiment 
a web service proliferate size and magnitude of uddi business registry ubrs are likely to increase the ability to discover web service of interest then across multiple ubrs becomes a major challenge specially when using primitive search method provided by existing uddi apis client do not have the time to endlessly search accessible ubrs for finding appropriate service particularly when operating via mobile device finding service of interest should be time effective and highly productive this paper address issue relating to the efficient access and discovery of web service across multiple ubrs and introduces a novel exploration engine the web service crawler engine wsce wsce is capable of crawling multiple ubrs and enables for the establishment of a centralized web service repository that can be used for discovering web service much more efficiently the paper present experimental validation result and analysis of the proposed idea 
subscriber to the popular news or blog feed r atom often face the problem of information overload a these feed source usually deliver large number of item periodically one solution to this problem could be clustering similar item in the feed reader to make the information more manageable for a user clustering item at the feed reader end is a challenging task a usually only a small part of the actual article is received through the feed in this paper we propose a method of improving the accuracy of clustering short text by enriching their representation with additional feature from wikipedia empirical result indicate that this enriched representation of text item can substantially improve the clustering accuracy when compared to the conventional bag of word representation 
this paper describes an application of ir and text categorization method to a highly practical problem in biomedicine specifically gene ontology go annotation go annotation is a major activity in most model organism database project and annotates gene function using a controlled vocabulary a a first step toward automatic go annotation we aim to assign go domain code given a specific gene and an article in which the gene appears which is one of the task challenge at the trec genomics track we approached the task with careful consideration of the specialized terminology and paid special attention to dealing with various form of gene synonym so a to exhaustively locate the occurrence of the target gene we extracted the word around the gene occurrence and used them to represent the gene for go domain code annotation a a classifier we adopted a variant of k nearest neighbor knn with supervised term weighting scheme to improve the performance making our method among the top performing system in the trec official evaluation moreover it is demonstrated that our proposed framework is successfully applied to another task of the genomics track showing comparable result to the best performing system 
in this paper we propose a novel chinese word segmentation method which leverage the huge deposit of web document and search technology it simultaneously solves ambiguous phrase boundary resolution and unknown word identification problem evaluation prove it effectiveness 
in current web service discovery and subscription consumer must pay too much time on manually selection and cannot easily benefit from the wide qos spectrum brought by the proliferating service in our approach we introduce the service pool a a virtual service grouping function identical service together and dispatching consumer request to the proper service in term of qos requirement 
we describe a technique to automatically classify a web page into an existing bookmark category to help a user to bookmark a page hyperbk compare a bag of word representation of the page to description of category in the user s bookmark file unlike default web browser dialog box in which the user may be presented with the category into which he or she saved the last bookmarked file hyperbk also offer the category most similar to the page being bookmarked the user can also opt to create a new category or save the page elsewhere in an evaluation the user s preferred category wa offered on average of the time 
document clustering is an important tool for text analysis and is used in many different application we propose to incorporate prior knowledge of cluster membership for document cluster analysis and develop a novel semi supervised document clustering model the method model a set of document with weighted graph in which each document is represented a a vertex and each edge connecting a pair of vertex is weighted with the similarity value of the two corresponding document the prior knowledge indicates pair of document that known to belong to the same cluster then the prior knowledge is transformed into a set of constraint the document clustering task is accomplished by finding the best cut of the graph under the constraint we apply the model to the normalized cut method to demonstrate the idea and concept our experimental evaluation show that the proposed document clustering model reveals remarkable performance improvement with very limited training sample and hence is a very effective semi supervised classification tool 
clustering algorithm have been widely used in information retrieval application however it is difficult to define an objective best result this article analyzes some document clustering algorithm and illustrates that they are equivalent to the optimization problem of some global function experiment show their good performance but there are still counter example where they fail to return the optimal solution we argue that monte carlo algorithm in the global optimization framework have the potential to find better solution than traditional clustering and they are able to handle more complex structure 
in the very near future complete household will be entirely networked a a de facto standard in this poster we briefly describe our work in the area of domotics where personalization semantics and agent technology come together we illustrate a home system oriented ontology and an intelligent agent based framework for the rapid development of home control and automation the ever changing nature of the home place the user in a position were he need to be involved and become through donet a part of an ongoing home system optimization process 
this paper proposes an incremental maintenance algorithm that efficiently update the materialized xpath xslt view defined using xpath expression in xp var the algorithm consists of two process the dynamic execution flow of an xslt program is stored a an xt xml transformation tree during the full transformation in response to a source xml data update the impacted portion of the xt tree are identified and maintained by partially re evaluating the xslt program this paper discus the xpath xslt feature of incremental view maintenance for subtree insertion deletion and applies them to the maintenance algorithm experiment show that the incremental maintenance algorithm outperforms full xml transformation algorithm by factor of up to 
in this poster we analyze recent work in the document identifier reassignment problem after that we present a formalization of a simple case of the problem a a psp pattern sequencing problem this may facilitate future work a it open a new research line to solve the general problem 
in most web site web based application such a web portal e marketplace search engine and in the file system of personal computer a wide variety of schema such a taxonomy directory tree thesaurus entity relationship schema rdf schema are published which i convey a clear meaning to human e g help in the navigation of large collection of document but ii convey only a small fraction if any of their meaning to machine a their intended meaning is not formally explicitly represented in this paper we present a general methodology for automatically eliciting and representing the intended meaning of these structure and for making this meaning available in domain like information integration and interoperability web service discovery and composition peer to peer knowledge management and semantic browser we also present an implementation called ctxmatch of how such a method can be used for semantic interoperability 
in this paper we propose a new system extracting potentially copyright infringement text from the web called epci epci extract them in the following way generating a set of query based on a given copyright reserved seed text putting every query to search engine api gathering the search result web page from high ranking until the similarity between the given seed text and the search result page becomes le than a given threshold value and merging all the gathered page then re ranking them in the order of their similarity our experimental result using seed text show that epci is able to extract potentially copyright infringement web page per a given copyright reserved seed text with precision in average 
due to resource constraint web archiving system and search engine usually have difficulty keeping the entire local repository synchronized with the web we advance the state of art of the sampling based synchronization technique by answering a challenging question given a sampled webpage and it change status which other webpage are also likely to change we present a study of various downloading granularity and policy and propose an adaptive model based on the update history and the popularity of the webpage we run extensive experiment on a large dataset of approximately webpage to demonstrate that it is most likely to find more updated webpage in the current or upper directory of the changed sample moreover the adaptive strategy outperform the non adaptive one in term of detecting important change 
the web crawler space is often delimited into two general area full web crawling and focused crawling we present netsifter a crawler system which integrates feature from these two area to provide an effective mechanism for web scale crawling netsifter utilizes a combination of page level analytics and heuristic which are applied to a sample of web page from a given website these algorithm score individual web page to determine the general utility of the overall website in doing so netsifter can formulate an in depth opinion of a website and the entirety of it web page with a relative minimum of work netsifter is then able to bias the future effort of it crawl towards higher quality website and away from the myriad of low quality website and crawler trap that litter the world wide web 
in this paper we show that most multiple term query include more than one topic and user usually reformulate their query by topic instead of term in order to provide empirical evidence on user s reformulation behavior and to help search engine better handle the query reformulation problem we focus on detecting internal topic in the original query and analyzing user reformulation to those topic particularly we utilize the interaction information ii to measure the degree of one sub query being a topic based on the local search result the experimental result on query log show that most user reformulate query at the topical level and our proposed ii based algorithm is a good method to detect topic from original query 
in this paper we present the cataclysm server platform for handling extreme overload in hosted internet application the primary contribution of our work is to develop a low overhead highly scalable admission control technique for internet application cataclysm provides several desirable feature such a guarantee on response time by conducting accurate size based admission control revenue maximization at multiple time scale via preferential admission of important request and dynamic capacity provisioning and the ability to be operational even under extreme overload cataclysm can transparently trade off the accuracy of it decision making with the intensity of the workload allowing it to handle incoming rate of several ten of thousand of request second we implement a prototype cataclysm hosting platform on a linux cluster and demonstrate the benefit of our integrated approach using a variety of workload 
the web ha become an excellent source for gathering consumer opinion there are now numerous web site containing such opinion e g customer review of product forum discussion group and blog this paper focus on online customer review of product it make two contribution first it proposes a novel framework for analyzing and comparing consumer opinion of competing product a prototype system called opinion observer is also implemented the system is such that with a single glance of it visualization the user is able to clearly see the strength and weakness of each product in the mind of consumer in term of various product feature this comparison is useful to both potential customer and product manufacturer for a potential customer he she can see a visual side by side and feature by feature comparison of consumer opinion on these product which help him her to decide which product to buy for a product manufacturer the comparison enables it to easily gather marketing intelligence and product benchmarking information second a new technique based on language pattern mining is proposed to extract product feature from pro and con in a particular type of review such feature form the basis for the above comparison experimental result show that the technique is highly effective and outperform existing method significantly 
this poster describes a potential problem with a relatively well used measure in information retrieval research kendall s tau rank correlation coefficient the coefficient is best known for it use in determining the similarity of test collection when ranking set of retrieval run threshold value for the coefficient have been defined and used in a number of published study in information retrieval however this poster present result showing that basing decision on such threshold is not a reliableas ha been assumed 
several approach to collaborative filtering have been studied but seldom have study been reported for large several millionusers and item and dynamic the underlying item set is continually changing setting in this paper we describe our approach to collaborative filtering for generating personalized recommendation for user of google news we generate recommendation using three approach collaborative filtering using minhash clustering probabilistic latent semantic indexing plsi and covisitation count we combine recommendation from different algorithm using a linear model our approach is content agnostic and consequently domain independent making it easily adaptable for other application and language with minimal effort this paper will describe our algorithm and system setup in detail and report result of running the recommendation engine on google news 
the enormous increase in recent year in the amount of information available online ha led to a renewed interest in a broad range of ir related area that go beyond plain document retrieval some of this new attention ha fallen on a subset of ir task in particular on entity retrieval task this emerging area differs from traditional document retrieval in a number of way entity are not represented directly a retrievable unit such a document and we need to identify them indirectly through occurrence in document this brings new exciting challenge to the field of information retrieval and information extraction in this thesis we focus on one particular type of entity people 
extracting morpheme from word is a nontrivial task rule based stemming approach such a porter s algorithm have encountered some success however they are restricted by their ability to identify a limited number of affix and are language dependent when dealing with language with many affix rule based approach generally require many more rule to deal with all the possible word form deriving these rule requires a larger effort on the part of linguist and in some instance can be simply impractical we propose an unsupervised ngram based approach named swordfish using ngram probability in the corpus possible morpheme are identified we look at two possible method for identifying candidate morpheme one using joint probability between two ngrams and the second based on log odds between prefix probability initial result indicate the joint probability approach to be better for english while the prefix ratio approach is better for finnish and turkish 
we study how to design experiment to measure the success rate of phishing attack that are ethical and accurate which are two requirement of contradictory force namely an ethical experiment must not expose the participant to any risk it should be possible to locally verify by the participant or representative thereof that this wa the case at the same time an experiment is accurate if it is possible to argue why it success rate is not an upper or lower bound of that of a real attack this may be difficult if the ethic consideration make the user perception of the experiment different from the user perception of the attack we introduce several experimental technique allowing u to achieve a balance between these two requirement and demonstrate how to apply these using a context aware phishing experiment on a popular online auction site which we call ronl our experiment exhibit a measured average yield of per collection of unique user this study wa authorized by the human subject committee at indiana university study 
a the web becomes a platform for implementing b b application the need arises of web conceptual model for describing web oriented workflow application implementing business process in this context new problem about process correctness arise due to the loose control of web application upon the behavior of their web client indeed incoherent user s behavior can lead to inconsistent process this paper present a high level approach to the management of exception that occur during the execution of process on the web we present a classification of exception that can be raised inside workflow driven web application and recovery policy to retrieve coherent status and data after an exception we devise these concept at high level and then we exploit them using a web modeling language webml that in turn provides development facility like automatic code generation validation of hypertext model and so on an industrial implementation experience is briefly presented too 
in this paper we propose an approach to automatically mine event evolution graph from newswires on the web event evolution graph is a directed graph in which the vertex and edge denote news event and the evolution between event respectively in a news affair our model utilizes the content similarity between event and incorporates temporal proximity and document distributional proximity a decaying function our approach is effective in presenting the inside development of news affair along the timeline which can facilitate user information browsing task 
this study investigates the impact of different search feature design in dl on user search experience the result indicate that the impact is significant in term of the number of query issued search step zero hit page returned and search error 
to increase confidence in the correctness of specified policy policy developer can conduct policy testing by supplying typical test input request and subsequently checking test output response against expected one unfortunately manual testing is tedious and few tool exist for automated testing of access control policy we present a fault model for access control policy and a framework to explore it the framework includes mutation operator used to implement the fault model mutant generation equivalent mutant detection and mutant killing determination this framework allows u to investigate our fault model evaluate coverage criterion for test generation and selection and determine a relationship between structural coverage and fault detection effectiveness we have implemented the framework and applied it to various policy written in xacml our experimental result offer valuable insight into choosing mutation operator in mutation testing and choosing coverage criterion in test generation and selection 
we report on two experiment performed to test the importance of term order in automatic summarisation experiment one wa undertaken a part of duc to which three system were submitted each with a different summarisation approach the system that used document term order outperformed those that did not use term order in the rouge evaluation experiment two made use of human evaluation of search engine result comparing our query term order summary with a simulation of current google search engine result summary in term of summary quality our qto system s summary aided user relevance judgement to a significantly greater extent than google s 
new communication technology can enable web user to access personalised information anytime anywhere however the network environment allowing this anytime anywhere access may have widely varying performance characteristic such a bandwidth level of congestion mobility support and cost of transmission it is unrealistic to expect that the quality of delivery of the same content can be maintained in this variable environment but rather an effort must be made to fit the content served to the current delivery condition thus ensuring high quality of experience qoe to the user this paper introduces an end user qoe aware adaptive hypermedia framework that extends the adaptation functionality of adaptive hypermedia system with a fine grained content based adaptation mechanism the proposed mechanism attempt to take into account multiple factor affecting qoe in relation to the delivery of web content various simulation test investigate the performance improvement provided by this mechanism in a home like low bit rate operational environment in term of access time per page aggregate access time per browsing session and quantity of transmitted information 
we propose a system for reminding a user of information obtained through a web browsing experience the system extract keywords from the content of the web page currently being viewed and retrieves the context of past web browsing related tothe keywords we define the context a a sequence of web browsing when many web page related to the keyword were viewed intensively because we assume that a lot of information connected to the current content wa obtained in the sequence the information is not only what page you viewed but also how you found those page and what knowledge you acquired from them specifically when you browse web page this system automatically display a list of the context judged to be important in relation to the current web page if you select the context detail of the context are shown graphically with marksindicating characteristic activity 
document in the web are often organized using category tree by information provider e g cnn bbc or search engine e g google yahoo such category tree are commonly known a web directory the category tree structure from different internet content provider may be similar to some extent but are usually not exactly the same a a result it is desirable to integrate these category tree together so that web user only need to browse through a unified category tree to extract information from multiple provider in this paper we address this problem by capturing structural information of multiple category tree which are embedded with the knowledge of professional in organizing the document our experiment with real web data show that the proposed technique is promising 
it is often argued that in information extraction ie certain machine learning ml approach save development time over others or that certain ml method e g active learning require le training data than others thus saving development cost however such development cost claim are not normally backed up by controlled study which show that such development cost saving actually occur this situation in language engineering is contrasted with software engineering in general where a lot of study investigating system development cost have been carried out we argue for the need of controlled study that measure actual system development time in language engineering to this end we carry out an experiment in resource monitoring for an ie task three named entity tagger for the same surprise domain are developed in parallel using competing method their human development time is accounted forusing a logging facility we report development cost result for parallel implementation of a named entity tagger and present a breakdown of the development time for the three alternative method we are not aware of detailed previous parallel study that detail how system development time is spent when creating a named entity tagger 
search system have for some time provided user with the ability to request document similar to a given document interface provide this feature via a link or button for each document in the search result we call this feature find similar or similarity browsing we examined find similar a a search tool like relevance feedback for improving retrieval performance our investigation focused on find similar s document to document similarity the reexamination of document during a search and the user s browsing pattern find similar with a query biased similarity avoiding the reexamination of document and a breadth like browsing pattern achieved a increase in the arithmetic mean average precision and a increase in the geometric mean average precision over our baseline retrieval this performance matched that of a more traditionally styled iterative relevance feedback technique 
in question answering two main kind of matching method for finding answer sentence for a question are term based approach which are simple efficient effective and yield high recall and event based approach that take syntactic and semantic information into account the latter often sacrifice recall for increased precision but actually capture the meaning of the event denoted by the textual unit of a passage or sentence we propose a robust data driven method that learns the mapping between question and answer using logistic regression and show that combining term based and event based approach significantly outperforms the individual method 
this paper describes two new technique for increasing the accuracy oftopic label assignment to conversational speech from oral history interview using supervised machine learning in conjunction with automatic speech recognition the first time shifted classification leverage local sequence information from the order in which the story is told the second temporal label weighting take the complementary perspective by using the position within an interview to bias label assignment probability these method when used in combination yield between and relative improvement in classification accuracy using a clipped r precision measure that model the utility of label set a segment summary in interactive speech retrieval application 
we consider the problem of evaluating retrieval system using a limited number of relevance judgment recent work ha demonstrated that one can accurately estimate average precision via a judged pool corresponding to a relatively small random sample of document in this work we demonstrate that given value or estimate of average precision one can accurately infer the relevance of unjudged document combined we thus show how one can efficiently and accurately infer a large judged pool from a relatively small number of judged document thus permitting accurate and efficient retrieval evaluation on a large scale 
we demonstrate a phonotactic semantic paradigm for spoken document categorization in this framework we define a set of acoustic word instead of lexical word to represent acoustic activity in spoken language the strategy for acoustic vocabulary selection is studied by comparing different feature selection method with an appropriate acoustic vocabulary a voice tokenizer convert a spoken document into a text like document of acoustic word thus a spoken document can be represented by a count vector named a bag of sound vector which characterizes a spoken document s semantic domain we study two phonotactic semantic classifier the support vector machine classifier and the latent semantic analysis classifier and their property the phonotactic semantic framework constitutes a new paradigm in spoken document classification a demonstrated by it success in the spoken language identification task it achieves error reduction over state of the art benchmark performance on the nist language recognition evaluation database 
learning village lv is an e learning platform for people s online discussion and frequently citing posting of one another in this paper we propose a novel method to rank credit author in the lv system we first propose a k eacm graph to describe the article citation structure in the lv system and then we build a weighted graph model k ucm graph to reveal the implicit relationship between author hidden behind the citation among their article furthermore we design a graph based ranking algorithm the credit author ranking car algorithm which can be applied to rank node in a graph with negative edge finally we perform experimental evaluation by simulation the result of evaluation illustrate that the proposed method work pretty well on ranking the credibility of user in the lv system 
schema statement in owl are interpreted quite differently from analogous statement in relational database if these statement are meant to be interpreted a integrity constraint ic owl s interpretation may seem confusing and or inappropriate therefore we propose an extension of owl with ic that capture the intuition behind ic in relational database we discus the algorithm for checking ic satisfaction for different type of knowledge base and show that if the constraint are satisfied we can disregard them while answering a broad range of positive query 
two year ago we conducted a study on the evolution of web page over time in the course of that study we discovered a large number of machine generated spam web page emanating from a handful of web server in germany these spam web page were dynamically assembled by stitching together grammatically well formed german sentence drawn from a large collection of sentence this discovery motivated u to develop technique for finding other instance of such slice and dice generation of web page where page are automatically generated by stitching together phrase drawn from a limited corpus we applied these technique to two data set a set of million web page collected in december and a set of million web page collected in june we found a number of other instance of large scale phrase level replication within the two data set this paper describes the algorithm we used to discover this type of replication and highlight the result of our data mining 
the increased importance of xml a a data representation format ha led to several proposal for facilitating the development of application that operate on xml data these proposal range from runtime api based interface to xml based programming language the subject of this paper is xj a research language that proposes novel mechanism for the integration of xml a a first class construct into java the design goal of xj distinguish it from past work on integrating xml support into programming language specifically the xj design adheres to the xml schema and xpath standard moreover it support in place update of xml data thereby keeping with the imperative nature of java we have built a prototype compiler for xj and our preliminary experiment demonstrate that the performance of xj program can approach that of traditional low level api based interface while providing a higher level of abstraction 
in this paper we investigate the effectiveness of a document independent technique for eliciting feedback from user about their information problem we propose that such a technique can be used to elicit term from user for use in query expansion and a a follow up when ambiguous query are initially posed by user we design a feedback form to obtain additional information from user administer the form to user after initial querying and create a series of experimental run based on the information that we obtained from the form result demonstrate that the form wa successful at eliciting more information from user and that this additional information significantly improved retrieval performance our result further demonstrate a strong relationship between query length and performance 
nowadays a large and growing percentage of information is stored in various multimedia format in order for multimedia information to be efficiently utilised by user it is very important to add suitable metadata in this paper we will present aktivemedia a tool for enriching multimedia document with semantic information 
a key challenge for dynamic web service selection is that web service are typically highly configurable and service requester often have dynamic preference on service configuration current approach such a w agreement describe web service by enumerating the various possible service configuration an inefficient approach when dealing with numerous service attribute with large value space we model web service configuration and associated price and preference more compactly using utility function policy which also allows u to draw from multi attribute decision theory method to develop an algorithm for optimal service selection in this paper we present an owl ontology for the specification of configurable web service offer and request and a flexible and extensible framework for optimal service selection that combine declarative logic based matching rule with optimization method such a linear programming assuming additive price preference function experimental result indicate that our algorithm introduces an overhead of only around sec compared to random service selection while giving optimal result the overhead a percentage of total time decrease a the number of offer and configuration increase 
substantial eort is wasted in scientific circle by researcher who rediscover idea that have already been published in the literature this problem ha been alleviated somewhat by the availability of recent academic work online however the kind of text search system in popular use today are poor at handling vocabulary mismatch so a researcher must know the word used in relevant document in order to find them this make serendipitous result unlikely we approach the problem of literature search by considering an unpublished manuscript a a query to a search system with this approach the entire text content of the paper can be used in the search process we use the text of previous literature a well a the citation graph that connects it to find relevant related material we evaluate our technique with manual and automatic evaluation method and find an order of magnitude improvement in mean average precision a compared to a text similarity baseline 
according to a recent survey made by nielsen netratings searching on news article is one of the most important activity online indeed google yahoo msn and many others have proposed commercial search engine for indexing news feed despite this commercial interest no academic research ha focused on ranking a stream of news article and a set of news source in this paper we introduce this problem by proposing a ranking framework which model the process of generation of a stream of news article the news article clustering by topic and the evolution of news story over the time the ranking algorithm proposed rank news information finding the most authoritative news source and identifying the most interesting event in the different category to which news article belongs all these ranking measure take in account the time and can be obtained without a predefined sliding window of observation over the stream the complexity of our algorithm is linear in the number of piece of news still under consideration at the time of a new posting this allow a continuous on line process of ranking our ranking framework is validated on a collection of more than piece of news produced in two month by more then news source belonging to different category world u s europe sport business etc this collection is extracted from the index of cometomyhead an academic news search engine available online 
when applying blind relevance feedback for ad hoc document retrieval is it possible to identify a priori the set of query term that will most improve retrieval performance can this complex problem be reduced into the simpler one of making independent decision about the performance effect of each query term our experiment suggest that for the selection of term for blind relevance feedback the term independence assumption may be empirically justified 
major search engine currently use the history of a user s action e g query click to personalize search result in this paper we present a new personalized service query specific web recommendation qsrs that retroactively answer query from a user s history a new result arise the qsr system address two important subproblems with application beyond the system itself automatic identification of query in a user s history that represent standing interest and unfulfilled need effective detection of interesting new result to these query we develop a variety of heuristic and algorithm to address these problem and evaluate them through a study of google history user our result strongly motivate the need for automatic detection of standing interest from a user s history and identifies the algorithm that are most useful in doing so our result also identify the algorithm some which are counter intuitive that are most useful in identifying interesting new result for past query allowing u to achieve very high precision over our data set 
in this paper we present a system that allows a user to explore or mine a document collection this system is based on domain and task knowledge modelled in the form of ontology and allows direct access both to information a it is stored and to information that is built from it the system ha been developed in java 
current keyword oriented search engine for theworld wideweb do not allow specifying the semantics of query we address this limitation with naga a new semantic search engine naga build on a large semantic knowledge base of binary relationship fact derived from the web naga provides a simple yet expressive query language to query this knowledge base the result are then ranked with an intuitive scoring mechanism we show the effectiveness and utility of naga by comparing it output with that of googleon some interesting query 
this paper study the problem of identifying comparative sentence in text document the problem is related to but quite different from sentiment opinion sentence identification or classification sentiment classification study the problem of classifying a document or a sentence based on the subjective opinion of the author an important application area of sentiment opinion identification is business intelligence a a product manufacturer always want to know consumer opinion on it product comparison on the other hand can be subjective or objective furthermore a comparison is not concerned with an object in isolation instead it compare the object with others an example opinion sentence is the sound quality of cd player x is poor an example comparative sentence is the sound quality of cd player x is not a good a that of cd player y clearly these two sentence give different information their language construct are quite different too identifying comparative sentence is also useful in practice because direct comparison are perhaps one of the most convincing way of evaluation which may even be more important than opinion on each individual object this paper proposes to study the comparative sentence identification problem it first categorizes comparative sentence into different type and then present a novel integrated pattern discovery and supervised learning approach to identifying comparative sentence from text document experiment result using three type of document news article consumer review of product and internet forum posting show a precision of and recall of more detailed result are given in the paper 
with the growing use of dynamic web content generated from relational database traditional caching solution for through put and latency improvement are ineffective we describe a middleware layer called ganesh that reduces the volume of data transmitted without semantic interpretation of query or result it achieves this reduction through the use of cryptographic hashing to detect similarity with previous result these benefit do not require any compromise of the strict consistency semantics provided by the back end database further ganesh doe not require modification to application web server or database server and work with closed source application and database using two bench mark representative of dynamic web site measurement of our prototype show that it can increase end to end throughput by a much a two fold for non data intensive application and by a much a ten fold for dataintensive one 
we present a novel algorithm dragpushing for automatic text classification using a training data set the algorithm first calculates the prototype vector or centroid for each of the available document class using misclassified example it then iteratively refines these centroid by dragging the centroid of a correct class towards a misclassified example and in the same time pushing the centroid of an incorrect class away from the misclassified example the algorithm is simple to implement and is computationally very efficient evaluation experiment conducted on two benchmark collection show that it classification accuracy is comparable to that of more complex method such a support vector machine svm 
we describe an approach designed to reduce the cost of ontology development through the use of untrained volunteer knowledge engineer result are provided from an experiment in which volunteer were asked to judge the correctness of automatically inferred subsumption relationship in the biomedical domain the experiment indicated that volunteer can be recruited fairly easily but that their attention is difficult to hold that most do not understand the subsumption relationship without training and that incorporating learned estimate of trust into voting system is beneficial to aggregate performance 
we propose using the stream control transmission protocol sctp a recent ietf transport layer protocol for reliable web transport although tcp ha traditionally been used we argue that sctp better match the need of http based network application this position paper discus sctp feature that address i head of line blocking within a single tcp connection ii vulnerability to network failure and iii vulnerability to denial of service syn attack we discus our experience in modifying the apache server and the firefox browser to benefit from sctp and demonstrate our http over sctp design via simple experiment we also discus the benefit of using sctp in other web domain through two example scenario multiplexing user request and multiplexing resource access finally we highlight several sctp feature that will be valuable to the design and implementation of current http based client server application 
we consider the problem of online keyword advertising auction among multiple bidder with limited budget and study a natural bidding heuristic in which advertiser attempt to optimize their utility by equalizing their return on investment across all keywords we show that existing auction mechanism combined with this heuristic can experience cycling a ha been observed in many current system and therefore propose a modified class of mechanism with small random perturbation this perturbation is reminiscent of the small time dependent perturbation employed in the dynamical system literature to convert many type of chaos into attracting motion we show that the perturbed mechanism provably converges in the case of first price auction and experimentally converges in the case of second price auction moreover the point of convergence ha a natural economic interpretation a the unique market equilibrium in the case of first price mechanism in the case of second price auction we conjecture that it converges to the supply aware market equilibrium thus our result can be alternatively described a a t tonnement process for convergence to market equilibriumin which price are adjusted on the side of the buyer rather than the seller we also observe that perturbation in mechanism design is useful in a broader context in general it can allow bidder to share a particular item leading to stable allocation and pricing for the bidder and improved revenue for the auctioneer 
the current boom of the web is associated with the revenue originated from on line advertising while search based advertising is dominant the association of ad with a web page during user navigation is becoming increasingly important in this work we study the problem of associating ad with a web page referred to a content targeted advertising from a computer science perspective we assume that we have access to the text of the web page the keywords declared by an advertiser and a text associated with the advertiser s business using no other information and operating in fully automatic fashion we propose ten strategy for solving the problem and evaluate their effectiveness our method indicate that a matching strategy that take into account the semantics of the problem referred to a aak for ad and keywords can yield gain in average precision figure of compared to a trivial vector based strategy further a more sophisticated impedance coupling strategy which expands the text of the web page to reduce vocabulary impedance with regard to an advertisement can yield extra gain in average precision of these are first result they suggest that great accuracy in content targeted advertising can be attained with appropriate algorithm 
we show that a set of independently developed spam filter may be combined in simple way to provide substantially better filtering than any of the individual filter the result of fifty three spam filter evaluated at the trec spam track were combined post hoc so a to simulate the parallel on line operation of the filter the combined result were evaluated using the trec methodology yielding more than a factor of two improvement over the best filter the simplest method averaging the binary classification returned by the individual filter yield a remarkably good result a new method averaging log odds estimate based on the score returned by the individual filter yield a somewhat better result and provides input to svmand logistic regression based stacking method the stacking method appear to provide further improvement but only for very large corpus of the stacking method logistic regression yield the better result finally we show that it is possible to select a priori small subset of the filter that when combined still outperform the best individual filter by a substantial margin 
due to the rapid acceptance of web service and it fast spreading a number of mission critical system will be deployed a web service in next year the availability of those system must be guaranteed in case of failure and network disconnection an example of web service for which availability will be a crucial issue are those belonging to coordination web service infrastructure such a web service for transactional coordination e g w caf and w transaction these service should remain available despite site and connectivity failure to enable business interaction on a x basis some of the common technique for attaining availability consist in the use of a clustering approach however in an internet setting a domain can get partitioned from the network due to a link overload or some other connectivity problem the unavailability of a coordination service impact the availability of all the partner in the business process that is coordination service are an example of critical component that need higher provision for availability in this paper we address this problem by providing an infrastructure w replication for wan replication of web service the infrastructure is based on a group communication web service w multicast that respect the web service autonomy the transport of w multicast is based on soap and relies exclusively on web service technology for interaction across organization we have replicated w caf using our w replication framework and evaluated it performance 
we present ester a modular and highly efficient system for combined full text and ontology search ester build on a query engine that support two basic operation prefix search and join both of these can be implemented very efficiently with a compact index yet in combination provide powerful querying capability we show how ester can answer basic sparql graph pattern query on the ontology by reducing them to a small number of these two basic operation ester further support a natural blend of such semantic query with ordinary full text query moreover the prefix search operation allows for a fully interactive and proactive user interface which after every keystroke suggests to the user possible semantic interpretation of his or her query and speculatively executes the most likely of these interpretation a a proof of concept we applied ester to the english wikipedia which contains about million document combined with the recent yago ontology which contains about million fact for a variety of complex query ester achieves worst case query processing time of a fraction of a second on a single machine with an index size of about gb 
the environment generated medium egm are defined here a being generated from a massive amount of and or incomprehensible environmental data by compressing them into average or representative value and or by converting them into such user friendly medium a text figure chart and animation a an application of egm an object participation type weblog is introduced where anthropomorphic indoor object with sensor node post weblog entry and comment about what happened to them in a sensor networked environment 
reputation system have been popular in estimating the trustworthiness and predicting the future behavior of node in a large scale distributed system where node may transact with one another without prior knowledge or experience one of the fundamental challenge in distributed reputation management is to understand vulnerability and develop mechanism that can minimize the potential damage to a system by malicious node in this paper we identify three vulnerability that are detrimental to decentralized reputation management and propose trustguard a safeguard framework for providing a highly dependable and yet efficient reputation system first we provide a dependable trust model and a set of formal method to handle strategic malicious node that continuously change their behavior to gain unfair advantage in the system second a transaction based reputation system must cope with the vulnerability that malicious node may misuse the system by flooding feedback with fake transaction third but not least we identify the importance of filtering out dishonest feedback when computing reputation based trust of a node including the feedback filed by malicious node through collusion our experiment show that comparing with existing reputation system our framework is highly dependable and effective in countering malicious node regarding strategic oscillating behavior flooding malevolent feedback with fake transaction and dishonest feedback 
metric such a click count are vital to online business but their measurement ha been problematic due to inclusion of high variance robot traffic we posit that by applying statistical method more rigorous than have been employed to date that we can build a robust model of thedistribution of click following which we can set probabilistically sound threshold to address outlier and robot prior research in this domain ha used inappropriate statistical methodology to model distribution and current industrial practice eschews this research for conservative ad hoc click level threshold prevailing belief is that such distribution are scale free power law distribution but using more rigorous statistical method we find the best description of the data is instead provided by a scale sensitive zipf mandelbrot mixture distribution our result are based on ten data set from various vertical in the yahoo domain since mixture model can overfit the data we take care to use the bic log likelihood method which penalizes overly complex model using a mixture model in the web activity domain make sense because there are likely multiple class of user in particular we have noticed that there is a significantly large set of user that visit the yahoo portal exactly once a day we surmise these may be robot testing internet connectivity by pinging the yahoo main website backing up our quantitative analysis is graphical analysis in which empirical distribution are plotted against heoretical distribution in log log space using robust cumulative distribution plot this methodology ha two advantage plotting in log log space allows one to visually differentiate the various exponential distribution and secondly cumulative plot are much more robust to outlier we plan to use the result of this work for application for robot removal from web metric business intelligence system 
this paper introduces a novel link based ranking algorithm based on a model of focused web surfer focusedrank is described and compared to implementation of pagerank and topic sensitive pagerank we report a user study that measure the relevance and precision of each approach focusedrank give superior relevancy over pagerank while significantly reducing the computational complexity compared to the topic senstivice pagerank 
geographic information retrieval gir system allow user to specify a geographic context in addition to a more traditional query enabling the system to pinpoint interesting search result whose relevancy is location dependent in particular local search service have become a widely used mechanism to find business such a hotel restaurant and shop which satisfy a geographical restriction unfortunately many useful type of geographic restriction are currently not supported in these system including restriction that specify the neighborhood in which the business should be located a the boundary of city neighborhood are not readily available automated technique to construct representation of the spatial extent of neighborhood are required to support this kind of restriction in this paper we propose such a technique using fuzzy footprint to cope with the inherent vagueness of most neighborhood boundary and we provide experimental result that demonstrate the potential of our technique in a local search setting 
translating out of vocabulary oov term is a great challenge for the cross lingual information retrieval and data driven machine translation system several approach have been proposed to mine translation for oov term from the web especially from page containing mixed language in this paper we propose a novel approach to automatically translate oov term on the fly through cross lingual query expansion the proposed approach doe not require any web crawling and ha achieved an inclusion rate of and overall translation accuracy of outperforming state of the art oov translation technique 
the i spy meta search engine us a technique called collaborative web search to leverage the past search behaviour query and selection of a community of user in order to promote search result that are relevant to the community in this paper we describe recent study to clarify the benefit of this approach in situation when the behaviour of user cannot be relied upon in term of their ability to consistently select relevant result during search session 
the ability to extract meaningful fragment from an ontology is key for ontology re use we propose a definition of a module that guarantee to completely capture the meaning of a given set of term i e to include all axiom relevant to the meaning of these term and study the problem of extracting minimal module we show that the problem of determining whether a subset of an ontology is a module for a given vocabulary is undecidable even for rather restricted sub language of owl dl hence we propose two approximation i e alternative definition of module for a vocabulary that still provide the above guarantee but that are possibly too strict and that may thus result in larger module the first approximation is semantic and can be computed using existing dl reasoner the second is syntactic and can be computed in polynomial time finally we report on an empirical evaluation of our syntactic approximation which demonstrates that the module we extract are surprisingly small 
personalized web search is a promising way to improve search quality by customizing search result for people with individual information goal however user are uncomfortable with exposing private preference information to search engine on the other hand privacy is not absolute and often can be compromised if there is a gain in service or profitability to the user thus a balance must be struck between search quality and privacy protection this paper present a scalable way for user to automatically build rich user profile these profile summarize a user s interest into a hierarchical organization according to specific interest two parameter for specifying privacy requirement are proposed to help the user to choose the content and degree of detail of the profile information that is exposed to the search engine experiment showed that the user profile improved search quality when compared to standard msn ranking more importantly result verified our hypothesis that a significant improvement on search quality can be achieved by only sharing some higher level user profile information which is potentially le sensitive than detailed personal information 
song and bruza introduce a framework for information retrieval ir based on gardenfor s three tiered cognitive model conceptual space they instantiate a conceptual space using hyperspace analogue to language hal to generate higher order concept which are later used for ad hoc retrieval in this poster we propose an alternative implementation of the conceptual space by using a probabilistic hal space phal to evaluate whether converting to such an implementation is beneficial we have performed an initial investigation comparing the concept combination of hal against phal for the task of query expansion our experiment indicate that phal outperforms the original hal method and that better query term selection method can improve performance on both hal and phal 
hierarchical model are commonly used to organize a website s content a website s content structure can be represented by a topic hierarchy a directed tree rooted at a website s homepage in which the vertex and edge correspond to web page and hyperlink in this work we propose a new method for constructing the topic hierarchy of a website we model the website s link structure using weighted directed graph in which the edge weight are computed using a classifier that predicts if an edge connects a pair of node representing a topic and a sub topic we then pose the problem of building the topic hierarchy a finding the shortest path tree and directed minimum spanning tree in the weighted graph we ve done extensive experiment using real website and obtained very promising result 
much of human activity defines an information context we awaken start work and hold meeting at roughly the same time every day and retrieve the same information item day planner itinerary schedule agenda report menu web page etc for many of these activity information retrieval system in general lack sensitivity to such recurrent context requiring user to remember and re enter search cue for object regardless of how regularly or consistently the object are used and to develop ad hoc storage strategy we propose that in addition to semantic cue information object should also be indexed by temporal and sensory cue such a clock time and location so that object can be retrieved by external environmental context in addition to any internal semantic content our cue event object ceo model us a network representation to associate information object with the time and condition location weather etc when they are typically used user can query the system to review their activity revealing what they do at particular time and which information object tend to be most often used and when the system can also pre fetch item that have proven useful in past similar situation the ceo model is incremental real time and dynamic maintaining an accurate summary even a a user s information behaviour change over time such environmentally aware system have application in personal information management mobile device and smart home a a memory prosthesis the model can support autonomous living for the cognitively impaired we present a comprehensive research agenda based on some promising preliminary finding 
this paper is concerned with automatic extraction of title from the body of html document title of html document should be correctly defined in the title field however in reality html title are often bogus it is desirable to conduct automatic extraction of title from the body of html document this is an issue which doe not seem to have been investigated previously in this paper we take a supervised machine learning approach to address the problem we propose a specification on html title we utilize format information such a font size position and font weight a feature in title extraction our method significantly outperforms the baseline method of using the line in largest font size a title improvement in f score a application we consider web page retrieval we use the trec web track data for evaluation we propose a new method for html document retrieval using extracted title experimental result indicate that the use of both extracted title and title field is almost always better than the use of title field alone the use of extracted title is particularly helpful in the task of named page finding improvement 
research in information retrieval usually show performanceimprovement when many source of evidence are combined to produce a ranking of document e g text picture sound etc in this paper we focus on the rank aggregation problem also called data fusion problem where ranking of document searched into the same collection and provided by multiple method are combined in order to produce a new ranking in this context we propose a rank aggregation method within a multiple criterion framework using aggregation mechanism based on decision rule identifying positive and negative reason for judging whether a document should get a better rank than another we show that the proposed method deal well with the information retrieval distinctive feature experimental result are reported showing that the suggested method performs better than the well known combsum and combmnz operator 
in this paper we focus on the development of a framework for automatic metadata generation the first step towards this framework is the definition of an application programmer interface api which we call the simple indexing interface sii the second step is the definition of a framework for implementation of the sii both step are presented in some detail in this paper we also report on empirical evaluation of the metadata that the sii and supporting framework generated in a real life context 
without the proliferation of formal semantic annotation the semantic web is certainly doomed to failure in earlier work we presented a new paradigm to avoid this the self annotating web in which globally available knowledge is used to annotate resource such a web page in particular we presented a concrete method instantiating this paradigm called pankow pattern based annotation through knowledge on the web in pankow a named entity to be annotated is put into several linguistic pattern that convey competing semantic meaning the pattern that are matched most often on the web indicate the meaning of the named entity leading to automatic or semi automatic annotation in this paper we present c pankow context driven pankow which alleviates several shortcoming of pankow first by downloading abstract and processing them off line we avoid the generation of large number of linguistic pattern and correspondingly large number of google query second by linguistically analyzing and normalizing the downloaded abstract we increase the coverage of our pattern matching mechanism and overcome several limitation of the earlier pattern generation process third we use the annotation context in order to distinguish the significance of a pattern match for the given annotation task our experiment show that c pankow inherits all the advantage of pankow no training required etc but in addition it is far more efficient and effective 
accurately and effectively detecting the location where search query are truly about ha huge potential impact on increasing search relevance in this paper we define a search query s dominant location qdl and propose a solution to correctly detect it qdl is geographical location s associated with a query in collective human knowledge i e one or few prominent location agreed by majority of people who know the answer to the query qdl is a subjective and collective attribute of search query and we are able to detect qdls from both query containing geographical location name and query not containing them the key challenge to qdl detection include false positive suppression not all contained location name in query mean geographical location and detecting implied location by the context of the query in our solution a query is recursively broken into atomic token according to it most popular web usage for reducing false positive if we do not find a dominant location in this step we mine the top search result and or query log with different approach discussed in this paper to discover implicit query location our large scale experiment on recent msn search query show that our query location detection solution ha consistent high accuracy for all query frequency range 
evaluation of information retrieval system is one of the core task in information retrieval problem include the inability to exhaustively label all document for a topic generalizability from a small number of topic and incorporating the variability of retrieval system previous work address the evaluation of system the ranking of query by difficulty and the ranking of individual retrieval by performance approach exist for the case of few and even no relevance judgment our focus is on zero judgment performance prediction of individual retrieval one common shortcoming of previous technique is the assumption of uncorrelated document score and judgment if document are embedded in a high dimensional space a they often are we can apply technique from spatial data analysis to detect correlation between document score we find that the low correlation between score of topically close document often implies a poor retrieval performance when compared to a state of the art baseline we demonstrate that the spatial analysis of retrieval score provides significantly better prediction performance these new predictor can also be incorporated with classic predictor to improve performance further we also describe the first large scale experiment to evaluate zero judgment performance prediction for a massive number of retrieval system over a variety collection in several language 
determining the similarity of short text snippet such a search query work poorly with traditional document similarity measure e g cosine since there are often few if any term in common between two short text snippet we address this problem by introducing a novel method for measuring the similarity between short text snippet even those without any overlapping term by leveraging web search result to provide greater context for the short text in this paper we define such a similarity kernel function mathematically analyze some of it property and provide example of it efficacy we also show the use of this kernel function in a large scale system for suggesting related query to search engine user 
it is difficult for user of mobile device such a cellular phone equipped with a small screen and a poor input interface to browse web page designed for desktop pc with large display many study and commercial product have tried to solve this problem web page include image that have various role such a site menu line header for itemization and page title however most study of mobile web browsing haven t paid much attention to the role of web image in this paper we define eleven web image category according to their role and use these category for proper web image handling we manually categorized web image collected from forty web site and extracted image feature of each category according to the classification by making use of the extracted feature we devised an automatic web image classification method furthermore we evaluated the automatic classification of real web page and achieved up to classification accuracy we also implemented an automatic web page scrolling system a an application of our automatic image classification method 
traditional clustering algorithm work on flat data making the assumption that the data instance can only be represented by a set of homogeneous and uniform feature many real world data however is heterogeneous in nature comprising of multiple type of interrelated component we present a clustering algorithm k svmeans that integrates the well known k mean clustering with the highly popular support vector machine svm in order to utilize the richness of data our experimental result on authorship analysis of scientific publication show that k svmeans achieves better clustering performance than homogeneous data clustering 
small xml element are often estimated relevant by the retrieval model but they are not desirable retrieval unit this paper present a generic model that exploit the information obtained from small element we identify relationship between small and relevant element and use this linking information to reinforce the relevance of other element before removing the small one our experiment using the inex testbed show the effectiveness of our approach 
previous research into the efficiency of text retrieval system ha dealt primarily with method that consider inverted list in sequence these method are known a term at a time method however the literature for optimizing document at a time system remains sparse we present an improvement to the max score optimization which is the most efficient known document at a time scoring method like max score our technique called term bounded max score is guaranteed to return exactly the same score and document a an unoptimized evaluation which is particularly useful for query model research we simulated our technique to explore the problem space then implemented it in indri our large scale language modeling search engine test with the gov corpus on title query show our method to be faster than max score alone and faster than our document at a time baseline our optimized query time are competitive with conventional term at a time system on this year s trec terabyte task 
understanding the extent to which people s search behavior differ in term of the interaction flow and information targeted is important in designing interface to help world wide web user search more effectively in this paper we describe a longitudinal log based study that investigated variability in people s interaction behavior when engaged in search related activity on the web allwe analyze the search interaction of more than two thousand volunteer user over a five month period with the aim of characterizing difference in their interaction style allthe finding of our study suggest that there are dramatic difference in variability in key aspect of the interaction within and between user and within and between the search query they submit allour finding also suggest two class of extreme user navigator and explorer whose search interaction is highly consistent or highly variable lesson learned from these user can inform the design of tool to support effective web search interaction for everyone 
the workflow language martlet described in this paper implement a new programming model that allows user to write parallel program and analyse distributed data without having to be aware of the detail of the parallelisation martlet abstract the parallelisation of the computation and the splitting of the data through the inclusion of construct inspired by functional programming these allow program to be written a an abstract description that can be adjusted automatically at runtime to match the data set and available resource using this model it is possible to write program to perform complex calculation across a distributed data set such a singular value decomposition or least square problem a well a creating an intuitive way of working with distributed system having described and evaluated martlet against other functional language for parallel computation this paper go on to look at how martlet might develop in doing so it cover both possible addition to the language itself and the use of jit compiler to increase the range of platform it is capable of running on 
search algorithm incorporating some form of topic model have a long history in information retrieval for example cluster based retrieval ha been studied since the s and ha recently produced good result in the language model framework an approach to building topic model based on a formal generative model of document latent dirichlet allocation lda is heavily cited in the machine learning literature but it feasibility and effectiveness in information retrieval is mostly unknown in this paper we study how to efficiently use lda to improve ad hoc retrieval we propose an lda based document model within the language modeling framework and evaluate it on several trec collection gibbs sampling is employed to conduct approximate inference in lda and the computational complexity is analyzed we show that improvement over retrieval using cluster based model can be obtained with reasonable efficiency 
in this paper we investigate the consistency of answer assessment in a complex question answering task examining feature of assessor consistency type of answer and question type 
this poster introduces a novel approach to information retrieval that us statistical model averaging to improve latent semantic indexing lsi instead of choosing a single dimensionality k for lsi we propose using several model of differing dimensionality to inform retrieval to manage this ensemble we weight each model s contribution to an extent inversely proportional to it aic akaike information criterion thus each model contributes proportionally to it expected kullback leibler divergence from the distribution that generated the data we present result on three standard ir test collection demonstrating significant improvement over both the traditional vector space model and single model lsi 
in this paper we study a new problem of mining causal relation of query in search engine query log causal relation between two query mean event on one query is the causation of some event on the other we first detect event in query log by efficient statistical frequency threshold then the causal relation of query is mined by the geometric feature of the event finally the granger causality test gct is utilized to further re rank the causal relation of query according to their gct coefficient in addition we develop a dimensional visualization tool to display the detected relationship of event in a more intuitive way the experimental result on the msn search engine query log demonstrate that our approach can accurately detect the event in temporal query log and the causal relation of query is detected effectively 
we describe a user study that examined the relationship between the quality of an information retrieval system and the effectiveness of it user in performing a task the task involves finding answer facet of question pertaining to a collection of newswire document over a six month period we artificially created set of ranked list at increasing level of quality by blending the output of a state of the art retrieval system with truth data created by annotator subject performed the task by using these ranked list to guide their labeling of answer passage in the retrieved article we found that a system accuracy improves subject time on task and error rate decrease and the rate of finding new correct answer increase there is a large intermediary region in which the utility difference is not significant our result suggest that there is some threshold of accuracy for this task beyond which user utility improves rapidly but more experiment are needed to examine the area around that threshold closely 
each month more attack are launched with the aim of making web user believe that they are communicating with a trusted entity for the purpose of stealing account information logon credential and identity information in general this attack method commonly known a phishing is most commonly initiated by sending out email with link to spoofed website that harvest information we present a method for detecting these attack which in it most general form is an application of machine learning on a feature set designed to highlight user targeted deception in electronic communication this method is applicable with slight modification to detection of phishing website or the email used to direct victim to these site we evaluate this method on a set of approximately such phishing email and non phishing email and correctly identify over of the phishing email while only mi classifying on the order of of the legitimate email we conclude with thought on the future for such technique to specifically identify deception specifically with respect to the evolutionary nature of the attack and information available 
modeling web query reformulation process is still an unsolved problem in this paper we argue that lexical analysis is highly beneficial for this purpose we propose to use the variation in query clarity a well a the part of speech pattern transition a indicator of user s search action experiment with a log of million query showed our technique to be more flexible than the current approach while also providing u with interesting insight into user s web behavioral pattern 
we present a paradigm for uniting the diverse strand of xml based web technology by allowing them to be incorporated within a single document this overcomes the distinction between program and data to make xml truly self describing a proposal for a lightweight yet powerful functional xml vocabulary called semantic fxml is detailed based on the well understood functional programming paradigm and resembling the embedding of lisp directly in xml infosets are made dynamic since document can now directly embed local process or web service into their infoset an optional typing regime for info set is provided by semantic web ontology by regarding web service a function and the semantic web a providing type and tying it all together within a single xml vocabulary the web can compute in this light the real web can be considered the transformation of the web from a universal information space to a universal computation space 
a fair contract signing protocol allows two potentially mistrusted parity to exchange their commitment i e digital signature to an agreed contract over the internet in a fair way so that either each of them obtains the other s signature or neither party doe based on the rsa signature scheme a new digital contract signing protocol is proposed in this paper like the existing rsa based solution for the same problem our protocol is not only fair but also optimistic since the third trusted party is involved only in the situation where one party is cheating or the communication channel is interrupted furthermore the proposed protocol satisfies a new property i e it is abuse free that is if the protocol is executed unsuccessfully none of the two party can show the validity of intermediate result to others technical detail are provided to analyze the security and performance of the proposed protocol in summary we present the first abuse free fair contract signing protocol based on the rsa signature and show that it is both secure and efficient 
one hundred user one hundred need a more and more topic are being discussed on the web and our vocabulary remains relatively stable it is increasingly difficult to let the search engine know what we want coping with ambiguous query ha long been an important part of the research on information retrieval but still remains a challenging task personalized search ha recently got significant attention in addressing this challenge in the web search community based on the premise that a user s general preference may help the search engine disambiguate the true intention of a query however study have shown that user are reluctant to provide any explicit input on their personal preference in this paper we study how a search engine can learn a user s preference automatically based on her past click history and how it can use the user preference to personalize search result our experiment show that user preference can be learned accurately even from little click history data and personalized search based on user preference yield significant improvement over the best existing ranking mechanism in the literature 
it is widely believed that some query submitted to search engine are by nature ambiguous e g java apple however few study have investigated the question of how many query are ambiguous and how can we automatically identify an ambiguous query this paper deal with these issue first we construct the taxonomy of query ambiguity and ask human annotator to manually classify query based upon it from manually labeled result we find that query ambiguity is to some extent predictable we then use a supervised learning approach to automatically classify query a being ambiguous or not experimental result show that we can correctly identify of labeled query finally we estimate that about of query in a real search log are ambiguous 
this paper revisits the static term based pruning technique presented in carmel et al sigir for ad hoc retrieval addressing different issue concerning it algorithmic design not yet taken into account although the original technique is able to retain precision when a considerable part of the inverted file is removed we show that it is possible to improve precision in some scenario if some key design feature are properly selected 
we present blogscope www blogscope net a system for analyzing the blogosphere blogscope is an information discovery and text analysis system that offer a set of unique feature such feature include spatio temporal analysis of blog flexible navigation of the blogosphere through information burst keyword correlation and burst synopsis a well a enhanced ranking function for improved query answer relevance we describe the system it design and the feature of the current version of blogscope 
for digital library and information retrieval technology to provide solution for bridging the digital divide in developing country we need to understand the information access practice of remote and often poor community in these country we must understand the information need of these community and the best mean to provide them access to relevant information to this end we investigated the current information access practice in an indian village 
user in an organization frequently request help by sending request message to assistant that express information intent an intention to update data in an information system human assistant spend a significant amount of time and effort processing these request for example human resource assistant process request to update personnel record and executive assistant process request to schedule conference room or to make travel reservation to process the intent of a request assistant read the request and then locate complete and submit a form that corresponds to the expressed intent automatically or semi automatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work for a well understood domain a straightforward application of natural language processing technique can be used to build an intelligent form interface to semi automatically process information intent request message however high performance parser are based on machine learning algorithm that require a large corpus of example that have been labeled by an expert the generation of a labeled corpus of request is a major barrier to the construction of a parser in this paper we investigate the construction of a natural language processing system and an intelligent form system that observes an assistant processing request the intelligent form system then generates a labeled training corpus by interpreting the observation this paper report on the measurement of the performance of the machine learning algorithm based on real data the combination of observation machine learning and interaction design produce an effective intelligent form interface based on natural language processing 
there ha been recent interest in studying the goal behind a user s web query so that this goal can be used to improve the quality of a search engine s result previous study have mainly focused on using manual query log investigation to identify web query goal in this paper we study whether and how we can automate this goal identification process we first present our result from a human subject study that strongly indicate the feasibility of automatic query goal identification we then propose two type of feature for the goal identification task user click behavior and anchor link distribution our experimental evaluation show that by combining these feature we can correctly identify the goal for of the query studied 
the recent evolution of internet driven by the web service technology is extending the role of the web from a support of information interaction to a middleware for b b interaction indeed the web service technology allows enterprise to outsource part of their business process using web service and it also provides the opportunity to dynamically offer new value added service through the composition of pre existing web service in spite of the growing interest in web service current technology are found lacking efficient transactional support for composite web service cs in this paper we propose a transactional approach to ensure the failure atomicity of a c required by partner we use the accepted termination state at property a a mean to express the required failure atomicity partner specify their c mainly it control flow and the required at then we use a set of transactional rule to assist designer to compose a valid c with regard to the specified at 
the semantic web consists of many rdf graph nameable by uris this paper extends the syntax and semantics of rdf to cover such named graph this enables rdf statement that describe graph which is beneficial in many semantic web application area a a case study we explore the application area of semantic web publishing named graph allow publisher to communicate assertional intent and to sign their graph information consumer can evaluate specific graph using task specific trust policy and act on information from those named graph that they accept graph are trusted depending on their content information about the graph and the task the user is performing the extension of rdf to named graph provides a formally defined framework to be a foundation for the semantic web trust layer 
there is a growing interest in estimating the effectiveness of search two approach are typically considered examining the search query and examining the retrieved document set in this paper we take the latter approach we use four measure to characterize the retrieved document set and estimate the quality of search these measure are i the clustering tendency a measured by the cox lewis statistic ii the sensitivity to document perturbation iii the sensitivity to query perturbation and iv the local intrinsic dimensionality we present experimental result for the task of ranking query according to the search effectiveness over the trec disc and dataset our ranking of query is compared with the ranking based on the average precision using the kendall t statistic the best individual estimator is the sensitivity to document perturbation and yield kendall t of when combined with the clustering tendency based on the cox lewis statistic and the query perturbation measure it result in kendall t of which to our knowledge is the highest correlation with the average precision reported to date 
tokenization is a fundamental preprocessing step in information retrieval system in which text is turned into index term this paper quantifies and compare the influence of various simple tokenization technique on document retrieval effectiveness in two domain biomedicine and news a expected biomedical retrieval is more sensitive to small change in the tokenization method the tokenization strategy can make the difference between a mediocre and well performing ir system especially in the biomedical domain 
information flow in a network where individual influence each other the diffusion rate capture how efficiently the information can diffuse among the user in the network we propose an information flow model that leverage diffusion rate for prediction identify where information should flow to and ranking identify who will most quickly receive the information for prediction we measure how likely information will propagate from a specific sender to a specific receiver during a certain time period accordingly a rate based recommendation algorithm is proposed that predicts who will most likely receive the information during a limited time period for ranking we estimate the expected time for information diffusion to reach a specific user in a network subsequently a diffusionrank algorithm is proposed that rank user based on how quickly information will flow to them experiment on two datasets demonstrate the effectiveness of the proposed algorithm to both improve the recommendation performance and rank user by the efficiency of information flow 
search engine advertising ha become a significant element of the web browsing experience choosing the right ad for the query and the order in which they are displayed greatly affect the probability that a user will see and click on each ad this ranking ha a strong impact on the revenue the search engine receives from the ad further showing the user an ad that they prefer to click on improves user satisfaction for these reason it is important to be able to accurately estimate the click through rate of ad in the system for ad that have been displayed repeatedly this is empirically measurable but for new ad other mean must be used we show that we can use feature of ad term and advertiser to learn a model that accurately predicts the click though rate for new ad we also show that using our model improves the convergence and performance of an advertising system a a result our model increase both revenue and user satisfaction 
we consider two of the most commonly cited measure of retrieval performance average precision and r precision it is well known that average precision and r precision are highly correlated and similarly robust measure of performance though the reason for this are not entirely clear in this paper we give a geometric argument which show that under a very reasonable set of assumption average precision and r precision both approximate the area under the precision recall curve thus explaining their high correlation we further demonstrate through the use of trec data that the similarity or difference between average precision and r precision is largely governed by the adherence to or violation of these reasonable assumption 
it is well understood that the key for successful semantic web application depends on the availability of machine understandable meta data we describe the information grid a practical approach to the semantic web and show a prototype implementation information grid resource span all the data in the organization and all the metadata required to make it meaningful the final goal is to let organization view their asset in a smooth continuum from the internet to the intranet with uniform semantically rich access 
in some ir application it is desirable to adopt a high precision search strategy to return a small set of document that are highly focused and relevant to the user s information need with these application in mind we investigate semantic search using the xml fragment query language on text corpus automatically pre processed to encode semantic information useful for retrieval we identify three xml fragment operation that can be applied to a query to conceptualize restrict or relate term in the query we demonstrate how these operation can be used to address four different query time semantic need to specify target information type to disambiguate keywords to specify search term context or to relate select term in the query we demonstrate the effectiveness of our semantic search technology through a series of experiment using the two application in which we embed this technology and show that it yield significant improvement in precision in the search result 
we propose a model for user purchase behavior in online store that provide recommendation service we model the purchase probability given recommendation for each user based on the maximum entropy principle using feature that deal with recommendation and user interest the proposed model enable u to measure the effect of recommendation on user purchase behavior and the effect can be used to evaluate recommender system we show the validity of our model using the log data of an online cartoon distribution service and measure the recommendation effect for evaluating the recommender system 
in this paper we apply method from educational testing to measure the reliability of an ir collection 
memory based collaborative filtering algorithm have been widely adopted in many popular recommender system although these approach all suffer from data sparsity and poor prediction quality problem usually the user item matrix is quite sparse which directly lead to inaccurate recommendation this paper focus the memory based collaborative filtering problem on two crucial factor similarity computation between user or item and missing data prediction algorithm first we use the enhanced pearson correlation coefficient pcc algorithm by adding one parameter which overcomes the potential decrease of accuracy when computing the similarity of user or item second we propose an effective missing data prediction algorithm in which information of both user and item is taken into account in this algorithm we set the similarity threshold for user and item respectively and the prediction algorithm will determine whether predicting the missing data or not we also address how to predict the missing data by employing a combination of user and item information finally empirical study on dataset movielens have shown that our newly proposed method outperforms other state of the art collaborative filtering algorithm and it is more robust against data sparsity 
increasingly manufacturing company are shifting their focus from selling product to providing service a a result when designing new product engineer must increasingly consider the life cycle cost in addition to any design requirement to identify possible area of concern designer are required to consult existing maintenance information from identical product however in a large engineering company the amount of information available is significant and in wide range of format this paper present a prototype knowledge desktop suitable for the design engineer the engineering knowledge desktop analysis and suggests relevant information from ontologically marked up heterogeneous web resource it is designed using a service oriented architecture with an ontology to mediate between web service it ha been delivered to the user community for evaluation 
we describe an approach for extracting semantics of tag unstructured text label assigned to resource on the web based on each tag s usage pattern in particular we focus on the problem of extracting place and event semantics for tag that are assigned to photo on flickr a popular photo sharing website that support time and location latitude longitude metadata we analyze two method inspired by well known burst analysis technique and one novel method scale structure identification we evaluate the method on a subset of flickr data and show that our scale structure identification method outperforms the existing technique the approach and method described in this work can be used in other domain such a geo annotated web page where text term can be extracted and associated with usage pattern 
while much of the data on the web is unstructured in nature there is also a significant amount of embedded structured data such a product information on e commerce site or stock data on financial site a large amount of research ha focused on the problem of generating wrapper i e software tool that allow easy and robust extraction of structured data from text and html source in many application such a comparison shopping data ha to be extracted from many different source making manual coding of a wrapper for each source impractical on the other hand fully automatic approach are often not reliable enough resulting in low quality of the extracted data we describe a complete system for semi automatic wrapper generation that can be trained on different data source in a simple interactive manner our goal is to minimize the amount of user effort for training reliable wrapper through design of a suitable training interface that is implemented based on a powerful underlying extraction language and a set of training and ranking algorithm our experiment show that our system achieves reliable extraction with a very small amount of user effort 
we present an extensible java based platform for contextual retrieval based on the probabilistic information retrieval model module for dual index relevance feedback with blind or machine learning approach and query expansion with context are integrated into the okapi system to deal with the contextual information this platform allows easy extension to include other type of contextual information 
document clustering method mostly reply on single word term analysis based on vector space model to achieve more accurate document clustering more informative feature including phrase and their weight are also considered in current relevant research work in this seminar we present a new phrase based similarity measure to compute the pairwise similarity of text document based on suffix tree document model by applying the new suffix tree similarity measure in group average hierarchical agglomerative clustering ghac algorithm we developed a new suffix tree document clustering algorithm nstc experimental result on two standard document clustering benchmark corpus ohsumed and rcv indicate that the new clustering algorithm is a very effective document clustering algorithm comparing with the result of traditional word term tf idf similarity measure in the same ghac algorithm nstc achieved an improvement of on the average of f measure score 
we show that incorporating user behavior data can significantly improve ordering of top result in real web search setting we examine alternative for incorporating feedback into the ranking process and explore the contribution of user feedback compared to other common web search feature we report result of a large scale evaluation over query and million user interaction with a popular web search engine we show that incorporating implicit feedback can augment other feature improving the accuracy of a competitive web search ranking algorithm by a much a relative to the original performance 
the mechanism for personalisation used in web application are currently the subject of much debate amongst researcher from many diverse subject area one of the most contemporary idea for user modelling in web application is that of cognitive style where a user s psychological preference are assessed stored in a database and then used to provide personalised content and or link we describe user trial of a case study that utilises visual verbal preference in an adaptive web based educational system awbes student in this trial were assessed by the felder solomon inventory of learning style il instrument and their preference were used a a mean of content personalisation contrary to previous finding by other researcher we found no significant difference in performance between matched and mismatched student conclusion are drawn about the value and validity of using cognitive style a a way of modelling user preference in educational web application 
many search on the web have a transactional intent we argue that page satisfying transactional need can be distinguished from the more common page that have some information and link but cannot be used to execute a transaction based on this hypothesis we provide a recipe for constructing a transaction annotator by constructing an annotator with one corpus and then demonstrating it classification performance on another we establish it robustness finally we show experimentally that a search procedure that exploit such pre annotation greatly outperforms traditional search for retrieving transactional page 
effective ranking function are an essential part of commercial search engine we focus on developing a regression framework for learning ranking function for improving relevance of search engine serving diverse stream of user query we explore supervised learning methodology from machine learning and we distinguish two type of relevance judgment used a the training data absolute relevance judgment arising from explicit labeling of search result and relative relevance judgment extracted from user click throughs of search result or converted from the absolute relevance judgment we propose a novel optimization framework emphasizing the use of relative relevance judgment the main contribution is the development of an algorithm based on regression that can be applied to objective function involving preference data i e data indicating that a document is more relevant than another with respect to a query experimental result are carried out using data set obtained from a commercial search engine our result show significant improvement of our proposed method over some existing method 
the world wide web www is rapidly becoming important for society a a medium for sharing data information and service and there is a growing interest in tool for understanding collective behavior and emerging phenomenon in the www in this paper we focus on the problem of searching and classifying community in the web loosely speaking a community is a group of page related to a common interest more formally community have been associated in the computer science literature with the existence of a locally dense sub graph of the web graph where web page are node and hyper link are arc of the web graph the core of our contribution is a new scalable algorithm for finding relatively dense subgraphs in massive graph we apply our algorithm on web graph built on three publicly available large crawl of the web with raw size up to m node and g arc the effectiveness of our algorithm in finding dense subgraphs is demonstrated experimentally by embedding artificial community in the web graph and counting how many of these are blindly found effectiveness increase with the size and density of the community it is close to for community of a thirty node or more even at low density it is still about even for community of twenty node with density over of the arc present at the lower extreme the algorithm catch of dense community made of ten node we complete our community watch system by clustering the community found in the web graph into homogeneous group by topic and labelling each group by representative keywords 
personal information management pim is a rapidly growing area of research concerned with how people store manage and refind information a feature of pim research is that many system have been designed to assist user manage and refind information but very few have been evaluated this ha been noted by several scholar and explained by the difficulty involved in performing pim evaluation the difficulty include that people re find information from within unique personal collection researcher know little about the task that cause people to re find information and numerous privacy issue concerning personal information in this paper we aim to facilitate pim evaluation by addressing each of these difficulty in the first part we present a diary study of information re finding task the study examines the kind of task that require user to refind information and produce a taxonomy of refinding task for email message and web page in the second part we propose a task based evaluation methodology based on our finding and examine the feasibility of the approach using two different method of task creation 
we show that the empirical distribution of the pagerank value in a large set of web page doe not follow a power law except for some particular choice of the damping factor we argue that for a graph with an in degree distribution following a power law with exponent between and choosing a damping factor around for pagerank yield a power law distribution of it value we suggest that power law distribution of pagerank in web graph have been observed because the typical damping factor used in practice is between and 
classical query expansion technique such a the local context analysis lca make use of term co occurrence statistic to incorporate additional contextual term for enhancing passage retrieval however relevant contextual term do not always co occur frequently with the query term and vice versa hence the use of such method often brings in noise which lead to reduced precision previous study have demonstrated the importance of relationship analysis for natural language query in passage retrieval however they found that without query expansion the performance is not satisfactory for short query in this paper we present two novel query expansion technique that make use of dependency relation analysis to extract contextual term and relation from external corpus the technique are used to enhance the performance of density based and relation based passage retrieval framework respectively we compare the performance of the resulting system with lca in a density based passage retrieval system db and a relation based system without any query expansion rb using the factoid question from the trec qa task the result show that in term of mrr score our relation based term expansion method with db outperforms the lca by while our relation expansion method outperforms rb by 
the paper describes a maximum entropy based story segmentation system for arabic chinese and english in experiment with broadcast news data from tdt tdt and corpus collected in the darpa gale project we obtain a substantial performance gain using multiple overlapping window for text based feature 
user in search of on line document source are usually looking for content not word hence ir researcher generally agree that search technique should be geared toward the meaning underlying document rather than toward the text itself the most visible example of such technique are latent semantic analysis lsa and the hyperspace analog to language hal if these technique really uncover semantic dependency then they should be applicable across language we investigated this using electronic version of three kind of translated material a novel a popular treatise about cosmology and a data base of technical specification we used the analogy of fingerprinting used in forensics to establish if individual are related genetic fingerprinting us enzyme to split the dna and then compare the resulting band pattern likewise in our research we use query to split a document into fragment if a search technique really isolates fragment related to the query then a document and it translation should have similar band pattern in this paper we present the fingerprinting technique introduce the material used and report preliminary result of an evaluation for two semantic indexing technique 
this paper present experiment using an algorithm of web page topic segmentation that show significant precision improvement in the retrieval of document issued from the web track corpus of trec instead of processing the whole document a web page is segmented into different semantic block according to visual criterion such a horizontal line color and structural tag such a heading paragraph we conclude that combining visual and content layout criterion give the best result for increasing the precision the ranking of the page is calculated for relevant segment of page resulting from the segmentation algorithm 
web site are designed for graphical mode of interaction sighted user can cut to the chase and quickly identify relevant information in web page on the contrary individual with visual disability have to use screen reader tobrowse the web a screen reader process page sequentially and read through everything web browsing can become strenuous and time consuming although the use ofshortcuts and searching offer some improvement the problem still remains in this paper we address the problemof information overload in non visual web access using thenotion of context our prototype system csurf embodyingour approach provides the usual feature of a screen reader however when a user follows a link csurf capture thecontext of the link using a simple topic boundary detectiontechnique and us it to identify relevant information onthe next page with the help of a support vector machine astatistical machine learning model then csurf read the web page starting from the most relevant section identifiedby the model we conducted a series experiment to evaluate the performance of csurf against the state of the artscreen reader jaw our result show that the use of context can potentially save browsing time and substantiallyimprove browsing experience of visually disabled people 
in this paper we consider a way to represent contact center application a a set of multiple xml document written in different markup including voicexml and ccxml application can comprise a dialog with ivr call routing and agent scripting functionality we also consider way how such application can be executed in run time contact center environment 
measuring the information retrieval effectiveness of web search engine can be expensive if human relevance judgment are required to evaluate search result using implicit user feedback for search engine evaluation provides a cost and time effective manner of addressing this problem web search engine can use human evaluation of search result without the expense of human evaluator an additional advantage of this approach is the availability of real time data regarding system performance wecapture user relevance judgment action such a print save and bookmark sending these action and the corresponding document identifier to a central server via a client application we use this implicit feedback to calculate performance metric such a precision we can calculate an overall system performance metric based on a collection of weighted metric 
an approach is presented to automatically build a search engine for large scale music collection that can be queried through natural language while existing approach depend on explicit manual annotation and meta data assigned to the individual audio piece we automatically derive description by making use of method from web retrieval and music information retrieval based on the id tag of a collection of mp file we retrieve relevant web page via google query and use the content of these page to characterize the music piece and represent them by term vector by incorporating complementary information about acous tic similarity we are able to both reduce the dimensionality of the vector space and improve the performance of retrieval i e the quality of the result furthermore the usage of audio similarity allows u to also characterize audio piece when there is no associated information found on the web 
the bottleneck for dictionary based cross language information retrieval is the lack of comprehensive dictionary in particular for many different language we here introduce a methodology by which multilingual dictionary for spanish and swedish emerge automatically from simple seed lexicon these seed lexicon are automatically generated by cognate mapping from previously manually constructed portuguese and german a well a english source lexical and semantic hypothesis are then validated and new one iteratively generated by making use of co occurrence pattern of hypothesized translation synonym in parallel corpus we evaluate these newly derived dictionary on a large medical document collection within a cross language retrieval setting 
website privacy policy state the way that a site will use personal identifiable information pii that is collected from field and form in web based transaction since these policy can be complex machine readable version have been developed that allow automatic comparison of a site s privacy policy with a user s privacy preference however it is still difficult for user to determine the cause and origin of conformance conflict because current standard operate at the page level they can only say that there is a conflict on the page not where the conflict occurs or what cause it in this paper we describe fine grained policy anchor an extension to the way a website implement the platform for privacy preference p p that solves this problem fine grained policy anchor enable field level comparison of policy and preference field specific conformance display and faster access to additional conformance information we built a prototype user agent based on these extension and tested it with representative user we found that fine grained anchor do help user understand how privacy policy relates to their privacy preference and where and why conformance conflict occur 
we present gogetit a tool for generating structure driven crawler that requires a minimum effort from the user the tool take a input a sample page and an entry point to a web site and generates a structure driven crawler based on navigation pattern sequence of pattern for the link a crawler ha to follow to reach the page structurally similar to the sample page in the experiment we have performed structure driven crawler generated by gogetit were able to collect all page that match the sample given including those page added after their generation 
identifying and tracking new information on the web is important in sociology marketing and survey research since new trend might be apparent in the new information such change can be observed by crawling the web periodically in practice however it is impossible to crawl the entire expanding web repeatedly this mean that the novelty of a page remains unknown even if that page did not exist in previous snapshot in this paper we propose a novelty measure for estimating the certainty that a newly crawled page appeared between the previous and current crawl using this novelty measure new page can be extracted from a series of unstable snapshot for further analysis and mining to identify new trend on the web we evaluated the precision recall and miss rate of the novelty measure using our japanese web archive and applied it to a web archive search engine 
xml is fast becoming the standard format to store exchange and publish over the web and is getting embedded in application two challenge in handling xml are it size the xml representation of a document is significantly larger than it native state and the complexity of it search xml search involves path and content search on labeled tree structure we address the basic problem of compression navigation and searching of xml document in particular we adopt recently proposed theoretical algorithm for succinct tree representation to design and implement a compressed index for xml called xbzipindex in which the xml document is maintained in a highly compressed format and both navigation and searching can be done uncompressing only a tiny fraction of the data this solution relies on compressing and indexing two array derived from the xml data with detailed experiment we compare this with other compressed xml indexing and searching engine to show that xbzipindex ha compression ratio up to better than the one achievable by those other tool and it time performance on some path and content search operation is order of magnitude faster few millisecond over hundred of mb of xml file versus ten of second on standard xml data source 
generally speaking digital library have multiple granularity of semantic unit book chapter page paragraph and word however there are two limitation of current ebook retrieval system the granularity of retrievable unit is either too big or too small scale such a chapter paragraph are ignored the retrieval result should be grouped by facet to facilitate user s browsing and exploration to overcome these limitation we propose a multi granularity multi facet ebook retrieval approach 
the thesis presented in this paper tackle selected issue in unstructured peer to peer information retrieval p pir system using world knowledge for solving p pir problem a first part us so called reference corpus for estimating global term weight such a idf instead of sampling them from the distributed collection a second part of the work will be dedicated to the question of query routing in unstructured p pir system using peer resource description and world knowledge for query expansion 
we discus information retrieval method that aim at serving a diverse stream of user query we propose method that emphasize the importance of taking into consideration of query difference in learning effective retrieval function we formulate the problem a a multi task learning problem using a risk minimization framework in particular we show how to calibrate the empirical risk to incorporate query difference in term of introducing nuisance parameter in the statistical model and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameter we illustrate the effectiveness of the proposed method using modeling data extracted from a commercial search engine 
personalized pagerank express link based page quality around user selected page the only previous personalized pagerank algorithm that can serve on line query for an unrestricted choice of page on large graph is our monte carlo algorithm waw in this paper we achieve unrestricted personalization by combining rounding and randomized sketching technique in the dynamic programming algorithm of jeh and widom www we evaluate the precision of approximation experimentally on large scale real world data and find significant improvement over previous result a a key theoretical contribution we show that our algorithm use an optimal amount of space by also improving earlier asymptotic worst case lower bound our lower bound and algorithm apply to the simrank a well of independent interest is the reduction of the simrank computation to personalized pagerank 
domain ontology ha been used in many semantic web application however few application explore the use of ontology for personalized service this paper proposes an ontology based user model consisting of both concept and semantic relation to represent user interest specifically we adopt a statistical approach to learning a semantic based user ontology model from domain ontology and a spreading activation procedure for inferencing in the user ontology model we apply the method of learning and exploiting user ontology to a semantic search engine for finding academic publication our experimental result support the efficacy of user ontology and spreading activation theory sat for providing personalized semantic service 
this paper present two new approach of lexical chain for topic segmentation using weighted lexical chain wlc or weighted lexical link wll between repeated occurrence of lemma along the text the main advantage of using these new approach is the suppression of the empirical parameter called hiatus in lexical chain processing an evaluation according to the windowdiff measure on a large automatically built corpus show slight improvement in wll compared to state of the art method based on lexical chain 
to maintain interoperability in the web environment it is necessary to comply with web standard current specification of html and xhtml language define conformance condition both in specification prose and in a formalized way utilizing dtd unfortunately dtd is a very limited schema language and can not express many constraint that are specified in the free text part of the specification this mean that a page which validates against dtd is not necessarily conforming to the specification in this article we analyze feature of modern schema language that can improve validation of web page by covering more x html language constraint then dtd our schema use combination of relax ng and schematron to check not only the structure of the web page but also datatypes of attribute and element more complex relation between element and some wcag checkpoint a modular approach for schema composition is presented together with usage example including sample schema for various compound document e g xhtml combined with mathml and svg the second part of this article contains description of relaxed validator application we have developed relaxed is an extensible and powerful validation engine offering a convenient web interface a web service api java api and command line interface combined with our relax ng schematron schema relaxed offer very valuable validation result that surpass w c validator in many aspect 
we present a novel approach for efficient and self tuning query expansion that is embedded into a top k query processor with candidate pruning traditional query expansion method select expansion term whose thematic similarity to the original query term is above some specified threshold thus generating a disjunctive query with much higher dimensionality this pose three major problem the need for hand tuning the expansion threshold the potential topic dilution with overly aggressive expansion and the drastically increased execution cost of a high dimensional query the method developed in this paper address all three problem by dynamically and incrementally merging the inverted list for the potential expansion term with the list for the original query term a priority queue is used for maintaining result candidate the pruning of candidate is based on fagin s family of top k algorithm and optionally probabilistic estimator of candidate score can be used for additional pruning experiment on the trec collection for the robust and terabyte track demonstrate the increased efficiency effectiveness and scalability of our approach 
near duplicate web document are abundant two such document differ from each other in a very small portion that display advertisement for example such difference are irrelevant for web search so the quality of a web crawler increase if it can ass whether a newly crawled web page is a near duplicate of a previously crawled web page or not in the course of developing a near duplicate detection system for a multi billion page repository we make two research contribution first we demonstrate that charikar s fingerprinting technique is appropriate for this goal second we present an algorithmic technique for identifying existing f bit fingerprint that differ from a given fingerprint in at most k bit position for small k our technique is useful for both online query single fingerprint and all batch query multiple fingerprint experimental evaluation over real data confirms the practicality of our design 
the union of interactive digital tv idtv and web promotes the development of new interactive multimedia service enjoyable while watching tv even on the new handheld digital tv receiver yet several design constraint complicate the deployment of this new pattern of service indeed for a suitable presentation on a tv set web content must be structured in such a way that they can be effectively displayed on tv screen via low end set top box stbs moreover usable interface for idtv platform are needed which ensure a smooth access to content our claim is that the distribution of web content over the idtv broadcast channel may bring idtv to a new life a failure of this attempt may put idtv on a progressive track towards irrelevance we propose a system for the distribution of web content towards idtv under the digital video broadcasting multimedia home platform dvb mhp standard our system is able to automatically transcode web content and ensure a proper visualization on idtv the system is endowed with a client application which permit to easily browse content on the tv via a remote control real assessment have confirmed the effectiveness for such an automatic online service able to reconfigure web content for an appropriate distribution and presentation on idtv 
one key to cross language information retrieval is how to efficiently resolve the translation ambiguity of query given their short length this problem is even more challenging when only bilingual dictionary are available which is the focus of this paper in the previous research of cross language information retrieval using bilingual dictionary the word co occurrence statistic is used to determine the most likely translation of query in this paper we propose a novel statistical model named maximum coherence model which estimate the translation probability of query word that are consistent with the word co occurrence statistic unlike the previous work where a binary decision is made for the selection of translation the new model maintains the uncertainty in translating query word when their sense ambiguity is difficult to resolve furthermore this new model is able to estimate translation of multiple query word simultaneously this is in contrast to many previous approach where translation of individual query word are determined independently empirical study with trec datasets have shown that the maximum coherence model achieves a relative improvement in cross language information retrieval comparing to other approach that also use word co occurrence statistic for sense disambiguation 
the development of user interface uis is one of the most time consuming aspect in software development in this context the lack of proper reuse mechanism for uis is increasingly becoming manifest especially a software development is more and more moving toward composite application in this paper we propose a framework for the integration of stand alone module or application where integration occurs at the presentation layer hence the final goal is to reduce the effort required for ui development by maximizing reus the design of the framework is inspired by lesson learned from application integration appropriately modified to account for the specificity of the ui integration problem we provide an abstract component model to specify characteristic and behavior of presentation component and propose an event based composition model to specify the composition logic component and composition are described by mean of a simple xml based language which is interpreted by a runtime middleware for the execution of the resulting composite application a proof of concept prototype allows u to show that the proposed component model can also easily be applied to existing presentation component built with different language and or component technology 
this paper study the problem of extracting data from a web page that contains several structured data record the objective is to segment these data record extract data item field from them and put the data in a database table this problem ha been studied by several researcher however existing method still have some serious limitation the first class of method is based on machine learning which requires human labeling of many example from each web site that one is interested in extracting data from the process is time consuming due to the large number of site and page on the web the second class of algorithm is based on automatic pattern discovery these method are either inaccurate or make many assumption this paper proposes a new method to perform the task automatically it consists of two step identifying individual data record in a page and aligning and extracting data item from the identified data record for step we propose a method based on visual information to segment data record which is more accurate than existing method for step we propose a novel partial alignment technique based on tree matching partial alignment mean that we align only those data field in a pair of data record that can be aligned or matched with certainty and make no commitment on the rest of the data field this approach enables very accurate alignment of multiple data record experimental result using a large number of web page from diverse domain show that the proposed two step technique is able to segment data record align and extract data from them very accurately 
the contribution of this paper includes three folder to introduce a topic oriented query expansion model based on the information bottleneck theory that classify term into distinct topical cluster in order to find out candidate term for the query expansion to define a term term similarity matrix that is available to improve the term ambiguous problem to propose two measure intracluster and intercluster similarity that are based on proximity between the topic represented by two cluster in order to evaluate the retrieval effectiveness result of several evaluation experiment in web search exhibit the average intracluster similarity wa improved for the gain of while the average intercluster similarity wa decreased for the loss of 
syndication system on the web have attracted vast amount of attention in recent year a technology have emerged and matured there ha been a transition to more expressive syndication approach that is subscriber and publisher are provided with more expressive mean of describing their interest and published content enabling more accurate information filtering in this paper we formalize a syndication architecture that utilizes expressive web ontology and logic based reasoning for selective content dissemination this provides finer grained control for filtering and automated reasoning for discovering implicit subscription match both of which are not achievable in le expressive approach we then address one of the main limitation with such a syndication approach namely matching newly published information with subscription request in an efficient and practical manner to this end we investigate continuous query answering for a large subset of the web ontology language owl specifically we formally define continuous query for owl knowledge base and present a novel algorithm for continuous query answering in a large subset of this language lastly an evaluation of the query approach is shown demonstrating it effectiveness for syndication purpose 
we propose an integration of term proximity scoring into okapi bm the relative retrieval effectiveness of our retrieval method compared to pure bm varies from collection to collection we present an experimental evaluation of our method and show that the gain achieved over bm a the size of the underlying text collection increase we also show that for stemmed query the impact of term proximity scoring is larger than for unstemmed query 
we introduce and evaluate a middleware clustering technology capable of allocating resource to web application through dynamic application instance placement we define application instance placement a the problem of placing application instance on a given set of server machine to adjust the amount of resource available to application in response to varying resource demand of application cluster the objective is to maximize the amount of demand that may be satisfied using a configured placement to limit the disturbance to the system caused by starting and stopping application instance the placement algorithm attempt to minimize the number of placement change it also strives to keep resource utilization balanced across all server machine two type of resource are managed one load dependent and one load independent when putting the chosen placement in effect our controller schedule placement change in a manner that limit the disruption to the system 
due to it high popularity weblogs or blog in short present a wealth of information that can be very helpful in assessing the general public s sentiment and opinion in this paper we study the problem of mining sentiment information from blog and investigate way to use such information for predicting product sale performance based on an analysis of the complex nature of sentiment we propose sentiment plsa s plsa in which a blog entry is viewed a a document generated by a number of hidden sentiment factor training an s plsa model on the blog data enables u to obtain a succinct summary of the sentiment information embedded in the blog we then present arsa an autoregressive sentiment aware model to utilize the sentiment information captured by s plsa for predicting product sale performance extensive experiment were conducted on a movie data set we compare arsa with alternative model that do not take into account the sentiment information a well a a model with a different feature selection method experiment confirm the effectiveness and superiority of the proposed approach 
existing method for measuring the quality of search algorithm use a static collection of document a set of query and a mapping from the query to the relevant document allow the experimenter to see how well different search engine or engine configuration retrieve the correct answer this methodology assumes that the document set and thus the set of relevant document are unchanging in this paper we abandon the static collection requirement we begin with a recent trec collection created from a web crawl and analyze how the document in that collection have changed over time we determine how decay of the document collection affect trec system and present the result of an experiment using the decayed collection to measure a live web search system we employ novel measure of search effectiveness that are robust despite incomplete relevance information lastly we propose a methodology of collection maintenance which support measuring search performance both for a single system and between system run at different point in time 
this paper present a study of three statistical query translation model that use different unit of translation we begin with a review of a word based translation model that us co occurrence statistic for resolving translation ambiguity the translation selection problem is then formulated under the framework of graphic model resorting to which the modeling assumption and limitation of the co occurrence model are discussed and the research of finding better translation unit is motivated then two other model that use larger linguistically motivated translation unit i e noun phrase and dependency triple are presented for each model the modeling and training method are described in detail all query translation model are evaluated using trec collection result show that larger translation unit lead to more specific model that usually achieve better translation and cross language information retrieval result 
in this paper we propose a decentralized collaborative filtering cf approach based on p p overlay network for the autonomous agent environment experiment show that our approach is more scalable than traditional centralized cf filtering system and alleviates the sparsity problem in distributed cf 
we present a model based on the maximum entropy method for analyzing various measure of retrieval performance such a average precision r precision and precision at cutoff our methodology treat the value of such a measure a a constraint on the distribution of relevant document in an unknown list and the maximum entropy distribution can be determined subject to these constraint for good measure of overall performance such a average precision the resulting maximum entropy distribution are highly correlated with actual distribution of relevant document in list a demonstrated through trec data for poor measure of overall performance the correlation is weaker a such the maximum entropy method can be used to quantify the overall quality of a retrieval measure furthermore for good measure of overall performance such a average precision we show that the corresponding maximum entropy distribution can be used to accurately infer precision recall curve and the value of other measure of performance and we demonstrate that the quality of these inference far exceeds that predicted by simple retrieval measure correlation a demonstrated through trec data 
the world wide web is a powerful platform for a wide range of information task dramatic advance in technology such a improved search capability and the ajax application model have enabled entirely new web based application and usage pattern making many task easier to perform than ever before however few tool have been developed to assist with sensemaking task complex research behavior in which user gather and comprehend information from many source to answer potentially vague non procedural question sensemaking task are common and include for example researching vacation destination or deciding how to invest this paper present the scratchpad an extension to the standard browser interface that is designed to capture organize and exploit the information discovered while performing a sensemaking task 
with the rapid growth of wireless technology and handheld device m commerce is becoming a promising research area personalization is especially important to the success of m commerce this paper proposes a novel collaborative filtering based framework for personalized service in m commerce the framework extends our previous work by using online analytical processing olap to represent the relation among user content and context information and adopting a multi dimensional collaborative filtering model to perform inference it provides a powerful and well founded mechanism to personalization for m commerce we implemented it in an existing m commerce platform and experimental result demonstrate it feasibility and correctness 
the music industry s business model is to produce star in order to do so musician producing music that fit into well defined cluster of factor explaining the demand of the majority of music consumer are disproportionately promoted this lead to a limitation of available diversity and therefore of a limitation of the end user s benefit from listening to music this paper analysis online music consumer s need and preference these factor are used in order to explain the demand for star and the impact of different online music service on promoting a more diverse music market 
during a lifecycle of a large scale web application web developer produce a wide variety of inter related web object following good web engineering practice developer often create them based on a web application development method which requires certain logical model for the development and maintenance process web development is dynamic thus those logical model a well a web artifact evolve over time however the task of managing their evolution is still very inefficient because design decision in model are not directly accessible in existing file based software configuration management repository key limitation of existing web version control tool include their inadequacy in representing semantics of design model and inability to manage the evolution of model based object and their logical connection to web document this paper present a framework that allows developer to manage version and configuration of model and to capture change to model to model relation among web object model based object web document and relation are directly represented and versioned in a structure oriented manner 
this paper present sring a structured non dht p p overlay that efficiently support exact and range query on multiple attribute value in sring all attribute value are interpreted a string formed by a base alphabet and are published in the lexicographical order two virtual ring are built n ring is built in a skip list way for range partition and query d ring is built in a small world way for the construction of n ring a leave and join based load balancing method is used to balance range overload in the network with heterogeneous node 
traditionally information extraction from web table ha focused on small more or le homogeneous corpus often based on assumption about the use of tag a multitude of different html implementation of web table make these approach difficult to scale in this paper we approach the problem of domain independent information extraction from web table by shifting our attention from the tree based representation of webpage to a variation of the two dimensional visual box model used by web browser to display the information on the screen the there by obtained topological and style information allows u to fill the gap created by missing domain specific knowledge about content and table template we believe that in a future step this approach can become the basis for a new way of large scale knowledge acquisition from the current visual web 
xml schema s abstract data model consists of component which are the structure that eventually define a schema a a whole xml schema s xml syntax on the other hand is not a direct representation of the schema component and it prof to be surprisingly hard to derive a schema s component from the xml syntax the schema component xml syntax scx is a representation which attempt to map schema component a faithfully a possible to xml structure scx serf a the starting point for application which need access to schema component and want to do so using standardized and widely available xml technology 
in this paper we address the task of automatically finding an expert within the organization known a the expert search problem we present the theoretically based probabilistic algorithm which model retrieved document a mixture of expert candidate language model experiment show that our approach outperforms existing theoretically sound solution 
design principle for xml schema that eliminate redundancy and avoid update anomaly have been studied recently several normal form generalizing those for relational database have been proposed all of them however are based on the assumption of anative xml storage while in practice most of xml data is stored inrelational database in this paper we study xml design and normalization for relational storage of xml document to be able to relate and compare xml and relational design we use an information theoretic framework that measure information content in relation and document with higher value corresponding to lower level of redundancy we show that most common relational storage scheme preserve the notion of being well designed i e anomaliesand redundancy free thus existing xml normal form guarantee well designed relational storagesas well we further show that if this perfect option is not achievable then a slight restriction on xml constraint guarantee a second best relational design according to possible value of the information theoretic measure we finally consider an edge based relational representation of xml document and show that while it ha similar information theoretic property with other relational representation it can behave significantly worse in term of enforcing integrity constraint 
this paper present a new perspective of the probability ranking principle prp by defining retrieval effectiveness in term of our novel expected rank measure of a set of document for a particular query this perspective is based on preserving decision preference and it imposes weaker condition on prp than the utility theoretic perspective of prp 
a new shot level video browsing method based on semantic visual feature e g car mountain and fire is proposed to facilitate content based retrieval the video s binary semantic feature vector is utilized to calculate the score of similarity between two shot keyframes the score is then used to browse the similar keyframes in term of semantic visual feature a pilot user study wa conducted to better understand user behavior in video retrieval context three video retrieval and browsing system are compared temporal neighbor semantic visual feature and fused browsing system the initial result indicated that the semantic visual feature browsing wa effective and efficient for visual centric task but not for non visual centric task 
early speech retrieval experiment focused on news broadcast for which adequate automatic speech recognition asr accuracy could be obtained like newspaper news broadcast are a manually selected and arranged set of story evaluation design reflected that using known story boundary a a basis for evaluation substantial advance in asr accuracy now make it possible to build search system for some type of spontaneous conversational speech but present evaluation design continue to rely on known topic boundary that are no longer well matched to the nature of the material we propose a new class of measure for speech retrieval based on manual annotation of point at which a user with specific topical interest would wish replay to begin 
finding contiguous sequential pattern csp is an important problem in web usage mining in this paper we propose a new data structure updown tree for csp mining an updown tree combine suffix tree and prefix tree for efficient storage of all the sequence that contain a given item the special structure of updown tree ensures efficient detection of csps experiment show that updown tree improves csp mining in term of both time and memory usage comparing to previous approach 
a the data and ontology layer of the semantic web stack have achieved a certain level of maturity in standard recommendation such a rdf and owl the current focus lie on two related aspect on the one hand the definition of a suitable query language for rdf sparql is close to recommendation status within the w c the establishment of the rule layer on top of the existing stack on the other hand mark the next step to be taken where language with their root in logic programming and deductive database are receiving considerable attention the purpose of this paper is threefold first we discus the formal semantics of sparqlextending recent result in several way second weprovide translation from sparql to datalog with negation a failure third we propose some useful and easy to implement extension of sparql based on this translation a it turn out the combination serf for direct implementation of sparql on top of existing rule engine a well a a basis for more general rule and query language on top of rdf 
a content based personalized recommendation system learns user specific profile from user feedback so that it can deliver information tailored to each individual user s interest a system serving million of user can learn a better user profile for a new user or a user with little feedback by borrowing information from other user through the use of a bayesian hierarchical model learning the model parameter to optimize the joint data likelihood from million of user is very computationally expensive the commonly used em algorithm converges very slowly due to the sparseness of the data in ir application this paper proposes a new fast learning technique to learn a large number of individual user profile the efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from netflix and movielens 
this paper is concerned with the problem of browsing social annotation today a lot of service e g del icio u filckr have been provided for helping user to manage and share their favorite url and photo based on social annotation due to the exponential increasing of the social annotation more and more user however are facing the problem how to effectively find desired resource from large annotation data existing method such a tag cloud and annotation matching work well only on small annotation set thus an effective approach for browsing large scale annotation set and the associated resource is in great demand by both ordinary user and service provider in this paper we propose a novel algorithm namely effective large scale annotation browser elsaber to browse large scale social annotation data elsaber help the user browse huge number of annotation in a semantic hierarchical and efficient way more specifically elsaber ha the following feature the semantic relation between annotation are explored for browsing of similar resource the hierarchical relation between annotation are constructed for browsing in a top down fashion the distribution of social annotation is studied for efficient browsing by incorporating the personal and time information elsaber can be further extended for personalized and time related browsing a prototype system is implemented and show promising result 
user modeling for information retrieval ha mostly been studied to improve the effectiveness of information access in centralized repository in this paper we explore user modeling in the context of full text federated search in peer to peer network our approach model a user s persistent long term interest based on past query and us the model to improve search efficiency for future query that represent interest similar to past query our approach also enables query representing a user s transient ad hoc interest to be automatically recognized so that search for these query can rely on a relatively large search radius to avoid sacrificing effectiveness for efficiency experimental result demonstrate that our approach can significantly improve the efficiency of full text federated search without degrading it accuracy furthermore the proposed approach doe not require a large amount of training data and is robust to a range of parameter value 
we present an adaptive distributed query sampling framework that is quality conscious for extracting high quality text database sample the framework divide the query based sampling process into an initial seed sampling phase and a quality aware iterative sampling phase in the second phase the sampling process is dynamically scheduled based on estimated database size and quality parameter derived during the previous sampling process the unique characteristic of our adaptive query based sampling framework is it self learning and self configuring ability based on the overall quality of all text database under consideration we introduce three quality conscious sampling scheme for estimating database quality and our initial result show that the proposed framework support higher quality document sampling than existing approach 
the debate within the web community over the optimal mean by which to organize information often pit formalized classification against distributed collaborative tagging system a number of question remain unanswered however regarding the nature of collaborative tagging system including whether coherent categorization scheme can emerge from unsupervised tagging by user this paper us data from the social bookmarking site delicio u to examine the dynamic of collaborative tagging system in particular we examine whether the distribution of the frequency of use of tag for popular site with a long history many tag and many user can be described by a power law distribution often characteristic of what are considered complex system we produce a generative model of collaborative tagging in order to understand the basic dynamic behind tagging including how a power law distribution of tag could arise we empirically examine the tagging history of site in order to determine how this distribution arises over time and to determine the pattern prior to a stable distribution lastly by focusing on the high frequency tag of a site where the distribution of tag is a stabilized power law we show how tag co occurrence network for a sample domain of tag can be used to analyze the meaning of particular tag given their relationship to other tag 
many text document on the web are not originally created but forwarded or copied from other source document the phenomenon of document forwarding or transmission between various web site is denoted a web information diffusion this paper focus on mining information diffusion process for specific topic on the web a novel system called lidpw is proposed to address this problem using matching learning technique the source site and source document of each document are identified and the diffusion process composed of a sequence of diffusion relationship is visually presented to user the effectiveness of lidpw is validated on a real data set a preliminary user study is performed and the result show that lidpw doe benefit user to monitor the information diffusion process of a specific topic and aid them to discover the diffusion start and diffusion center of the topic 
we define a method to estimate the random and systematic error resulting from incomplete relevance assessment mean average precision map computed over a large number of topic with a shallow assessment pool substantially outperforms for the same adjudication effort map computed over fewer topic with deeper pool and p k computed with pool of the same depth move to front pooling previously reported to yield substantially better rank correlation yield similar power and lower bias compared tofixed depth pooling 
service oriented computing is emerging a the main approach to build distributed enterprise application on the web the widespread use of web service is hindered by the lack of adequate security and privacy support in this paper we present a novel framework for enforcing access control in conversation based web service our approach take into account the conversational nature of web service this is in contrast with existing approach to access control enforcement that assume a web service a a set of independent operation furthermore our approach achieves a tradeoff between the need to protect web service s access control policy and the need to disclose to client the portion of access control policy related to the conversation they are interested in this is important to avoid situation where the client cannot progress in the conversation due to the lack of required security requirement we introduce the concept of k trustworthiness that defines the conversation for which a client can provide credential maximizing the likelihood that it will eventually hit a final state 
information on the web is not only abundant but also redundant this redundancy of information ha an important consequence on the relation between the recall of an information gathering system and it capacity to harvest the core information of a certain domain of knowledge this paper provides a new idea for estimating the necessary web coverage of a knowledge acquisition system in order to achieve a certain desired coverage of the contained core information 
an important issue arising from peer to peer application is how to accurately and efficiently retrieve a set of k best matching data object from different source while minimizing the number of object that have to be accessed this paper resolve this issue by organizing peer in a semantic link network overlay where semantic link are established to denote the semantic relationship between peer data schema a query request will be routed to appropriate peer according to the semantic link type and a lower bound of rank function optimization strategy are proposed to reduce the total amount of data transmitted 
we describe a browser for the past web it can retrieve data from multiple past web resource and feature a passive browsing style based on change detection and presentation the browser show past page one by one along a time line the part that were changed between consecutive page version are animated to reflect their deletion or insertion thereby drawing the user s attention to them the browser enables automatic skipping of changeless period and filtered browsing based on user specified query 
this paper present a method for finding a specification page on the web for a given object e g titanic and it class label e g film a specification page for an object is a web page which give concise attribute value information about the object e g director james cameron for titanic a simple unsupervised method using layout and symbolic decoration cue wa applied to a large number of web page to acquire the class attribute we used these acquired attribute to select a representative specification page for a given object from the web page retrieved by a normal search engine experimental result revealed that our method greatly outperformed the normal search engine in term of specification retrieval 
a speechweb is a collection of hyperlinked application which are accessed remotely by speech browser running on end user device link are activated through spoken command despite the fact that protocol and technology for creating and deploying speech application have been readily available for several year we have not seen the development of a public domain speechweb in this paper we show how freely available software and commonly used communication protocol can be used to change this situation 
text clustering is most commonly treated a a fully automated task without user feedback however a variety of researcher have explored mixed initiative clustering method which allow a user to interact with and advise the clustering algorithm this mixed initiative approach is especially attractive for text clustering task where the user is trying to organize a corpus of document into cluster for some particular purpose e g clustering their email into folder that reflect various activity in which they are involved this paper introduces a new approach to mixed initiative clustering that handle several natural type of user feedback we first introduce a new probabilistic generative model for text clustering the speclustering model and show that it outperforms the commonly used mixture of multinomial clustering model even when used in fully autonomous mode with no user input we then describe how to incorporate four distinct type of user feedback into the clustering algorithm and provide experimental evidence showing substantial improvement in text clustering when this user feedback is incorporated 
user browsing the internet seem relatively satisfied with the performance of search engine an optimistic explanation would be the high quality of search engine a more pessimistic one would be that people just adapt easily to any new technology a third explanation is people s ignorance about recall a they simply don t know what relevant document are missed they can hardly be expected to worry about them and so they easily conceive the result a the best they can get to allow the user to better ass the quality of the search result an algorithm wa developed that computes a visual representation of the document space in the neighborhood of the user s query the paper outline the algorithm show how user can explore the neighborhood of a query and demonstrates how user can guess more judiciously whether they need to further elaborate their query to improve retrieval result 
electronic democracy should provide information and service for the citizen on the internet allowing room for debate participation and electronic voting the language being adopted by mass communication mean especially reality show are efficient and encourage public participation in decision making this paper discus a citizen government interaction language intended to facilitate citizen participation in the government s decision an e democracy model for people participation through web based technology is conceived this model specifies the syntax of an democracy interaction language a demil such language incorporates characteristic of reality show format and it is the back end of a web interface project in the domain researched the study of case participative budget of brazil represents the language proposed 
web page are more than text and they contain much contextual and structural information e g the title the meta data the anchor text etc each of which can be seen a a data source or are presentation due to the different dimensionality and different representing form of these heterogeneous data source simply putting them together would not greatly enhance the classification performance we observe that via a kernel function different dimension and type of data source can be represented into acommon format of kernel matrix which can be seen a a generalized similarity measure between a pair of web page in this sense a kernel learning approach is employed to fuse these heterogeneous data source the experimental result on a collection of the odp database validate the advantage of the proposed method over traditional method based on any single data source and the uniformly weighted combination of them 
topical classification of web query ha drawn recent interest because of the promise it offer in improving retrieval effectiveness and efficiency however much of this promise depends on whether classification is performed before or after the query is used to retrieve document we examine two previously unaddressed issue in query classification pre versus post retrieval classification effectiveness and the effect of training explicitly from classified query versus bridging a classifier trained using a document taxonomy bridging classifier map the category of a document taxonomy onto those of a query classification problem to provide sufficient training data we find that training classifier explicitly from manually classified query outperforms the bridged classifier by in f score also a pre retrieval classifier using only the query term performs merely worse than the bridged classifier which requires snippet from retrieved document 
current search engine do not fully leverage semantically rich datasets or specialise in indexing just one domain specific dataset we present a search engine that us the rdf data model to enable interactive query answering over richly structured and interlinked data collected from many disparate source on the web 
in this paper we describe a semantic web application that detects conflict of interest coi relationship among potential reviewer and author of scientific paper this application discovers various semantic association between the reviewer and author in a populated ontology to determine a degree of conflict of interest this ontology wa created by integrating entity and relationship from two social network namely know from a foaf friend of a friend social network and co author from the underlying co authorship network of the dblp bibliography we describe our experience developing this application in the context of a class of semantic web application which have important research and engineering challenge in common in addition we present an evaluation of our approach for real life coi detection 
we present a usage consultation tool based on internet searching for language learner when a user enters a string of word for which he want to find usage the system sends this string a a query to a search engine and obtains search result about the string the usage are extracted by performing statistical analysis on snippet and then fed back to the user unlike existing tool this usage consultation tool is multi lingual so that usage can be obtained even in a language for which there are no well established analytical method our evaluation ha revealed that usage can be obtained more effectively than by only using a search engine directly also we have found that the resulting usage doe not depend on the search engine for a prominent usage when the amount of data downloaded from the search engine is increased 
online curriculum portal aim to support network of instructor and learner by providing a space of convergence for enhancing peer to peer learning interaction among individual of an educational institution to this end effective open and scalable e learning system are required to acquire store and share knowledge under the form of learning object lo in this paper we are interested in exploiting the semantic relationship that characterize these los e g prerequisite part of or see also in order to capture and access individual and group knowledge in conjunction with the learning process supported by educational institution to achieve this functionality semantic web e g rdf s and declarative query language e g rql are employed to represent los and their relationship e g lom a well a to support navigation at the conceptual e learning portal space in this way different los could be presented to the same learner according to the traversed schema navigation path i e learning path using the apache jetspeed framework we are able to generate and assemble at run time portlets i e pluggable web component for visualizing personalized view a dynamic web page last but not least both learner and instructor can employ the same portal gui for updating semantically described los and thus support an open ended continuum of learning to the best of our knowledge the work presented in this paper is the first online curriculum portal platform supporting the aforementioned functionality 
electronic mail pose a number of unusual challenge for the design of information retrieval system and test collection including informal expression conversational structure variable document granularity e g message thread or longer term interaction a naturally occuring integration between free text and structural metadata and incompletely characterized user need this paper report on initial experiment with a large collection of public mailing list from the world wide web consortium that will be used for the trec enterprise search track automatic subject line threading and removal of duplicated text were found to have little effect in a small pilot study those observation motivated development of a question typology and more detailed analysis of collection characteristic preliminary result for both are reported 
this paper report a cross benchmark evaluation of regularized logistic regression lr and incremental rocchio for adaptive filtering using four corpus from the topic detection and tracking tdt forum and the text retrieval conference trec we evaluated these method with non stationary topic at various granularity level and measured performance with different utility setting we found that lr performs strongly and robustly in optimizing t su a trec utility function while rocchio is better for optimizing ctrk the tdt tracking cost a high recall oriented objective function using systematic cross corpus parameter optimization with both method we obtained the best result ever reported on tdt trec and trec relevance feedback on a small portion of the tdt test document yielded significant performance improvement measuring up to a reduction in ctrk and a increase in t su with b compared to the result of the top performing system in tdt without relevance feedback information 
secondary teacher across the country are being asked to use formative assessment data to inform their classroom instruction at the same time critic of no child left behind are calling the bill no child left untested emphasizing the negative side of assessment in that every hour spent assessing student is an hour lost from instruction or doe it have to be what if we better integrated assessment into the classroom and we allowed student to learn during the test maybe we could even provide tutoring on the step of solving problem our hypothesis is that we can achieve more accurate assessment by not only using data on whether student get test item right or wrong but by also using data on the effort required for student to learn how to solve a test item we provide evidence for this hypothesis using data collected with our e assistment system by more than student over the course of the school year we also show that we can track student knowledge over time using modern longitudinal data analysis technique in a separate paper we report on the assistment system s architecture and scalability while this paper is focused on how we can reliably ass student learning 
in this paper we use a unified relationship matrix urm to represent a set of heterogeneous data object e g web page query and their interrelationship e g hyperlink user click through sequence we claim that iterative computation over the urm can help overcome the data sparseness problem and detect latent relationship among heterogeneous data object thus can improve the quality of information application that require combination of information from heterogeneous source to support our claim we present a unified similarity calculating algorithm simfusion by iteratively computing over the urm simfusion can effectively integrate relationship from heterogeneous source when measuring the similarity of two data object experiment based on a web search engine query log and a web page collection demonstrate that simfusion can improve similarity measurement of web object over both traditional content based algorithm and the cutting edge simrank algorithm 
query ambiguity prevents existing retrieval system from returning reasonable result for every query a there is already lot of work done on resolving ambiguity vague query could be handled using corresponding approach separately if they can be identified in advance quantification of the degree of lack of ambiguity laysthe groundwork for the identification in this poster we propose such a measure using query topic based on the topic structure selected from the open directory project odp taxonomy we introduce clarity score to quantify the lack of ambiguity with respect to data set constructed from the trec collection and the rank correlation test result demonstrate a strong positive association between the clarity score and retrieval precision for query 
for enterprise search there exists a relationship between work task and document type that can be used to refine search result in this poster we adapt the popular okapi bm scoring function to weight term frequency based on the relevance of a document type to a work task also we use click frequency for each task type pair to estimate a realistic weight using the w c collection from the trec enterprise track for evaluation our approach lead to significant improvement on search precision 
automatically finding suitable web service given a request is a difficult problem because the interface description of web service are often terse and cryptic dictionary and information retrieval based technique have proven useful in disambiguating the semantics of service description but they are limited in their capability to consider the relationship between the word describing the web service current ontology based approach typically require a user to explicitly create domain ontology this paper present a novel technique that significantly improves the quality of semantic web service matching by automatically generating ontology based on web service description and using these ontology to guide the mapping between web service our approach differs from earlier work on service matching by considering the relationship between word rather than treating them a a bag of unrelated word the experimental result indicate that with our unsupervised approach we can eliminate up to of incorrect match that are made by dictionary based approach 
recent trend in the development of mobile device wireless communication sensor technology weblogs and peer to peer communication have prompted a new design opportunity for enhancing social interaction this paper introduces our preliminary experience in designing a prototype utilizing the aforementioned technology to share life experience user equipped with camera phone coupled with short range communication technology such a rfid can capture life experience and share it a weblogs to other people however in reality this is easier said than done the success of weblogs relies on the active participation and willingness of people to contribute to encourage active participation a ranking system agreerank is specifically developed to get them motivated 
compared with traditional association rule mining in the structured world e g relational database mining from xml data is confronted with more challenge due to the inherent flexibility of xml in both structure and semantics the major challenge include a more complicated hierarchical data structure an ordered data context and a much bigger size for each data element in order to make xml enabled association rule mining truly practical and computationally tractable we propose a practical model for mining association rule from xml document and demonstrate the usability and effectiveness of model through a set of experiment on real life data 
although web application are gaining popularity on mobile wireless pda web browser on these system can be quite slow and often lack adequate functionality to access many web site we have developed pthinc a pda thin client solution that leverage more powerful server to run full function web browser and other application logic then sends simple screen update to the pda for display pthinc us server side screen scaling to provide high fidelity display and seamless mobility across a broad range of different client and screen size including both portrait and landscape viewing mode pthinc also leverage existing pda control button to improve system usability and maximize available screen resolution for application display we have implemented pthinc on window mobile and evaluated it performance on mobile wireless device our result compared to local pda web browser and other thin client approach demonstrate that pthinc provides superior web browsing performance and is the only pda thin client that effectively support crucial browser helper application such a video playback 
data driven web application are usually structured in three tier with different programming model at each tier this division force developer to manually partition application functionality across the tier resulting in complex logic suboptimal partitioning and expensive re partitioning of application in this paper we introduce a unified platform for automatic partitioning of data driven web application our approach is based on hilda a high level declarative programming language with a unified data and programming model for all the layer of the application based on run time property of the application hilda s run time system automatically partition the application between the tier to improve response time while adhering to memory and or processing constraint at the client we evaluate our methodology with trace from a real application and with tpc w and our result show that automatic partitioning outperforms manual partitioning without the associated development overhead 
in this paper we extend the state of the art in utilizing background knowledge for supervised classification by exploiting the semantic relationship between term explicated in ontology preliminary evaluation indicate that the new approach generally improves precision and recall more so for hard to classify case and reveals pattern indicating the usefulness of such background knowledge 
whereas school typically record mound of data regarding student performance attendance and other behavior over the course of a school year rarely is that data consulted and used to inform day to day instructional practice in the classroom a teacher come under increasing pressure to ensure success for all of their student we are attempting to provide tool to help teacher make sense of what is happening in their classroom and take appropriate proactive and or remedial action one such tool is a web service we ve dubbed the classroom sentinel the classroom sentinel mine electronic gradebook and other student information system data source to detect critical teaching and learning pattern and bring those pattern to the attention of the teacher in the form of timely alert in this paper we introduce the notion of classroom pattern present some example and describe a framework for alert generation and delivery 
for the task of near duplicated document detection both traditional fingerprinting technique used in database community and bag of word comparison approach used in information retrieval community are not sufficiently accurate this is due to the fact that the characteristic of near duplicated document are different from that of both almost identical document in the data cleaning task and relevant document in the search task this paper present an instance level constrained clustering approach for near duplicate detection the framework incorporates information such a document attribute and content structure into the clustering process to form near duplicate cluster gathered from several collection of public comment sent to u s government agency on proposed new regulation the experimental result demonstrate that our approach outperforms other near duplicate detection algorithm and a about a effective a human assessor 
the need of formal verification is a problem that involves all the field in which sensible data are managed in this context the verification of data stream became a fundamental task the purpose of this paper is to present a framework based on the model checker spin for the verification of data stream the proposed method us a linear temporal logic called trio to describe data constraint and property constraint are automatically translated into promela the input language of the model checker spin in order to verify them 
current prediction technique which are generally designed for content based query and are typically evaluated on relatively homogenous test collection of small size face serious challenge in web search environment where collection are significantly more heterogeneous and different type of retrieval task exist in this paper we present three technique to address these challenge we focus on performance prediction for two type of query in web search environment content based and named page finding our evaluation is mainly performed on the gov collection in addition to evaluating our model for the two type of query separately we consider a more challenging and realistic situation that the two type of query are mixed together without prior information on query type to assist prediction under the mixed query situation a novel query classifier is adopted result show that our prediction of web query performance is substantially more accurate than the current state of the art prediction technique consequently our paper provides a practical approach to performance prediction in real world web setting 
several research effort a well a deployment have chosen ieee a a low cost long distance access technology to bridge the digital divide in this paper we consider the important issue of planning such network to the minimize system cost this is a non trivial task since it involves several set of variable the network topology tower height antenna type to be used and the irorientations and radio transmit power the task is further complicated due to the presence of network performance constraint and the inter dependence among the variable our first contribution in this paper is the formulation of this problem in term of the variable constraint and the optimization criterion our second contribution is in identifying the dependency among the variable and breaking down the problem into four tractable sub part in this process we extensively use domain knowledge to strike a balance between tractability and practicality we have evaluated the proposed algorithm using random input set a well a real life instance with success we have been able to show detailed planning of network topology required tower height antenna type and transmit power for the ashwini project a long distance wifi network under deployment in andhra pradesh india in this case we are able to achieve within additional cost of a lower bound estimate 
we are interested in retrieving information from speech data like broadcast news telephone conversation and roundtable meeting today most system use large vocabulary continuous speech recognition tool to produce word transcript the transcript are indexed and query term are retrieved from the index however query term that are not part of the recognizer s vocabulary cannot be retrieved and the recall of the search is affected in addition to the output word transcript advanced system provide also phonetic transcript against which query term can be matched phonetically such phonetic transcript suffer from lower accuracy and cannot be an alternative to word transcript we present a vocabulary independent system that can handle arbitrary query exploiting the information provided by having both word transcript and phonetic transcript a speech recognizer generates word confusion network and phonetic lattice the transcript are indexed for query processing and ranking purpose the value of the proposed method is demonstrated by the relative high performance ofour system which received the highest overall ranking for u english speech data in the recent nist spoken term detection evaluation 
the radio oranje demonstrator show an attractive multimedia user experience in the cultural heritage domain based on a collection of mono medium audio document it support online search and browsing of the collection using indexing technique specialized content visualization and a related photo database 
in xml database materializing query and their result into view in a semantic cache can improve the performance of query evaluation by reducing computational complexity and i o cost although there are a number of proposal of semantic cache for xml query the issue of fast cache lookup and compensation query construction could be further studied in this paper based on sequential xpath query we propose fastclu a fast cache lookup algorithm and efficq an efficient compensation query constructing algorithm to solve these two problem experimental result show that our algorithm outperform previous algorithm and can achieve good performance of query evaluation 
wide area distributed application are challenging to debug optimize and maintain we present wide area project wap which aim to make these task easier by exposing the causal structure of communication within an application and by exposing delay that imply bottleneck these bottleneck might not otherwise be obvious with or without the application s source code previous research project have presented algorithm to reconstruct application structure and the corresponding timing information from black box message trace of local area system in this paper we present a new algorithm for reconstructing application structure in both localand wide area distributed system an infrastructure for gathering application trace in planetlab and our experience tracing and analyzing three system codeen and coral two content distribution network in planetlab and slurpee an enterprise scale incident monitoring system 
this work try to answer the question of what make a query difficult it address a novel model that capture the main component of a topic and the relationship between those component and topic difficulty the three component of a topic are the textual expression describing the information need the query or query the set of document relevant to the topic the qrels and the entire collection of document we show experimentally that topic difficulty strongly depends on the distance between these component in the absence of knowledge about one of the model component the model is still useful by approximating the missing component based on the other component we demonstrate the applicability of the difficulty model for several us such a predicting query difficulty predicting the number of topic aspect expected to be covered by the search result and analyzing the findability of a specific domain 
we propose the study of visibly pushdown automaton vpa for processing xml document vpas are pushdown automaton where the input determines the stack operation and xml document are naturally visibly pushdown with the vpa pushing onto the stack on open tag and popping the stack on close tag in this paper we demonstrate the power and ease visibly pushdown automaton give in the design of streaming algorithm for xml document we study the problem of type checking streaming xml document against sdtd schema and the problem of typing tag in a streaming xml document according to an sdtd schema for the latter problem we consider both pre order typing and post order typing of a document which dynamically determines type at open tag and close tag respectively a soon a they are met we also generalize the problem of pre order and post order typing to prefix querying we show that a deterministic vpa yield an algorithm to the problem of answering in one pas the set of all answer to any query that ha the property that a node satisfying the query is determined solely by the prefix leading to the node all the streaming algorithm we develop in this paper are based on the construction of deterministic vpas and hence for any fixed problem the algorithm process each element of the input in constant time and use space d where d is the depth of the document 
we have developed a system that allows user to add annotation immediately onto a web page they are viewing and share the information via a network a novel feature of our method is that p p node in the system determine their role autonomously and share the annotation data our method is based on p p however p p node in the system change their role and data transfer procedure depending on their network topology or the status of other node our method is robust to node or network problem and ha flexible scalability 
we explore interactive method to further improve the performance of pseudo relevance feedback study citeria suggest that new method for tackling difficult query are required our approach is to gather more information about the query from the user by asking her simple question the equally simple response are used to modify the original query our experiment using the trec robust track query show that we can obtain a significant improvement in mean average precision averaging around over pseudo relevance feedback this improvement is also spread across more query compared to ordinary pseudo relevance feedback a suggested by geometric mean average precision 
semantic smoothing which incorporates synonym and sense information into the language model is effective and potentially significant to improve retrieval performance the implemented semantic smoothing model such a the translation model which statistically map document term to query term and a number of work that have followed have shown good experimental result however these model are unable to incorporate contextual information thus the resulting translation might be mixed and fairly general to overcome this limitation we propose a novel context sensitive semantic smoothing method that decomposes a document or a query into a set of weighted context sensitive topic signature and then translate those topic signature into query term in detail we solve this problem through choosing concept pair a topic signature and adopting an ontology based approach to extract concept pair estimating the translation model for each topic signature using the em algorithm and expanding document and query model based on topic signature translation the new smoothing method is evaluated on trec genomics track collection and significant improvement are obtained the map mean average precision achieves a maximal gain over the simple language model a well a a gain over the language model with context insensitive semantic smoothing 
in information retrieval ir the dirichlet prior have been applied to the smoothing technique of the language modeling approach in this paper we apply the dirichlet prior to the term frequency normalisation of the classical bm probabilistic model and the divergence from randomness pl model the contribution of this paper are twofold first through extensive experiment on four trec collection we show that the newly generated model to which the dirichlet prior normalisation is applied provide robust and effective performance second we propose a novel theoretically driven approach to the automatic parameter tuning of the dirichlet prior normalisation experiment show that this tuning approach optimises the retrieval performance of the newly generated dirichlet prior based weighting model 
we summarize the impact of the first five year of activity of the cross language evaluation forum clef on multilingual text retrieval system performance and show how the clef evaluation campaign have contributed to advance in the state of the art 
query suggestion aim to suggest relevant query for a given query which help user better specify their information need previously the suggested term are mostly in the same language of the input query in this paper we extend it to cross lingual query suggestion clqs for a query in one language we suggest similar or relevant query in other language this is very important to scenario of cross language information retrieval clir and cross lingual keyword bidding for search engine advertisement instead of relying on existing query translation technology for clqs we present an effective mean to map the input query of one language to query of the other language in the query log important monolingual and cross lingual information such a word translation relation and word co occurrence statistic etc are used to estimate the cross lingual query similarity with a discriminative model benchmark show that the resulting clqs system significantly out performs a baseline system based on dictionary based query translation besides the resulting clqs is tested with french to english clir task on trec collection the result demonstrate higher effectiveness than the traditional query translation method 
we address the problem of answering broad topic query on the world wide web we present a link based analysis algorithm selhits which is an improvement over kleinberg s hit algorithm we introduce the concept of virtual link to exploit the latent information in the hyperlinked environment we propose a novel approach to calculate hub and authority value we also present a selective expansion method which avoids topic drift and provides result consistent with only one interpretation of the query even if the query is ambiguous initial experimental evaluation and user feedback show that our algorithm indeed distills the most important and relevant page for broad topic query we also infer that there exists a uniform notion of quality of search result within user 
for the identification of plagiarized passage in large document collection we present retrieval strategy which rely on stochastic sampling and chunk index using the entire wikipedia corpus we compile n gram index and compare them to a new kind of fingerprint index in a plagiarism analysis use case our index provides an analysis speed up by factor and is an order of magnitude smaller while being equivalent in term of precision and recall 
a the competition of web search market increase there is a high demand for personalized web search to conduct retrieval incorporating web user information need this paper focus on utilizing clickthrough data to improve web search since million of search are conducted everyday a search engine accumulates a large volume of clickthrough data which record who submits query and which page he she click on the clickthrough data is highly sparse and contains different type of object user query and web page and the relationship among these object are also very complicated by performing analysis on these data we attempt to discover web user interest and the pattern that user locate information in this paper a novel approach cubesvd is proposed to improve web search the clickthrough data is represented by a order tensor on which we perform mode analysis using the higher order singular value decomposition technique to automatically capture the latent factor that govern the relation among these multi type object user query and web page a tensor reconstructed based on the cubesvd analysis reflects both the observed interaction among these object and the implicit association among them therefore web search activity can be carried out based on cubesvd analysis experimental evaluation using a real world data set collected from an msn search engine show that cubesvd achieves encouraging search result in comparison with some standard method 
previous study comparing the prediction accuracy of effort model built using web crossand single company data set have been inconclusive and a such replicated study are necessary to determine under what circumstance a company can place reliance on a cross company effort model this paper therefore replicates a previous study by investigating how successful a cross company effort model is i to estimate effort for web project that belong to a single company and were not used to build the cross company model ii compared to a single company effort model our single company data set had data on web project from a single company and our cross company data set had data on web project from different company the effort estimate used in our analysis were obtained by mean of two effort estimation technique namely forward stepwise regression and case based reasoning our result were similar to those from the replicated study showing that prediction based on the single company model were significantly more accurate than those based on the cross company model 
search engine result are usually presented in some form of text summary e g document title some snippet of the page s content a url etc based on the information contained within these summary user make relevance judgment about what link best suit their information need current research suggests that these relevance judgment are in the service of some search strategy in this paper we model two different search strategy the comparison and threshold strategy and determine how well they fit data gathered from an experiment on user search within a simulated google environment 
a the popularity of the web increase and web application become tool of everyday use the role of web security ha been gaining importance a well the last year have shown a significant increase in the number of web based attack for example there ha been extensive press coverage of recent security incidence involving the loss of sensitive credit card information belonging to million of customer many web application security vulnerability result from generic input validation problem example of such vulnerability are sql injection and cross site scripting x although the majority of web vulnerability are easy to understand and to avoid many web developer are unfortunately not security aware a a result there exist many web site on the internet that are vulnerable this paper demonstrates how easy it is for attacker to automatically discover and exploit application level vulnerability in a large number of web application to this end we developed secubat a generic and modular web vulnerability scanner that similar to a port scanner automatically analyzes web site with the aim of finding exploitable sql injection and x vulnerability using secubat we were able to find many potentially vulnerable web site to verify the accuracy of secubat we picked one hundred interesting web site from the potential victim list for further analysis and confirmed exploitable flaw in the identified web page among our victim were well known global company and a finance ministry of course we notified the administrator of vulnerable site about potential security problem more than fifty responded to request additional information or to report that the security hole wa closed 
in this paper we present a way to integrate web proxy with smart card based authentication system 
nowadays mobile user with global positioning device canaccess location based service lb and query about pointsof interest in their proximity for such application to succeed privacy and confidentiality are essential encryptionalone is not adequate although it safeguard the systemagainst eavesdropper the query themselves may disclosethe location and identity of the user recently there havebeen proposed centralized architecture based on k anonymity which utilize an intermediate anonymizer between themobile user and the lb however the anonymizer mustbe updated continuously with the current location of allusers moreover the complete knowledge of the entire systemposes a security threat if the anonymizer is compromised in this paper we address two issue i we show thatexisting approach may fail to provide spatial anonymityfor some distribution of user location and describe a noveltechnique which solves this problem ii we propose prive a decentralized architecture for preserving the anonymityof user issuing spatial query to lb mobile user self organizeinto an overlay network with good fault toleranceand load balancing property prive avoids the bottleneckcaused by centralized technique both in term of anonymizationand location update moreover the system state isdistributed in numerous user rendering prive resilient toattacks extensive experimental study suggest that priveis applicable to real life scenario with large population ofmobile user 
many variant of language model have been proposed for information retrieval most existing model are based on multinomial distribution and would score document based on query likelihood computed based on a query generation probabilistic model in this paper we propose and study a new family of query generation model based on poisson distribution we show that while in their simplest form the new family of model and the existing multinomial model are equivalent however based on different smoothing method the two family of model behave differently we show that the poisson model ha several advantage including naturally accommodating per term smoothing and modeling accurate background more efficiently we present several variant of the new model corresponding to different smoothing method and evaluate them on four representative trec test collection the result show that while their basic model perform comparably the poisson model can out perform multinomial model with per term smoothing the performance can be further improved with two stage smoothing 
in a dynamic service oriented environment it is desirable for service consumer and provider to offer and obtain guarantee regarding their capability and requirement w agreement defines a language and protocol for establishing agreement between two party the agreement are complex and expressive to the extent that the manual matching of these agreement would be expensive both in time and resource it is essential to develop a method for matching agreement automatically this work present the framework and implementation of an innovative tool for the matching provider and consumer based on w agreement the approach utilizes semantic web technology to achieve rich and accurate match a key feature is the novel and flexible approach for achieving user personalized match 
in this paper we describe how domain ontology are used in a dialogue system guiding the user to access web public administration content the current implementation of the system support speech through the telephone and text mode in different language english spanish catalan and italian 
bag of word retrieval is popular among question answering qa system developer but it doe not support constraint checking and ranking on the linguistic and semantic information of interest to the qa system we present anapproach to retrieval for qa applying structured retrieval technique to the type of text annotation that qa system use we demonstrate that the structured approach can retrieve more relevant result more highly ranked compared with bag of word on a sentence retrieval task we also characterize the extent to which structured retrieval effectiveness depends on the quality of the annotation 
service discovery is one of challenging issue in service oriented computing currently most of the existing service discovering and matching approach are based on keywords based strategy however this method is inefficient and time consuming in this paper we present a novel approach for discovering web service based on the current dominating mechanism of discovering and describing web service with uddi and wsdl the proposed approach utilizes probabilistic latent semantic analysis plsa to capture semantic concept hidden behind word in the query and advertisement in service so that service matching is expected to carry out at concept level we also present related algorithm and preliminary experiment to evaluate the effectiveness of our approach 
a part of the language observatory project we have been crawling all the web space since we have collected terabyte of data mostly from asian and african cctlds in this paper we present result of the current status of the african web and compare it with it status in and this paper focus on the accessibility of the web page the web tree growth web technology privacy protection and web interconnection 
in this paper we propose a new method to discover collection adapted ranking function based on genetic programming gp our combined component approach cca is based on the combination of several term weighting component i e term frequency collection frequency normalization extracted from well known ranking function in contrast to related work the gp terminal in our cca are not based on simple statistical information of a document collection but on meaningful effective and proven component experimental result show that our approach wa able to outper form standard tf idf bm and another gp based approach in two different collection cca obtained improvement in mean average precision up to for the trec collection and for the wbr collection a large brazilian web collection over the baseline function the cca evolution process also wa able to reduce the overtraining commonly found in machine learning method especially genetic programming and to converge faster than the other gp based approach used for comparison 
we propose a fully decentralized collaborative filtering approach that is self organizing and operates in a distributed way the relevance between downloading file item are stored locally at these item in so called item based buddy table and are updated each time that the item are downloaded we then propose to use the language model to build recommendation for the different user based on the buddy table of those item a user ha downloaded previously we have tested and compared our distributed collaborative filtering approach to centralized collaborative filtering and showed that it ha similar performance it is therefore a promising technique to facilitate recommendation in peer to peer network 
the role of network structure ha grown in significance over the past ten year in the field of information retrieval stimulated to a great extent by the importance of link analysis in the development of web search technique this body of work ha focused primarily on the network that is most clearly visible on the web the network of hyperlink connecting document to document but the web ha always contained a second network le explicit but equally important and this is the social network on it user with latent person to person link encoding a variety of relationship including friendship information exchange and influence development over the past few year including the emergence of social networking system and rich social medium a well a the availability of large scale e mail and instant messenging datasets have highlighted the crucial role played by on line social network and at the same time have made them much easier to uncover and analyze there is now a considerable opportunity to exploit the information content inherent in these network and this prospect raise a number of interesting research challenge within this context we focus on some recent effort to formalize the problem of searching a social network the goal is to capture the issue underlying a variety of related scenario a member of a social networking system such a myspace seek a piece of information that may be held by a friend of a friend an employee in a large company search his or her network of colleague for expertise in a particular subject a node in a decentralized peer to peer file sharing system query for a file that is likely to be a small number of hop away or a user in a distributed ir or federated search setting traverse a network of distributed resource connected by link that may not just be informational but also economic or contractual in their most basic form these scenario have some essential feature in common a node in a network without global knowledge must find a short path to a desired target node or to one of several possible target node to frame the underlying problem we go back to one of the most well known piece of empirical social network analysis stanley milgram s research into the small world phenomenon also known a the six degree of separation the form of milgram s experiment in which randomly chosen starter had to forward a letter to a designated target individual established not just that short chain connecting far flung pair of people are abundant in large social network but also that the individual in these network operating with purely local information about their own friend and acquaintance are able to actually find these chain the milgram experiment thus constituted perhaps the earliest indication that large scale social network are structured to support this type of decentralized search within a family of random graph model proposed by watt and strogatz we have shown that the ability of a network to support this type of decentralized search depends in subtle way on how it long range connection are correlated with the underlying spatial or organizational structure in which it is embedded recent study using data on communication within organization and the friendship within large on line community have established the striking fact that real social network closely match some of the structural feature predicted by these mathematical model if one look further at the on line setting that provide the initial motivation for these issue there is clearly interest from many direction in their long term economic implication essentially the consequence that follow from viewing distributed information retrieval application peer to peer system or social networking site a providing marketplace for information and service how doe the problem of decentralized search in a network change when the participant are not simply agent following a fixed algorithm but strategic actor who make decision in their own self interest and may demand compensation for taking part in a protocol such consideration bring u into the realm of algorithmic game theory an active area of current research that us game theoretic notion to quantify the performance of system in which the participant follow their own self interest in a simple model for decentralized search in the presence of incentive we find that performance depends crucially on both the rarity of the information and the richness of the network topology if the network is too structurally impoverished an enormous investment may be required to produce a path from a query to an answer 
this paper make an intensive investigation of the application of bayesian network in sentence retrieval and introduces three bayesian network based sentence retrieval model with or without consideration of term relationship term relationship in this paper are considered from two perspective relationship between pair of term and relationship between term and term set experiment have proven the efficiency of bayesian network in the application of sentence retrieval particularly retrieval result with consideration of the second kind of term relationship performs better in improving retrieval precision 
new event detection ned aim at detecting from one or multiple stream of news story that which one is reported on a new event i e not reported previously with the overwhelming volume of news available today there is an increasing need for a ned system which is able to detect new event more efficiently and accurately in this paper we propose a new ned model to speed up the ned task by using news indexing tree dynamically moreover based on the observation that term of different type have different effect for ned task two term reweighting approach are proposed to improve ned accuracy in the first approach we propose to adjust term weight dynamically based on previous story cluster and in the second approach we propose to employ statistic on training data to learn the named entity reweighting model for each class of story experimental result on two linguistic data consortium ldc datasets tdt and tdt show that the proposed model can improve both efficiency and accuracy of ned task significantly compared to the baseline system and other existing system 
abstract we created a proximity measure called proximity within paragraph pwp which is based on the concept of value assignment to queried word grouped by associated idea within paragraph based on the wt g dataset a test system comprising three test set and fifty query were constructed to evaluate the effectiveness of pwp by comparing it with the existing method minimum distance between queried pair a further experiment combine the score obtained from both method and the result suggest that the combination can significantly improve the effectiveness category and subject descriptor h information system information search and retrieval general term algorithm keywords proximity measure ranking algorithm 
we present a way of estimating term weight for information retrieval ir using term co occurrence a a measure of dependency between term we use the random walk graph based ranking algorithm on a graph that encodes term and co occurrence dependency in text from which we derive term weight that represent a quantification of how a term contributes to it context evaluation on two trec collection and topic show that the random walk based term weight perform at least comparably to the traditional tf idf term weighting while they outperform it when the distance between co occurring term is between and term 
this poster investigates the use of theoretical benchmark to describe the matching function of xml retrieval system and the property of specificity and exhaustivity in xml retrieval theoretical benchmark concern the formal representation of qualitative property of ir model to this end situation theory framework for the meta evaluation of xml retrieval is presented 
previous study have highlighted the high arrival rate of new content on the web we study the extent to which this new content can be eciently discovered by a crawler our study ha two part first we study the inherent diculty of the discovery problem using a maximum cover formulation under an assumption of perfect estimate of likely source of link to new content second we relax this assumption and study a more realistic setting in which algorithm must use historical statistic to estimate which page are most likely to yield link to new content we recommend a simple algorithm that performs comparably to all approach we consider we measure the overhead of discovering new content defined a the average number of fetch required to discover one new page we show first that with perfect foreknowledge of where to explore for link to new content it is possible to discover of all new content with under overhead and of new content with overhead but actual algorithm which do not have access to perfect foreknowledge face a more dicult task one quarter of new content is simply not amenable to ecient discovery of the remaining three quarter of new content during a given week may be discovered with overhead if content is recrawled fully on a monthly basis 
strong user involvement and clear business objective both relying on efficient communication between the developer and the business are key factor for a project s success domain specific language dsl being simple highly focused and tailored to a clear problem domain are a promising alternative to heavy weight modeling approach in the field of web engineering thus they enable stakeholder to validate modify and even develop part of a distributed web based solution 
over the last five year a range of project have focused on progressively more elaborated technique for adaptive news delivery however the adaptation process in these system ha become more complicated and thus le transparent to the user in this paper we concentrate on the application of open user model in adding transparency and controllability to adaptive news system we present a personalized news system yournews which allows user to view and edit their interest profile and report a user study on the system our result confirm that user prefer transparency and control in their system and generate more trust to such system however similar to previous study our study demonstrate that this ability to edit user profile may also harm the system s performance and ha to be used with caution 
abstract we address the problem of extracting semantics of tag short unstructured text label assigned to resource on the web based on each tag s metadata pattern in particular we describe an approach for extracting place and event semantics for tag that are assigned to photo on flickr a popular photo sharing website supporting time and location latitude longitude metadata the approach can be generalized to other domain where text term can be extracted and associated with metadata pattern such a geoannotated web page category and subject descriptor 
through a variety of mean including a range of browser cache method and inspecting the color of a visited hyperlink client side browser state can be exploited to track user against their wish this tracking is possible because persistent client side browser state is not properly partitioned on per site basis in current browser we address this problem by refining the general notion of a same origin policy and implementing two browser extension that enforce this policy on the browser cache and visited link we also analyze various degree of cooperation between site to track user and show that even if long term browser state is properly partitioned it is still possible for site to use modern web feature to bounce user between site and invisibly engage in cross domain tracking of their visitor cooperative privacy attack are an unavoidable consequence of all persistent browser state that affect the behavior of the browser and disabling or frequently expiring this state is the only way to achieve true privacy against colluding party 
online product review are one of the important opinion source on the web this paper study the problem of determining the semantic orientation positive or negative of opinion expressed on product feature in review most existing approach use a set of opinion word for the purpose however the semantic orientation of many word are context dependent in this paper we propose to use some linguistic rule to deal with the problem together with a new opinion aggregation function extensive experiment show that these rule and the function are highly effective a system called opinion observer ha also been built 
a major difference between corporate intranet and the internet is that in intranet the barrier for user to create web page is much higher this limit the amount and quality of anchor text one of the major factor used by internet search engine making intranet search more difficult the social phenomenon at play also mean that spam is relatively rare both on the internet and in intranet user are often willing to cooperate with the search engine in improving the search experience these characteristic naturally lead to considering using user feedback to improve search quality in intranet in this paper we show how a particular form of feedback namely user annotation can be used to improve the quality of intranet search an annotation is a short description of the content of a web page which can be considered a substitute for anchor text we propose two way to obtain user annotation using explicit and implicit feedback and show how they can be integrated into a search engine preliminary experiment on the ibm intranet demonstrate that using annotation improves the search quality 
the usual approach for automatic summarization is sentence extraction where key sentence from the input document are selected based on a suite of feature while word frequency often is used a a feature in summarization it impact on system performance ha not been isolated in this paper we study the contribution to summarization of three factor related to frequency content word frequency composition function for estimating sentence importance from word frequency and adjustment of frequency weight based on context we carry out our analysis using datasets from the document understanding conference studying not only the impact of these feature on automatic summarizers but also their role in human summarization our research show that a frequency based summarizer can achieve performance comparable to that of state of the art system but only with a good composition function context sensitivity improves performance and significantly reduces repetition 
in the allright project we are developing an algorithm for unsupervised table detection and segmentation that us the visual rendition of a web page rather than the html code our algorithm work bottom up by grouping word bounding box into larger group and us a set of heuristic it ha already been implemented and a preliminary evaluation on about web document ha been carried out 
suppose you are on a mobile device with no keyboard e g a cell phone and you want to perform a near me search where is the nearest pizza how do you enter query quickly t the wild thing encourages user to enter pattern with implicit and explicit wild card regular expression the search engine us microsoft local live log to find the most likely query for a particular location for example is short hand for the regular expression pqrs mno which match post office in many place but space needle in seattle some query are more local than others pizza is likely everywhere whereas boeing company is very likely in seattle and chicago moderately likely nearby and somewhat likely elsewhere smoothing is important not every query is observed everywhere 
clustering of search result is an important feature in many of today s information retrieval application the notion of hit list clustering appears in web search engine and enterprise search engine a a mechanism that allows user to further explore the coverage of a query however there ha been little work on exposing temporal attribute for constructing and presentation of cluster these attribute appear in document a part of the textual content e g a a date and time token or a a temporal reference in a sentence in this paper we outline a model and describe a prototype that show the main idea 
term relevance feedback ha had a long history in information retrieval however research on interactive term relevance feedback ha yielded mixed result in this paper we investigate several aspect related to the elicitation of term relevance feedback the display of document surrogate the technique for identifying or selecting term and source of expansion term we conduct a between subject experiment n of three term relevance feedback interface using the trec hard collection and evaluate each interface with respect to query length and retrieval performance result demonstrate that query created with each experimental interface significantly outperformed corresponding baseline query even though there were no difference in performance between interface condition result also demonstrate that pseudo relevance feedback run outperformed both baseline and experimental run a assessed by recall oriented measure but that user generated term improved precision 
in this paper we present a novel frequent generalized pattern mining algorithm called gp close for mining generalized association from rdf metadata to solve the over generalization problem encountered by existing method gp close employ the notion of emphgeneralization closure for systematic over generalization reduction 
in this paper we propose a novel approach for automatic music video summarization based on audio visual text analysis and alignment the music video is separated into the music and video track for the music track the chorus is detected based on music structure analysis for the video track we first segment the shot and classify the shot into close up face shot and non face shot then we extract the lyric and detect the most repeated lyric from the shot the music video summary is generated based on the alignment of boundary of the detected chorus shot class and the most repeated lyric from the music video the experiment on chorus detection shot classification and lyric detection using english music video are described subjective user study have been conducted to evaluate the quality and effectiveness of summary the comparison with the summary based on our previous method and the manual method indicate that the result of summarization using the proposed method are better at meeting user expectation 
in this paper we propose a novel dependency language modeling approach for information retrieval the approach extends the existing language modeling approach by relaxing the independence assumption our goal is to build a language model in which various word relationship can be integrated in this work we integrate two type of relationship extracted from wordnet and co occurrence relationship respectively the integrated model ha been tested on several trec collection the result show that our model achieves substantial and significant improvement with respect to the model without these relationship these result clearly show the benefit of integrating word relationship into language model for ir 
we describe a query driven indexing framework for scalable text retrieval over structured p p network to cope with the bandwidth consumption problem that ha been identified a the major obstacle for full text retrieval in p p network we truncate posting list associated with indexing feature to a constant size storing only top k ranked document reference to compensate for the loss of information caused by the truncation we extend the set of indexing feature with carefully chosen term set indexing term set are selected based on the query statistic extracted from query log thus we index only such combination that are a frequently present in user query and b non redundant w r t the rest of the index the distributed index is compact and efficient a it constantly evolves adapting to the current query popularity distribution moreover it is possible to control the tradeoff between the storage bandwidth requirement and the quality of query answering by tuning the indexing parameter our theoretical analysis and experimental result indicate that we can indeed achieve scalable p p text retrieval for very large document collection and deliver good retrieval performance 
the unification of semantic web query language under the sparql standard and the development of commercial quality implementation are encouraging industry to use semantic technology for managing information current implementation however lack the performance monitoring and management service that the industry expects in this paper we present a performance and management framework interface to a generic sparql web server we leverage existing standard for instrumentation to make the system ready to manage through existing monitoring application and we provide a performance framework which ha the distinct feature of providing measurement result through the same sparql interface used to query data eliminating the need for special interface 
web catalog integration is an interesting problem in current digital content management past study have shown that using a flattened structure with auxiliary information extracted from the source catalog can improve the integration result however the nature of a flattened structure ignores the hierarchical relationship and thus the performance improvement of catalog integration may be reduced in this paper we propose an enhanced hierarchical catalog integration ehci approach with conceptual thesaurus extracted from the source catalog the result show that our enhanced hierarchical integration approach effectively boost the accuracy of hierarchical catalog integration 
integration of web search with geographic information ha recently attracted much attention there are a number of local web search system enabling user to find location specific web content in this paper however we point out that this integration is still at a superficial level most local web search system today only link local web content to a map interface they are extension of a conventional stand alone geographic information system gi applied to a web based client server architecture in this paper we discus the direction available for tighter integration of web search with a gi in term of extraction knowledge discovery and presentation we also describe implementation to support our argument that the integration must go beyond the simple map and hyperlink architecture 
the difficulty of developing and deploying commercial web application increase a the number of technology they use increase and a the interaction between these technology become more complex this paper describes a way to avoid this increasing complexity by re examining the basic requirement of web application our approach is to first separate client concern from server concern and then to reduce the interaction between client and server to it most elemental parameter passing we define a simplified programming model for form based web application and we use xforms and a subset of j ee a enabling technology we describe our implementation of an mvc based application builder for this model which automatically generates the code needed to marshal input and output data between client and server this marshalling us type checking and other form of validation on both client and server we also show how our programming model and application builder support the customization of web application for different execution target including for example different client device 
in a social network node correspond topeople or other social entity and edge correspond to social link between them in an effort to preserve privacy the practice of anonymization replaces name with meaningless unique identifier we describe a family of attack such that even from a single anonymized copy of a social network it is possible for an adversary to learn whether edge exist or not between specific targeted pair of node 
broder et al s shingling algorithm and charikar s random projection based approach are considered state of the art algorithm for finding near duplicate web page both algorithm were either developed at or used by popular web search engine we compare the two algorithm on a very large scale namely on a set of b distinct web page the result show that neither of the algorithm work well for finding near duplicate pair on the same site while both achieve high precision for near duplicate pair on different site since charikar s algorithm find more near duplicate pair on different site it achieves a better precision overall namely versus for broder et al s algorithm we present a combined algorithm which achieves precision with of the recall of the other algorithm 
high throughput glycoproteomics similar to genomics and proteomics involves extremely large volume of distributed heterogeneous data a a basis for identification and quantification of a structurally diverse collection of biomolecules the ability to share compare query for and most critically correlate datasets using the native biological relationship are some of the challenge being faced by glycobiology researcher a a solution for these challenge we are building a semantic structure using a suite of ontology which support management of data and information at each step of the experimental lifecycle this framework will enable researcher to leverage the large scale of glycoproteomics data to their benefit in this paper we focus on the design of these biological ontology schema with an emphasis on relationship between biological concept on the use of novel approach to populate these complex ontology including integrating extremely large datasets mb a part of the instance base and on the evaluation of ontology using ontoqa metric the application of these ontology in providing informatics solution for high throughput glycoproteomics experimental domain is also discussed we present our experience a a use case of developing two ontology in one domain to be part of a set of use case which are used in the development of an emergent framework for building and deploying biological ontology 
it is now feasible to view medium at home a easily a text based page were viewed when the world wide web www first emerged this development ha supported medium sharing and search service providing hosting indexing and access to large online medium repository many of these sharing service also have a social aspect to them this paper provides an initial analysis of the social interaction on a video sharing and search service www youtube com result show that many user do not form social network in the online community and a very small number do not appear to contribute to the wider community however it doe seem those people who do use the available tool have much a greater tendency to form social connection 
those who want to conceal the content of their communication can do so by replacing word that might trigger attention for example instead of writing the bomb is in position a terrorist may chose to write the flower is in position the substituted sentence would sound a bit odd for a human reader and it ha been shown in prior research that such oddity is detectable by text mining approach however the importance of each component in the suggested oddity detection approach ha not been thoroughly investigated also the approach ha not been compared with such an obvious candidate for the task a hidden markov model hmm in this work we explore further oddity detection algorithm reported earlier specifically those based on pointwise mutual information pmi and hidden markov model hmm 
thanks to the ubiquity of the internet search engine search box user have come to depend on search engine both to find and re find information however re finding behavior ha not been significantly addressed here we look at re finding query issued to the yahoo search engine by user over a year 
we describe the architecture of the odesew semantic web application development platform which ha been used to generate the internal and external web site of several r d project 
we report the statistically significant mean impact of blind feedback a implemented by participant for the reliable information access ria workshop on retrieval measure including several primary recall measure not originally reported we find that blind feedback wa detrimental to measure focused on the first relevant item even when it boosted early precision measure such a mean precision implying that the conventional reporting of ad hoc precision need enhancement 
existing web browser handle security error in a manner that often confuses user in particular when a user visit a secure site whose certificate the browser cannot verify the browser typically allows the user to view and install the certificate and connect to the site despite the verification failure however few user understand the risk of man in the middle attack and the principle behind certificate based authentication we propose context sensitive certificate verification cscv whereby the browser interrogates the user about the context in which a certificate verification error occurs considering the context the browser then guide the user in handling and possibly overcoming the security error we also propose specific password warning spw when user are about to send password in a form vulnerable to eavesdropping we performed user study to evaluate cscv and spw our result suggest that cscv and spw can greatly improve web browsing security and are easy to use even without training moreover cscv had greater impact than did staged security training 
xml schema document are defined using an xml syntax which mean that the idea of generating schema documentation through standard xml technology is intriguing we present x doc a framework for generating schema documentation solely through xslt the framework us scx an xml syntax for xml schema component a intermediate format and produce xml based output format using a modular set of xslt stylesheets x doc is highly configurable and carefully crafted towards extensibility this prof especially useful for composite schema where additional schema information like schematron rule are embedded into xml schema 
from his twin perspective a a career long telecommunication engineer and chairman of one of the uk s largest electronics company sir david brown will reflect on whether and when the new economy seemingly so long coming will finally arrive he will begin by exploring how the prospect of everything being digital everyone having broadband and intelligence being everywhere is changing our understanding of mobility then he will comment on the economic effect of that changed understanding under three heading the macroeconomy microeconomy and socioeconomy before suggesting the criterion we might use to decide when the new economy ha arrived 
we are interested in retrieving information from conversational speech corpus such a call center data this data comprises spontaneous speech conversation with low recording quality which make automatic speech recognition asr a highly difficult task for typical call center data even state of the art large vocabulary continuous speech recognition system produce a transcript with word error rate of or higher in addition to the output transcript advanced system provide word confusion network wcns a compact representation of word lattice associating each word hypothesis with it posterior probability our work exploit the information provided by wcns in order to improve retrieval performance in this paper we show that the mean average precision map is improved using wcns compared to the raw word transcript finally we analyze the effect of increasing asr word error rate on search effectiveness we show that map is still reasonable even under extremely high error rate 
we present and evaluate method for diversifying search result to improve personalized web search a common personalization approach involves reranking the top n search result such that document likely to be preferred by the user are presented higher the usefulness of reranking is limited in part by the number and diversity of result considered we propose three method to increase the diversity of the top result and evaluate the effectiveness of these method 
recent work on ontology based information extraction ie ha tried to make use of knowledge from the target ontology in order to improve semantic annotation result however very few approach exploit the ontology structure itself and those that do so have some limitation this paper introduces a hierarchical learning approach for ie which us the target ontology a an essential part of the extraction process by taking into account the relation between concept the approach is evaluated on the largest available semantically annotated corpus the result demonstrate clearly the benefit of using knowledge from the ontology a input to the information extraction process we also demonstrate the advantage of our approach over other state of the art learning system on a commonly used benchmark dataset 
the exponential growth of the web and the increasing ability of web search engine to index data have led to a problem of plenty the number of result returned per query is typically in the order of million of document for many common query although there is the benefit of added coverage for every query the problem of ranking these document and giving the best result get worse the problem is even more difficult in case of temporal and ambiguous query we try to address this problem using feedback from user query log we leverage a technology called unit for generating query refinement which are shown a also try query on yahoo search we consider these refinement a sub concept which help define user intent and use them to improve search relevance the result obtained via live testing on yahoo search are encouraging 
this paper proposes owl fa a decidable extension of owl dl with the metamodeling architecture of rdfs fa it show that the knowledge base satisfiability problem of owl fa can be reduced to that of owl dl and compare the fa semantics with the recently proposed contextual semantics and hilog semantics for owl category and subject descriptor i artificial intel 
search engine largely rely on web robot to collect information from the web due to the unregulated open access nature of the web robot activity are extremely diverse such crawling activity can be regulated from the server side by deploying the robot exclusion protocol in a flle called robot txt although it is not an enforcement standard ethical robot and many commercial will follow the rule specifled in robot txt with our focused crawler we investigate website from education government news and business domain five crawl have been conducted in succession to study the temporal change through statistical analysis of the data we present a survey of the usage of web robot rule at the web scale the result also show that the usage of robot txt ha increased over time 
in this poster we present an approach to query answering over knowledge source that make use of different ontology management component within an application scenario of the bt digital library the novelty of the approach lie in the combination of different semantic technology providing a clear benefit for the application scenario considered 
when searching people s information need flowthrough to expressing an information retrieval request posed to asearch engine we hypothesise that the degree of specificity of anir request might correspond to the length of a search query ourresults show a strong correlation between decreasing query lengthand increasing broadness or generality of the ir request we foundan average cross over point of specificity from broad to narrow of word in the query these result have implication for searchengines in responding to query of differing length 
in this paper we define the problem of topic sentiment analysis on weblogs and propose a novel probabilistic model to capture the mixture of topic and sentiment simultaneously the proposed topic sentiment mixture tsm model can reveal the latent topical facet in a weblog collection the subtopics in the result of an ad hoc query and their associated sentiment it could also provide general sentiment model that are applicable to any ad hoc topic with a specifically designed hmm structure the sentiment model and topic model estimated with tsm can be utilized to extract topic life cycle and sentiment dynamic empirical experiment on different weblog datasets show that this approach is effective for modeling the topic facet and sentiment and extracting their dynamic from weblog collection the tsm model is quite general it can be applied to any text collection with a mixture of topic and sentiment thus ha many potential application such a search result summarization opinion tracking and user behavior prediction 
microformats are a clever adaptation of semantic xhtml that make it easier to publish index and extract semi structured information such a tag calendar entry contact information and review on the web this make it a pragmatic path towards achieving the vision set forth for the semantic web even though it sidestep the existing technology stack of rdf ontology and artificial intelligence inspired processing tool various microformats have emerged that parallel the goal of several well known semantic web project this poster compare their prospect to the semantic web according to rogers diffusion of innovation model 
across the world wide web there is government censorship and monitoring of political message and morally corrupting material google have been in the news recently for capitulating to the chinese government s demand to ban certain kind of content and also for refusing topass log of browsing habit to the u government while microsoft and yahoo complied wth the request how can the web survive a a unified global information environment in the face of government censorship can government and the private sector come to an agreement on international legal standard for the free flow of information and privacy 
content based multimedia information retrieval can be defined a the task of matching a multi modal information need against various component of a multimedia corpus and retrieving relevant element generally the matching and retrieval take place across multiple feature which can either be visual or audio or can be high level or low level and each of which can be seen to be an independent retrieval expert the task of answering a query can thus be formulated a a data fusion problem depending on the query each expert may perform differently and so retrieval coefficient can be used to weight each expert to increase overall performance previous approach to expert coefficient generation have included query independent coefficient identification of query class and machine learning method to name a few the approach i propose is different a it seek to dynamically create expert coefficient which are query dependent this approach is based upon earlier experiment where an initial correlation wa observed between the score distribution of a retrieval expert and it relative performance when compared against other expert for that query i have created a basic method which leverage these observation to create query time coefficient which achieve comparable performance to oracle determined query independent weight for the expert and collection used in the aforementioned experiment previous research which examinedscore distribution did so with respect to relevance whereas this work seek to compare expert score for a given query to determine relative performance in my work i aim to explore this correlation by eliminating potential bias from the data collection the retrieval expert and the query used in experiment to obtain more robust observation using and extending previous investigation into data fusion i will explore where data fusion succeeds in multimedia retrieval and where it doe not i then aim to refine and extend my existing technique for automatic coefficient generation to incorporate the new observation so a to improve performance finally i will combine this approach with existing data fusion method such a query class coefficient with each approach complimenting the other to achieve further performance improvement 
a large number of question and answer pair can be collected from question and answer board and faq page on the web this paper proposes an automatic method of finding the question that have the same meaning the method can detect semantically similar question that have little word overlap because it calculates question question similarity by using the corresponding answer a well a the question we develop two different similarity measure based on language modeling and compare them with the traditional similarity measure experimental result show that semantically similar question pair can be effectively found with the proposed similarity measure 
in this paper we describe several finding from a multi year multi method study of how information and communication technology have been adopted and adapted in central asia we have found that mobile phone usage is outpacing the rate of internet adoption that access to the internet is primarily through public access site carrying with it issue regarding privacy and surveillance that people rely on their social network a information source that public institution tend to be fairly weak a citizen resource and that information seeking and communication are conflated in people s usage pattern with different technology in addition in the developed world social networking software ha grown rapidly and shown itself to have significant potential for mobilizing a population based on the collection of finding from central asia and observing pattern of technology usage in other part of the world our research lead to the conclusion that exploring mobile social software hold significant potential a an ict that mesh well with preexisting pattern of communication and information seeking and also leverage the most predominant pattern of technology adoption many of the finding from this research echo result from study in other geographic area and so we anticipate that much of this research will be relevant to developing region generally 
in this paper we analyze the web coverage of three search engine google yahoo and msn we conducted a month study collecting web content or information page linked from australian federal and local government web page the key feature of this domain is that new information page are constantly added but the web page tend to provide link only to the more recently added information page search engine list only some of the information page and their coverage varies from month to month meta search engine do little to improve coverage of information page because the problem is not the size of web coverage but the frequency with which information is updated we conclude that organization such a government which post important information on the web cannot rely on all relevant page being found with conventional search engine and need to consider other strategy to ensure important information can be found 
text message stream is a newly emerging type of web data which is produced in enormous quantity with the popularity of instant messaging and internet relay chat it is beneficial for detecting the thread contained in the text stream for various application including information retrieval expert recognition and even crime prevention despite it importance not much research ha been conducted so far on this problem due to the characteristic of the data in which the message are usually very short and incomplete in this paper we present a stringent definition of the thread detection task and our preliminary solution to it we propose three variation of a single pas clustering algorithm for exploiting the temporal information in the stream an algorithm based on linguistic feature is also put forward to exploit the discourse structure information we conducted several experiment to compare our approach with some existing algorithm on a real dataset the result show that all three variation of the single pas algorithm outperform the basic single pas algorithm our proposed algorithm based on linguistic feature improves the performance relatively by and when compared with the basic single pas algorithm and the best variation algorithm in term of f respectively 
currently there is an increasing effort to provide various personalized service on museum web site this paper present an approach for determining user interest in a museum collection with the help of an interactive dialog it us a semantically annotated collection of the rijksmuseum amsterdam to elicit specific user s interest in artist period genre and theme and us these value to recommend relevant artefact and related concept from the museum collection in the presented prototype we show how constructing a user profile and applying recommender strategy in this way enable dynamical generation personalized museum tour for different user 
in this paper we present a system we have developed for automatic tv news video indexing that successfully combine result from the field of speaker verification acoustic analysis very large vocabulary video ocr content based sampling of video information retrieval dialogue system and asf medium delivery over ip the prototype of tv news content processing web wa completed in july since then the system ha been up running continuously up to the date when this message is written march the system record and analyzes the prime time evening news program in taiwan every day of these year except a few power failure shutdown the tv news web is at http newsquery main a 
an important issue arising from large scale data integration is how to efficiently select the top k ranking answer from multiple source while minimizing the transmission cost this paper resolve this issue by proposing an efficient pruning based approach to answer top k join query the total amount of transmitted data can be greatly reduced by pruning tuples that can not produce the desired join result with a rank value greater than or equal to the rank value generated so far 
graph ranking based algorithm e g textrank have been proposed for multi document summarization in recent year however these algorithm miss an important dimension the temporal dimension for summarizing evolving topic for an evolving topic recent document are usually more important than earlier document because recent document contain much more novel information than earlier document and a novelty oriented summary should be more appropriate to reflect the changing topic we propose the timedtextrank algorithm to make use of the temporal information of document based on the graph ranking based algorithm a preliminary study is performed to demonstrate the effectiveness of the proposed timedtextrank algorithm for dynamic multi document summarization 
many classification problem require classifier to assign each single document into more than one category which is called multi labelled classification the category in such problem usually are neither conditionally independent from each other nor mutually exclusive therefore it is not trivial to directly employ state of the art classification algorithm without losing information of relation among category in this paper we explore correlation among category with maximum entropy method and derive a classification algorithm for multi labelled document our experiment show that this method significantly outperforms the combination of single label approach 
existing measure for evaluating clustering result e g f measure have the limitation of overestimating cluster quality because they usually adopt the greedy matching between class reference cluster and cluster system cluster to allow multiple class to correspond to one same cluster which is in fact a locally optimal solution this paper proposes a new evaluation strategy to overcome the limitation of existing evaluation measure by using optimal matching in graph theory a weighted bipartite graph is built with class and cluster a two disjoint set of vertex and the edge weight between any class and any cluster is computed using a basic metric then the total weight of the optimal matching in the graph is acquired and we use it to evaluate the quality of the cluster the optimal matching allows only one to one matching between class and cluster and a globally optimal solution can be achieved a preliminary study is performed to demonstrate the effectiveness of the proposed evaluation strategy 
implicit relevance feedback irf is the process by which a search system unobtrusively gather evidence on searcher interest from their interaction with the system irf is a new method of gathering information on user interest and if irf is to be used in operational ir system it is important to establish when it performs well and when it performs poorly in this paper we investigate how the use and effectiveness of irf is affected by three factor search task complexity the search experience of the user and the stage in the search our finding suggest that all three of these factor contribute to the utility of irf 
almost all existing method conduct the summarization task for single document separately without interaction for each document under the assumption that the document are considered independent of each other this paper proposes a novel framework called collabsum for collaborative single document summarization by making use of mutual influence of multiple document within a cluster context in this study collabsum is implemented by first employing the clustering algorithm to obtain appropriate document cluster and then exploiting the graph ranking based algorithm for collaborative document summarization within each cluster both the with document and cross document relationship between sentence are incorporated in the algorithm experiment on the duc and duc datasets demonstrate the encouraging performance of the proposed approach different clustering algorithm have been investigated and we find that the summarization performance relies positively on the quality of document cluster 
in this work we present topic diversification a novel method designed to balance and diversify personalized recommendation list in order to reflect the user s complete spectrum of interest though being detrimental to average accuracy we show that our method improves user satisfaction with recommendation list in particular for list generated using the common item based collaborative filtering algorithm our work build upon prior research on recommender system looking at property of recommendation list a entity in their own right rather than specifically focusing on the accuracy of individual recommendation we introduce the intra list similarity metric to ass the topical diversity of recommendation list and the topic diversification approach for decreasing the intra list similarity we evaluate our method using book recommendation data including offline analysis on rating and an online study involving more than subject 
a severe potential security problem in utilization of unicode on the web is identified which is resulted from the fact that there are many similar character in the universal character set ucs the foundation of our solution relies on evaluating the similarity of character in ucs we develop a solution based on the renowned kernel density estimation kde method to establish such a unicode similarity list uc simlist 
high precision at the top rank ha become a new focus of research in information retrieval this paper present the multiple nested ranker approach that improves the accuracy at the top rank by iteratively re ranking the top scoring document at each iteration this approach us the ranknet learning algorithm to re rank a subset of the result this split the problem into smaller and easier task and generates a new distribution of the result to be learned by the algorithm we evaluate this approach using different setting on a data set labeled with several degree of relevance we use the normalized discounted cumulative gain ndcg to measure the performance because it depends not only on the position but also on the relevance score of the document in the ranked list our experiment show that making the learning algorithm concentrate on the top scoring result improves precision at the top ten document in term of the ndcg score 
previous work on spatio temporal analysis of news item and other document ha largely focused on broad categorization of small text collection by region or country a system for large scale spatio temporal analysis of online news medium and blog is presented together with an analysis of global news medium coverage over a nine year period we demonstrate the benefit of using a hierarchical geospatial database to disambiguate between geographical named entity and provide result for an extremely fine grained analysis of news item aggregate map of medium attention for particular place around the world are compared with geographical and socio economic data our analysis suggests that gdp per caput is the best indicator for medium attention 
we develop a framework to compose service through discovery and orchestration for a given goal service tightening technique are used in composition algorithm to achieve completeness 
ontology summarization is very important to quick understanding and selection of ontology in this paper we study extractive summarization of ontology we propose a notion of rdf sentence a the basic unit of summarization an rdf sentence graph is proposed to characterize the link between rdf sentence derived from a given ontology the salience of each rdf sentence is assessed in term of it centrality in the graph we propose to summarize an ontology by extracting a set of salient rdf sentence according to a re ranking strategy we compare several measurement in assessing the salience of rdf sentence and give an overall evaluation of experiment result which show that our approach to ontology summarization is feasible 
popularity based search engine have served to stagnate information retrieval from the web developed to deal with the very real problem of degrading quality within keyword based search they have had the unintended side effect of creating iceberg around topic where only a small minority of the information is above the popularity water line this problem is especially pronounced with emerging information new site are often hidden until they become popular enough to be considered above the water line in domain new to a user this is often helpful they can focus on popular site first unfortunately it is not the best tool for a professional seeking to keep up to date with a topic a it emerges and evolves we present a tool focused on this audience a system that address the very large scale information gathering filtering and routing and presentation problem associated with creating a useful incremental stream of information from the web a a whole utilizing the webfountain platform a the primary data engine and really simple syndication r a the delivery mechanism our daily delta delta application is able to provide an informative feed of relevant content directly to a user individual receive a personalized incremental feed of page related to their topic allowing them to track their interest independent of the overall popularity of the topic 
the internet wa the inspiration of j c r licklider when he wa at the advanced research project agency in the s in those pre moore s law day licklider imagined a future in which researcher could access and use computer and data from anywhere in the world today a everyone know the killer application for the internet were email in the s and the world wide web in the s which wa developed initially a a collaboration tool for the particle physic academic community in the future frontier research in many field will increasingly require the collaboration of globally distributed group of researcher needing access to distributed computing data resource and support for remote access to expensive multi national specialized facility such a telescope and accelerator or specialist data archive in the context of science and engineering this is the e science agenda robust middleware service deployed on top of research network will constitute a powerful cyberinfrastructure for collaborative science and engineering this talk will review the element of this vision and describe the present status of effort to build such an internet scale distributed infrastructure based on web service the goal is to provide robust middleware component that will allow scientist and engineer to routinely construct the inter organizational virtual organization given the present state of web service we argue for the need to define such virtual organization grid service on well established web service specification that are widely supported by the it industry only industry can provide the necessary tooling and development environment to enable widespread adoption of such grid service extension to these basic grid service can be added a more web service mature and the research community ha had the opportunity to experiment with new service providing potentially useful new functionality the new cyberinfrastructure will be of relevance to more than just the research community it will impact both the e learning and digital library community allow the creation of scientific mash ups of service giving significant added value 
in order to obtain a machine understandable semantics for web resource research on the semantic web try to annotate web resource with concept and relation from explicitly defined formal ontology this kind of formal annotation is usually done manually or semi automatically in this paper we explore a complement approach that focus on the social annotation of the web which are annotation manually made by normal web user without a pre defined formal ontology compared to the formal annotation although social annotation are coarse grained informal and vague they are also more accessible to more people and better reflect the web resource meaning from the user point of view during their actual usage of the web resource using a social bookmark service a an example we show how emergent semantics can be statistically derived from the social annotation furthermore we apply the derived emergent semantics to discover and search shared web bookmark the initial evaluation on our implementation show that our method can effectively discover semantically related web bookmark that current social bookmark service can not discover easily 
we report the result of a large scale study of password use andpassword re use habit the study involved half a million user over athree month period a client component on user machine recorded a variety of password strength usage and frequency metric this allows u to measure or estimate such quantity a the average number of password and average number of account each user ha how many password she type per day how often password are shared among site and how often they are forgotten we get extremely detailed data on password strength the type and length of password chosen and how they vary by site the data is the first large scale study of it kind and yield numerous other insight into the role the password play in user online experience 
informal communication e mail bulletin board pose a difficult learning environment because traditional grammatical and lexical information are noisy other information is necessary for task such a named entity detection how topic centric or informative a word is can be valuable information it is well known that informative word are best modeled by heavy tailed distribution such a mixture model however informativeness score do not take full advantage of this fact we introduce a new informativeness score that directly utilizes mixture model likelihood to identify informative word we use the task of extracting restaurant name from bulletin board post a a way to determine effectiveness we find that our mixture score is weakly effective alone and highly effective when combined with inverse document frequency we compare against other informativeness criterion and find that only residual idf is competitive against our combined idf mixture score 
in this paper we continue our investigation of web spam the injection of artificially created page into the web in order to influence the result from search engine to drive traffic to certain page for fun or profit this paper considers some previously undescribed technique for automatically detecting spam page examines the effectiveness of these technique in isolation and when aggregated using classification algorithm when combined our heuristic correctly identify of the spam page in our judged collection of page while misidentifying spam and non spam page 
web query classification qc aim to classify web user query which are often short and ambiguous into a set of target category qc ha many application including page ranking in web search targeted advertisement in response to query and personalization in this paper we present a novel approach for qc that outperforms the winning solution of the acm kddcup competition whose objective is to classify real user query in our approach we first build a bridging classifier on an intermediate taxonomy in an offline mode this classifier is then used in an online mode to map user query to the target category via the above intermediate taxonomy a major innovation is that by leveraging the similarity distribution over the intermediate taxonomy we do not need to retrain a new classifier for each new set of target category and therefore the bridging classifier need to be trained only once in addition we introduce category selection a a new method for narrowing down the scope of the intermediate taxonomy based on which we classify the query category selection can improve both efficiency and effectiveness of the online classification by combining our algorithm with the winning solution of kddcup we made an improvement by and in term of precision and f respectively compared with the best result of kddcup 
we present a content driven reputation system for wikipedia author in our system author gain reputation when the edits they perform to wikipedia article are preserved by subsequent author and they lose reputation when their edits are rolled back or undone in short order thus author reputation is computed solely on the basis of content evolution user to user comment or rating are not used the author reputation we compute could be used to flag new contribution from low reputation author or it could be used to allow only author with high reputation to contribute to controversialor critical page a reputation system for the wikipedia could also provide an incentive for high quality contribution we have implemented the proposed system and we have used it to analyze the entire italian and french wikipedias consisting of a total of page and revision our result show that our notion of reputation ha good predictive value change performed by low reputation author have a significantly larger than average probability of having poor quality a judged by human observer and of being later undone a measured by our algorithm 
this paper present a new discriminative model for information retrieval ir referred to a linear discriminant model ldm which provides a flexible framework to incorporate arbitrary feature ldm is different from most existing model in that it take into account a variety of linguistic feature that are derived from the component model of hmm that is widely used in language modeling approach to ir therefore ldm is a mean of melding discriminative and generative model for ir we present two algorithm of parameter learning for ldm one is to optimize the average precision ap directly using an iterative procedure the other is a perceptron based algorithm that minimizes the number of discordant document pair in a rank list the effectiveness of our approach ha been evaluated on the task of ad hoc retrieval using six english and chinese trec test set result show that in most test set ldm significantly outperforms the state of the art language modeling approach and the classical probabilistic retrieval model it is more appropriate to train ldm using a measure of ap rather than likelihood if the ir system is graded on ap and linguistic feature e g phrase and dependence are effective for ir if they are incorporated properly 
we propose a novel probabilistic retrieval model which weight term according to their context in document the term weighting function of our model is similar to the language model and the binary independence model the retrospective experiment i e relevance information is present illustrate the potential of our probabilistic context based retrieval where the precision at the top document is about for trec data and for trec data 
we investigate the difficult problem of matching semi structured resume and job in a large scale real world collection we compare standard approach to structured relevance model srm an extensionof relevance based language model for modeling and retrieving semi structured document preliminary experiment show that the srm approach achieved promising performance and performed better than typical unstructured relevance model 
this paper introduces a locality discriminating indexing ldi algorithm for document classification based on the hypothesis that sample from different class reside in class specific manifold structure ldi seek for a projection which best preserve the within class local structure while suppresses the between class overlap comparative experiment show that the proposed method isable to derives compact discriminating document representation for classification 
we study a new task proactive information retrieval by combining implicit relevance feedback and collaborative filtering we have constructed a controlled experimental setting a prototype application in which the user try to find interesting scientific article by browsing their title implicit feedback is inferred from eye movement signal with discriminative hidden markov model estimated from existing data in which explicit relevance feedback is available collaborative filtering is carried out using the user rating profile model a state of the art probabilistic latent variable model computed using markov chain monte carlo technique for new document title the prediction accuracy with eye movement collaborative filtering and their combination wa significantly better than by chance the best prediction accuracy still leaf room for improvement but show that proactive information retrieval and combination of many source of relevance feedback is feasible 
ranking method like pagerank ass the importance of web page based on the current state of the rapidly evolving web graph the dynamic of the resulting importance score however have not been considered yet although they provide the key to an understanding of the zeitgeist on the web this paper proposes the buzzrank method that quantifies trend in time series of importance score and is based on a relevant growth model of importance score we experimentally demonstrate the usefulness of buzzrank on a bibliographic dataset 
modern retrieval test collection are built through a process called pooling in which only a sample of the entire document set is judged for each topic the idea behind pooling is to find enough relevant document such that when unjudged document are assumed to be nonrelevant the resulting judgment set is sufficiently complete and unbiased a document set grow larger a constant size pool represents an increasingly small percentage of the document set and at some point the assumption of approximately complete judgment must become invalid this paper demonstrates that the aquaint test collection exhibit bias caused by pool that were too shallow for the document set size despite having many diverse run contribute to the pool the existing judgment set favor relevant document that contain topic title word even though relevant document containing few topic title word are known to exist in the document set the paper concludes with suggested modification to traditional pooling and evaluation methodology that may allow very large reusable test collection to be built 
we consider the relationship between training set size and the parameter k for the k nearest neighbor knn classifier when few example are available we observe that accuracy is sensitive to k and that best k tends to increase with training size we explore the subsequent risk that k tuned on partition will be suboptimal after aggregation and re training this risk is found to be most severe when little data is available for larger training size accuracy becomes increasingly stable with respect to k and the risk decrease 
say you are looking for information about a particular person a search engine return many page for that person s name but which page are about the person you care about and which are about other people who happen to have the same name furthermore if we are looking for multiple people who are related in some way how can we best leverage this social network this paper present two unsupervised framework for solving this problem one based on link structure of the web page another using agglomerative conglomerative double clustering a cdc an application of a recently introduced multi way distributional clustering method to evaluate our method we collected and hand labeled a dataset of over web page retrieved from google query on personal name appearing together in someone in an email folder on this dataset our method outperform traditional agglomerative clustering by more than achieving over f measure 
we study the classification of news article into emotion they invoke in their reader our work differs from previous study which focused on the classification of document into their author emotion instead of the reader we use various combination of feature set to find the best combination for identifying the emotional influence of news article on reader 
a page or web snippet is document excerpt allowing a user to understand if a document is indeed relevant without accessing it this paper proposes an effective snippet generation method the pseudo relevance feedback technique and text summarization technique are applied to salient sentence extraction for generating good quality snippet in the experimental result the proposed method showed much better performance than other method including google and naver 
this paper present a semantic portal semport which provides better user support with personalized view semantic navigation ontology based search and three different kind of semantic hyperlink distributed content editing and provision is supplied for the maintenance of the content in real time a a case study semport is tested on the course module web page cmwp of the school of electronics and computer science ec 
we present a novel model for validating and improving the content and structure organization of a website this model study the website a a graph and evaluates it interconnectivity in relation to the similarity of it document the aim of this model is to provide a simple way for improving the overall structure content and interconnectivity of a website this model ha been implemented a a prototype and applied to several website showing very interesting result our model is complementary to other method of website personalization and improvement 
this paper report the design and development of the e sprint learning management system which ha been derived from sistem pengurusan rangkaian integrasi notakuliah dalam talian mod elektronik and currently being implemented at universiti putra malaysia upm the e sprint wa developed by utilizing perl practical extraction and report language and wa supported by standard database in linux unix environment operating system the system is currently being used to supplement and complement part of the classroom based teaching this paper cover the architecture and feature of the e sprint system which consists of five main module some general issue and challenge of such e learning initiative implementation will also be discussed 
by supplying different version of a web page to search engine and to browser a content provider attempt to cloak the real content from the view of the search engine semantic cloaking refers to difference in meaning between page which have the effect of deceiving search engine ranking algorithm in this paper we propose an automated two step method to detect semantic cloaking page based on different copy of the same page downloaded by a web crawler and a web browser the first step is a filtering step which generates a candidate list of semantic cloaking page in the second step a classifier is used to detect semantic cloaking page from the candidate generated by the filtering step experiment on manually labeled data set show that we can generate a classifier with a precision of and a recall of we apply our approach to link from the dmoz open directory project and estimate that more than of these page employ semantic cloaking 
in today s data rich networked world people express many aspect of their life online it is common to segregate different aspect in different place you might write opinionated rant about movie in your blog under a pseudonym while participating in a forum or web site for scholarly discussion of medical ethic under your real name however it may be possible to link these separate identity because the movie journal article or author you mention are from a sparse relation space whose property e g many item related to by only a few user allow re identification this re identification violates people s intention to separate aspect of their life and can have negative consequence it also may allow other privacy violation such a obtaining a stronger identifier like name and address this paper examines this general problem in a specific setting re identification of user from a public web movie forum in a private movie rating dataset we present three major result first we develop algorithm that can re identify a large proportion of public user in a sparse relation space second we evaluate whether private dataset owner can protect user privacy by hiding data we show that this requires extensive and undesirable change to the dataset making it impractical third we evaluate two method for user in a public forum to protect their own privacy suppression and misdirection suppression doesn t work here either however we show that a simple misdirection strategy work well mention a few popular item that you haven t rated 
to investigate the nature of people s understanding for how search engine work we collected data from undergraduate and graduate student student were asked to draw a labeled sketch of how search engine work a reference model wa constructed and each sketch wa analyzed and compared against it for completeness the paper present preliminary result and discus the implication for educational assessment and curriculum design on the one hand and information system design on the other 
click fraud is jeopardizing the industry of internet advertising internet advertising is crucial for the thriving of the entire internet since it allows producer to advertise their product and hence contributes to the well being of e commerce moreover advertising support the intellectual value of the internet by covering the running expense of publishing content some content publisher are dishonest and use automation to generate traffic to defraud the advertiser similarly some advertiser automate click on the advertisement of their competitor to deplete their competitor advertising budget this paper describes the advertising network model and focus on the most sophisticated type of fraud which involves coalition among fraudsters we build on several published theoretical result to devise the similarity seeker algorithm that discovers coalition made by pair of fraudsters we then generalize the solution to coalition of arbitrary size before deploying our system on a real network we conducted comprehensive experiment on data sample for proof of concept the result were very accurate we detected several coalition formed using various technique and spanning numerous site this reveals the generality of our model and approach 
dimensionality reduction play an important role in efficient similarity search which is often based on k nearest neighbor k nn query over a high dimensional feature space in this paper we introduce a novel type of k nn query namely conditional k nn ck nn which considers dimension specific constraint in addition to the inter point distance however existing dimensionality reduction method are not applicable to this new type of query we propose a novel mean std standard deviation guided dimensionality reduction msdr to support a pruning based efficient ck nn query processing strategy our preliminary experimental result on d protein structure data demonstrate that the msdr method is promising 
we investigate the idea of finding semantically related search engine query based on their temporal correlation in other word we infer that two query are related if their popularity behave similarly over time to this end we first define a new measure of the temporal correlation of two query based on the correlation coefficient of their frequency function we then conduct extensive experiment using our measure on two massive query stream from the msn search engine revealing that this technique can discover a wide range of semantically similar query finally we develop a method of efficiently finding the highest correlated query for a given input query using far le space and time than the naive approach making real time implementation possible 
this paper report on an experiment comparing the retrieval effectiveness of an interactive information retrieval iir system which adapts to support different information seeking strategy with that of a standard baseline iir system the experiment with subject each searching on different topic indicates that using the integrated iir system resulted in significantly better performance including user satisfaction with search result significantly more effective interaction and significantly better usability than using the baseline system 
we present wap mead a wap enabled text summarization system it incorporates a state of the art text summarizer enhanced to produce hierarchical summary that are appropriate for various type of mobile device including cellular phone 
portlets i e multi step user facing application to be syndicated within a portal are currently supported by most portal framework however there is not yet a definitive answer to portlet interoperation whereby data flow smoothly from one portlet to a neighbouring one both data based and api based approach exhibit some drawback in either the limitation of the sharing scope or the standardization effort required we argue that these limitation can be overcome by using deep annotation technique by providing additional markup about the background service deep annotation strives to interact with these underlying service rather than with the html surface that conveys the markup in this way the portlet producer can extend a portlet markup a fragment with data about the process whose rendering this fragment support then the portlet consumer e g a portal can use deep annotation to map an output process in fragment a to an input process in fragment b this mapping result in fragment b having it input form or other input widget filled up we consider deep annotation a particularly valid for portlet interoperation due to the controlled and cooperative environment that characterizes the portal setting 
enterprise and web data processing and content aggregation system often require extensive use of human reviewed data e g for training and monitoring machine learning based application today these need are often met by in house effort or out sourced offshore contracting emerging application attempt to provide automated collection of human reviewed data at internet scale we conduct extensive experiment to study the effectiveness of one such application we also study the feasibility of using yahoo answer a general question answering forum for human reviewed data collection 
fully automated trading such a e procurement using the internet is virtually unheard of today three core technology are needed to fully automate the trading process data mining intelligent trading agent and virtual institution in which informed trading agent can trade securely both with each other and with human agent in a natural way this paper describes a demonstrable prototype e trading system that integrates these three technology and is available on the world wide web this is part of a larger project that aim to make informed automated trading a reality 
this paper present a simple and intuitive method for mining search engine query log to get fast query recommendation on a large scale industrial strength search engine in order to get a more comprehensive solution we combine two method together on the one hand we study and model search engine user sequential search behavior and interpret this consecutive search behavior a client side query refinement that should form the basis for the search engine s own query refinement process on the other hand we combine this method with a traditional content based similarity method to compensate for the high sparsity of real query log data and more specifically the shortness of most query session to evaluate our method we use one hundred day worth query log from sina search engine to do off line mining then we analyze three independent editor evaluation on a query test set based on their judgement our method wa found to be effective for finding related query despite it simplicity in addition to the subjective editor rating we also perform test based on actual anonymous user search session 
we present yago a light weight and extensible ontology with high coverage and quality yago build on entity and relation and currently contains more than million entity and million fact this includes the is a hierarchy a well a non taxonomic relation between entity such a hasoneprize the fact have been automatically extracted from wikipedia and unified with wordnet using a carefully designed combination of rule based and heuristic method described in this paper the resulting knowledge base is a major step beyond wordnet in quality by adding knowledge about individual like person organization product etc with their semantic relationship and in quantity by increasing the number of fact by more than an order of magnitude our empirical evaluation of fact correctness show an accuracy of about yago is based on a logically clean model which is decidable extensible and compatible with rdfs finally we show how yago can be further extended by state of the art information extraction technique 
professional in the workplace need high precision search tool capable of retrieving information that is useful and appropriate to the task at hand one approach to identifying content which is not only relevant but also useful is to make use of the task context of the search we present x site an enterprise search engine for the software engineering domain that exploit relationship between user s task and document genre in the collection to improve retrieval precision 
a methodology based on information nugget ha recently emerged a the de facto standard by which answer to complex question are evaluated after several implementation in the trec question answering track the community ha gained a better understanding of it many characteristic this paper focus on one particular aspect of the evaluation the human assignment of nugget to answer string which serf a the basis of the f score computation a a byproduct of the trec ciqa task identical answer string were independently evaluated twice which allowed u to ass the consistency of human judgment based on these result we explored simulation of assessor behavior that provide a method to quantify scoring variation understanding these variation in turn let researcher be more confident in their comparison of system 
a common language modeling approach assumes the data d is generated from a mixture of several language model em algorithm is usually used to flnd the maximum likelihood estimation of one unknown mixture component given the mixture weight and the other language model in this paper we provide an e cient algorithm of o k complexity to flnd the exact solution where k is the number of word occurred at least once in d another merit is that the probability of many word are exactly zero which mean that the mixture language model also serf a a feature selection technique 
this paper proposes gbm gravitation based model a physical model for information retrieval inspired by newton s theory of gravitation a mapping is built in this model from concept of information retrieval document query relevance etc to those of physic mass distance radius attractive force etc this model actually provides a new perspective on ir problem a family of effective term weighting function can be derived from it including the well known bm formula this model ha some advantage over most existing one first because it is directly based on basic physical law the derived formula and algorithm can have their explicit physical interpretation second the ranking formula derived from this model satisfy more intuitive heuristic than most of existing one thus have the potential to behave empirically better and to be used safely on various setting finally a new approach for structured document retrieval derived from this model is more reasonable and behaves better than existing one 
in this paper we present oyster a peer to peer system for exchanging ontology metadata among community in the semantic web oyster exploit semantic web technique in data representation query formulation and query result presentation to provide an online solution for sharing ontology thus assisting researcher in re using existing ontology 
a map is one of the most useful medium in disseminating spatial information a mobile device are becoming increasingly powerful and ubiquitous new possibility to access map information are created however mobile device still face severe constraint that limit the possibility that a mobile map application may offer we present the m chartis system a device independent mobile map application that enables mobile user to access map information from their device 
open information space have several unique characteristic such a their changeability large size complexity and diverse user base these result in novel challenge during user navigation information retrieval and data visualization in open information space we propose a method of navigation in open information space based on an enhanced faceted browser with support for dynamic facet generation and adaptation based on user characteristic 
this paper describes a method of detecting japanese katakana variant from a large corpus katakana word which are mainly used a loanword cause problem with information retrieval and so on because transliteration creates several variation in spelling and all of these can be orthographic previous work manually defined katakana rewrite rule such a y be and t ve being replaceable with each other for generating variant and also defined the weight of each operation to edit one string into another to detect these variant however these previous research have not been able to keep up with the ever increasing number of loanword and their variant with our method proposed in this paper the weight of each edit operation is mechanically assigned based on web data in experiment it performed almost a well a one with manually determined weight thus the advantage of our method are need no expertise in linguistics to determine weight of each operation and able to keep up with new katakana loanword only by collecting text data from web and acquiring new weight of edit operation automatically it also achieved recall and precision in the task of extracting katakana variant pair from year s worth of corpus of japanese newspaper article 
search engine provide a small window to the vast repository of data they index and against which they search they try their best to return the document that are of relevance to the user but often a large number of result may be returned user struggle to manage this vast result set looking for the item of interest clustering search result is one way of alleviating this navigational pain in this paper we describe a clustering system that enables clustering search result in an online marketplace search system 
we propose a novel algorithm for extracting information by mining the feature space cluster and then assigning salient concept to them bayesian technique for extracting concept from multimedia usually suffer either from lack of data or from too complex concept to be represented by a single statistical model an incremental information extraction approach working at different level of abstraction would be able to handle concept of varying complexity we present the result of our research on the initial part of an incremental approach the extraction of the most salient concept from multimedia information 
question answer portal such a naver and yahoo answer are growing in popularity however despite the increased popularity the quality of answer is uneven and while some user usually provide good answer many others often provide bad answer hence estimating the authority or the expected quality of user is a crucial task for this emerging domain with potential application to answer ranking and to incentive mechanism design we adapt a powerful link analysis methodology from the web domain a a first step towards estimating authority in question answer portal our experimental result over more than million answer from yahoo answer are promising and warrant further exploration along the line outlined in this poster 
we introduce a novel approach to combining ranking from multiple retrieval system we use a logistic regression model or an svm to learn a ranking from pairwise document preference our approach requires no training data or relevance score and outperforms a popular voting algorithm 
we present a simple way to improve document retrieval for question answering system the method bias the retrieval system toward document that contain word that have appeared in other document containing answer to the same type of question the method work with virtually any retrieval system and exhibit a statistically significant performance improvement over a strong baseline 
user query is an element that specifies an information need but it is not the only one study in literature have found many contextual factor that strongly influence the interpretation of a query recent study have tried to consider the user s interest by creating a user profile however a single profile for a user may not be sufficient for a variety of query of the user in this study we propose to use query specific context instead of user centric one including context around query and context within query the former specifies the environment of a query such a the domain of interest while the latter refers to context word within the query which is particularly useful for the selection of relevant term relation in this paper both type of context are integrated in an ir model based on language modeling our experiment on several trec collection show that each of the context factor brings significant improvement in retrieval effectiveness 
effective organization of search result is critical for improving the utility of any search engine clustering search result is an effective way to organize search result which allows a user to navigate into relevant document quickly however two deficiency of this approach make it not always work well the cluster discovered do not necessarily correspond to the interesting aspect of a topic from the user s perspective and the cluster label generated are not informative enough to allow a user to identify the right cluster in this paper we propose to address these two deficiency by learning interesting aspect of a topic from web search log and organizing search result accordingly and generating more meaningful cluster label using past query word entered by user we evaluate our proposed method on a commercial search engine log data compared with the traditional method of clustering search result our method can give better result organization and more meaningful label 
using saracevic s relevance type we explore approach to combining algorithm and cognitive relevance in a term relevance feedback scenario data collected from user who provided relevance feedback about term suggested by a system for trec hard topic are used the former type of feedback is considered a cognitive relevance and the latter type is considered a algorithm relevance we construct retrieval run using these two type of relevance feedback and experiment with way of combining them with simple boolean operator result show minimal difference in performance with respect to the different technique 
the evolution of the web requires to consider an increasing number of context dependency issue therefore in our research we focus on how to extend a web application with additional adaptation concern without having to redesign the entire application based on a generic transcoding tool we illustrate here how we can add adaptation functionality to an existing web application furthermore we consider how an aspect oriented approach can support the high level specification of such additional concern in the design of the web application 
this paper describes a comprehensive method for presenting mathematical equation and expression using only pure html and cs this method render the equation portable and editable and contrast with previous procedure that represent equation a whole graphic object method for generating and documenting the equation using html and javascript are also described such that the equation can be interpreted and converted to or from other format such a latex mathml or linear representation 
automatic classification of data item based on training sample can be boosted by considering the neighborhood of data item in a graph structure e g neighboring document in a hyperlink environment or co author and their publication for bibliographic data entry this paper present a new method for graph based classification with particular emphasis on hyperlinked text document but broader applicability our approach is based on iterative relaxation labeling and can be combined with either bayesian or svm classifier on the feature space of the given data item the graph neighborhood is taken into consideration to exploit locality pattern while at the same time avoiding overfitting in contrast to prior work along these line our approach employ a number of novel technique dynamically inferring the link class pattern in the graph in the run of the iterative relaxation labeling judicious pruning of edge from the neighborhood graph based on node dissimilarity and node degree weighting the influence of edge based on a distance metric between the classification label of interest and weighting edge by content similarity measure our technique considerably improve the robustness and accuracy of the classification outcome a shown in systematic experimental comparison with previously published method on three different real world datasets 
many study focus on the web but yet few focus on peer to peer file sharing system query despite their massive scale in term of internet traffic we analyzed several million query collected on the gnutella network and differentiated our finding from those of web query 
text search over temporally versioned document collection such a web archive ha received little attention a a research problem a a consequence there is no scalable and principled solution to search such a collection a of a specified time in this work we address this shortcoming and propose an efficient solution for time travel text search by extending the inverted file index to make it ready for temporal search we introduce approximate temporal coalescing a a tunable method to reduce the index size without significantly affecting the quality of result in order to further improve the performance of time travel query we introduce two principled technique to trade off index size for it performance these technique can be formulated a optimization problem that can be solved to near optimality finally our approach is evaluated in a comprehensive series of experiment on two large scale real world datasets result unequivocally show that our method make it possible to build an efficient time machine scalable to large versioned text collection 
with the explosive growth and spread of internet web access from mobile and rural user ha become significant but these user face problem of low bandwidth and intermittent internet connectivity to make the benefit of the internet reach the common man in developing country accessibility and availability of the information ha to be improved aaqua is an online multilingual multimedia agricultural portal for disseminating information from and to rural community considering resource constrained rural environment we have designed and implemented an offline solution which provides an online experience to user in disconnected mode our solution is based on heterogeneous database synchronization which involves only a small synchronization payload ensuring an efficient use of available bandwidth offline aaqua ha been deployed in the field and systematic study of our solution show that user experience ha improved tremendously not only in disconnected mode but also in connected mode 
this paper is to investigate the group behavior pattern of search activity based on web search history data i e clickthrough data to boost search performance we propose a collaborative web search cws framework based on the probabilistic modeling of the co occurrence relationship among the heterogeneous web object user query and web page the cws framework consists of two step a cube clustering approach is put forward to estimate the semantic cluster structure of the web object web search activity are conducted by leveraging the probabilistic relation among the estimated cluster structure experiment on a real world clickthrough data set validate the effectiveness of our cws approach 
since conventional historical record have been written assuming human reader they are not well suited for computer to collect and process automatically if computer could understand description in historical record and process them automatically it would be easy to analyze them from different perspective in this paper we review a number of existing framework used to describe historical event and make a comparative assessment of these framework in term of usability based on deep case of fillmore s core grammar based on this assessment we propose a new description framework and have created a microformat vocabulary set suitable for that framework 
we propose a method for estimating the credibility of the posted information from user the system display these information on the map since posted information can include subjective information from various perspective we can t trust all of the posting a they are we propose and integrate factor of the user s geographic posting tendency and vote by other user 
we propose an approach qrrs query relaxative ranking svm that divide a ranking function into different relaxation step so that only cheap feature are used in ranking svm of early step for query efficiency we show search quality in the approach is improved compared to conventional ranking svm 
in this paper we describe sgsdesigner the odesgs environment user interface odesgs environment the realization of the odesgs framework is an environment for supporting both a the annotation of pre existing grid service g and b the design of new complex semantic grid service sg in a semi automatic way 
we consider the problem of duplicate document detection for search evaluation given a query and a small number of web result for that query we show how to detect duplicate web document with precision and recall in contrast charikar s algorithm designed for duplicate detection in an indexing pipeline achieves precision but with a recall of our improvement in recall while maintaining high precision come from combining three idea first because we are only concerned with duplicate detection among result for the same query the number of pairwise comparison is small therefore we can afford to compute multiple pairwise signal for each pair of document a model learned with standard machine learning technique improves recall to with precision second most duplicate detection ha focused on text analysis of the html content of a document in some web page the html is not a good indicator of the final content of the page we use extended fetching technique to fill in frame and execute java script including signal based on our richer fetch further improves the recall to and the precision to finally we also explore using signal based on the query comparing contextual snippet based on the richer fetch improves the recall to we show that the overall accuracy of this final model approach that of human judge 
the paper is concerned with applying learning to rank to document retrieval ranking svm is a typical method of learning to rank we point out that there are two factor one must consider when applying ranking svm in general a learning to rank method to document retrieval first correctly ranking document on the top of the result list is crucial for an information retrieval system one must conduct training in a way that such ranked result are accurate second the number of relevant document can vary from query to query one must avoid training a model biased toward query with a large number of relevant document previously when existing method that include ranking svm were applied to document retrieval none of the two factor wa taken into consideration we show it is possible to make modification in conventional ranking svm so it can be better used for document retrieval specifically we modify the hinge loss function in ranking svm to deal with the problem described above we employ two method to conduct optimization on the loss function gradient descent and quadratic programming experimental result show that our method referred to a ranking svm for ir can outperform the conventional ranking svm and other existing method for document retrieval on two datasets 
finding relationship between entity on the web e g the connection between different place or the commonality of people is a novel and challenging problem existing web search engine excel in keyword matching and document ranking but they cannot well handle many relationship query this paper proposes a new method for answering relationship query on two entity our method first respectively retrieves the top web page for either entity from a web search engine it then match these web page and generates an ordered list of web page pair each web page pair consists of one web page for either entity the top ranked web page pair are likely to contain the relationship between the two entity one main challenge in the ranking process is to effectively filter out the large amount of noise in the web page without losing much useful information to achieve this our method assigns appropriate weight to term in web page and intelligently identifies the potential connecting term that capture the relationship between the two entity only those top potential connecting term with large weight are used to rank web page pair finally the top ranked web page pair are presented to the searcher for each such pair the query term and the top potential connecting term are properly highlighted so that the relationship between the two entity can be easily identified we implemented a prototype on top of the google search engine and evaluated it under a wide variety of query scenario the experimental result show that our method is effective at finding important relationship with low overhead 
in this poster we describe a framework composed of the r o mapping language and the odemapster processor to upgrade relational legacy data to the semantic web the framework is based on the declarative description of mapping between relational and ontology element and the exploitation of such mapping description by a generic processor capable of performing both massive and query driven data upgrade 
internet traffic is bursty and network server are often overloaded with surprising event or abnormal client request pattern this paper study a load shedding mechanism called selective early request termination sert for network service that use thread to handle multiple incoming request continuously and concurrently our investigation with application from ask com show that during overloaded situation a relatively small percentage of long request that require excessive computing resource can dramatically affect other short request and reduce the overall system throughput by actively detecting and aborting overdue long request service can perform significantly better to achieve qos objective compared to a purely admission based approach we have proposed a termination scheme that monitor running time of request account for their resource usage adaptively adjusts the selection threshold and performs a safe termination for a class of request this paper present the design and implementation of this scheme and describes experimental result to validate the proposed approach 
people often repeat web search both to find new information on topic they have previously explored and to re find information they have seen in the past the query associated with a repeat search may differ from the initial query but can nonetheless lead to click on the same result this paper explores repeat search behavior through the analysis of a one year web query log of anonymous user and a separate controlled survey of an additional volunteer our study demonstrates that a many a of all query are re finding query re finding appears to be an important behavior for search engine to explicitly support and we explore how this can be done we demonstrate that change to search engine result can hinder re finding and provide a way to automatically detect repeat search and predict repeat click 
the objective of web forum is to create a shared space for open communication and discussion of specific topic and issue the tremendous information behind forum site is not fully utilized yet most link between forum page are automatically created which mean the link based ranking algorithm cannot be applied efficiently in this paper we proposed a novel ranking algorithm which try to introduce the content information into link based method a implicit link the basic idea is derived from the more focused random surfer the surfer may more likely jump to a page which is similar to what he is reading currently in this manner we are allowed to introduce the content similarity into the link graph a a personalization bias our method named fine grained rank fgrank can be efficiently computed based on an automatically generated topic hierarchy not like the topic sensitive pagerank our method only need to compute single pagerank score for each page another contribution of this paper is to present a very efficient algorithm for automatically generating topic hierarchy and map each page in a large scale collection onto the computed hierarchy the experimental result show that the proposed method can improve retrieval performance and reveal that content based link graph is also important compared with the hyper link graph 
this paper present a novel online video recommendation system called videoreach which alleviates user effort on finding the most relevant video according to current viewing without a sufficient collection of user profile a required in traditional recommenders in this system video recommendation is formulated a finding a list of relevant video in term of multimodal relevance i e textual visual and aural relevance and user click through since different video have different intra weight of relevance within an individual modality and inter weight among different modality we adopt relevance feedback to automatically find optimal weight by user click though a well a an attention fusion function to fuse multimodal relevance we use clip a the representative test video which are searched by top query from more than k online video and report superior performance compared with an existing video site 
question classification ha become a crucial step in modern question answering system previous work ha demonstrated the effectiveness of statistical machine learning approach to this problem this paper present a new approach to building a question classifier using log linear model evidence from a rich and diverse set of syntactic and semantic feature is evaluated a well a approach which exploit the hierarchical structure of the question class 
in a world where all device will be interconnected the boundary between the different device will start to disappear device will be able to access each other s application session can be suspended on one device and resumed on another device device can serve a each other s input and output device and all device will be able to connect to the internet this will give true mobility to the user a he she will not be restricted to the time and location where he she access an application of course we need a variety of different mechanism and technology to enable this such a remote rendering of uis on other device in the network infrastructure for discovering client and server in a network mechanism to exchange capability information between device and to adapt the ui based on these capability mechanism to deal with session migration support for a wide range of consumer device ranging from mobile phone to high end tv this requires technology that cross different domain i e the pc domain mobile domain and tv domain several major company within these different domain have decided to work together on these issue one of the result is a framework for remote user interface for both upnp network and the internet this framework is called web ce a k a cea and ha been accepted a the baseline remote user interface technology within the digital living network alliance dlna which is a large industry wide effort for creating true interoperability between network enabled device this paper provides a short overview of the web ce framework and some of the use case that it enables 
contextual advertising or context match cm refers to the placement of commercial textual advertisement within the content of a generic web page while sponsored search s advertising consists in placing ad on result page from a web search engine with ad driven by the originating query in cm there is usually an intermediary commercial ad network entity in charge of optimizing the ad selection with the twin goal of increasing revenue shared between the publisher and the ad network and improving the user experience with these goal in mind it is preferable to have ad relevant to the page content rather than generic ad the s market developed quicker than the cm market and most textual ad are still characterized by bid phrase representing those query where the advertiser would like to have their ad displayed hence the first technology for cm have relied on previous solution for s by simply extracting one or more phrase from the given page content and displaying ad corresponding to search on these phrase in a purely syntactic approach however due to the vagary of phrase extraction and the lack of context this approach lead to many irrelevant ad to overcome this problem we propose a system for contextual ad matching based on a combination of semantic and syntactic feature 
this paper proposes a semantic overlay based on the small world phenomenon that facilitates efficient search for information retrieval in unstructured p p system in the semantic overlay each node maintains a number of short range link which are semantically similar to each other together with a small collection of long range link that help increasing recall rate of information retrieval and reduce network traffic a well experimental result show that our model can improve performance by compared to gnutella and by up to compared to the interest based model a similar shortcut based search technique 
the direct application of standard ranking technique to retrieve individual element from a collection of xml document often produce a result set in which the top rank are dominated by a large number of element taken from a small number of highly relevant document this paper present and evaluates an algorithm that re rank this result set with the aim of minimizing redundant content while preserving the benefit of element retrieval including the benefit of identifying topic focused component contained within relevant document the test collection developed by the initiative for the evaluation of xml retrieval inex form the basis for the evaluation 
we present a novel language modeling approach to capturing the query reformulation behavior of web search user based on a framework that categorizes eight different type of user move adding removing query term etc we treat search session a sequence data and build n gram language model to capture user behavior we evaluated our model in a prediction task the result suggest that useful pattern of activity can be extracted from user history furthermore by examining prediction performance under different order n gram model we gained insight into the amount of history context that is associated with different type of user action our work serf a the basis for more refined user model 
this paper present a structured p p overlay scan that augments can overlay with long link based on kleinberg s small world model in a d dimensional cartesian space the construction of long link doe not require the estimate of network size query in multi dimensional data space can achieve o log n hop by equipping each node with o log n long link and o d short link 
the aim of this study is to investigate whether element retrieval a opposed to full text retrieval is meaningful and useful for searcher when carrying out information seeking task our result suggest that searcher find the structural breakdown of document useful when browsing within retrieved document and provide support for the usefulness of element retrieval in interactive setting 
the content of a webpage is usually contained within a small body of text and image or perhaps several article on the same page however the content may be lost in the clutter particularly hurting user browsing on small cell phone and pda screen and visually impaired user relying on speed rendering of web page using the genre of a web page we have created a solution crunch that automatically identifies clutter and remove it thus leaving a clean content full page in order to evaluate the improvement in the application for this technology we identified a number of experiment in this paper we have those experiment the associated result and their evaluation 
the logical approach to information retrieval try to model the relevance of a document given a query a the logical implication between document and query in early work van rijsbergen state that the retrieval status value of a document given a query is proportional to the degree of implication between a document and a query based on this several probabilistic logic for information retrieval have been conceived which add an additional layer of abstraction to the information retrieval task probabilistic model for the retrieval of document are expressed in those logic rather than implemented directly the aim of the research presented here is to develop a logic for the ir task of document summarisation such a summarisation logic add an abstraction layer to the summarisation task similarly to the way a logic for document retrieval add a layer to the retrieval task probabilistic model for document summarisation will thus be expressed a logical formula with the actual implementation hidden by the logical expression the extract worthyness of textual component in this logic is measured a the degree of implication from those textual component to their surrounding context providing a measure of how much said component are about the context within which they are situated 
we consider the problem of large scale retrieval evaluation and we propose a statistical method for evaluating retrieval system using incomplete judgment unlike existing technique that rely on effectively complete and thus prohibitively expensive relevance judgment set produce biased estimate of standard performance measure or produce estimate of non standard measure thought to be correlated with these standard measure our proposed statistical technique produce unbiased estimate of the standard measure themselves our proposed technique is based on random sampling while our estimate are unbiased by statistical design their variance is dependent on the sampling distribution employed a such we derive a sampling distribution likely to yield low variance estimate we test our proposed technique using benchmark trec data demonstrating that a sampling pool derived from a set of run can be used to efficiently and effectively evaluate those run we further show that these sampling pool generalize well to unseen run our experiment indicate that highly accurate estimate of standard performance measure can be obtained using a number of relevance judgment a small a of the typical trec style judgment pool 
we revisit a problem introduced by bharat and broder almost a decade ago how to sample random page from a search engine s index using only the search engine s public interface such a primitive is particularly useful in creating objective benchmark for search engine the technique of bharat and broder suffers from two well recorded bias it favor long document and highly ranked document in this paper we introduce two novel sampling technique a lexicon based technique and a random walk technique our method produce biased sample document but each sample is accompanied by a corresponding weight which represents the probability of this document to be selected in the sample the sample in conjunction with the weight are then used to simulate near uniform sample to this end we resort to three well known monte carlo simulation method rejection sampling importance sampling and the metropolis hastings algorithm we analyze our method rigorously and prove that under plausible assumption our technique are guaranteed to produce near uniform sample from the search engine s index experiment on a corpus of million document substantiate our analytical finding and show that our algorithm do not have significant bias towards long or highly ranked document we use our algorithm to collect fresh data about the relative size of google msn search and yahoo category and subject descriptor h information search and retrieval 
non negative matrix factorization nmf and probabilistic latent semantic analysis plsa have been successfully applied to a number of text analysis task such a document clustering despite their different inspiration both method are instance of multinomial pca we further explore this relationship and first show that plsa solves the problem of nmf with kl divergence and then explore the implication of this relationship 
the web is rapidly moving towards a platform for mass collaboration in content production and consumption fresh content on a variety of topic people and place is being created and made available on the web at breathtaking speed navigating the content effectively not only requires technique such a aggregating various r enabled feed but it also demand a new browsing paradigm in this paper we present novel geospatial and temporal browsing technique that provide user with the capability of aggregating and navigating r enabled content in a timely personalized and automatic manner in particular we describe a system called geotracker that utilizes both a geospatial representation and a temporal chronological presentation to help user spot the most relevant update quickly within the context of this work we provide a middleware engine that support intelligent aggregation and dissemination of r feed with personalization to desktop and mobile device we study the navigation capability of this system on two kind of data set namely world cup soccer data collected over two month and breaking news item that occur every day we also demonstrate that the application of such technology to the video search result returned by youtube and google greatly enhances a user s ability in locating and browsing video based on his or her geographical interest finally we demonstrate that the location inference performance of geotracker compare well against machine learning technique used in the natural language processing information retrieval community despite it algorithm simplicity it preserve high recall percentage 
the reasoning task that can be performed with semantic web service description depend on the quality of the domain ontology used to create these description however building such domain ontology is a time consuming and difficult task we describe an automatic extraction method that learns domain ontology for web service description from textual documentation attached to web service we conducted our experiment in the field of bioinformatics by learning an ontology from the documentation of the web service used in mygrid a project that support biology experiment on the grid based on the evaluation of the extracted ontology in the context of the project we conclude that the proposed extraction method is a helpful tool to support the process of building domain ontology for web service description 
enriching web application with personalized data is of major interest for facilitating the user access to the published content and therefore for guaranteeing successful user navigation we propose a conceptual model for extracting personalized recommendation based on user profiling ontological domain model and semantic reasoning the approach offer a high level representation of the designed application based on a domain specific metamodel for web application called webml 
we introduce and validate bootstrap technique to compute confidence interval that quantify the effect of test collection variability on average precision ap and mean average precision map ir effectiveness measure we consider the test collection in ir evaluation to be a representative of a population of materially similar collection whose document are drawn from an infinite pool with similar characteristic our model accurately predicts the degree of concordance between system result on randomly selected half of the trec ad hoc corpus we advance a framework for statistical evaluation that us the same general framework to model other source of chance variation a a source of input for meta analysis technique 
in this paper we present a password stretching method using user specific salt our scheme take similar time to stretch a password a recent password stretching algorithm but the complexity of a pre computation attack increase by time and the storage required to store the pre computation result increase by time 
peer to peer technology are increasingly becoming the medium of choice for deliveringmedia content both professional and home grown to large user population indeed current p p swarming system have been shown to be very efficient for large scale content distribution with few server resource however such system have been designed for generic file distribution and provide a limited user experience for viewing medium content for example user need to wait to download the full video before they can start watching it in general the main challenge resides in designing system that ensure that user can start watching a movie at any point in time with small start up time and sustainable playback rate in this work we address the issue of providing a video on demand vod using p p mesh based network we show that providing high quality vod using p p is feasible using a combination of techniquesincluding a network coding b optimized resource allocation across different part of the video and c overlay topology management algorithm our evaluation also show that system that do not use these technique and do not optimize all of those dimension can significantly under utilize the network resource and result in poor vod performance our result are based on simulation and result from a prototype implementation 
in recent work conditional markov chain model cmm have been used to extract information from semi structured text one example is the conditional random field application range from finding the author and title in research paper to finding the phone number and street address in a web page the cmm framework combine a priori knowledge encoded a feature with a set of labeled training data to learn an efficient extraction process we will show that similar problem can be solved more effectively by learning a discriminative context free grammar from training data the grammar ha several distinct advantage long range even global constraint can be used to disambiguate entity label training data is used more efficiently and a set of new more powerful feature can be introduced the grammar based approach also result in semantic information encoded in the form of a parse tree which could be used for ir application like question answering the specific problem we consider is of extracting personal contact or address information from unstructured source such a document and email while linear chain cmms perform reasonably well on this task we show that a statistical parsing approach result in a reduction in error rate this system also ha the advantage of being interactive similar to the system described in in case where there are multiple error a single user correction can be propagated to correct multiple error automatically using a discriminatively trained grammar of all token are labeled correctly compared to for a cmm and of record have all token labeled correctly compared to for the cmm 
we introduce a stricter web community definition to overcome boundary ambiguity of a web community defined by flake lawrence and giles and consider the problem of finding community that satisfy our definition we discus how to find such community and hardness of this problem we also propose web page partitioning by equivalence relation defined using the class of community of our definition though the problem of efficiently finding all community of our definition is np complete we propose an efficient method of finding a subclass of community among the set partitioned by each of n cut represented by a gomory hu tree and partitioning a web graph by equivalence relation defined using the subclass according to our preliminary experiment partitioning by our method divided the page retrieved by keyword search into several different category to some extent 
a new shot level video retrieval system that support semantic visual feature e g car mountain and fire browsing is developed to facilitate content based retrieval the video s binary semantic feature vector is utilized to calculate the score of similarity between two shot keyframes the score is then used to browse the similar keyframes in term of semantic visual feature 
the goal of this poster is to describe our implementation of a newarchitecture enabling efficient integration between mobile phoneapplications and web service using this architecture we haveimplemented a mobile shopping assistant described further in orderto build this architecture we designed an innovative xmlcompression mechanism to facilitate data exchange between mobilephones and web service we also designed a smart connection managerto control asynchronous communication for all possible channel of amobile phone in addition we used diverse input mode in order toextend user access to web service 
this paper introduces a general framework for the use of translation probability in cross language information retrieval based on the notion that information retrieval fundamentally requires matching what the searcher mean with what the author of a document meant that perspective yield a computational formulation that provides a natural way of combining what have been known a query and document translation two well recognized technique are shown to be a special case of this model under restrictive assumption cross language search result are reported that are statistically indistinguishable from strong monolingual baseline for both french and chinese document 
we propose a new system to mine visual knowledge on the web there are huge image data a well a text data on the web however mining image data from the web is paid le attention than mining text data since treating semantics of image are much more difficult in this paper we propose introducing a latest image recognition technique which is the bag of keypoints representation into web image gathering task by the experiment we show theproposed system outperforms our previous system and google imagesearch greatly 
spammer use questionable search engine optimization seo technique to promote their spam link into top search result in this paper we focus on one prevalent type of spam redirection spam where one can identify spam page by the third party domain that these page redirect traffic to we propose a five layer double funnel model for describing end to end redirection spam present a methodology for analyzing the layer and identify prominent domain on each layer using two set of commercial keywords one targeting spammer and the other targeting advertiser the methodology and finding are useful for search engine to strengthen their ranking algorithm against spam for legitimate website owner to locate and remove spam doorway page and for legitimate advertiser to identify unscrupulous syndicator who serve ad on spam page 
in this paper we study the trade offs in designing efficient caching system for web search engine we explore the impact of different approach such a static v dynamic caching and caching query result v caching posting list using a query log spanning a whole year we explore the limitation of caching and we demonstrate that caching posting list can achieve higher hit rate than caching query answer we propose a new algorithm for static caching of posting list which outperforms previous method we also study the problem of finding the optimal way to split the static cache between answer and posting list finally we measure how the change in the query log affect the effectiveness of static caching given our observation that the distribution of the query change slowly over time our result and observation are applicable to different level of the data access hierarchy for instance for a memory disk layer or a broker remote server layer 
there are principal difference between the relational model and xml s tree model this cause problem in all case where information from these two world ha to be brought together using a few rule for mapping the incompatible aspect of the two model it becomes easier to process data in system which need to work with relational and tree data the most important requirement for a good mapping is that the conceptual model is available and can thus be used for making mapping decision 
addressed in this paper is the issue of semantic relationship extraction from semi structured document many research effort have been made so far on the semantic information extraction however much of the previous work focus on detecting isolated semantic information by making use of linguistic analysis or linkage information in web page and limited research ha been done on extracting semantic relationship from the semi structured document in this paper we propose a method for semantic relationship extraction by using the logical information in the semi structured document semi structured document usually ha various type of structure information e g a semi structured document may be hierarchical laid out to the best of our knowledge extracting semantic relationship by using logical information ha not been investigated previously a probabilistic approach ha been proposed in the paper feature used in the probabilistic model have been defined 
we introduce a new powerful class of text proximity query find an instance of a given answer type person place distance near selector token matching given literal or satisfying given ground predicate an example query is type distance near hamburg munich nearness is defined a a flexible trainable parameterized aggregation function of the selector their frequency in the corpus and their distance from the candidate answer such query provide a key data reduction step for information extraction data integration question answering and other text processing application we describe the architecture of a next generation information retrieval engine for such application and investigate two key technical problem faced in building it first we propose a new algorithm that estimate a scoring function from past log of query and answer span plugging the scoring function into the query processor give high accuracy typically an answer is found at rank second we exploit the skew in the distribution over type seen in query log to optimize the space required by the new index structure required by our system extensive performance study with a gb million document trec corpus and several hundred trec query show both the accuracy and the efficiency of our system from an initial gb index using type from wordnet we can discard of the space while inflating query time by a factor of only our final index overhead is only of the total index space needed 
portlets strive to play at the front end the same role that web service currently enjoy at the back end namely enablers of application assembly through reusable service however it is well known in the component community that the larger the component the more reduced the reuse hence the coarse grained nature of portlets they encapsulate also the presentation layer can jeopardize this vision of portlets a reusable service to avoid this situation this work proposes a perspective shift in portlet development by introducing the notion of consumer profile while the user profile characterizes the end user e g age name etc the consumer profile capture the idiosyncrasy of the organization through which the portlet is being delivered e g the portal owner a far a the portlet functionality is concerned the user profile can be dynamic and hence requires the portlet to be customized at runtime by contrast the consumer profile is known at registration time and it is not always appropriate possible to consider it at runtime rather it is better to customize the code at development time and produce an organization specific portlet which built in custom functionality in this scenario we no longer have a portlet but a family of portlets and the portlet provider becomes the assembly line of this family this work promotes this vision by introducing an organization aware wsrpcompliant architecture that let portlet consumer registry and handle family portlets in the same way that traditional portlets in so doing portlets are nearer to become truly reusable service 
given a set of machine and a set of web application with dynamically changing demand an online application placement controller decides how many instance to run for each application and where to put them while observing all kind of resource constraint this np hard problem ha real usage in commercial middleware product existing approximation algorithm for this problem can scale to at most a few hundred machine and may produce placement solution that are far from optimal when system resource are tight in this paper we propose a new algorithm that can produce within second high quality solution for hard placement problem with thousand of machine and thousand of application this scalability is crucial for dynamic resource provisioning in large scale enterprise data center our algorithm allows multiple application to share a single machine and strivesto maximize the total satisfied application demand to minimize the number of application start and stop and to balance the load across machine compared with existing state of the art algorithm for system with machine or le our algorithm is up to time faster reduces application start and stop by up to and produce placement solution that satisfy up to more application demand our algorithm ha been implemented and adopted in a leading commercial middleware product for managing the performance of web application 
the automatic identification of a user s task ha the potential to improve information filtering system that rely on implicit measure of interest and whose effectiveness may be dependant upon the task at hand knowledge of a user s current task type would allow information filtering system to apply the most useful measure of user interest we recently conducted a field study in which we logged all participant interaction with their web browser and asked participant to categorize their web usage according to a high level task schema using the data collected during this study we have conducted a preliminary exploration of the usefulness of logged web browser interaction to predict user task the result of this initial analysis suggest that individual model of user web browser interaction may be useful in predicting task type 
in this paper we present a novel solution to the image annotation problem which annotates image using search and data mining technology an accurate keyword is required to initialize this process and then leveraging a large scale image database it search for semantically and visually similar image and mine annotation from them a notable advantage of this approach is that it enables unlimited vocabulary while it is not possible for all existing approach experimental result on real web image show the effectiveness and efficiency of the proposed algorithm 
a large fraction of query submitted to web search enginesoccur very infrequently we describe search log studiesaimed at elucidating behavior associated with rare andcommon query we present several analysis and discussresearch direction 
this work address conceptual modeling and automatic code generation for rich internet application a variant of web based system bridging the gap between desktop and web interface the approach we propose is a first step towards a full integration of ria paradigm into the web development process enabling the specification of complex web solution mixing http html and rich internet application using a single modeling language and tool 
evaluating user preference of web search result is crucial for search engine development deployment and maintenance we present a real world study of modeling the behavior of web search user to predict web search result preference accurate modeling and interpretation of user behavior ha important application to ranking click spam detection web search personalization and other task our key insight to improving robustness of interpreting implicit feedback is to model query dependent deviation from the expected noisy user behavior we show that our model of clickthrough interpretation improves prediction accuracy over state of the art clickthrough method we generalize our approach to model user behavior beyond clickthrough which result in higher preference prediction accuracy than model based on clickthrough information alone we report result of a large scale experimental evaluation that show substantial improvement over published implicit feedback interpretation method 
webdav need awareness support in order to be a full fledged collaboration system this paper introduces awaredav a new webdav extension framework enabling shared awareness through event notification by extending the webdav protocol with seven new request method and an extensible xml based event subscription scheme awaredav support fine grained event subscription over a range of transport mechanism and enables a wide range of collaboration scenario this paper describes the design of awaredav it api experience with it initial implementation a well a a comparison with microsoft exchange and webdav notify 
web search using stationary desktop computer ha become a pervasive activity the mobile user in need of information however face several problem in his or her quest to satisfy an information need mobile device have small display and mobile user interface are often le then usable because they impose the desktop web search paradigm on the mobile user we present a wireless search engine based on natural language query transmitted via popular small message service sm text message besides traditional keyword based query the system can accept question or phrase and return response that contain likely answer figure instead of traditional list of hyperlink the additional precision gained from performing a linguistic analysis of the query help extracting answer from web page directly which requires no navigation the system is implemented using a nlir system residing on a server which can translate question or phrase into search engine query or query to soap web service where a gateway mediates between the mobile network and the internet figure whereas on the desktop keyboard based search still prevails we find that in a mobile context question answering technique can help overcome the output constraint 
email are example of structured document with various field these field can be exploited to enhance the retrieval effectiveness of an information retrieval ir system that mailing list archive in recent experiment of the trec enterprise track various field were applied to varying degree of success by the participant in his work using a field based weighting model we investigate the retrieval performance attainable by each field and examine when field evidence should be combined or not 
in this paper we first introduce the notion of gloss for ontology engineering purpose we propose that each vocabulary in an ontology should have a gloss a gloss basically is an informal description of the meaning of a vocabulary that is supposed to render factual and critical knowledge to understanding a concept but that is unreasonable or very difficult to formalize and or articulate formally we present a set of guideline on what should and should not be provided in a gloss second we propose to incorporate linguistic resource in the ontology engineering process we clarify the importance of using lexical resource a a consensus reference in ontology engineering and so enabling the adoption of the gloss found in these resource a linguistic resource i e it list of term and their definition shall be seen a a shared vocabulary space for ontology we present an ontology engineering software tool called dogmamodeler and illustrate it support of reusing of wordnet s term and gloss in ontology modeling 
this paper describes xsqirrel a new xml query language that transforms a document into a sub document i e a tree where the root to leaf path are a subset of the root to leaf path from the original document we show that this type of query is extremely useful for various application e g web service and that the currently existing query language are poorly equipped to express reason and evaluate such query in particular we emphasize the need to be able to compose such query we present the xsqirrel language with it syntax semantics and two language specific operator union and composition for the evaluation of the language we leverage well established query technology by translating xsqirrel expression into xpath program xquery query or xslt stylesheets we provide some experimental result that compare our various evaluation strategy we also show the runtime benefit of query composition over sequential evaluation 
traffic classification is the ability to identify and categorize network traffic by application type in this paper we consider the problem of traffic classification in the network core classification at the core is challenging because only partial information about the flow and their contributor is available we address this problem by developing a framework that can classify a flow using only unidirectional flow information we evaluated this approach using recent packet trace that we collected and pre classified to establish a base truth from our evaluation we find that flow statistic for the server to client direction of a tcp connection provide greater classification accuracy than the flow statistic for the client to server direction because collection of the server to client flow statistic may not always be feasible we developed and validated an algorithm that can estimate the missing statistic froma unidirectional packet trace 
pagerank is defined a the stationary state of a markov chain the chain is obtained by perturbing the transition matrix induced by a web graph with a damping factor that spread uniformly part of the rank the choice of is eminently empirical and in most case the original suggestion by brin and page is still used recently however the behaviour of pagerank with respect to change in wa discovered to be useful in link spam detection moreover an analytical justification of the value chosen for is still missing in this paper we give the first mathematical analysis of pagerank when change in particular we show that contrarily to popular belief for real world graph value of close to do not give a more meaningful ranking then we give closed form formula for pagerank derivative of any order and an extension of the power method that approximates them with convergence o tk t for the k th derivative finally we show a tight connection between iterated computation and analytical behaviour by proving that the k th iteration of the power method give exactly the pagerank value obtained using a maclaurin polynomial of degree k the latter result pave the way towards the application of analytical method to the study of pagerank 
computer aided learning is fast gaining traction in developing region a a mean to augment classroom instruction reason for using computer aided learning range from supplementing teacher shortage to starting underprivileged child off in technology and funding for such initiative range from state education fund to international agency and private group interested in child development the interaction of child with computer is seen at various level from unsupervised self guided learning at public booth without specific curriculum to highly regulated in class computer application with module designed to go with school curriculum such learning is used at various level from child a young a year old to high schoolers this paper us field observation of primary school child in india using computer aided learning module and find pattern by which child who perform better in classroom activity seat themselves in front of computer monitor and control the mouse in case where child are required to share computer resource we find that in such circumstance there emerges a pattern of learning unique to multi user environment wherein certain child tend to learn better because of their control of the mouse this research also show that while computer aided learning software for child is primarily designed for single user the implementation reality of resource strapped learning environment in developing region present a strong case for multi user design 
how can we solve the problem of information overload in news syndication this poster outline the path from keyword based body text matching to distance measurable taxonomic tag matching on to context scale and practical us 
design of high performance web server ha become a recent research thrust to meet the increasing demand of network based service in this paper we propose a new web server architecture called multi threaded pipelined web server suitable for symmetric multi processor smp or system on chip soc architecture the proposed pipelined model consists of multiple thread pool where each thread pool consists of five basic thread and two helper thread the main advantage of the proposed model are global information sharing by the thread minimal synchronization overhead due to le number of thread and non blocking i o operation possible with the helper thread we have conducted an in depth performance analysis of the proposed server model along with four prior web server model multi process mp multi thread mt single process event driven sped and asynchronous multi process event driven amped via simulation using six web server workload the experiment are conducted to investigate the impact of various factor such a the memory size disk speed and number of client the simulation result indicate that the proposed pipelined web server architecture show the best performance across all system and workload parameter compared to the mp mt sped and amped model although the mt and amped model show competitive performance with le number of processor the advantage of the pipelined model becomes obvious a the number of processor or client in an smp soc machine increase the mp model show the worst performance in most of the case the result indicate that the proposed server architecture can be used in future large scale smp soc machine to boost system performance 
we describe elvis the ecosystem location visualization and information system a suite of tool for constructing food web for a given location we express both elvis input and output data in owl thereby enabling it integration with other semantic web resource in particular we describe using a triple shop application to answer sparql query from a collection of semantic web document this is an end to end case study of the semantic web s utility for ecological and environmental research 
the presentation of query biased document snippet a part of result page presented by search engine ha become an expectation of search engine user in this paper we explore the algorithm and data structure required a part of a search engine to allow efficient generation of query biased snippet we begin by proposing and analysing a document compression method that reduces snippet generation time by over a baseline using the zlib compression library these experiment reveal that finding document on secondary storage dominates the total cost of generating snippet and so caching document in ram is essential for a fast snippet generation process using simulation we examine snippet generation performance for different size ram cache finally we propose and analyse document reordering and compaction revealing a scheme that increase the number of document cache hit with only a marginal affect on snippet quality this scheme effectively double the number of document that can fit in a fixed size cache 
web based community have become important place for people to seek and share expertise we find that network in these community typically differ in their topology from other online network such a the world wide web system targeted to augment web based community by automatically identifying user with expertise for example need to adapt to the underlying interaction dynamic in this study we analyze the java forum a large online help seeking community using social network analysis method we test a set of network based ranking algorithm including pagerank and hit on this large size social network in order to identify user with high expertise we then use simulation to identify a small number of simple simulation rule governing the question answer dynamic in the network these simple rule not only replicate the structural characteristic and algorithm performance on the empirically observed java forum but also allow u to evaluate how other algorithm may perform in community with different characteristic we believe this approach will be fruitful for practical algorithm design and implementation for online expertise sharing community 
combining the output from multiple retrieval source over the same document collection is of great importance to a number of retrieval task such a multimedia retrieval web retrieval and meta search to merge retrieval source adaptively according to query topic we propose a series of new approach called probabilistic latent query analysis plqa which can associate non identical combination weight with latent class underlying the query space compared with previous query independent and query class based combination method the proposed approach have the advantage of being able to discover latent query class automatically without using prior human knowledge to assign one query to a mixture of query class and to determine the number of query class under a model selection principle experimental result on two retrieval task i e multimedia retrieval and meta search demonstrate that the proposed method can uncover sensible latent class from training data and can achieve considerable performance gain 
in a new model for answer retrieval document collection are distilled offline into large repository of fact each fact constitutes a potential direct answer to question seeking a particular kind of entity or relation such a question asking about the date of particular event question answering becomes equivalent to online fact retrieval which greatly simplifies the de facto system architecture 
in this paper we propose the integration of intelligent component technology natural language and discourse management in voice web interface to make them smarter we describe how we have integrated reusable component of dialogue management and language processing in a multilingual voice system to improve it friendliness and portability the dialogue management component deal with complex dialogue phenomenon such a user initiative dialogue and follows the information state based theory the resulting dialogue system support friendly communication through the telephone and the web in several language english spanish catalan and italian the dialogue system ha been adapted to guide the user to access online public administration service 
message hierarchy in web discussion board grow with new posting thread of message evolve a new posting focus within or diverge from the original theme of the thread thus just by investigating the subject heading or content of earlier posting in a message thread one may not be able to guess the content of the later posting the resulting navigation problem is further compounded for blind user who need the help of a screen reader program that can provide only a linear representation of the content we see that in order to overcome the navigation obstacle for blind a well a sighted user it is essential to develop technique that help identify how the content of a discussion board grows through generalization and specialization of topic this knowledge can be used in segmenting the content in coherent unit and guiding the user through segment relevant to their navigational goal our experimental result showed that the segmentation algorithm described in this paper provides up to success rate in labeling message the algorithm is being deployed in a software system to reduce the navigational load of blind student in accessing web based electronic course material however we note that the technique are equally applicable for developing web indexing and summarization tool for user with sight 
despite the success of web search engine search over large enterprise intranet still suffers from poor result quality earlier work that compared intranet and the internet from the view point of keyword search ha pointed to several reason why the search problem is quite different in these two domain in this paper we address the problem of providing high quality answer to navigational query in the intranet e g query intended to find product or personal home page service page etc our approach is based on offline identification of navigational page intelligent generation of term variant to associate with each page and the construction of separate index exclusively devoted to answering navigational query using a testbed of m page from the ibm intranet we present evaluation result that demonstrate that for navigational query our approach of using custom index produce result of significantly higher precision than those produced by a general purpose search algorithm 
current web search engine focus on searching only themost recentsnapshot of the web in some case however it would be desirableto search over collection that include many different crawl andversions of each page one important example of such a collectionis the internet archive though there are many others sincethe data size of such an archive is multiple time that of a singlesnapshot this present u with significant performance challenge current engine use various technique for index compression andoptimized query execution but these technique do not exploit thesignificant similarity between different version of a page or betweendifferent page in this paper we propose a general framework for indexing andquery processing of archival collection and more generally anycollections with a sufficient amount of redundancy our approachresults in significant reduction in index size and query processingcosts on such collection and it is orthogonal to and can be combinedwith the existing technique it also support highly efficientupdates both locally and over a network within this framework we describe and evaluate different implementation that trade offindex size versus cpu cost and other factor and discus applicationsranging from archival web search to local search of web site email archive or file system we present experimental resultsbased on search engine query log and a large collection consistingof multiple crawl 
rank aggregation is a pervading operation in ir technology we hypothesize that the performance of score based aggregation may be affected by artificial usually meaningless deviation consistently occurring in the input score distribution which distort the combined result when the individual bias differ from each other we propose a score based rank aggregation model where the source score are normalized to a common distribution before being combined early experiment on available data from several trec collection are shown to support our proposal 
in federated text retrieval system the query is sent to multiple collection at the same time the result returned by collection are gathered and ranked by a central broker that present them to the user it is usually assumed that the collection have little overlap however in practice collection may share many common document a either exact or near duplicate potentially leading to high number of duplicate in the final result considering the natural band width restriction and efficiency issue of federated search sendingqueries to redundant collection lead to unnecessary cost we propose a novel method for estimating the rate of over lap among collection based on sampling then using theestimated overlap statistic we propose two collection selection method that aim to maximize the number of unique relevant document in the final result we show experimentally that although our estimate of overlap are not in exact our suggested technique can significantly improve the search effectiveness when collection overlap 
in this paper we present a novel filtering system based on a new model which reshapes the aim of content based filtering the filtering system ha been developed within the ec project peng aimed at providing news professional such a journalist with a system supporting both filtering and retrieval capability in particular we suggest that in tackling the problem of information overload it is necessary for filtering system to take into account multiple aspect of incoming document in order to estimate their relevance to a user s profile and in order to help user better understand document a distinct from solely attempting to either select relevant material from a stream or block inappropriate material aiming to so this a filtering model based on multiple criterion ha been defined based on the idea gleamed in the project requirement stage the filtering model is briefly described in this paper 
semantic caching is an important technology for improving the response time of future user query specified over remote server this paper deal with the fundamental query containment problem in an xquery based semantic caching system to our best knowledge the impact of subtle difference in xquery semantics caused by different way of specifying variable on query containment ha not yet been studied we introduce the concept of variable binding dependency for representing the hierarchical element dependency preserved by an xquery we analyze the problem of xquery containment in the presence of such dependency we propose a containment mapping technique for nested xquery in presence of variable binding dependency the implication of the nested block structure on xquery containment is also considered we mention the performance gain achieved by a semantic caching system we build based on the proposed technique 
classification of document by genre is typically done either using linguistic analysis or term frequency based technique the former provides better classification accuracy than the latter but at the cost of two order of magnitude more computation time while term frequency analysis requires much le computational resource than linguistic analysis it return poor classification accuracy when the genre are not sufficiently distinct a method that remove or approximates the expensive portion of linguistic analysis is presented the accuracy and computation time of this method then compared with both linguistic analysis and term frequency analysis the result in this paper show that this method can significantly reduce the computation of both time of linguistic analysis and term frequency analysis while retaining an accuracy that is higher than that of term frequency analysis 
ontology are at the heart of the semantic web they define the concept and relationship that make global interoperability possible however a these ontology grow in size they become more and more difficult to create use understand maintain transform and classify we present and evaluate several algorithm for extracting relevant segment out of large description logic ontology for the purpose of increasing tractability for both human and computer the segment are not mere fragment but stand alone a ontology in their own right this technique take advantage of the detailed semantics captured within an owl ontology to produce highly relevant segment the research wa evaluated using the galen ontology of medical term and procedure 
given a large collection of sparse vector data in a high dimensional space we investigate the problem of finding all pair of vector whose similarity score a determined by a function such a cosine distance is above a given threshold we propose a simple algorithm based on novel indexing and optimization strategy that solves this problem without relying on approximation method or extensive parameter tuning we show the approach efficiently handle a variety of datasets across a wide setting of similarity threshold with large speedup over previous state of the art approach 
given a document repository search engine is very helpful to retrieve information currently vertical search is a hot topic and google scholar is an example for academic search however most vertical search engine only return the flat ranked list without an efficient result exhibition for given user we study this problem and designed a vertical search engine prototype dolphin where the flexible user oriented template can be defined and the survey like result are presented according to the template 
the world wide web contains rich textual content that areinterconnected via complex hyperlink this huge database violates the assumption held by most of conventional statistical method that each web page is considered a an independent and identical sample it is thus difficult to apply traditional mining or learning method for solving web mining problem e g web page classification by exploiting both the content and the link structure the research in this direction ha recently received considerable attention but are still in an early stage though a few method exploit both the link structure or the content information some of them combine the only authority information with the content information and the others first decompose the link structure into hub and authority feature then apply them a additional document feature being practically attractive for it great simplicity this paper aim to design an algorithm that exploit both the content and linkage information by carrying out a joint factorization on both the linkage adjacency matrix and the document term matrix and derives a new representation for web page in a low dimensional factor space without explicitly separating them a content hub or authority factor further analysis can be performed based on the compact representation of web page in the experiment the proposed method is compared with state of the art method and demonstrates an excellent accuracy in hypertext classification on the webkb and cora benchmark 
this paper present result of a long term client side web usage study updating previous study that range in age from five to ten year we focus on three aspect of web navigation change in the distribution of navigation action speed of navigation and within page navigation navigation action corresponding to user individual page request are discussed by type we reconfirm link to be the most important navigation element while backtracking ha lost more than half of it previously reported share and form submission ha become far more common change of the web and the browser interface are candidate for causing these change analyzing the time user stayed on page we confirm web navigation to be a rapidly interactive activity a breakdown of page characteristic show that user often do not take the time to read the available text or consider all link the performance of the web is analyzed and reassessed against the resulting requirement finally habit of within page navigation are presented although most selected hyperlink are located in the top left corner of the screen in nearly a quarter of all case people choose link that require scrolling we analyzed the available browser real estate to gain insight for the design of non scrolling web page 
since the early day of the web identity management ha been a big issue a the famous cartoon from the new yorker reminds u on the internet nobody know you are a dog this wa true back in july this is true today for the last few year numerous initiative have emerged to tackle this issue microsoft passport liberty alliance gpp gup shibboleth to name a few major investment are being made in this area and this is foreseen a a multi billion dollar market yet a of this writing there is still no widespread identity management infrastructure in place ready to be used by the general public on converged network the goal of this panel is to do a reality check and try to answer the following five question what is identity management who need identity management and why what will the identity management ecosystem look like what s agreed upon what s next 
we propose a method for document ranking that combine a simple document centric view of text and fast evaluation strategy that have been developed in connection with the vector space model the new method defines the importance of a term within a document qualitatively rather than quantitatively and in doing so reduces the need for tuning parameter in addition the method support very fast query processing with most of the computation carried out on small integer and dynamic pruning an effective option experiment on a wide range of trec data show that the new method provides retrieval effectiveness a good a or better than the okapi bm formulation and variant of language model 
we describe a method for improving the precision of metasearch result based upon scoring the visual feature of document surrogate representation these surrogate score are used during fusion in place of the original score or rank provided by the underlying search engine visual feature are extracted from typical search result surrogate information such a title snippet url and rank this approach specifically avoids the use of search engine specific score and collection statistic that are required by most traditional fusion strategy this restriction correctly reflects the use of metasearch in practice in which knowledge of the underlying search engine strategy cannot be assumed we evaluate our approach using a precision oriented test collection of manually constructed binary relevance judgment for the top ten result from ten web search engine over query we show that our visual fusion approach significantly outperforms the rcombmnz fusion algorithm by with confidence and the best individual web search engine by with confidence 
xml extensible markup language processing can incur significant runtime overhead in xml based infrastructural middleware such a web service application server this paper proposes a novel mechanism for efficiently processing similar xml document given a new xml document a a byte sequence the xml parser proposed in this paper normally avoids syntactic analysis but simply match the document with previously processed one reusing those result our parser is adaptive since it partially par and then remembers xml document fragment that it ha not met before moreover it process safely since it partial parsing correctly check the well formedness of document our implementation of the proposed parser complies with the jsr standard of the java api for xml processing jaxp specification we evaluated deltarser performance with message using google web service comparing to piccolo and apache xerces it effectively par faster in a server side use case scenario and faster in a client side use case scenario 
active learning efficiently hone in on the decision boundary between relevant and irrelevant document but in the process can miss entire cluster of relevant document yielding classifier with low recall in this paper we propose a method to increase active learning recall by constraining sampling to a document subset rich in relevant example 
traditional text classification studied in the ir literature is mainly based on topic that is each class or category represents a particular topic e g sport politics or science however many real world text classification problem require more refined classification based on some semantic aspect for example in a set of document about a particular disease some document may report the outbreak of the disease some may describe how to cure the disease some may discus how to prevent the disease and yet some others may include all the above information to classify text at this semantic level the traditional bag of word model is no longer sufficient in this paper we report a text classification study at the semantic level and show that sentence semantic and structure feature are very useful for such kind of classification our experimental result based on a disease outbreak dataset demonstrated the effectiveness of the proposed approach 
we consider the problem of indexing high dimensional data for answering approximate similarity search query similarity index prove to be important in a wide variety of setting web search engine desire fast parallel main memory based index for similarity search on text data database system desire disk based similarity index for high dimensional data including text and image peer to peer system desire distributed similarity index with low communication cost we propose an indexing scheme called lsh forest which is applicable in all the above context our index us the well known technique of locality sensitive hashing lsh but improves upon previous design by a eliminating the different data dependent parameter for which lsh must be constantly hand tuned and b improving on lsh s performance guarantee for skewed data distribution while retaining the same storage and query overhead we show how to construct this index in main memory on disk in parallel system and in peer to peer system we evaluate the design with experiment on multiple text corpus and demonstrate both the self tuning nature and the superior performance of lsh forest 
we consider classification of email message a to whether or not they contain certain email act such a a request or a commitment we show that exploiting the sequential correlation among email message in the same thread can improve email act classification more specifically we describe a new text classification algorithm based on a dependency network based collective classification method in which the local classifier are maximum entropy model based on word and certain relational feature we show that statistically significant improvement over a bag of word baseline classifier can be obtained for some but not all email act class performance improvement obtained by collective classification appears to be consistent across many email act suggested by prior speech act theory 
in this paper we describe a system that can extract recordstructures from web page with no direct human supervision record are commonly occurring html embedded data tuples that describe people offered course product company profile etc we present a simplified frameworkfor studying the problem of unsupervised record extraction one which separate the algorithm from the feature engineering our system u rest formalizes an approach tothe problem of unsupervised record extraction using a simple two stage machine learning framework the first stage involves clustering where structurally similar region are discovered and the second stage involves classification where discovered grouping cluster of region are ranked by their likelihood of being record in our work we describe and summarize the result of an extensive survey of feature for both stage we conclude by comparing u rest to related system the result of our empirical evaluation show encouraging improvement in extraction accuracy 
