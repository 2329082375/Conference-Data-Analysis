in conventional supervised learning one search for vertical pattern coupling input directly to output one can instead search for horizontal pattern which go across the input space coupling output value on one part of the input space with output value on another one way to do this is to pre process the problem in a manner similar to the embedding process of non linear time series analysis the training set produced by this pre processing is constructed solely from the output component of the original training set the input component of the original training set are used in concert with cross validation to determine the detail of the processing of those output component this paper present three set of experiment of the efficacy of such pre processing involving numerical boolean and visual task the first set involves small bit problem in all but one of the experiment in that set the pre processing improved the generalization performance often inducing perfect generalization the average ratio of the generalization error rate with the pre processing to the error rate without it is the second set of experiment involve a bit input space for a number of different target function a training set is actively constructed by sampling the target function at pre specified point in that space a decisiondirected version of the pre processing is then used to extrapolate from that training set to the remaining point in the input space the average error rate across here is the third set of experiment is a variation of the robot arm problem recently investigated by mackay here the rms error rate for extrapolation wa only time the noise level 
machine discovery is concerned with the taskof finding law from experimental and or observationaldata existing machine discoverysystems have mostly generated law describingstatic situation the paper present lagrange a system that construct a set ofdifferential and or algebraic equation thatdescribe an observed behavior of a dynamicsystem a such lagrange extends thescope of machine discovery to dynamic system we show that lagrange is ableto generate appropriate 
many learning situation involve multiple set of training example drawn from different but related underlying model family dis covery is the task of discovering a parameterized family of model from this kind of training set the task naturally arises in density estimation classification regression manifold learning reinforce ment learning clustering hmm learning and other setting we describe three family discovery algorithm which are based on tech niques for manifold learning we compare these algorithm on a classification task against two alternative approach and find sig nificant performance improvement 
we discus the use of database method for data mining recently impressive result have been achieved for some data mining problem using highly specialized and clever data structure we study how well one can manage by using general purpose database management system we illustrate our idea by investigating the use of a dbms for a well researched area the discovery of association rule we present a simple algorithm consisting of only union and intersection operation and show that it achieves quite good performance on an efficient dbms our method can incorporate inheritance hierarchy to the association rule algorithm easily we also present a technique that effectively reduces the number of database operation when searching large search space that contain only few interesting item our work show that database technique are promising for data mining general architecture can achieve reasonable result 
a new boosting algorithm of freund and schapire is used to improve the performance of decision tree which are constructed usin the information ratio criterion of quinlan s c algorithm this boosting algorithm iteratively construct a series of decision tress each decision tree being trained and pruned on example that have been filtered by previously trained tree example that have been incorrectly classified by the previous tree in the ensemble are resampled with higher probability to give a new probability distribution for the next ace in the ensemble to tnin on result from optical cha xc er reco tion ocr and knowledge discovery and data mining problem show that in comparison to single tree or to tree trained independenrly or to tree trained on subset of the feature space the boosring ensemble is much better 
we introduce an active data mining paradigm thatcombines the recent work in data mining with therich literature on active database system in thisparadigm data is continuously mined at a desiredfrequency a rule are discovered they are addedto a rulebase and if they already exist the historyof the statistical parameter associated withthe rule is updated when the history start exhibitingcertain trend specified a shape queriesin the user specified trigger the trigger are 
we present a silicon model of an axon which show promise a abuilding block for pulse based neural computation involving correlationsof pulse across both space and time the circuit sharesa number of feature with it biological counterpart including anexcitation threshold a brief refractory period after pulse completion pulse amplitude restoration and pulse width restoration weprovide a simple explanation of circuit operation and present datafrom a chip fabricated in a standard 
many real world domain bless u with a wealth of attribute to use for learning this blessing is often a curse most inductive method generalize worse given too many attribute than if given a good subset of those attribute we examine this problem for two learning task taken from a calendar scheduling domain we show that id c generalizes poorly on these task if allowed to use all available attribute we examine five greedy hillclimbing procedure that search for attribute set that generalize well with id c experiment suggest hillclimbing in attribute space can yield substantial improvement in generalization performance we present a caching scheme that make attribute hillclimbing more practical computationally we also compare the result of hillclimbing in attribute space with focus and relief on the two task 
the facial action coding system facs devised by ekman andfriesen provides an objective mean for measuring the facialmuscle contraction involved in a facial expression in this paper we approach automated facial expression analysis by detecting andclassifying facial action we generated a database of over image sequence of subject performing over distinct facialactions or action combination we compare three different approachesto classifying the facial 
we explore algorithm for learning classification procedure that attempt to minimize the cost of misclassifying example first we consider inductive learning of classificatio n rule the reduced cost ordering algorithm a new method for creating a decision list i e an ordered set of rule is described and compared to a variety of inductive learning approach next we describe approach that attempt to minimize cost while avoiding overfitting and introduce the clause prefix method for pruning decision list finally we consider reducing misclassification cost when a prior domain theory is available 
topographic mapping occur frequently in the brain a popular approach to understanding the structure of such mapping is to map point representing input feature in a space of a few dimension to point in a dimensional space using some selforganizing algorithm we argue that a more general approach may be useful where similarity between feature are not constrained to be geometric distance and the objective function for topographic matching is chosen explicitly rather than being specified implicitly by the self organizing algorithm we investigate analytically an example of this more general approach applied to the structure of interdigitated mapping such a the pattern of ocular dominance column in primary visual cortex 
abstract 
in this paper the problem of learning appropriate domain specificbias is addressed it is shown that this can be achieved by learningmany related task from the same domain and a theorem is givenbounding the number task that must be learnt a corollary of thetheorem is that if the task are known to posse a common internalrepresentation or preprocessing then the number of examplesrequired per task for good generalisation when learning n task simultaneouslyscales like o a bn 
we introduce a new algorithm designed to learn sparse perceptrons over input representation which include high order feature our algorithm which is based on a hypothesis boosting method is able to pac learn a relatively natural class of target concept moreover the algorithm appears to work well in practice on a set of three problem domain the algorithm produce classiflers that utilize small number of feature yet exhibit good generalization performance perhaps most importantly our algorithm generates concept description that are easy for human to understand 
we present a new algorithm for finding low complexity network with high generalization capability the algorithm search for large connected region of so called quot flat quot minimum of the error function in the weight space environment of a quot flat quot minimum the error remains approximately constant using an mdl based argument flat minimum can be shown to correspond to low expected overfitting although our algorithm requires the computation of second order derivative it ha backprop s order of 
while exploring to find better solution an agent performing onlinereinforcement learning rl can perform worse than is acceptable in some case exploration might have unsafe or even catastrophic result often modeled in term of reaching failure statesof the agent s environment this paper present a method that usesdomain knowledge to reduce the number of failure during exploration this method formulates the set of action from which therl agent composes a control policy 
to appear in g tesauro d s touretzky and t k leen ed advance in neuralinformation processing system mit press cambridge ma a straightforward approach to the curse of dimensionality in reinforcementlearning and dynamic programming is to replace thelookup table with a generalizing function approximator such a a neuralnet although this ha been successful in the domain of backgammon there is no guarantee of convergence in this paper we showthat the combination of 
the hypothesis selection problem or the k armed bandit problem is central to the realizationof many learning system thispaper study the minimization of samplingcost in hypothesis selection under a probablyapproximately optimal pao learningframework hypothesis selection algorithmscould be exploration oriented or exploitationoriented 
this paper present an alternating minimization algorithm used to train radial basisfunction network the algorithm is a modification of an interior point method used insolving primal linear program the resulting algorithm is shown to have a convergencerate on the order ofpnl iteration where n is a measure of the network size and l isa measure of the resulting solution s accuracy introductionin recent year considerable research ha investigated the use of alternating 
this paper present the hdg learning algorithm which us a hierarchical decomposition of the state space to make learning to achieve goal more efficient with a small penalty in path quality special care must be taken when performing hierarchical planning and learning in stochastic domain because macro operator cannot be executed ballistically the hdg algorithm which is a descendent of watkins q learning algorithm is described here and preliminary empirical result are presented 
we have analyzed the relationship between correlated spike count and the peak in the cross correlation of spike train for pair of simultaneously recorded neuron from a previous study of area mt in the macaque monkey zohary et al we conclude that common input responsible for creating peak on the order of ten millisecond wide in the spike train cross correlograms ccgs is also responsible for creating the correlation in spike count observed at the two second time scale of the trial we argue that both common excitation and inhibition may play signican t role in establishing this correlation 
we propose strategy for selecting a good neural network architecture for modeling any specificdata set our approach involves efficiently searching the space of possible architecturesand selecting a quot best quot architecture based on estimate of generalization performance sincean exhaustive search over the space of architecture is computationally infeasible we proposeheuristic strategy which dramatically reduce the search complexity these employ directedsearch algorithm including 
in conventional supervised learning one search for vertical pattern coupling input directly to output one can instead search for horizontal pattern which go across the input space coupling output value on one part of the input space with output value on another one way to do this is to pre process the problem in a manner similar to the embedding process of non linear time series analysis the training set produced by this pre processing is constructed solely from the output component of the original training set the input component of the original training set are used in concert with cross validation to determine the detail of the processing of those output component this paper present three set of experiment of the efficacy of such pre processing involving numerical boolean and visual task the first set involves small bit problem in all but one of the experiment in that set the pre processing improved the generalization performance often inducing perfect generalization the average ratio of the generalization error rate with the pre processing to the error rate without it is the second set of experiment involve a bit input space for a number of different target function a training set is actively constructed by sampling the target function at pre specified point in that space a decisiondirected version of the pre processing is then used to extrapolate from that training set to the remaining point in the input space the average error rate across here is the third set of experiment is a variation of the robot arm problem recently investigated by mackay here the rms error rate for extrapolation wa only time the noise level 
machine discovery is concerned with the taskof finding law from experimental and or observationaldata existing machine discoverysystems have mostly generated law describingstatic situation the paper present lagrange a system that construct a set ofdifferential and or algebraic equation thatdescribe an observed behavior of a dynamicsystem a such lagrange extends thescope of machine discovery to dynamic system we show that lagrange is ableto generate appropriate 
many learning situation involve multiple set of training example drawn from different but related underlying model family dis covery is the task of discovering a parameterized family of model from this kind of training set the task naturally arises in density estimation classification regression manifold learning reinforce ment learning clustering hmm learning and other setting we describe three family discovery algorithm which are based on tech niques for manifold learning we compare these algorithm on a classification task against two alternative approach and find sig nificant performance improvement 
we discus the use of database method for data mining recently impressive result have been achieved for some data mining problem using highly specialized and clever data structure we study how well one can manage by using general purpose database management system we illustrate our idea by investigating the use of a dbms for a well researched area the discovery of association rule we present a simple algorithm consisting of only union and intersection operation and show that it achieves quite good performance on an efficient dbms our method can incorporate inheritance hierarchy to the association rule algorithm easily we also present a technique that effectively reduces the number of database operation when searching large search space that contain only few interesting item our work show that database technique are promising for data mining general architecture can achieve reasonable result 
a new boosting algorithm of freund and schapire is used to improve the performance of decision tree which are constructed usin the information ratio criterion of quinlan s c algorithm this boosting algorithm iteratively construct a series of decision tress each decision tree being trained and pruned on example that have been filtered by previously trained tree example that have been incorrectly classified by the previous tree in the ensemble are resampled with higher probability to give a new probability distribution for the next ace in the ensemble to tnin on result from optical cha xc er reco tion ocr and knowledge discovery and data mining problem show that in comparison to single tree or to tree trained independenrly or to tree trained on subset of the feature space the boosring ensemble is much better 
we introduce an active data mining paradigm thatcombines the recent work in data mining with therich literature on active database system in thisparadigm data is continuously mined at a desiredfrequency a rule are discovered they are addedto a rulebase and if they already exist the historyof the statistical parameter associated withthe rule is updated when the history start exhibitingcertain trend specified a shape queriesin the user specified trigger the trigger are 
we present a silicon model of an axon which show promise a abuilding block for pulse based neural computation involving correlationsof pulse across both space and time the circuit sharesa number of feature with it biological counterpart including anexcitation threshold a brief refractory period after pulse completion pulse amplitude restoration and pulse width restoration weprovide a simple explanation of circuit operation and present datafrom a chip fabricated in a standard 
many real world domain bless u with a wealth of attribute to use for learning this blessing is often a curse most inductive method generalize worse given too many attribute than if given a good subset of those attribute we examine this problem for two learning task taken from a calendar scheduling domain we show that id c generalizes poorly on these task if allowed to use all available attribute we examine five greedy hillclimbing procedure that search for attribute set that generalize well with id c experiment suggest hillclimbing in attribute space can yield substantial improvement in generalization performance we present a caching scheme that make attribute hillclimbing more practical computationally we also compare the result of hillclimbing in attribute space with focus and relief on the two task 
the facial action coding system facs devised by ekman andfriesen provides an objective mean for measuring the facialmuscle contraction involved in a facial expression in this paper we approach automated facial expression analysis by detecting andclassifying facial action we generated a database of over image sequence of subject performing over distinct facialactions or action combination we compare three different approachesto classifying the facial 
we explore algorithm for learning classification procedure that attempt to minimize the cost of misclassifying example first we consider inductive learning of classificatio n rule the reduced cost ordering algorithm a new method for creating a decision list i e an ordered set of rule is described and compared to a variety of inductive learning approach next we describe approach that attempt to minimize cost while avoiding overfitting and introduce the clause prefix method for pruning decision list finally we consider reducing misclassification cost when a prior domain theory is available 
topographic mapping occur frequently in the brain a popular approach to understanding the structure of such mapping is to map point representing input feature in a space of a few dimension to point in a dimensional space using some selforganizing algorithm we argue that a more general approach may be useful where similarity between feature are not constrained to be geometric distance and the objective function for topographic matching is chosen explicitly rather than being specified implicitly by the self organizing algorithm we investigate analytically an example of this more general approach applied to the structure of interdigitated mapping such a the pattern of ocular dominance column in primary visual cortex 
abstract 
in this paper the problem of learning appropriate domain specificbias is addressed it is shown that this can be achieved by learningmany related task from the same domain and a theorem is givenbounding the number task that must be learnt a corollary of thetheorem is that if the task are known to posse a common internalrepresentation or preprocessing then the number of examplesrequired per task for good generalisation when learning n task simultaneouslyscales like o a bn 
we introduce a new algorithm designed to learn sparse perceptrons over input representation which include high order feature our algorithm which is based on a hypothesis boosting method is able to pac learn a relatively natural class of target concept moreover the algorithm appears to work well in practice on a set of three problem domain the algorithm produce classiflers that utilize small number of feature yet exhibit good generalization performance perhaps most importantly our algorithm generates concept description that are easy for human to understand 
we present a new algorithm for finding low complexity network with high generalization capability the algorithm search for large connected region of so called quot flat quot minimum of the error function in the weight space environment of a quot flat quot minimum the error remains approximately constant using an mdl based argument flat minimum can be shown to correspond to low expected overfitting although our algorithm requires the computation of second order derivative it ha backprop s order of 
while exploring to find better solution an agent performing onlinereinforcement learning rl can perform worse than is acceptable in some case exploration might have unsafe or even catastrophic result often modeled in term of reaching failure statesof the agent s environment this paper present a method that usesdomain knowledge to reduce the number of failure during exploration this method formulates the set of action from which therl agent composes a control policy 
to appear in g tesauro d s touretzky and t k leen ed advance in neuralinformation processing system mit press cambridge ma a straightforward approach to the curse of dimensionality in reinforcementlearning and dynamic programming is to replace thelookup table with a generalizing function approximator such a a neuralnet although this ha been successful in the domain of backgammon there is no guarantee of convergence in this paper we showthat the combination of 
the hypothesis selection problem or the k armed bandit problem is central to the realizationof many learning system thispaper study the minimization of samplingcost in hypothesis selection under a probablyapproximately optimal pao learningframework hypothesis selection algorithmscould be exploration oriented or exploitationoriented 
this paper present an alternating minimization algorithm used to train radial basisfunction network the algorithm is a modification of an interior point method used insolving primal linear program the resulting algorithm is shown to have a convergencerate on the order ofpnl iteration where n is a measure of the network size and l isa measure of the resulting solution s accuracy introductionin recent year considerable research ha investigated the use of alternating 
this paper present the hdg learning algorithm which us a hierarchical decomposition of the state space to make learning to achieve goal more efficient with a small penalty in path quality special care must be taken when performing hierarchical planning and learning in stochastic domain because macro operator cannot be executed ballistically the hdg algorithm which is a descendent of watkins q learning algorithm is described here and preliminary empirical result are presented 
we have analyzed the relationship between correlated spike count and the peak in the cross correlation of spike train for pair of simultaneously recorded neuron from a previous study of area mt in the macaque monkey zohary et al we conclude that common input responsible for creating peak on the order of ten millisecond wide in the spike train cross correlograms ccgs is also responsible for creating the correlation in spike count observed at the two second time scale of the trial we argue that both common excitation and inhibition may play signican t role in establishing this correlation 
we propose strategy for selecting a good neural network architecture for modeling any specificdata set our approach involves efficiently searching the space of possible architecturesand selecting a quot best quot architecture based on estimate of generalization performance sincean exhaustive search over the space of architecture is computationally infeasible we proposeheuristic strategy which dramatically reduce the search complexity these employ directedsearch algorithm including 
integrated mean squared error imse is a version of the usual mean squared errorcriterion averaged over all possible training set of a given size if it could be observed it could be used to determine optimal network complexity or optimal data subset forefficient training we show that two common method of cross validating average squarederror deliver unbiased estimate of imse converging to imse with probability one theseestimates thus make possible approximate imse based choice of 
inductive logic programming ilp involves the construction of first order definite clause theory from example and background knowledge unlike both traditional machine learning and computational learning theory ilp is based on lock step development of theory implementation and application ilp system have successful application in the learning of structure activity rule for drug design semantic grammar rule finite element mesh design rule and rule for prediction of protein structure and mutagenic molecule the strong application in ilp can be contrasted with relatively weak pac learning result even highly restricted form of logic program are known to be prediction hard it ha been recently argued that the mismatch is due to distributional assumption made in application domain these assumption can be modelled a a bayesian prior probability representing subjective degree of belief other author have argued for the use of bayesian prior distribution for reason different to those here though this ha not lead to a new model of polynomial time learnability incorporation of bayesian prior distribution over time bounded hypothesis in pac lead to a new model called u learnability it is argued that u learnability is more appropriate than pac for universal turing computable language time bounded logic program have been shown to be polynomially u learnable under certain distribution the use of time bounded hypothesis enforces decidability and allows a unified characterization of speed up learning and inductive learning u learnability ha a special case pac and natarajan s model of speed up learning 
many existing rule learning system are computationally expensive on large noisy datasets in this paper we evaluate the recently proposed rule learning algorithm irep on a large and diverse collection of benchmark problem we show that while irep is extremely efficient it frequently give error rate higher than those of c and c rule we then propose a number of modification resulting in an algorithm ripperk that is very competitive with c rule with respect to error rate but much more efficient on large sample ripperk obtains error rate lower than or equivalent to c rule on of benchmark problem scale nearly linearly with the number of training example and can efficiently process noisy datasets containing hundred of thousand of example 
planning system often make the assumption that omniscient world knowledge isavailable our approach make the more realistic assumption that the initial knowledgeabout the action is incomplete and us experimentation a a learning mechanismwhen the missing knowledge cause an execution failure previous work on learning byexperimentation ha not addressed the issue of how to choose good experiment andmuch research on learning from failure ha relied on background knowledge to build 
in this paper we introduce new algorithm for optimizing noisyplants in which each experiment is very expensive the algorithmsbuild a global non linear model of the expected output at the sametime a using bayesian linear regression analysis of locally weightedpolynomial model the local model answer query about confidence noise gradient and hessian and use them to make automateddecisions similar to those made by a practitioner of responsesurface methodology the global and local 
we describe a new representation for learningconcepts that differs from the traditional decisiontree and rule approach this representation called prototypical concept description can represent several prototype for aconcept we also describe pl our algorithmfor learning these prototype and demonstratethat prototypical concept descriptionscan in some situation classify more accuratelythan standard machine learning algorithm more importantly we show that theyyield 
this paper outline some problem that may occur with reduced errorpruning in inductive logic programming most notably efficiency thereaftera new method incremental reduced error pruning is proposed thatattempts to address all of these problem experiment show that in manynoisy domain this method is much more efficient than alternative algorithm along with a slight gain in accuracy however the experimentsshow a well that the use of this algorithm cannot be recommended for 
deformable model are an attractive approach to recognizing nonrigidobjects which have considerable within class variability however there are severe search problem associated with fitting themodels to data we show that by using neural network to providebetter starting point the search time can be significantly reduced the method is demonstrated on a character recognition task in previous work we have developed an approach to handwritten character recognitionbased on the use of 
simard lecun amp denker showed that the performanceof near neighbor classification scheme for handwritten characterrecognition can be improved by incorporating invariance to specifictransformations in the underlying distance metric the socalled tangent distance the resulting classifier however can beprohibitively slow and memory intensive due to the large amountofprototypes that need to be stored and used in the distance comparison in this paper wedevelop rich 
abstract the important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining technique on a massive scale advance in parallel supercomputing technology enabling high resolution modeling a well a in sensor technology allowing data capture on an unprecedented scale conspire to overwhelm present day analysis approach we present here early experience with a prototype exploratory data analysis environment conquest designed to provide content based access to such massive scientific datasets conquest content based querying in space and time employ a combination of workstation and massively parallel processor mpp s to mine geophysical datasets possessing a prominent tempord component it is designed to enable complex multi modal interactive querying and knowledge discovery while simultaneously coping with the extraordinary computational demand posed by the scope of the datasets involved af 
we propose a learning algorithm for a variable memory lengthmarkov process human communication whether given a text handwriting or speech ha multi characteristic time scale onshort scale it is characterized mostly by the dynamic that generatethe process whereas on large scale more syntactic and semanticinformation is carried for that reason the conventionallyused fixed memory markov model cannot capture effectively thecomplexity of such structure on the other hand using 
we describe the reinforcement learning problem motivate algorithmswhich seek an approximation to the q function and presentnew convergence result for two such algorithm introduction and backgroundimagine an agent acting in some environment at time t the environment is in somestate x t chosen from a finite set of state the agent perceives x t and is allowed tochoose an action a t from some finite set of action the environment then changesstate so that at time t it is 
additive fashion like in mycin type expert system shortliffe sh the main idea is to allow the abthis paper present decision committee a sence of an underlying rdering in the decision p rodecision committee contains rule each of cedure unlike t e ordering of liter l s m a decision these beeing a couple monomial vector tree or the ordering of rule m a decision list and to each monomial is a condition that when take advantage of multiple knowledge gam gam matched by an instance return it vector kononenko and kovacic kk when each monomi l is tested the sum of decision committee may be viewed a a generalizat e re urned ctors is used to take the clastion of threshold function bruck bru in the sification decision we show h t for eve y multiclass case they are also a very special case constant k the subclass of decision commltof neural network where there would be constraint tee whose element have monomial of length over the activation function and the network architec k is pac learnable and that it properly conture the idea of combining the decision of rule is not tains k dl howe ver we also show th t the new see for example nilsson nii bongard bon problem of mducmg the shortest consistent quinqueton and sallantin q cestnik and bratko decision c mmittee is np hard t s lead cb gam gam gascuel gas kononenko to theore tlcal re llts o non learnabii ty and and kovacic kk decision committee can be to negative consideration for practical opviewed a a formalism that allows to express class of timization proble s on dec sion comm ittees concept using shared knowledge such a sum of disa two stage heuristic algori hm idc is pretribution voting method of kononenko and kovacic sen e that lea n by a particular subclass of kk voting method of gascuel gas our aim decision committee it first chooses monois to use this formalization to provide theoretical pacmials by a breadth first earch inspire d from learn ability result regarding class of decision combranch and bound algorithm then it clusmittees having practical and theoretical relevance ters gradually the resulting rule to form deci sion committee according to the minimizaone essenti al problem m s ystems that use shared tion of empirical risk finally it selects the knowledge is how to combme rule t class f exdecision committee over the final population amples if we restrict the monomials to smgle which is the best according to the learnin mo otono l te al this pro blem is s bsum d by that sample experimental result on artiof linear discrimmation whlthout this restrictlo n t e ficial and real domain tend to show that problem appears to be harder and a good practical idc achieves good result while constructing lustr i on of it d ifliculty is given by kononen o and small and interpretable decision committee ovaclc kk nd ed they observe hat their voting and sum of distribution method give bad or unstable result we propose a two stage algorithm 
differentiation between the node of a competitive learning network is conventionally achieved through competition on the basis of neural activity simple inhibitory mechanism are limited to sparse representation while decorrelation and factorization scheme that support distributed representation are computationally unattractive by letting neural plasticity mediate the competitive interaction instead we obtain diffuse nonadaptive alternative for fully distributed representation we use this technique to simplify and improve our binary information gain optimization algorithm for feature extraction schraudolph and sejnowski the same approach could be used to improve other learning algorithm 
we present a method for learning heuristic employed by an automated proverto control it inference machine the hub of the method is the adaptation of theparameters of a heuristic adaptation is accomplished by a genetic algorithm the necessary guidance during the learning process is provided by a proof problemand a proof of it found in the past the objective of learning consists infinding a parameter configuration that avoids redundant effort w r t this problemand the particular proof 
the problem of interpolating between specified image in an image sequence is a simple but important task in model based vision we describe an approach based on the abstract task of manifold learning and present result on both synthetic and real image se quences this problem arose in the development of a combined lip reading and speech recognition system 
we propose a way of using boolean circuit to perform real valuedcomputation in a way that naturally extends their boolean functionality the functionality of multiple fan in threshold gate inthis model is shown to mimic that of a hardware implementationof continuous neural network a vapnik chervonenkis dimensionand sample size analysis for the system is performed giving bestknown sample size for a real valued neural network experimentalresults confirm the conclusion that the sample 
this paper compare two method for refininguncertain knowledge base using propositionalcertainty factor rule the firstmethod implemented in the rapture system employ neural network training to refinethe certainty of existing rule but usesa symbolic technique to add new rule thesecond method based on the one used inthe kbann system initially add a completeset of potential new rule with very low certaintyand allows neural network training tofilter and adjust these rule 
a practical method for bayesian training of feed forward neural network using sophisticated monte carlo method is presented and evaluated in reasonably small amount of computer time this approach outperforms other state of the art method on datalimited task from real world domain 
finding and removing outlier is an important problem in data mining error in large database can be extremely common so an important property of a data mining algorithm is robustness with respect to error in the database most sophisticated method in machine learning address this problem to some extent but not fully and can be improved by addressing the problem more directly in this paper we examine c a decision tree algorithm that is already quite robust few algorithm have been shown to consistently achieve higher accuracy c incorporates a pruning scheme that partially address the outfier removal problem in our robust c algorithm we extend the pruning method to fully remove the effect of outlier and this result in improvement on many database 
we describe a case study in data mining for personal loan evaluation performed at the abn amro bank in the netherlands historical data of client and their pay back behaviour are used to learn to predict whether a client will default or not it is shown that due to the pre selection by a credit scoring system the data base is a sample from a different population than the bank is actually interested in this necessarily restricts inference a well furthermore we point out the importance of integrity and consistency checking when the data are entered into the system noise is a serious problem the actual experimental comparison involves a classical statistical method linear discriminant analysis and the classification tree algorithm c both method use one and the same training set drawn from the historical database to learn a classification function the percentage of correct classification on an independent test set are and respectively mcnemar s test show that the null hypothesis of equal performance ha a p value of the classification tree constructed by c us out of attribute to distinguish between defaulter and non defaulter and is consistent with the available theory on credit scoring the linear discriminant function us variable to make the classification both from the viewpoint of predictive accuracy and comprehensibilty the classification tree performs better in this study to make furhter progress the level of noise in the data ha to be reduced and data ha to be collected on loan that are rejected by the credit scoring system 
an extended version of the dual constraint model of motor end plate morphogenesis is presented that includes activity dependent and independent competition it is supported by a wide range of recent neurophysiological evidence that indicates a strong relation ship between synaptic e cacy and survival the computational model is justi ed at the molecular level and it prediction match the developmental and regenerative behaviour of real synapsis 
an application of reinforcement learning to a linear quadratic differential game is presented the reinforcement learning system us a recently developed algorithm the residual gradient form of advantage updating the game is a markov decision process mdp with continuous time state and action linear dynamic and a quadratic cost function the game consists of two player a missile and a plane the missile pursues the plane and the plane evades the missile the reinforcement learning algorithm for optimal control is modified for differential game in order to find the minimax 
the evidence approximationit ha recently become popular to consider the problem of training neural net from abayesian viewpoint buntine and weigend mackay the usual way of doingthis start by assuming that there is some underlying target function f from rnto r parameterizedby an n dimensional weight vector w we are provided with a training set lof noise corrupted sample of f our goal is to make a guess for w basing that guess onlyon l now assume we have i i d 
abstract in this paper we consider speech coding a a problem of speech modelling in particular prediction of parameterised speech over short time segment is performed using the hierarchical mixture of expert hme jordan jacob the hme give two ad vantage over traditional non linear function approximators such a the multi layer perceptron mlp a statistical understand ing of the operation of the predictor and provision of information about the performance of the predictor in the form of likelihood information and local error bar these two issue are examined on both toy and real world problem of regression and time series prediction in the speech coding context we extend the principle of combining local prediction via the hme to a vector quantiza tion scheme in which xed local codebooks are combined on line for each observation 
we consider the solution to large stochastic control problem by mean of method that rely on compact representation and a variant of the value iteration algorithm to compute approximate costto go function while such method are known to be unstable in general we identify a new class of problem for which convergence a well a graceful error bound are guaranteed this class involves linear parameterizations of the cost to go function together with an assumption that the dynamic programming operator is a contraction with respect to the euclidean norm when applied to function in the parameterized class we provide a special case where this assumption is satisfied which relies on the locality of transition in a state space other case will be discussed in a full length version of this paper 
dynamic programming provides a methodology to develop planner and controllersfor nonlinear system however general dynamic programming is computationallyintractable we have developed procedure that allow more complex planning andcontrol problem to be solved we use second order local trajectory optimization togenerate locally optimal plan and local model of the value function and it derivative we maintain global consistency of the local model of the value function guaranteeing 
we compare two regularization method which can be used to improve the generalizationcapabilities of gaussian mixture density estimate the first method consistsof defining a bayesian prior distribution on the parameter space we derive em expectation maximization update rule which maximize the a posterior parameterprobability in contrast to the usual em rule for gaussian mixture which maximizethe likelihood function in the second approach we apply ensemble averaging to density 
a new learning algorithm is developed for the design of statisticalclassifiers minimizing the rate of misclassification the method which is based on idea from information theory and analogy tostatistical physic assigns data to class in probability the distributionsare chosen to minimize the expected classification errorwhile simultaneously enforcing the classifier s structure and a levelof quot randomness quot measured by shannon s entropy achievement ofthe classifier structure is 
many data mining algorithm developed recently are based on inductive learning method very few are based on similarity based learning however similarity based learning accrues advantage such a simple representation for concept description low incremental learning cost small storage requirement etc we present a similarity based learning method from database in the context of rough set theory unlike the previous similarity based learning method which only consider the syntactic distance between instance and treat all attribute equally important in the similarity measure our method can analyse the attribute in the database by using rough set theory and identify the relevant attribute to the task attribute we also eliminate superfluous attribute for the task attribute and assign a weight to the relevant attribute according to their significance to the task attribute our similarity measure take into account the semantic information embedded in the database 
computation quot tal grossman complex system los alamo presenteda neural network algorithm for finding small minimal cover of hypergraphs thenetwork ha two set of unit the first representing the hyperedges to be coveredand the second representing the vertex the connection between the unit aredetermined by the edge of the incidence graph the dynamic of these two typesof unit are different when the parameter of the unit are correctly tuned thestable state of the 
neuron learning under an unsupervised hebbian learning rule can perform a nonlinear generalization of principal component analysis this relationship between nonlinear pca and nonlinear neuron is reviewed the stable fixed point of the neuron learning dynamic correspond to the maximum of the statistic optimized under nonlinear pca however in order to predict what the neuron learns knowledge of the basin of attraction of the neuron dynamic is required here the correspon dence between nonlinear pca and neural network break down this is shown for a simple model method of statistical mechanic can be used to find the optimum of the objective function of non linear pca this determines what the neuron can learn in order to find how the solution are partitioned amoung the neuron however one must solve the dynamic 
database often inaccurately identify entity of interest two operation consolidation and link formation which complement the usual machine learning technique that use similarity based clustering to discover classification are proposed a essential component of kdd system for certain application consolidation relates identifier present in a database to a set of real world entity rwe s which are not uniquely identified in the database consolidation may also be viewed a a transformation of representation from the identifier present in the original database to the rwe s link formation construct structured relationship between consolidated rwe s through identifier and event explicitly represented in the database consolidation and link formation are easily implemented a index creation in relational database management system an operational knowledge discovery system identifies potential money laundering in a database of large cash transaction using consolidation and link formation 
reinforcement learning method based on approximating dynamicprogramming dp are receiving increased attention due to theirutility in forming reactive control policy for system embeddedin dynamic environment environment are usually modeled ascontrolled markov process but when the environment model isnot known a priori adaptive method are necessary adaptive controlmethods are often classified a being direct or indirect directmethods directly adapt the control policy 
speaker recognition is the identification of a speaker from feature of his or her speech this paper describes the use of decision tree induction technique to induce classification rule that automatically identify speaker in a population of speaker the method described ha a recognition rate of for both text dependent and text independent utterance training time scale linearly with the population size 
ideally pattern recognition machine provide constant outputwhen the input are transformed under a group g of desiredinvariances these invariance can be achieved by enhancing thetraining data to include example of input transformed by elementsof g while leaving the corresponding target unchanged alternatively the cost function for training can include aregularization term that penalizes change in the output when theinput is transformed under the group this paper relates the twoapproaches showing precisely the sense in which the regularizedcost function approximates the result of adding transformedexamples to the training data we introduce the notion of aprobability distribution over the group transformation and usethis to rewrite the cost function for the enhanced training data under certain condition the new cost function is equivalent tothe sum of the original cost function plus a regularizer forunbiased model the regularizer reduces to the intuitively obviouschoice a term that penalizes change in the output when theinputs are transformed under the group for infinitesimaltransformations the coefficient of the regularization term reducesto the variance of the distortion introduced into the trainingdata this correspondence provides a simple bridge between the twoapproaches 
we address the problem of finding the parametersettings that will result in optimalperformance of a given learning algorithmusing a particular dataset a training data we describe a quot wrapper quot method consideringdetermination of the best parametersas a discrete function optimization problem the method us best first search and crossvalidationto wrap around the basic inductionalgorithm the search explores the spaceof parameter value running the basic algorithmmany time on training 
in this paper we investigate enhancement to an upper classifier a decision algorithm generated by an upper classification method which is one of the clsssification method in rough set theory specifically we consider two enhancement first we present a stepwise backward feature selection algorithm to prepro li c n mn co nf fcmt v x t h c ic imnnrtont hclrs rar buy tjd v y cl i iuiyuyauu a yi lu l y l u y iuuuui rough classification method are incapable of removing superfluous feature we prove that the stepwise backward selection algorithm find a small subset of relevant feature that are ideally sufficient and necessary to define target concept with respect to a given threshold this threshold value indicates an acceptable degradation in the quality of an upper classifier second to make an upper classifier adaptive we associate it with some kind of frequency information which we call incremental information an extended decision table is used to represent an adaptive upper classifier it is also used for interpreting an upper classifier either deterministically or nondeterministically 
current environmental monitoring system assume particle to bespherical and do not attempt to classify them a laser based systemdeveloped at the university of hertfordshire aim at classifyingairborne particle through the generation of two dimensionalscattering profile the performance of template matching andtwo type of neural network hypernet and semi linear unit arecompared for image classification the neural network approach isshown to be capable of comparable 
abstract on large problem reinforcement learning system must use parameterized function approximators such a neural network in order to generalize between similar situation and action in these case there are no strong theoretical result on the accuracy of convergence and computational result have been mixed in particular boyan and moore reported at last year s meeting a series of negative result in attempting to apply dynamic programming together with function approximation to simple control problem with continuous state space in this paper we present positive result for all the control task they attempted and for one that is significantly larger the most important dierences are that we used sparse coarse coded function approximators cmacs whereas they used mostly global function approximators and that we learned online whereas they learned oine boyan and moore and others have suggested that the problem they encountered could be solved by using actual outcome rollouts a in classical monte carlo method and a in the td algorithm when however in our experiment this always resulted in substantially poorer performance we conclude that reinforcement learning can work robustly in conjunction with function approximators and that there is little justification at present for avoiding the case of general reinforcement learning and function approximation 
the minimum description length principle mdl can be used totrain the hidden unit of a neural network to extract a representationthat is cheap to describe but nonetheless allows the input tobe reconstructed accurately we show how mdl can be used todevelop highly redundant population code each hidden unit hasa location in a low dimensional implicit space if the hidden unitactivities form a bump of a standard shape in this space they canbe cheaply encoded by the center of 
this paper present neurochess a program which learns to play chess from the final outcome of game neurochess learns chess board evaluation function represented by artificial neural network it integrates inductive neural network learning temporal differencing and a variant of explanation based learning performance result illustrate some of the strength and weakness of this approach 
an incremental network model is introduced which is able to learnthe important topological relation in a given set of input vector bymeans of a simple hebb like learning rule in contrast to previousapproaches like the quot neural gas quot method of martinetz and schulten this model ha no parameter which change over timeand is able to continue learning adding unit and connection untila performance criterion ha been met application of the modelinclude vector quantization 
this paper describes the facility available for knowledge discovery in database using the tetrad ii program while a year or two shy of state of the most advanced research on discovery we believe this program provides the most flexible and reliable suite of procedure so far availabie commercially for discovering causal structure semiautomatically constructing bayes network estimating nmmntnrr in c w k n akmrb nnrl naa nn l h ntrrnrsm lrnn suauiiwly yl u c apiy yxlu yvu qj i ya cu yull also be used to red e the number of variable needed for classification or prediction for example a a neural net preproceesor the theoretical principle on which the program is based are described in detail in spirtes glymour and scheines under assumption described there each of the search and discovery procedure we will describe have been proved to give correct information when statistical decision are made correctly what doe tetrad do this paper describes the facility available for howledge discovery in database using the tetrad ii program while a year or two shy of state of the most advanced research on discovery we believe this program provides the most flexible and reliable suite of procedure so far available commercially for discovering causal structure semiautomatically constructing bayes network estimating parameter in such network and updating the theoretical principle on which the program is based are described in detail in spirtes glymour and scheines under assumption described there each of the search and discovery procedure we will describe have been proved to give correct information when statistical decision about independence and conditional independence research supported by the navy office of personnel research and development and the office of naval research contract n this work is a collaboration with c meek r scheines and p spirtes 
in this paper we investigate the efficiency of subsumption the basic provability relation in ilp a is np complete even if we restrict ourselves to linked horn clause and fix to contain only a small constant number of literal we investigate in several restriction of we first adapt the notion of determinate clause used in ilp and show that subsumption is decidable in polynomial time if is determinate with respect to secondly we adapt the notion of local horn clause and show that subsumption is efficiently computable for some reasonably small we then show how these result can be combined to give an efficient reasoning procedure for determinate local horn clause an ilp problem recently suggested to be polynomial predictable by cohen by a simple counting argument we finally outline how the reduction algorithm an essential part of every lgg ilp learning algorithm can be improved by these idea 
recent year have seen the increasing development of knowledge discovery or database mining system that combine database management technology with machine learning technique and algorithm to perform the analysis of data many of these system use passive database management system to hold the data rather than active database however application such a battlemanagement situation or stock market trading would benefit from the use of an active database management system since the data is being constantly updated and other action should be triggered based on database event in this paper we present a general description of an active knowledge mining system that combine an active database with machine learning operator we introduce the notion of an intelligent mediator that contains knowledge about both the database and the capability of the learning operator the mediator chooses which of the operator to use to achieve a learning goal then determines how the discovered knowledge should be used and where it should be stored and maintained 
we develop a refined mean field approximation for inference andlearning in probabilistic neural network our mean field theory unlike most doe not assume that the unit behave a independentdegrees of freedom instead it exploit in a principled way theexistence of large substructure that are computationally tractable to illustrate the advantage of this framework we show how toincorporate weak higher order interaction into a first order hiddenmarkov model treating the 
this paper introduces a new measurement robustness to measure the quality of machine discovered knowledge from real world database that change over time a piece of knowledge is robust if it is unlikely to become inconsistent with new database state robustness is different from predictive accuracy in that by the latter the system considers only the consistency of a rule with unseen data while by the former the consistency after deletion and update of existing data is also considered combining robustness with other utility measurement a system can make intelligent decision in learning and maintenance of knowledge learned from changing database this paper defines robustness then present an estimation approach for the robustness of horn clause rule learned from a relational database the estimation approach applies the laplace law of succession which can be efficiently computed the estimation is based on database schema and transaction log no domainspecific information is required however if it is available the approach can exploit it 
previous research ha shown that a techniquecalled error correcting output coding ecoc can dramatically improve theclassification accuracy of supervised learningalgorithms that learn to classify datapoints into one of k ae class thispaper present an investigation of why theecoc technique work particularly whenemployed with decision tree learning algorithm it show that the ecoc method like any form of voting or committee canreduce the variance of the learning 
this paper introduces gnarl an evolutionary program which induces recurrent neural network that are structurally unconstrained in contrast to constructive and destructive algorithm gnarl employ a population of network and us a fitness function s unsupervised feedback to guide search through network space annealing is used in generating both gaussian weight change and structural modification applying gnarl to a complex search and collection task demonstrates that the system is capable of inducing network with complex internal dynamic 
we describe an analog vlsi implementation of the art algorithm carpenter a prototype chip ha been fabricated in a standard low cost m double metal single poly cmos process it ha a die area of cm and is mounted in a pin pga package the chip realizes a modified version of the original art architecture such modification ha been shown to preserve all computational property of the original algorithm serrano a while being more appropriate for vlsi realization the chip implement an art network with f node and f node it can therefore cluster binary pixel input pattern into up to different category modular expansibility of the system is possible by assembling an n m array of chip without any extra interfacing circuitry resulting in an f layer with n node and an f layer with m node pattern classification is performed in le than s which mean an equivalent computing power of connection and connection update per second although internally the chip is analog in nature it interface to the outside world through digital signal thus having a true asynchrounous digital behavior experimental chip test result are available which have been obtained through test equipment for digital chip 
a significant limitation of neural network is that the representation they learn are usually incomprehensible to human we present a novel algorithm trepan for extracting comprehensible symbolic representation from trained neural network our algorithm us query to induce a decision tree that approximates the concept represented by a given network our experiment demonstrate that trepan is able to produce decision tree that maintain a high level of fidelity to their respective 
most current method for prediction of protein secondary structureuse a small window of the protein sequence to predict the structureof the central amino acid we describe a new method for predictionof the non local structure called fi sheet which consists of two ormore fi strand that are interconnected by hydrogen bond sincefi strand are often widely separated in the protein chain a networkwith two window is introduced after training on a set of protein the network predicts the 
model of analog retrieval require a computationally cheap method of estimating similarity between a probe and the candidate in a large pool of memory item the vector dot product operation would be ideal for this purpose if it were possible to encode complex structure s a vector representation in such a way that the superficial similarity of vector representation reflected underlying structural similari ty this paper describes how such an encoding is provided by holographic reduced representation hrrs which are a method for encoding nested relational structure a fixed width distributed representation the condition under which structural similarity is reflected in the dot prod uct ranking of hrrs are discussed 
we describe a system that can track a hand in a sequence of videoframes and recognize hand gesture in a user independent manner the system locates the hand in each video frame and determinesif the hand is open or closed the tracking system is able to trackthe hand to within sigma pixel of it correct location in ofthe frame from a test set containing video sequence from differentindividuals captured in different room environment thegesture recognition network 
most existing decision tree system use a greedy approachto induce tree locally optimal split are inducedat every node of the tree although the greedyapproach is suboptimal it is believed to produce reasonablygood tree in the current work we attempt toverify this belief we quantify the goodness of greedytree induction empirically using the popular decisiontree algorithm c and cart we induce decisiontrees on thousand of synthetic data set and comparethem to the 
exception directed acyclic graph edags are knowledge structure that subsume tree and rule but can be substantially more compact manually constructed and induced edags are compared by reconstructing shapiro s structured induction of a chess end game it is shown that the induced edag is very similar to that produced through consultation with exnerts and that both are small comorehensible 
system that learn from example often create a disjunctive concept definition small disjuncts are those disjuncts which cover only a few training example the problem with small disjuncts is that they are more error prone than large disjuncts this paper investigates the reason why small disjuncts are more error prone than large disjuncts it show that when there are rare case within a domain then factor such a attribute noise missing attribute class noise and training set size can result in small disjuncts being more error prone than large disjuncts and in rare case being more error prone than common case this paper also ass the impact that these error prone small disjuncts and rare case have on inductive learning i e on error rate one key conclusion is that when low level of attribute noise are applied only to the training set the ability to learn the correct concept is being evaluated rare case within a domain are primarily responsible for making learning difficult 
several research group are implementing analog integrated circuit model of biological auditory processing the output of these circuit model have taken several form including video format for monitor display simple scanned output for oscilloscope display and parallel analog output suitable for data acquisition system here an alternative output method for silicon auditory model suitable for direct interface to digital computer is described a a prototype of this method an integrated circuit model of temporal adaptation in the auditory nerve that function a a peripheral to a workstation running unix is described data from a working hybrid system that includes the auditory model a digital interface and asynchronous software are given this system produce a real time x window display of the response of the auditory nerve model 
change in lighting condition strongly effect the perform ance and reliability of computer vision system we report face recogniti on result under drastically changing lighting condition for a compu ter vision system which concurrently us a contrast sensitive silico n retina and a conventional gain controlled ccd camera for both input device the face recognition system employ an elastic matching algorithm with wavelet based feature to classify unknown face to ass the effect of analog on chip preprocessing by the silicon retina the ccd image have been digitally preprocessed with a bandpass filter to adjust the power spectrum the silicon retina with it ability to adjus t sensitivity increase the recognition rate up to percent these comparative experiment demonstrate that preprocessing with an analog vlsi silicon retina generates image data enriched with object constantfeatures 
a goal of the multi tac project is to make combinatorial problem solving technologyavailable to user who have no formal training in ai or or to achieve this aim we have built a system vicss visual constraint specification system that enables user to specify problem graphically to simplify the specification process vicss relies heavily on programming by demonstrationtechniques we believe however that one key to making programming by demonstration succeedfor our application is 
we describe a framework for learning saccadic eye movement usinga photometric representation of target point in natural scene therepresentation take the form of a high dimensional vector comprisedof the response of spatial filter at different orientation and scale we first demonstrate the use of this response vector in the task oflocating previously foveated point in a scene and subsequently usethis property in a multisaccade strategy to derive an adaptive motormap for 
most reinforcement learning rl work supposes policy for sequential decisiontasks to be optimal that minimize the expected total discounted cost e g q learning wat ahc bar sut and on the other hand it is well knownthat it is not always reliable and can be treacherous to use the expected value a adecision criterion tha a lot of alternative decision criterion have beensuggested in decision theory to get a more sophisticated considaration of risk butmost rl 
this paper we propose to use ordered decision graph odgs a the underlying description in odgs thevariables can only be tested in accordance with a previously specified order by using odgs we can effectively avoidsome of the problem encountered by other researcher when trying to identify common subtrees we describe asimple but relatively effective algorithm that derives an ordered decision graph with minimal description length froma decision tree built using standard technique 
a self organizing neural network for sequence classification called sardnet is described and analyzed experimentally sardnet extends the kohonen feature map architecture with activation retention and decay in order to create unique distributed response pattern for different sequence sardnet yield extremely dense yet descriptive representation of sequential input in very few training iteration the network ha proven successful on mapping arbitrary sequence of binary and real number a well a phonemic representation of english word potential application include isolated spoken word recognition and cognitive science model of sequence processing 
the use of entropy a a distance measure ha several benefit amongst other thing it provides a consistent approach to handling of symbolic attribute real valued attribute and missing value the approach of taking all possible transformation path is discussed we describe k an instance based learner which us such a measure and result are presented which compare favourably with several machine learning algorithm 
the process of machine learning can be considered in two stage modelselection and parameter estimation in this paper a technique is presentedfor constructing dynamical system with desired qualitative property theapproach is based on the fact that an n dimensional nonlinear dynamicalsystem can be decomposed into one gradient and n gamma hamiltonian system thus the model selection stage consists of choosing the gradient andhamiltonian portion appropriately so that a certain 
this paper present a method for acquiringa semantic hierarchy and updating an incompletehierarchy the creation of a comprehensivehierarchy is one important step in constructinga system for translating japanesetexts into english the hierarchy is usedto bias the learning of rule that indicatethe english translation of a japanese verb the task is particularly challenging becausetraining example are ambiguous in the sensethat each of the attribute forming an examplemay 
we apply active exemplar selection plutowski amp white to predicting a chaotic time series given a fixed set of example the method chooses a concise subset for training fittingthese exemplar result in the entire set being fit a well a desired the algorithm incorporates a method for regulating networkcomplexity automatically adding exemplar and hidden unit asneeded fitting example generated from the mackey glass equationwith fractal dimension to an rmse 
we examine the issue of evaluation of model specific parameter in a modified vc formalism two example are analyzed the dim ensional homogeneous perceptron and the dimensional higher order neuron both model are solved theoretically and their learning cu rf are compared against true learning curve it is shown that the form alism ha the potential to generate a variety of learning curve incl uding one displaying phase transition 
the wake sleep algorithm hinton dayan frey and neal is a relatively efficient method of fitting a multilayer stochastic generative model to high dimensional data in addition to the top down connection in the generative model it make use of bottom up connection for approximating the probability distribution over the hidden unit given the data and it train these bottom up connection using a simple delta rule we use a variety of synthetic and real data set to compare the performance of the wake sleep algorithm with monte carlo and mean field method for fitting the same generative model and also compare it with other model that are le powerful but easier to fit 
a framework for knowledge based scientificdiscovery in geological database ha been developed the discovery process consists of twomain step context definition and equationderivation context definition properly definesand formulates homogeneous region each ofwhich is likely to produce a unique and meaningfulanalytic formula for the goal variable clustering technique and a suite of visualizationand interpretation routine make up a toolbox that assist the context definition 
wolfgang maassinstitute for theoretical computer sciencetechnische universitaet grazklosterwiesgasse a graz austriae mail maass igi tu graz ac atneurocolt technical report seriesnc tr december produced a part of the esprit working groupin neural and computational learning neurocolt neurocolt coordinating partner department of computer scienceegham surrey tw ex englandfor more information contact john 
abstract an alternative model is proposed for mixture of expert by utiliz ing a di erent parametric form for the gating network the mod i ed model is trained by an em algorithm in comparison with earlier model trained by either em or gradient ascent there is no need to select a learning stepsize to guarantee the convergence of the learning procedure we report simulation experiment which show that the new architecture yield signi cantly faster conver gence we also apply the new model to two problem domain piecewise nonlinear function approximation and combining multi ple previously trained classi er 
this paper present an unsupervised learning scheme for categorizing d object from their d projected image the schemeexploits an auto associative network s ability to encode each viewof a single object into a representation that indicates it view direction we propose two model that employ different classificationmechanisms the first model selects an auto associative networkwhose recovered view best match the input view and the secondmodel is based on a modular 
multi class classification problem can be efficiently solved by partitioning the original problem into sub problem involving only two class for each pair of class a potentially small neural network is trained using only the data of these two class we show how to combine the output of the two class neural network in order to obtain posterior probability for the class decision the resulting probabilistic pairwise classifier is part of a handwriting recognition system which is currently applied to check reading we present result on real world data base and show that from a practical point of view these result compare favorably to other neural network approach 
abstract when solving homework exercise human student often notice that the problem they are about to solve is similar to an example they then deliberate over whether to refer to the example or to solve the problem without looking at the example we present protocol analysis showing that e ective human learn er prefer not to use analogical problem solv ing for achieving the base level goal of the problem although they do use it occasionally for achieving meta level goal such a check ing solution or resolving certain kind of im pass on the other hand ine ective learn er use analogical problem solving in place of ordinary problem solving and this prevents them from discovering gap in their domain theory an analysis of the task domain col lege physic reveals a testable heuristic for when to use analogy and when to avoid it the heuristic may be of use in guiding mul tistrategy learner 
visual cognition depends critically on the ability to make rapid eye movement known a saccade that orient the fovea over target of interest in a visual scene saccade are known to be ballistic the pattern of muscle activation for foveating a prespecified target location is computed prior to the movement and visual feedback is precluded despite these distinctive property there ha been no general model of the saccadic targeting strategy employed by the human visual system during visual search in natural scene this paper proposes a model for saccadic targeting that us iconic scene representation derived from oriented spatial filter at multiple scale visual search proceeds in a coarse to fine fashion with the largest scale filter response being compared first the model wa empirically tested by comparing it performance with actual eye movement data from human subject in a natural visual search task preliminary result indicate substantial agreement between eye movement predicted by the model and those recorded from human subject 
introductionin the conventional bayesian view of backpropagation bp buntine and weigend nowlan and hinton mackay wolpert one start with the quot likelihood quot conditional distribution p training set t weight vector w and the quot prior quot distributionp w a an example in regression one might have a quot gaussian likelihood quot p t w exp c w t o p i exp net w t x i t y i s for some constant s t x i and t y i are the 
we study bayesian network for continuous variable using nonlinear conditional density estimator we demonstrate that useful structure can be extracted from a data set in a self organized way and we present sampling technique for belief update based on markov blanket conditional density model 
this paper present an algorithm for incremental induction of decision tree that is ableto handle both numeric and symbolic variable in order to handle numeric variable a newtree revision operator called slewing is introduced finally a non incremental method isgiven for finding a decision tree based on a direct metric of a candidate tree content introduction design goal an improved algorithm incorporating a training instance 
when training neural network by the classical backpropagation algorithm the whole problem to learn must be expressed by a set of input and desired output however we often have high level knowledge about the learning problem in optical character recognition ocr for instance we know that the classification should be invariant under a set of transformation like rotation or translation we propose a new mo dular classification system based on several autoassociative multilayer perceptrons which allows the efficient incorporation of such knowledge result are reported on the nist database of upper case handwritten letter and compared to other approach to the invariance problem 
knowledge discovery in database ha become an increasingly important research topic with the advent of wide area network computing one of the crucial problem we study in this paper is how to scale machine learning algorithm that typically are designed to deal with main memory based datasets to efficiently learn from large distributed database we have explored an approach called meta learning that is related to the traditional approach of data reduction commonly employed in distributed query processing system here we seek efficient mean to learn how to combine a number of base classifier which are learned from subset of the data so that we scale efficiently to larger learning problem and boost the accuracy of the constituent classifier if possible in this paper we compare the arbiter tree strategy to a new but related approach called the combiner tree strategy 
in this paper we present result from the first use of neuralnetworks for real time control of the high temperature plasma in atokamak fusion experiment the tokamak is currently the principalexperimental device for research into the magnetic confinementapproach to controlled fusion in an effort to improve the energyconfinement property of the high temperature plasma insidetokamaks recent experiment have focused on the use of noncircularcross sectional plasma shape however the accurate generation ofsuch plasma represents a demanding problem involving simultaneouscontrol of several parameter on a time scale a short a a fewtens of microsecond application of neural network to thisproblem requires fast hardware for which we have developed a fullyparallel custom implementation of a multilayer perceptron based ona hybrid of digital and analogue technique 
our work tackle the problem of finding partial determination in databasesand proposes a compression based measure to evaluate them partialdeterminations can be viewed a generalization of both functionaldependencies and association rule in that they are relational in natureand may have exception extending the measure used for evaluatingassociation rule namely support and confidence to partial determinationsleads to a few problem we therefore propose a measure based onthe 
this paper describes several mean for sharingbetween related concept to improvelearning in the same domain the sharingcomes in the form of substructure or possiblyentire structure of previous conceptswhich may aid in learning other concept these substructure highlight useful informationin the domain using two domain weevaluate the effectiveness of concept sharingwith respect to accuracy concept size searchcomplexity and noise resistance introductionhumans 
this paper discus the use of artificial neural network for dynamic modelling of time series we argue that multistep prediction is more appropriate to capture the dynamic of the underlying dynamical system because it constrains the iterated model we show how this method can be implemented by a recurrent ann trained with trajectory learning we also show how to select the trajectory length to train the iterated predictor for the case of chaotic time series experimental result corroborate the proposed method 
in this paper we define the task of place learning anddescribe one approach to this problem the frameworkrepresents distinct place using evidence grid a probabilistic description of occupancy place recognitionrelies on case based classification augmentedby a registration process to correct for translation the learning mechanism is also similar to that in casebasedsystems involving the simple storage of inferredevidence grid experimental study with both physicaland simulated 
this paper introduces the recurrence surfaceapproximation an inductive learningmethod based on linear programming thatpredicts recurrence time using censoredtraining example that is example in whichthe available training output may be only alower bound on the quot right answer quot this approachis augmented with a feature selectionmethod that chooses an appropriate featureset within the context of the linear programminggeneralizer computational result inthe field of breast cancer 
many real world planning domain are complex and uncertain preventing complete a priori planning however real world planner can also rely on runtime information to facilitate additional planning during execution the completable approach to planning introduces the idea of completable step which represent deferred planning decision through completable step a planner can defer particular goal until execution time when additional information may be used for their achievement to maintain the provably correct nature of plan afforded by classical planning completable step have the additional requirement of achievability unfortunately without additional higher order knowledge for reasoning about achievability proving achievability becomes infeasible for any real world domain we thus developed an incremental approach for learning completable plan using this approach instead of proving achievability a planner us feedback from it experience with the real world to construct completable plan which cover an increasing space of situation this approach to real world planning ha been successfully tested in a simple simulated robot navigation domain 
with a point matching distance measure which is invariant undertranslation rotation and permutation we learn d point set object by clustering noisy point set image unlike traditional clusteringmethods which use distance measure that operate on featurevectors a representation common to most problem domain thisobject based clustering technique employ a distance measure specificto a type of object within a problem domain formulatingthe clustering problem a two nested 
compositional q learning cq l singh is a modular approach to learning to perform composite task made up of several elemental task by reinforcement learning skill acquired while performing elemental task are also applied to solve composite task individual skill compete for the right to act and only winning skill are included in the decomposition of the composite task we extend the original cq l concept in two way a more general reward function and the agent can have more than one actuator we use the cq l architecture to acquire skill for performing composite task with a simulated twolinked manipulator having large state and action space the manipulator is a non linear dynamical system and we require it end effector to be at specific position in the workspace fast function approximation in each of the q module is achieved through the use of an array of cerebellar model articulation controller cmac albus structure 
biological sensorimotor system are not static map that transforminput sensory information into output motor behavior evidencefrom many line of research suggests that their representationsare plastic experience dependent entity while this plasticityis essential for flexible behavior it present the nervous systemwith difficult organizational challenge if the sensorimotor systemadapts itself to perform well under one set of circumstance will itthen perform poorly 
this paper suggests that it may be easier to learn several hard task at one time than to learn these same task separately in effect the information provided by the training signal for each task serf a a domain specific inductive bias for the other task frequently the world give u cluster of related task to learn when it doe not it is often straightforward to create additional task for many domain acquiring inductive bias by collecting additional teaching signal may be more practical than the traditional approach of codifying domain specific bias acquired from human expertise we call this approach multitask learning mtl since much of the power of an inductive learner follows directly from it inductive bias multitask learning may yield more powerful learning an empirical example of multitask connectionist learning is presented where learning improves by training one network on several related task at the same time multitask decision tree induction is also outlined part of this methodology may be wrong the reductionist method ha caused u to attempt learning on simple isolated task before earnestly attempting learning on larger rich er task by adhering to this method we may be ignoring a critical source of inductive bias for real world problem the inductive bias inherent in the similarity of related ta k drawn from the same domain if an inductive learner is given several related task at the same time these task can be used a valuable source of inductive bias for each other this may make learning faster or more accurate and may allow hard task to be learned that are not learnable in isolation we call this approach multitask learning mtl first we introduce multitask learning at an abstract level and explain how related task can be used a source of mutual inductive bias then we present a concrete example of mtl applied to artificial neural network after this demonstration we discus multitask connectionist learnin g in more detail then we briefly describe multitask decisiontree learning after this we discus related work most notably that on giving hint to network and on transferring knowledge between related task learned one at a time finally we summarize the mtl methodology and highlight the research problem that will have to be tackled for mtl to succeed on complex real world task 
we present a neural network based face detection system a retinally connected neural network examines small window of an image and decides whether each window contains a face the system arbitrates between multiple network to improve performance over a single network we use a bootstrap algorithm for training which add false detection into the training set a training progress this eliminates the difficult task of manually selecting non face training example which must be chosen to span the entire space of non face image comparison with another state of the art face detection system are presented our system ha better performance in term of detection and false positive rate 
the chou fasman algorithm and it associatedtheory are non learning method to predictthe secondary structure for protein fromthe string of amino acid forming the protein however the overall accuracy of thepredictions is not high and the predictionsfor the important structure are particularlylow therefore a range of method have beenused to improve the prediction of the secondarystructure we have applied our symbolicknowledge refinement system krustto the chou fasman theory to 
a number of reinforcement learning algorithm have been developed that are guaranteed to converge to the optimal solution when used with lookup table it is shown however that these algorithm can easily become unstable when implemented directly with a general function approximation system such a a sigmoidal multilayer perceptron a radial basisfunction system a memory based learning system or even a linear function approximation system a new class of algorithm residual gradient algorithm is proposed which perform gradient descent on the mean squared bellman residual guaranteeing convergence it is shown however that they may learn very slowly in some case a larger class of algorithm residual algorithm is proposed that ha the guaranteed convergence of the residual gradient algorithm yet can retain the fast learning speed of direct algorithm in fact both direct and residual gradient algorithm are shown to be special case of residual algorithm and it is shown that residual algorithm can combine the advantage of each approach the direct residual gradient and residual form of value iteration qlearning and advantage learning are all presented theoretical analysis is given explaining the property these algorithm have and simulation result are given that demonstrate these property 
no finite sample is sufficient to determine the density and therefore the entropy of a signal directly some assumption about either the functional form of the density or about it smoothness is necessary both amount to a prior over the space of possible density function by far the most common approach is to assume that the density ha a parametric form by contrast we derive a differential learning rule called emma that optimizes entropy by way of kernel density estimation entropy and it derivative can then be calculated by sampling from this density estimate the resulting parameter update rule is surprisingly simple and efficient we will show how emma can be used to detect and correct corruption in magnetic resonance image mri this application is beyond the scope of existing parametric entropy model 
we analyze how data with uncertain or missing input feature canbe incorporated into the training of a neural network the generalsolution requires a weighted integration over the unknown oruncertain input although computationally cheaper closed form solutionscan be found for certain gaussian basis function gbf network we also discus case in which heuristical solution suchas substituting the mean of an unknown input can be harmful introductionthe ability to learn from data with 
determination are a useful type of functional knowledge representation application include knowledge based system analogical reasoning database design and robotic sensing system this paper present an efficient batch algorithm for inducing all minimal determination from observed data the algorithm is based on breadth first search and run in polynomial time and space given a user supplied parameter limiting the maximum size of a determination the algorithm us probabilistic measure to induce determination despite noisy data one key contribution is the identification of an enumeration order in the space of possible determination that affords a complete and systematic search another contribution list axiom that relate neighboring state and allow the construction of pruning rule a third contribution formulates a perfect hash function for state in this space and facilitates optimal use of the pruning rule this paper also sketch an algorithm that can incrementally revise a set of determination s given additional data 
in the last decade the outline of the neural structure subservingthe sense of direction have begun to emerge several investigationshave shed light on the effect of vestibular input and visual inputon the head direction representation in this paper a model isformulated of the neural mechanism underlying the head directionsystem the model is built out of simple ingredient depending onnothing more complicated than connectional specificity attractordynamics hebbian learning and 
this paper describes probabilistic method for novelty detection when using pattern recognitionfor monitoring of dynamic l he of detection is particularlyacute when prior knowledge and data only allow one to construct an incomplete prior of thesystem hence some allowance must be made in so that a classifier will robustto generated by class not in training phase the fault apractical approach is to construct both an input and classconstruction input model for data of is cansolved in 
compliant control is a standard method for performing fine manipulation task like grasping and assembly but it requires estimation of the state of contact between the robot arm and the object involved here we present a method to learn a model of the movement from measured data the method requires little or no prior knowledge and the resulting model explicitly estimate the state of contact the current state of contact is viewed a the hidden state variable of a discrete hmm the control dependent transition probability between state are modeled a parametrized function of the measurement we show that their parameter can be estimated from measurement concurrently with the estimation of the parameter of the movement in each state of contact the learning algorithm is a variant of the em procedure the e step is computed exactly solving the m step exactly would require solving a set of coupled nonlinear algebraic equation in the parameter instead gradient ascent is used to produce an increase in likelihood 
many machine learning algorithm aim at finding quot simple quot rule to explain training data theexpectation is the quot simpler quot the rule the better the generalization on test data occam srazor most practical implementation however use measure for quot simplicity quot that lack the power universality and elegance of those based on kolmogorov complexity and solomonoff s algorithmicprobability likewise most previous approach especially those of the quot bayesian quot kind sufferfrom the problem of 
protein structure analysis from dna sequence is an important and fast growing area in both computet science and biochemistry although interesting approach have been studied it is very dificult to capture the characteristic of protein since even a simple protein are made of more than amino acid which make biochemical experiment very dificult to detect functional component for this reason almost all the problem in this field are left unsolved and it is very important to develop a system which assist researcher on molecular biology to remove the dificulties caused by combinatorial explosion in this paper we report a system called mwi molecular biologist workbench version l o which extract knowledge from amino acid sequence by controlling application of domain knowledge automatically we apply this method to comparative analysis of lysozyme and lxlactalbumin the result show that we obtain several interesting result from amino acid sequence which have not been reported before 
this paper relates the computational power of fahlman s recurrent cascade correlation rcc architecture to that of finite state automaton fsa while some recurrent network are fsa equivalent rcc is not the paper present a theoretical analysis of the rcc architecture in the form of a proof describing a large class of fsa which cannot be realized by rcc 
we develop in the context of discriminant analysis a general approach to the design of neural architecture it consists in building a neural net around a statistical model family larger network made up of such elementary network are then constructed it is shown that on the one hand the statistical modeling approach provides a systematic way to obtaining good approximation in the neural network context while on the other neural network offer a powerful expansion to classical model family a novel integrated approach emerges which stress both flexibility contribution of neural net and interpretability contribution of statistical modeling a well known data set on birth weight is analyzed by this new approach the result are rather promising and open the way to many potential application 
reinforcement learning address the problem of learning to select action in order tomaximize one s performance in unknown environment to scale reinforcement learningto complex real world task such a typically studied in ai one must ultimately be ableto discover the structure in the world in order to abstract away the myriad of detail andto operate in more tractable problem space this paper present the skill algorithm skill discovers skill which are partiallydefined action 
we present a fast algorithm for non linear dimension reduction the algorithm build a local linear model of the data by mergingpca with clustering based on a new distortion measure experimentswith speech and image data indicate that the local linearalgorithm produce encoding with lower distortion than those builtby five layer auto associative network the local linear algorithmis also more than an order of magnitude faster to train introductionfeature set can be more compact 
increasing attention ha been paid to reinforcement learning algorithmsin recent year partly due to success in the theoreticalanalysis of their behavior in markov environment if the markovassumption is removed however neither generally the algorithmsnor the analysis continue to be usable we propose and analyzea new learning algorithm to solve a certain class of non markovdecision problem our algorithm applies to problem in whichthe environment is markov but the learner ha 
prior knowledge constraint are imposed upon a learning problem in the form of distance measure prototypical d point set and graph are learned by clustering with point matching and graph matching distance measure the point matching distance measure is approximately invariant under affine transformation translation rotation scale and shear and permutation it operates between noisy image with missing and spurious point the graph matching distance measure operates on weighted graph and is invariant under permutation learning is formulated a an optimization problem large objective so formulated million variable are efficiently minimized using a combination of optimization technique softassign algebraic transformation clocked objective and deterministic annealing 
real world learning task may involve high dimensional data setswith arbitrary pattern of missing data in this paper we presenta framework based on maximum likelihood density estimation forlearning from such data set we use mixture model for the densityestimates and make two distinct appeal to the expectationmaximization em principle dempster et al in derivinga learning algorithm em is used both for the estimation of mixturecomponents and for coping with missing data 
todate visualization ha not been extensively harnessed in knowledge discovery in database kdd in this paper we show that a multidimensional visualization mdv technique can be used synergistically with a machine learning program like c to uncover new knowledge used together the two approach span the kdd spectrum between complete automation on one hand and fully manual on the other we introduce mdv it implementation in a tool named winviz and show how winviz support the various task in kdd 
the fundamental tradeoff that is well known in knowledge representation and reasoning affect concept learning from example too representation of learning example using at tribute value ha proved to support efficient inductive algorithm but limited expressiveness whereas more expressive representation language typically subset of first order logic fol are supported by le efficient algorithm in fact an underlying problem is that of the number of different way of matching example just one in attribute val ue representatio n and potentially large in fol representation this paper describes a novel approach to perform representation shift on learning example the structure of these learning example initially represented using a subset of fol based language is reformulated so a to produce new learning example that are represented using an attribute value language what is considered to be an adequate structure varies according to the learning task we introduce the notion of morion from the greek to qualify this structure and show through a concrete example the advantage it of fers we then describe an algorithm which reformulates learning example automatically and go on to analyze it complexity this approach to deductive reformulation is implemented in the remo system that ha been experimented on the learning of the construction of chinese character 
several recurrent network have been proposed a representation for the task of formal language learning after training a recurrent network recognize a formal language or predict the next symbol of a sequence the next logical step is to understand the information processing carried out by the network some researcher have begun to extracting finite state machine from the internal state trajectory of their recurrent network this paper describes how sensitivity to initial condition and discrete measurement can trick these extraction method to return illusory finite state description 
ensemble learning by variational free energy minimization is a tool introduced toneural network by hinton and van camp in which learning is described in term ofthe optimization of an ensemble of parameter vector the optimized ensemble is anapproximation to the posterior probability distribution of the parameter this toolhas now been applied to a variety of statistical inference problem in this paper i study a linear regression model with both parameter and hyperparameters i 
we evaluate crustacean an inductivelogic programming algorithm that us inverseimplication to induce recursive clausesfrom example this approach is wellsuited for learning a class of self recursiveclauses which commonly appear in logic program because it search for common substructuresamong the example however little evidence exists that inverse implicationapproaches perform well when given onlyrandomly selected positive and negative example we show that crustaceanlearns 
we consider the effect of combining several least square estimator on the expected performanceof a regression problem computing the exact bias and variance curve a a function of the samplesize we are able to quantitatively compare the effect of the combination on the bias and varianceseparately and thus on the expected error which is the sum of the two first we show that bysplitting the data set into several independent part and training each estimator on a differentsubset the 
abstract we give an analysis of the generalization error of cross validation in term of two natural measure of the difficulty of the problem under consideration the approximation rate the accuracy to which the target function can be ideally approximated a a function of the number of hypothesis parameter and the estimation rate the deviation between the training and generalization error a a function of the number of hypothesis parameter the approximation rate capture the complexity of the target function with respect to the hypothesis model and the estimation rate capture the extent to which the hypothesis model suffers from overfitting using these two measure we give a rigorous and general bound on the error of cross validation the bound clearly show the tradeoff involved with making the fraction of data saved for testing too large or too small by optimizing the bound with respect to we then argue through a combination of formal analysis plotting and controlled experimentation that the following qualitative property of cross validation behavior should be quite robust to significant change in the underlying model selection problem 
technique for learning from data typically require data to be in standard form measurement must be encoded in a numerical format such a binary true or false feature numerical feature or possibly numerical code in addition for classification a clear goal for learning must be specified while some database may readily be arranged in standard form many others may be combination of numerical field or text with thousand of possibility for each data field and multiple instance of the same field specification a significant portion of the effort in real world data mining application involves defining identifying and encoding the data into suitable feature in this paper we describe an automatic feature extraction procedure adapted from modern text categorization technique that map very large database into manageable datasets in standard form we describe a commercial application of this procedure to mining a collection of very large database of home appliance service record for a major international retailer 
we consider the problem of on line gradient descent learning forgeneral two layer neural network an analytic solution is presentedand used to investigate the role of the learning rate in controllingthe evolution and convergence of the learning process learning in layered neural network refers to the modification of internal parametersfjg which specify the strength of the interneuron coupling so a to bring the mapfjimplemented by the network a close a possible to a desired 
we exhibit a theoretically founded algorithm t for agnostic pac learning of decision tree of at most level whose computation time is almost linear in the size of the training set we evaluate the performance of this learning algorithm t on common real world datasets and show that for most of these datasets t provides simple decision tree with little or no loss in predictive power compared with c in fact for datasets with continuous attribute it error rate tends to be lower than that of c to the best of our knowledge this is the first time that a pac learning algorithm is shown to be applicable to real world classification problem since one can prove that t is an agnostic paclearning algorithm t is guaranteed to produce close to optimal level decision tree from sufficiently large training set for any distribution of data in this regard t differs strongly from all other learning algorithm that are considered in applied machine learning for which no guarantee can be given about their performance on new datasets we also demonstrate that this algorithm t can be used a a diagnostic tool for the investigation of the expressive limit of level decision tree finally t in combination with new bound on the vc dimension of decision tree of bounded depth that we derive provides u now for the first time with the tool necessary for comparing learning curve of decision tree for real world datasets with the theoretical estimate of paclearning theory valiant val had introduced the model for probably approximately correct learning in in applied machine learning an even larger literature exists about the performance of various other learning algorithm on realworld classification task however curiously enough this article apparently mark the first time that the performance of a pac learning algorithm for a model powerful enough to cover real world datasets a a special case is evaluated on real world classification task the paclearning algorithm t that we have developed for this purpose is described in section of this article and result about it performance on real world classification problem are discussed in the subsequent section in this introduction we will define some basic notion from theoretical and applied machine learning and also address some obstacle which one ha to overcome in order to combine both approach it should be mentioned in this context that although t is apparently the first pac learning algorithm that is tested on real world classification problem there ha previously been already a fruitful migration of various idea from pac learning theory into application see e g ds 
it is widely accepted that the use of more compact representationsthan lookup table is crucial to scaling reinforcement learning rl algorithm to real world problem unfortunately almost all of thetheory of reinforcement learning assumes lookup table representation in this paper we address the pressing issue of combiningfunction approximation and rl and present a function approximatorbased on a simple extension to state aggregation a commonlyused form of compact 
in this paper we describe the theoretical formulation of remap an approach for the training and estimation of posterior probability using a recursive algorithm that is reminiscent of the em expectation maximization algorithm dempster et al for the estimation of data likelihood although very general the method is developed in the context of a statistical model for transition based speech recognition using artificial neural network ann to generate probability for hidden markov model hmms in the new approach we use local conditional posterior probability of transition to estimate global posterior probability of word sequence given acoustic speech data although we still use anns to estimate posterior probability the network is trained with target that are themselves estimate of local posterior probability these target are iteratively re estimated by the remap equivalent of the forward and backward recursion of the baum welch algorithm baum et al baum to guarantee regular increase up to a local maximum of the global posterior probability convergence of the whole scheme is proven unlike most previous hybrid hmm ann system that we and others have developed the new formulation determines the most probable word sequence rather than the utterance corresponding to the most probable state sequence also in addition to using all possible state sequence the proposed training algorithm us posterior probability at both local and global level and is discriminant in nature 
this paper study the convergence property of the well knownk mean clustering algorithm the k mean algorithm can be describedeither a a gradient descent algorithm or by slightly extendingthe mathematics of the em algorithm to this hard thresholdcase we show that the k mean algorithm actually minimizes thequantization error using the very fast newton algorithm introductionk mean is a popular clustering algorithm used in many application including theinitialization of more 
we have been developing a methodology system called gls global learning scheme for knowledge discovery in database the development of gls ha two main aspect the first is to develop a m i strategy system that is many kind of discovery learning method are cooperatively used in multiple learning phase for performing multi aspect intelligent data analysis a well a multi level conceptual abstraction and learning a a multi strategy system gls is implemented a a toolkit composed of several sub system and optional part with a multi level structure we have finished main part belonging to this aspect and have undertaken another aspect i e extending gls into a multi agent distributed and cooperative discovery system we try to increase versatility and autonomy of gls by multi strategy and distributed cooperation this paper briefly discus these two aspect of gls 
the recently introduced quot parameterized self organizing map quot quot psom quot show excellent function mapping capability after learning of a remarkablesmall set of training data this is a very important feature in field wherethe acquisition of training data is costly for example in robotics a a firstdemonstration we compare result for the task of kinematic mapping of a dof robot finger obtained by a psom and a standard backprop network a new way of structuring learning becomes 
the most commonly used neural network model are not well suitedto direct digital implementation because each node need to performa large number of operation between floating point value fortunately the ability to learn from example and to generalize isnot restricted to network of this type indeed network where eachnode implement a simple boolean function boolean network canbe designed in such a way a to exhibit similar property twoalgorithms that generate boolean 
we derive global h optimal training algorithm for neural network these algorithm guarantee the smallest possible predictionerror energy over all possible disturbance of fixed energy and aretherefore robust with respect to model uncertainty and lack ofstatistical information on the exogenous signal the ensuing estimatorsare infinite dimensional in the sense that updating theweight vector estimate requires knowledge of all previous weightesimates a certain finite dimensional 
we consider algorithm for learning functionsf x y where x and y are finite andthere is assumed to be no noise in the data learning algorithm alg are connected with gamma alg the set of prior probability distributionsfor which they are optimal a methodfor constructing gamma alg from alg is given andthe relationship between the various gamma alg is discussed improper algorithm are identifiedas those for which gamma alg ha zero volume improper algorithm are 
a novel method for combining decision treesand kernel density estimator is proposed standard classification tree or class probabilitytrees provide piecewise constant estimatesof class posterior probability kerneldensity estimator can provide smoothnon parametric estimate of class probability but scale poorly a the dimensionalityof the problem increase this paperdiscusses a hybrid scheme which us decisiontrees to find the relevant structure in 
we describe clip r a theory revision system for the revision of clip rule base clip r differs from previous theory revision system in that it operates on forward chaining production system revision of production system rulebases is important because production system can perform a variety of task such a monitoring and design in addition to classification task that have been addressed by previous research we show that clip r can take advantage of a variety of user specified constraint on the correct processing of instance such a ordering constraint on the displaying of information and the content of the final fact list in addition we show that clip r can operate a well a existing system when the only constraint on processing an instance is the correct classification of the instance 
unsupervised learning procedure have been successful at low levelfeature extraction and preprocessing of raw sensor data so far however they have had limited success in learning higher orderrepresentations e g of object in visual image a promising approachis to maximize some measure of agreement between theoutputs of two group of unit which receive input physically separatedin space time or modality a in becker and hinton becker de sa using the same 
although artificial neural network have been applied in a variety of real world scenario with remarkable success they have often been criticized for exhibiting a low degree of human comprehensibility technique that compile compact set of symbolic rule out of artificial neural network offer a promising perspective to overcome this obvious deficiency of neural network representation this paper present an approach to the extraction of if then rule from artificial neural network it key mechanism is validity interval analysis which is a generic tool for extracting symbolic knowledge by propagating rule like knowledge through backpropagation style neural network empirical study in a robot arm domain illustrate the appropriateness of the proposed method for extracting rule from network with real valued and distributed representation 
this paper investigates learning in a lifelong context lifelong learning address situation in which a learner face a whole stream of learn ing task such scenario provide the opportunity to transfer knowledge across multiple learning task in order to generalize more accurately from le training data in this paper several different approach to lifelong learning are described and applied in an object recognition domain it is shown that across the board lifelong learning approach generalize consistently more accurately from le training data by their ability to transfer knowledge across learning task 
much of the research in inductive learning concentrate on problem with relatively small amount of data with the coming age of very large network computing it is likely that order of magnitude more data in database will be available for various learning problem of real world importance some learning algorithm assume that the entire data set fit into main memory which is not feasible for massive amount of data one approach to handling a large data set is to partition the data set into subset run the learning algorithm on each of the subset and combine the result in this paper we evaluate different technique for learning from partitioned data our meta learningapproach is empirically compared with technique in the literature that aim to combine multiple evidence to arrive at one prediction 
for many type of learner one can compute the statistically optimal way to select data we review how these technique have been used with feedforward neural network we then show how the same principle may be used to select data for two alternative statistically based learning architecture mixture of gaussians and locally weighted regression while the technique for neural network are expensive and approximate the technique for mixture of gaussians and locally weighted regression are both efficient and accurate 
a new model for chemosensory reception is presented it model reaction between odor molecule and receptor protein and the activation of second messenger by receptor protein the mathematical formulation of the reaction kinetics is transformed into an artificial neural network ann the resulting feed forward network provides a powerful mean for parameter fitting by applying learning algorithm the weight of the network corresponding to chemical parameter can be trained by presenting experimental data we demonstrate the simulation capability of the model with experimental data from honey bee chemosensory neuron it can be shown that our model is sufficient to rebuild the observed data and that simpler model are not able to do this task 
we develop a principled strategy to sample a function optimally forfunction approximation task within a bayesian framework usingideas from optimal experiment design we introduce an objectivefunction incorporating both bias and variance to measure the degreeof approximation and the potential utility of the data pointstowards optimizing this objective we show how the general strategycan be used to derive precise algorithm to select data for twocases learning unit step function 
we introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the em algorithm the resulting model ha similarity to hidden markov model but support recurrent network processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation 
we analyse the geometry of eye rotation and in particularsaccades using basic lie group theory and differential geometry various parameterizations of rotation are related througha unifying mathematical treatment and transformation betweenco ordinate system are computed using the campbell baker hausdorff formula next we describe listing s law by mean ofthe lie algebra so this enables u to demonstrate a directconnection to donders law by showing that eye orientation 
an application of data mining technique to heterogeneous database schema integration is introduced we use attribute oriented induction to mine for characteristic and classification rule about individual attribute from heterogeneous database each mining request is conditioned on a subset of attribute identified a common between the multiple database we develop a method to compare the rule for two or more attribute from different database and use the similarity between the rule a a basis to suggest similarity between attribute a a result we use relationship between and among entire set of attribute from multiple database to drive the schema integration process our initial effort and prototype applying data mining to assist schema integration prove promising and we feel identify a fruitful application area for data mining research 
fraud and uncollectible debt are multi billion dollar problem in the telecommunication industry because it is difficult to know which account will go bad we are faced with the difficult knowledge discovery task of characterizing a rare binary outcome using large amount of noisy high dimensional data binary characterizatioos may be of interest but will not be especially useful in this domain instead proposing an action requires an estimate of the probability that a customer or a call is uncollectible this paper address the discovery of predictive knowledge bearing on fraud and uncollectible debt using a supervised machine leamiog method that construct bayesiao network model the new method is able to predict rare event outcome and cope with the quirk and copious amount of input data the bay an network model it produce serve a ao input module to a normative decision support system and suggest way to reinforce or redirect existing effort in the problem area we compare the performance of several conditionally independent model with the conditionally dependent model discovered by the new learning system using real world datasets of million record and million byte 
sequence of event describing the behavior and action of user or system can be collected in several domain in this paper we consider the problem of recognizing frequent episode in such sequence of event an episode is defined to be a collection of event that occur within time interval of a given size in a given partial order once such episode are known one can produce rule for describing or predicting the behavior of the sequence we describe an efficient algorithm for the discovery of all frequent episode from a given class of episode and present experimental result 
a gradient descent algorithm for parameter estimation which issimilar to those used for continuous time recurrent neural networkswas derived for hodgkin huxley type neuron model using membranepotential trajectory a target the parameter maximalconductances threshold and slope of activation curve time constant were successfully estimated the algorithm wa applied tomodeling slow non spike oscillation of an identified neuron in thelobster stomatogastric ganglion a 
the spatial distribution and time course of electrical signal in neuron have important theoretical and practical consequence because it is difficult to infer how neuronal form affect electrical signaling we have developed a quantitative yet intuitive approach to the analysis of electrotonus this approach transforms the architecture of the cell from anatomical to electrotonic space using the logarithm of voltage attenuation a the distance metric we describe the theory behind this approach and illustrate it use 
a new on line learning algorithm which minimizes a statistical dependency among output is derived for blind separation of mixed signal the dependency is measured by the average mutual information mi of the output the source signal and the mixing matrix are unknown except for the number of the source thegram charlier expansion instead of the edge worth expansion isused in evaluating the mi the natural gradient approach is usedto minimize the mi a novel activation function is 
ion hierarchy for constraintsatisfaction by clustering approximatelyequivalent objectsthomas ellmandepartment of computer sciencehill center for mathematical sciencesrutgers university new brunswick nj ellman c rutgers edulcsr tr abstractabstraction technique are important for solving constraint satisfaction problemswith global constraint and low solution density in the presence of global constraint backtracking search is unable to prune partial solution it 
we present utile suffix memory a reinforcement learning algorithm that us short term memory to overcome the state aliasing that result from hidden state by combining the advantage of previous work in instance based or memorybased learning and previous work with statistical test for separating noise from task structure the method learns quickly creates only a much memory a needed for the task at hand and handle noise well utile suffix memory us a tree structured representation and is related to work on prediction suffix tree ron et al parti game moore g algorithm chapman and kaelbling and variable resolution dynamic programming moore 
in this paper we describe an information retrieval problem called collection fusion the collection fusion problem is to maximize the number of relevant natural language document retrieved given a natural language query multiple collection of document and a fixed total number of document to retrieve we describe two algorithm that use past query to learn collection fusion strategy test of these algorithm on a corpus of document indicate that they can learn good fusion strategy moreover the strategy learned by our method are consistently superior to those learned by a standard learning algorithm 
recurrent perceptron classifies generalize the classical perceptron model they take into account those correlation and dependence among input coordinate which arise from linear digital filtering this paper provides tight bound on sample complexity associated to the fitting of such model to experimental data this research wa supported in part by u air force grant afosr 
a statistical approach to decision tree modeling is described in this approach each decision in the tree is modeled parametrically a is the process by which an output is generated from an input and a sequence of decision the resulting model yield a likelihood measure of goodness of fit allowing ml and map estimation technique to be utilized an efficient algorithm is presented to estimate the parameter in the tree the model selection problem is presented and several alternative proposal are considered a hidden markov version of the tree is described for data sequence that have temporal dependency 
many learning system must confront theproblem of run time after learning beinggreater than run time before learning thisutility problem ha been a particular focusof research in explanation based learning inpast work we have examined an approach tothe utility problem that is based on restrictingthe expressiveness of the rule languageso a to guarantee polynomial bound on thecost of using learned rule in this articlewe propose a new approach that limit thecost of learned rule 
we introduce a new approach for on line recognition of handwrittenwords written in unconstrained mixed style the preprocessorperforms a word level normalization by fitting a model of the wordstructure using the em algorithm word are then coded into lowresolution quot annotated image quot where each pixel contains informationabout trajectory direction and curvature the recognizer is aconvolution network which can be spatially replicated from thenetwork output a hidden markov model 
we describe a density adaptive reinforcementlearning and a density adaptive forgetting algorithm this learning algorithm us hybridk d k tree to allow for a variable resolutionpartitioning and labelling of the inputspace the density adaptive forgetting algorithmdeletes observation from the learningset depending on whether subsequent evidenceis available in a local region of the parameterspace the algorithm are demonstratedin a simulation for learning feasiblerobotic grasp 
nearest neighbor classification expects the class conditional probability to be locally constant and suffers from bias in high dimension we propose a locally adaptive form of nearest neighbor classification to try to ameliorate this curse of dimensionality we use a local linear discriminant analysis to estimate an effective metric for computing neighborhood we determine the local decision boundary from centroid information and then shrink neighborhood in direction orthogonal to these local decision boundary and elongate them parallel to the boundary thereafter any neighborhood based classifier can be employed using the modified neighborhood the posterior probability tend to be more homogeneous in the modified neighborhood we also propose a method for global dimension reduction that combine local dimension information in a number of example the method demonstrate the potential for substantial improvement over nearest neighbor classification 
multi armed bandit may be viewed a decompositionally structured markov decision process mdp s with potentially very large state set a particularly elegant methodology for computing optimal policy wa developed over twenty ago by gittins gittins jones gittins approach reduces the problem of finding optimal policy for the original mdp to a sequence of low dimensional stopping problem whose solution determine the optimal policy through the so called gittins index katehakis and veinott katehakis veinott have shown that the gittins index for a task in state i may be interpreted a a particular component of the maximum value function associated with the restart in i process a simple mdp to which standard solution method for computing optimal policy such a successive approximation apply this paper explores the problem of learning the gittins index on line without the aid of a process model it suggests utilizing task state specific q learning agent to solve their respective restart in state i subproblems and includes an example in which the online reinforcement learning approach is applied to a simple problem of stochastic scheduling one instance drawn from a wide class of problem that may be formulated a bandit problem 
this paper describes the application of reinforcement learning rl to the difficult real world problem of elevator dispatching the elevatordomain pose a combination of challenge not seen in mostrl research to date elevator system operate in continuous statespaces and in continuous time a discrete event dynamic system their state are not fully observable and they are nonstationarydue to changing passenger arrival rate in addition we use a teamof rl agent each of which is 
we discus the implication of holte s recentlypublished article which demonstrated that on the most commonly used data very simple classification rule are almost a accurate a decision tree produced by quinlan s c we consider in particular what is the significance of holte s result for the future of top down induction of decision tree to an extent holte questioned the sense of further research on multilevel decision tree learning we go in detail through all the part of holte s study we try to put the result into perspective we argue that the in absolute term small difference in accuracy between r and c that wa witnessed by holte is still significant we claim that c posse additional accuracy related advantage over r in addition we discus the representativeness of the database used by holte we compare empirically the optimal accuracy of multilevel and one level decision tree and observe some significant difference we point out several deficiency of limited complexity classifier 
although recurrent neural net have been moderately successfulin learning to emulate finite state machine fsms the continuousinternal state dynamic of a neural net are not well matchedto the discrete behavior of an fsm we describe an architecture called dolce that allows discrete state to evolve in a net a learningprogresses dolce consists of a standard recurrent neural nettrained by gradient descent and an adaptive clustering techniquethat quantizes the state space dolce is 
radial basis function rbf network also known a networksof locally tuned processing unit see are well known for theirease of use most algorithm used to train these type of network however require a fixed architecture in which the numberof unit in the hidden layer must be determined before trainingstarts the rce training algorithm introduced by reilly cooperand elbaum see and it probabilistic extension the p rcealgorithm take advantage of a growing 
diagnosis of human disease or machine fault is a missing data problem since many variable are initially unknown additional information need to be obtained the joint probability distribution of the da ta can be used to solve this problem we model this with mixture model whose parameter are estimated by the em algorithm this give the benefit that missing data in the database itself can also be handled correctly th e request for new information to refine the diagnosis is performed using the maximum utility principle since the system is based on learning it i s domain independent and le labor intensive than expert system or probabilistic network an example using a heart disease database is presented 
we present two addition to the hierarchical mixture of expert hme architecture we view the hme a a tree structured classifier firstly by applying a likelihood splitting criterion to eachexpert in the hme we quot grow quot the tree adaptively during training secondly by considering only the most probable path through thetree we may quot prune quot branch away either temporarily or permanentlyif they become redundant we demonstrate result forthe growing and pruning algorithm which show 
this paper discus why traditional reinforcement learning method and algorithm applied to those model result in poor performance in situated domain characterized by multiple goal noisy state and inconsistent reinforcement we propose a methodology for designing reinforcement function that take advantage of implicit domain knowledge in order to accelerate learning in such domain the methodology involves the use of heterogeneous reinforcement function and progress estimator and applies to learning in domain with a single agent or with multiple agent the methodology is experimentally validated on a group of mobile robot learning a foraging task 
tr paul r cohen adam carlson lisa ballesteros robert st amant automating path analysis for building causal model from data path analysis is a generalization of multiple linear regression that build model with causal interpretation it is an exploratory or discovery procedure for finding causal structure in correlational data recently we have applied statistical method such a path analysis to the problem of building model of ai program which are generally complex and poorly understood for example we built by hand a path analytic causal model of the behavior of the phoenix planner path analysis ha a huge search space however if one measure see hardcopy parameter of a system then one can build see hardcopy causal model relating these parameter for this reason we have developed an algorithm that heuristically search the space of causal model this paper describes path analysis and the algorithm and present preliminary empirical result including what we believe is the first example of a causal model of an ai system induced from performance data by another ai system 
we show that for a single neuron with the logistic function a the transfer function the number of local minimum of the error function based on the square loss can grow exponentially in the dimension 
semi markov decision problem are continuous time generalizationsof discrete time markov decision problem a number ofreinforcement learning algorithm have been developed recentlyfor the solution of markov decision problem based on the ideasof asynchronous dynamic programming and stochastic approximation among these are td q learning and real time dynamicprogramming after reviewing semi markov decision problemsand bellman s optimality equation in that context we propose 
the performance of on line algorithm for learning dichotomy is studied in on line learning the number of example p is equivalent to the learning time since each example ispresented only once the learning curve or generalization error a a function of p dependson the schedule at which the learning rate is lowered for a target that is a perceptron rule the learning curve of the perceptron algorithm can decrease a fast a p gamma if the scheduleis optimized if the target is 
we present a new approach to theory revision that us a linguistically based semantics to help detect and correct error in classification rule the idea is that preferring linguistically cohesive revision will enhance the comprehensibility and ultimately the accuracy of rule we explain how to associate term in the rule with element in a lexical class hierarchy and use distance within the hierarchy to estimate linguistic cohesiveness we evaluate the utility of this approach empirically using two relational domain 
we present a statistical method that pac learns the class of stochastic perceptrons with arbitrary monotonic activation function and weightswi f g when the probability distribution that generates the input example is member of a family that we call k blocking distribution such distribution represent an important step beyond the case where each input variable is statistically independent since the k blocking family contains all the markov distribution of order k by stochastic perceptron we mean a perceptron which upon presentation of input vector x output with probability f p iwixi because the same algorithm work for any monotonic nondecreasing or nonincreasing activation function f on boolean domain it handle the well studied case of sigmo d and the usual radial basis function 
for semantic query optimization one need detailedknowledge about the content of the database traditionaltechniques use static knowledge about all possiblestates of the database which is already given new technique use knowledge only about the currentstate of the database which can be found by methodsof knowledge discovery in database database areoften very large and permanently in use therefore method of knowledge discovery are only allowed totake a small amount of the 
we describe two parallel analog vlsi architecture that integrate optical ow data obtained from array of elementary velocity sen sors to estimate heading direction and time to contact for heading direction computation we performed simulation to evaluate the most important qualitative property of the optical ow eld and determine the best functional operator for the implementation of the architecture for time to contact we exploited the divergence theorem to integrate data from all velocity sensor present in the architecture and average out possible error 
we study the problem of when to stop learning a class of feedforward network network with linear output neuron and fixed input weight when they aretrained with a gradient descent algorithm on a finite number of example undergeneral regularity condition it is shown that there are in general three distinctphases in the generalization performance in the learning process and in particular the network ha better generalization performance when learning is stopped at acertain time 
intelligent human agent exist in a cooperativesocial environment that facilitateslearning they learn not only by trialand error but also through cooperation bysharing instantaneous information episodicexperience and learned knowledge thekey investigation of this paper are quot giventhe same number of reinforcement learningagents will cooperative agent outperformindependent agent who do not communicateduring learning quot and quot what is the pricefor such cooperation quot using 
this paper explores the application of the minimum description length principle for pruning decision tree we present a new algorithm that intuitively capture the primary goal of reducing the misclassification error an experimental comparison is presented with three other pruning algorithm the result show that the mdl pruning algorithm achieves good accuracy small tree and fast execution time 
the problem of temporal credit assignment in reinforcement learning is typically solved using algorithm based on the method of temporal difference td lambda of those q learning is currently best understood and most widely used using td based algorithm with often allows one to speed up the propagation of credit significantly but it involves certain implementational problem the traditional implementation of td based on eligibility trace suffers from lack of generality and 
this paper continues work reported at ml on the use of the minimum descriptionlength principle with non probabilistic theory a new encoding scheme is developedthat ha similar benefit to the adhocpenalty function used previously thescheme ha been implemented in c rulesand empirical trial on real world datasetsreveal a small but useful improvement in classificationaccuracy introductionwhen classifier are induced from data the resultingtheories are commonly 
an approach to improve accuracy of incorrectdomain theory is presented that learnsconcept description from positive and negativeexamples of the concept the methoduses the available domain theory that mightbe both overly general and overly specific to group training example before attemptingconcept induction gentre is a systemthat ha been implemented to test theperformance of the method gentre is notlimited to variable free function free or nonrecursivedomains a 
one of the advantage of supervised learning is that the final error metricis available during training for classifier the algorithm can directly reducethe number of misclassifications on the training set unfortunately when modeling human learning or constructing classifier for autonomousrobots supervisory label are often not available or too expensive in thispaper we show that we can substitute for the label by making use ofstructure between the pattern distribution to 
a distinct advantage of symbolic learningalgorithms over artificial neural network isthat typically the concept representationsthey form are more easily understood by human one approach to understanding therepresentations formed by neural network isto extract symbolic rule from trained network in this paper we describe and investigatean approach for extracting rule fromnetworks that us the nofm extractionalgorithm and the network trainingmethod of soft 
visualizing and structuring pairwise dissimilarity data a re difficult combinatorial optimization problem known a multidimensional scaling or pairwise data clustering algorithm for embedding dissimilarity data set in a euclidian space for clustering these data and for actively selecting data to support the clu stering process are discussed in the maximum entropy framework active data selection provides a strategy to discover structure in a data set efficiently with partially unknown data grouping experimental data into compact cluster arises a a data analysis problem in psychology linguistics genetics and other experimental sci ences the data which are supposed to be clustered are either given by an explicit coordinate re presentation central clustering or in the non metric case they are characterized by dissimilarity value for pa irs of data point pairwise clustering in this paper we study algorithm i for embedding non metric data in a dimensional euclidian space ii for simultaneous clustering and embedding of non metric data and iii for active data selection to determine a particular cluste r structure with minimal number of data query all algorithm are derived from the maximum entropy principle hertz et al which guarantee robust statistic tikochinsky et al the data are given by a real valued symmetric proximity matrix ir being the pairwise dissimilarity between the data point apart from the symmetry constraint we make no further assumption about the dissimilarity i e we do not require being a metric the number quite often violate the triangular inequality and the dissi milarity of a datum to itself could be finite 
i describe an exploration criterion that attempt to minimize the error of a learner by minimizing it estimated squared bias i describe experiment with locally weighted regression on two simple kinematics problem and observe that this bias only approach outperforms the more common variance only exploration approach even in the presence of noise 
the theory of concept or galois lattice provides a natural and formal setting in which to discover and represent concept hierarchy in this paper we present a system galois which is able to determine the concept lattice corresponding to a given set of object galois is incremental and relatively efficient the time complexity of each update ranging from o n to o n where n is the number of concept in the lattice unlike most approach to conceptual clustering galois represents and update all possible class in a restricted concept space therefore the concept hierarchy it find are always justified and are not sensitive to object ordering we experimentally demonstrate using several machine learning data set that galois can be successfully used for class discovery and class prediction we also point out application of galois in field related to machine learning i e information retrieval and database 
the parietal cortex is thought to represent the egocentric positionsof object in particular coordinate system we propose analternative approach to spatial perception of object in the parietalcortex from the perspective of sensorimotor transformation the response of single parietal neuron can be modeled a a gaussianfunction of retinal position multiplied by a sigmoid functionof eye position which form a set of basis function we show herehow these basis function can be used to 
many induction program use decision treesboth a the basis for their search and a arepresentation of their classifier solution inthis paper we propose a new structure calledse tree a a more general alternative introductionmany learning algorithm use decision tree a an underlyingframework for search and a a representationof their classifier solution e g id quinlan cart breiman et al this framework however is known to mix search bias introduced when 
in the single rent to buy decision problem without a priori knowledge of the amount oftime a resource will be used we need to decidewhen to buy the resource given that wecan rent the resource for per unit time orbuy it once and for all for c in this paper westudy algorithm that make a sequence of singlerent to buy decision using the assumptionthat the resource use time are independentlydrawn from an unknown probabilitydistribution our study of this rent to buyproblem is 
four version of a k nearest neighbor algorithm with locally adaptive k are introduced and compared to the basic k nearest neighboralgorithm knn locally adaptive knn algorithm choose thevalue of k that should be used to classify a query by consulting theresults of cross validation computation in the local neighborhoodof the query local knn method are shown to perform similar toknn in experiment with twelve commonly used data set encouragingresults in three constructed task 
pruning a decision tree is considered by some researcher to be the most important part of tree building in noisy domain while there are many approach to pruning an alternative approach of averaging over decision tree ha not received a much attention we perform an empirical comparison of pruning with the approach of averaging over decision tree for this comparison we use a computationally efficient method of averaging namely averaging over the extended fanned set of a tree since 
knowledge discovery system for database are employed to provide valuable insight into characteristic and relationshi that may exist in the data but are unknown to the user s paper describes a methodology and system for perfonrung knowledge discovery across multiple database these en hancements have been integrated into the prototype knowledge discovery system called inlen the enhancement include the incorporation of primary and foreign key a well a the development and processing of knowledge segment 
a neural network model for the self organization of ocular dominance and lateral connection from binocular input is presented the self organizing process result in a network where afferent weight of each neuron organize into smooth hill shaped receptive field primarily on one of the retina neuron with common eye preference form connected intertwined patch and lateral connection primarily link region of the same eye preference similar self organization of cortical structure ha been observed experimentally in strabismic kitten the model show how patterned lateral connection in the cortex may develop based on correlated activity and explains why lateral connection pattern follow receptive field property such a ocular dominance 
one of the most important problem in rule induction method is how to estimate which method is the best to use in an applied domain while some method are useful in some domain they ate not useful in other domain therefore it is very dificult to choose one of these method fot this purpose we introduce multiple testing based on recursive iteration of resampling method for rule induction mult recite r this method consists of four procedure which includes the inner loop and the outer loop procedure first orkginal training sample are randomly split into new training sample and teat sample t using a tesampiing scheme second are again spiii inio training sample and training sample li using the same resampling scheme rule induction method ave applied and predefined metric ate calculated this second procedure a the inner loop is repeated for time then third rule induction method are applied to and the met s calculated by tl are cornpaved with those by tz if the metric derived by tz predicts those by tl then we count it a a success the second and third procedure a the outet loop are iterated fot time finally fourth the overall result are interpreted and the best method is selected if the resampling scheme performs well in otdet to evaluate this system we apply this mult reciter method to three uci database the result show that this method give the best selection of estimation method statistically 
this paper study the problem of diffusion in markovian model such a hidden markov model hmms and how it make verydifficult the task of learning of long term dependency in sequence using result from markov chain theory we show that the problemof diffusion is reduced if the transition probability approach or under this condition standard hmms have very limited modelingcapabilities but input output hmms can still perform interestingcomputations introduction 
the dynamic of complex neural network modelling the self organization process in cortical map must include the aspect oflong and short term memory the behaviour of the network is suchcharacterized by an equation of neural activity a a fast phenomenonand an equation of synaptic modification a a slow part of theneural system we present a quadratic type lyapunov function forthe flow of a competitive neural system with fast and slow dynamicvariables we also show the consequence 
this paper present a method by which areinforcement learning agent can solve theincomplete perception problem using memory the agent us a hidden markov model hmm to represent it internal state spaceand creates memory capacity by splittingstates of the hmm the key idea is a test todetermine when and how a state should besplit the agent only split a state when doingso will help the agent predict utility thusthe agent can create only a much memory asneeded to perform the task 
an attribute oriented rough set method for knowledge discovery in database is described the method is based on information generalization which examines the data at various level of abstraction followed by the discovery analysis and simplification of significant data relationship first an attribute oriented concept tree ascension technique is applied to generalize the information this step substantially reduces the overall computational cost then rough set technique are applied to the generalized information system to derive rule the rule represent data dependency occurring in the database we focus on discovering hidden pattern in the database rather than statistical summary 
each year people spend a huge amount of time typing the text people type typically contains a tremendous amount of redundancy due to predictable word usage pattern and the text s structure this paper describes a neural network system call auto qpist that monitor a person s typing and predicts what will be entered next auto qpist display the most likely subsequent word to the typist who can accept it with a single keystroke instead of typing it in it entirety the multi layer perceptron at the heart of auto qpist adapts it prediction of likely subsequent text to the user s word usage pattern and to the characteristic of the text currently being typed increase in typing speed of when typing english prose and when typing c code have been demonstrated using the system suggesting a potential time saving of more than hour per user per year in addition to increasing typing speed autotypist reduces the number of keystroke a user must type by a similar amount for english for computer program this keystroke saving ha the potential to significantly reduce the frequency and seventy of repeated stress injury caused by typing which are the most common injury suffered in today s office environment 
this paper show that decision tree can beused to improve the performance of casebasedlearning cbl system we introducea performance task for machine learning systemscalled semi flexible prediction that liesbetween the classification task performed bydecision tree algorithm and the flexible predictiontask performed by conceptual clusteringsystems in semi flexible prediction learning should improve prediction of a specificset of feature known a priori ratherthan a single known 
the bayesian classifier is a simple approach to classification that produce result that are easy for people to interpret in many case the bayesian classifier is at least a accurate a much more sophisticated learning algorithm that produce result that are more difficult for people to interpret to use numeric attribute with bnyesian classifier often requires the attribute value to be discretized into a number of interval we show that the discretization of numeric attribute is critical to successful application of the bayesinn classifier and propose a new method based on iterative improvement search we compare this method to previous approach and show that it result in signiticrmt reduction in misclassification error and cost on an industrial problem of troubleshooting the local loop in a telephone network the approach can take prior knowledge into account by improving upon a user provided set of boundxy point or can operate autonomously 
we present an integrated analog processor for real time wavelet decompositi on and reconstruction of continuous temporal signal covering the audio frequen cy range the processor performs complex harmonic modulation and gaussian lowpass filtering i n parallel identical channel each channel clocked at a different rate to produce a multiresolution m apping on a logarithmic frequency scale our implementation us mixed mode analog and digital circuit oversampling technique and switched capacitor filter to achieve a wide linear d ynamic range while maintaining compact circuit size and low power consumption we include experimental result on the channel processor and characterize it component separately from measurement on a reduced scale single channel test chip 
a statistical theory for overtraining is proposed the analysistreats realizable stochastic neural network trained with kullbackleiblerloss in the asymptotic case it is shown that the asymptoticgain in the generalization error is small if we perform early stopping even if we have access to the optimal stopping time consideringcross validation stopping we answer the question in what ratiothe example should be divided into training and testing set in orderto obtain the optimum 
we present an abstraction of the genetic algorithm ga termed population based incremental learning pbil that explicitly maintains the statistic contained in a ga s population but which abstract away the crossover operator and redefines the role of the population this result in pbil being simpler both computationally and theoretically than the ga empirical result reported elsewhere show that pbil is faster and more effective than the ga on a large set of commonly used benchmark problem here we present result on a problem custom designed to benefit both from the ga s crossover operator and from it use of a population the result show that pbil performs a well a or better than gas carefully tuned to do well on this problem this suggests that even on problem custom designed for gas much of the power of the ga may derive from the statistic maintained implicitly in it population and not from the population itself nor from the crossover operator 
selective suppression of transmission at feedback synapsis during learning is proposed a a mechanism for combining associative feedback with self organization of feedforward synapsis experimental data demonstrates cholinergic suppression of synaptic transmission in layer i feedback synapsis and a lack of suppression in layer iv feedforward synapsis a network with this feature us local rule to learn mapping which are not linearly separable during learning sensory stimulus and desired response are simultaneously presented a input feedforward connection form self organized representation of input while suppressed feedback connection learn the transpose of feedforward connectivity during recall suppression is removed sensory input activates the self organized representation and activity generates the learned response 
we present experimental result on supervised learning of dynamicalfeatures in an analog vlsi neural network chip the recurrentnetwork containing six continuous time analog neuron and free parameter connection strength and threshold is trained togenerate time varying output approximating given periodic signalspresented to the network the chip implement a stochastic perturbativealgorithm which observes the error gradient along randomdirections in the parameter space for 
array x fc c hierarchical or multi level planning and reinforcement learning in this paper we treat only the prediction problem that of learning a model and value function for the case of fixed agent behavior within this context we establish the theoretical foundation of multi scale model and derive td algorithm for learning them two small computational experiment are presented to test and illustrate the theory this work is an extension and generalization of the work of singh dayan and sutton and pinette 
temporal difference td learning can beused not just to predict reward a is commonlydone in reinforcement learning butalso to predict state i e to learn a modelof the world s dynamic we present theoryand algorithm for intermixing td modelsof the world at different level of temporalabstraction within a single structure such multi scale td model can be used inmodel based reinforcement learning architecturesand dynamic programming method inplace of conventional markov 
in many vision based task the ability to focus attention on the important portion of a scene is crucial for good performance on the task in this paper we present a simple method of achieving spatial selective attention through the use of a saliency map the saliency map indicates which region of the input retina are important for performing the task the saliency map is created through predictive autoencoding the performance of this method is demonstrated on two simple task which have multiple very strong distracting feature in the input retina architectural extension and application direction for this model are presented motivation many real world task have the property that only a small fraction of the available input is important at any particular time on some task this extra input can easily be ignored nonetheless often the similarity between the important input feature and the irrelevant feature is great enough to interfere with task performance ho example of this phenomenon are the famous cocktail party effect otherwise known a speech recognition in a noisy environment and image processing of a cluttered scene in both case the extraneous information in the input signal can be easily confused with the important feature making the task much more difficult the concrete real world task which motivates this work is vision based road following in this domain the goal is to control a robot vehicle by analyzing the scene ahead and choosing a direction to travel based on the location of important feature like lane marking and road edge this is a difficult task since the scene ahead is often cluttered with extraneous feature such a other vehicle pedestrian tree guardrail crosswalk road sign and many other object that can appear on or around a roadway while we have had significant success on the road following task using simple feed forward neural network to transform image of the road ahead into steering command for the vehicle pomerleau b these method fail when presented with cluttered environment like those encoun for the general task of autonomous navigation these extra feature are extremely important but for restricted task of road following which is the focus of this paper these feature are merely distraction although we are addressing the more general task using the technique described here in combination with other method a description of these effort is beyond the scope of this paper 
this paper discus the linearly weighted combination of estimator in which the weighting function are dependent on the input we show that the weighting function can be derived either by evaluating the input dependent variance of each estimator or by estimating how likely it is that a given estimator ha seen data in the region of the input space close to the input pattern the latter solution is closely related to the mixture of expert approach and we show how learning rule for the mixture of expert can be derived from the theory about learning with missing feature the presented approach are modular since the weighting function can easily be modifled no retraining if more estimator are added furthermore it is easy to incorporate estimator which were not derived from data such a expert system or algorithm 
both the number and the size of spatial database are rapidly growing because of the large amount of data obtained from satellite image x ray crystallography or other scientific equipment therefore automated knowledge discovery becomes more and more important in spatial database so far most of the method for knowledge discovery in database kdd have been based on relational database system in this paper we address the task of class identification in spatial database using clustering technique we present an interface to the database management system dbms which is crucial for the efficiency of kdd on large database this interface is based on a spatial access method the r tree it cluster the object according to their spatial neighborhood and support efficient processing of spatial query furthermore we propose a method for spatial data sampling a part of the focusing component significantly reducing the number of object to be clustered thus we achieve a considerable speed up for clustering in large database we have applied the proposed technique to real data from a lar ge protein database used for predicting protein protein docking a performance evaluation on this database indicates that clustering on lar ge spatial database can be performed both efficiently and effectively using our approach 
a method for incorporating context dependent phone class ina connectionist hmm hybrid speech recognition system is introduced a modular approach is adopted where single layer networksdiscriminate between different context class given the phone classand the acoustic data the context network are combined with acontext independent ci network to generate context dependent cd phone probability estimate experiment show an averagereduction in word error rate of and from the 
an application of laterally interconnected self organizing map lissom to handwritten digit recognition is presented the lat eral connection learn the correlation of activity between unit on the map the resulting excitatory connection focus the activity into local patch and the inhibitory connection decorrelate redun dant activity on the map the map thus form internal representa tions that are easy to recognize with e g a perceptron network the recognition rate on a subset of nist database is higher with lissom than with a regular self organizing map som a the front end and higher than recognition of raw input bitmap directly these result form a promising starting point for building pattern recognition system with a lissom map a a front end 
this paper present a method for using qualitativemodels to guide inductive learning our objective are to induce rule whichare not only accurate but also explainablewith respect to the qualitative model andto reduce learning time by exploiting domainknowledge in the learning process such explainabilityis essential both for practical applicationof inductive technology and for integratingthe result of learning backintoan existing knowledge base we apply thismethod to 
we construct a mixture of locally linear generative model of a collection of pixel based image of digit and use them for recognition different model of a given digit are used to capture different style of writing and new image are classified by evaluating their log likelihood under each model we use an em based algorithm in which the m step is computationally straightforward principal component analysis pca incorporating tangent plane information about expected local deformation only requires adding tangent vector into the sample covariance matrix for the pca and it demonstrably improves performance 
abstract uncertainty sampling method iteratively request class label for training instance whose class are uncertain despite the previous labeled instance these method can greatly reduce the number of instance that an expert need label one problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instance we test the use of one classifier a highly efficient probabilistic one to select example for training another the c rule induction program despite being chosen by this heterogeneous approach the uncertainty sample yielded classifier with lower error rate than random sample ten time larger 
this paper present the design and simulation result of a selforganizing neural network which induces a grammar from example sentence input sentence are generated from a simple phrase structure grammar including number agreement verb transitivity and recursive noun phrase construction rule the network induces a grammar explicitly in the form of symbol categorization rule and phrase structure rule 
this paper present instance based state identification an approachto reinforcement learning and hidden state that build disambiguatingamounts of short term memory on line and also learns with anorder of magnitude fewer training step than several previous approach inspired by a key similarity between learning with hiddenstate and learning in continuous geometrical space this approachuses instance based or quot memory based quot learning a method thathas worked well in continuous 
explanation based learning ha typicallybeen considered a symbolic learning method an explanation based learning method thatutilizes purely neural network representation called ebnn ha recently been developed and ha been shown to have several desirableproperties including robustness to errorsin the domain theory this paper brieflysummarizes the ebnn algorithm then exploresthe correspondence between this neuralnetwork based ebl method and eblmethods based on symbolic 
we evaluate the first order learning system foil on a series of text categorization problem it is shown that foil usually form classifier with lower error rate and higher rate of precision and recall with a relational encoding than with a propositional encoding we show that foil s performance can be improved by relation selection a first order analog of feature selection relation selection improves foil s performance a measured by any of recall precision f measure or error rate with an appropriate level of relation selection foil appears to be competitive with or superior to existing propositional technique 
this paper present an approach to automatic discovery of function in genetic programming the approach is based on discovery of useful building block by analyzing the evolution trace generalizing block to define new function and finally adapting the problem representation onthe fly adaptating the representation determines a hierarchical organization of the extended function set which enables a restructuring of the search space so that solution can be found more easily measure of complexity of solution tree are defined for an adaptive representation framework the minimum description length principle is applied to justify the feasibility of approach based on a hierarchy of discovered function and to suggest alternative way of defining a problem s fitness function preliminary empirical result are presented next we introduce different notion of complexity useful in controlling the algorithm we view the problem of determining a gp program that satisfies the criterion imposed by the fitness function a one of hypothesis formation this enables u to apply the minimum description length principle to analyze the power of approach based on automatic discovery of function we conclude by analyzing an example mentioning related work and considering direction for future work 
the information age is characterized by a rapid growth in the amount of information available in electronic medium traditional data handling method are not adequate to cope with this information flood knowledge discovery in database kdd is a new paradigm that focus on computerized exploration of large amount of data and on discovery of relevant and interesting pattern within them while most work on kdd is concerned with structured database it is clear that this paradigm is required for handling the huge amount of information that is available only in unstructured textual form to apply traditional kdd on text it is necessary to impose some structure on the data that would be rich enough to allow for interesting kdd operation on the other hand we have to consider the severe limitation of current text processing technology and define rather simple structure that can be extracted from text fairly automatically and in a reasonable cost we propose using a text categorization paradigm to annotate text article with meaningful concept that are organized in hierarchical structure we suggest that this relatively simple annotation is rich enough to provide the basis for a kdd framework enabling data summarization exploration of interesting pattern and trend analysis this research combine the kdd and text categorization paradigm and suggests advance to the state of the art in both area 
reinforcement learning rl algorithm providea sound theoretical basis for buildinglearning control architecture for embeddedagents unfortunately all of the theory andmuch of the practice see barto et al for an exception of rl is limited to markoviandecision process mdps many realworlddecision task however are inherentlynon markovian i e the state of the environmentis only incompletely known to the learningagent in this paper we consider only partially 
an autoencoder network us a set of recognition weight to convertan input vector into a code vector it then us a set of generative weight to convert the code vector into an approximate reconstructionof the input vector we derive an objective function fortraining autoencoders based on the minimum description length mdl principle the aim is to minimize the information requiredto describe both the code vector and the reconstruction error weshow that this information is 
recent development in the area of reinforcement learning haveyielded a number of new algorithm for the prediction and control ofmarkovian environment these algorithm including the td algorithmof sutton and the q learning algorithm of watkins can be motivated heuristically a approximation to dynamic programming dp in this paper we provide a rigorous proof of convergence ofthese dp based learning algorithm by relating them to the powerfultechniques of stochastic 
we prove that the so called loading problem for recurrent neural network is unsolvable this extends several result which already demonstrated that training and related design problem for neura l network are at least np complete our result also implies that it is imp ossible to find or to formulate a universal training algorithm which for any neural network architecture could determine a correct set of we ights for the simple proof of this we will just show that the loading pr oblem is equivalent to hilbert s tenth problem which is known to be unsolvable 
eficient and efiective discovery of resource and knowledge from the internet ha become an imminent research issue especially with the advent of the information super highway a multiple layered database mldb approach is proposed to handle the resource and knowledge discovery in global information base a preliminary experiment show the advantage of such an approach information retrieval data mining and data analysis technique can be used to extract and transform information from a lower layer database to a higher one resource can be found by controlled search through dinerent layer of the database and knowledge discovery can be performed eficiently in such a layered database 
we report a novel possibility for extracting asmall subset of a data base which contains allthe information necessary to solve a given classificationtask using the support vector algorithmto train three different type of handwrittendigit classifier we observed that these typesof classifier construct their decision surface fromstrongly overlapping small subset of thedata base this finding open up the possibilityof compressing data base significantly by disposingof the 
with the goal of reducing computational cost without sacrificing accuracy we describe two algorithm to find set of prototype for nearest neighbor classification here the term prototype refers to the reference instance used in a nearest neighbor computation the instance with respect to which similarity is assessed in order to assign a class to a new data item both algorithm rely on stochastic technique to search the space of set of prototype and are simple to implement the first is a monte carlo sampling algorithm the second applies random mutation hill climbing on four datasets we show that only three or four prototype sufficed to give predictive accuracy equal or superior to a basic nearest neighbor algorithm whose run time storage cost were approximately to time greater we briefly investigate how random mutation hill climbing may be applied to select feature and prototype simultaneously finally we explain the performance of the sampling algorithm on these datasets in term of a statistical measure of the extent of clustering displayed by the target class 
in this paper we introduce ant q a family of algorithm which present many similarity with q learning watkins and which we apply to the solution of symmetric and asymmetric instance of the traveling salesman problem tsp ant q algorithm were inspired by work on the ant system a a distributed algorithm for combinatorial optimization based on the metaphor of ant colony which wa recently proposed in dorigo dorigo maniezzo and colorni we show that a is a particular instance of the ant q family and that there are instance of this family which perform better than a we experimentally investigate the functioning of ant q and we show that the result obtained by ant q on symmetric tsp s are competitive with those obtained by other heuristic approach based on neural network or local search finally we apply ant q to some difficult asymmetric tsp s obtaining very good result ant q wa able to find solution of a quality which usually can be found only by very specialized algorithm 
experiment were performed to reveal some of the computationalproperties of the human motor memory system we show thatas human practice reaching movement while interacting with anovel mechanical environment they learn an internal model of theinverse dynamic of that environment subject show recall of thismodel at testing session hour after the initial practice therepresentation of the internal model in memory is such that thereis interference when there is an attempt to learn 
abstract random error and insufficiency in database limit the performance of any classifier trained from commadatabase in this paper we propose a method to estimate the limiting performance of classifier imposed by the database we demonstrate this technique on the task of predicting failure in telecommunication path 
we have developed an artificial neural network based gaze tracking system which can be customized to individual user a three layer feed forward network trained with standard error back propagation is used to determine the position of a user s gaze from the appearance of the user s eye unlike other gaze tracker which normally require the user to wear cumbersome headgear or to use a chin rest to ensure head immobility our system is entirely non intrusive currently the best intrusive gaze tracking system are accurate to approximately degree in our experiment we have been able to achieve an accuracy of degree while allowing head mobility in it current implementation our system work at hz in this paper we present an empirical analysis of the performance of a large number of artificial neural network architecture for this task suggestion for further exploration for neurally based gaze tracker are presented and are related to other similar artificial neural network application such a autonomous road following 
we present a bottom up generalization whichbuilds the maximally general term coveringa positive example and rejecting negativeexamples in first order logic fol i e interms of version space the set g this algorithm is based on rewriting negativeexamples a constraint upon the generalizationof the positive example at hand theconstraints space is partially ordered inducinga partial order on negative example thenear miss a defined by winston can thenbe formalized in fol 
many machine learning algorithm aim atfinding quot simple quot rule to explain trainingdata the expectation is the quot simpler quot the rule the better the generalization ontest data occam s razor most practicalimplementations however use measuresfor quot simplicity quot that lack the power universalityand elegance of those based on kolmogorovcomplexity and solomonoff s algorithmicprobability likewise most previousapproaches especially those of the quot bayesian quot kind suffer from the problem 
the success of reinforcement learning in practical problem depends on the ability to combine function approximation with temporal difference method such a value iteration experiment in this area have produced mixed result there have been both notable success and notable disappointment theory ha been scarce mostly due to the difficulty of reasoning about function approximators that generalize beyond the observed data we provide a proof of convergence for a wide class of temporal difference method involving function approximators such a k nearest neighbor and show experimentally that these method can be useful the proof is based on a view of function approximators a expansion or contraction mapping in addition we present a novel view of approximate value iteration an approximate algorithm for one environment turn out to be an exact algorithm for a different environment 
incorporating declarative bias or priorknowledge into learning is an active researchtopic in machine learning treestructuredbias specifies the prior knowledgeas a tree of quot relevance quot relationshipsbetween attribute this paper present alearning algorithm that implement treestructuredbias i e learns any target functionprobably approximately correctly fromrandom example and membership queriesif it obeys a given tree structured bias thetheoretical prediction of the paper are 
an approach is presented to learning high dimensional function in the case where the learning algorithm can affect the generation of new data a local modeling algorithm locally weighted regression is used to represent the learned function architectural parameter of the approach such a distance metric are also localized and become a function of the query point instead of being global statistical test are given for when a local model is good enough and sampling should be moved to a new area our method explicitly deal with the case where prediction accuracy requirement exist during exploration by gradually shifting a center of exploration and controlling the speed of the shift with local prediction accuracy a goal directed exploration of state space take place along the fringe of the current data support until the task goal is achieved we illustrate this approach with simulation result and result from a real robot learning a complex juggling task 
the feature correspondence problem is a classic hurdle in visualobject recognition concerned with determining the correct mappingbetween the feature measured from the image and the feature expectedby the model in this paper we show that determining goodcorrespondences requires information about the joint probabilitydensity over the image feature we propose quot likelihood basedcorrespondence matching quot a a general principle for selecting optimalcorrespondences the approach is 
we propose a statistical mechanical framework for the modelingof discrete time series maximum likelihood estimation is done viaboltzmann learning in one dimensional network with tied weight we call these network boltzmann chain and show that theycontain hidden markov model hmms a a special case ourframework also motivates new architecture that address particularshortcomings of hmms we look at two such architecture parallel chain that model feature set with disparate time 
this paper describes an approach to automatically learn planning operator by observing expert solution trace and to further refine the operator through practice in a learning by doing paradigm this approach us the knowledge naturally observable when expert solve problem without need of explicit instruction or interrogation the input to our learning system are the description language for the domain expert problem solving trace and practice problem to allow learning by doing operator refinement given these input our system automatically acquires the precondition and effect including conditional effect and precondition of the operator we present empirical result to demonstrate the validity of our approach in the process planning domain these result show that the system learns operator in this domain well enough to solve problem a effectively a human expert coded operator our approach differs from knowledge acquisition toolsin that it doe not require a considerable amount of direct interaction with domain expert it differs from other work on automaticallylearning operator in that it doe not require initial approximate planning operator or strong background knowledge 
this paper concern learning task that require the prediction of a continuous value rather than a discrete class a general method is presented that allows prediction to use both instance based and model based learning result with three approach to constructing model and with eight datasets demonstrate improvement due to the composite method 
we consider the question how should one act when the only goal is to learn a much a possible building on the theoretical result of fedorov and mackay we apply technique from optimal experiment design oed to guide the query action selection of a neural network learner we demonstrate that these technique allow the learner to minimize it generalization error by exploring it domain efficiently and completely we conclude that while not a panacea oed based query action ha much to offer especially in domain where it high computational cost can be tolerated 
we are developing special purpose low power analog to digitalconverters for speech and music application that feature analogcircuit model of biological audition to process the audio signalbefore conversion this paper describes our most recent converterdesign and a working system that us several copy of the chip tocompute multiple representation of sound from an analog input this multi representation system demonstrates the plausibility ofinexpensively implementing an auditory 
we describe a framework for real time tracking of facial exp ressions that us neurally inspired correlation and interpolatio n method a distributed view based representation is used to characte rize facial state and is computed using a replicated correlation network the ensemble response of the set of view correlation score is input to a ne twork based interpolation method which map perceptual state to motor control state for a simulated d face model activation level of the motor state correspond to muscle activation in an anatomically derived model by integrating fast and robust d processing with d model we obtain a system that is able to quickly track and interpret complex fa cial motion in real time 
learning of continuous valued function using neural network ensemble committee can give improved accuracy reliable estimationof the generalization error and active learning the ambiguityis defined a the variation of the output of ensemble member averagedover unlabeled data so it quantifies the disagreement amongthe network it is discussed how to use the ambiguity in combinationwith cross validation to give a reliable estimate of the ensemblegeneralization error and how this 
most connectionist research ha focused on learning mapping from one space to another eg classification and regression this paper introduces the more general task of learning constraint surface it describes a simple but powerful architecture for learning and manipulating nonlinear surface from data we demonstrate the technique on low dimensional synthetic surface and compare it to nearest neighbor approach we then show it utility in learning the space of lip image in a system for improving speech recognition by lip reading this learned surface is used to improve the visual tracking performance during recognition 
we present a new incremental radial basis function network suitablefor classification and regression problem center positionsare continuously updated through soft competitive learning thewidth of the radial basis function is derived from the distanceto topological neighbor during the training the observed erroris accumulated locally and used to determine where to insert thenext unit this lead in case of classification problem to theplacement of unit near class border 
we introduce a constructive incremental learning system for regression problem that model data by mean of locally linear expert in contrast to other approach the expert are trained independently and do not compete for data during learning only when a prediction for a query is required do the expert cooperate by blending their individual prediction each expert is trained by minimizing a penalized local cross validation error using second order method in this way an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which it prediction are valid and also to detect relevant input feature by adjusting it bias on the importance of individual input dimension we derive asymptotic result for our method in a variety of simulation the property of the algorithm are demonstrated with respect to interference learning speed prediction accuracy feature detection and task oriented incremental learning 
we discus two type of algorithm for selectingrelevant example that have been developed in thecontext of computation learning theory the examplesare selected out of a stream of examplesthat are generated independently at random thefirst two algorithm are the so called quot boosting quot algorithm of schapire schapire and freund freund and the query by committeealgorithm of seung seung et al we describethe algorithm and some of their provenproperties 
the game of go ha a high branching factor that defeat the tree search approach used in computer chess and long range spatiotemporal interaction that make position evaluation extremely difficult development of conventional go program is hampered by their knowledge intensive nature we demonstrate a viable alternative by training network to evaluate go position via temporal difference td learning our approach is based on network architecture that reflect the spatial organization of both input and reinforcement signal on the go board and training protocol that provide exposure to competent though unlabelled play these technique yield far better performance than undifferentiated network trained by selfplay alone a network with le than weight learned within game of x go a position evaluation function that enables a primitive one ply search to defeat a commercial go program at a low playing level 
we describe the use of smoothing spline analysis of variance ssanova in the penalized log likelihood context for learning estimating the probability p of a outcome given a trainingset with attribute vector and outcome p is of the formp t ef t ef t where if t is a vector of attribute fis learned a a sum of smooth function of one attribute plus asum of smooth function of two attribute etc the smoothingparameters governing f are obtained by an 
we have recently developed a theory of spatial representation inwhich the position of an object is not encoded in a particular frameof reference but instead involves neuron computing basis functionsof their sensory input this type of representation is ableto perform nonlinear sensorimotor transformation and is consistentwith the response property of parietal neuron we now askwhether the same theory could account for the behavior of humanpatients with parietal lesion these 
in this paper we examine a perceptron learning task the task isrealizable since it is provided by another perceptron with identicalarchitecture both perceptrons have nonlinear sigmoid outputfunctions the gain of the output function determines the level ofnonlinearity of the learning task it is observed that a high levelof nonlinearity lead to overfitting we give an explanation for thisrather surprising observation and develop a method to avoid theoverfitting this method ha two 
a theory of early stopping a applied to linear model is presented the backpropagation learning algorithm is modeled a gradientdescent in continuous time given a training set and a validationset all weight vector found by early stopping must lie on a certainquadric surface usually an ellipsoid given a training set anda candidate early stopping weight vector all validation set haveleast square weight lying on a certain plane this latter fact canbe exploited to estimate the 
efficient utilization of cell switched network supporting diverse application will require service discipline that are well designed for the particular quality of service constraint and traffic mix a difficult task in view of the paucity of information about the expected traffic we demonstrate the use of on line dynamic programming in an adaptive cell scheduling mechanism that can easily be engineered to meet arbitrary quality of service constraint when the objective is to minimize the total cell loss rate our algorithm urgency scheduling compare favorably with the optimal earliest deadline first algorithm for more complex quality of service constraint where optimal scheduling algorithm are unavailable the simulation show urgency scheduling can provide significant increase in the usable bandwidth of a link the learning technique we develop are quite general and should be readily applicable to other network control problem 
the main objective of machine discovery is the determination of relation between data and of data model in the paper we describe a method for discovery of data model represented by concurrent system from experimental table the basic step consists in a determination of role which yield a decomposition of experimental data table the component are then used to define fragment of the global system corresponding to a table the method ha been applied for automatic data model discovery from experimental table with petri net a model for concurrency 
discovering repetitive interesting and functional substructure in a structural database improves the ability to interpret and compress the data however scientist working with a database in their area of expertise often search for predetermined type of structure or for structure exhibiting characteristic specific to the domain this paper present a method for guiding the discovery process with domain specific knowledge in this paper the jbdufi discovery system is used to evaluate the benefit of using domain knowledge to guide the discovery process the domain knowledge is incorporated into subdue following a single general methodology to guide the discovery process result show that domain specific knowledge improves the search for substructure which are useful to the domain and lead to greater compression of the data to illustrate these benefit example and experiment from the computer programming computer aided design circuit and artificially generated domain are presented 
a new learning algorithm is derived which performs online stochasticgradient ascent in the mutual information between output andinputs of a network in the absence of a priori knowledge aboutthe signal and noise component of the input propagation ofinformation depends on calibrating network non linearity to thedetailed higher order moment of the input density function byincidentally minimising mutual information between output aswell a maximising their individual 
probabilistic expert system based on bayesian network bns require initial specification both a qualitative graphical structure and quantitative assessment of conditional probability table this paper considers statistical batch learning of the probability table on the basis of incomplete data and expert knowledge the em algorithm with a generalized conjugate gradient acceleration method ha been dedicated to quantification of bns by maximum posterior likelihood estimation for a super class of the recursive graphical model this new class of model allows a great variety of local functional restriction to be imposed on the statistical model which hereby extent the control and applicability of the constructed method for quantifying bns 
model selection is important in many area of supervised learning given a dataset and a set of model for predicting with that dataset we must choose the model which is expected to best predict future data in some situation such a online learning for control of robot or factory data is cheap and human expertise costly cross validation can then be a highly effective method for automatic model selection large scale cross validation search can however be computationally expensive this paper introduces new algorithm to reduce the computational burden of such search we show how experimental design method can achieve this using a technique similar to a bayesian version of kaelbling s interval estimation several improvement are then given including the use of blocking to quickly spot near identical model and schema search a new method for quickly finding family of relevant feature experiment are presented for robot data and noisy synthetic datasets the new algorithm speed up computation without sacrificing reliability and in some case are more reliable than conventional technique 
hinton proposed that generalization in artificial neural net should improve if net learn to represent the domain s underlying regularity abu mustafa s hint work show that the output of a backprop net can be used a input through which domain specific information can be given to the net we extend these idea by showing that a backprop net learning many related task at the same time can use these task a inductive bias for each other and thus learn better we identify five 
many different discrete time recurrent neural network architectureshave been proposed however there ha been virtually noeffort to compare these architecture experimentally in this paperwe review and categorize many of these architecture and comparehow they perform on various class of simple problem includinggrammatical inference and nonlinear system identification introductionin the past few year several recurrent neural network architecture have emerged in this paper we 
we present a mean field theory method for locating twodimensionalobjects that have undergone rigid transformation the resulting algorithm is a coarse to fine correlation matching we first consider problem of matching synthetic point data andderive a point matching objective function a tractable line segmentmatching objective function is derived by considering eachline segment a a dense collection of point and approximating itby a sum of gaussians the algorithm is tested on real 
one of the central problem in the field of knowledge discovery is the development of good measure of interestingness of discovered pattern such measure of interestingness are divided into objective measure those that depend only on the structure of a pattern and the underlying data used in the discovery process and the subjective measure those that also depend on the class of user who examine the pattern the purpose of this paper is to lay the groundwork for a comprehensive study of subjective measure of interestingness in the paper we classify these measure into actionable and unexpected and examine the relationship between them the unexpected measure of interestingness is defined in term of the belief system that the user ha interestingness of a pattern is expressed in term of how it affect the belief system 
this paper describes a new method for inducinglogic program from example which attemptsto integrate the best aspect of existingilp method into a single coherent framework in particular it combine a bottomupmethod similar to golem with a topdownmethod similar to foil it also includesa method for predicate invention similarto champ and an elegant solution tothe quot noisy oracle quot problem which allows thesystem to learn recursive program withoutrequiring a complete set of 
short term memory is indispensable for the processing of timevarying information with artificial neural network in this paper amodel for linear memory is presented and way to includememories in connectionist topology are discussed a comparisonis drawn among different memory type with indication of what isthe salient characteristic of each memory model 
an important area of learning in autonomous agent is the ability to learn domain specific model of action to be used by planning system in this paper we present method by which an agent learns action model from it own experience and from it observation of a domain expert these method differ from previous work in the area in two way the use of an action model formalism which is better suited to the need of a reactive agent and successful implementation of noise handling 
understanding high dimensional real worlddata usually requires learning the structureof the data space the structure may containhigh dimensional cluster that are related incomplex way method such a merge clusteringand self organizing map are designedto aid the visualization and interpretation ofsuch data however these method often failto capture critical structural property of theinput although self organizing map capturehigh dimensional topology they do notrepresent 
we present a new method for obtaining local error bar for nonlinear regression i e estimate of the confidence in predicted value that depend on the input we approach this problem by applying a maximumlikelihood framework to an assumed distribution of error we demonstrate our method first on computer generated data with locally varying normally distributed target noise we then apply it to laser data from the santa fe time series competitionwhere the underlying system noise is known quantization error and the error bar give local estim ate of model misspecification in both case the method also provides a weightedregression effect that improves generalization performan ce 
in this paper we will consider the problem of classifying electroencephalogram eeg signal of normal subject and subject suffering from psychiatric disorder e g obsessivecompulsive disorder schizophrenia using a class of artificial neural network viz multilayerperceptron it is shown that the multilayer perceptron is capable of classifyingunseen test eeg signal to a high degree of accuracy introductionthe spontaneous electrical activity of the brain wa first 
concept learned by neural network are difficultto understand because they are representedusing large assemblage of real valuedparameters one approach to understandingtrained neural network is to extractsymbolic rule that describe their classificationbehavior there are several existingrule extraction approach that operate bysearching for such rule we present a novelmethod that cast rule extraction not a asearch problem but instead a a learningproblem in addition to 
many machine learning either supervised or unsupervised technique assume that data present themselves in an attribute value form but this formalism is largely insufficient to account for many application therefore much of the ongoing research now focus on first order learning system but complex formalism lead to high computational complexity on the other hand most of the currently installed database have been designed according to a formalism known a entity relationship and usually implemented on a relational database management system this formalism is far le complex than first order logic but much more expressive than attribute value list in that context the database schema defines an abstraction space and learning must occur at each level of abstraction this paper describes a clustering system able to discover useful grouping in structured database it is based in the cobweb algorithm to which it add the ability to cluster structured object 
i propose a learning algorithm for learning hierarchical mo dels for object recognition the model architecture is a compositional hierarchy that represents part whole relationship part are descr ibed in the local context of substructure of the object the focus of this report is learning hierarchical model from data i e inducing the s tructure of model prototype from observed exemplar of an object at each node in the hierarchy a probability distribution governing it parameter must be learned the connection between node reflects the struc ture of the object the formulation of substructure is encouraged such that their part become conditionally independent the resulting model can be interpreted a a bayesian belief networkand also is in many respect similar to the stochastic visual grammardescribed by mjolsness 
we estimate the number of training sample required to ensure that the performance of a neural network on it training data match that obtained when fresh data is applied to the network existing estimate are higher by order of magnitude than practice indicates this work seek to narrow the gap between theory and practice by transforming the problem into determining the distribution of the supremum of a random field in the space of weight vector which in turn is attacked by application of a recent technique called the poisson clumping heuristic 
in this paper we present a novel induction algorithmfor bayesian network this selectivebayesian network classifier selects a subset ofattributes that maximizes predictive accuracyprior to the network learning phase therebylearning bayesian network with a bias for small high predictive accuracy network we comparethe performance of this classifier with selectiveand non selective naive bayesian classifier weshow that the selective bayesian network classifierperforms significantly 
we present e cient algorithm for dealing with the problem of missing input incomplete feature vector during training and recall our approach is based on the approximation of the input data distribution using parzen window for recall we obtain closed form solution for arbitrary feedforward network for training we show how the backpropagation step for an incomplete pattern can be approximated by a weighted averaged backpropagation step the complexity of the solution for training and recall is independent of the number of missing feature we verify our theoretical result using one classiflcation and one regression problem 
we compare activation function in term of the approximationpower of their feedforward net we consider the case of analog aswell a boolean input introductionwe consider efficient approximation of a given multivariate function f gamma m r by feedforward neural network we first introduce the notion of a feedforwardnet let gamma be a class of real valued function where each function is defined on somesubset of r a gamma net c is an unbounded fan in circuit whose 
in the markov decision process mdp formalization of reinforcement learning a single adaptive agent interacts with an environment defined by a probabilistic transition function in this solipsistic view secondary agent can only be part of the environment and are therefore fixed in their behavior the framework of markov game allows u to widen this view to include multiple adaptive agent with interacting or competing goal this paper considers a step in this direction in which exactly two agent with diametrically opposed goal share an environment it describes a q learning like algorithm for finding optimal policy and demonstrates it application to a simple two player game in which the optimal policy is probabilistic 
this paper describes experimental result on using winnow and weighted majority based algorithm on a real world calendar scheduling domain these two algorithm have been highly studied in the theoretical machine learning literature we show here that these algorithm can be quite competitive practically outperforming the decision tree approach currently in use in the calendar apprentice system in term of both accuracy and speed one of the contribution of this paper is a new variant on the 
we present a statistical method that exactly learns the class of constant depth perceptron network with weight taken from f gand arbitrary threshold when the distribution that generates the input example is member of the family of product distribution these network also known a nonoverlapping perceptron network or read once formula over a weighted threshold basis are loop free neural net in which each node ha only one outgoing weight with arbitrary high probability the learner is able to exactly identify the connectivity or skeleton of the target perceptron network by using a new statistical test which exploit the strong unimodality property of sum of independent random variable 
this paper describes a policy iteration algorithm for optimizing theperformance of a harmonic function based controller with respectto a user defined index value function are represented a potentialdistributions over the problem domain being control policiesrepresented a gradient field over the same domain all intermediatepolicies are intrinsically safe i e collision are not promotedduring the adaptation process the algorithm ha efficient implementationin parallel simd 
the result of empirical comparison of existinglearning algorithm illustrate that eachalgorithm ha a selective superiority it is bestfor some but not all task given a data set it is often not clear beforehand which algorithmwill yield the best performance insuch case one must search the space of availablealgorithms to find the one that producesthe best classifier in this paper we presentan approach that applies knowledge aboutthe representational bias of a set of learning 
in consideration of attention a a mean for goal directed behaviorin non stationary environment we argue that the dynamic ofattention should satisfy two opposing demand long term maintenanceand quick transition these two characteristic are contradictorywithin the linear domain we propose the near saddlenodebifurcation behavior of a sigmoidal unit with self connectionas a candidate of dynamical mechanism that satisfies both of thesedemands we further show in simulation of the 
random error and insufficiency in database limit the performance of any classifier trained from and applied to the database in this paper we propose a method to estimate the limiting performance of classifier imposed by the database we demonstrate this technique on the task of predicting failure in telecommunication path 
the optical plume anomaly detection opad program at the nasa marshall space flight center is ubing plume spectroscopy for the diagnosis of the space shuttle main engine a challenging part of this program is matching high resolution spectral data with a physicist s model of spectroscopy to produce estimate of metallic erosion through the plume this paper describes the discovery process used in doing this the physicist s model had to be debugged in order to discover the various instrument characteristic discover critical element of the data and in general perform exploratory analysis to understand the instrument and the data it produce this model give u strong prior knowledge however this need to be incorporated with care we had to use a range of statistical technique in our analysis including onedimensional super resolution to determine the instrument response function the paper concludes with a discussion of the role of discovery in building intelligent instrument that turn real time data into useful information 
this paper introduces a new algorithm called siao for learning first order logic rule with genetic algorithm siao us the covering principle developed in aq where seed example are generalized into rule using however a genetic search a initially introduced in the sia algorithm for attribute based representation the genetic algorithm us a high level representation for learning rule in first order logic and may deal with numerical data a well a background knowledge such a hierarchy over the predicate or tree structured value the genetic operator may for instance change a predicate into a more general one according to background knowledge or change a constant into a variable the evaluation function may take into account user preference bias 
