in the last year a lot of attention wa attracted by the problem of page authority computation based on user browsing behavior however the proposed method have a number of limitation in particular they run on a single snapshot of a user browsing graph ignoring substantially dynamic nature of user browsing activity which make such method recency unaware this paper proposes a new method for computing page importance referred to a fresh browserank the score of a page by our algorithm equal to the weight in a stationary distribution of a flexible random walk which is controlled by recency sensitive weight of vertex and edge our method generalizes some previous approach provides better capability for capturing the dynamic of the web and user behavior and overcomes essential limitation of browserank the experimental result demonstrate that our method enables to achieve more relevant and fresh ranking result than the classic browserank 
an essential aspect for building effective crowdsourcing computation is the ability of controlling the crowd i e of dynamically adapting the behaviour of the crowdsourcing system a response to the quantity and quality of completed task or to the availability and reliability of performer most crowdsourcing system only provide limited and predefined control in contrast we present an approach to crowdsourcing which provides fine level powerful and flexible control we model each crowdsourcing application a composition of elementary task type and we progressively transform these high level specification into the feature of a reactive execution environment that support task planning assignment and completion a well a performer monitoring and exclusion control are specified a active rule on top of data structure which are derived from the model of the application rule can be added dropped or modified thus guaranteeing maximal flexibility with limited effort we also report on our prototype platform that implement the proposed framework and we show the result of our experimentation with different rule set demonstrating how simple change to the rule can substantially affect time effort and quality involved in crowdsourcing activity 
we describe a method for automatically generating subjectivity clue for a specific topic and a set of relevant document evaluating it on the task of classifying sentence w r t subjectivity with improvement over previous work 
we present a novel interpretation of clarity a widely used query performance predictor while clarity is commonly described a a measure of the distance between the language model of the top retrieved document and that of the collection we show that it actually quantifies an additional property of the result list namely it diversity this analysis along with empirical evaluation help to explain the low prediction quality of clarity for large scale web collection 
the need for a search engine to deal with ambiguous query ha been known for a long time diversification however it is only recently that this need ha become a focus within information retrieval research how to respond to indication that a result is relevant to a query relevance feedback ha also been a long focus of research when thinking about the result for a query a being clustered by topic these two area of information retrieval research appear to be opposed to each other interestingly though they both appear to improve the performance of search engine raising the question they can be combined or made to work with each other when presented with an ambiguous query there are a number of technique that can be employed to better select result the primary technique being researched now is diversification which aim to populate the result with a set of document that cover different possible interpretation for the query while maintaining a degree of relevance a determined by the search engine for example given a query of java it is unclear whether the user without any other information mean the programming language the coffee the island of indonesia or a multitude of other meaning in order to do this the assumption that document are independent of each other when assessing potential relevance ha to be broken that is a document relevance a calculated by the search engine is no longer dependent only on the query but also the other document that have been selected how a document is identified a being similar to previously selected document and the trade off between estimated relevance and topic coverage are current area for information retrieval research for unambiguous query or for search engine that do not perform diversification it is possible to improve the result selected by reacting to information identifying a given result a truly relevant or not this mechanism is known a relevance feedback the most common response to relevance feedback is to investigate the document for their most content bearing term and either add or subtract their influence to a newly formed query which is then re run on the remaining document to re order them there ha been a scant amount of research into the combination of these method however carbonell et al show that an initially diverse result set can provide a better approach for identifying the topic a user is interested in for a relevance feedback style approach this approach wa further extended by raman et al an important aspect of relevance feedback is the selection of document to use in the trec relevance feedback track meij et al generated a diversified result set which outperformed other ranking a a source of feedback document the use of pseudo relevance feedback assuming the top ranked document are relevant to extract sub topic for use in diversification wa explored by santos et al these previous approach suggest that these two idea are more linked than expected the atire search engine will be used to further explore the relationship between diversification and relevance feedback atire wa selected because it is developed locally and is designed to be small and fast atire also produce a competitive baseline which would have placed th in the trec diversity task while performing no diversification and index time spam filtering although we concede this is not equivalent to submitting a run 
we study a novel problem of social context summarization for web document traditional summarization research ha focused on extracting informative sentence from standard document with the rapid growth of online social network abundant user generated content e g comment associated with the standard document is available which part in a document are social user really caring about how can we generate summary for standard document by considering both the informativeness of sentence and interest of social user this paper explores such an approach by modeling web document and social context into a unified framework we propose a dual wing factor graph dwfg model which utilizes the mutual reinforcement between web document and their associated social context to generate summary an efficient algorithm is designed to learn the proposed factor graph model experimental result on a twitter data set validate the effectiveness of the proposed model by leveraging the social context information our approach obtains significant improvement averagely over several alternative method crf svm lr pr and doclead on the performance of summarization 
we present political search trend a browser based web search analysis tool that i assigns a political leaning to web search query ii detects trending political query in a given week and iii link search query to fact checked statement in term of methodology it showcase the power of analyzing query leading to click on selected annotated web site of interest 
we address the problem of query segmentation given a keyword query the task is to group the keywords into phrase if possible previous approach to the problem achieve reasonable segmentation performance but are tested only against a small corpus of manually segmented query in addition many of the previous approach are fairly intricate a they use expensive feature and are difficult to be reimplemented the main contribution of this paper is a new method for query segmentation that is easy to implement fast and that come with a segmentation accuracy comparable to current state of the art technique our method us only raw web n gram frequency and wikipedia title that are stored in a hash table at the same time we introduce a new evaluation corpus for query segmentation with about human annotated query it is two order of magnitude larger than the corpus being used up to now 
bucket testing also known a a b testing is a practice that is widely used by on line site with large audience in a simple version of the methodology one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring it effect on this exposed group for traditional us of this technique uniform independent sampling of the population is often enough to produce an exposed group that can serve a a statistical proxy for the full population in on line social network application however one often wish to perform a more complex test evaluating a new social feature that will only produce an effect if a user and some number of his or her friend are exposed to it in this case independent uniform draw from the population will be unlikely to produce group that contains user together with their friend and so the construction of the sample must take the network structure into account this lead quickly to challenging combinatorial problem since there is an inherent tension between producing enough correlation to select user and their friend but also enough uniformity and independence that the selected group is a reasonable sample of the full population here we develop an algorithmic framework for bucket testing in a network that address these challenge first we describe a novel walk based sampling method for producing sample of node that are internally well connected but also approximately uniform over the population then we show how a collection of multiple independent subgraphs constructed this way can yield reasonable sample for testing we demonstrate the effectiveness of our algorithm through computational experiment on large portion of the facebook network 
a frequent problem when dealing with data gathered from multiple source on the web ranging from bookseller to wikipedia page to stock analyst prediction is that these source disagree and we must decide which of their often mutually exclusive claim we should accept current state of the art information credibility algorithm known a fact finder are transitive voting system with rule specifying how vote iteratively flow from source to claim and then back to source while this is quite tractable and often effective fact finder also suffer from substantial limitation in particular a lack of transparency obfuscates their credibility decision and make them difficult to adapt and analyze knowing the mechanic of how vote are calculated doe not readily tell u what those vote mean and finding for example that a source ha a score of is not informative we introduce a new approach to information credibility latent credibility analysis lca constructing strongly principled probabilistic model where the truth of each claim is a latent variable and the credibility of a source is captured by a set of model parameter this give lca model clear semantics and modularity that make extending them to capture additional observed and latent credibility factor straightforward experiment over four real world datasets demonstrate that lca model can outperform the best fact finder in both unsupervised and semi supervised setting 
how often do individual perform a given communication activity in the web such a posting comment on blog or news could we have a generative model to create communication event with realistic inter event time distribution ied which property should we strive to match current literature ha seemingly contradictory result for ied some study claim good fit with power law others with non homogeneous poisson process given these two approach we ask which is the correct one can we reconcile them all we show here that surprisingly both approach are correct being corner case of the proposed self feeding process sfp we show that the sfp a exhibit a unifying power which generates power law tail including the so called top concavity that real data exhibit a well a short term poisson behavior b avoids the i i d fallacy which none of the prevailing model have studied before and c is extremely parsimonious requiring usually only one and in general at most two parameter experiment conducted on eight large diverse real datasets e g youtube and blog comment e mail sm etc reveal that the sfp mimic their property very well 
entity alias commonly exist and accurately detecting these alias play a vital role in various application in this paper we use an active learning based method to detect alias without string similarity to minimize the cost on pairwise comparison a subset based method restricts the alias selection within a small scale entity set within each generated entity set an active learning based logistic regression classifier is employed to predict whether a candidate is the alias of a given entity the experimental result on three datasets clearly demonstrate that our proposed approach can effectively detect this kind of entity alias 
news source around the world generate constant stream of information but effective streaming news retrieval requires an intimate understanding of the geographic content of news this process of understanding known a geotagging consists of first finding word in article text that correspond to location name toponym and second assigning each toponym it correct lat long value the latter step called toponym resolution can also be considered a classification problem where each of the possible interpretation for each toponym is classified a correct or incorrect hence technique from supervised machine learning can be applied to improve accuracy new classification feature to improve toponym resolution termed adaptive context feature are introduced that consider a window of context around each toponym and use geographic attribute of toponym in the window to aid in their correct resolution adaptive parameter controlling the window s breadth and depth afford flexibility in managing a tradeoff between feature computation speed and resolution accuracy allowing the feature to potentially apply to a variety of textual domain extensive experiment with three large datasets of streaming news demonstrate the new feature effectiveness over two widely used competing method 
community based question answering cqa site such a yahoo answer baidu know naver and quora have been rapidly growing in popularity the resulting archive of posted answer to question in yahoo answer alone already exceed in size billion and are aggressively indexed by web search engine in fact a large number of search engine user benefit from these archive by finding existing answer that address their own query this scenario pose new challenge and opportunity for both search engine and cqa site to this end we formulate a new problem of predicting the satisfaction of web searcher with cqa answer we analyze a large number of web search that result in a visit to a popular cqa site and identify unique characteristic of searcher satisfaction in this setting namely the effect of query clarity query to question match and answer quality we then propose and evaluate several approach to predicting searcher satisfaction that exploit these characteristic to the best of our knowledge this is the first attempt to predict and validate the usefulness of cqa archive for external searcher rather than for the original asker our result suggest promising direction for improving and exploiting community question answering service in pursuit of satisfying even more web search query 
recent year have witnessed a persistent interest in generating pseudo test collection both for training and evaluation purpose we describe a method for generating query and relevance judgment for microblog search in an unsupervised way our starting point is this intuition tweet with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag our baseline method selects all commonly used hashtags and all associated tweet a relevance judgment we then generate a query from these tweet next we generate a timestamp for each query allowing u to use temporal information in the training process we then enrich the generation process with knowledge derived from an editorial test collection for microblog search we use our pseudo test collection in two way first we tune parameter of a variety of well known retrieval method on them correlation with parameter sweep on an editorial test collection are high on average with a large variance over retrieval algorithm second we use the pseudo test collection a training set in a learning to rank scenario performance close to training on an editorial test collection is achieved in many case our result demonstrate the utility of tuning and training microblog search algorithm on automatically generated training material 
web search performance can be improved by either improving the search engine itself or by educating the user to search more efficiently there is a large amount of literature describing technique for measuring the former whereas improvement resulting from the latter are more difficult to quantify in this paper we demonstrate an experimental methodology that prof to successfully quantify improvement from user education the user education in our study is realized in the form of tactical search feature tip that expand user awareness of task relevant tool and feature of the search application initially these tip are presented in an idealized situation each tip is shown at the same time a the study participant are given a task that is constructed to benefit from the specific tip however we also present a follow up study roughly one week later in which the search tip are no longer presented but the study participant who previously were shown search tip still demonstrate improved search efficiency compared to the control group this research ha implication for search user interface designer and the study of information retrieval system 
query suggestion or auto completion mechanism are widely used by search engine and are increasingly attracting interest from the research community however the lack of commonly accepted evaluation methodology and metric mean that it is not possible to compare result and approach from the literature moreover often the metric used to evaluate query suggestion tend to be an adaptation from other domain without a proper justification hence it is not necessarily clear if the improvement reported in the literature would result in an actual improvement in the user experience inspired by the cascade user model and state of the art evaluation metric in the web search domain we address the query suggestion evaluation by first studying the user behaviour from a search engine s query log and thereby deriving a new family of user model describing the user interaction with a query suggestion mechanism next assuming a query log based evaluation approach we propose two new metric to evaluate query suggestion psaved and esaved both metric are parameterised by a user model psaved is defined a the probability of using the query suggestion while submitting a query esaved equates to the expected relative amount of effort keypresses a user can avoid due to the deployed query suggestion mechanism finally we experiment with both metric using four user model instantiation a well a metric previously used in the literature on a dataset of m session our result demonstrate that psaved and esaved show the best alignment with the user satisfaction amongst the considered metric 
current user interface for online video consumption are mostly browser based lean forward require constant interaction and provide a fragmented view of the total content available for easier consumption the user interface and interaction need to be redesigned for le interruptive and lean back experience in this paper we describe personalized video an application that convert the online video experience into a personalized lean back experience it ha been implemented on the window platform and integrated with intuitive user interaction like gesture and face recognition it also support group personalization for concurrent user 
twitter ha attracted hundred million of user to share and disseminate most up to date information however the noisy and short nature of tweet make many application in information retrieval ir and natural language processing nlp challenging recently segment based tweet representation ha demonstrated effectiveness in named entity recognition ner and event detection from tweet stream to split tweet into meaningful phrase or segment the previous work is purely based on external knowledge base which ignores the rich local context information embedded in the tweet in this paper we propose a novel framework for tweet segmentation in a batch mode called hybridseg hybridseg incorporates local context knowledge with global knowledge base for better tweet segmentation hybridseg consists of two step learning from off the shelf weak ners and learning from pseudo feedback in the first step the existing ner tool are applied to a batch of tweet the named entity recognized by these ners are then employed to guide the tweet segmentation process in the second step hybridseg adjusts the tweet segmentation result iteratively by exploiting all segment in the batch of tweet in a collective manner experiment on two tweet datasets show that hybridseg significantly improves tweet segmentation quality compared with the state of the art algorithm we also conduct a case study by using tweet segment for the task of named entity recognition from tweet the experimental result demonstrate that hybridseg significantly benefit the downstream application 
pseudo relevance feedback is an important strategy to improve search accuracy it is often implemented a a two round retrieval process the first round is to retrieve an initial set of document relevant to an original query and the second round is to retrieve final retrieval result using the original query expanded with term selected from the previously retrieved document this two round retrieval process is clearly time consuming which could arguably be one of main reason that hinder the wide adaptation of the pseudo relevance feedback method in real world ir system in this paper we study how to improve the efficiency of pseudo relevance feedback method the basic idea is to reduce the time needed for the second round of retrieval by leveraging the query processing result of the first round specifically instead of processing the expand query a a newly submitted query we propose an incremental approach which resume the query processing result i e document accumulator for the first round of retrieval and process the second round of retrieval mainly a a step of adjusting the score in the accumulator experimental result on trec terabyte collection show that the proposed incremental approach can improve the efficiency of pseudo relevance feedback method by a factor of two without sacrificing their effectiveness 
due to an explosion in the amount of medical information available search technique are gaining importance in the medical domain this tutorial discus recent result on search in the medical domain including the outcome of survey on end user requirement research relevant to the field and current medical and health search application available finally the extent to which available technique meet user requirement are discussed and open challenge in the field are identified 
latent semantic analysis lsa ha been intensively studied because of it wide application to information retrieval and natural language processing yet traditional model such a lsa only examine one current version of the document however due to the recent proliferation of collaboratively generated content such a thread in online forum collaborative question answering archive wikipedia and other versioned content the document generation process is now directly observable in this study we explore how this additional temporal information about the document evolution could be used to enhance the identification of latent document topic specifically we propose a novel hidden topic modeling algorithm temporal latent semantic analysis tlsa which elegantly extends lsa to modeling document revision history using tensor decomposition our experiment show that tlsa outperforms lsa on word relatedness estimation using benchmark data and explore application of tlsa for other task 
compromised website are often used by attacker to deliver malicious content or to host phishing page designed to steal private information from their victim unfortunately most of the targeted website are managed by user with little security background often unable to detect this kind of threat or to afford an external professional security service in this paper we test the ability of web hosting provider to detect compromised website and react to user complaint we also test six specialized service that provide security monitoring of web page for a small fee during a period of day we hosted our own vulnerable website on shared hosting provider including of the most popular one we repeatedly ran five different attack against each of them our test included a bot like infection a drive by download the upload of malicious file an sql injection stealing credit card number and a phishing kit for a famous american bank in addition we also generated traffic from seemingly valid victim of phishing and drive by download site we show that most of these attack could have been detected by free network or file analysis tool after day if no malicious activity wa detected we started to file abuse complaint to the provider this allowed u to study the reaction of the web hosting provider to both real and bogus complaint the general picture we drew from our study is quite alarming the vast majority of the provider or add on security monitoring service are unable to detect the most simple sign of malicious activity on hosted website 
micro blogging system such a twitter expose digital trace of social discourse with an unprecedented degree of resolution of individual behavior they offer an opportunity to investigate how a large scale social system responds to exogenous or endogenous stimulus and to disentangle the temporal spatial and topical aspect of user activity here we focus on spike of collective attention in twitter and specifically on peak in the popularity of hashtags user employ hashtags a a form of social annotation to define a shared context for a specific event topic or meme we analyze a large scale record of twitter activity and find that the evolution of hashtag popularity over time defines discrete class of hashtags we link these dynamical class to the event the hashtags represent and use text mining technique to provide a semantic characterization of the hashtag class moreover we track the propagation of hashtags in the twitter social network and find that epidemic spreading play a minor role in hashtag popularity which is mostly driven by exogenous factor 
prior research in resource selection for federated search mainly focused on selecting a small number of information source that are most relevant to a user query however result novelty and diversification are largely unexplored which doe not reflect the various kind of information need of user in real world application this paper proposes two general approach to model both result relevance and diversification in selecting source in order to provide more comprehensive coverage of multiple aspect of a user query the first approach focus on diversifying the document ranking on a centralized sample database before selecting information source under the framework of relevant document distribution estimation redde the second approach first evaluates the relevance of information source with respect to each aspect of the query and then rank the source based on the novelty and relevance that they offer both approach can be applied with a wide range of existing resource selection algorithm such a redde crcs cori and big document moreover this paper proposes a learning based approach to combine multiple resource selection algorithm for result diversification which can further improve the performance we propose a set of new metric for resource selection in federated search to evaluate the diversification performance of different approach to our best knowledge this is the first piece of work that address the problem of search result diversification in federated search the effectiveness of the proposed approach ha been demonstrated by an extensive set of experiment on the federated search testbed of the clueweb dataset 
in this talk we present a perspective across multiple industry problem including safety and security medical web social and mobile medium and motivate the need for large scale analysis and retrieval of multimedia data we describe a multi layer architecture that incorporates capability for audio visual feature extraction machine learning and semantic modeling and provides a powerful framework for learning and classifying content of multimedia data we discus the role semantic ontology for representing audio visual concept and relationship which are essential for training semantic classifier we discus the importance of using faceted classification scheme in particular for organizing multimedia semantic concept in order to achieve effective learning and retrieval we also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platform to address both massive scale analysis and low latency processing we describe multiple effort at ibm on image and video analysis and retrieval including ibm multimedia analysis and retrieval system imars and show recent result for semantic based classification and retrieval we conclude with future direction for improving analysis of multimedia through interactive and curriculum based technique for multimedia semantics based learning and retrieval 
we evaluate statistical inference procedure for small scale ir experiment that involve multiple comparison against the baseline these procedure adjust for multiple comparison by ensuring that the probability of observing at least one false positive in the experiment is below a given threshold we use only publicly available test collection and make our software available for download in particular we employ the trec run and run constructed from the microsoft learning to rank mslr data set our focus is on non parametric statistical procedure that include the holm bonferroni adjustment of the permutation test p value the maxt permutation test and the permutation based closed testing in trec based simulation these procedure retain from to of individually significant result i e those obtained without taking other comparison into account similar retention rate are observed in the mslr simulation for the largest evaluated query set size i e procedure that adjust for multiplicity find at most fewer true difference compared to unadjusted test at the same time unadjusted test produce many more false positive 
we study information goal and pattern of attention in explorato ry search for health information on the web reporting result of a large scale log based study we examine search activity associated with the goal of diagnosing illness from symptom versus more general information seeking about health and illness we decom pose exploratory health search into evidence based and hypothe si directed information seeking evidence based search center on the pursuit of detail and relevance of sign and symptom hypothesis directed search includes the pursuit of content on one or more illness including risk factor treatment and therapy for illness and on the discrimination among different disease under the uncertainty that exists in advance of a confirmed diag nosis these different goal of exploratory health search are not independent and transition can occur between them within or across search session we construct a classifier that identifies medically related search session in log data given a set of search session flagged a health related we show how we can identify different intention persisting a focus of attention within those session finally we discus how insight about focus dynamic can help u better understand exploratory health search behavior and better support health search on the web 
exploratory search in which a user investigates complex concept is cumbersome with today s search engine we present a new exploratory search approach that generates interactive visualization of query concept using thematic cartography e g choropleth map heat map we show how the approach can be applied broadly across both geographic and non geographic context through explicit spatialization a novel method that leverage any figure or diagram from a periodic table to a parliamentary seating chart to a world map a a spatial search environment we enable this capability by introducing explanatory semantic relatedness measure these measure extend frequently used semantic relatedness measure to not only estimate the degree of relatedness between two concept but also generate human readable explanation for their estimate by mining wikipedia s text hyperlink and category structure we implement our approach in a system called atlasify evaluate it key component and present several use case 
most of the online advertising today is sold via an auction which requires the advertiser to respond with a valid bid within a fraction of a second a such most advertiser employ bidding agent to submit bid on their behalf the architecture of such agent typically ha an offline optimization phase which incorporates the bidder s knowledge about the market and an online bidding strategy which simply executes the offline strategy the online strategy is typically highly dependent on both supply and expected price distribution both of which are forecast using traditional machine learning method in this work we investigate the optimum strategy of the bidding agent when faced with incorrect forecast at a high level the agent can invest resource in improving the forecast or can tighten the loop between successive offline optimization cycle in order to detect error more quickly we show analytically that the latter strategy while simple is extremely effective in dealing with forecast error and confirm this finding with experimental evaluation 
method that reduce the amount of labeled data needed for training have focused more on selecting which document to label than on which query should be labeled one exception to this long et al us expected loss optimization elo to estimate which query should be selected but is limited to ranker that predict absolute graded relevance in this work we demonstrate how to easily adapt elo to work with any ranker and show that estimating expected loss in dcg is more robust than ndcg even when the final performance measure is ndcg 
query suggestion is a useful tool to help user formulate better query although this ha been found highly useful globally it effect on different query may vary in this paper we examine the impact of query suggestion on query of different degree of difficulty it turn out that query suggestion is much more useful for difficult query than easy query in addition the suggestion for difficult query should rely le on their similarity to the original query in this paper we use a learning to rank approach to select query suggestion based on several type of feature including a query performance prediction a query suggestion ha different impact on different query we propose an adaptive suggestion approach that make suggestion only for difficult query we carry out experiment on real data from a search engine our result clearly indicate that an approach targeting difficult query can bring higher gain than a uniform suggestion approach 
in recent year social networking site have not only enabled people to connect with each other using social link but have also allowed them to share communicate and interact over diverse geographical region social network provide a rich source of heterogeneous data which can be exploited to discover previously unknown relationship and interest among group of people in this paper we address the problem of discovering topically meaningful community from a social network we assume that a person membership in a community is conditioned on it social relationship the type of interaction and the information communicated with other member of that community we propose generative model that can discover community based on the discussed topic interaction type and the social connection among people in our model a person can belong to multiple community and a community can participate in multiple topic this allows u to discover both community interest and user interest based on the information and linked association we demonstrate the effectiveness of our model on two real word data set and show that it performs better than existing community discovery model 
the betweenness centrality of a vertex in a graph is a measure for the participation of the vertex in the shortest path in the graph the betweenness centrality is widely used in network analysis especially in a social network the recursive computation of the betweenness centrality of vertex is performed for the community detection and finding the influential user in the network since a social network graph is frequently updated it is necessary to update the betweenness centrality efficiently when a graph is changed the betweenness centrality of all the vertex should be recomputed from scratch using all the vertex in the graph to the best of our knowledge this is the first work that proposes an efficient algorithm which handle the update of the betweenness centrality of vertex in a graph in this paper we propose a method that efficiently reduces the search space by finding a candidate set of vertex whose betweenness centrality can be updated and computes their betweenness centeralities using candidate vertex only a the cost of calculating the betweenness centrality mainly depends on the number of vertex to be considered the proposed algorithm significantly reduces the cost of calculation the proposed algorithm allows the transformation of an existing algorithm which doe not consider the graph update experimental result on large real datasets show that the proposed algorithm speed up the existing algorithm to time depending on the dataset 
in this work we study the cluster hypothesis for entity oriented search eos specifically we show that the hypothesis can hold to a substantial extent for several entity similarity measure we also demonstrate the retrieval effectiveness merit of using cluster of similar entity for eos 
short url have become ubiquitous especially popular within social networking service short url have seen a significant increase in their usage over the past year mostly due to twitter s restriction of message length to character in this paper we provide a first characterization on the usage of short url specifically our goal is to examine the content short url point to how they are published their popularity and activity over time a well a their potential impact on the performance of the web our study is based on trace of short url a seen from two different perspective i collected through a large scale crawl of url shortening service and ii collected by crawling twitter message the former provides a general characterization on the usage of short url while the latter provides a more focused view on how certain community use shortening service our analysis highlight that domain and website popularity a seen from short url significantly differs from the distribution provided by well publicised service such a alexa the set of most popular website pointed to by short url appears stable over time despite the fact that short url have a limited high popularity lifetime surprisingly short url are not ephemeral a a significant fraction roughly appears active for more than three month overall our study emphasizes the fact that short url reflect an alternative web and hence provide an additional view on web usage and content consumption complementing traditional measurement source furthermore our study reveals the need for alternative shortening architecture that will eliminate the non negligible performance penalty imposed by today s shortening service 
entity relationship search at web scale depends on adding dozen of entity annotation to each of billion of crawled page and indexing the annotation at rate comparable to regular text indexing even small entity search benchmark from trec and inex suggest that the entity catalog support thousand of entity type and ten to hundred of million of entity the above target raise many challenge major one being the design of highly compressed data structure in ram for spotting and disambiguating entity mention and highly compressed disk based annotation index these data structure cannot be readily built upon standard inverted index here we present a web scale entity annotator and annotation index using a new workload sensitive compressed multilevel map we fit statistical disambiguation model for million of entity within gb of ram and spend about core millisecond per disambiguation in contrast dbpedia spotlight spends millisecond wikipedia miner spends millisecond and zemanta spends millisecond our annotation index use idea from vertical database to reduce storage by on x core with x disk spindle we can annotate and index in about a day a billion web page with two million entity and type from wikipedia index decompression and scan speed are comparable to mg j 
it is well known that anchor text play an important role in search providing signal that are often not present in the source document itself the paper report result of a preliminary investigation on the value of tweet and tweet conversation a anchor text we show that using tweet a anchor improves significantly over using html anchor and significantly increase recall of news item retrieval 
computing the degree of semantic relatedness of word is a key functionality of many language application such a search clustering and disambiguation previous approach to computing semantic relatedness mostly used static language resource while essentially ignoring their temporal aspect we believe that a considerable amount of relatedness information can also be found in studying pattern of word usage over time consider for instance a newspaper archive spanning many year two word such a war and peace might rarely co occur in the same article yet their pattern of use over time might be similar in this paper we propose a new semantic relatedness model temporal semantic analysis tsa which capture this temporal information the previous state of the art method explicit semantic analysis esa represented word semantics a a vector of concept tsa us a more refined representation where each concept is no longer scalar but is instead represented a time series over a corpus of temporally ordered document to the best of our knowledge this is the first attempt to incorporate temporal evidence into model of semantic relatedness empirical evaluation show that tsa provides consistent improvement over the state of the art esa result on multiple benchmark 
we demonstrate the yali browser plug in which discovers named entity in web page and provides background knowledge about them the plug in is implemented with two purpose from a user perspective it enriches the browsing experience with entity helping user with their information need from the research perspective we aim to improve the method that are used for named entity recognition and disambiguation nerd by leveraging the plug in a an implicit crowdsourcing platform yali track the system s error and the user correction and also gather implicit training data for improving nerd accuracy 
query expansion is a classical topic in the field of information retrieval which is proposed to bridge the gap between searcher information intent and their query previous research usually expand query based on document collection or some external resource such a wordnet and wikipedia however it seems that independently using one of these resource ha some defect document collection lack semantic information of word while wordnet and wikipedia may not include domain specific knowledge in certain document collection our work aim to combine these two kind of resource to establish an expansion model which represents not only domain specific information but also semantic information in our preliminary experiment we construct a two layer word graph and use random walk algorithm to calculate the weight of each term in pseudo relevance feedback document then select the highest weighted term to expand original query the first layer of the word graph contains term in related document while the second layer contains semantic sens corresponding to these term these term and semantic sens are treated a vertex of the graph and connected with each other by all possible relationship such a mutual information and semantic similarity we utilized mutual information semantic similarity and uniform distribution a the weight of term term relation sense sense relation and word sense relation respectively though these experiment show that our expansion outperform original query we are troubled with some difficult problem given the framework of semantic graph model we need more effort to find out an optimal graph to represent the relationship between term and their semantic sens we utilized a two layer graph model in our preliminary research where term from different document are treated equally maybe we can introduce the document a a third layer in future work where we can differ the same term in different document according to document relevance and context then we need appropriately represent initial weight of this word sens and relationship various measure for weight of term and term relation have been proved effective in other information retrieval task such a tfidf mutual information mi but there is little research on weight for semantic sens and their relation for polysemous word we add all of their semantic sens to the graph and assume that these sens are uniformly distributed actually it is not precise for a word in a special document and query a we know a polysemous word may have only one or two sens in a document and they are not uniformly distributed give a word what we should do is to determine it word sens in a relevant document and estimate the distribution of these sens word sense disambiguation may help u in this problem then there are many method to compute word similarity according to wordnet which we use to represent the weight of relationship between word sens varelas et al implemented some popular method to compute semantic similarity by mapping term to an ontology and examining their relationship in that ontology we also need to know which algorithm for semantic similarity is most suitable for our model additional wordnet is suitable to calculate word similarity but not suitable to measure word relevance the inner hyperlink of wikipedia could help u to calculate word relevance we wish to find an effective way to combine the similarity measure from wordnet and relevance measure from wikipedia which may completely reflect word relationship 
acronym are abbreviation formed from the initial component of word or phrase acronym usage is becoming more common in web search email text message tweet blog and post acronym are typically ambiguous and often disambiguated by context word given either just an acronym a a query or an acronym with a few context word it is immensely useful for a search engine to know the most likely intended meaning ranked by their likelihood to support such online scenario we study the offline mining of acronym and their meaning in this paper for each acronym our goal is to discover all distinct meaning and for each meaning compute the expanded string it popularity score and a set of context word that indicate this meaning existing approach are inadequate for this purpose our main insight is to leverage co click in search engine query click log to mine expansion of acronym there are several technical challenge such a ensuring mapping between expansion and meaning handling of tail meaning and extracting context word we present a novel end to end solution that address the above challenge we further describe how web search engine can leverage the mined information for prediction of intended meaning for query containing acronym our experiment show that our approach i discovers the meaning of acronym with high precision and recall ii significantly complement existing meaning in wikipedia and iii accurately predicts intended meaning for online query with over precision 
result merging is an important research problem in federated search for merging document retrieved from multiple ranked list of selected information source into a single list the state of the art result merging algorithm such a semi supervised learning ssl and sample agglomerate fitting estimate safe try to map document score retrieved from different source to comparable score according to a single centralized retrieval algorithm for ranking those document both ssl and safe arbitrarily select a single centralized retrieval algorithm for generating comparable document score which is problematic in a heterogeneous federated search environment since a single centralized algorithm is often suboptimal for different information source based on this observation this paper proposes a novel approach for result merging by utilizing multiple centralized retrieval algorithm one simple approach is to learn a set of combination weight for multiple centralized retrieval algorithm e g logistic regression to compute comparable document score the paper show that this simple approach generates suboptimal result a it is not flexible enough to deal with heterogeneous information source a mixture probabilistic model is thus proposed to learn more appropriate combination weight with respect to different type of information source with some training data an extensive set of experiment on three datasets have proven the effectiveness of the proposed new approach 
a knowledge base kb store organizes and share information pertinent to entity i e kb node such a people organization and event a large kb system such a wikipedia relies on human curator to create and maintain the content in the system however it ha become challenging for human curator to sift through the rapidly growing amount of information and filter out the information irrelevant to a kb node the area of knowledge base enhancement kbe aim to explore and identify automatic method to assist human curator to accelerate the process kbe can be viewed a a special case of information filtering if however the lack of high quality labelled data introduces a major challenge to train a satisfying model for the task transfer learning provides solution to the problem and ha explored application in the area of text mining whereas direct application to kbe or if remains absent transfer learning is a research area in machine learning emphasizing the reuse of previously acquired knowledge to another applicable task the method is particularly useful in the situation where labeled instance are absent or difficult to obtain to accelerate the growth of a kb a transfer learning approach enables leveraging the heuristic and model learned from one kb node to another for example reusing the learned filtering model from willie nelson a famous country singer to eddie rabbitt another country singer transfer learning requires three component the target task e g the problem to be solved the source task s e g auxiliary data previously studied problem and criterion to select appropriate source task the objective of my dissertation are twofold first it explores method to identify informative source node from which to transfer second it construct a knowledge transfer network to represent the transfer learning relationship between kb node this proposed research applies a transfer learning method segmented transfer st and a knowledge representation knowledge transfer network ktn to approach the area of kbe the primary research question include what are the transferable object in information filtering algorithm what are the kb node of high transferability what are the factor that determine the transfer learning relationship doe it manifest on a knowledge transfer network representation this interdisciplinary research cross the study area of information filtering machine learning knowledge representation and network analysis this proposal motivates the problem of kbe discus the research methodology and proposed experiment and review related work in information filtering and transfer learning this line of research hope to extend the application of transfer learning to kbe and to explore a new dimension of if the proposed st and ktn intends to bring interdisciplinary approach in the emerging field of kbe 
classic news summarization play an important role with the exponential document growth on the web many approach are proposed to generate summary but seldom simultaneously consider evolutionary characteristic of news plus to traditional summary element therefore we present a novel framework for the web mining problem named evolutionary timeline summarization ets given the massive collection of time stamped web document related to a general news query ets aim to return the evolution trajectory along the timeline consisting of individual but correlated summary of each date emphasizing relevance coverage coherence and cross date diversity ets greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity we formally formulate the task a an optimization problem via iterative substitution from a set of sentence to a subset of sentence that satisfies the above requirement balancing coherence diversity measurement and local global summary quality the optimized substitution is iteratively conducted by incorporating several constraint until convergence we develop experimental system to evaluate on instinctively different datasets which amount to document performance comparison between different system generated timeline and manually created one by human editor demonstrate the effectiveness of our proposed framework in term of rouge metric 
we introduce the problem of domain adaptation for content based retrieval and propose a domain adaptation method based on relative aggregation point rap content based retrieval including image retrieval and spoken document retrieval enables a user to input example a a query and retrieves relevant data based on the similarity to the example however input example and relevant data can be dissimilar especially when domain from which the user selects example and from which the system retrieves data are different in content based geographic object retrieval for example suppose that a user who life in beijing visit kyoto japan and want to search for relatively inexpensive restaurant serving popular local dish by mean of a content based retrieval system since such restaurant in beijing and kyoto are dissimilar due to the difference in the average cost and area popular dish it is difficult to find relevant restaurant in kyoto based on example selected in beijing we propose a solution for this problem by assuming that rap in different domain correspond which may be dissimilar but play the same role a rap is defined a the expectation of instance in a domain that are classified into a certain class e g the most expensive restaurant average restaurant and restaurant serving the most popular dish our proposed method construct a new feature space based on rap estimated in each domain and bridge the domain difference for improving content based retrieval in heterogeneous domain to verify the effectiveness of our proposed method we evaluated various method with a test collection developed for content based geographic object retrieval experimental result show that our proposed method achieved significant improvement over baseline method moreover we observed that the search performance of content based retrieval in heterogeneous domain wa significantly lower than that in homogeneous domain this finding suggests that relevant data for the same search intent depend on the search context that is the location where the user search and the domain from which the system retrieves data 
aggregated search is the task of blending result from specialized search service or vertical into the web search result while many study have focused on aggregated search technique few study have tried to better understand how user interact with aggregated search result this study investigates how task complexity and vertical display the blending of vertical result into the web result affect the use of vertical content twenty nine subject completed six search task of varying level of task complexity using two aggregated search interface one that blended vertical result into the web result and one that only provided indirect vertical access our result show that more complex task required significantly more interaction and that subject completing these task examined more vertical result while the amount of interaction wa the same between interface subject clicked on more vertical result when these were blended into the web result our result also show an interaction between task complexity and vertical display subject clicked on more vertical when completing the more complex task with the interface that blended vertical result subject evaluation of the two interface were nearly identical but when analyzed with respect to their interface preference we found a positive relationship between system evaluation and individual preference subject justified their preference using similar rationale and their comment illustrate how the display itself can influence judgment of information quality especially in case when the vertical result might not be relevant to the search task 
online forum discussion are emerging a valuable information repository where knowledge is accumulated by the interaction among user leading to multiple thread with structure such replying structure in each thread conveys important information about the discussion content unfortunately not all the online forum site would explicitly record such replying relationship making it hard to for both user and computer to digest the information buried in a thread discussion in this paper we propose a probabilistic model in the conditional random field framework to predict the replying structure for a threaded online discussion different from previous thread reconstruction method most of which fail to consider dependency between the post we cast the problem a a supervised structure learning problem to incorporate the feature describing the structural dependency among the discussion content and learn their relationship experiment result on three different online forum show that the proposed method can well capture the replying structure in online discussion thread and multiple task such a forum search and question answering can benefit from the reconstructed replying structure 
in this paper we propose a joint probabilistic topic model for simultaneously modeling the content of multi typed object of a heterogeneous information network the intuition behind our model is that different object of the heterogeneous network share a common set of latent topic so a to adjust the multinomial distribution over topic for different object collectively experimental result demonstrate the effectiveness of our approach for the task of topic modeling and object clustering 
in the last year a lot of attention wa attracted by the problem of page authority computation based on user browsing behavior however the proposed method have a number of limitation in particular they run on a single snapshot of a user browsing graph ignoring substantially dynamic nature of user browsing activity which make such method recency unaware this paper proposes a new method for computing page importance referred to a fresh browserank the score of a page by our algorithm equal to the weight in a stationary distribution of a flexible random walk which is controlled by recency sensitive weight of vertex and edge our method generalizes some previous approach provides better capability for capturing the dynamic of the web and user behavior and overcomes essential limitation of browserank the experimental result demonstrate that our method enables to achieve more relevant and fresh ranking result than the classic browserank 
an essential aspect for building effective crowdsourcing computation is the ability of controlling the crowd i e of dynamically adapting the behaviour of the crowdsourcing system a response to the quantity and quality of completed task or to the availability and reliability of performer most crowdsourcing system only provide limited and predefined control in contrast we present an approach to crowdsourcing which provides fine level powerful and flexible control we model each crowdsourcing application a composition of elementary task type and we progressively transform these high level specification into the feature of a reactive execution environment that support task planning assignment and completion a well a performer monitoring and exclusion control are specified a active rule on top of data structure which are derived from the model of the application rule can be added dropped or modified thus guaranteeing maximal flexibility with limited effort we also report on our prototype platform that implement the proposed framework and we show the result of our experimentation with different rule set demonstrating how simple change to the rule can substantially affect time effort and quality involved in crowdsourcing activity 
we describe a method for automatically generating subjectivity clue for a specific topic and a set of relevant document evaluating it on the task of classifying sentence w r t subjectivity with improvement over previous work 
we present a novel interpretation of clarity a widely used query performance predictor while clarity is commonly described a a measure of the distance between the language model of the top retrieved document and that of the collection we show that it actually quantifies an additional property of the result list namely it diversity this analysis along with empirical evaluation help to explain the low prediction quality of clarity for large scale web collection 
the need for a search engine to deal with ambiguous query ha been known for a long time diversification however it is only recently that this need ha become a focus within information retrieval research how to respond to indication that a result is relevant to a query relevance feedback ha also been a long focus of research when thinking about the result for a query a being clustered by topic these two area of information retrieval research appear to be opposed to each other interestingly though they both appear to improve the performance of search engine raising the question they can be combined or made to work with each other when presented with an ambiguous query there are a number of technique that can be employed to better select result the primary technique being researched now is diversification which aim to populate the result with a set of document that cover different possible interpretation for the query while maintaining a degree of relevance a determined by the search engine for example given a query of java it is unclear whether the user without any other information mean the programming language the coffee the island of indonesia or a multitude of other meaning in order to do this the assumption that document are independent of each other when assessing potential relevance ha to be broken that is a document relevance a calculated by the search engine is no longer dependent only on the query but also the other document that have been selected how a document is identified a being similar to previously selected document and the trade off between estimated relevance and topic coverage are current area for information retrieval research for unambiguous query or for search engine that do not perform diversification it is possible to improve the result selected by reacting to information identifying a given result a truly relevant or not this mechanism is known a relevance feedback the most common response to relevance feedback is to investigate the document for their most content bearing term and either add or subtract their influence to a newly formed query which is then re run on the remaining document to re order them there ha been a scant amount of research into the combination of these method however carbonell et al show that an initially diverse result set can provide a better approach for identifying the topic a user is interested in for a relevance feedback style approach this approach wa further extended by raman et al an important aspect of relevance feedback is the selection of document to use in the trec relevance feedback track meij et al generated a diversified result set which outperformed other ranking a a source of feedback document the use of pseudo relevance feedback assuming the top ranked document are relevant to extract sub topic for use in diversification wa explored by santos et al these previous approach suggest that these two idea are more linked than expected the atire search engine will be used to further explore the relationship between diversification and relevance feedback atire wa selected because it is developed locally and is designed to be small and fast atire also produce a competitive baseline which would have placed th in the trec diversity task while performing no diversification and index time spam filtering although we concede this is not equivalent to submitting a run 
we study a novel problem of social context summarization for web document traditional summarization research ha focused on extracting informative sentence from standard document with the rapid growth of online social network abundant user generated content e g comment associated with the standard document is available which part in a document are social user really caring about how can we generate summary for standard document by considering both the informativeness of sentence and interest of social user this paper explores such an approach by modeling web document and social context into a unified framework we propose a dual wing factor graph dwfg model which utilizes the mutual reinforcement between web document and their associated social context to generate summary an efficient algorithm is designed to learn the proposed factor graph model experimental result on a twitter data set validate the effectiveness of the proposed model by leveraging the social context information our approach obtains significant improvement averagely over several alternative method crf svm lr pr and doclead on the performance of summarization 
we present political search trend a browser based web search analysis tool that i assigns a political leaning to web search query ii detects trending political query in a given week and iii link search query to fact checked statement in term of methodology it showcase the power of analyzing query leading to click on selected annotated web site of interest 
we address the problem of query segmentation given a keyword query the task is to group the keywords into phrase if possible previous approach to the problem achieve reasonable segmentation performance but are tested only against a small corpus of manually segmented query in addition many of the previous approach are fairly intricate a they use expensive feature and are difficult to be reimplemented the main contribution of this paper is a new method for query segmentation that is easy to implement fast and that come with a segmentation accuracy comparable to current state of the art technique our method us only raw web n gram frequency and wikipedia title that are stored in a hash table at the same time we introduce a new evaluation corpus for query segmentation with about human annotated query it is two order of magnitude larger than the corpus being used up to now 
bucket testing also known a a b testing is a practice that is widely used by on line site with large audience in a simple version of the methodology one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring it effect on this exposed group for traditional us of this technique uniform independent sampling of the population is often enough to produce an exposed group that can serve a a statistical proxy for the full population in on line social network application however one often wish to perform a more complex test evaluating a new social feature that will only produce an effect if a user and some number of his or her friend are exposed to it in this case independent uniform draw from the population will be unlikely to produce group that contains user together with their friend and so the construction of the sample must take the network structure into account this lead quickly to challenging combinatorial problem since there is an inherent tension between producing enough correlation to select user and their friend but also enough uniformity and independence that the selected group is a reasonable sample of the full population here we develop an algorithmic framework for bucket testing in a network that address these challenge first we describe a novel walk based sampling method for producing sample of node that are internally well connected but also approximately uniform over the population then we show how a collection of multiple independent subgraphs constructed this way can yield reasonable sample for testing we demonstrate the effectiveness of our algorithm through computational experiment on large portion of the facebook network 
a frequent problem when dealing with data gathered from multiple source on the web ranging from bookseller to wikipedia page to stock analyst prediction is that these source disagree and we must decide which of their often mutually exclusive claim we should accept current state of the art information credibility algorithm known a fact finder are transitive voting system with rule specifying how vote iteratively flow from source to claim and then back to source while this is quite tractable and often effective fact finder also suffer from substantial limitation in particular a lack of transparency obfuscates their credibility decision and make them difficult to adapt and analyze knowing the mechanic of how vote are calculated doe not readily tell u what those vote mean and finding for example that a source ha a score of is not informative we introduce a new approach to information credibility latent credibility analysis lca constructing strongly principled probabilistic model where the truth of each claim is a latent variable and the credibility of a source is captured by a set of model parameter this give lca model clear semantics and modularity that make extending them to capture additional observed and latent credibility factor straightforward experiment over four real world datasets demonstrate that lca model can outperform the best fact finder in both unsupervised and semi supervised setting 
how often do individual perform a given communication activity in the web such a posting comment on blog or news could we have a generative model to create communication event with realistic inter event time distribution ied which property should we strive to match current literature ha seemingly contradictory result for ied some study claim good fit with power law others with non homogeneous poisson process given these two approach we ask which is the correct one can we reconcile them all we show here that surprisingly both approach are correct being corner case of the proposed self feeding process sfp we show that the sfp a exhibit a unifying power which generates power law tail including the so called top concavity that real data exhibit a well a short term poisson behavior b avoids the i i d fallacy which none of the prevailing model have studied before and c is extremely parsimonious requiring usually only one and in general at most two parameter experiment conducted on eight large diverse real datasets e g youtube and blog comment e mail sm etc reveal that the sfp mimic their property very well 
entity alias commonly exist and accurately detecting these alias play a vital role in various application in this paper we use an active learning based method to detect alias without string similarity to minimize the cost on pairwise comparison a subset based method restricts the alias selection within a small scale entity set within each generated entity set an active learning based logistic regression classifier is employed to predict whether a candidate is the alias of a given entity the experimental result on three datasets clearly demonstrate that our proposed approach can effectively detect this kind of entity alias 
news source around the world generate constant stream of information but effective streaming news retrieval requires an intimate understanding of the geographic content of news this process of understanding known a geotagging consists of first finding word in article text that correspond to location name toponym and second assigning each toponym it correct lat long value the latter step called toponym resolution can also be considered a classification problem where each of the possible interpretation for each toponym is classified a correct or incorrect hence technique from supervised machine learning can be applied to improve accuracy new classification feature to improve toponym resolution termed adaptive context feature are introduced that consider a window of context around each toponym and use geographic attribute of toponym in the window to aid in their correct resolution adaptive parameter controlling the window s breadth and depth afford flexibility in managing a tradeoff between feature computation speed and resolution accuracy allowing the feature to potentially apply to a variety of textual domain extensive experiment with three large datasets of streaming news demonstrate the new feature effectiveness over two widely used competing method 
community based question answering cqa site such a yahoo answer baidu know naver and quora have been rapidly growing in popularity the resulting archive of posted answer to question in yahoo answer alone already exceed in size billion and are aggressively indexed by web search engine in fact a large number of search engine user benefit from these archive by finding existing answer that address their own query this scenario pose new challenge and opportunity for both search engine and cqa site to this end we formulate a new problem of predicting the satisfaction of web searcher with cqa answer we analyze a large number of web search that result in a visit to a popular cqa site and identify unique characteristic of searcher satisfaction in this setting namely the effect of query clarity query to question match and answer quality we then propose and evaluate several approach to predicting searcher satisfaction that exploit these characteristic to the best of our knowledge this is the first attempt to predict and validate the usefulness of cqa archive for external searcher rather than for the original asker our result suggest promising direction for improving and exploiting community question answering service in pursuit of satisfying even more web search query 
recent year have witnessed a persistent interest in generating pseudo test collection both for training and evaluation purpose we describe a method for generating query and relevance judgment for microblog search in an unsupervised way our starting point is this intuition tweet with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag our baseline method selects all commonly used hashtags and all associated tweet a relevance judgment we then generate a query from these tweet next we generate a timestamp for each query allowing u to use temporal information in the training process we then enrich the generation process with knowledge derived from an editorial test collection for microblog search we use our pseudo test collection in two way first we tune parameter of a variety of well known retrieval method on them correlation with parameter sweep on an editorial test collection are high on average with a large variance over retrieval algorithm second we use the pseudo test collection a training set in a learning to rank scenario performance close to training on an editorial test collection is achieved in many case our result demonstrate the utility of tuning and training microblog search algorithm on automatically generated training material 
web search performance can be improved by either improving the search engine itself or by educating the user to search more efficiently there is a large amount of literature describing technique for measuring the former whereas improvement resulting from the latter are more difficult to quantify in this paper we demonstrate an experimental methodology that prof to successfully quantify improvement from user education the user education in our study is realized in the form of tactical search feature tip that expand user awareness of task relevant tool and feature of the search application initially these tip are presented in an idealized situation each tip is shown at the same time a the study participant are given a task that is constructed to benefit from the specific tip however we also present a follow up study roughly one week later in which the search tip are no longer presented but the study participant who previously were shown search tip still demonstrate improved search efficiency compared to the control group this research ha implication for search user interface designer and the study of information retrieval system 
query suggestion or auto completion mechanism are widely used by search engine and are increasingly attracting interest from the research community however the lack of commonly accepted evaluation methodology and metric mean that it is not possible to compare result and approach from the literature moreover often the metric used to evaluate query suggestion tend to be an adaptation from other domain without a proper justification hence it is not necessarily clear if the improvement reported in the literature would result in an actual improvement in the user experience inspired by the cascade user model and state of the art evaluation metric in the web search domain we address the query suggestion evaluation by first studying the user behaviour from a search engine s query log and thereby deriving a new family of user model describing the user interaction with a query suggestion mechanism next assuming a query log based evaluation approach we propose two new metric to evaluate query suggestion psaved and esaved both metric are parameterised by a user model psaved is defined a the probability of using the query suggestion while submitting a query esaved equates to the expected relative amount of effort keypresses a user can avoid due to the deployed query suggestion mechanism finally we experiment with both metric using four user model instantiation a well a metric previously used in the literature on a dataset of m session our result demonstrate that psaved and esaved show the best alignment with the user satisfaction amongst the considered metric 
current user interface for online video consumption are mostly browser based lean forward require constant interaction and provide a fragmented view of the total content available for easier consumption the user interface and interaction need to be redesigned for le interruptive and lean back experience in this paper we describe personalized video an application that convert the online video experience into a personalized lean back experience it ha been implemented on the window platform and integrated with intuitive user interaction like gesture and face recognition it also support group personalization for concurrent user 
twitter ha attracted hundred million of user to share and disseminate most up to date information however the noisy and short nature of tweet make many application in information retrieval ir and natural language processing nlp challenging recently segment based tweet representation ha demonstrated effectiveness in named entity recognition ner and event detection from tweet stream to split tweet into meaningful phrase or segment the previous work is purely based on external knowledge base which ignores the rich local context information embedded in the tweet in this paper we propose a novel framework for tweet segmentation in a batch mode called hybridseg hybridseg incorporates local context knowledge with global knowledge base for better tweet segmentation hybridseg consists of two step learning from off the shelf weak ners and learning from pseudo feedback in the first step the existing ner tool are applied to a batch of tweet the named entity recognized by these ners are then employed to guide the tweet segmentation process in the second step hybridseg adjusts the tweet segmentation result iteratively by exploiting all segment in the batch of tweet in a collective manner experiment on two tweet datasets show that hybridseg significantly improves tweet segmentation quality compared with the state of the art algorithm we also conduct a case study by using tweet segment for the task of named entity recognition from tweet the experimental result demonstrate that hybridseg significantly benefit the downstream application 
pseudo relevance feedback is an important strategy to improve search accuracy it is often implemented a a two round retrieval process the first round is to retrieve an initial set of document relevant to an original query and the second round is to retrieve final retrieval result using the original query expanded with term selected from the previously retrieved document this two round retrieval process is clearly time consuming which could arguably be one of main reason that hinder the wide adaptation of the pseudo relevance feedback method in real world ir system in this paper we study how to improve the efficiency of pseudo relevance feedback method the basic idea is to reduce the time needed for the second round of retrieval by leveraging the query processing result of the first round specifically instead of processing the expand query a a newly submitted query we propose an incremental approach which resume the query processing result i e document accumulator for the first round of retrieval and process the second round of retrieval mainly a a step of adjusting the score in the accumulator experimental result on trec terabyte collection show that the proposed incremental approach can improve the efficiency of pseudo relevance feedback method by a factor of two without sacrificing their effectiveness 
due to an explosion in the amount of medical information available search technique are gaining importance in the medical domain this tutorial discus recent result on search in the medical domain including the outcome of survey on end user requirement research relevant to the field and current medical and health search application available finally the extent to which available technique meet user requirement are discussed and open challenge in the field are identified 
latent semantic analysis lsa ha been intensively studied because of it wide application to information retrieval and natural language processing yet traditional model such a lsa only examine one current version of the document however due to the recent proliferation of collaboratively generated content such a thread in online forum collaborative question answering archive wikipedia and other versioned content the document generation process is now directly observable in this study we explore how this additional temporal information about the document evolution could be used to enhance the identification of latent document topic specifically we propose a novel hidden topic modeling algorithm temporal latent semantic analysis tlsa which elegantly extends lsa to modeling document revision history using tensor decomposition our experiment show that tlsa outperforms lsa on word relatedness estimation using benchmark data and explore application of tlsa for other task 
compromised website are often used by attacker to deliver malicious content or to host phishing page designed to steal private information from their victim unfortunately most of the targeted website are managed by user with little security background often unable to detect this kind of threat or to afford an external professional security service in this paper we test the ability of web hosting provider to detect compromised website and react to user complaint we also test six specialized service that provide security monitoring of web page for a small fee during a period of day we hosted our own vulnerable website on shared hosting provider including of the most popular one we repeatedly ran five different attack against each of them our test included a bot like infection a drive by download the upload of malicious file an sql injection stealing credit card number and a phishing kit for a famous american bank in addition we also generated traffic from seemingly valid victim of phishing and drive by download site we show that most of these attack could have been detected by free network or file analysis tool after day if no malicious activity wa detected we started to file abuse complaint to the provider this allowed u to study the reaction of the web hosting provider to both real and bogus complaint the general picture we drew from our study is quite alarming the vast majority of the provider or add on security monitoring service are unable to detect the most simple sign of malicious activity on hosted website 
micro blogging system such a twitter expose digital trace of social discourse with an unprecedented degree of resolution of individual behavior they offer an opportunity to investigate how a large scale social system responds to exogenous or endogenous stimulus and to disentangle the temporal spatial and topical aspect of user activity here we focus on spike of collective attention in twitter and specifically on peak in the popularity of hashtags user employ hashtags a a form of social annotation to define a shared context for a specific event topic or meme we analyze a large scale record of twitter activity and find that the evolution of hashtag popularity over time defines discrete class of hashtags we link these dynamical class to the event the hashtags represent and use text mining technique to provide a semantic characterization of the hashtag class moreover we track the propagation of hashtags in the twitter social network and find that epidemic spreading play a minor role in hashtag popularity which is mostly driven by exogenous factor 
prior research in resource selection for federated search mainly focused on selecting a small number of information source that are most relevant to a user query however result novelty and diversification are largely unexplored which doe not reflect the various kind of information need of user in real world application this paper proposes two general approach to model both result relevance and diversification in selecting source in order to provide more comprehensive coverage of multiple aspect of a user query the first approach focus on diversifying the document ranking on a centralized sample database before selecting information source under the framework of relevant document distribution estimation redde the second approach first evaluates the relevance of information source with respect to each aspect of the query and then rank the source based on the novelty and relevance that they offer both approach can be applied with a wide range of existing resource selection algorithm such a redde crcs cori and big document moreover this paper proposes a learning based approach to combine multiple resource selection algorithm for result diversification which can further improve the performance we propose a set of new metric for resource selection in federated search to evaluate the diversification performance of different approach to our best knowledge this is the first piece of work that address the problem of search result diversification in federated search the effectiveness of the proposed approach ha been demonstrated by an extensive set of experiment on the federated search testbed of the clueweb dataset 
in this talk we present a perspective across multiple industry problem including safety and security medical web social and mobile medium and motivate the need for large scale analysis and retrieval of multimedia data we describe a multi layer architecture that incorporates capability for audio visual feature extraction machine learning and semantic modeling and provides a powerful framework for learning and classifying content of multimedia data we discus the role semantic ontology for representing audio visual concept and relationship which are essential for training semantic classifier we discus the importance of using faceted classification scheme in particular for organizing multimedia semantic concept in order to achieve effective learning and retrieval we also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platform to address both massive scale analysis and low latency processing we describe multiple effort at ibm on image and video analysis and retrieval including ibm multimedia analysis and retrieval system imars and show recent result for semantic based classification and retrieval we conclude with future direction for improving analysis of multimedia through interactive and curriculum based technique for multimedia semantics based learning and retrieval 
we evaluate statistical inference procedure for small scale ir experiment that involve multiple comparison against the baseline these procedure adjust for multiple comparison by ensuring that the probability of observing at least one false positive in the experiment is below a given threshold we use only publicly available test collection and make our software available for download in particular we employ the trec run and run constructed from the microsoft learning to rank mslr data set our focus is on non parametric statistical procedure that include the holm bonferroni adjustment of the permutation test p value the maxt permutation test and the permutation based closed testing in trec based simulation these procedure retain from to of individually significant result i e those obtained without taking other comparison into account similar retention rate are observed in the mslr simulation for the largest evaluated query set size i e procedure that adjust for multiplicity find at most fewer true difference compared to unadjusted test at the same time unadjusted test produce many more false positive 
we study information goal and pattern of attention in explorato ry search for health information on the web reporting result of a large scale log based study we examine search activity associated with the goal of diagnosing illness from symptom versus more general information seeking about health and illness we decom pose exploratory health search into evidence based and hypothe si directed information seeking evidence based search center on the pursuit of detail and relevance of sign and symptom hypothesis directed search includes the pursuit of content on one or more illness including risk factor treatment and therapy for illness and on the discrimination among different disease under the uncertainty that exists in advance of a confirmed diag nosis these different goal of exploratory health search are not independent and transition can occur between them within or across search session we construct a classifier that identifies medically related search session in log data given a set of search session flagged a health related we show how we can identify different intention persisting a focus of attention within those session finally we discus how insight about focus dynamic can help u better understand exploratory health search behavior and better support health search on the web 
exploratory search in which a user investigates complex concept is cumbersome with today s search engine we present a new exploratory search approach that generates interactive visualization of query concept using thematic cartography e g choropleth map heat map we show how the approach can be applied broadly across both geographic and non geographic context through explicit spatialization a novel method that leverage any figure or diagram from a periodic table to a parliamentary seating chart to a world map a a spatial search environment we enable this capability by introducing explanatory semantic relatedness measure these measure extend frequently used semantic relatedness measure to not only estimate the degree of relatedness between two concept but also generate human readable explanation for their estimate by mining wikipedia s text hyperlink and category structure we implement our approach in a system called atlasify evaluate it key component and present several use case 
most of the online advertising today is sold via an auction which requires the advertiser to respond with a valid bid within a fraction of a second a such most advertiser employ bidding agent to submit bid on their behalf the architecture of such agent typically ha an offline optimization phase which incorporates the bidder s knowledge about the market and an online bidding strategy which simply executes the offline strategy the online strategy is typically highly dependent on both supply and expected price distribution both of which are forecast using traditional machine learning method in this work we investigate the optimum strategy of the bidding agent when faced with incorrect forecast at a high level the agent can invest resource in improving the forecast or can tighten the loop between successive offline optimization cycle in order to detect error more quickly we show analytically that the latter strategy while simple is extremely effective in dealing with forecast error and confirm this finding with experimental evaluation 
method that reduce the amount of labeled data needed for training have focused more on selecting which document to label than on which query should be labeled one exception to this long et al us expected loss optimization elo to estimate which query should be selected but is limited to ranker that predict absolute graded relevance in this work we demonstrate how to easily adapt elo to work with any ranker and show that estimating expected loss in dcg is more robust than ndcg even when the final performance measure is ndcg 
query suggestion is a useful tool to help user formulate better query although this ha been found highly useful globally it effect on different query may vary in this paper we examine the impact of query suggestion on query of different degree of difficulty it turn out that query suggestion is much more useful for difficult query than easy query in addition the suggestion for difficult query should rely le on their similarity to the original query in this paper we use a learning to rank approach to select query suggestion based on several type of feature including a query performance prediction a query suggestion ha different impact on different query we propose an adaptive suggestion approach that make suggestion only for difficult query we carry out experiment on real data from a search engine our result clearly indicate that an approach targeting difficult query can bring higher gain than a uniform suggestion approach 
in recent year social networking site have not only enabled people to connect with each other using social link but have also allowed them to share communicate and interact over diverse geographical region social network provide a rich source of heterogeneous data which can be exploited to discover previously unknown relationship and interest among group of people in this paper we address the problem of discovering topically meaningful community from a social network we assume that a person membership in a community is conditioned on it social relationship the type of interaction and the information communicated with other member of that community we propose generative model that can discover community based on the discussed topic interaction type and the social connection among people in our model a person can belong to multiple community and a community can participate in multiple topic this allows u to discover both community interest and user interest based on the information and linked association we demonstrate the effectiveness of our model on two real word data set and show that it performs better than existing community discovery model 
the betweenness centrality of a vertex in a graph is a measure for the participation of the vertex in the shortest path in the graph the betweenness centrality is widely used in network analysis especially in a social network the recursive computation of the betweenness centrality of vertex is performed for the community detection and finding the influential user in the network since a social network graph is frequently updated it is necessary to update the betweenness centrality efficiently when a graph is changed the betweenness centrality of all the vertex should be recomputed from scratch using all the vertex in the graph to the best of our knowledge this is the first work that proposes an efficient algorithm which handle the update of the betweenness centrality of vertex in a graph in this paper we propose a method that efficiently reduces the search space by finding a candidate set of vertex whose betweenness centrality can be updated and computes their betweenness centeralities using candidate vertex only a the cost of calculating the betweenness centrality mainly depends on the number of vertex to be considered the proposed algorithm significantly reduces the cost of calculation the proposed algorithm allows the transformation of an existing algorithm which doe not consider the graph update experimental result on large real datasets show that the proposed algorithm speed up the existing algorithm to time depending on the dataset 
in this work we study the cluster hypothesis for entity oriented search eos specifically we show that the hypothesis can hold to a substantial extent for several entity similarity measure we also demonstrate the retrieval effectiveness merit of using cluster of similar entity for eos 
short url have become ubiquitous especially popular within social networking service short url have seen a significant increase in their usage over the past year mostly due to twitter s restriction of message length to character in this paper we provide a first characterization on the usage of short url specifically our goal is to examine the content short url point to how they are published their popularity and activity over time a well a their potential impact on the performance of the web our study is based on trace of short url a seen from two different perspective i collected through a large scale crawl of url shortening service and ii collected by crawling twitter message the former provides a general characterization on the usage of short url while the latter provides a more focused view on how certain community use shortening service our analysis highlight that domain and website popularity a seen from short url significantly differs from the distribution provided by well publicised service such a alexa the set of most popular website pointed to by short url appears stable over time despite the fact that short url have a limited high popularity lifetime surprisingly short url are not ephemeral a a significant fraction roughly appears active for more than three month overall our study emphasizes the fact that short url reflect an alternative web and hence provide an additional view on web usage and content consumption complementing traditional measurement source furthermore our study reveals the need for alternative shortening architecture that will eliminate the non negligible performance penalty imposed by today s shortening service 
entity relationship search at web scale depends on adding dozen of entity annotation to each of billion of crawled page and indexing the annotation at rate comparable to regular text indexing even small entity search benchmark from trec and inex suggest that the entity catalog support thousand of entity type and ten to hundred of million of entity the above target raise many challenge major one being the design of highly compressed data structure in ram for spotting and disambiguating entity mention and highly compressed disk based annotation index these data structure cannot be readily built upon standard inverted index here we present a web scale entity annotator and annotation index using a new workload sensitive compressed multilevel map we fit statistical disambiguation model for million of entity within gb of ram and spend about core millisecond per disambiguation in contrast dbpedia spotlight spends millisecond wikipedia miner spends millisecond and zemanta spends millisecond our annotation index use idea from vertical database to reduce storage by on x core with x disk spindle we can annotate and index in about a day a billion web page with two million entity and type from wikipedia index decompression and scan speed are comparable to mg j 
it is well known that anchor text play an important role in search providing signal that are often not present in the source document itself the paper report result of a preliminary investigation on the value of tweet and tweet conversation a anchor text we show that using tweet a anchor improves significantly over using html anchor and significantly increase recall of news item retrieval 
computing the degree of semantic relatedness of word is a key functionality of many language application such a search clustering and disambiguation previous approach to computing semantic relatedness mostly used static language resource while essentially ignoring their temporal aspect we believe that a considerable amount of relatedness information can also be found in studying pattern of word usage over time consider for instance a newspaper archive spanning many year two word such a war and peace might rarely co occur in the same article yet their pattern of use over time might be similar in this paper we propose a new semantic relatedness model temporal semantic analysis tsa which capture this temporal information the previous state of the art method explicit semantic analysis esa represented word semantics a a vector of concept tsa us a more refined representation where each concept is no longer scalar but is instead represented a time series over a corpus of temporally ordered document to the best of our knowledge this is the first attempt to incorporate temporal evidence into model of semantic relatedness empirical evaluation show that tsa provides consistent improvement over the state of the art esa result on multiple benchmark 
we demonstrate the yali browser plug in which discovers named entity in web page and provides background knowledge about them the plug in is implemented with two purpose from a user perspective it enriches the browsing experience with entity helping user with their information need from the research perspective we aim to improve the method that are used for named entity recognition and disambiguation nerd by leveraging the plug in a an implicit crowdsourcing platform yali track the system s error and the user correction and also gather implicit training data for improving nerd accuracy 
query expansion is a classical topic in the field of information retrieval which is proposed to bridge the gap between searcher information intent and their query previous research usually expand query based on document collection or some external resource such a wordnet and wikipedia however it seems that independently using one of these resource ha some defect document collection lack semantic information of word while wordnet and wikipedia may not include domain specific knowledge in certain document collection our work aim to combine these two kind of resource to establish an expansion model which represents not only domain specific information but also semantic information in our preliminary experiment we construct a two layer word graph and use random walk algorithm to calculate the weight of each term in pseudo relevance feedback document then select the highest weighted term to expand original query the first layer of the word graph contains term in related document while the second layer contains semantic sens corresponding to these term these term and semantic sens are treated a vertex of the graph and connected with each other by all possible relationship such a mutual information and semantic similarity we utilized mutual information semantic similarity and uniform distribution a the weight of term term relation sense sense relation and word sense relation respectively though these experiment show that our expansion outperform original query we are troubled with some difficult problem given the framework of semantic graph model we need more effort to find out an optimal graph to represent the relationship between term and their semantic sens we utilized a two layer graph model in our preliminary research where term from different document are treated equally maybe we can introduce the document a a third layer in future work where we can differ the same term in different document according to document relevance and context then we need appropriately represent initial weight of this word sens and relationship various measure for weight of term and term relation have been proved effective in other information retrieval task such a tfidf mutual information mi but there is little research on weight for semantic sens and their relation for polysemous word we add all of their semantic sens to the graph and assume that these sens are uniformly distributed actually it is not precise for a word in a special document and query a we know a polysemous word may have only one or two sens in a document and they are not uniformly distributed give a word what we should do is to determine it word sens in a relevant document and estimate the distribution of these sens word sense disambiguation may help u in this problem then there are many method to compute word similarity according to wordnet which we use to represent the weight of relationship between word sens varelas et al implemented some popular method to compute semantic similarity by mapping term to an ontology and examining their relationship in that ontology we also need to know which algorithm for semantic similarity is most suitable for our model additional wordnet is suitable to calculate word similarity but not suitable to measure word relevance the inner hyperlink of wikipedia could help u to calculate word relevance we wish to find an effective way to combine the similarity measure from wordnet and relevance measure from wikipedia which may completely reflect word relationship 
acronym are abbreviation formed from the initial component of word or phrase acronym usage is becoming more common in web search email text message tweet blog and post acronym are typically ambiguous and often disambiguated by context word given either just an acronym a a query or an acronym with a few context word it is immensely useful for a search engine to know the most likely intended meaning ranked by their likelihood to support such online scenario we study the offline mining of acronym and their meaning in this paper for each acronym our goal is to discover all distinct meaning and for each meaning compute the expanded string it popularity score and a set of context word that indicate this meaning existing approach are inadequate for this purpose our main insight is to leverage co click in search engine query click log to mine expansion of acronym there are several technical challenge such a ensuring mapping between expansion and meaning handling of tail meaning and extracting context word we present a novel end to end solution that address the above challenge we further describe how web search engine can leverage the mined information for prediction of intended meaning for query containing acronym our experiment show that our approach i discovers the meaning of acronym with high precision and recall ii significantly complement existing meaning in wikipedia and iii accurately predicts intended meaning for online query with over precision 
result merging is an important research problem in federated search for merging document retrieved from multiple ranked list of selected information source into a single list the state of the art result merging algorithm such a semi supervised learning ssl and sample agglomerate fitting estimate safe try to map document score retrieved from different source to comparable score according to a single centralized retrieval algorithm for ranking those document both ssl and safe arbitrarily select a single centralized retrieval algorithm for generating comparable document score which is problematic in a heterogeneous federated search environment since a single centralized algorithm is often suboptimal for different information source based on this observation this paper proposes a novel approach for result merging by utilizing multiple centralized retrieval algorithm one simple approach is to learn a set of combination weight for multiple centralized retrieval algorithm e g logistic regression to compute comparable document score the paper show that this simple approach generates suboptimal result a it is not flexible enough to deal with heterogeneous information source a mixture probabilistic model is thus proposed to learn more appropriate combination weight with respect to different type of information source with some training data an extensive set of experiment on three datasets have proven the effectiveness of the proposed new approach 
a knowledge base kb store organizes and share information pertinent to entity i e kb node such a people organization and event a large kb system such a wikipedia relies on human curator to create and maintain the content in the system however it ha become challenging for human curator to sift through the rapidly growing amount of information and filter out the information irrelevant to a kb node the area of knowledge base enhancement kbe aim to explore and identify automatic method to assist human curator to accelerate the process kbe can be viewed a a special case of information filtering if however the lack of high quality labelled data introduces a major challenge to train a satisfying model for the task transfer learning provides solution to the problem and ha explored application in the area of text mining whereas direct application to kbe or if remains absent transfer learning is a research area in machine learning emphasizing the reuse of previously acquired knowledge to another applicable task the method is particularly useful in the situation where labeled instance are absent or difficult to obtain to accelerate the growth of a kb a transfer learning approach enables leveraging the heuristic and model learned from one kb node to another for example reusing the learned filtering model from willie nelson a famous country singer to eddie rabbitt another country singer transfer learning requires three component the target task e g the problem to be solved the source task s e g auxiliary data previously studied problem and criterion to select appropriate source task the objective of my dissertation are twofold first it explores method to identify informative source node from which to transfer second it construct a knowledge transfer network to represent the transfer learning relationship between kb node this proposed research applies a transfer learning method segmented transfer st and a knowledge representation knowledge transfer network ktn to approach the area of kbe the primary research question include what are the transferable object in information filtering algorithm what are the kb node of high transferability what are the factor that determine the transfer learning relationship doe it manifest on a knowledge transfer network representation this interdisciplinary research cross the study area of information filtering machine learning knowledge representation and network analysis this proposal motivates the problem of kbe discus the research methodology and proposed experiment and review related work in information filtering and transfer learning this line of research hope to extend the application of transfer learning to kbe and to explore a new dimension of if the proposed st and ktn intends to bring interdisciplinary approach in the emerging field of kbe 
classic news summarization play an important role with the exponential document growth on the web many approach are proposed to generate summary but seldom simultaneously consider evolutionary characteristic of news plus to traditional summary element therefore we present a novel framework for the web mining problem named evolutionary timeline summarization ets given the massive collection of time stamped web document related to a general news query ets aim to return the evolution trajectory along the timeline consisting of individual but correlated summary of each date emphasizing relevance coverage coherence and cross date diversity ets greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity we formally formulate the task a an optimization problem via iterative substitution from a set of sentence to a subset of sentence that satisfies the above requirement balancing coherence diversity measurement and local global summary quality the optimized substitution is iteratively conducted by incorporating several constraint until convergence we develop experimental system to evaluate on instinctively different datasets which amount to document performance comparison between different system generated timeline and manually created one by human editor demonstrate the effectiveness of our proposed framework in term of rouge metric 
we introduce the problem of domain adaptation for content based retrieval and propose a domain adaptation method based on relative aggregation point rap content based retrieval including image retrieval and spoken document retrieval enables a user to input example a a query and retrieves relevant data based on the similarity to the example however input example and relevant data can be dissimilar especially when domain from which the user selects example and from which the system retrieves data are different in content based geographic object retrieval for example suppose that a user who life in beijing visit kyoto japan and want to search for relatively inexpensive restaurant serving popular local dish by mean of a content based retrieval system since such restaurant in beijing and kyoto are dissimilar due to the difference in the average cost and area popular dish it is difficult to find relevant restaurant in kyoto based on example selected in beijing we propose a solution for this problem by assuming that rap in different domain correspond which may be dissimilar but play the same role a rap is defined a the expectation of instance in a domain that are classified into a certain class e g the most expensive restaurant average restaurant and restaurant serving the most popular dish our proposed method construct a new feature space based on rap estimated in each domain and bridge the domain difference for improving content based retrieval in heterogeneous domain to verify the effectiveness of our proposed method we evaluated various method with a test collection developed for content based geographic object retrieval experimental result show that our proposed method achieved significant improvement over baseline method moreover we observed that the search performance of content based retrieval in heterogeneous domain wa significantly lower than that in homogeneous domain this finding suggests that relevant data for the same search intent depend on the search context that is the location where the user search and the domain from which the system retrieves data 
aggregated search is the task of blending result from specialized search service or vertical into the web search result while many study have focused on aggregated search technique few study have tried to better understand how user interact with aggregated search result this study investigates how task complexity and vertical display the blending of vertical result into the web result affect the use of vertical content twenty nine subject completed six search task of varying level of task complexity using two aggregated search interface one that blended vertical result into the web result and one that only provided indirect vertical access our result show that more complex task required significantly more interaction and that subject completing these task examined more vertical result while the amount of interaction wa the same between interface subject clicked on more vertical result when these were blended into the web result our result also show an interaction between task complexity and vertical display subject clicked on more vertical when completing the more complex task with the interface that blended vertical result subject evaluation of the two interface were nearly identical but when analyzed with respect to their interface preference we found a positive relationship between system evaluation and individual preference subject justified their preference using similar rationale and their comment illustrate how the display itself can influence judgment of information quality especially in case when the vertical result might not be relevant to the search task 
online forum discussion are emerging a valuable information repository where knowledge is accumulated by the interaction among user leading to multiple thread with structure such replying structure in each thread conveys important information about the discussion content unfortunately not all the online forum site would explicitly record such replying relationship making it hard to for both user and computer to digest the information buried in a thread discussion in this paper we propose a probabilistic model in the conditional random field framework to predict the replying structure for a threaded online discussion different from previous thread reconstruction method most of which fail to consider dependency between the post we cast the problem a a supervised structure learning problem to incorporate the feature describing the structural dependency among the discussion content and learn their relationship experiment result on three different online forum show that the proposed method can well capture the replying structure in online discussion thread and multiple task such a forum search and question answering can benefit from the reconstructed replying structure 
in this paper we propose a joint probabilistic topic model for simultaneously modeling the content of multi typed object of a heterogeneous information network the intuition behind our model is that different object of the heterogeneous network share a common set of latent topic so a to adjust the multinomial distribution over topic for different object collectively experimental result demonstrate the effectiveness of our approach for the task of topic modeling and object clustering 
we describe crowdlogging an approach for distributed search log collection storage and mining with the dual goal of preserving privacy and making the mined information broadly available most search log mining approach and most privacy enhancing scheme have focused on centralized search log and method for disseminating them to third party in our approach a user s search log is encrypted and shared in such a way that a the source of a search behavior artifact such a a query is unknown and b extremely rare artifact that is artifact more likely to contain private information are not revealed the approach work with any search behavior artifact that can be extracted from a search log including query query reformulations and query click pair in this work we present a distributed search log collection storage and mining framework compare several privacy policy including differential privacy showing the trade offs between strong guarantee and the utility of the released data demonstrate the impact of our approach using two existing research query log and describe a pilot study for which we implemented a version of the framework 
commodity information such a price and public review is always the concern of consumer helping them conveniently acquire these information a an instant reference is often of practical significance for their purchase activity nowadays web linked data cloud and the pervasiveness of smart hand held device have created opportunity for this demand i e user could just snap a photo of any commodity that is of interest at anytime and anywhere and retrieve the relevant information via their internet linked mobile device nonetheless compared with the traditional keyword based information retrieval extracting the hidden information related to the commodity in photo is a much more complicated and challenging task involving technique such a pattern recognition knowledge base construction semantic comprehension and statistic deduction in this paper we propose a framework to address this issue by leveraging on various technique and evaluate the effectiveness and efficiency of this framework with experiment on a prototype 
twitter ha rapidly grown to a popular social network in recent year and provides a large number of real time message for user tweet are presented in chronological order and user scan the followees timeline to find what they are interested in however an information overload problem ha troubled many user especially those with many followees and thousand of tweet arriving every day in this paper we focus on recommending useful tweet that user are really interested in personally to reduce the user effort to find useful information many kind of information on twitter are available for helping recommendation including the user s own tweet history retweet history and social relation between user we propose a method of making tweet recommendation based on collaborative ranking to capture personal interest it can also conveniently integrate the other useful contextual information our final method considers three major element on twitter tweet topic level factor user social relation factor and explicit feature such a authority of the publisher and quality of the tweet the experiment show that all the proposed element are important and our method greatly outperforms several baseline method 
choreography analysis ha been a crucial problem in service oriented computing interaction among service involve message exchange across organizational boundary in a distributed computing environment and in order to build such system in a reliable manner it is necessary to develop technique for analyzing such interaction choreography conformance involves verifying that a set of service behave according to a given choreography specification that characterizes their interaction unfortunately this is an undecidable problem when service interact with asynchronous communication in this paper we present technique that identify if the interaction behavior for a set of service remain the same when asynchronous communication is replaced with synchronous communication this is called the synchronizability problem and determining the synchronizability of a set of service ha been an open problem for several year we solve this problem in this paper our result can be used to identify synchronizable service for which choreography conformance can be checked efficiently our result on synchronizability are applicable to any software infrastructure that support message based interaction 
in many domain of information retrieval system estimate of document relevance are based on multidimensional quality criterion that have to be accommodated in a unidimensional result ranking current solution to this challenge are often inconsistent with the formal probabilistic framework in which constituent score were estimated or use sophisticated learning method that make it difficult for human to understand the origin of the final ranking to address these issue we introduce the use of copula a powerful statistical framework for modeling complex multi dimensional dependency to information retrieval task we provide a formal background to copula and demonstrate their effectiveness on standard ir task such a combining multidimensional relevance estimate and fusion of result from multiple search engine we introduce copula based version of standard relevance estimator and fusion method and show that these lead to significant performance improvement on several task a evaluated on large scale standard corpus compared to their non copula counterpart we also investigate criterion for understanding the likely effect of using copula model in a given retrieval scenario 
document clustering is a popular research topic which aim to partition document into group of similar object i e cluster and ha been widely used in many application such a automatic topic extraction document organization and filtering a a recently proposed concept universum is a collection of non example that do not belong to any concept cluster of interest this paper proposes a novel document clustering technique document clustering with universum which utilizes the universum example to improve the clustering performance the intuition is that the universum example can serve a supervised information and help improve the performance of clustering since they are known not belonging to any meaningful concept cluster in the target domain in particular a maximum margin clustering method is proposed to model both target example and universum example for clustering an extensive set of experiment is conducted to demonstrate the effectiveness and efficiency of the proposed algorithm 
we investigate the design of mechanism to incentivize high quality outcome in crowdsourcing environment with strategic agent when entry is an endogenous strategic choice modeling endogenous entry in crowdsourcing market is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating and indeed many site based on crowdsourced content do not have adequate participation we use a mechanism with monotone rank based reward in a model where agent strategically make participation and quality choice to capture a wide variety of crowdsourcing environment ranging from conventional crowdsourcing contest with monetary reward such a topcoder to crowdsourced content a in online q a forum we begin by explicitly constructing the unique mixed strategy equilibrium for such monotone rank order mechanism and use the participation probability and distribution of quality from this construction to address the question of designing incentive for two kind of reward that arise in the context of crowdsourcing we first show that for attention reward that arise in the crowdsourced content setting the entire equilibrium distribution and therefore every increasing statistic including the maximum and average quality accounting for participation improves when the reward for every rank but the last are a high a possible in particular when the cost of producing the lowest possible quality content is low the optimal mechanism display all but the poorest contribution we next investigate how to allocate reward in setting where there is a fixed total reward that can be arbitrarily distributed amongst participant a in crowdsourcing contest unlike model with exogenous entry here the expected number of participant can be increased by subsidizing entry which could potentially improve the expected value of the best contribution however we show that subsidizing entry doe not improve the expected quality of the best contribution although it may improve the expected quality of the average contribution in fact we show that free entry is dominated by taxing entry making all entrant pay a small fee which is rebated to the winner along with whatever reward were already assigned can improve the quality of the best contribution over a winner take all contest with no tax 
click model aim at extracting intrinsic relevance of document to query from biased user click one basic modeling assumption made in existing work is to treat such intrinsic relevance a an atomic query document specific parameter which is solely estimated from historical click without using any content information about a document or relationship among the clicked skipped document under the same query due to this overly simplified assumption existing click model can neither fully explore the information about a document s relevance quality nor make prediction of relevance for any unseen document in this work we proposed a novel bayesian sequential state model for modeling the user click behavior where the document content and dependency among the sequential click event within a query are characterized by a set of descriptive feature via a probabilistic graphical model by applying the posterior regularized expectation maximization algorithm for parameter learning we tailor the model to meet specific ranking oriented property e g pairwise click preference so a to exploit richer information buried in the user click experiment result on a large set of real click log demonstrate the effectiveness of the proposed model compared with several state of the art click model 
singing is a popular social activity and a good way of expressing one s feeling one important reason for unsuccessful singing performance is because the singer fails to choose a suitable song in this paper we propose a novel singing competence based song recommendation framework it is distinguished from most existing music recommendation system which rely on the computation of listener interest or similarity we model a singer s vocal competence a singer profile which take voice pitch intensity and quality into consideration then we propose technique to acquire singer profile we also present a song profile model which is used to construct a human annotated song database finally we propose a learning to rank scheme for recommending song by singer profile the experimental study on real singer demonstrates the effectiveness of our approach and it advantage over two baseline method to the best of our knowledge our work is the first to study competence based song recommendation 
content on the internet is always changing we explore the value of biasing search result snippet towards new webpage content we present result from a user study comparing traditional query focused snippet with snippet that emphasize new page content for two query type general and trending our result indicate that searcher prefer the inclusion of temporal information for trending query but not for general query and that this is particularly valuable for page that have not been recently crawled 
proliferation of ubiquitous access to the internet enables million of web user to collaborate online on a variety of activity many of these activity result in the construction of large repository of knowledge either a their primary aim e g wikipedia or a a by product e g yahoo answer in this tutorial we will discus organizing and exploiting collaboratively generated content cgc for information organization and retrieval specifically we intend to cover two complementary area of the problem using such content a a powerful enabling resource for knowledge enriched intelligent representation and new information retrieval algorithm and development of supporting technology for extracting filtering and organizing collaboratively created content the unprecedented amount of information in cgc enable new knowledge rich approach to information access which are significantly more powerful than the conventional word based method considerable progress ha been made in this direction over the last few year example include explicit manipulation of human defined concept and their use to augment the bag of word cf explicit semantic analysis using large scale taxonomy of topic from wikipedia or the open directory project to construct additional class based feature or using wikipedia for better word sense disambiguation however the quality and comprehensiveness of collaboratively created content vary widely and in order for this resource to be useful a significant amount of preprocessing filtering and organization is necessary consequently new method for analyzing cgc and corresponding user interaction are required to effectively harness the resulting knowledge thus not only the content repository can be used to improve ir method but the reverse pollination is also possible a better information extraction method can be used for automatically collecting more knowledge or verifying the contributed content this natural connection between modeling the generation process of cgc and effectively using the accumulated knowledge suggests covering both area together in a single tutorial the intended audience of the tutorial includes ir researcher and graduate student who would like to learn about the recent advance and research opportunity in working with collaboratively generated content the emphasis of the tutorial is on comparing the existing approach and presenting practical technique that ir practitioner can use in their research we also cover open research challenge a well a survey available resource software tool and data for getting started in this research field 
this paper present a framework for evaluating information retrieval of medical record we use the blulab corpus a large collection of real world de identified medical record the collection ha been hand coded by clinical terminologists using the icd medical classification system the icd code are used to devise query and relevance judgement for this collection result of initial test run using a baseline ir system show that there is room for improvement in medical information retrieval query and relevance judgement are made available at http aehrc com med eval 
this full day tutorial present a comprehensive introduction to entity linking and retrieval part i provides a detailed overview of entity linking identifying and disambiguating entity occurrence in unstructured text part ii focus on entity retrieval by first considering scenario where explicit representation of entity are available and then moving to a setting where evidence need to be collected and aggregated from multiple document or even collection thereby combining technique from both entity linking and entity retrieval part iii concludes the tutorial with an overview and hand on comparative analysis of application and publicly available toolkits and web service 
over the year private file sharing community built on the bittorrent protocol have developed their own policy and mechanism for motivating member to share content and contribute resource by requiring member to maintain a minimum ratio between uploads and downloads private community effectively establish credit system and with them full fledged economy we report on a half year long measurement study of dime a community for sharing live concert recording that shed light on the economic force affecting user in such community a key observation is that while the download of file is priced only according to the size of the file the rate of return for seeding new file is significantly greater than for seeding old file we find via a natural experiment that user react to such difference in resale value by preferentially consuming older file during a free leech period we consider implication of these finding on a user s ability to earn credit and meet ratio enforcement focusing in particular on the relationship between visitation frequency and wealth and on low bandwidth user we then share detail from an interview with dime moderator which highlight the goal of the community based on which we make suggestion for possible improvement 
the standard system based evaluation paradigm ha focused on assessing the performance of retrieval system in serving the best result for a single query real user however often begin an interaction with a search engine with a sufficiently under specified query that they will need to reformulate before they find what they are looking for in this work we consider the problem of evaluating retrieval system over test collection of multi query session we propose two family of measure a model free family that make no assumption about the user s behavior over a session and a model based family with a simple model of user interaction over the session in both case we generalize traditional evaluation metric such a average precision to multi query session evaluation we demonstrate the behavior of the proposed metric by using the new trec session track collection and simulation over the trec query track collection 
recommender system r attempt to discover user preference and to learn about them in order to anticipate their need the main task normally associated with a r is to offer suggestion for item however for most user r are black box computerized oracle that give advice but cannot be questioned in order to improve the quality of prediction and the satisfaction of the user explanation facility are needed we present a novel methodology to explain recommendation showing prediction over a set of observed item our proposal ha been validated by mean of user study and lab experiment using movielens dataset 
in recent year online shopping is becoming more and more popular user type keyword query on product search system to find relevant product accessory and even related product however existing product search system always return very similar product on the first several page instead of taking diversity into consideration in this paper we propose a novel approach to address the diversity issue in the context of product search we transform search result diversification into a combination of diversifying product category and diversifying product attribute value within each category the two sub problem are optimization problem which can be reduced into well known np hard problem respectively we further leverage greedy based approximation algorithm for efficient product search result re ranking 
graph simulation ha been adopted for pattern matching to reduce the complexity and capture the need of novel application with the rapid development of the web and social network data is typically distributed over multiple machine hence a natural question raised is how to evaluate graph simulation on distributed data to our knowledge no such distributed algorithm are in place yet this paper settle this question by providing evaluation algorithm and optimization for graph simulation in a distributed setting we study the impact of component and data locality on the evaluation of graph simulation we give an analysis of a large class of distributed algorithm captured by a message passing model for graph simulation we also identify three complexity measure visit time makespan and data shipment for analyzing the distributed algorithm and show that these measure are essentially controversial with each other we propose distributed algorithm and optimization technique that exploit the property of graph simulation and the analysis of distributed algorithm we experimentally verify the effectiveness and efficiency of these algorithm using both real life and synthetic data 
aggregating search result from a variety of heterogeneous source or vertical such a news image and video into a single interface is a popular paradigm in web search although various approach exist for selecting relevant vertical or optimising the aggregated search result page evaluating the quality of an aggregated page is an open question this paper proposes a general framework for evaluating the quality of aggregated search page we evaluate our approach by collecting annotated user preference over a set of aggregated search page for topic and vertical we empirically demonstrate the fidelity of metric instantiated from our proposed framework by showing that they strongly agree with the annotated user preference of pair of simulated aggregated page furthermore we show that our metric agree with the majority preference more often than current diversity based information retrieval metric finally we demonstrate the flexibility of our framework by showing that personalised historical preference data can be used to improve the performance of our proposed metric 
distributed information retrieval dir also known a federated search integrates multiple searchable collection and provides direct access to them through a unified interface this is done by a centralized broker that receives user query forward them to appropriate collection and return merged result to user in practice most of federated resource do not cooperate with a broker and do not provide neither their content nor the statistic used for retrieval this is known a uncooperative dir in this case a broker creates a resource representation by sending sample query to a collection and analyzing retrieved document this process is called query based sampling the key issue here is the following how many document have to be retrieved from a resource in order to obtain a representative sample although there have been a number of attempt to address this issue it is still not solved appropriately for a given user query resource are ranked according to their similarity to the query or based on the number of relevant document they contain since resource representation are usually incomplete the similarity or the number of relevant document cannot be calculated precisely resource selection algorithm proposed in the literature estimate these number based on incomplete sample however these estimate are subject to error in practice inaccurate estimate that have high error should be trusted le then the more accurate estimate with low error unfortunately none of the existing algorithm can make the calculation of the estimation error possible therefore the following question arise how to estimate resource score so that the estimation error can be calculated how to use these error in order to improve the resource selection performance existing result merging algorithm estimate normalized document score based on score of document that appear both in a sample and in a result list the problem similar to the resource selection one arises the normalized document score are only the estimate and are subject to error inaccurate estimate should be trusted le then the more accurate one again none of the existing algorithm provide a way for calculating these error thus the two question to be address on the result merging phase are similar to the resource selection one how to estimate normalized document score so that the estimation error can be calculated how to use these error in order to improve the result merging performance in this work we address the above issue by applying score distribution model sdm to different phase of dir in particular we discus the sdm based resource selection technique that allows the calculation of resource score estimation error and can be extended in order to calculate the number of document to be sampled from each resource for a given query we have performed initial experiment comparing the sdm based resource selection technique to the state of the art algorithm and we are currently experimenting with the sdm based result merging method we plan to apply the existing score normalization technique from meta search to the dir result merging problem however the sdm based result merging approach require the relevance score to be returned together with retrieved document it is not yet clear how to relax this strong assumption that doe not always hold in practice 
web search query are an encoding of the user s search intent and extracting structured information from them can facilitate central search engine operation like improving the ranking of search result and advertisement not surprisingly this area ha attracted a lot of attention in the research community in the last few year the problem is however made challenging by the fact that search query tend to be extremely succinct a condensation of user search need to the bare minimum set of keywords in this paper we consider the problem of extracting with no manual intervention the hidden structure behind the observed search query in a domain the origin of the constituent keywords a well a the manner the individual keywords are assembled together we formalize important property of the problem and then give a principled solution based on generative model that satisfies these property using manually labeled data we show that the query template extracted by our solution are superior to those discovered by strong baseline method the query template extracted by our approach have potential us in many search engine task query answering advertisement matching and targeting to name a few in this paper we study one such task estimating query advertisability and empirically demonstrate that using extracted template information can improve performance over and above the current state of the art 
in this paper we analyze a crowdsourcing system consisting of a set of user and a set of binary choice question each user ha an unknown fixed reliability that determines the user s error rate in answering question the problem is to determine the truth value of the question solely based on the user answer although this problem ha been studied extensively theoretical error bound have been shown only for restricted setting when the graph between user and question is either random or complete in this paper we consider a general setting of the problem where the user question graph can be arbitrary we obtain bound on the error rate of our algorithm and show it is governed by the expansion of the graph we demonstrate using several synthetic and real datasets that our algorithm outperforms the state of the art 
we address a question that ha been somewhat overlooked throughout the transition from classical ad hoc retrieval to web search how is the performance of classical retrieval approach affected by the presence of content manipulation our initial experiment have shown that the relative performance pattern of some classical retrieval strategy might change in the transition from non manipulated to manipulated corpus a natural future venue to explore is how to mix these strategy and make some of them more robust under presumed content manipulation condition 
this paper present an automatic method for understanding and interpreting the semantics of unannotated web image we observe that the relation between object in an image carry important semantics about the image to capture and describe such semantics we propose object relation network orn a graph model representing the most probable meaning of the object and their relation in an image guided and constrained by an ontology orn transfer the rich semantics in the ontology to image object and the relation between them while maintaining semantic consistency e g a soccer player can kick a soccer ball but cannot ride it we present an automatic system which take a raw image a input and creates an orn based on image visual appearance and the guide ontology we demonstrate various useful web application enabled by orns such a automatic image tagging automatic image description generation and image search by image 
this tutorial serf a an introductory course to the field of and state of the art in music information retrieval mir and in particular to music similarity estimation which is an essential component of music retrieval apart from explaining approach that estimate similarity based on acoustic property of an audio signal we review method that exploit mostly textual meta data from the web to build representation of music then used for similarity calculation additionally topic such a large scale music indexing information extraction for music personalization in music retrieval and evaluation of mir system are addressed 
textrank is a variant of pagerank typically used in graph that represent document and where vertex denote term and edge denote relation between term quite often the relation between term is simple term co occurrence within a fixed window of k term the output of textrank when applied iteratively is a score for each vertex i e a term weight that can be used for information retrieval ir just like conventional term frequency based term weight so far when computing textrank term weight over co occurrence graph the window of term co occurrence is always fixed this work departs from this and considers dynamically adjusted window of term co occurrence that follow the document structure on a sentenceand paragraph level the resulting textrank term weight are used in a ranking function that re rank initially returned search result in order to improve the precision of the ranking experiment with two ir collection show that adjusting the vicinity of term co occurrence when computing textrank term weight can lead to gain in early precision 
collaborative filtering cf aim to recommend item based on prior user interaction despite their success cf technique do not handle data sparsity well especially in the case of the cold start problem where there is no past rating for an item in this paper we provide a framework which is able to tackle such issue by considering item related emotion and semantic data in order to predict the rating of an item for a given user this framework relies on an extension of latent dirichlet allocation and on gradient boosted tree for the final prediction we apply this framework to movie recommendation and consider two emotion space extracted from the movie plot summary and the review and three semantic space actor director and genre experiment with the k and m movielens datasets show that including emotion and semantic information significantly improves the accuracy of prediction and improves upon the state of the art cf technique we also analyse the importance of each feature space and describe some uncovered latent group 
a large collection of historically significant recorded speech become increasingly available scholar are faced with the challenge of making sense of what they hear this paper proposes automatically linking conversational speech to related resource a one way of supporting that sense making task experiment result with transcribed conversation suggest that this kind of linking ha promise for helping to contextualize recording of detail oriented conversation and that simple sliding window bag of word technique can identify some useful link 
information spreading process are central to human interaction despite recent study in online domain little is known about factor that could affect the dissemination of a single piece of information in this paper we address this challenge by combining two related but distinct datasets collected from a large scale privacy preserving distributed social sensor system we find that the social and organizational context significantly impact to whom and how fast people forward information yet the structure within spreading process can be well captured by a simple stochastic branching model indicating surprising independence of context our result build the foundation of future predictive model of information flow and provide significant insight towards design of communication platform 
we analyze the information credibility of news propagated through twitter a popular microblogging service previous research ha shown that most of the message posted on twitter are truthful but the service is also used to spread misinformation and false rumor often unintentionally on this paper we focus on automatic method for assessing the credibility of a given set of tweet specifically we analyze microblog posting related to trending topic and classify them a credible or not credible based on feature extracted from them we use feature from user posting and re posting re tweeting behavior from the text of the post and from citation to external source we evaluate our method using a significant number of human assessment about the credibility of item on a recent sample of twitter posting our result show that there are measurable difference in the way message propagate that can be used to classify them automatically a credible or not credible with precision and recall in the range of to 
in this paper we study usefulness of user gender information for improving ranking of ambiguous query in personalized and non contextual setting this study is performed a a sequence of offline re ranking experiment and it demonstrates that the proposed gender aware ranking feature provide improvement in ranking quality it is also shown that the proposed personalized feature exhibit performance superior to non contextual one 
we report on a new kind of group conversation on twitter that we call a group chat these chat are periodic synchronized group conversation focused on specific topic and they exist at a massive scale the group and the member of these group are not explicitly known rather member agree on a hashtag and a meeting time e g pm pacific time every wednesday to discus a subject of interest topic of these chat are numerous and varied some are support group for example post partum depression and mood disorder group others are about a passionate interest topic include skiing photography movie wine and foodie community we develop a definition of a group that is inspired by how sociologist define group and present an algorithm for discovering group we prove that our algorithm find all group under certain assumption while these group are of course known to the people who participate in the discussion what we do not believe is known is the scale and variety of group we provide some insight into the nature of these group based on over two year of tweet finally we show that group chat are a growing phenomenon on twitter and hope that reporting their existence propels their growth even further 
online gaming is a multi billion dollar industry that entertains a large global population one unfortunate phenomenon however poison the competition and the fun cheating the cost of cheating span from industry supported expenditure to detect and limit cheating to victim monetary loss due to cyber crime this paper study cheater in the steam community an online social network built on top of the world s dominant digital game delivery platform we collected information about more than million gamers connected in a global social network of which more than thousand have their profile flagged a cheater we also collected in game interaction data of over thousand player from a popular multiplayer gaming server we show that cheater are well embedded in the social and interaction network their network position is largely indistinguishable from that of fair player we observe that the cheating behavior appears to spread through a social mechanism the presence and the number of cheater friend of a fair player is correlated with the likelihood of her becoming a cheater in the future also we observe that there is a social penalty involved with being labeled a a cheater cheater are likely to switch to more restrictive privacy setting once they are tagged and they lose more friend than fair player finally we observe that the number of cheater is not correlated with the geographical real world population density or with the local popularity of the steam community 
twitter and foursquare are two well connected platform for sharing information where growing number of user post location related message in contrast to the longitude latitude geotags commonly used online e g on photo and tweet new place tag containing category information show more human readable high level information rather than a pair of coordinate this grant an opportunity for better understanding user physical location which can be used a context to facilitate other application e g location context aware advertisement in this paper we verify the assumption that user current trail contain cue of their future route the result from the preliminary experiment show promising performance of a basic markov chain based model 
accessing online information from various data source ha become a necessary part of our everyday life unfortunately such information is not always trustworthy a different source are of very different quality and often provide inaccurate and conflicting information existing approach attack this problem using unsupervised learning method and try to infer the confidence of the data value and trustworthiness of each source from each other by assuming value provided by more source are more accurate however because false value can be widespread through copying among different source and out of date data often overwhelm up to date data such bootstrapping method are often ineffective in this paper we propose a semi supervised approach that find true value with the help of ground truth data such ground truth data even in very small amount can greatly help u identify trustworthy data source unlike existing study that only provide iterative algorithm we derive the optimal solution to our problem and provide an iterative algorithm that converges to it experiment show our method achieves higher accuracy than existing approach and it can be applied on very huge data set when implemented with mapreduce 
this paper address blog feed retrieval where the goal is to retrieve the most relevant blog feed for a given user query since the retrieval unit is a blog a a collection of post performing relevance feedback technique and selecting the most appropriate document for query expansion becomes challenging by assuming time a an effective parameter on the blog post content we propose a time based query expansion method in this method we select term for expansion using most relevant day for the query a opposed to most relevant document this provide u with more trustable term for expansion our preliminary experiment on blog collection show that this method can outperform state of the art relevance feedback method in blog retrieval 
when generating query recommendation for a user a natural approach is to try and leverage not only the user s most recently submitted query or reference query but also information about the current search context such a the user s recent search interaction we focus on two important class of query that make up search context those that address the same information need a the reference query on task query and those that do not off task query we analyze the effect on query recommendation performance of using context consisting of only on task query only off task query and a mix of the two using trec session track data for simulation we demonstrate that on task context is helpful on average but can be easily overwhelmed when off task query are interleaved a common situation according to several analysis of commercial search log to minimize the impact of off task query on recommendation performance we consider automatic method of identifying such query using a state of the art search task identification technique our experimental result show that automatic search task identification can eliminate the effect of off task query in a mixed context we also introduce a novel generalized model for generating recommendation over a search context while we only consider query text in this study the model can handle integration over arbitrary user search behavior such a page visit dwell time and query abandonment in addition it can be used for other type of recommendation including personalized web search 
user vote are important signal in community question answering cqa system many feature of typical cqa system e g the best answer to a question status of a user are dependent on rating or vote cast by the community in a popular cqa site yahoo answer user vote for the best answer to their question and can also thumb up or down each individual answer prior work ha shown that these vote provide useful predictor for content quality and user expertise where each vote is usually assumed to carry the same weight a others in this paper we analyze a set of possible factor that indicate bias in user voting behavior these factor encompass different gaming behavior a well a other eccentricity e g vote to show appreciation of answerer these observation suggest that vote need to be calibrated before being used to identify good answer or expert to address this problem we propose a general machine learning framework to calibrate such vote through extensive experiment based on an editorially judged cqa dataset we show that our supervised learning method of content agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking 
topic modeling can boost the performance of information retrieval but it real world application is limited due to scalability issue scaling to larger document collection via parallelization is an active area of research but most solution require drastic step such a vastly reducing input vocabulary we introduce regularized latent semantic indexing rlsi a new method which is designed for parallelization it is a effective a existing topic model and scale to larger datasets without reducing input vocabulary rlsi formalizes topic modeling a a problem of minimizing a quadratic loss function regularized by l and or l norm this formulation allows the learning process to be decomposed into multiple sub optimization problem which can be optimized in parallel for example via mapreduce we particularly propose adopting l norm on topic and l norm on document representation to create a model with compact and readable topic and useful for retrieval relevance ranking experiment on three trec datasets show that rlsi performs better than lsi plsi and lda and the improvement are sometimes statistically significant experiment on a web dataset containing about million document and million term demonstrate a similar boost in performance on a larger corpus and vocabulary than in previous study 
we estimate that nearly one third of news article contain reference to future event while this information can prove crucial to understanding news story and how event will develop for a given topic there is currently no easy way to access this information we propose a new task to address the problem of retrieving and ranking sentence that contain mention to future event which we call ranking related news prediction in this paper we formally define this task and propose a learning to rank approach based on class of feature term similarity entity based similarity topic similarity and temporal similarity through extensive evaluation using a corpus consisting of million news article and manually judged relevance pair we show that our approach is able to retrieve a significant number of relevant prediction related to a given topic 
topic model have shown great promise in discovering latent semantic structure from complex data corpus ranging from text document and web news article to image video and even biological data in order to deal with massive data collection and dynamic text stream probabilistic online topic model such a online latent dirichlet allocation olda have recently been developed however due to normalization constraint olda can be ineffective in controlling the sparsity of discovered representation a desirable property for learning interpretable semantic pattern especially when the total number of topic is large in contrast sparse topical coding stc ha been successfully introduced a a non probabilistic topic model for effectively discovering sparse latent pattern by using sparsity inducing regularization but unfortunately stc cannot scale to very large datasets or deal with online text stream partly due to it batch learning procedure in this paper we present a sparse online topic model which directly control the sparsity of latent semantic pattern by imposing sparsity inducing regularization and learns the topical dictionary by an online algorithm the online algorithm is efficient and guaranteed to converge extensive empirical result of the sparse online topic model a well a it collapsed and supervised extension on a large scale wikipedia dataset and the medium sized newsgroups dataset demonstrate appealing performance 
for increased efficiency an information retrieval system can split it index into multiple shard and then replicate these shard across many query server for each new query an appropriate replica for each shard must be selected such that the query is answered a quickly a possible typically the replica with the lowest number of queued query is selected however not every query take the same time to execute particularly if a dynamic pruning strategy is applied by each query server hence the replica s queue length is an inaccurate indicator of the workload of a replica and can result in inefficient usage of the replica in this work we propose that improved replica selection can be obtained by using query efficiency prediction to measure the expected workload of a replica experiment are conducted using k query over various number of shard and replica for the large gov collection our result show that query waiting and completion time can be markedly reduced showing that accurate response time prediction can improve scheduling accuracy and attesting the benefit of the proposed scheduling algorithm 
recently a number of algorithm have been proposed to obtain hierarchical structure so called folksonomies from social tagging data work on these algorithm is in part driven by a belief that folksonomies are useful for task such a a navigating social tagging system and b acquiring semantic relationship between tag while the promise and pitfall of the latter have been studied to some extent we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging system this paper set out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies we model exploratory navigation of a tagging system a decentralized search on a network of tag evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used the key idea of our approach is to use hierarchical structure learned by folksonomy algorithm a background knowledge for decentralized search utilizing decentralized search on tag network in combination with different folksonomies a hierarchical background knowledge allows u to evaluate navigational task in social tagging system our experiment with four state of the art folksonomy algorithm on five different social tagging datasets reveal that existing folksonomy algorithm exhibit significant previously undiscovered difference with regard to their utility for navigation our result are relevant for engineer aiming to improve navigability of social tagging system and for scientist aiming to evaluate different folksonomy algorithm from a pragmatic perspective 
at ebay market place listing conversion rate can be measured by number of item sold divided by number of item in a sample set for a given item conversion rate can also be treated a the probability of sale by investigating ebay listing transactional pattern a well a item attribute and user click through data we developed conversion model that allow u to predict a live listing s probability of sale in this paper we discus the design and implementation of such conversion model these model are highly valuable in analysis of inventory quality and ranking our work reveals the uniqueness of sale oriented search at ebay and it similarity to general web search problem 
online evaluation is amongst the few evaluation technique available to the information retrieval community that is guaranteed to reflect how user actually respond to improvement developed by the community broadly speaking online evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural context however it is rarely employed outside of large commercial search engine due primarily to a perception that it is impractical at small scale the goal of this tutorial is to familiarize information retrieval researcher with state of the art technique in evaluating information retrieval system based on natural user clicking behavior a well a to show how such method can be practically deployed in particular our focus will be on demonstrating how the interleaving approach and other click based technique contrast with traditional offline evaluation and how these online method can be effectively used in academic scale research in addition to lecture note we will also provide sample software and code walk throughs to showcase the ease with which interleaving and other click based method can be employed by student academic and other researcher 
to construct a diversified search test collection a set of possible subtopics or intent need to be determined for each topic in one way or another and perintent relevance assessment need to be obtained in the trec web track diversity task subtopics are manually developed at nist based on result of automatic click log analysis in the ntcir intent task intent are determined by manually clustering subtopics string returned by participating system in this study we address the following research question doe the choice of intent for a test collection affect relative performance of diversified search system to this end we use the trec web track diversity task data and the ntcir intent task data which share a set of topic but have different intent set our initial result suggest that the choice of intent may affect relative performance and that this choice may be far more important than how many intent are selected for each topic 
it is common to develop and validate classifier through a process of repeated testing with nested training and or test set of increasing size we demonstrate in this paper that such repeated testing lead to biased estimate of classifier effectiveness experiment on a range of text classification task under three sequential testing framework show all three lead to optimistic estimate of effectiveness we calculate empirical adjustment to unbias estimate on our data set and identify direction for research that could lead to general technique for avoiding bias while reducing labeling cost 
despite the growing popularity of mobile web browsing the energy consumed by a phone browser while surfing the web is poorly understood we present an infrastructure for measuring the precise energy used by a mobile browser to render web page we then measure the energy needed to render financial e commerce email blogging news and social networking site our tool are sufficiently precise to measure the energy needed to render individual web element such a cascade style sheet cs javascript image and plug in object our result show that for popular site downloading and parsing cascade style sheet and javascript consumes a significant fraction of the total energy needed to render the page using the data we collected we make concrete recommendation on how to design web page so a to minimize the energy needed to render the page a an example by modifying script on the wikipedia mobile site we reduced by the energy needed to download and render wikipedia page with no change to the user experience we conclude by estimating the point at which offloading browser computation to a remote proxy can save energy on the phone 
this demonstration paper introduces alf which provides a light weight client side logging application and a server for collecting user interaction data alf ha been designed a a loosely coupled independent service that run in parallel with the ir web application that requires logging 
the information extraction task of named entity recognition ner ha been recently applied to search engine query in order to better understand their semantics here we concentrate on the task prior to the classification of the named entity ne into a set of category which is the problem of detecting candidate ne via the subtask of query segmentation we present a novel method for detecting candidate ne using grammar annotation and query segmentation with the aid of top n snippet from search engine result and a web n gram model to accurately identify ne boundary the proposed method address the problem of accurately setting boundary of ne and the detection of multiple ne in query 
we present an initial study identifying a form of content based grey hat search engine optimization in which a web page contains both potentially relevant content and manipulated content we call such page sham document because they lie in the grey area between ham clearly normal and spam clearly fake sham document are often ranked artificially high in response to certain query but also may contain some useful information and cannot be considered a absolute spam we report a novel annotation effort performed with the clueweb benchmark where page were labeled a being spam sham or legitimate content significant inter annotator agreement rate support the claim that there are sham document that are highly ranked by a very effective retrieval approach yet are not spam we also present an initial study of predictor that may indicate whether a query is the target of shamming 
search query have evolved beyond keyword query many complex query such a verbose query natural language question query and document based query are widely used in a variety of application processing these complex query usually requires a series of query operation which result in multiple sequence of reformulated query however previous query representation either the bag of word method or the recently proposed query distribution method cannot effectively model these query sequence since they ignore the relationship between two query in this paper a reformulation tree framework is proposed to organize multiple sequence of reformulated query a a tree structure where each path of the tree corresponds to a sequence of reformulated query specifically a two level reformulation tree is implemented for verbose query this tree effectively combine two query operation i e subset selection and query substitution within the same framework furthermore a weight estimation approach is proposed to assign weight to each node of the reformulation tree by considering the relationship with other node and directly optimizing retrieval performance experiment on trec collection show that this reformulation tree based representation significantly outperforms the state of the art technique 
the detection and improvement of low quality information is a key concern in web application that are based on user generated content a popular example is the online encyclopedia wikipedia existing research on quality assessment of user generated content deal with the classification a to whether the content is high quality or low quality this paper go one step further it target the prediction of quality flaw this way providing specific indication in which respect low quality content need improvement the prediction is based on user defined cleanup tag which are commonly used in many web application to tag content that ha some shortcoming we apply this approach to the english wikipedia which is the largest and most popular user generated knowledge source on the web we present an automatic mining approach to identify the existing cleanup tag which provides u with a training corpus of labeled wikipedia article we argue that common binary or multiclass classification approach are ineffective for the prediction of quality flaw and hence cast quality flaw prediction a a one class classification problem we develop a quality flaw model and employ a dedicated machine learning approach to predict wikipedia s most important quality flaw since in the wikipedia setting the acquisition of significant test data is intricate we analyze the effect of a biased sample selection in this regard we illustrate the classifier effectiveness a a function of the flaw distribution in order to cope with the unknown real world flaw specific class imbalance the flaw prediction performance is evaluated with wikipedia article that have been tagged with the ten most frequent quality flaw provided test data with little noise four flaw can be detected with a precision close to 
web content increasingly reflects the current state of the physical and social world manifested both in traditional news medium source along with user generated publishing site such a twitter foursquare and facebook at the same time web searching increasingly reflects problem grounded in the real world a a result of this blending of the web with the real world we observe that the web both in it composition and use ha incorporated many of the dynamic of the real world few of the problem associated with searching dynamic collection are well understood such a defining time sensitive relevance understanding user query behavior over time and understanding why certain web content change we believe that just a static collection often benefit from modeling topic dynamic collection will likely benefit from temporal modeling of event and time sensitive user interest and intent which were rarely addressed in the literature there have been preliminary effort in the research and industrial community to address algorithm architecture evaluation methodology and metric we aim to bring together practitioner and researcher to discus their recent breakthrough and the challenge with addressing time aware information access both from the algorithmic and the architectural perspective this workshop is a successor to the successful sigir workshop on time aware information access taia where the edition wa the first to bring together a broad set of academic and industrial researcher around the topic of time aware information access the specific focus of this workshop is on the many time aware benchmarking activity that are ongoing in 
distributed search engine comprise multiple site deployed across geographically distant region each site being specialized to serve the query of local user when a search site cannot accurately compute the result of a query it must forward the query to other site this paper considers the problem of selecting the document indexed by each site focusing on replication to increase the fraction of query processed locally we propose rip an algorithm for replicating document and posting list that is practical and ha two important feature rip evaluates user interest in an online fashion and us only local data of a site being an online approach simplifies the operational complexity while locality enables higher performance when processing query and document the decision procedure on top of being online and local incorporates document popularity and user query which is critical when assuming a replication budget for each site having a replication budget reflects the hardware constraint of any given site we evaluate rip against the approach of replicating popular document statically and show that we achieve significant gain while having the additional benefit of supporting incremental index 
due to the increasing complexity of web application and emerging html standard a large amount of runtime state is created and managed in the user s browser while such complexity is desirable for user experience it make it hard for developer to implement mechanism that provide user ubiquitous access to the data they create during application use this paper present our research into browser session migration for javascript based web application session migration is the act of transferring a session between browser at runtime without burden to developer our system allows user to create a snapshot image that capture all runtime state needed to resume the session elsewhere our system work completely in the javascript layer and thus snapshot can be transfered between different browser vendor and hardware device we report on performance metric of the system using five application four different browser and three different device 
online video chat service such a chatroulette omegle and vchatter that randomly match pair of user in video chat session are fast becoming very popular with over a million user per month in the case of chatroulette a key problem encountered in such system is the presence of flasher and obscene content this problem is especially acute given the presence of underage minor in such system this paper present safevchat a novel solution to the problem of flasher detection that employ an array of image detection algorithm a key contribution of the paper concern how the result of the individual detector are fused together into an overall decision classifying the user a misbehaving or not based on dempster shafer theory the paper introduces a novel motion based skin detection method that achieves significantly higher recall and better precision the proposed method have been evaluated over real world data and image trace obtained from chatroulette com 
classical probabilistic information retrieval ir model e g bm deal with document length based on a trade off between the verbosity hypothesis which assumes the independence of a document s relevance of it length and the scope hypothesis which assumes the opposite despite the effectiveness of the classical probabilistic model the potential relationship between document length and relevance is not fully explored to improve retrieval performance in this paper we conduct an in depth study of this relationship based on the scope hypothesis that document length doe have it impact on relevance we study a list of probability density function and examine which of the density function fit the best to the actual distribution of the document length based on the studied probability density function we propose a length based bm relevance weighting model called bm l which incorporates document length a a substantial weighting factor extensive experiment conducted on standard trec collection show that our proposed bm l markedly outperforms the original bm model even if the latter is optimized 
the world wide web www is a huge information network from which retrieving and organizing quality relevant content remains an open question for mostly all ambiguous query a an example many query have temporal implicit intent associated with them but they are not inferred by search engine inferring the user intention and the period he ha in mind may therefore play an extremely important role in the improvement of the result our work go in this direction we aim to introduce a temporal analysis framework for analyzing document in a temporal dimension in order to identify and understand the temporal nature of any given query namely implicit one our analysis is not based on metadata but on the exploitation of temporal information from the content itself particularly within web snippet which are interesting piece of concentrated information where time clue especially year often appear our intention is to develop a language independent solution and to model the degree of relationship between the term and date identified this is the core part of the framework and the basis for both temporal query understanding and search result exploration such a temporal clustering we believe that inferring this knowledge is a very important step in the process of adding a temporal dimension to ir system thus disambiguating a large class of query for which search engine continue to fail 
mining the latent topic from web search data and capturing their spatiotemporal pattern have many application in information retrieval a web search is heavily influenced by the spatial and temporal factor the latent topic usually demonstrate a variety of spatiotemporal pattern in the face of the diversity of these pattern existing model are increasingly ineffective since they capture only one dimension of the spatiotemporal pattern either the spatial or temporal dimension or simply assume that there exists only one kind of spatiotemporal pattern such oversimplification risk distorting the latent data structure and hindering the downstream usage of the discovered topic in this paper we introduce the spatiotemporal search topic model sstm to discover the latent topic from web search data with capturing their diverse spatiotemporal pattern simultaneously the sstm can flexibly support diverse spatiotemporal pattern and seamlessly integrate the unique feature in web search such a query word url timestamps and search session the sstm is demonstrated a an effective exploratory tool for large scale web search data and it performs superiorly in quantitative comparison to several state of the art topic model 
we propose a novel algorithm for uncovering the colloquial boundary of locally characterizing region present in collection of labeled geospatial data we address the problem by first modeling the data using scale space theory allowing u to represent it simultaneously across different scale a a family of increasingly smoothed density distribution we then derive region boundary by applying localized label weighting and image processing technique to the scale space representation of each label important insight into the data can be acquired by visualizing the shape and size of the resulting boundary for each label at multiple scale we demonstrate our technique operating at scale by discovering the boundary of the most geospatially salient tag associated with a large collection of georeferenced photo from flickr and compare our characterizing region that emerge from the data with those produced by a recent technique from the research literature 
filtering a time ordered corpus for document that are highly relevant to an entity is a task receiving more and more attention over the year one application is to reduce the delay between the moment an information about an entity is being first observed and the moment the entity entry in a knowledge base is being updated current state of the art approach are highly supervised and require training example for each entity monitored we propose an approach which doe not require new training data when processing a new entity to capture intrinsic characteristic of highly relevant document our approach relies on three type of feature document centric feature entity profile related feature and time feature evaluated within the framework of the knowledge base acceleration track at trec it outperforms current state of the art approach 
vast amount of structured information have been published in the semantic web s linked open data lod cloud and their size is still growing rapidly yet access to this information via reasoning and querying is sometimes difficult due to lod s size partial data inconsistency and inherent noisiness machine learning offer an alternative approach to exploiting lod s data with the advantage that machine learning algorithm are typically robust to both noise and data inconsistency and are able to efficiently utilize non deterministic dependency in the data from a machine learning point of view lod is challenging due to it relational nature and it scale here we present an efficient approach to relational learning on lod data based on the factorization of a sparse tensor that scale to data consisting of million of entity hundred of relation and billion of known fact furthermore we show how ontological knowledge can be incorporated in the factorization to improve learning result and how computation can be distributed across multiple node we demonstrate that our approach is able to factorize the yago core ontology and globally predict statement for this large knowledge base using a single dual core desktop computer furthermore we show experimentally that our approach achieves good result in several relational learning task that are relevant to linked data once a factorization ha been computed our model is able to predict efficiently and without any additional training the likelihood of any of the possible triple in the yago core ontology 
identifying similar professional is an important task for many core service in professional social network information about user can be obtained from heterogeneous information source and different source provide different insight on user similarity this paper proposes a discriminative probabilistic model that identifies latent content and graph class for people with similar profile content and social graph similarity pattern and learns a specialized similarity model for each latent class to the best of our knowledge this is the first work on identifying similar professional in professional social network and the first work that identifies latent class to learn a separate similarity model for each latent class experiment on a real world dataset demonstrate the effectiveness of the proposed discriminative learning model 
the web application domain is one of the fastest growing and most wide spread application domain today by utilizing fast modern web browser and advanced scripting technique web developer are developing highly interactive application that can in term of user experience and responsiveness compete with standard desktop application a web application is composed of two equally important part the server side and the client side the client side act a a user interface to the application and can be viewed a a collection of behavior similar behavior are often used in a large number of application and facilitating their reuse offer considerable benefit however due to client side specific such a multi language implementation and extreme dynamicity identifying and extracting code responsible for a certain behavior is difficult in this paper we present a semi automatic method for extracting client side web application code implementing a certain behavior we show how by analyzing the execution of a usage scenario code responsible for a certain behavior can be identified how dependency between different part of the application can be tracked and how in the end only the code responsible for a certain behavior can be extracted our evaluation show that the method is capable of extracting stand alone behavior while achieving considerable saving in term of code size and application performance 
computation in most music retrieval system strongly depend on the size of data compared we propose to enhance performance of a music retrieval system namely a harmonic similarity evaluation method by first indexing relevant part of music piece the indexing algorithm represents each audio piece exclusively by it major repetition using harmonic description and string matching technique evaluation are performed in the context of a state of the art retrieval method namely cover song identification and result highlight the success of our indexing system in keeping similar result while yielding a substantial gain in computation time 
recent trend in public key infrastructure research explore the tradeoff between decreased trust in certificate authority ca resilience against attack communication overhead bandwidth and latency for setting up an ssl tl connection and availability with respect to verifiability of public key information in this paper we propose aki a a new public key validation infrastructure to reduce the level of trust in ca aki integrates an architecture for key revocation of all entity e g ca domain with an architecture for accountability of all infrastructure party through check and balance aki efficiently handle common certification operation and gracefully handle catastrophic event such a domain key loss or compromise we propose aki to make progress towards a public key validation infrastructure with key revocation that reduces trust in any single entity 
this paper exploit web search log for query expansion qe by presenting a new qe method based on path constrained random walk pcrw where the search log are represented a a labeled directed graph and the probability of picking an expansion term for an input query is computed by a learned combination of constrained random walk on the graph the method is shown to be generic in that it cover most of the popular qe model a special case and flexible in that it provides a principled mathematical framework in which a wide variety of information useful for qe can be incorporated in a unified way evaluation is performed on the web document ranking task using a real world data set result show that the pcrw based method is very effective for the expansion of rare query i e low frequency query that are unseen in search log and that it outperforms significantly other state of the art qe meth od 
the standard deviation of score in the top k document of a ranked list ha been shown to be significantly correlated with average precision and ha been the basis of a number of query performance predictor in this paper we outline two hypothesis that aid in understanding this correlation using score distribution sd model with known parameter we create a large number of document ranking using monte carlo simulation to test the validity of these hypothesis 
this paper is concerned with the problem of learning a model to rank object web page ad and etc we propose a framework where the ranking model is both optimized and evaluated using the same information retrieval measure such a normalized discounted cumulative gain ndcg and mean average precision map the main difficulty in direct optimization of ndcg and map is that these measure depend on the rank of object and are not differentiable most learning to rank method that attempt to optimize ndcg or map approximate such measure so that they can be differentiable in this paper we propose a simple yet effective stochastic optimization algorithm to directly minimize any loss function which can be defined on ndcg or map for the learning to rank problem the algorithm employ simulated annealing along with simplex method for it parameter search and find the global optimal parameter experiment result using ndcg annealing algorithm an instance of the proposed algorithm on letor benchmark data set show that the proposed algorithm is both effective and stable when compared to the baseline provided in letor in addition we applied the algorithm for ranking ad in contextual advertising our method ha shown to significantly improve relevance in offline evaluation and business metric in online test in a real large scale advertising serving system to scale our computation we parallelize the algorithm in a mapreduce framework running on hadoop 
in this paper we study the problem of transfer learning from text to image in the context of network data in which link based bridge are available to transfer the knowledge between the different domain the problem of classification of image data is often much more challenging than text data because of the following two reason a labeled text data is very widely available for classification purpose on the other hand this is often not the case for image data in which a lot of image are available from many source but many of them are often not labeled b the image feature are not directly related to semantic concept inherent in class label on the other hand since text data tends to have natural semantic interpretability because of their human origin they are often more directly related to class label therefore the relationship between the image and text feature also provide additional hint for the classification process in term of the image feature transformation which provide the most effective result the semantic challenge of image feature are glaringly evident when we attempt to recognize complex abstract concept and the visual feature often fail to discriminate such concept however the copious availability of bridging relationship between text and image in the context of web and social network data can be used in order to design for effective classifier for image data one of our goal in this paper is to develop a mathematical model for the functional relationship between text and image feature so a indirectly transfer semantic knowledge through feature transformation this feature transformation is accomplished by mapping instance from different domain into a common space of unspecific topic this is used a a bridge to semantically connect the two heterogeneous space this is also helpful for the case where little image data is available for the classification process we evaluate our knowledge transfer technique on an image classification task with labeled text corpus and show the effectiveness with respect to competing algorithm 
bucket testing also known a split testing a b testing or testing is a widely used method for evaluating user satisfaction with new feature product or service in order not to expose the whole user base to the new service the mean user satisfaction rate is estimated by exposing the service only to a few uniformly chosen random user in a recent work backstrom and kleinberg defined the notion of network bucket testing for social service in this context user interaction are only valid for measurement if some minimal number of their friend are also given the service the goal is to estimate the mean user satisfaction rate while providing the service to the least number of user this constraint make uniform sampling which is optimal for the traditional case grossly inefficient in this paper we introduce a simple general framework for designing and evaluating sampling technique for network bucket testing the framework is constructed in a way that sampling algorithm are only required to generate set of user to which the service should be provided given an algorithm the framework produce an unbiased user satisfaction rate estimator and a corresponding variance bound for any network and any user satisfaction function furthermore we present several simple sampling algorithm that are evaluated using both synthetic and real social network our experiment corroborate the theoretical result and demonstrate the effectiveness of the proposed framework and algorithm 
previous paper in ad hoc ir reported that scoring function should satisfy a set of heuristic retrieval constraint providing a mathematical justification for the normalization historically applied to the term frequency tf in this paper we propose a further level of abstraction claiming that the successive normalization are carried out through composition thus we introduce a principled framework that fully explains bm a a variant of tf idf with an inverse order of function composition our experiment over standard datasets indicate that the respective order of composition chosen in the original paper for both tf idf and bm are the most effective one moreover since the order is different between the two model they also demonstrated that the order is instrumental in the design of weighting model in fact while considering more complex scoring function such a bm we discovered a novel weighting model in term of order of composition that consistently outperforms all the rest our contribution here is twofold we provide a unifying mathematical framework for ir and a novel scoring function discovered using this framework 
cross modal retrieval is a classic research topic in multimedia information retrieval the traditional approach study the problem a a pairwise similarity function problem in this paper we consider this problem from a new perspective a a listwise ranking problem and propose a general cross modal ranking algorithm to optimize the listwise ranking loss with a low rank embedding which we call latent semantic cross modal ranking lscmr the latent low rank embedding space is discriminatively learned by structural large margin learning to optimize for certain ranking criterion directly we evaluate lscmr on the wikipedia and nu wide dataset experimental result show that this method obtains significant improvement over the state of the art method 
online service platform osps such a search engine news website ad provider etc serve highly personalized content to the user based on the profile extracted from her history with the osp in this paper we capture osp s personalization for an user in a new data structure called the personalization vector which is a weighted vector over a set of topic and present efficient algorithm to learn it our approach treat osps a black box and extract by mining only their output specifically the personalized for an user and vanilla without any user information content served and the difference in these content we believe that such treatment of osps is a unique aspect of our work not just enabling access to so far hidden profile in osps but also providing a novel and practical approach for retrieving information from osps by mining difference in their output we formulate a new model called latent topic personalization ltp that capture the personalization vector in a learning framework and present efficient inference algorithm for determining it we perform extensive experiment targeting search engine personalization using data from both real google user and synthetic setup our result indicate that ltp achieves high accuracy r pre in discovering personalized topic for google data our qualitative result demonstrate that the topic determined by ltp for a user correspond well to his ad category determined by google 
search engine are continuously looking into method to alleviate user effort in finding desired information for this all major search engine employ query suggestion method to facilitate effective query formulation and reformulation providing high quality query suggestion is a critical task for search engine and so far most research effort have focused on tapping various information available in search query log to identify potential suggestion by relying on this single source of information suggestion providing system often restrict themselves to only previously observed query session therefore a critical challenge faced by query suggestion provision mechanism is that of coverage i e the number of unique query for which user are provided with suggestion while keeping the suggestion quality high to address this problem we propose a novel way of generating suggestion for user search query by moving beyond the dependency on search query log and providing synthetic suggestion for web search query the key challenge in providing synthetic suggestion include identifying important concept in a query and systematically exploring related concept while ensuring that the resulting suggestion are relevant to the user query and of high utility we present an end to end system to generate synthetic suggestion that build upon novel query level operation and combine information available from various textual source we evaluate our suggestion system over a large scale real world dataset of query log and show that our method increase the coverage of query suggestion pair by up to without compromising the quality or the utility of the suggestion 
in this paper we perform an empirical analysis of the cyber criminal ecosystem on twitter essentially through analyzing inner social relationship in the criminal account community we find that criminal account tend to be socially connected forming a small world network we also find that criminal hub sitting in the center of the social graph are more inclined to follow criminal account through analyzing outer social relationship between criminal account and their social friend outside the criminal account community we reveal three category of account that have close friendship with criminal account through these analysis we provide a novel and effective criminal account inference algorithm by exploiting criminal account social relationship and semantic coordination 
current search system are designed to find relevant article especially topically relevant one but the notion of relevance largely depends on search task we study the specific task that scientist are searching for worth reading article beneficial for their research our study find user perception of relevance and preference of reading are only moderately correlated current system can effectively find reading that are highly relevant to the topic but of the worth reading article are only marginally relevant or even non relevant our system can effectively find those worth reading but marginally relevant or non relevant article by taking advantage of scientist recommendation in social website 
the popularity of social medium website like flickr and twitter ha created enormous collection of user generated content online latent in these content collection are observation of the world each photo is a visual snapshot of what the world looked like at a particular point in time and space for example while each tweet is a textual expression of the state of a person and his or her environment aggregating these observation across million of social sharing user could lead to new technique for large scale monitoring of the state of the world and how it is changing over time in this paper we step towards that goal showing that by analyzing the tag and image feature of geo tagged time stamped photo we can measure and quantify the occurrence of ecological phenomenon including ground snow cover snow fall and vegetation density we compare several technique for dealing with the large degree of noise in the dataset and show how machine learning can be used to reduce error caused by misleading tag and ambiguous visual content we evaluate the accuracy of these technique by comparing to ground truth data collected both by surface station and by earth observing satellite besides the immediate application to ecology our study give insight into how to accurately crowd source other type of information from large noisy social sharing datasets 
modern search engine make extensive use of people s contextual information to finesse result ranking using a searcher s location provides an especially strong signal for adjusting result for certain class of query where people may have clear preference for local result without explicitly specifying the location in the query direct ly however if the location estimate is inaccurate or searcher want to obtain many result from a particular location they have limited control on the location focus in the search result returned in this paper we describe a user study that examines the effect of offering searcher more control over how local preference are gathered and used we studied providing user with functionality to offer explicit relevance feedback erf adjacent to result automatically identi fied a location dependent i e more from this location they can use this functionality to indicate whether they are interested in a particular search result and desire more result from that result s location we compared the erf system against a baseline noerf that used the same underlying mechanism to retrieve and rank result but did not offer erf support user performance wa a sessed across experimental participant over location sensitive topic in a fully counter balanced design we found that participant interacted with erf frequently and there were sign that erf ha the potential to improve success rate and lead to more efficient searching for location sensitive search task than noerf 
digital geolinguistic system encourage collaboration between linguist historian archaeologist ethnographer a they explore the relationship between language and cultural adaptation and change in this demo we propose a linked open data approach for increasing the level of interoperability of geolinguistic application and the reuse of the data we present a case study of a geolinguistic project named atlante sintattico d italia syntactic atlas of italy asit 
opinionated social medium such a product review are now widely used by individual and organization for their decision making however due to the reason of profit or fame people try to game the system by opinion spamming e g writing fake review to promote or demote some target product for review to reflect genuine user experience and opinion such spam review should be detected prior work on opinion spam focused on detecting fake review and individual fake reviewer however a fake reviewer group a group of reviewer who work collaboratively to write fake review is even more damaging a they can take total control of the sentiment on the target product due to it size this paper study spam detection in the collaborative setting i e to discover fake reviewer group the proposed method first us a frequent itemset mining method to find a set of candidate group it then us several behavioral model derived from the collusion phenomenon among fake reviewer and relation model based on the relationship among group individual reviewer and product they reviewed to detect fake reviewer group additionally we also built a labeled dataset of fake reviewer group although labeling individual fake review and reviewer is very hard to our surprise labeling fake reviewer group is much easier we also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which make the classic supervised learning approach le effective experimental result show that the proposed method outperforms multiple strong baseline including the state of the art supervised classification regression and learning to rank algorithm 
display advertising is a multi billion dollar industry where advertiser promote their product to user by having publisher display their advertisement on popular web page an important problem in online advertising is how to forecast the number of user visit for a web page during a particular period of time prior research addressed the problem by using traditional time series forecasting technique on historical data of user visit e g via a single regression model built for forecasting based on historical data for all web page and did not fully explore the fact that different type of web page have different pattern of user visit in this paper we propose a probabilistic latent class model to automatically learn the underlying user visit pattern among multiple web page experiment carried out on real world data demonstrate the advantage of using latent class in forecasting online user visit 
large search engine process thousand of query per second over billion of document making query processing a major performance bottleneck an important class of optimization technique called early termination achieves faster query processing by avoiding the scoring of document that are unlikely to be in the top result we study new algorithm for early termination that outperform previous method in particular we focus on safe technique for disjunctive query which return the same result a an exhaustive evaluation over the disjunction of the query term the current state of the art method for this case the wand algorithm by broder et al and the approach of strohman and croft achieve great benefit but still leave a large performance gap between disjunctive and even non early terminated conjunctive query we propose a new set of algorithm by introducing a simple augmented inverted index structure called a block max index essentially this is a structure that store the maximum impact score for each block of a compressed inverted list in uncompressed form thus enabling u to skip large part of the list we show how to integrate this structure into the wand approach leading to considerable performance gain we then describe extension to a layered index organization and to index with reassigned document id that achieve additional gain that narrow the gap between disjunctive and conjunctive top k query processing 
web search engine cache result of frequent and or recent query result caching strategy can be evaluated using different metric hit rate being the most well known recent work take the processing overhead of query into account when evaluating the performance of result caching strategy and propose cost aware caching strategy in this paper we propose a financial cost metric that go one step beyond and take also the hourly electricity price into account when computing the cost we evaluate the most well known static dynamic and hybrid result caching strategy under this new metric moreover we propose a financial cost aware version of the well known lru strategy and show that it outperforms the original lru strategy in term of the financial cost metric 
automated face annotation aim to automatically detect human face from a photo and further name the face with the corresponding human name in this paper we tackle this open problem by investigating a search based face annotation sbfa paradigm for mining large amount of web facial image freely available on the www given a query facial image for annotation the idea of sbfa is to first search for top n similar facial image from a web facial image database and then exploit these top ranked similar facial image and their weak label for naming the query facial image to fully mine those information this paper proposes a novel framework of learning to name face l nf a unified multimodal learning approach for search based face annotation which consists of the following major component i we enhance the weak label of top ranked similar image by exploiting the label smoothness assumption ii we construct the multimodal representation of a facial image by extracting different type of feature iii we optimize the distance measure for each type of feature using distance metric learning technique and finally iv we learn the optimal combination of multiple modality for annotation through a learning to rank scheme we conduct a set of extensive empirical study on two real world facial image database in which encouraging result show that the proposed algorithm significantly boost the naming accuracy of search based face annotation task 
brand and agency use marketing a a tool to influence customer one of the major decision in a marketing plan deal with the allocation of a given budget among medium channel in order to maximize the impact on a set of potential customer a similar situation occurs in a social network where a marketing budget need to be distributed among a set of potential influencers in a way that provides high impact we introduce several probabilistic model to capture the above scenario the common setting of these model consists of a bipartite graph of source and target node the objective is to allocate a fixed budget among the source node to maximize the expected number of influenced target node the concrete way in which source node influence target node depends on the underlying model we primarily consider two model a source side influence model in which a source node that is allocated a budget of k make k independent trial to influence each of it neighboring target node and a target side influence model in which a target node becomes influenced according to a specified rule that depends on the overall budget allocated to it neighbor our main result are an optimal e approximation algorithm for the source side model and several inapproximability result for the target side model establishing that influence maximization in the latter model is provably harder 
we study how to best use crowdsourced relevance judgment learning to rank we integrate two line of prior work unreliable crowd based binary annotation for binary classification and aggregating graded relevance judgment from reliable expert for ranking to model varying performance of the crowd we simulate annotation noise with varying magnitude and distributional property evaluation on three letor test collection reveals a striking trend contrary to prior study single labeling outperforms consensus method in maximizing learner accuracy relative to annotator e ort we also see surprising consistency of the learning curve across noise distribution a well a greater challenge with the adversarial case for multi class labeling 
freshness of result is important in modern web search failing to recognize the temporal aspect of a query can negatively affect the user experience and make the search engine appear stale while freshness and relevance can be closely related for some topic e g news query they are more independent in others e g time insensitive query therefore optimizing one criterion doe not necessarily improve the other and can even do harm in some case we propose a machine learning framework for simultaneously optimizing freshness and relevance in which the trade off is automatically adaptive to query temporal characteristic we start by illustrating different temporal characteristic of query and the feature that can be used for capturing these property we then introduce our supervised framework that leverage the temporal profile of query inferred from pseudo feedback document along with the other ranking feature to improve both freshness and relevance of search result our experiment on a large archival web corpus demonstrate the efficacy of our technique 
learning to rank represents a category of effective ranking method for information retrieval while the primary concern of existing research ha been accuracy learning efficiency is becoming an important issue due to the unprecedented availability of large scale training data and the need for continuous update of ranking function in this paper we investigate parallel learning to rank targeting simultaneous improvement in accuracy and efficiency 
in many situation human judging document relevance are forced to trade off accuracy for speed the development of better interactive retrieval system and relevance assessing platform requires the measurement of assessor accuracy but to date the subjective nature of relevance ha prevented such measurement to quantify assessor performance we define relevance to be a group s majority opinion and demonstrate the value of this approach by comparing the performance of nist assessor to a group of assessor representative of participant in many information retrieval user study using data collected a part of a user study with participant we found that nist assessor discriminate between relevant and non relevant document better than the average participant in our study but that nist assessor true positive rate is no better than that of the study participant in addition we found nist assessor to be conservative in their judgment of relevance compared to the average participant 
the intent oriented search diversification method developed in the field so far tend to build on generative view of the retrieval system to be diversified core algorithm component in particular redundancy assessment are expressed in term of the probability to observe document rather than the probability that the document be relevant this ha been sometimes described a a view considering the selection of a single document in the underlying task model in this paper we propose an alternative formulation of aspect based diversification algorithm which explicitly includes a formal relevance model we develop mean for the effective computation of the new formulation and we test the resulting algorithm empirically we report experiment on search and recommendation task showing competitive or better performance than the original diversification algorithm the relevance based formulation ha further interesting property such a unifying two well known state of the art algorithm into a single version the relevance based approach open alternative possibility for further formal connection and development a natural extension of the framework we illustrate this by modeling tolerance to redundancy a an explicit configurable parameter which can be set to better suit the characteristic of the ir task or the evaluation metric a we illustrate empirically 
collaborative web site such a collaborative encyclopedia blog and forum are characterized by a loose edit control which allows anyone to freely edit their content a a consequence the quality of this content raise much concern to deal with this many site adopt manual quality control mechanism however given their size and change rate manual assessment strategy do not scale and content that is new or unpopular is seldom reviewed this ha a negative impact on the many service provided such a ranking and recommendation to tackle with this problem we propose a learning to rank l r approach for ranking answer in q a forum in particular we adopt an approach based on random forest and represent query and answer pair using eight different group of feature some of these feature are used in the q a domain for the first time our l r method wa trained to learn the answer rating based on the feedback user give to answer in q a forum using the proposed method we were able i to outperform a state of the art baseline with gain of up to in ndcg a metric used to evaluate ranking we also conducted a comprehensive study of the feature showing that ii review and user feature are the most important in the q a domain although text feature are useful for assessing quality of new answer and iii the best set of new feature we proposed wa able to yield the best quality ranking 
we investigate the problem of learning to rank with document retrieval from the perspective of learning for multiple objective function we present solution to two open problem in learning to rank first we show how multiple measure can be combined into a single graded measure that can be learned this solves the problem of learning from a scorecard of measure by making such scorecard comparable and we show result where a standard web relevance measure ndcg is used for the top tier measure and a relevance measure derived from click data is used for the second tier measure the second tier measure is shown to significantly improve while leaving the top tier measure largely unchanged second we note that the learning to rank problem can itself be viewed a changing a the ranking model learns for example early in learning adjusting the rank of all document can be advantageous but later during training it becomes more desirable to concentrate on correcting the top few document for each query we show how an analysis of these problem lead to an improved iteration dependent cost function that interpolates between a cost function that is more appropriate for early learning with one that is more appropriate for late stage learning the approach result in a significant improvement in accuracy with the same size model we investigate these idea using lambdamart a state of the art ranking algorithm 
this paper investigates the influence of pruning feature list to keep a given budget for the evaluation of ranking method we learn from a given training set how important the individual prefix are for the ranking quality based on there importance we choose the best prefix to calculate the ranking while keeping the budget 
ajax becomes more and more important for web application that care about client side user experience it allows sending request asynchronously without blocking client from continuing execution callback function are only executed upon receiving the response while such mechanism make browsing a smooth experience it may cause severe problem in the presence of unexpected network latency due to the non determinism of asynchronism in this paper we demonstrate the possible problem caused by the asynchronism and propose a static program analysis to automatically detect such bug in web application a client side ajax code is often wrapped in server side script we also develop a technique that extract client side javascript code from server side script we evaluate our technique on a number of real world web application our result show that it can effectively identify real bug we also discus possible way to avoid such bug 
cross language information retrieval clir today is dominated by technique that use token to token mapping from bilingual dictionary yet state of the art statistical translation model e g using synchronous context free grammar are far richer capturing multi term phrase term dependency and contextual constraint on translation choice we present a novel clir framework that is able to reach inside the translation black box and exploit these source of evidence experiment on the trec english chinese test collection show this approach to be promising 
query auto completion qac is one of the most prominent feature of modern search engine the list of query candidate is generated according to the prefix entered by the user in the search box and is updated on each new key stroke query prefix tend to be short and ambiguous and existing model mostly rely on the past popularity of matching candidate for ranking however the popularity of certain query may vary drastically across different demographic and user for instance while instagram and imdb have comparable popularity overall and are both legitimate candidate to show for prefix i the former is noticeably more popular among young female user and the latter is more likely to be issued by men in this paper we present a supervised framework for personalizing auto completion ranking we introduce a novel labelling strategy for generating offline training label that can be used for learning personalized ranker we compare the effectiveness of several user specific and demographic based feature and show that among them the user s long term search history and location are the most effective for personalizing auto completion ranker we perform our experiment on the publicly available aol query log and also on the larger scale log of bing the result suggest that supervised ranker enhanced by personalization feature can significantly outperform the existing popularity based base line in term of mean reciprocal rank mrr by up to 
a large amount of research ha focused on faster method for finding top k result in large document collection one of the main scalability challenge for web search engine in this paper we propose a method for accelerating such top k query that build on and generalizes method recently proposed by several group of researcher based on block max index in particular we describe a system that us a new filtering mechanism based on a combination of block maximum and bitmap that radically reduces the number of document that have to be further evaluated our filtering mechanism exploit the simd processing capability of current microprocessor and it is optimized through caching policy that select and store suitable filter structure based on property of the query load our experimental evaluation show that the mechanism result in very significant speed ups for disjunctive top k query under several state of the art algorithm including a speed up of more than a factor of over the fastest previously known method 
understanding a network s temporal evolution appears to require multiple observation of the graph over time these often expensive repeated crawl are only able to answer question about what happened from observation to observation and not what happened before or between network snapshot contrary to this picture we propose a method for twitter s social network that take a single static snapshot of network edge and user account creation time to accurately infer when these edge were formed this method can be exact in theory and we demonstrate empirically for a large subset of twitter relationship that it is accurate to within a few hour in practice we study user who have a very large number of edge or who are recommended by twitter we examine the graph formed by these nearly twitter celebrity and their million edge in detail showing that a single static snapshot can give novel insight about twitter s evolution we conclude from this analysis that real world event and change to twitter s interface for recommending user strongly influence network growth 
a online social medium further integrates deeper into our life we spend more time consuming social update stream that come from our online connection although social update stream provide a tremendous opportunity for u to access information on the fly we often complain about it relevance some of u are flooded with a steady stream of information and simply cannot process it in full ranking the incoming content becomes the only solution for the overwhelmed user for some others in contrast the incoming information stream is pretty weak and they have to actively search for relevant information which is quite tedious for these user augmenting their incoming content flow with relevant information from outside their first degree network would be a viable solution in that case the problem of relevance becomes even more prominent in this paper we start an open discussion on how to build effective system for ranking social update from a unique perspective of linkedin the largest professional network in the world more specifically we address this problem a an intersection of learning to rank collaborative filtering and clickthrough modeling while leveraging idea from information retrieval and recommender system we propose a novel probabilistic latent factor model with regression on explicit feature and compare it with a number of non trivial baseline in addition to demonstrating superior performance of our model we shed some light on the nature of social update on linkedin and how user interact with them which might be applicable to social update stream in general 
nowadays due to the increasing user requirement of efficient and personalized service a perfect travel plan is urgently needed however at present it is hard for people to make a personalized traveling plan most of them follow other people s general travel trajectory so only after finishing their travel do they know which scene is their favorite which is not and what is the perfect order of visit in this research we propose a novel spatio temporal sequence sts searching which mainly includes two step firstly we propose a novel method to detect tourist feature of every scene and it difference in different season secondly combined with personal profile and scene feature a set of interesting scene will be chosen and each scene ha a specific weight for each user the goal of our research is to provide the traveler with the sts which pass through a many chosen scene a possible with the maximum weight and the minimum distance within his travel time we propose a method based on topic model to detect scene feature and provide two approximate algorithm to mine sts a local optimization algorithm and a global optimization algorithm system evaluation have been conducted and the performance result show the efficiency 
social medium provides a new and potentially rich source of information for emergency management service however extracting the relevant information from such stream pose a number of difficult challenge in this short paper we survey emergency management professional to ascertain how social medium is used when responding to incident the search strategy that they undertake and the challenge that they face when using social medium stream this research indicates that emergency management professional employ two main strategy when searching social medium stream keyword centric and account centric search strategy furthermore current search interface are inadequate regarding the requirement of command and control environment in the emergency management domain where the process of information seeking is collaborative in nature and need to support multiple information seeker 
we provide a first large scale analysis of the evolution of query result obtained from a real search engine at two distant point in time namely in and for a set of real query 
web service composition provides a mean of customized and flexible integration of service functionality quality of service qos optimization algorithm select service in order to adapt workflow to the non functional requirement of the user with increasing number of service in a workflow previous approach fail to achieve a sufficient reliability moreover expensive ad hoc replanning is required to deal with service failure the major problem with such sequential application of planning and replanning is that it ignores the potential cost during the initial planning and they consequently are hidden from the decision maker our basic idea to overcome this substantial problem is to compute a qos optimized selection of service cluster that includes a sufficient number of backup service for each service employed to support the human decision maker in the service selection task our approach considers the possible repair cost directly in the initial composition on the basis of a multi objective approach and using a suitable service selection interface the decision maker can select composition in line with his her personal risk preference 
we present a stemming algorithm for text retrieval the algorithm us the statistic collected on the basis of certain corpus analysis based on the co occurrence between two word variant we use a very simple co occurrence measure that reflects how often a pair of word variant occurs in a document a well a in the whole corpus a graph is formed where the word variant are the node and two word variant form an edge if they co occur on the basis of the co occurrence measure a certain edge strength is defined for each of the edge finally on the basis of the edge strength we propose a partition algorithm that group the word variant based on their strongest neighbor that is the neighbor with largest strength our stemming algorithm ha two static parameter and doe not use any other information except the co occurrence statistic from the corpus the experiment on trec clef and fire data consisting of four european and two asian language show a significant improvement over no stem strategy on all the language also the proposed algorithm significantly outperforms a number of strong stemmer including the rule based one on a number of language for highly inflectional language a relative improvement of about is obtained compared to un normalized word and a relative improvement ranging from to is obtained compared to the rule based stemmer for the concerned language 
for over forty year the dominant data structure for ranked document retrieval ha been the inverted index inverted index are effective for a variety of document retrieval task and particularly efficient for large data collection scenario that require disk access and storage however many efficiency bound search task can now easily be supported entirely in memory a a result of recent hardware advance in this paper we present a hybrid algorithmic framework for in memory bag of word ranked document retrieval using a self index derived from the fm index wavelet tree and the compressed suffix tree data structure and evaluate the various algorithmic trade offs for performing efficient query entirely in memory we compare our approach with two classic approach to bag of word query using inverted index term at a time taat and document at a time daat query processing we show that our framework is competitive with state of the art indexing structure and describe new capability provided by our algorithm that can be leveraged by future system to improve effectiveness and efficiency for a variety of fundamental search operation 
web search query are often ambiguous or multi faceted which make a simple ranked list of result inadequate to assist information finding for such faceted query we explore a technique that explicitly represents interesting facet of a query using group of semantically related term extracted from search result a an example for the query baggage allowance these group might be different airline different flight type domestic international or different travel class first business economy we name these group query facet and the term in these group facet term we develop a supervised approach based on a graphical model to recognize query facet from the noisy candidate found the graphical model learns how likely a candidate term is to be a facet term a well a how likely two term are to be grouped together in a query facet and capture the dependency between the two factor we propose two algorithm for approximate inference on the graphical model since exact inference is intractable our evaluation combine recall and precision of the facet term with the grouping quality experimental result on a sample of web query show that the supervised method significantly outperforms existing approach which are mostly unsupervised suggesting that query facet extraction can be effectively learned 
elsevier sigir application challenge is an international competition that encourages software developer to create application that run on elsevier s sciverse platform the challenge is open to all sigir conference participant 
recently twitter ha emerged a a popular platform for discovering real time information on the web such a news story and people s reaction to them like the web twitter ha become a target for link farming where user especially spammer try to acquire large number of follower link in the social network acquiring follower not only increase the size of a user s direct audience but also contributes to the perceived influence of the user which in turn impact the ranking of the user s tweet by search engine in this paper we first investigate link farming in the twitter network and then explore mechanism to discourage the activity to this end we conducted a detailed analysis of link acquired by over spammer account suspended by twitter we find that link farming is wide spread and that a majority of spammer link are farmed from a small fraction of twitter user the social capitalist who are themselves seeking to amass social capital and link by following back anyone who follows them our finding shed light on the social dynamic that are at the root of the link farming problem in twitter network and they have important implication for future design of link spam defense in particular we show that a simple user ranking scheme that penalizes user for connecting to spammer can effectively address the problem by disincentivizing user from linking with other user simply to gain influence 
there is a rising concern among parent who have experienced unreliable content maturity rating for mobile application apps that result in inappropriate risk exposure for their child and adolescent in reality there is no consistent maturity rating policy for mobile application the maturity rating of android apps are provided purely by developer self disclosure and are rarely verified while apple s io app rating are considered to be more accurate they can also be inconsistent with apple s published policy to address these issue this research aim to systematically uncover the extent and severity of unreliable maturity rating for mobile apps specifically we develop mechanism to verify the maturity rating of mobile apps and investigate possible reason behind the incorrect rating we believe that our finding have important implication for platform provider e g google or apple a well a for regulatory body and application developer 
the field of entity search using semantic web rdf data ha gained more interest recently in this paper we propose a probabilistic entity retrieval model for rdf graph using path in the graph unlike previous work which assumes that all description of an entity are directly linked to the entity node we assume that an entity can be described with any node that can be reached from the entity node by following path in the rdf graph our retrieval model simulates the generation process of query term from an entity node by traversing the graph we evaluate our approach using a standard evaluation framework for entity search 
this paper present a different perspective on diversity in search result diversity by proportionality we consider a result list most diverse with respect to some set of topic related to the query when the number of document it provides on each topic is proportional to the topic s popularity consequently we propose a framework for optimizing proportionality for search result diversification which is motivated by the problem of assigning seat to member of competing political party our technique iteratively determines for each position in the result ranked list the topic that best maintains the overall proportionality it then selects the best document on this topic for this position we demonstrate empirically that our method significantly outperforms the top performing approach in the literature not only on our proposed metric for proportionality but also on several standard diversity measure this result indicates that promoting proportionality naturally lead to minimal redundancy which is a goal of the current diversity approach 
social recommendation problem have drawn a lot of attention recently due to the prevalence of social networking site the experiment in previous literature suggest that social information is very effective in improving traditional recommendation algorithm however explicit social information is not always available in most of the recommender system which limit the impact of social recommendation technique in this paper we study the following two research problem in some system without explicit social information can we still improve recommender system using implicit social information in the system with explicit social information can the performance of using implicit social information outperform that of using explicit social information in order to answer these two question we conduct comprehensive experimental analysis on three recommendation datasets the result indicates that implicit user and item social information including similar and dissimilar relationship can be employed to improve traditional recommendation method when comparing implicit social information with explicit social information the performance of using implicit information is slightly worse this study provides additional insight to social recommendation technique and also greatly widens the utility and spread the impact of previous and upcoming social recommendation approach 
learning to adapt in a new setting is a common challenge to our knowledge and capability new life would be easier if we actively pursued supervision from the right mentor chosen with our relevant but limited prior knowledge this variant principle of active learning seems intuitively useful to many domain adaptation problem in this paper we substantiate it power for advancing automatic ranking adaptation which is important in web search since it s prohibitive to gather enough labeled data for every search domain for fully training domain specific ranker for the cost effectiveness it is expected that only those most informative instance in target domain are collected to annotate while we can still utilize the abundant ranking knowledge in source domain we propose a unified ranking framework to mutually reinforce the active selection of informative target domain query and the appropriate weighting of source training data a related prior knowledge we select to annotate those target query whose document order most disagrees among the member of a committee built on the mixture of source training data and the already selected target data then the replenished labeled set is used to adjust the importance of source query for enhancing their rank transfer this procedure iterates until labeling budget exhaust based on letor and yahoo learning to rank challenge data set our approach significantly outperforms the random query annotation commonly used in ranking adaptation and the active rank learner on target domain data only 
user generated text such a review comment or discussion are valuable indicator of user preference unlike previous work which focus on labeled data from user contributed review we focus here on user comment which are not accompanied by explicit rating label we investigate their utility for a one class collaborative filtering task such a bookmarking where only the user action are given a ground truth we propose a sentiment aware nearest neighbor model sann for multimedia recommendation over ted talk which make use of user comment the model outperforms significantly by more than on unseen data several competitive baseline 
gradient boosted regression tree gbrt are the current state of the art learning paradigm for machine learned web search ranking a domain notorious for very large data set in this paper we propose a novel method for parallelizing the training of gbrt our technique parallelizes the construction of the individual regression tree and operates using the master worker paradigm a follows the data are partitioned among the worker at each iteration the worker summarizes it data partition using histogram the master processor us these to build one layer of a regression tree and then sends this layer to the worker allowing the worker to build histogram for the next layer our algorithm carefully orchestrates overlap between communication and computation to achieve good performance since this approach is based on data partitioning and requires a small amount of communication it generalizes to distributed and shared memory machine a well a cloud we present experimental result on both shared memory machine and cluster for two large scale web search ranking data set we demonstrate that the loss in accuracy induced due to the histogram approximation in the regression tree creation can be compensated for through slightly deeper tree a a result we see no significant loss in accuracy on the yahoo data set and a very small reduction in accuracy for the microsoft letor data in addition on shared memory machine we obtain almost perfect linear speed up with up to about core on the large data set on distributed memory machine we get a speedup of with processor due to data partitioning our approach can scale to even larger data set on which one can reasonably expect even higher speedup 
while a user s preference is directly reflected in the interactive choice process between her and the recommender this wealth of information wa not fully exploited for learning recommender model in particular existing collaborative filtering cf approach take into account only the binary event of user action but totally disregard the context in which user decision are made in this paper we propose collaborative competitive filtering ccf a framework for learning user preference by modeling the choice process in recommender system ccf employ a multiplicative latent factor model to characterize the dyadic utility function but unlike cf ccf model the user behavior of choice by encoding a local competition effect in this way ccf allows u to leverage dyadic data that wa previously lumped together with missing data in existing cf model we present two formulation and an efficient large scale optimization algorithm experiment on three real world recommendation data set demonstrate that ccf significantly outperforms standard cf approach in both offline and online evaluation 
community based question answering site such a yahoo answer or baidu zhidao allow user to get answer to complex detailed and personal question from other user however since answering a question depends on the ability and willingness of user to address the asker s need a significant fraction of the question remain unanswered we measured that in yahoo answer this fraction represents of all incoming english question at the same time we discovered that around of question in certain category are recurrent at least at the question title level over a period of one year we attempt to reduce the rate of unanswered question in yahoo answer by reusing the large repository of past resolved question openly available on the site more specifically we estimate the probability whether certain new question can be satisfactorily answered by a best answer from the past using a statistical model specifically trained for this task we leverage concept and method from query performance prediction and natural language processing in order to extract a wide range of feature for our model the key challenge here is to achieve a level of quality similar to the one provided by the best human answerer we evaluated our algorithm on offline data extracted from yahoo answer but more interestingly also on online data by using three live answering robot that automatically provide past answer to new question when a certain degree of confidence is reached we report the success rate of these robot in three active yahoo answer category in term of both accuracy coverage and asker satisfaction this work present a first attempt to the best of our knowledge of automatic question answering to question of social nature by reusing past answer of high quality 
automatic classification with graph containing annotated edge is an interesting problem and ha many potential application we present a risk minimization formulation that exploit the annotated edge for classification task one major advantage of our approach compared to other method is that the weight of each edge in the graph structure in our model including both positive and negative weight can be learned automatically from training data based on edge feature the empirical result show that our approach can lead to significantly improved classification performance compared to several baseline approach 
in current commercial web search engine query are processed in the conjunctive mode which requires the search engine to compute the intersection of a number of posting list to determine the document matching all query term in practice the intersection operation take a significant fraction of the query processing time for some query dominating the total query latency hence efficient posting list intersection is critical for achieving short query latency in this work we focus on improving the performance of posting list intersection by leveraging the compute capability of recent multicore system to this end we consider various coarse grained and fine grained parallelization model for list intersection specifically we present an algorithm that partition the work associated with a given query into a number of small and independent task that are subsequently processed in parallel through a detailed empirical analysis of these alternative model we demonstrate that exploiting parallelism at the finest level of granularity is critical to achieve the best performance on multicore system on an eight core system the fine grained parallelization method is able to achieve more than five time reduction in average query processing time while still exploiting the parallelism for high query throughput 
mobile search is still in infancy compared with general purpose web search with limited training data and weak relevance feature the ranking performance in mobile search is far from satisfactory to address this problem we propose to leverage the knowledge of web search to enhance the ranking of mobile search in this paper we first develop an equivalent page conversion between web search and mobile search then we design a few novel ranking feature generated from the click through data in web search for estimating the relevance of mobile search large scale evaluation demonstrate that the knowledge from web search is quite effective for boosting the relevance of ranking on mobile search 
there ha been considerable interest in incorporating diversity in search result to account for redundancy and the space of possible user need most work on this problem is based on subtopics diversity ranker score document against a set of hypothesized subtopics and diversity ranking are evaluated by assigning a value to each ranked document based on the number of novel and redundant subtopics it is relevant to this can be seen a modeling a user who is always interested in seeing more novel subtopics with progressively decreasing interest in seeing the same subtopic multiple time we put this model to test if it is correct then user when given a choice should prefer to see a document that ha more value to the evaluation we formulate some specific hypothesis from this model and test them with actual user in a novel preference based design in which user express a preference for document a or document b given document c we argue that while the user study show the subtopic model is good there are many other factor apart from novelty and redundancy that may be influencing user preference from this we introduce a new framework to construct an ideal diversity ranking using only preference judgment with no explicit subtopic judgment whatsoever 
pseudo relevance feedback ha proven effective for improving the average retrieval performance unfortunately many experiment have shown that although pseudo relevance feedback help many query it also often hurt many other query limiting it usefulness in real retrieval application thus an important yet difficult challenge is to improve the overall effectiveness of pseudo relevance feedback without sacrificing the performance of individual query too much in this paper we propose a novel learning algorithm feedbackboost based on the boosting framework to improve pseudo relevance feedback through optimizing the combination of a set of basis feedback algorithm using a loss function defined to directly measure both robustness and effectiveness feedbackboost can potentially accommodate many basis feedback method a feature in the model making the proposed method a general optimization framework for pseudo relevance feedback a an application we apply feedbackboost to improve pseudo feedback based on language model through combining different document weighting strategy the experiment result demonstrate that feedbackboost can achieve better average precision and meanwhile dramatically reduce the number and magnitude of feedback failure a compared to three representative pseudo feedback method and a standard learning to rank approach for pseudo feedback 
we investigate the impact of query result prefetching on the efficiency and effectiveness of web search engine we propose offline and online strategy for selecting and ordering query whose result are to be prefetched the offline strategy rely on query log analysis and the query are selected from the query issued on the previous day the online strategy select the query from the result cache relying on a machine learning model that estimate the arrival time of query we carefully evaluate the proposed prefetching technique via simulation on a query log obtained from yahoo web search we demonstrate that our strategy are able to improve various performance metric including the hit rate query response time result freshness and query degradation rate relative to a state of the art baseline 
a typical problem for a search engine hosting sponsored search service is to provide the advertiser with a forecast of the number of impression his her ad is likely to obtain for a given bid accurate forecast have high business value since they enable advertiser to select bid that lead to better return on their investment they also play an important role in service such a automatic campaign optimization despite it importance the problem ha remained relatively unexplored in literature existing method typically overfit to the training data leading to inconsistent performance furthermore some of the existing method cannot provide prediction for new ad i e for ad that are not present in the log in this paper we develop a generative model based approach that address these drawback we design a bayes net to capture inter dependency between the query traffic feature and the competitor in an auction furthermore we account for variability in the volume of query traffic by using a dynamic linear model finally we implement our approach on a production grade mapreduce framework and conduct extensive large scale experiment on substantial volume of sponsored search data from bing our experimental result demonstrate significant advantage over existing method a measured using several accuracy error criterion improved ability to provide estimate for new ad and more consistent performance with smaller variance in accuracy our method can also be adapted to several other related forecasting problem such a predicting average position of ad or the number of click under budget constraint 
automatic image annotation play a critical role in keyword based image retrieval system recently the nearest neighbor based scheme ha been proposed and achieved good performance for image annotation given a new image the scheme is to first find it most similar neighbor from labeled image and then propagate the keywords associated with the neighbor to it many study focused on designing a suitable distance metric between image so that all labeled image can be ranked by their distance to the given image however higher accuracy in distance prediction doe not necessarily lead to better ordering of labeled image in this paper we propose a ranking oriented neighbor search mechanism to rank labeled image directly without going through the intermediate step of distance prediction in particular a new learning to rank algorithm is developed which exploit the implicit preference information of labeled image and underline the accuracy of the top ranked result experiment on two benchmark datasets demonstrate the effectiveness of our approach for image annotation 
most information retrieval ir software is designed to fit a general user where user are submitting query and the retrieval system return a ranked list of result regardless of the user the query always return the same list of result individual aspect like age gender profession or experience are often not taken into account for example the difference in searching between child and adult although long challenged by work such a bates berrypicking model common system still assume that the user ha a static information need which remains unchanged during the seeking process moreover many system are strongly optimized for lookup search expecting that the user is only interested in fact and not in complex problem solving but in many everyday situation people search for information to gain knowledge which allows them to fulfill a specific work task e g like answering research question investigating for a publication or thesis comparing different product or learning a language such complex task can be divided into sub task and generally include multiple exploratory search session in which the user strongly interacts with the system this is a longitudinal process where the searcher necessarily gather collect aggregate interprets process and evaluates information object from one or more source in such complex search scenario all three activity lookup learn and investigate are used in conjunction with one another to bridge the user knowledge gap in each step of this process the user face a new situation in which knowledge and information need change this inuences the relevance of information object and may direct the user to different topic domain or also task the goal of this research is to effectively assist at fulfilling complex work task consisting of multi session exploratory search activity to achieve this information retrieval need personalization and ha to close the gap between the different search session this can be done by enabling the user to collect information object into a personal reference library and visualizing past search activity in a kind of breadcrumb or time line thinking one step further a personalized ir system pir ha to adapt to relevant factor and commit itself to the specific user and the personal search behavior this mean the system need to guide the user through the searching process suggesting useful search action like effective search strategy or query formulation and ha to recommend information object relevant to the work task and the user current situation thereby the system ha to be aware of the user and specific contextual circumstance general information about the user like gender or age can be fetched explicitly allowing to adapt in a more coarse grained way i e decide the way of presenting result based on the user group moreover integrating used application or providing other way to let the user explicitly manage task will help to understand the goal of the user search activity and will provide much better way of user assistance to close the gap between user and system both behavioral and contextual information are necessary information about the search behavior and indirectly the user knowledge and expertise can be conveyed by logging e g query log and examining system interaction the fetched data should be made transparent to the user showing what kind of information ha been gathered so far the implicit information ha to be refined with other contextual information collected implicitly from different interface or sensor e g time location and explicitly by direct user input from e g relevance feedback interaction this will allow a more fine grained way of system adaption and offer new option in assisting the user during the long term search activity showing personalized search strategy and possible next step appropriate to the information need and level of experience 
in many instance offensive comment on the internet attract a disproportionate number of positive rating from highly biased user this result in an undesirable scenario where these offensive comment are the top rated one in this paper we develop semi supervised learning technique to correct the bias in user rating of comment our scheme us a small number of comment label in conjunction with user rating information to iteratively compute user bias and unbiased rating for unlabeled comment we show that the running time of each iteration is linear in the number of rating and the system converges to a unique fixed point to select the comment to label we devise an active learning algorithm based on empirical risk minimization our active learning method incrementally update the risk for neighboring comment each time a comment is labeled and thus can easily scale to large comment datasets on real life comment from yahoo news our semi supervised and active learning algorithm achieve higher accuracy than simple baseline with few labeled example 
the real time information on news site blog and social networking site change dynamically and spread rapidly through the web developing method for handling such information at a massive scale requires that we think about how information content varies over time how it is transmitted and how it mutates a it spread we describe the news information flow tracking yay nifty system for large scale real time tracking of meme short textual phrase that travel and mutate through the web nifty is based on a novel highly scalable incremental meme clustering algorithm that efficiently extract and identifies mutational variant of a single meme nifty run order of magnitude faster than our previous memetracker system while also maintaining better consistency and quality of extracted meme we demonstrate the effectiveness of our approach by processing a terabyte dataset of billion blog post and news article that we have been continuously collecting for the last four year nifty extracted billion unique textual phrase and identified more than million meme our meme tracking algorithm wa able to process the entire dataset in le than five day using a single machine furthermore we also provide a live deployment of the nifty system that allows user to explore the dynamic of online news in near real time 
test collection are the primary driver of progress in information retrieval they provide yardstick for assessing the effectiveness of ranking function in an automatic rapid and repeatable fashion and serve a training data for learning to rank model however manual construction of test collection tends to be slow labor intensive and expensive this paper examines the feasibility of constructing web search test collection in a completely unsupervised manner given only a large web corpus a input within our proposed framework anchor text extracted from the web graph is treated a a pseudo query log from which pseudo query are sampled for each pseudo query a set of relevant and non relevant document are selected using a variety of web specific feature including spam and aggregated anchor text weight the automatically mined query and judgment form a pseudo test collection that can be used for training ranking function experiment carried out on trec web track data show that learning to rank model trained using pseudo test collection outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgment demonstrating the usefulness of our approach in extracting reasonable quality training data for free 
vibrant online community are in constant flux a member join and depart the interactional norm evolve stimulating further change to the membership and it social dynamic linguistic change in the sense of innovation that becomes accepted a the norm is essential to this dynamic process it both facilitates individual expression and foster the emergence of a collective identity we propose a framework for tracking linguistic change a it happens and for understanding how specific user react to these evolving norm by applying this framework to two large online community we show that user follow a determined two stage lifecycle with respect to their susceptibility to linguistic change a linguistically innovative learning phase in which user adopt the language of the community followed by a conservative phase in which user stop changing and the evolving community norm pas them by building on this observation we show how this framework can be used to detect early in a user s career how long she will stay active in the community thus this work ha practical significance for those who design and maintain online community it also yield new theoretical insight into the evolution of linguistic norm and the complex interplay between community level and individual level linguistic change 
question recommendation that automatically recommends a new question to suitable user to answer is an appealing and challenging problem in the research area of community question answering cqa unlike in general recommender system where a user ha only a single role each user in cqa can play two different role dual role simultaneously a an asker and a an answerer to the best of our knowledge this paper is the first to systematically investigate the distinction between the two role and their different influence on the performance of question recommendation in cqa moreover we propose a dual role model drm to model the dual role of user effectively with different indepen dence assumption two variant of drm are achieved finally we present the drm based approach to question recommendation which provides a mechanism for naturally integrating the user relation between the answerer and the asker with the content re levance between the answerer and the question into a uni fied probabilistic framework experiment using a real world data crawled from yahoo answer show that there are evident distinction between the two role of user in cqa additionally the answerer role is more effective than the asker role for modeling candidate user in question recommendation compared with baseline utilizing a single role or blended role based method our drm based approach consistently and significantly improves the performance of question recommendation demonstrating that our approach can model the user in cqa more reasonably and precisely 
in performance based display advertising campaign effectiveness is often measured in term of conversion that represent some desired user action like purchase and product information request on advertiser website hence identifying and targeting potential converter is of vital importance to boost campaign performance this is often accomplished by marketer who define the user base of campaign based on behavioral demographic search social purchase and other characteristic such a process is manual and subjective it often fails to utilize the full potential of targeting in this paper we show that by using past converted user of campaign and campaign meta data e g ad creatives landing page we can combine disparate user information in a principled way to effectively and automatically target converter for new existing campaign at the heart of our approach is a factor model that estimate the affinity of each user feature to a campaign using historical conversion data in fact our approach allows building a conversion model for a brand new campaign through campaign meta data alone and hence target potential converter even before the campaign is run through extensive experiment we show the superiority of our factor model approach relative to several other baseline moreover we show that the performance of our approach at the beginning of a campaign s life is typically better than the other model even when they are trained using all conversion data after the campaign ha completed this clearly show the importance and value of using historical campaign data in constructing an effective audience selection strategy for display advertising 
we develop and discus a news comment miner that present distinct viewpoint on a given theme or event given a query the system us metasearch technique to find relevant news article relevant article are then scraped for both article content and comment snippet from the comment are sampled and presented to the user based on theme popularity and contrastiveness to previously selected snippet the system design focus on being quicker and more lightweight than recent topic modelling approach while still focusing on selecting orthogonal snippet 
book only represented by brief metadata book record are particularly hard to retrieve one way of improving their retrieval is by extracting retrieval enhancing feature from them this work focus on scientific physic book record we ask if their technical terminology can be used a a retrieval enhancing feature a study of book record show a strong correlation between their technical terminology and their likelihood of relevance using this finding for retrieval yield precision and recall gain 
based on a new framework for capturing dynamic area of interest in eye tracking we model the user search process a a markov chain the analysis indicates possible system improvement and yield parameter estimate for the interactive probability ranking principle iprp 
entity extraction and sentiment classification are among the most common type of information derived from document but the problem of directly associating entity and sentiment ha received le attention we use textrank on a graph linking entity and sentiment laden word and phrase we extract from the resulting eigenvector the final sentiment weight of the entity we then explore the algorithm s performance and accuracy compared to a baseline 
there is little information from independent source in the public domain about mobile malware infection rate the only previous independent estimate wa based on indirect measurement obtained from domain name resolution trace in this paper we present the first independent study of malware infection rate and associated risk factor using data collected directly from over android device we find that the malware infection rate in android device estimated using two malware datasets and though small are significantly higher than the previous independent estimate based on the hypothesis that some application store have a greater density of malicious application and that advertising within application and cross promotional deal may act a infection vector we investigate whether the set of application used on a device can serve a an indicator for infection of that device our analysis indicates that while not an accurate indicator of infection by itself the application set doe serve a an inexpensive method for identifying the pool of device on which more expensive monitoring and analysis mechanism should be deployed using our two malware datasets we show that this indicator performs up to about five time better at identifying infected device than the baseline of random check such indicator can be used for example in the search for new or previously undetected malware it is therefore a technique that can complement standard malware scanning our analysis also demonstrates a marginally significant difference in battery use between infected and clean device 
automatic term extraction ate aim at extracting domain specific term from a corpus of a certain domain termhood is one essential measure for judging whether a phrase is a term previous research on termhood mainly depend on the word frequency information in this paper we propose to compute termhood based on semantic representation of word a novel topic model namely i swb is developed to map the domain corpus into a latent semantic space which is composed of some general topic a background topic and a document specific topic experiment on four domain demonstrate that our approach outperforms the state of the art ate approach 
in this paper we study the problem of expanding a set of given seed entity into a more complete set by discovering other entity that also belong to the same concept set a typical example is to use canon and nikon a seed entity and derive other entity e g olympus in the same concept set of camera brand in order to discover such relevant entity we exploit several web data source including list extracted from web page and user query from a web search engine while these web data are highly diverse with rich information that usually cover a wide range of the domain of interest they tend to be very noisy we observe that previously proposed random walk based approach do not perform very well on these noisy data source accordingly we propose a new general framework based on iterative similarity aggregation and present detailed experimental result to show that when using general purpose web data for set expansion our approach outperforms previous technique in term of both precision and recall 
information about how to segment a web page can be used nowadays by application such a segment aware web search classification and link analysis in this research we propose a fully automatic method for page segmentation and evaluate it application through experiment with four separate web site while the method may be used in other application our main focus in this article is to use it a input to segment aware web search system our result indicate that the proposed method produce better segmentation result when compared to the best segmentation method we found in literature further when applied a input to a segment aware web search method it produce result close to those produced when using a manual page segmentation method 
hundred of million of people today rely on web based search engine to satisfy their information need in order to meet the expectation of this vast and diverse user population the search engine should present a list of result such that the probability of satisfying the average user is maximized this lead u to the problem of search result diversification given a user submitted query the search engine should include result that are relevant to the user query and at the same time diverse enough to meet the expectation of diverse user population however it is not clear in what respect the result should be diversified much of the current work in diversity focus on ambiguous and underspecified query and try to include result corresponding to diverse interpretation of the ambiguous query this is not always sufficient my analysis of a commercial web search engine s log reveals that even for well specified informational query click entropy is very high indicating that different user prefer different type of document very recently a diversification algorithm fine tuned for such informational query ha been proposed further high click entropy were also observed for a large fraction of transactional query one major goal of my phd thesis will then be to identify the various possible dimension along which the search result can be diversified having such an information will enhance our understanding about the expectation of an average user from the search engine by utilizing aggregate statistic about query user and their interaction with the search engine for different query more concrete evidence about diverse user preference a well a relative importance of different diversity dimension can be derived once we know different diversity dimension the next natural question is given a query how can we determine the diversification requirement best suited for the query for some query sub topic coverage may be more important while for others diversification with respect to document source or stylistics might be important this problem is related to the problem of selective diversification where the goal is to identify query for which diversification technique should be used however in addition we are also interested in identifying different diversity class a given query belongs to further for some query it may be required to diversify along multiple diversity dimension in such case it is also important to determine the relative importance of different diversity dimension for the given query by utilizing past user interaction data query level feature like query clarity entropy lexical feature etc and document level feature e g popularity content quality previous click history etc classifier for diversification requirement can be developed given a user query once we know the type of diversity requirement for the user an appropriate diversification technique is required i would like to study the problem of simultaneously diversifying search result along multiple dimension a discussed above one possible way here could be to build upon the nugget based framework introduced by clarke et al where we represent each document a a set of nugget each nugget corresponding to a diversity dimension 
million of post are being generated in real time by user in social networking service such a twitter however a considerable number of those post are mundane post that are of interest to the author and possibly their friend only this paper investigates the problem of automatically discovering valuable post that may be of potential interest to a wider audience specifically we model the structure of twitter a a graph consisting of user and post a node and retweet relation between the node a edge we propose a variant of the hit algorithm for producing a static ranking of post experimental result on real world data demonstrate that our method can achieve better performance than several baseline method 
in the era of social commerce user often connect from e commerce website to social networking venue such a facebook and twitter however there have been few effort on understanding the correlation between user social medium profile and their e commerce behavior this paper present a system for predicting a user s purchase behavior on e commerce website from the user s social medium profile we specifically aim at understanding if the user s profile information in a social network for example facebook can be leveraged to predict what category of product the user will buy from for example ebay electronics the paper provides an extensive analysis on how user facebook profile information correlate to purchase on ebay and analyzes the performance of different feature set and learning algorithm on the task of purchase behavior prediction 
when searching a temporal document collection e g news archive or blog the time dimension must be explicitly incorporated into a retrieval model in order to improve relevance ranking previous work ha followed one of two main approach a mixture model linearly combining textual similarity and temporal similarity or a probabilistic model generating a query from the textual and temporal part of a document independently in this paper we compare the effectiveness of different time aware ranking method by using a mixture model applied to all method extensive evaluation is conducted using the new york time annotated corpus query and relevance judgment obtained using the amazon mechanical turk 
auction are widely used on the web application range from internet advertising to platform such a ebay in most of these application the auction in use are single multi item auction with unit demand the main drawback of standard mechanism for this type of auction such a vcg and gsp is the limited expressiveness that they offer to the bidder the general auction mechanism gam of is taking a first step towards addressing the problem of limited expressiveness by computing a bidder optimal envy free outcome for linear utility function with identical slope and a single discontinuity per bidder item pair we show that in many practical situation this doe not suffice to adequately model the preference of the bidder and we overcome this problem by presenting the first mechanism for piece wise linear utility function with non identical slope and multiple discontinuity our mechanism run in polynomial time like gam it is incentive compatible for input that fulfill a certain non degeneracy requirement but our requirement is more general than the requirement of gam for discontinuous utility function that are non degenerate a well a for continuous utility function the outcome of our mechanism is a competitive equilibrium we also show how our mechanism can be used to compute approximately bidder optimal envy free outcome for a general class of continuous utility function via piece wise linear approximation finally we prove hardness result for even more expressive setting 
with the explosive growth of online news readership recommending interesting news article to user ha become extremely important while existing web service such a yahoo and digg attract user initial click by leveraging various kind of signal how to engage such user algorithmically after their initial visit is largely under explored in this paper we study the problem of post click news recommendation given that a user ha perused a current news article our idea is to automatically identify related news article which the user would like to read afterwards specifically we propose to characterize relatedness between news article across four aspect relevance novelty connection clarity and transition smoothness motivated by this understanding we define a set of feature to capture each of these aspect and put forward a learning approach to model relatedness in order to quantitatively evaluate our proposed measure and learn a unified relatedness function we construct a large test collection based on a four month commercial news corpus with editorial judgment the experimental result show that the proposed heuristic can indeed capture relatedness and that the learned unified relatedness function work quite effectively 
query suggestion is a useful tool to help user express their information need by supplying alternative query when evaluating the effectiveness of query suggestion algorithm many previous study focus on measuring whether a suggestion query is relevant or not to the input query this assessment criterion is too simple to describe user requirement in this paper we introduce two scenario of query suggestion the first scenario represents case where the search result of the input query is unsatisfactory the second scenario represents case where the search result is satisfactory but the user may be looking for alternative solution based on the two scenario we propose two assessment criterion our labeling result indicate that the new assessment criterion provide finer distinction among query suggestion than the traditional relevance based criterion 
search engine can improve their efficiency by selecting only few promising shard for each query state of the art shard selection algorithm first query a central index of sampled document and their effectiveness is similar to searching all shard however the search in the central index also hurt efficiency additionally we show that the effectiveness of these approach varies substantially with the sampled document this paper proposes taily a novel shard selection algorithm that model a query s score distribution in each shard a a gamma distribution and selects shard with highly scored document in the tail of the distribution taily estimate the parameter of score distribution based on the mean and variance of the score function s feature in the collection and shard because taily operates on term statistic instead of document sample it is efficient and ha deterministic effectiveness experiment on large web collection gov clueweba and cluewebb show that taily achieves similar effectiveness to sample based approach and improves upon their efficiency by roughly in term of used resource and response time 
evaluation of information retrieval ir system ha recently been exploring the use of preference judgment over two search result list unlike the traditional method of collecting relevance label per single result this method allows to consider the interaction between search result a part of the judging criterion for example one result list may be preferred over another if it ha a more diverse set of relevant result covering a wider range of user intent in this paper we investigate how assessor determine their preference for one list of result over another with the aim to understand the role of various relevance dimension in preference based evaluation we run a series of experiment and collect preference judgment over different relevance dimension in side by side comparison of two search result list a well a relevance judgment for the individual document our analysis of the collected judgment reveals that preference judgment combine multiple dimension of relevance that go beyond the traditional notion of relevance centered on topicality measuring performance based on single document judgment and ndcg aligns well with topicality based preference but show misalignment with judge overall preference largely due to the diversity dimension a a judging method dimensional preference judging is found to lead to improved judgment quality 
in microblog like twitter popular tweet are usually retweeted by many user for different tweet their lifespan i e how long they will stay popular vary this paper present a simple yet effective approach to predict the lifespan of popular tweet based on their static characteristic and dynamic retweeting pattern for a potentially popular tweet we generate a time series based on it first hour retweeting information and compare it with those of historic tweet of the same author and post time at the granularity of hour the top k historic tweet are identified whose mean lifespan is estimated a the lifespan of the new tweet our experiment on a three month real data set from tencent microblog demonstrate the effectiveness of the approach 
real life information retrieval take place in session where user search by iterating between various cognitive perceptual and motor subtasks through an interactive interface the session may follow diverse strategy which together with the interface characteristic affect user effort cost experience and session effectiveness in this paper we propose a pragmatic evaluation approach based on scenario with explicit subtask cost we study the limit of effectiveness of diverse interactive searching strategy in two searching environment the scenario under overall cost constraint this is based on a comprehensive simulation of million session in each scenario we analyze the effectiveness of the session strategy over time and the property of the most and the least effective session in each case furthermore we will also contrast the proposed evaluation approach with the traditional one rank based evaluation and show how the latter may hide essential factor that affect user performance and satisfaction and give even counter intuitive result 
mobile sm spam is on the rise and is a prevalent problem while recent work ha shown that simple machine learning technique can distinguish between ham and spam with high accuracy this paper explores the individual contribution of various textual feature in the classification process our result reveal the surprising finding that simple is better using the largest spam corpus of which we are aware we find that using simple textual feature is sufficient to provide accuracy that is nearly identical to that achieved by the best known technique while achieving a twofold speedup 
the ubiquitous availability of digital camera ha made it easier than ever to capture moment of life especially the one accompanied with friend and family it is generally believed that most family photo are with face that are sparsely tagged therefore a better solution to manage and search in the tremendously growing personal or group photo is highly anticipated in this paper we propose a novel way to search for face photo by simultaneously considering attribute e g gender age and race position and size of the target face to better match the content and layout of the multiple face in mind our system allows the user to graphically specify the face position and size on a query canvas where each attribute combination is defined a an icon for easier representation a a secondary feature the user can even place specific face from the previous search result for appearance based retrieval the scenario ha been realized on a tablet device with an intuitive touch interface experimenting with a large scale flickr dataset of more than k face the proposed formulation and joint ranking have made u achieve a hit rate of at rank significantly improving from of the prior search scheme using attribute alone we have also achieved an average running time of second by the proposed block based indexing approach 
thanks to information extraction and semantic web effort search on unstructured text is increasingly refined using semantic annotation and structured knowledge base however most user cannot become familiar with the schema of knowledge base and ask structured query interpreting free format query into a more structured representation is of much current interest the dominant paradigm is to segment or partition query token by purpose reference to type entity attribute name attribute value relation and then launch the interpreted query on structured knowledge base given that structured knowledge extraction is never complete here we choose a le trodden path a data representation that retains the unstructured text corpus along with structured annotation mention of entity and relationship on it we propose two new natural formulation for joint query interpretation and response ranking that exploit bidirectional flow of information between the knowledge base and the corpus one inspired by probabilistic language model computes expected response score over the uncertainty of query interpretation the other is based on max margin discriminative learning with latent variable representing those uncertainty in the context of typed entity search both formulation bridge a considerable part of the accuracy gap between a generic query that doe not constrain the type at all and the upper bound where the perfect target entity type of each query is provided by human our formulation are also superior to a two stage approach of first choosing a target type using recent query type prediction technique and then launching a type restricted entity search query 
modeling a user s click through behavior in click log is a challenging task due to the well known position bias problem recent advance in click model have adopted the examination hypothesis which distinguishes document relevance from position bias in this paper we revisit the examination hypothesis and observe that user click cannot be completely explained by relevance and position bias specifically user with different search intent may submit the same query to the search engine but expect different search result thus there might be a bias between user search intent and the query formulated by the user which can lead to the diversity in user click this bias ha not been considered in previous work such a ubm dbn and ccm in this paper we propose a new intent hypothesis a a complement to the examination hypothesis this hypothesis is used to characterize the bias between the user search intent and the query in each search session this hypothesis is very general and can be applied to most of the existing click model to improve their capacity in learning unbiased relevance experimental result demonstrate that after adopting the intent hypothesis click model can better interpret user click and achieve a significant ndcg improvement 
collaborative network are a special type of social network formed by member who collectively achieve specific goal such a fixing software bug and resolving customer problem in such network information flow among member is driven by the task assigned to the network and by the expertise of it member to complete those task in this work we analyze real life collaborative network to understand their common characteristic and how information is routed in these network our study show that collaborative network exhibit significantly different property compared with other complex network collaborative network have truncated power law node degree distribution and other organizational constraint furthermore the number of step along which information is routed follows a truncated power law distribution based on these observation we developed a network model that can generate synthetic collaborative network subject to certain structure constraint moreover we developed a routing model that emulates task driven information routing conducted by human being in a collaborative network together these two model can be used to study the efficiency of information routing for different type of collaborative network a problem that is important in practice yet difficult to solve without the method proposed in this paper 
a digital collection expand the importance of the temporal aspect of information ha become increasingly apparent the aim of this paper is to investigate the effect of using long term temporal profile of term in information retrieval by enhancing the term selection process of pseudo relevance feedback prf for this purpose two temporal prf approach were introduced considering only temporal aspect and temporal along with textual aspect experiment used the ap and wsj test collection with trec ad hoc topic term temporal profile are extracted from the google book n gram dataset the result show that the long term temporal aspect of term are capable of enhancing retrieval effectiveness 
a the abundance of information on the internet grows an increasing burden is placed on the user to specify his or her query precisely in order to avoid extraneous result that may be relevant but not useful at the same time user have a tendency to repeat their search behavior seeking the same url re finding a well a issuing the same query re searching these repeated action reveal a form of user preference that the search engine can utilize to personalize the result in our approach we personalize search result related to ongoing task allowing for a different degree of strength of interest and diversity of interest per task we focus on high valued query query that are both related to past query and will be related to future query given the ongoing nature of the task 
search engine switching is the voluntary transition between web search engine engine switching can occur for a number of reason including user dissatisfaction with search result a desire for broader topic coverage or verification user preference or even unintentionally an improved understanding of switching rationale allows search provider to tailor the search experience according to the different cause in this paper we study the reason behind search engine switching within a session we address the challenge of identifying switching rationale by designing and implementing client side instrumentation to acquire in situ feedback from user using this feedback we investigate in detail the reason that user switch engine within a session we also study the relationship between implicit behavioral signal and the switching cause and develop and evaluate model to predict the reason for switching in addition we collect editorial judgment of switching rationale by third party judge and show that we can recover switching cause a posteriori our finding provide valuable insight into why user switch search engine in a session and demonstrate the relationship between search behavior and switching motivation the finding also reveal sufficient behavioral consistency to afford accurate prediction of switching rationale which can be used to dynamically adapt the search experience and derive more accurate competitive metric 
compression of large text corpus ha the potential to drastically reduce both storage requirement and per document access cost adaptive method used for general purpose compression are ineffective for this application and historically the most successful method have been based on word based dictionary which allow use of global property of the text however these are dependent on the text complying with assumption about content and lead to dictionary of unpredictable size in recent work we have described an lz like approach in which sampled block of a corpus are used a a dictionary against which the complete corpus is compressed giving compression twice a effective than that of zlib here we explore how pre processing can be used to eliminate redundancy in our sampled dictionary our experiment show that dictionary size can be reduced by or more le than of the collection size with no significant effect on compression or access speed 
in recent year many model have been proposed that are aimed at predicting click of web search user in addition some information retrieval evaluation metric have been built on top of a user model in this paper we bring these two direction together and propose a common approach to converting any click model into an evaluation metric we then put the resulting model based metric a well a traditional metric like dcg or precision into a common evaluation framework and compare them along a number of dimension one of the dimension we are particularly interested in is the agreement between offline and online experimental outcome it is widely believed especially in an industrial setting that online a b testing and interleaving experiment are generally better at capturing system quality than offline measurement we show that offline metric that are based on click model are more strongly correlated with online experimental outcome than traditional offline metric especially in situation when we have incomplete relevance judgement 
we study how potential attacker can identify account on different social network site that all belong to the same user exploiting only innocuous activity that inherently come with posted content we examine three specific feature on yelp flickr and twitter the geo location attached to a user s post the timestamp of post and the user s writing style a captured by language model we show that among these three feature the location of post is the most powerful feature to identify account that belong to the same user in different site when we combine all three feature the accuracy of identifying twitter account that belong to a set of flickr user is comparable to that of existing attack that exploit usernames our attack can identify more account than using usernames when we instead correlate yelp and twitter our result have significant privacy implication a they present a novel class of attack that exploit user tendency to assume that if they maintain different persona with different name the account cannot be linked together whereas we show that the post themselves can provide enough information to correlate the account 
currently several online business deem that advertising revenue alone are not sufficient to generate profit and are therefore set to charge for online content in this paper we explore a complement to the current advertisement model more specifically we propose a micropayment model for non specialized commodity web service based on microcomputations in our model a user that wish to access online content offered by a website doe not need to register or pay to access the website instead he will accept to run microcomputations on behalf of the website in exchange for access to the content these microcomputations can for example support ongoing computing project that have clear social benefit e g project relating to hiv dengue cancer etc or can contribute towards commercial computing project we argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing framework e g the boinc platform we implement a preliminary prototype of a system based on our model through which we evaluate it performance and usability finally we analyze the security and privacy of our proposal and we show that it ensures payment for the content while preserving the privacy of user 
topic model are used to group word in a text dataset into a set of relevant topic unfortunately when a few word frequently appear in a dataset the topic group identified by topic model become noisy because these frequent word repeatedly appear in irrelevant topic group this noise ha not been a serious problem in a text dataset because the frequent word e g the and is do not have much meaning and have been simply removed before a topic model analysis however in a social network dataset we are interested in they correspond to popular person e g barack obama and justin bieber and cannot be simply removed because most people are interested in them to solve this popularity problem we explicitly model the popularity of node word in topic model for this purpose we first introduce a notion of a popularity component and propose topic model extension that effectively accommodate the popularity component we evaluate the effectiveness of our model with a real world twitter dataset our proposed model achieve significantly lower perplexity i e better prediction power compared to the state of the art baseline in addition to the popularity problem caused by the node with high incoming edge degree we also investigate the effect of the outgoing edge degree with another topic model extension we show that considering outgoing edge degree doe not help much in achieving lower perplexity 
gps data tracked on mobile device contains rich information about human activity and preference in this paper gps data is used in location based service lb to provide collaborative location recommendation we observe that most existing lb provide location recommendation by clustering the user location matrix since the user location matrix created based on gps data is huge there are two major problem with these method first the number of similar location that need to be considered in computing the recommendation can be numerous a a result the identification of truly relevant location from numerous candidate is challenging second the clustering process on large matrix is time consuming thus when new gps data arrives complete re clustering of the whole matrix is infeasible to tackle these two problem we propose the collaborative location recommendation clr framework for location recommendation by considering activity i e temporal preference and different user class i e pattern user normal user and traveler in the recommendation process clr is capable of generating more precise and refined recommendation to the user compared to the existing method moreover clr employ a dynamic clustering algorithm cadc to cluster the trajectory data into group of similar user similar activity and similar location efficiently by supporting incremental update of the group when new gps trajectory data arrives we evaluate clr with a real world gps dataset and confirm that the clr framework provides more accurate location recommendation compared to the existing method 
synchronous social question and answer q a system match asker to answerer and support real time dialog between them to resolve question these system typically find answerer based on the degree of expertise match with the asker s initial question however since synchronous social q a involves a dialog between asker and answerer difference in expertise may also matter e g extreme novice and expert may have difficulty establishing common ground in this poster we use data from a live social q a system to explore the impact of expertise difference on answer quality and aspect of the dialog itself the finding of our study suggest that synchronous social q a system should consider the relative expertise of candidate answerer with respect to the asker and offer interactive dialog support to help establish common ground between asker and answerer 
in recent year a number of machine learning approach to literature based gene function annotation have been proposed however due to issue such a lack of labeled data class imbalance and computational cost they have usually been unable to surpass simpler approach based on string matching in this paper we propose a principled machine learning approach based on kernel classifier we show that kernel can address the task s inherent data scarcity by embedding additional knowledge and we propose a simple yet effective solution to deal with class imbalance from experiment on the trec genomics track data our approach achieves better f score than two state of the art approach based on string matching and cross specie information 
search result clustering src is a post retrieval process that hierarchically organizes search result the hierarchical structure offer overview for the search result and display an information lay of land that intent to guide the user throughout a search session however src hierarchy are sensitive to query change which are common among query in the same session this instability may leave user seemly random overview throughout the session we present a new tool called infoland that integrates external knowledge from wikipedia when building src hierarchy and increase their stability evaluation on trec session track show that infoland produce more stable result organization than a commercial search engine 
for many information retrieval application we need to deal with the ranking problem on very large scale graph however it is non trivial to perform efficient and effective ranking on them on one aspect we need to design scalable algorithm on another aspect we also need to develop powerful computational infrastructure to support these algorithm this tutorial aim at giving a timely introduction to the promising advance in the aforementioned aspect in recent year and providing the audience with a comprehensive view on the related literature 
personalized search system tailor search result to the current user intent using historic search interaction this relies on being able to find pertinent information in that user s search history which can be challenging for unseen query and for new search scenario building richer model of user current and historic search task can help improve the likelihood of finding relevant content and enhance the relevance and coverage of personalization method the task based approach can be applied to the current user s search history or a we focus on here all user search history a so called groupization a variant of personalization whereby other user profile can be used to personalize the search experience we describe a method whereby we mine historic search engine log to find other user performing similar task to the current user and leverage their on task behavior to identify web page to promote in the current ranking we investigate the effectiveness of this approach versus query based matching and finding related historic activity from the current user i e group versus individual a part of our study we also explore the use of the on task behavior of particular user cohort such a people who are expert in the topic currently being searched rather than all other user our approach yield promising gain in retrieval performance and ha direct implication for improving personalization in search system 
we tackle the problem of entity linking for large collection of online page our system zencrowd identifies entity from natural language text using state of the art technique and automatically connects them to the linked open data cloud we show how one can take advantage of human intelligence to improve the quality of the link by dynamically generating micro task on an online crowdsourcing platform we develop a probabilistic framework to make sensible decision about candidate link and to identify unreliable human worker we evaluate zencrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing technique can significantly improve the quality of the link while limiting the amount of work performed by the crowd 
many private and or public organization have been reported to create and monitor targeted twitter stream to collect and understand user opinion about the organization targeted twitter stream is usually constructed by filtering tweet with user defined selection criterion e g tweet published by user from a selected region or tweet that match one or more predefined keywords targeted twitter stream is then monitored to collect and understand user opinion about the organization there is an emerging need for early crisis detection and response with such target stream such application require a good named entity recognition ner system for twitter which is able to automatically discover emerging named entity that is potentially linked to the crisis in this paper we present a novel step unsupervised ner system for targeted twitter stream called twiner in the first step it leverage on the global context obtained from wikipedia and web n gram corpus to partition tweet into valid segment phrase using a dynamic programming algorithm each such tweet segment is a candidate named entity it is observed that the named entity in the targeted stream usually exhibit a gregarious property due to the way the targeted stream is constructed in the second step twiner construct a random walk model to exploit the gregarious property in the local context derived from the twitter stream the highly ranked segment have a higher chance of being true named entity we evaluated twiner on two set of real life tweet simulating two targeted stream evaluated using labeled ground truth twiner achieves comparable performance a with conventional approach in both stream various setting of twiner have also been examined to verify our global context local context combo idea 
it is an important research problem to design efficient and effective solution for large scale similarity search one popular strategy is to represent data example a compact binary code through semantic hashing which ha produced promising result with fast search speed and low storage cost many existing semantic hashing method generate binary code for document by modeling document relationship based on similarity in a keyword feature space two major limitation in existing method are tag information is often associated with document in many real world application but ha not been fully exploited yet the similarity in keyword feature space doe not fully reflect semantic relationship that go beyond keyword matching this paper proposes a novel hashing approach semantic hashing using tag and topic modeling shttm to incorporate both the tag information and the similarity information from probabilistic topic modeling in particular a unified framework is designed for ensuring hashing code to be consistent with tag information by a formal latent factor model and preserving the document topic semantic similarity that go beyond keyword matching an iterative coordinate descent procedure is proposed for learning the optimal hashing code an extensive set of empirical study on four different datasets ha been conducted to demonstrate the advantage of the proposed shttm approach against several other state of the art semantic hashing technique furthermore experimental result indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately while the combination of these two technique in the unified framework obtains even better result 
this paper describes a live and scalable system that automatically extract information nugget for entity topic from a continuously updated corpus for effective exploration and analysis a nugget is a piece of semantic information that must be mapped semantically to the transitive closure of a pre defined ontology is explicitly supported by text and ha a natural language description that completely conveys it semantic to a user fig show a type of nugget involvement in event for a person entity leon panetta each nugget ha a short description meeting news conference with a list of supporting passage our key contribution are we extract nugget and remove redundancy to produce a summary of salient information with supporting cluster of passage we present an entity topic centric exploration interface that also allows user to navigate to other entity involved in a nugget we use the statistical nlp technology developed over the year in the ace gale and tac kbp program including parsing mention detection within and cross document coreference resolution relation detection and slot filler extraction our system is flexible and easily adaptable across domain a demonstrated on two corpus generic news and scientific paper search engine such a google news and scholar do not retrieve nugget and only remove redundancy at document level news aggregation application such a evri categorize news article based on the entity of topic but do not extract nugget other system extract richer information but not all of it ha clear semantics e g silobreaker present result a the relationship between x and y in the context of keyphrase leaving user with the task of interpreting the semantics a it is not tied to a clear ontology in contrast we remove redundancy summarize result and present nugget that have clear semantics 
retrieving similar document from a large scale text corpus according to a given document is a fundamental technique for many application however most of existing indexing technique have difficulty to address this problem due to special property of a document query e g high dimensionality sparse representation and semantic concern towards addressing this problem we propose a two level retrieval solution based on a document decomposition idea a document is decomposed to a compact vector and a few document specific keywords by a dimension reduction approach the compact vector embodies the major semantics of a document and the document specific keywords complement the discriminative power lost in dimension reduction process we adopt locality sensitive hashing lsh to index the compact vector which guarantee to quickly find a set of related document according to the vector of a query document then we re rank document in this set by their document specific keywords in experiment we obtained promising result on various datasets in term of both accuracy and performance we demonstrated that this solution is able to index large scale corpus for efficient similarity based document retrieval 
a blog service company provides a function named blogcast that expose quality post on the blog main page to vitalize a blogosphere this paper analyzes a new type of information diffusion via blogcast we show that there exists a strong halo effect in a blogosphere via thorough investigation on a huge volume of blog data 
we introduce in this work a novel approach for semantic indexing and mental image search given semantic concept defined by few training example our formulation is transductive and learns a mapping from an initial ambient space related to low level visual feature to an output space spanned by a well defined semantic basis where data can be easily explored with this method searching for a mental visual target reduces to scanning data according to their coordinate in the learned semantic space we illustrate the proposed method through our graphical user interface spacious for the purpose of visualization and interactive navigation in generic image database and satellite image 
manifold ranking mr a graph based ranking algorithm ha been widely applied in information retrieval and shown to have excellent performance and feasibility on a variety of data type particularly it ha been successfully applied to content based image retrieval because of it outstanding ability to discover underlying geometrical structure of the given image database however manifold ranking is computationally very expensive both in graph construction and ranking computation stage which significantly limit it applicability to very large data set in this paper we extend the original manifold ranking algorithm and propose a new framework named efficient manifold ranking emr we aim to address the shortcoming of mr from two perspective scalable graph construction and efficient computation specifically we build an anchor graph on the data set instead of the traditional k nearest neighbor graph and design a new form of adjacency matrix utilized to speed up the ranking computation the experimental result on a real world image database demonstrate the effectiveness and efficiency of our proposed method with a comparable performance to the original manifold ranking our method significantly reduces the computational time make it a promising method to large scale real world retrieval problem 
social feature are increasingly integrated within the search result page of the main commercial search engine there is however little understanding of the utility of social feature in traditional search in this paper we study utility in the context of social annotation which are marking indicating that a person in the social network of the user ha liked or shared a result document we introduce a taxonomy of social relevance aspect that influence the utility of social annotation in search spanning query class the social network and content relevance we present the result of a user study quantifying the utility of social annotation and the interplay between social relevance aspect through the user study we gain insight on condition under which social annotation are most useful to a user finally we present machine learned model for predicting the utility of a social annotation using the user study judgment a an optimization criterion we model the learning task with feature drawn from web usage log and show empirical evidence over real world head and tail query that the problem is learnable and that in many case we can predict the utility of a social annotation 
this paper examines a multi stage retrieval architecture consisting of a candidate generation stage a feature extraction stage and a reranking stage using machine learned model given a fixed set of feature and a learning to rank model we explore effectiveness efficiency tradeoff with three candidate generation approach posting intersection with svs conjunctive query evaluation with wand and disjunctive query evaluation with wand we find no significant difference in end to end effectiveness a measured by ndcg between conjunctive and disjunctive wand but conjunctive query evaluation is substantially faster posting intersection with svs while fast yield substantially lower end to end effectiveness suggesting that document and term frequency remain important in the initial ranking stage these finding show that conjunctive wand is the best overall candidate generation strategy of those we examined 
content sharing in social network is a powerful mechanism for discovering content on the internet the degree to which content is disseminated within the network depends on the connectivity relationship among network node existing scheme for recommending connection in social network are based on the number of common neighbor similarity of user profile etc however such similarity based connection do not consider the amount of content discovered in this paper we propose novel algorithm for recommending connection that boost content propagation in a social network without compromising on the relevance of the recommendation unlike existing work on influence propagation in our environment we are looking for edge instead of node with a bound on the number of incident edge per node we show that the content spread function is not submodular and develop approximation algorithm for computing a near optimal set of edge through experiment on real world social graph such a flickr and twitter we show that our approximation algorithm achieve content spread that are a much a time higher compared to existing heuristic for recommending connection 
due to the prevalence of personalization and information filtering application modeling user interest on the web ha become increasingly important during the past few year in this paper aiming at providing accurate personalized web site recommendation for web user we propose a novel probabilistic factor model based on dimensionality reduction technique we also extend the proposed method to collective probabilistic factor modeling which further improves model performance by incorporating heterogeneous data source the proposed method is general and can be applied to not only web site recommendation but also a wide range of web application including behavioral targeting sponsored search etc the experimental analysis on web site recommendation show that our method outperforms other traditional recommendation approach moreover the complexity analysis indicates that our approach can be applied to very large datasets since it scale linearly with the number of observation 
caching is an important optimization in search engine architecture existing caching technique for search engine optimization are mostly biased towards the reduction of random access to disk because random access are known to be much more expensive than sequential access in traditional magnetic hard disk drive hdd recently solid state drive ssd ha emerged a a new kind of secondary storage medium and some search engine like baidu have already used ssd to completely replace hdd in their infrastructure one notable property of ssd is that it random access latency is comparable to it sequential access latency therefore the use of ssds to replace hdds in a search engine infrastructure may void the cache management of existing search engine in this paper we carry out a series of empirical experiment to study the impact of ssd on search engine cache management the result give insight to practitioner and researcher on how to adapt the infrastructure and how to redesign the caching policy for ssd based search engine 
the aim of a web based recommender system is to provide highly accurate and up to date recommendation to it user in practice it will hope to retain it user over time however this raise unique challenge to achieve complex goal such a keeping the recommender model up to date over time we need to consider a number of external requirement generally these requirement arise from the physical nature of the system for instance the available computational resource ideally we would like to design a system that doe not deviate from the required outcome modeling such a system over time requires to describe the internal dynamic a a combination of the underlying recommender model and the it user behavior we propose to solve this problem by applying the principle of modern control theory a powerful set of tool to deal with dynamical system to construct and maintain a stable and robust recommender system for dynamically evolving environment in particular we introduce a design principle by focusing on the dynamic relationship between the recommender system s performance and the number of new training sample the system requires this enables u to automate the control other external factor such a the system s update frequency we show that by using a proportional integral derivative controller a recommender system is able to automatically and accurately estimate the required input to keep the output close to a pre defined requirement our experiment on a standard rating dataset show that by using a feedback loop between system performance and training the trade off between the effectiveness and efficiency of the system can be well maintained we close by discussing the widespread applicability of our approach to a variety of scenario that recommender system face 
we consider the problem of identifying the most respected authoritative member of a large scale online social network osn by constructing a global ranked list of it member the problem is distinct from the problem of identifying influencers we are interested in identifying member who are influential in the real world even when not necessarily so on the osn we focus on two source for information about user authority a invitation to connect which are usually sent to people whom the inviter respect and b member browsing behavior a profile of more important people are viewed more often than others we construct two directed graph over the same set of node representing member profile the invitation graph and the navigation graph respectively we show that the standard pagerank algorithm a baseline in web page ranking is not effective in people ranking and develop a social capital based model called the fair bet model a a viable solution we then propose a novel approach called bimodal fair bet for combining information from two or more endorsement graph drawn from the same osn by simultaneously using the authority score of node in one graph to inform the other and vice versa in a mutually reinforcing fashion we evaluate the ranking result on the linkedin social network using this model where member who have wikipedia profile are assumed to be authoritative experimental result show that our approach outperforms the baseline approach by a large margin 
temporal aspect of document can impact relevance for certain kind of query in this paper we build on earlier work of modeling temporal information we propose an extension to the query likelihood model that incorporates query specific information to estimate rate parameter and we introduce a temporal factor into language model smoothing and query expansion using pseudo relevance feedback we evaluate these extension using a twitter corpus and two newspaper article collection result suggest that compared to prior approach our model are more effective at capturing the temporal variability of relevance associated with some topic 
we present a recommendation method based on the well known concept of center piece subgraph that allows for the time space efficient generation of suggestion also for rare i e long tail query our method is scalable with respect to both the size of datasets from which the model is computed and the heavy workload that current web search engine have to deal with basically we relate term contained into query with highly correlated query in a query flow graph this enables a novel recommendation generation method able to produce recommendation for approximately of the workload of a real world search engine the method is based on a graph having term node query node and two kind of connection term query and query query the first connects a term to the query in which it is contained the second connects two query node if the likelihood that a user submits the second query after having issued the first one is sufficiently high on such large graph we need to compute the center piece subgraph induced by term contained into query in order to reduce the cost of the above computation we introduce a novel and efficient method based on an inverted index representation of the model we experiment our solution on two real world query log and we show that it effectiveness is comparable and in some case better than state of the art method for head query more importantly the quality of the recommendation generated remains very high also for long tail query where other method fail even to produce any suggestion finally we extensively investigate scalability and efficiency issue and we show the viability of our method in real world search engine 
term proximity retrieval reward a document where the matched query term occur close to each other although term proximity is known to be effective in many information retrieval ir application the within document distribution of each individual query term and how the query term associate with each other are not fully considered in this paper we introduce a pseudo term namely cross term to model term proximity for boosting retrieval performance an occurrence of a query term is assumed to have an impact towards it neighboring text which gradually weakens with the increase of the distance to the place of occurrence we use a shape function to characterize such an impact a cross term occurs when two query term appear close to each other and their impact shape function have an intersection we propose a cross term retrieval crter model that combine the cross term information with basic probabilistic weighting model to rank the retrieved document extensive experiment on standard trec collection illustrate the effectiveness of our proposed crter model 
in this demo we study one category of query refinement problem in the context of xml keyword search where what user search for do not exist in the data while useless result are returned by the search engine it is a hidden but important problem we refer to it a the mismatch problem we propose a practical yet efficient way to detect the mismatch problem and generate helpful suggestion to user namely mismatch detector and suggester our approach can be viewed a a post processing job of query evaluation an online xml keyword search engine embedding the mismatch detector and suggester ha been built and is available at 
collaborative filtering technique rely on aggregated user preference data to make personalized prediction in many case user are reluctant to explicitly express their preference and many recommender system have to infer them from implicit user behavior such a clicking a link in a webpage or playing a music track the click and the play are good for indicating the item a user liked i e positive training example but the item a user did not like negative training example are not directly observed previous approach either randomly pick negative training sample from unseen item or incorporate some heuristic into the learning model leading to a biased solution and a prolonged training period in this paper we propose to dynamically choose negative training sample from the ranked list produced by the current prediction model and iteratively update our model the experiment conducted on three large scale datasets show that our approach not only reduces the training time but also lead to significant performance gain 
in commerce search the set of product returned by a search engine often form the basis for all user interaction leading up to a potential transaction on the web such a set of product is known a the consideration set in this study we consider the problem of generating consideration set of product in commerce search so a to maximize user satisfaction one of the key feature of commerce search that we exploit in our study is the association of a set of important attribute with the product and a set of specified attribute with the user query those important attribute not used in the query are treated a unspecified the attribute space admits a natural definition of user satisfaction via user preference on the attribute and their value viz require that the surfaced product be close to the specified attribute value in the query and diverse with respect to the unspecified attribute we model this a a general max sum dispersion problem wherein we are given a set of n node in a metric space and the objective is to select a subset of node with total cost at most a given budget and maximize the sum of the pairwise distance between the selected node in our setting each node denotes a product the cost of a node being inversely proportional to it relevance with respect to specified attribute the distance between two node quantifies the diversity with respect to the unspecified attribute the problem is np hard and a approximation wa previously known only when all the node have unit cost in our setting we do not make any assumption on the cost we label this problem a the general max sum dispersion problem we give the first constant factor approximation algorithm for this problem achieving an approximation ratio of further we perform extensive empirical analysis on real world data to show the effectiveness of our algorithm 
text classifier are frequently used for high yield retrieval from large corpus such a in e discovery the classifier is trained by annotating example document for relevance these example may however be assessed by people other than those whose conception of relevance is authoritative in this paper we examine the impact that disagreement between actual and authoritative assessor ha upon classifier effectiveness when evaluated against the authoritative conception we find that using alternative assessor lead to a significant decrease in binary classification quality though le so ranking quality a ranking consumer would have to go on average deeper in the ranking produced by alternative assessor training to achieve the same yield a for authoritative assessor training 
a the internet grows explosively search engine play a more and more important role for user in effectively accessing online information recently it ha been recognized that a query is often triggered by a search task that the user want to accomplish similarly many web page are specifically designed to help accomplish a certain task therefore learning hidden task behind query and web page can help search engine return the most useful web page to user by task matching for instance the search task that trigger query thinkpad t broken is to maintain a computer and it is desirable for a search engine to return the lenovo troubleshooting page on the top of the list however existing search engine technology mainly focus on topic detection or relevance ranking which are not able to predict the task that trigger a query and the task a web page can accomplish in this paper we propose to simultaneously classify query and web page into the popular search task by exploiting their content together with click through log specifically we construct a taskoriented heterogeneous graph among query and web page each pair of object in the graph are linked together a long a they potentially share similar search task a novel graph based regularization algorithm is designed for search task prediction by leveraging the graph extensive experiment in real search log data demonstrate the effectiveness of our method over state of the art classifier and the search performance can be significantly improved by using the task prediction result a additional information 
the clustering coefficient of a node in a social network is a fundamental measure that quantifies how tightly knit the community is around the node it computation can be reduced to counting the number of triangle incident on the particular node in the network in case the graph is too big to fit into memory this is a non trivial task and previous researcher showed how to estimate the clustering coefficient in this scenario a different avenue of research is to to perform the computation in parallel spreading it across many machine in recent year mapreduce ha emerged a a de facto programming paradigm for parallel computation on massive data set the main focus of this work is to give mapreduce algorithm for counting triangle which we use to compute clustering coefficient our contribution are twofold first we describe a sequential triangle counting algorithm and show how to adapt it to the mapreduce setting this algorithm achieves a factor of speed up over the naive approach second we present a new algorithm designed specifically for the mapreduce framework a key feature of this approach is that it allows for a smooth tradeoff between the memory available on each individual machine and the total memory available to the algorithm while keeping the total work done constant moreover this algorithm can use any triangle counting algorithm a a black box and distribute the computation across many machine we validate our algorithm on real world datasets comprising of million of node and over a billion edge our result show both algorithm effectively deal with skew in the degree distribution and lead to dramatic speed ups over the naive implementation 
search engine today offer a rich user experience no longer restricted to ten blue link for example the query canon eos digital camera return a photo of the digital camera and a list of suitable merchant and price similar result are offered in other domain like food entertainment travel etc all these experience are fueled by the availability of structured data about the entity of interest to obtain this structured data it is necessary to solve the following problem given a category of entity with it schema and a set of web page that mention and describe entity belonging to the category build a structured representation for the entity under the given schema specifically collect structured numerical or discrete attribute of the entity most previous approach regarded this a an information extraction problem on individual document and made no special use of numerical attribute in contrast we present an end to end framework which leverage signal not only from the web page context but also from a collective analysis of all the page corresponding to an entity and from constraint related to the actual value within the domain our current implementation us a general and flexible integer linear program ilp to integrate all these signal into holistic decision over all attribute there is one ilp per entity and it is small enough to be solved in under millisecond in our experiment we apply the new framework to a setting of significant practical importance catalog expansion for commerce search engine using data from bing shopping finally we present experiment that validate the effectiveness of the framework and it superiority to local extraction 
modern massively multiplayer online role playing game mmorpgs provide lifelike virtual environment in which player can conduct a variety of activity including combat trade and chat with other player while the game world and the available action therein are inspired by their offline counterpart the game popularity and dedicated fan base are testament to the allure of novel social interaction granted to people by allowing them an alternative life a a new character and persona in this paper we investigate the phenomenon of gender swapping which refers to player choosing avatar of gender opposite to their natural one we report the behavioral pattern observed in player of fairyland online a globally serviced mmorpg during social interaction when playing a in game avatar of their own real gender or gender swapped we also discus the effect of gender role and self image in virtual social situation and the potential of our study for improving mmorpg quality and detecting online identity fraud 
this study us regression modeling to predict a user s domain knowledge level dk from implicit evidence provided by certain search behavior a user study n with recall oriented search task in the genomic domain wa conducted a number of regression model of a person s dk were generated using different behavior variable selection method the best model highlight three behavior variable a dk predictor the number of document saved the average query length and the average ranking position of document opened the model is validated using the split sampling method limitation and future research direction are discussed 
many of the recent and more effective retrieval model have incorporated dependency between the term in the query in this paper we advance this query representation one step further and propose a retrieval framework that model higher order term dependency i e dependency between arbitrary query concept rather than just query term in order to model higher order term dependency we represent a query using a hypergraph structure a generalization of a graph where a hyper edge connects an arbitrary subset of vertex a vertex in a query hypergraph corresponds to an individual query concept and a dependency between a subset of these vertex is modeled through a hyperedge an extensive empirical evaluation using both newswire and web corpus demonstrates that query representation using hypergraphs is highly beneficial for verbose natural language query for these query query hypergraphs significantly improve the retrieval effectiveness of several state of the art model that do not employ higher order term dependency 
we study several longstanding question in medium communication research in the context of the microblogging service twitter regarding the production flow and consumption of information to do so we exploit a recently introduced feature of twitter known a list to distinguish between elite user by which we mean celebrity blogger and representative of medium outlet and other formal organization and ordinary user based on this classification we find a striking concentration of attention on twitter in that roughly of url consumed are generated by just k elite user where the medium produce the most information but celebrity are the most followed we also find significant homophily within category celebrity listen to celebrity while blogger listen to blogger etc however blogger in general rebroadcast more information than the other category next we re examine the classical two step flow theory of communication finding considerable support for it on twitter third we find that url broadcast by different category of user or containing different type of content exhibit systematically different lifespan and finally we examine the attention paid by the different user category to different news topic 
real time news and social medium quickly reflect large scale phenomenon and event a user become exposed to this information time play a central role in prompting both information authorship and seeking activity the objective of this research is to develop a retrieval system which can anticipate a user s likely temporal intent s considering recent or ongoing real world event such a system should not only provide recent news when relevant but also higher rank non timestamped or even older document which are temporally pertinent a they cover aspect related to recent event topic key challenge to be addressed in this work include a suitable source and method for event detection and tracking an intent aware ranking approach and an evaluation methodology 
estimating the geographic location of image is a task which ha received increasing attention recently large number of image uploaded to platform such a flickr do not contain gps based latitude longitude coordinate obtaining such geographic information is beneficial for a variety of application including travelogue visual place description and personalized travel recommendation while most work in this area only exploit an image s textual meta data tag title etc to estimate at what geographic location the image wa taken we consider an additional textual dimension the image owner s trace on the social web specifically we hypothesize that information extracted from a person s microblog stream s can be utilized to improve the accuracy with which the geographic location of the image is estimated in this paper we investigate this hypothesis on the example of twitter stream and find it to be confirmed the median error distance in kilometre decrease by up to in comparison to existing state of the art the best result are achieved when tweet that were posted up to two day before and after an image wa taken are considered moreover we also find another type of additional information useful population density data 
positional ranking function widely used in web search engine improve result quality by exploiting the position of the query term within document however it is well known that positional index demand large amount of extra space typically about three time the space of a basic nonpositional index textual data on the other hand is needed to produce text snippet in this paper we study time space trade offs for search engine with positional ranking function and text snippet generation we consider both index based and non index based alternative for positional data we aim to answer the question of whether one should index positional data or not we show that there is a wide range of practical time space trade offs moreover we show that both position and textual data can be stored using about of the space used by traditional positional index with a minor increase in query time this yield considerable space saving and outperforms both in space and time recent alternative from the literature we also propose several efficient compressed text representation for snippet generation which are able to use about half of the space of current state of the art alternative with little impact in query processing time 
most of existing e commerce recommender system aim to recommend the right product to a user based on whether the user is likely to purchase or like a product on the other hand the effectiveness of recommendation also depends on the time of the recommendation let u take a user who just purchased a laptop a an example she may purchase a replacement battery in year assuming that the laptop s original battery often fails to work around that time and purchase a new laptop in another year in this case it is not a good idea to recommend a new laptop or a replacement battery right after the user purchased the new laptop it could hurt the user s satisfaction of the recommender system if she receives a potentially right product recommendation at the wrong time we argue that a system should not only recommend the most relevant item but also recommend at the right time this paper study the new problem how to recommend the right product at the right time we adapt the proportional hazard modeling approach in survival analysis to the recommendation research field and propose a new opportunity model to explicitly incorporate time in an e commerce recommender system the new model estimate the joint probability of a user making a follow up purchase of a particular product at a particular time this joint purchase probability can be leveraged by recommender system in various scenario including the zero query pull based recommendation scenario e g recommendation on an e commerce web site and a proactive push based promotion scenario e g email or text message based marketing we evaluate the opportunity modeling approach with multiple metric experimental result on a data collected by a real world e commerce website shop com show that it can predict a user s follow up purchase behavior at a particular time with descent accuracy in addition the opportunity model significantly improves the conversion rate in pull based system and the user satisfaction utility in push based system 
in this poster we propose two evaluation task for mobile information access the first task evaluates the system s ability to guess what the user s query should be given a context knowing what the second task evaluates the system s ability to decide when to proactively deploy a given query knowing when we conduct a preliminary manual analysis of a mobile query log to limit the space of possible query so a to design feasible and practical evaluation task 
this poster describes an alternative approach to handling the best document selection problem best document selection is a common problem with many real world application but is not a well studied problem by itself a simple solution would be to treat it a a ranking problem and to use existing ranking algorithm to rank all document we could then select only the first element of the sorted list however because ranking model optimize for all rank the model may sacrifice accuracy of the top rank for the sake of overall accuracy this is an unnecessary trade off we begin by first defining an appropriate objective function for the domain then create a boosting algorithm that explicitly target this function based on experiment on a benchmark retrieval data set and digg com news commenting data set we find that even a simple algorithm built for this specific problem give better result than baseline algorithm that were designed for the more complicated ranking task 
this paper describes an entity ranking model for example based person search in email evaluation by comparison to manually resolved named reference in enron email yield result that correspond to typically placing the correct entity in the first or second rank 
simfusion ha become a captivating measure of similarity between object in a web graph it is iteratively distilled from the notion that the similarity between two object is reinforced by the similarity of their related object the existing simfusion model usually exploit the unified relationship matrix urm to represent latent relationship among heterogeneous data and adopts an iterative paradigm for simfusion computation however due to the row normalization of urm the traditional simfusion model may produce the trivial solution worse still the iterative computation of simfusion may not ensure the global convergence of the solution this paper study the revision of this model providing a full treatment from complexity to algorithm we propose simfusion based on a notion of the unified adjacency matrix uam a modification of the urm to prevent the trivial solution and the divergence issue of simfusion we show that for any vertex pair simfusion can be performed in o time and o n space with an o km time precomputation done only once a opposed to the o kn time and o n space of it traditional counterpart where n m and k denote the number of vertex edge and iteration respectively we also devise an incremental algorithm for further improving the computation of simfusion when network are dynamically updated with performance guarantee for similarity estimation we experimentally verify that these algorithm scale well and the revised notion of simfusion is able to converge to a non trivial solution and allows u to identify more sensible structure information in large real world network 
the retrieval result of online product information in e commerce web site are often difficult for user to use because of different description for the same product this paper proposes productseeker a product retrieval system organizing result according to their referring real world entity for the convenience of user in the demonstration we will present our system providing friendly interface to retrieve fresh product information and refining result according to feedback 
medium study concern the study of production content and or reception of various type of medium today s continuous production and storage of medium is changing the way medium study researcher work and requires the development of new search model and tool we investigate the research cycle of medium study researcher and find that it is an iterative process consisting of several search process in which data is gathered and the research question is refined change in the research question however trigger new data gathering process based on these outcome we propose a subjunctive exploratory search interface to support medium study researcher in refining their research question in an earlier stage of their research to ass the subjunctive interface we conduct a user study and compare to a traditional exploratory search interface we find that with the subjunctive interface user explore more diverse topic than with the standard interface and that user formulate more specific research question although the subjunctive interface is more complex this doe not decrease it usability these finding suggest that the subjunctive interface support medium study researcher the advantage of a subjunctive interface for exploration suggests a new direction for the development of exploratory search system 
it is often considered that high abandonment rate corresponds to poor ir system performance however several study suggested that there are so called good abandonment i e situation when search engine result page contains enough detail to satisfy the user information need without necessity to click on search result in this work we propose to look at query extension we see that an extension by itself might motivate abandonment type good or bad for the underlying query to some degree we also propose a way to find potentially good abandonment extension in an automated manner 
query result caching is an efficient technique for web search engine in this paper we present user aware cache a novel approach tailored for query result caching that is based on user characteristic we then use a trace of around million query to evaluate user aware cache a well a traditional method and theoretical upper bound experimental result show that this approach can achieve hit ratio better than state of the art method 
when human assessor judge document for their relevance to a search topic it is possible for error in judging to occur a part of the analysis of the data collected from a participant user study we have discovered that when the participant made relevance judgment the average participant spent more time to make errorful judgment than to make correct judgment thus in relevance assessing scenario similar to our user study it may be possible to use the time taken to judge a document a an indicator of assessor error such an indicator could be used to identify document that are candidate for adjudication or reassessment 
modern information retrieval interface typically involve multiple page of search result and user who are recall minded or engaging in exploratory search using ad hoc query are likely to access more than one page document ranking for such query can be improved by allowing additional context to the query to be provided by the user herself using explicit rating or implicit action such a clickthroughs existing method using this information usually involved detrimental ui change that can lower user satisfaction instead we propose a new feedback scheme that make use of existing uis and doe not alter user s browsing behaviour to maximise retrieval performance over multiple result page we propose a novel retrieval optimisation framework and show that the optimal ranking policy should choose a diverse exploratory ranking to display on the first page then a personalised re ranking of the next page can be generated based on the user s feedback from the first page we show that document correlation used in result diversification have a significant impact on relevance feedback and it effectiveness over a search session trec evaluation demonstrate that our optimal rank strategy including approximative monte carlo sampling can naturally optimise the trade off between exploration and exploitation and maximise the overall user s satisfaction over time against a number of similar baseline 
in this demo we present imecho a context aware desktop search system to help user get more relevant result different from other desktop search engine imecho rank result not only by the content of the query but also the context of the query it employ an hidden markov model hmm based user model which is learned from user s activity log to estimate the query context when he submits the query the result from keyword search are re ranked by their relevance to the context with acceptable overhead 
online social network have become important for networking communication sharing and discovery a considerable challenge these network face is the fact that an online social network is partially observed because two individual might know each other but may not have established a connection on the site therefore link prediction and recommendation are important task for any online social network in this paper we address the problem of computing edge affinity between two user on a social network based on the user belonging to organization such a company school and online group we present experimental insight from social network data on organizational overlap a novel mathematical model to compute the probability of connection between two people based on organizational overlap and experimental validation of this model based on real social network data we also present novel way in which the organization overlap model can be applied to link prediction and community detection which in itself could be useful for recommending entity to follow and generating personalized news feed 
topic modeling can reveal the latent structure of text data and is useful for knowledge discovery search relevance ranking document classification and so on one of the major challenge in topic modeling is to deal with large datasets and large number of topic in real world application in this paper we investigate technique for scaling up the non probabilistic topic modeling approach such a rlsi and nmf we propose a general topic modeling method referred to a group matrix factorization gmf to enhance the scalability and efficiency of the non probabilistic approach gmf assumes that the text document have already been categorized into multiple semantic class and there exist class specific topic for each of the class a well a shared topic across all class topic modeling is then formalized a a problem of minimizing a general objective function with regularization and or constraint on the class specific topic and shared topic in this way the learning of class specific topic can be conducted in parallel and thus the scalability and efficiency can be greatly improved we apply gmf to rlsi and nmf obtaining group rlsi grlsi and group nmf gnmf respectively experiment on a wikipedia dataset and a real world web dataset each containing about million document show that grlsi and gnmf can greatly improve rlsi and nmf in term of scalability and efficiency the topic discovered by grlsi and gnmf are coherent and have good readability further experiment on a search relevance dataset containing labeled query show that the use of topic learned by grlsi and gnmf can significantly improve search relevance 
the rising popularity of social medium in the enterprise present new opportunity for one of the organization s most important need expertise location social medium data can be very useful for expertise mining due to the variety of existing application the rich metadata and the diversity of user association with content in this work we provide an extensive study that explores the use of social medium to infer expertise within a large global organization we examine eight different social medium application by evaluating the data they produce through a large user survey with enterprise social medium user we distinguish between two semantics that relate a user to a topic expertise in the topic and interest in it and compare these two semantics across the different social medium application 
context and social network information have been proven to be valuable information for building accurate recommender system however to the best of our knowledge no existing work systematically combine diverse type of such information to further improve recommendation quality in this paper we propose soco a novel context aware recommender system incorporating elaborately processed social network information we handle contextual information by applying random decision tree to partition the original user item rating matrix such that the rating with similar context are grouped matrix factorization is then employed to predict missing preference of a user for an item using the partitioned matrix in order to incorporate social network information we introduce an additional social regularization term to the matrix factorization objective function to infer a user s preference for an item by learning opinion from his her friend who are expected to share similar taste a context aware version of pearson correlation coefficient is proposed to measure user similarity real datasets based experiment show that soco improves the performance in term of root mean square error of the state of the art context aware recommender system and social recommendation model by and respectively 
rocchio s relevance feedback model is a classic query expansion method and it ha been shown to be effective in boosting information retrieval performance the selection of expansion term in this method however doe not take into account the relationship between the candidate term and the query term e g term proximity intuitively the proximity between candidate expansion term and query term can be exploited in the process of query expansion since term closer to query term are more likely to be related to the query topic in this paper we study how to incorporate proximity information into the rocchio s model and propose a proximity based rocchio s model called proc with three variant in our proc model a new concept proximity based term frequency ptf is introduced to model the proximity information in the pseudo relevant document which is then used in three kind of proximity measure experimental result on trec collection show that our proposed proc model are effective and generally superior to the state of the art relevance feedback model with optimal parameter a direct comparison with positional relevance model prm on the gov collection also indicates our proposed model is at least competitive to the most recent progress 
the majority of the current information retrieval model weight the query concept e g term or phrase in an unsupervised manner based solely on the collection statistic in this paper we go beyond the unsupervised estimation of concept weight and propose a parameterized concept weighting model in our model the weight of each query concept is determined using a parameterized combination of diverse importance feature unlike the existing supervised ranking method our model learns importance weight not only for the explicit query concept but also for the latent concept that are associated with the query through pseudo relevance feedback the experimental result on both newswire and web trec corpus show that our model consistently and significantly outperforms a wide range of state of the art retrieval model in addition our model significantly reduces the number of latent concept used for query expansion compared to the non parameterized pseudo relevance feedback based model 
web usage mining ha traditionally focused on the individual query or query word leading to a web site or web page visit mining pattern in such data in our work we aim to characterize website in term of the semantics of the query that lead to them by linking query to large knowledge base on the web we demonstrate how to exploit such link for more effective pattern mining on query log data we also show how such pattern can be used to qualitatively describe the difference between competing website in the same domain and to quantitatively predict website abandonment 
mapping relational database to rdf is a fundamental problem for the development of the semantic web we present a solution inspired by draft method defined by the w c where relational database are directly mapped to rdf and owl given a relational database schema and it integrity constraint this direct mapping produce an owl ontology which provides the basis for generating rdf instance the semantics of this mapping is defined using datalog two fundamental property are information preservation and query preservation we prove that our mapping satisfies both condition even for relational database that contain null value we also consider two desirable property monotonicity and semantics preservation we prove that our mapping is monotone and also prove that no monotone mapping including ours is semantic preserving we realize that monotonicity is an obstacle for semantic preservation and thus present a non monotone direct mapping that is semantics preserving 
this paper proposes a prototype system called gaze learning access and search engine glase which can perform image relevance ranking based on gaze data and within session learning we developed a search user interface that us an eye tracker a an input device and employed a relevance re ranking algorithm based on the gaze length the preliminary experimental result showed that using our gaze driven system reduced the task completion time an average of in a search session 
this paper describes a search system which includes topic model visualization to improve the user search experience the system graphically render the topic in a retrieved set of document enables a user to selectively refine search result and allows easy navigation through information on selective topic within document 
many information retrieval ir technique have been proposed to improve the performance and some combination of these technique ha been demonstrated to be effective however how to effectively combine them is largely unexplored it is possible that a method reduces the positive influence of the other one even if both of them are effective separately in this paper we propose a new hybrid model which can simply and flexibly combine component of three different ir technique under a uniform framework extensive experiment on the trec standard collection indicate that our proposed model can outperform the best trec system consistently in the ad hoc retrieval it show that the combination strategy in our proposed model is very effective meanwhile this method is also re useable for other researcher to test whether their new method are additive to the current technology 
improving verbose or long query pose a new challenge for search system previous technique mainly focused on two aspect weighting the important word or phrase and selecting the best subset query the former doe not consider how word and phrase are used in actual subset query while the latter ignores alternative subset query recently a novel reformulation framework ha been proposed to transform the original query a a distribution of reformulated query which overcomes the disadvantage of previous technique in this paper we apply this framework to verbose query where a reformulated query is specified a a subset query experiment on trec collection show that the query distribution based framework outperforms the state of the art technique 
the web evolved from a text based system to the current rich and interactive medium that support image d graphic audio and video the major medium type that is still missing is d graphic although various approach have been proposed most notably vrml x d they have not been widely adopted one reason for the limited acceptance is the lack of d interaction technique that are optimal for the hypertext based web interface we present a novel strategy for accessing integrated information space where hypertext and d graphic data are simultaneously available and linked we introduce a user interface that ha two mode between which a user can switch anytime the driven by simple hypertext based interaction don t make me think mode where a d scene is embedded in hypertext and the more immersive d take me to the wonderland mode which immerses the hypertextual annotation into the d scene a user study is presented which characterizes the user interface in term of it efficiency and usability 
content delivery network cdns have become a crucial part of the modern web infrastructure this paper study the performance of the leading content delivery provider akamai it measure the performance of the current akamai platform and considers a key architectural question faced by both cdn designer and their prospective customer whether the co location approach to cdn platform adopted by akamai which try to deploy server in numerous internet location brings inherent performance benefit over a more consolidated data center approach pursued by other influential cdns such a limelight we believe the methodology we developed for this study will be useful for other researcher in the cdn arena 
the tutorial aim to provide the ir researcher with an understanding of how the patent system work the challenge that patent searcher face in using the existing tool and in adopting new method developed in academia at the same time the tutorial will inform the ir researcher about the unique opportunity that the patent domain provides a large amount of multi lingual and multi modal document the widest possible span of covered domain a highly annotated corpus and very importantly relevance judgement created by expert in the field and recorded electronically in the document the combination of these two objective lead to the main purpose of the tutorial to create awareness and to encourage more emphasis on the patent domain in the ir community table provides detail on how the tutorial cover the topic of the sigir conference 
often an interesting true value such a a stock price sport score or current temperature is only available via the observation of noisy and potentially conflicting source several technique have been proposed to reconcile these conflict by computing a weighted consensus based on source reliability but these technique focus on static value when the real world entity evolves over time the noisy source can delay or even miss reporting some of the real world update this temporal aspect introduces two key challenge for consensus based approach i due to delay the mapping between a source s noisy observation and the real world update it observes is unknown and ii missed update may translate to missing value for the consensus problem even if the mapping is known to overcome these challenge we propose a formal approach that model the history of update of the real world entity a a hidden semi markovian process hsmm the noisy source are modeled a observation of the hidden state but the mapping between a hidden state i e real world update and the observation i e source value is unknown we propose algorithm based on gibbs sampling and em to jointly infer both the history of real world update a well a the unknown mapping between them and the source value we demonstrate using experiment on real world datasets how our history based technique improve upon history agnostic consensus based approach 
we propose a description logic style extension of owl with nominal schema which can be used like variable nominal class within axiom this feature allows ontology language to express arbitrary dl safe rule a expressible in swrl or rif in their native syntax we show that adding nominal schema to owl doe not increase the worst case reasoning complexity and we identify a novel tractable language sroelv x that is versatile enough to capture the lightweight language owl el and owl rl 
the number of smart mobile device such a wireless phone and tablet computer ha been rapidly growing these mobile device are equipped with a variety of sensor such a camera gyroscope accelerometer compass nfc wifi gps etc these sensor can be used to capture image and voice detect motion pattern and predict location to name just a few this keynote depicts technique in configuration calibration computation and fusion for improving sensor performance and conserving power consumption we also present novel mobile information management and retrieval application that can benefit a great deal from enhanced sensor technology 
despite the abundance of useful information on the web different web source often provide conflicting data some being out of date inaccurate or erroneous data fusion aim at resolving conflict and finding the truth advanced fusion technique apply iterative map maximum a posteriori analysis that reason about trustworthiness of source and copying relationship between them providing explanation for such decision is important for a better understanding but can be extremely challenging because of the complexity of the analysis during decision making this paper proposes two type of explanation for data fusion result snapshot explanation take the provided data and any other decision inferred from the data a evidence and provide a high level understanding of a fusion decision comprehensive explanation take only the data a evidence and provide an in depth understanding of a fusion decision we propose technique that can efficiently generate correct and compact explanation experimental result show that we generate correct explanation our technique can significantly reduce the size of the explanation and we can generate the explanation efficiently 
concurrently processing thousand of web query each with a response time under a fraction of a second necessitates maintaining and operating massive data center for large scale web search engine this translates into high energy consumption and a huge electric bill this work take the challenge to reduce the electric bill of commercial web search engine operating on data center that are geographically far apart based on the observation that energy price and query workload show high spatio temporal variation we propose a technique that dynamically shift the query workload of a search engine between it data center to reduce the electric bill experiment on real life query workload obtained from a commercial search engine show that significant financial saving can be achieved by this technique 
hashtags are widely used in twitter to define a shared context for event or topic in this paper we aim to predict hashtag popularity in near future i e next day given a hashtag that ha the potential to be popular in the next day we construct a hashtag profile using the tweet containing the hashtag and extract both content and context feature for hashtag popularity prediction we model this prediction problem a a classification problem and evaluate the effectiveness of the extracted feature and classification model 
watson named after ibm founder thomas j watson wa built by a team of ibm researcher who set out to accomplish a grand challenge build a computing system that rival a human s ability to answer question posed in natural language with speed accuracy and confidence the quiz show jeopardy provided the ultimate test of this technology because the game s clue involve analyzing subtle meaning irony riddle and other complexity of natural language in which human excel and computer traditionally fail watson passed it first test on jeopardy beating the show s two greatest champion in a televised exhibition match but the real test will be in applying the underlying natural language processing and analytics technology in business and across industry in this talk i will introduce the jeopardy grand challenge present an overview of watson and the deepqa technology upon which watson is built and explore future application of this technology 
we investigate the effect of rewarding term according to their location in document for probabilistic information retrieval the intuition behind our approach is that a large amount of author would summarize their idea in some particular part of document in this paper we focus on the beginning part of document several shape function are defined to simulate the influence of term location information we propose a reward term retrieval model that combine the reward term information with bm to enhance probabilistic information retrieval performance 
this paper present a new unsupervised approach to generating ultra concise summary of opinion we formulate the problem of generating such a micropinion summary a an optimization problem where we seek a set of concise and non redundant phrase that are readable and represent key opinion in text we measure representativeness based on a modified mutual information function and model readability with an n gram language model we propose some heuristic algorithm to efficiently solve this optimization problem evaluation result show that our unsupervised approach outperforms other state of the art summarization method and the generated summary are informative and readable 
most query in web search are ambiguous and multifaceted identifying the major sens and facet of query from search log data referred to a query subtopic mining in this paper is a very important issue in web search through search log analysis we show that there are two interesting phenomenon of user behavior that can be leveraged to identify query subtopics referred to a one subtopic per search and subtopic clarification by keyword one subtopic per search mean that if a user click multiple url in one query then the clicked url tend to represent the same sense or facet subtopic clarification by keyword mean that user often add an additional keyword or keywords to expand the query in order to clarify their search intent thus the keywords tend to be indicative of the sense or facet we propose a clustering algorithm that can effectively leverage the two phenomenon to automatically mine the major subtopics of query where each subtopic is represented by a cluster containing a number of url and keywords the mined subtopics of query can be used in multiple task in web search and we evaluate them in aspect of the search result presentation such a clustering and re ranking we demonstrate that our clustering algorithm can effectively mine query subtopics with an f measure in the range of our experimental result show that the use of the subtopics mined by our approach can significantly improve the state of the art method used for search result clustering experimental result based on click data also show that the re ranking of search result based on our method can significantly improve the efficiency of user ability to find information 
when information is abundant it becomes increasingly difficult to fit nugget of knowledge into a single coherent picture complex story spaghetti into branch side story and intertwining narrative in order to explore these story one need a map to navigate unfamiliar territory we propose a methodology for creating structured summary of information which we call metro map our proposed algorithm generates a concise structured set of document maximizing coverage of salient piece of information most importantly metro map explicitly show the relation among retrieved piece in a way that capture story development we first formalize characteristic of good map and formulate their construction a an optimization problem then we provide efficient method with theoretical guarantee for generating map finally we integrate user interaction into our framework allowing user to alter the map to better reflect their interest pilot user study with a real world dataset demonstrate that the method is able to produce map which help user acquire knowledge efficiently 
modern web scale behavioral targeting platform leverage historical activity of billion of user to predict user interest and inclination and consequently future activity future activity of particular interest involve purchase or transaction and are referred to a conversion unlike ad click conversion directly translate to advertiser s revenue and thus provide a very concrete metric for return on advertising investment a typical behavioral targeting system face two main challenge the web scale amount of user history to process on a daily basis and the relative sparsity of conversion compared to click in a traditional setting these challenge call for generation of effective and efficient user profile most existing work use the historical intensity of a user s interest in various topic to model future interest in this paper we explore how the change in user behavior can be used to predict future action and show how it complement the traditional model of decaying interest and action recency to build a complete picture about the user interest and better predict conversion our evaluation over a real world set of campaign indicates that the combination of change of interest decaying intensity and action recency help in scoring significant improvement in optimizing for conversion over traditional baseline substantially improving the targeting efficiency for campaign with highly sparse conversion and highly reducing the overall history size used in targeting furthermore our technique have been deployed to production and scored a substantial improvement in targeting performance while imposing a negligible overhead in term of overall platform running time 
word ambiguity and vocabulary mismatch are critical problem in information retrieval to deal with these problem this paper proposes the use of translated word to enrich document representation going beyond the word in the original source language to represent a document in our approach each original document is automatically translated into an auxiliary language and the resulting translated document serf a a semantically enhanced representation for supplementing the original bag of word the core of our translation representation is the expected term frequency of a word in a translated document which is calculated by averaging the term frequency over all possible translation rather than focusing on the best translation only to achieve better efficiency of translation we do not rely on full fledged machine translation but instead use monotonic translation by removing the time consuming reordering component experiment carried out on standard trec test collection show that our proposed translation representation lead to statistically significant improvement over using only the original language of the document collection 
click through event in search result page serps are not reliable implicit indicator of document relevance a user s task and domain knowledge are key factor in recognition and link selection and the most useful serp document link may be those that best match the user s domain knowledge user study participant rated their knowledge of genomics mesh term before conducting trec genomics track task each participant s document knowledge wa represented by their knowledge of the indexing mesh term result show high intermediate and low domain knowledge group had similar document selection serp rank distribution serp link selection distribution varied when participant knowledge of the available document wa analyzed high domain knowledge participant usually selected a document with the highest personal knowledge rating low domain knowledge participant were reasonably successful at selecting available document of which they had the most knowledge while intermediate knowledge participant often failed to do so this evidence for knowledge effect on serp link selection may contribute to understanding the potential for personalization of search result ranking based on user domain knowledge 
news source on the web generate constant stream of information describing many aspect of the event that shape our world in particular geography play a key role in the news and enabling geographic retrieval of news article involves recognizing the textual reference to geographic location called toponym present in the article which can be difficult due to ambiguity in natural language toponym recognition in news is often accomplished with algorithm designed and tested around small corpus of news article but these static collection do not reflect the streaming nature of online news a evidenced by poor performance in test in contrast a method for toponym recognition is presented that is tuned for streaming news by leveraging a wide variety of recognition component both rule based and statistical an evaluation of this method show that it outperforms two prominent toponym recognition system when tested on large datasets of streaming news indicating it suitability for this domain 
entity on social system such a user on twitter and image on flickr are at the core of many interesting application they can be ranked in search result recommended to user or used in contextual advertising such application assume knowledge of an entity s nature and characteristic attribute an effective way to encode such knowledge is in the form of tag an untagged entity is practically inaccessible since it is hard to retrieve or interact with to address this some platform allow user to manually tag entity however while such tag can be informative they can oftentimes be inadequate trivial ambiguous or even plain false numerous automated tagging method have been proposed to address these issue however most of them require pre existing high quality tag or descriptive text for every entity that need to be tagged in our work we propose a method based on social endorsement that is free from such constraint virtually every major social networking platform allows user to endorse entity that they find appealing example include following twitter user or favoriting flickr photo these endorsement are abundant and directly capture the preference of user in this paper we pose and solve the problem of using the underlying social endorsement network to extract useful tag for entity in a social system our work leverage technique from topic modeling to capture the interest of user and then us them to extract relevant and descriptive tag for the entity they endorse we perform an extensive evaluation of our proposed approach on real large scale datasets from both twitter and flickr and show that it significantly outperforms meaningful and competitive baseline 
how can we best design web technology to support the feature we would like of our society such a openness justice transparency accountability participation innovation science and democracy 
in this paper we present two new algorithm designed to reduce the overall time required to process top k query these algorithm are based on the document at a time approach and modify the best baseline we found in the literature blockmax wand bmw to take advantage of a two tiered index in which the first tier is a small index containing only the higher impact entry of each inverted list this small index is used to pre process the query before accessing a larger index in the second tier resulting in considerable speeding up the whole process the first algorithm we propose named bmw c achieves higher performance but may result in small change in the top result provided in the final ranking the second algorithm named bmw t preserve the top result and while slower than bmw c it is faster than bmw in our experiment bmw c wa more than time faster than bmw when computing top result and while it doe not guarantee preserving the top result it preserved all ranking result evaluated at this level 
we propose and implement a robust text detection system which is a prominent step in for text retrieval in natural scene image or video our system includes several key component a fast and effective pruning algorithm is designed to extract maximally stable extremal region a character candidate using the strategy of minimizing regularized variation character candidate are grouped into text candidate by the single link clustering algorithm where distance weight and threshold of clustering are learned automatically by a novel self training distance metric learning algorithm the posterior probability of text candidate corresponding to non text are estimated with an character classifier text candidate with high probability are then eliminated and finally text are identified with a text classifier the proposed system is evaluated on the icdar robust reading competition dataset and a publicly available multilingual dataset the f measure are over and which are significantly better than the state of the art performance of and respectively 
with the recent advance in information network the problem of community detection ha attracted much attention in the last decade while network community detection ha been ubiquitous the task of collecting complete network data remains challenging in many real world application usually the collected network is incomplete with most of the edge missing commonly in such network all node with attribute are available while only the edge within a few local region of the network can be observed in this paper we study the problem of detecting community in incomplete information network with missing edge we first learn a distance metric to reproduce the link based distance between node from the observed edge in the local information region we then use the learned distance metric to estimate the distance between any pair of node in the network a hierarchical clustering approach is proposed to detect community within the incomplete information network empirical study on real world information network demonstrate that our proposed method can effectively detect community structure within incomplete information network 
personalized recommender system aim to push only the relevant item and information directly to the user without requiring them to browse through million of web resource the challenge of these system is to achieve a high user acceptance rate on their recommendation in this paper we aim to increase the user acceptance of recommendation by providing more intuitive tag based explanation of why the item are recommended tag are used a intermediary entity that not only relate target user to the recommended item but also understand user intent our system also allows tag based online relevance feedback experiment result on the movielens dataset show that the proposed approach is able to increase the acceptance rate of recommendation and improve user satisfaction 
a concept hierarchy created from a document collection can be used for query recommendation on intranet by ranking term according to the strength of their link to the query within the hierarchy a major limitation is that this model produce the same recommendation for identical query and rebuilding it from scratch periodically can be extremely inefficient due to the high computational cost we propose to adapt the model by incorporating query refinement from search log our intuition is that the concept hierarchy built from the collection and the search log provide complementary conceptual view on the same search domain and their integration should continually improve the effectiveness of recommended term two adaptation approach using query log with and without click information are compared we evaluate the concept hierarchy model static and adapted version built from the intranet collection of two academic institution and compare them with a state of the art log based query recommender the query flow graph built from the same log our adaptive model significantly outperforms it static version and the query flow graph when tested over a period of time on data document and search log from two institution intranet 
modern search engine are expected to make document searchable shortly after they appear on the ever changing web to satisfy this requirement the web is frequently crawled due to the sheer size of their index search engine distribute the crawled document among thousand of server in a scheme called local index partitioning such that each server index only several million page to ensure document from the same host e g www nytimes com are distributed uniformly over the server for load balancing purpose random routing of document to server is common to expedite the time document become searchable after being crawled document may be simply appended to the existing index partition however indexing by merely appending document result in larger index size since document reordering for index compactness is no longer performed this in turn degrades search query processing performance which depends heavily on index size a possible way to balance quick document indexing with efficient query processing is to deploy online document routing strategy that are designed to reduce index size this work considers the effect of several online document routing strategy on the aggregated partitioned index size we show that there exists a tradeoff between the compression of a partitioned index and the distribution of document from the same host across the index partition i e host distribution we suggest and evaluate several online routing strategy with regard to their compression host distribution and complexity in particular we present a term based routing algorithm which is shown analytically to provide better compression result than the industry standard random routing scheme in addition our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristic our finding are validated by experimental evaluation performed on a large benchmark collection of web page 
with the ever increasing speed of content turnover on the web it is particularly important to understand the pattern that page popularity follows this paper focus on the dynamical part of the web i e page that have a limited lifespan and experience a short popularity outburst within it we classify these page into five pattern based on how quickly they gain popularity and how quickly they lose it we study the property of page that belong to each pattern and determine content topic that contain disproportionately high fraction of particular pattern these development are utilized to create an algorithm that approximates with reasonable accuracy the expected popularity pattern of a web page based on it url and if available prior knowledge about it domain s topic 
search task comprising a series of search query serving the same information need have recently been recognized a an accurate atomic unit for modeling user search intent most prior research in this area ha focused on short term search task within a single search session and heavily depend on human annotation for supervised classification model learning in this work we target the identification of long term or cross session search task transcending session boundary by investigating inter query dependency learned from user searching behavior a semi supervised clustering model is proposed based on the latent structural svm framework and a set of effective automatic annotation rule are proposed a weak supervision to release the burden of manual annotation experimental result based on a large scale search log collected from bing com confirms the effectiveness of the proposed model in identifying cross session search task and the utility of the introduced weak supervision signal our learned model enables a more comprehensive understanding of user search behavior via search log and facilitates the development of dedicated search engine support for long term task 
over the past year semantic web and linked data technology have reached the backend of a considerable number of application consequently large amount of rdf data are constantly being made available across the planet while expert can easily gather information from this wealth of data by using the w c standard query language sparql most lay user lack the expertise necessary to proficiently interact with these application consequently non expert user usually have to rely on form query builder question answering or keyword search tool to access rdf data however these tool have so far been unable to explicate the query they generate to lay user making it difficult for these user to i ass the correctness of the query generated out of their input and ii to adapt their query or iii to choose in an informed manner between possible interpretation of their input this paper address this drawback by presenting sparql nl a generic approach that allows verbalizing sparql query i e converting them into natural language our framework can be integrated into application where lay user are required to understand sparql or to generate sparql query in a direct form query builder or an indirect keyword search question answering manner we evaluate our approach on the dbpedia question set provided by qald within a survey setting with both sparql expert and lay user the result of the filled survey show that sparql nl can generate complete and easily understandable natural language description in addition our result suggest that even sparql expert can process the natural language representation of sparql query computed by our approach more efficiently than the corresponding sparql query moreover non expert are enabled to reliably understand the content of sparql query 
existing multimedia recommenders suggest a specific type of multimedia item rather than item of different type personalized for a user based on his her preference assume that a user is interested in a particular family movie it is appealing if a multimedia recommendation system can suggest other movie music book and painting closely related to the movie we propose a comprehensive personalized multimedia recommendation system denoted mudrecs which make recommendation on movie music book and painting similar in content to other movie music book and or painting that a mudrecs user is interested in mudrecs doe not rely on user access pattern history connection information extracted from social networking site collaborated filtering method or user personal attribute such a gender and age to perform the recommendation task it simply considers the user rating genre role player author or artist and review of different multimedia item which are abundant and easy to find on the web mudrecs predicts the rating of multimedia item that match the interest of a user to make recommendation the performance ofmudrecs ha been compared with current state of the art multimedia recommenders using various multimedia datasets and the experimental result show that mudrecs significantly outperforms other system in accurately predicting the rating of multimedia item to be recommended 
in this paper we aim to develop a travelogue service that discovers and conveys various travelogue digest in form of theme location geographical scope traveling trajectory and location snippet to user in this service theme location in a travelogue are the core information to discover thus we aim to address the problem of theme location discovery to enable the above travelogue service due to the inherent ambiguity of location relevance we perform location relevance mining lrm in two complementary angle relevance classification and relevance ranking to provide comprehensive understanding of location furthermore we explore the textual e g surrounding word and geographical e g geographical relationship among location feature of location to develop a co training model for enhancement of classification performance built upon the mining result of lrm we develop a series of technique for provisioning of the aforementioned travelogue digest in our travelogue system finally we conduct comprehensive experiment on collected travelogue to evaluate the performance of our location relevance mining technique and demonstrate the effectiveness of the travelogue service 
session search is the information retrieval ir task that performs document retrieval for an entire session during a session user often change query to explore and investigate the information need in this paper we propose to use query change a a new form of relevance feedback for better session search evaluation conducted over trec session track show that query change is a highly effective form of feedback a compared with existing relevance feedback method the proposed method outperforms the state of the art relevance feedback method for the trec session track by a significant improvement of 
electronic medical record emrs are being increasingly used worldwide to facilitate improved healthcare service they describe the clinical decision process relating to a patient detailing the observed symptom the conducted diagnostic test the identified diagnosis and the prescribed treatment however medical record search is challenging due to the implicit knowledge inherent within the medical record such knowledge may be known by medical practitioner but hidden to an information retrieval ir system for instance the mention of a treatment such a a drug may indicate to a practitioner that a particular diagnosis ha been made even if this wa not explicitly mentioned in the patient s emrs moreover the fact that a symptom ha not been observed by a clinician may rule out some specific diagnosis our work focus on searching emrs to identify patient with medical history relevant to the medical condition s stated in a query the resulting system can be beneficial to healthcare provider administrator and researcher who may wish to analyse the effectiveness of a particular medical procedure to combat a specific disease during retrieval a healthcare provider may indicate a number of inclusion criterion to describe the type of patient of interest for example the used criterion may include personal profile e g age and gender or some specific medical symptom and test allowing to identify patient that have emrs matching the criterion to attain effective retrieval performance we hypothesise that in such a medical ir system both the information need and patient should be modelled based on how the medical process is developed specifically our thesis state that since the medical decision process typically encompasses four aspect symptom diagnostic test diagnosis and treatment a medical search system should take into account these aspect and apply inference to recover possible implicit knowledge we postulate that considering these aspect and their derived implicit knowledge at different level of the retrieval process namely sentence record and inter record level enhances the retrieval performance indeed we propose to build a query and patient understanding framework that can gain insight from emrs and query by modelling and reasoning during retrieval in term of the four aforementioned aspect symptom diagnostic test diagnosis and treatment at three different level of the retrieval process 
in semi automated text classification satc an automatic classifier f label a set of unlabelled document d following which a human annotator inspects and corrects when appropriate the label attributed by f to a subset d of d with the aim of improving the overall quality of the labelling an automated system can support this process by ranking the automatically labelled document in a way that maximizes the expected increase in effectiveness that derives from inspecting d an obvious strategy is to rank d so that the document that f ha classified with the lowest confidence are top ranked in this work we show that this strategy is suboptimal we develop a new utility theoretic ranking method based on the notion of inspection gain defined a the improvement in classification effectiveness that would derive by inspecting and correcting a given automatically labelled document we also propose a new effectiveness measure for satc oriented ranking method based on the expected reduction in classification error brought about by partially inspecting a list generated by a given ranking method we report the result of experiment showing that with respect to the baseline method above and according to the proposed measure our ranking method can achieve substantially higher expected reduction in classification error 
online content recommendation aim to identify trendy article in a continuously changing dynamic content pool most of existing work rely on online user feedback notably click a the objective and maximize it by showing article with highest click through rate recently click shaping wa introduced to incorporate multiple objective in a constrained optimization framework the work showed that significant tradeoff among the competing objective can be observed and thus it is important to consider multiple objective however the proposed click shaping approach is segment based and can only work with a few non overlapping user segment it remains a challenge of how to enable deep personalization in click shaping in this paper we tackle the challenge by proposing personalized click shaping the main idea is to work with the lagrangian duality formulation and explore strong convexity to connect dual and primal solution we show that our formulation not only allows efficient conversion from dual to primal for online personalized serving but also enables u to solve the optimization faster by approximation we conduct extensive experiment on a large real data set and our experimental result show that the personalized click shaping can significantly outperform the segmented one while achieving the same ability to balance competing objective 
cumulative citation recommendation refers to the task of filtering a time ordered corpus for document that are highly relevant to a predefined set of entity this task ha been introduced at the trec knowledge base acceleration track in where two main family of approach emerged classification and ranking in this paper we perform an experimental comparison of these two strategy using supervised learning with a rich feature set our main finding is that ranking outperforms classification on all evaluation setting and metric our analysis also reveals that a ranking based approach ha more potential for future improvement 
user search activity ha been used a implicit feedback to model search interest and improve the performance of search system in search engine this behavior usually take the form of query and result click however richer data on how people engage with search result can now be captured at scale creating new opportu nities to enhance search in this poster we focus on one type of newly observable behavior text selection event on search result caption we show that we can use text selection a implicit feedback to significantly improve search result relevance 
with the explosive growth of microblogging service short text message also known a tweet are being created and shared at an unprecedented rate tweet in it raw form can be incredibly informative but also overwhelming for both end user and data analyst it is a nightmare to plow through million of tweet which contain enormous noise and redundancy in this paper we study continuous tweet summarization a a solution to address this problem while traditional document summarization method focus on static and small scale data we aim to deal with dynamic quickly arriving and large scale tweet stream we propose a novel prototype called sumblr summarization by stream clustering for tweet stream we first propose an online tweet stream clustering algorithm to cluster tweet and maintain distilled statistic called tweet cluster vector then we develop a tcv rank summarization technique for generating online summary and historical summary of arbitrary time duration finally we describe a topic evolvement detection method which consumes online and historical summary to produce timeline automatically from tweet stream our experiment on large scale real tweet demonstrate the efficiency and effectiveness of our approach 
traditional information retrieval ir model use bag of word a the basic representation and assume that some form of independence hold between term representing term dependency and defining a scoring function capable of integrating such additional evidence is theoretically and practically challenging recently quantum theory qt ha been proposed a a possible more general framework for ir however only a limited number of investigation have been made and the potential of qt ha not been fully explored and tested we develop a new generalized language modeling approach for ir by adopting the probabilistic framework of qt in particular quantum probability could account for both single and compound term at once without having to extend the term space artificially a in previous study this naturally allows u to avoid the weight normalization problem which arises in the current practice by mixing score from matching compound term and from matching single term our model is the first practical application of quantum probability to show significant improvement over a robust bag of word baseline and achieves better performance on a stronger non bag of word baseline 
in on line review author often use a short passage to describe the overall feeling about a product or a service a review a a whole can mention many detail not in line with the overall feeling so capturing this key passage is important to understand the overall sentiment of the review this paper investigates the use of extractive summarisation in the context of sentiment classification the aim is to find the summary sentence or the short passage which give the overall sentiment of the review filtering out potential noisy information experiment on a movie review data set show that subjectivity detection play a central role in building summary for sentiment classification subjective extract carry the same polarity of the full text review while statistical and positional approach are not able to capture this aspect 
third party application apps drive the attractiveness of web and mobile application platform many of these platform adopt a decentralized control strategy relying on explicit user consent for granting permission that the apps request user have to rely primarily on community rating a the signal to identify the potentially harmful and inappropriate apps even though community rating typically reflect opinion about perceived functionality or performance rather than about risk with the arrival of html web apps such user consent permission system will become more widespread we study the effectiveness of user consent permission system through a large scale data collection of facebook apps chrome extension and android apps our analysis confirms that the current form of community rating used in app market today are not reliable indicator of privacy risk of an app we find some evidence indicating attempt to mislead or entice user into granting permission free application and application with mature content request more permission than is typical look alike application which have name similar to popular application also request more permission than is typical we also find that across all three platform popular application request more permission than average 
people and information are two core dimension in a social network people sharing information such a blog news album etc is the basic behavior in this paper we focus on predicting item level social influence to answer the question who should share what which can be extended into two information retrieval scenario user ranking given an item who should share it so that it diffusion range can be maximized in a social network web post ranking given a user what should she share to maximize her influence among her friend we formulate the social influence prediction problem a the estimation of a user post matrix in which each entry represents the strength of influence of a user given a web post we propose a hybrid factor non negative matrix factorization hf nmf approach for item level social influence modeling and devise an efficient projected gradient method to solve the hf nmf problem intensive experiment are conducted and demonstrate the advantage and characteristic of the proposed method 
collective intelligence which aggregate the shared information from large crowd is often negatively impacted by unreliable information source with the low quality data this becomes a barrier to the effective use of collective intelligence in a variety of application in order to address this issue we propose a probabilistic model to jointly ass the reliability of source and find the true data we observe that different source are often not independent of each other instead source are prone to be mutually influenced which make them dependent when sharing information with each other high dependency between source make collective intelligence vulnerable to the overuse of redundant and possibly incorrect information from the dependent source thus we reveal the latent group structure among dependent source and aggregate the information at the group level rather than from individual source directly this can prevent the collective intelligence from being inappropriately dominated by dependent source we will also explicitly reveal the reliability of group and minimize the negative impact of unreliable group experimental result on real world data set show the effectiveness of the proposed approach with respect to existing algorithm 
xml is the most used language for structuring data and document besides being the de facto standard for data exchange keyword based search ha been implemented by the xquery full text language extension allowing document fragment to be retrieved and ranked via keyword based matching in the information retrieval style in this demo the implementation of an xquery extension allowing user to express their vague knowledge of the underlying xml structure is presented the integration ha been performed on top of the basex query engine the work a initially done by panzeri at al in iir a a proof of concept ha been further enhanced and extended 
in the light of the web movement web based collaboration tool such a google doc have become mainstream and in the meantime serve million of user apart from established collaborative web application numerous web editor lack multi user support even though they are suitable for collaborative work enhancing these single user editor with shared editing capability is a costly endeavor since the implementation of a collaboration infrastructure accommodating conflict resolution document synchronization etc is required in this paper we present a generic transformation approach capable of converting single user web editor into multi user editor since our approach only requires the configuration of a generic collaboration infrastructure gci the effort to inject shared editing support is significantly reduced in contrast to conventional implementation approach neglecting reuse we also report on experimental result of a user study showing that converted editor meet user requirement with respect to software and collaboration quality moreover we define the characteristic that editor must adhere to in order to leverage the gci 
this paper present a user intent method to generate blacklist for collaborative cyberporn filtering a novel porn detection framework that find new pornographic web page by mining user search behavior is proposed it employ user click in search query log to select the suspected web page without extra human effort to label data for training and determines their category with the help of url host name and path information but without web page content we adopt an msn porn data set to explore the effectiveness of our method this user intent approach achieves high precision while maintaining favorably low false positive rate in addition real life filtering simulation reveals that our user intent method with it accumulative update strategy achieves of blocking rate while maintaining a steadily le than of over blocking rate 
the task of question answering qa is to find correct answer to user question expressed in natural language in the last few year non factoid qa received more attention it focus on causation manner and reason question where the expected answer ha the form of a passage of text the presence of question and answer corpus allows the adoption of learning to rank mlr algorithm in order to output a sensible ranking of the candidate answer the importance and effectiveness of linguistically motivated feature obtained from syntax lexical semantics and semantic role labeling wa shown in literature but there are still several different possible semantic feature that have not been taken into account so far and our goal is to find out if their use could lead to performance improvement in particular feature coming from semantic model sm like distributional semantic model dsms explicit semantic analysis esa latent dirichlet allocation lda induced topic have never been applied to the task so far based on the usefulness that those model show in other task we think that sm can have a significant role in improving current state of the art system performance in answer re ranking the question this research want to answer are do semantic feature bring information that is not present in the bag of word and syntactic feature do they bring different information or doe it overlap with that of other feature are additional semantic feature useful for answer re ranking doe their adoption improve system performance which of them is more effective and under which circumstance we performed a preliminary evaluation of dsms on the respubliqa dataset we built a dsm based answer scorer that represents the question and the answer a the sum of the vector of their term taken term term co occurrence matrix and calculates their cosine similarity we replaced the term term matrix with the one obtained by random indexing ri latent semantic analysis lsa and lsa over the ri considering each dsm on it own the result prove that all the dsms are better than the baseline the standard term term co occurrence matrix and the improvement is always significant the best improvement for the mrr in english is obtained by lsa while in italian by lsari we also showed that combining the dsms with overlap based measure via combsum the ranking is significantly better than the baseline obtained by the overlap measure alone for english we have obtained an improvement in mrr of about and for italian we achieve a even higher improvement in mrr of finally adopting ranknet for combining the overlap feature and the dsms feature improves the mrr of about more detail can be found in in order to investigate the effectiveness of the semantic feature we still need to incorporate other semantic feature such a esa lda and other state of the art linguistic feature other operator for semantic compositionality like product tensor product and circular convolution will also be investigated moreover we will experiment on different datasets focusing mainly on non factoid qa the yahoo answer manner question datasets are a good starting point a new dataset will also be collected with question from the user of wikiedi a qa system over wikipedia article www wikiedi it and answer in the form of paragraph from wikipedia page 
recommending phrase from web page for advertiser to bid on against search engine query is an important research problem with direct commercial impact most approach have found it infeasible to determine the relevance of all possible query to a given ad landing page and have focussed on making recommendation from a small set of phrase extracted and expanded from the page using nlp and ranking based technique in this paper we eschew this paradigm and demonstrate that it is possible to efficiently predict the relevant subset of query from a large set of monetizable one by posing the problem a a multi label learning task with each query being represented by a separate label we develop multi label random forest to tackle problem with million of label our proposed classifier ha prediction cost that are logarithmic in the number of label and can make prediction in a few millisecond using gb of ram we demonstrate that it is possible to generate training data for our classifier automatically from click log without any human annotation or intervention we train our classifier on ten of million of label feature and training point in le than two day on a thousand node cluster we develop a sparse semi supervised multi label learning formulation to deal with training set bias and noisy label harvested automatically from the click log this formulation is used to infer a belief in the state of each label for each training ad and the random forest classifier is extended to train on these belief rather than the given label experiment reveal significant gain over ranking and nlp based technique on a large test set of million ad using multiple metric 
the sigir workshop on modeling user behavior for information retrieval evaluation mube brings together people to discus existing and new approach way to collaborate and other idea and issue involved in improving information retrieval evaluation through the modeling of user behavior 
we propose clausie a novel clause based approach to open information extraction which extract relation and their argument from natural language text clausie fundamentally differs from previous approach in that it separate the detection of useful piece of information expressed in a sentence from their representation in term of extraction in more detail clausie exploit linguistic knowledge about the grammar of the english language to first detect clause in an input sentence and to subsequently identify the type of each clause according to the grammatical function of it constituent based on this information clausie is able to generate high precision extraction the representation of these extraction can be flexibly customized to the underlying application clausie is based on dependency parsing and a small set of domain independent lexica operates sentence by sentence without any post processing and requires no training data whether labeled or unlabeled our experimental study on various real world datasets suggests that clausie obtains higher recall and higher precision than existing approach both on high quality text a well a on noisy text a found in the web 
trust inference which is the mechanism to build new pair wise trustworthiness relationship based on the existing one is a fundamental integral part in many real application e g e commerce social network peer to peer network etc state of the art trust inference approach mainly employ the transitivity property of trust by propagating trust along connected user a k a trust propagation but largely ignore other important property e g prior knowledge multi aspect etc in this paper we propose a multi aspect trust inference model by exploring an equally important property of trust i e the multi aspect property the heart of our method is to view the problem a a recommendation problem and hence open the door to the rich methodology in the field of collaborative filtering the proposed multi aspect model directly characterizes multiple latent factor for each trustor and trustee from the locally generated trust relationship moreover we extend this model to incorporate the prior knowledge a well a trust propagation to further improve inference accuracy we conduct extensive experimental evaluation on real data set which demonstrate that our method achieves significant improvement over several existing benchmark approach overall the proposed method matri lead to improvement over it best known competitor in prediction accuracy and up to order of magnitude speedup with linear scalability 
search result diversification ha recently gained attention a a mean to tackle ambiguous query while query ambiguity is of particular concern for the short query commonly observed in a web search scenario it is unclear how much diversity is actually promoted by web search engine w in this paper we ass the diversification performance of two leading w in the context of the diversity task of the trec and web track our result show that these w perform effectively for query with multiple interpretation but not for those open to multiple aspect related to a single interpretation moreover by deploying a state of the art diversification approach based on query suggestion from these w themselves we show that their diversification performance can be further improved 
twitter or the world of character pose serious challenge to the efficacy of topic model on short messy text while topic model such a latent dirichlet allocation lda have a long history of successful application to news article and academic abstract they are often le coherent when applied to microblog content like twitter in this paper we investigate method to improve topic learned from twitter content without modifying the basic machinery of lda we achieve this through various pooling scheme that aggregate tweet in a data preprocessing step for lda we empirically establish that a novel method of tweet pooling by hashtags lead to a vast improvement in a variety of measure for topic coherence across three diverse twitter datasets in comparison to an unmodified lda baseline and a variety of pooling scheme an additional contribution of automatic hashtag labeling further improves on the hashtag pooling result for a subset of metric overall these two novel scheme lead to significantly improved lda topic model on twitter content 
in this paper we consider the problem of estimating the relative expertise score of user in community question and answering service cqa previous approach typically only utilize the explicit question answering relationship between asker and an swerers and apply link analysis to address this problem the im plicit pairwise comparison between two user that is implied in the best answer selection is ignored given a question and answering thread it s likely that the expertise score of the best answerer is higher than the asker s and all other non best answerer the goal of this paper is to explore such pairwise comparison inferred from best answer selection to estimate the relative expertise score of user formally we treat each pairwise comparison between two user a a two player competition with one winner and one loser two competition model are proposed to estimate user expertise from pairwise comparison using the ntcir cqa task data with million question and introducing answer quality prediction based evaluation metric the experimental result show that the pairwise comparison based competition model significantly outperforms link analysis based approach pagerank and hit and pointwise approach number of best answer and best answer ratio for estimating the expertise of active user furthermore it s shown that pairwise comparison based competi tion model have better discriminative power than other method it s also found that answer quality best answer is an important factor to estimate user expertise 
increasingly web recommender system face scenario where they need to serve suggestion to group of user for example when family share e commerce or movie rental web account research to date in this domain ha proposed two approach computing recommendation for the group by merging any member rating into a single profile or computing ranked recommendation for each individual that are then merged via a range of heuristic in doing so none of the past approach reason on the preference that arise in individual when they are member of a group in this work we present a probabilistic framework based on the notion of information matching for group recommendation this model defines group relevance a a combination of the item s relevance to each user a an individual and a a member of the group it can then seamlessly incorporate any group recommendation strategy in order to rank item for a set of individual we evaluate the model s efficacy at generating recommendation for both single individual and group using the movielens and moviepilot data set in both case we compare our result with baseline and state of the art collaborative filtering algorithm and show that the model outperforms all others over a variety of ranking metric 
finding expert in question answering platform ha important application such a question routing or identification of best answer addressing the problem of ranking user with respect to their expertise we propose competition based expertise network cben a novel community expertise network structure based on the principle of competition among the answerer of a question we evaluate our approach on a very large dataset from yahoo answer using a variety of centrality measure we show that it outperforms state of the art network structure and unlike previous method is able to consistly outperform simple metric like best answer count we also analyse question answering forum in yahoo answer and show that they can be characterised by factual or subjective information seeking behavior social discussion and the conducting of poll or survey we find that the ability to identify expert greatly depends on the type of forum which is directly reflected in the structural property of the expertise network 
planner and social psychologist have suggested that the recognizability of the urban environment is linked to people s socio economic well being we build a web game that put the recognizability of london s street to the test it follows a closely a possible one experiment done by stanley milgram in the game pick up random location from google street view and test user to see if they can judge the location in term of closest subway station borough or region each participant dedicates only few minute to the task a opposed to minute in milgram s we collect data from participant one order of magnitude a larger sample and build a recognizability map of london based on their response we find that some borough have little cognitive representation that recognizability of an area is explained partly by it exposure to flickr and foursquare user and mostly by it exposure to subway passenger and that area with low recognizability do not fare any worse on the economic indicator of income education and employment but they do significantly suffer from social problem of housing deprivation poor living condition and crime these result could not have been produced without analyzing life offand online that is without considering the interaction between urban place in the physical world and their virtual presence on platform such a flickr and foursquare this line of work is at the crossroad of two emerging theme in computing research a crossroad where web science meet the smart city agenda 
the query performance prediction qpp task is estimating retrieval effectiveness with no relevance judgment we present a novel probabilistic framework for qpp that give rise to an important aspect that wa not addressed in previous work namely the extent to which the query effectively represents the information need for retrieval accordingly we devise a few query representativeness measure that utilize relevance language model experiment show that integrating the most effective measure with state of the art predictor in our framework often yield prediction quality that significantly transcends that of using the predictor alone 
the reliability of a test collection is proportional to the number of query it contains but building a collection with many query is expensive so researcher have to find a balance between reliability and cost previous work on the measurement of test collection reliability relied on data based approach that contemplated random what if scenario and provided indicator such a swap rate and kendall tau correlation generalizability theory wa proposed a an alternative founded on analysis of variance that provides reliability indicator based on statistical theory however these reliability indicator are hard to interpret in practice because they do not correspond to well known indicator like kendall tau correlation we empirically established these relationship based on data from over trec collection thus filling the gap in the practical interpretation of generalizability theory we also review the computation of these indicator and show that they are extremely dependent on the sample of system and query used so much that the required number of query to achieve a certain level of reliability can vary in order of magnitude we discus the computation of confidence interval for these statistic providing a much more reliable tool to measure test collection reliability reflecting upon all these result we review a wealth of trec test collection arguing that they are possibly not a reliable a generally accepted and that the common choice of query is insufficient even for stable ranking 
people are seldom aware that their search query frequently mismatch a majority of the relevant document this may not be a big problem for topic with a large and diverse set of relevant document but would largely increase the chance of search failure for le popular search need we aim to address the mismatch problem by developing accurate and simple query that require minimal effort to construct this is achieved by targeting retrieval intervention at the query term that are likely to mismatch relevant document for a given topic the proportion of relevant document that do not contain a term measure the probability for the term to mismatch relevant document or the term mismatch probability recent research demonstrates that this probability can be estimated reliably prior to retrieval typically it is used in probabilistic retrieval model to provide query dependent term weight this paper develops a new use automatic diagnosis of term mismatch a search engine can use the diagnosis to suggest manual query reformulation guide interactive query expansion guide automatic query expansion or motivate other response the research described here us the diagnosis to guide interactive query expansion and create boolean conjunctive normal form cnf structured query that selectively expand problem query term while leaving the rest of the query untouched experiment with trec ad hoc and legal track datasets demonstrate that with high quality manual expansion this diagnostic approach can reduce user effort by and produce simple and effective structured query that surpass their bag of word counterpart 
web retrieval method have evolved through three major step in the last decade or so they started from standard document centric ir in the early day of the web then made a major step forward by leveraging the structure of the web using link analysis technique in both crawling and ranking challenge a more recent no le important but maybe more discrete step forward ha been to enter the user in this equation in two way implicitly through the analysis of usage data captured by query log and session and click information in general the goal here being to improve ranking a well a to measure user s happiness and engagement explicitly by offering novel interactive feature the goal here being to better answer user need this half day tutorial cover the user related challenge associated with the implicit and explicit role of user in web retrieval more specifically we review and discus challenge associated with usage data analysis and metric it is critical to monitor how user take advantage and interact with web retrieval system a this implicit relevant feedback aggregated at a large scale can provide insight on user underlying intent a well a approximate quite accurately the level of success of a given feature here we have to consider not only click statistic the sequence of query the time spent in a page the number of action per session etc this is the focus of the first part of the tutorial user interaction given the intrinsic problem posed by the web the key challenge for the user is to conceive a good query to be submitted to the search system one that lead to a manageable and relevant answer the retrieval system must assist user during two key stage of interaction efore the query is fully expressed and after the result are returned after quite some stagnation on the front end of web retrieval we have seen numerous novel interactive feature appear in the last to year a the leading commercial search engine seem to compete for user attention the second part of the tutorial will be dedicated to explicit user interaction we will introduce novel material a compared to previous version of this tutorial that were given at sigir wsdm and ecir in order to reflect recent web search feature such a google instant or yahoo direct search the goal of this tutorial is to teach the key principle and technology behind the activity and challenge briefly outlined above bring new understanding and insight to the attendee and hopefully foster future research a previous version of this tutorial wa offered at the acm sigir wsdm and ecir 
method for fusing document list that were retrieved in response to a query often use retrieval score or rank of document in the list we present a novel probabilistic fusion approach that utilizes an additional source of rich information namely inter document similarity specifically our model integrates information induced from cluster of similar document created across the list with that produced by some fusion method that relies on retrieval score rank empirical evaluation show that our approach is highly effective for fusion for example the performance of our model is consistently better than that of the standard effective fusion method that it integrates the performance also transcends that of standard fusion of re ranked list where list re ranking is based on cluster created from document in the list 
the complex and dynamic nature of search process surrounding information seeking have been exhaustively studied recent study have highlighted search process with different intention such a those for entertainment purpose or re finding a visited information object are fundamentally different in nature to typical information seeking intention despite the popularity of such search process on the web they have not yet been thoroughly explored using a video retrieval system a a use case we study the characteristic of four different search task type seeking information re finding a particular information object and two different entertainment intention i e entertainment by adjusting arousal level and entertainment by adjusting mood in particular we looked at the cognition emotion and action aspect of these search task at different phase of a search process this follows the common assumption in the information seeking and retrieval community that a complex search process can be broken down into a relatively small number of activity phase our experimental result show significant difference in the characteristic of studied search task furthermore we investigate whether we can predict these search task given user s interaction with the system result show that we can learn a model that predicts the search task type with reasonable accuracy overall these finding may help to steer search engine to better satisfy searcher need beyond typically assumed information seeking process 
this paper examines the problem of social collaborative filtering cf to recommend item of interest to user in a social network setting unlike standard cf algorithm using relatively simple user and item feature recommendation in social network pose the more complex problem of learning user preference from a rich and complex set of user profile and interaction information many existing social cf method have extended traditional cf matrix factorization but have overlooked important aspect germane to the social setting we propose a unified framework for social cf matrix factorization by introducing novel objective function for training our new objective function have three key feature that address main drawback of existing approach a we fully exploit feature based user similarity b we permit direct learning of user to user information diffusion and c we leverage co preference dis agreement between two user to learn restricted area of common interest we evaluate these new social cf objective comparing them to each other and to a variety of social cf baseline and analyze user behavior on live user trial in a custom developed facebook app involving data collected over five month from over app user and their friend 
aspect based opinion mining from online review ha attracted a lot of attention recently the main goal of all of the proposed method is extracting aspect and or estimating aspect rating recent work which are often based on latent dirichlet allocation lda consider both task simultaneously these model are normally trained at the item level i e a model is learned for each item separately learning a model per item is fine when the item ha been reviewed extensively and ha enough training data however in real life data set such a those from epinions com and amazon com more than of item have le than review so called cold start item state of the art lda model for aspect based opinion mining are trained at the item level and therefore perform poorly for cold start item due to the lack of sufficient training data in this paper we propose a probabilistic graphical model based on lda called factorized lda flda to address the cold start problem the underlying assumption of flda is that aspect and rating of a review are influenced not only by the item but also by the reviewer it further assumes that both item and reviewer can be modeled by a set of latent factor which represent their aspect and rating distribution different from state of the art lda model flda is trained at the category level and learns the latent factor using the review of all the item of a category in particular the non cold start item and us them a prior for cold start item our experiment on three real life data set demonstrate the improved effectiveness of the flda model in term of likelihood of the held out test set we also evaluate the accuracy of flda based on two application oriented measure 
search system use context to effectively satisfy a user s information need a expressed by a query task are important factor in determining user context during search and many study have been conducted that identify task and task stage through user interaction behavior with search system the type of interaction available to user however depends on the type of search interface feature available query are the most pervasive input from user to express their information need regardless of the input method e g typing keywords or clicking facet instead of characterizing interaction behavior in term of interface specific component we propose to characterize user search behavior in term of two type of query modification i direct modification which refers to reformulations of query and ii indirect modification which refers to user operation on additional input component provided by various search interface we investigate the utility of characterizing task stage through direct and indirect query reformulations in a case study and find that it is possible to effectively differentiate subsequent stage of the search task we found that describing user interaction behavior in such a generic form allowed u to relate user action to search task stage independent from the specific search interface deployed the next step will then be to validate this idea in a setting with a wider palette of search task and tool 
the interest of user in handheld device is strongly related to their location therefore the user location is important a a user context for news article recommendation in a mobile environment this paper proposes a novel news article recommendation that reflects the geographical context of the user for this purpose we propose the explicit localized semantic analysis elsa an esa based topical representation of document every location ha it own geographical topic which can be captured from the geo tagged document related to the location thus not only news article but location are also represented a topic vector the main advantage of elsa is that it stress only the topic that are relevant to a given location whereas all topic are equally important in esa a a result geographical topic have different importance according to the user location in elsa even if they come from the same article another advantage of elsa is that it allows a simple comparison of the user location and news article because it project both location and article onto an identical space composed of wikipedia topic in the evaluation of elsa with the new york time corpus it outperformed two simple baseline of bag of word and lda a well a two esa based method rt of elsa wa improved up to over other method and it ndcg k wa always higher than those of the others regardless of k 
the first computer were people today internet based access to online human crowd ha led to a renaissance of research in human computation and the advent of crowdsourcing these new opportunity have brought a disruptive shift to research and practice for how we build intelligent system today not only can labeled data for training and evaluation be collected faster cheaper and easier than ever before but we now see human computation being integrated into the system themselves operating in concert with automation this tutorial introduces opportunity and challenge of human computation and crowdsourcing particularly for search evaluation and developing hybrid search solution that integrate human computation with traditional form of automated search we review methodology and finding of recent research and survey current generation crowdsourcing platform now available analyzing method potential and limitation across platform 
content reuse is extremely common in user generated medium reuse detection serf a be the basis for many application however along with the explosion of internet and continuously growing us of user generated medium the task becomes more critical and difficult in this paper we present a novel efficient and scalable approach to detect content reuse we propose a new signature generation algorithm which is based on learned hash function for word in order to deal with ten of billion of document we implement the detection approach on graphical processing unit gpus the experimental comparison in this paper involves study of efficiency and effectiveness of the proposed approach in different type of document collection including clueweb tweet and so on experimental result show that the proposed approach can achieve the same detection rate with state of the art system while us significantly le execution time than them from x to x speedup 
an important challenge in cluster based document retrieval is ranking document cluster by their relevance to the query we present a novel cluster ranking approach that utilizes markov random field mrfs mrfs enable the integration of various type of cluster relevance evidence e g the query similarity value of the cluster s document and query independent measure of the cluster we use our method to re rank an initially retrieved document list by ranking cluster that are created from the document most highly ranked in the list the resultant retrieval effectiveness is substantially better than that of the initial list for several list that are produced by effective retrieval method furthermore our cluster ranking approach significantly outperforms state ofthe art cluster ranking method we also show that our method can be used to improve the performance of state ofthe art result diversification method 
nowadays child a young a two year old can easily interact with mobile touch screen device and personal computer to watch online video through service such a youtube however such service present a number of challenge for young child e g fine grain gesture interaction and good typing literacy skill in addition when child use such service there is a risk that they may stumble upon content that is inappropriate yoosee is a web based application developed using the puppyir framework and designed for child aged between two and six year old yoosee enables child to search and browse through video content using an engaging novel interaction paradigm and be able to safely enjoy moderated video content 
we proposed a method to classify song in the million song dataset according to song genre since song have several data type we trained sub classifier by different type of data these sub classifier are combined using both classifier authority and classification confidence for a particular instance in the experiment the combined classifier surpasses all of these sub classifier and the svm classifier using concatenated vector from all data type finally the genre label for the million song dataset are provided 
we study how an advertiser change his her bid price in sponsored search by modeling his her rationality predicting the bid change of advertiser with respect to their campaign performance is a key capability of search engine since it can be used to improve the offline evaluation of new advertising technology and the forecast of future revenue of the search engine previous work on advertiser behavior modeling heavily relies on the assumption of perfect advertiser rationality however in most case this assumption doe not hold in practice advertiser may be unwilling incapable and or constrained to achieve their best response in this paper we explicitly model these limitation in the rationality of advertiser and build a probabilistic advertiser behavior model from the perspective of a search engine we then use the expected payoff to define the objective function for an advertiser to optimize given his her limited rationality by solving the optimization problem with monte carlo we get a prediction of mixed bid strategy for each advertiser in the next period of time we examine the effectiveness of our model both directly using real historical bid and indirectly using revenue prediction and click number prediction our experimental result based on the sponsored search log from a commercial search engine show that the proposed model can provide a more accurate prediction of advertiser bid behavior than several baseline method 
the problem of identifying user intent ha received considerable attention in recent year particularly in the context of improving the search experience via query contextualization intent can be characterized by multiple dimension which are often not observed from query word alone accurate identification of intent from query word remains a challenging problem primarily because it is extremely difficult to discover these dimension the problem is often significantly compounded due to lack of representative training sample we present a generic extensible framework for learning the multi dimensional representation of user intent from the query word the approach model the latent relationship between facet using tree structured distribution which lead to an efficient and convergent algorithm fastq for identifying the multi faceted intent of user based on just the query word we also incorporated wordnet to extend the system capability to query which contain word that do not appear in the training data empirical result show that fastq yield accurate identification of intent when compared to a gold standard 
similarity search is a key challenge for multimedia retrieval application where data are usually represented in high dimensional space among various algorithm proposed for similarity search in high dimensional space locality sensitive hashing lsh is the most popular one which recently ha been extended to kernelized locality sensitive hashing klsh by exploiting kernel similarity for better retrieval efficacy typically klsh work only with a single kernel which is often limited in real world multimedia application where data may originate from multiple resource or can be represented in several different form for example in content based multimedia retrieval a variety of feature can be extracted to represent content of an image to overcome the limitation of regular klsh we propose a novel boosting multi kernel locality sensitive hashing bmklsh scheme that significantly boost the retrieval performance of klsh by making use of multiple kernel we conduct extensive experiment for large scale content based image retrieval in which encouraging result show that the proposed method outperforms the state of the art technique 
recent advance in music retrieval and recommendation algorithm highlight the necessity to follow multimodal approach in order to transcend limit imposed by method that solely use audio web or collaborative filtering data in this paper we propose hybrid music recommendation algorithm that combine information on the music content the music context and the user context in particular integrating location aware weighting of similarity using state of the art technique to extract audio feature and contextual web feature and a novel standardized data set of music listening activity inferred from microblogs musicmicro we propose several multimodal retrieval function the main contribution of this paper are i a systematic evaluation of mixture coefficient between state of the art audio feature and web feature using the first standardized microblog data set of music listening event for retrieval purpose and ii novel geospatial music recommendation approach using location information of microblog user and a comprehensive evaluation thereof 
given an ambiguous or underspecified query search result diversification aim at accomodating different user intent within a single entry point result page however some intent are informational for which many relevant page may help while others are navigational for which only one web page is required we propose new evaluation metric for search result diversification that considers this distinction a well a a simple method for comparing the intuitiveness of a given pair of metric quantitatively our main experimental finding are a in term of discriminative power which reflects statistical reliability the proposed metric din ndcg and p q are comparable to intent recall and d ndcg and possibly superior to ndcg b in term of preference agreement with intent recall p q is superior to other diversity metric and therefore may be the most intuitive a a metric that emphasis diversity and c in term of preference agreement with effective precision din ndcg is superior to other diversity metric and therefore may be the most intuitive a a metric that emphasis relevance moreover din ndcg may be the most intuitive a a metric that considers both diversity and relevance in addition we demonstrate that the randomised tukey s honestly significant difference test that take the entire set of available run into account is substantially more conservative than the paired bootstrap test that only considers one run pair at a time and therefore recommend the former approach for significance testing when a set of run is available for evaluation 
the one click access task click of ntcir requires system to return a concise multi document summary of web page in response to a query which is assumed to have been submitted in a mobile context system are evaluated based on information unit or iunits and are required to present important piece of information first and to minimise the amount of text the user ha to read using the official japanese result of the second round of the click task from ntcir we discus our task setting and evaluation framework our analysis show that simple baseline method that leverage search engine snippet or wikipedia are effective for lookup type query but not necessarily for other query type there is still a substantial gap between manual and automatic run and our evaluation metric are relatively robust to the incompleteness of iunits 
crowdsourcing is now widely used to replace judgement or evaluation by an expert authority with an aggregate evaluation from a number of non expert in application ranging from rating and categorizing online content all the way to evaluation of student assignment in massively open online course moocs via peer grading a key issue in these setting where direct monitoring of both effort and accuracy is infeasible is incentivizing agent in the crowd to put in effort to make good evaluation a well a to truthfully report their evaluation we study the design of mechanism for crowdsourced judgement elicitation when worker strategically choose both their report and the effort they put into their evaluation this lead to a new family of information elicitation problem with unobservable ground truth where an agent s proficiency the probability with which she correctly evaluates the underlying ground truth is endogenously determined by her strategic choice of how much effort to put into the task our main contribution is a simple new mechanism for binary information elicitation for multiple task when agent have endogenous proficiency with the following property i exerting maximum effort followed by truthful reporting of observation is a nash equilibrium ii this is the equilibrium with maximum payoff to all agent even when agent have different maximum proficiency can use mixed strategy and can choose a different strategy for each of their task our information elicitation mechanism requires only minimal bound on the prior asks agent to only report their own evaluation and doe not require any condition on a diverging number of agent report per task to achieve it incentive property the main idea behind our mechanism is to use the presence of multiple task and rating to estimate a reporting statistic to identify and penalize low effort agreement the mechanism reward agent for agreeing with another reference report on the same task but also penalizes for blind agreement by subtracting out this statistic term designed so that agent obtain reward only when they put in effort into their observation 
we introduce a new representation of the inverted index that performs faster ranked union and intersection while using le space our index is based on the treap data structure which allows u to intersect merge the document identifier while simultaneously thresholding by frequency instead of the costlier two step classical processing method to achieve compression we represent the treap topology using compact data structure further the treap invariant allow u to elegantly encode differentially both document identifier and frequency result show that our index us about le space and performs query up to three time faster than state of the art compact representation 
we describe an innovative and scalable recommendation system successfully deployed at ebay to build recommenders for long tail marketplace requires projection of volatile item into a persistent space of latent product we first present a generative clustering model for collection of unstructured heterogeneous and ephemeral item data under the assumption that item are generated from latent product an item is represented a a vector of independently and distinctly distributed variable while a latent product is characterized a a vector of probability distribution respectively the probability distribution are chosen a natural stochastic model for different type of data the learning objective is to maximize the total intra cluster coherence measured by the sum of log likelihood of item under such a generative process in the space of latent product robust recommendation can then be derived using naive bayes for ranking from historical transactional data item based recommendation are achieved by inferring latent product from unseen item in particular we develop a probabilistic scoring function of recommended item which take into account item product membership product purchase probability and the important auction end time factor with the holistic probabilistic measure of a prospective item purchase one can further maximize the expected revenue and the more subjective user satisfaction a well we evaluated the latent product clustering and recommendation ranking model using real world e commerce data from ebay in both form of offline simulation and online a b testing in the recent production launch our system yielded fold improvement over the existing production system in click through purchase through and gross merchandising value thus now driving related recommendation traffic with billion of item at ebay we believe that this work provides a practical yet principled framework for recommendation in the domain with affluent user self input data 
this paper proposes a method based on conditional random field to incorporate sentence structure syntax and semantics and context information to identify sentiment of sentence within a document it also proposes and evaluates two different active learning strategy for labeling sentiment data the experiment with the proposed approach demonstrate a improvement in accuracy on amazon customer review compared to existing supervised learning and rule based method 
we propose a novel model for landmark discovery that locates region based landmark on map in contrast to the traditional point based landmark the proposed method preserve more information and automatically identifies candidate region on map by crowdsourcing geo referenced photo gaussian kernel convolution is applied to remove noise and generate detected region we adopt f measure to evaluate discovered landmark and manually check the association between tag and region the experiment result show that more than of attraction in the selected city can be correctly located by this method 
we introduce a novel sentence ranking problem called explanatory sentence extraction ese which aim to rank sentence in opinionated text based on their usefulness for helping user understand the detailed reason of sentiment i e explanatoriness we propose and study several general method for scoring the explanatoriness of a sentence we create new data set and propose a new measure for evaluation experiment result show that the proposed method are effective outperforming a state of the art sentence ranking method for standard text summarization 
statistical translation model and latent semantic analysis lsa are two effective approach to exploit click through data for web search ranking this paper present two document ranking model that combine both approach by explicitly modeling word pair the first model called pairmodel is a monolingual ranking model based on word pair that are derived from click through data it map query and document into a concept space spanned by these word pair the second model called bilingual paired topic model bptm us bilingual word pair and jointly model a bilingual query document collection this model map query and document in multiple language into a lower dimensional semantic subspace experimental result on web search task show that they significantly outperform the state of the art baseline model and the best result is obtained by interpolating pairmodel and bptm 
effort such a wikipedia have shown the ability of user community to collect organize and curate information on the internet recently a number of question and answer q a site have successfully built large growing knowledge repository each driven by a wide range of question and answer from it user community while site like yahoo answer have stalled and begun to shrink one site still going strong is quora a rapidly growing service that augments a regular q a system with social link between user despite it success however little is known about what drive quora s growth and how it continues to connect visitor and expert to the right question a it grows in this paper we present result of a detailed analysis of quora using measurement we shed light on the impact of three different connection network or graph inside quora a graph connecting topic to user a social graph connecting user and a graph connecting related question our result show that heterogeneity in the user and question graph are significant contributor to the quality of quora s knowledge base one drive the attention and activity of user and the other directs them to a small set of popular and interesting question 
similarity search application with a large amount of text and image data demand an efficient and effective solution one useful strategy is to represent the example in database a compact binary code through semantic hashing which ha attracted much attention due to it fast query search speed and drastically reduced storage requirement all of the current semantic hashing method only deal with the case when each example is represented by one type of feature however example are often described from several different information source in many real world application for example the characteristic of a webpage can be derived from both it content part and it associated link to address the problem of learning good hashing code in this scenario we propose a novel research problem composite hashing with multiple information source chmis the focus of the new research problem is to design an algorithm for incorporating the feature from different information source into the binary hashing code efficiently and effectively in particular we propose an algorithm chmis aw chmis with adjusted weight for learning the code the proposed algorithm integrates information from several different source into the binary hashing code by adjusting the weight on each individual source for maximizing the coding performance and enables fast conversion from query example to their binary hashing code experimental result on five different datasets demonstrate the superior performance of the proposed method against several other state of the art semantic hashing technique 
display advertising is the graphical advertising on the world wide web www that appears next to content on web page instant messaging im application email etc over the past decade display ad have evolved from simple banner and pop up ad to include various combination of text image audio video and animation a a market segment display continues to show substantial growth potential a evidenced by company such a microsoft yahoo and google actively vying for market share a a sale process display ad are typically sold in package the result of negotiation between sale and advertising agent a key component to any successful business model in display advertising is sound pricing main objective for on line publisher e g amazon youtube cnn are maximizing revenue while managing their available inventory appropriately and pricing must reflect these consideration this paper address the problem of maximizing revenue by adjusting price of display inventory we cast this a an inventory allocation problem our formal objective a maximizes revenue using b iterative price adjustment in the direction of the gradient of an appropriately constructed lagrangian relaxation we show that our optimization approach drive the revenue towards local maximum under mild condition on the property of the unknown demand curve the major unknown for optimizing revenue in display environment is how the demand for display ad change to price the classical demand curve this we address directly by way of a factorial pricing experiment this enables u to estimate the gradient of the revenue function with respect to inventory price overall the result is a principled risk aware and empirically efficient methodology this paper is based on research undertaken on behalf of one of google s client 
we present the chatnoir search engine which index the entire english part of the clueweb corpus besides carnegie mellon s indri system chatnoir is the second publicly available search engine for this corpus it implement the classic bm f information retrieval model including pagerank and spam likelihood the search engine is scalable and return the first result within three second which is significantly faster than indri a convenient api allows for implementing reproducible experiment based on retrieving document from the clueweb corpus the search engine ha successfully accomplished a load test involving query 
with it close tie to the web the ir community is destined to leverage the dissemination and collaboration capability that the web provides today especially with the advent of the software a a service principle an ir community is conceivable that publishes experiment executable by anyone over the web a review of recent sigir paper show that we are far away from this vision of collaboration the benefit of publishing ir experiment a a service are striking for the community a a whole and include potential to boost research profile and reputation however the additional work must be kept to a minimum and sensitive data must be kept private for this paradigm to become an accepted practice to foster experiment a a service in ir we present a web framework for experiment that address the outlined challenge and posse a unique set of compelling feature in comparison to existing solution we also describe how our reference implementation is already used officially a an evaluation platform for an established international plagiarism detection competition 
using relevance feedback can significantly improve ad hoc retrieval effectiveness yet if little feedback is available effectively exploiting it is a challenge to that end we present a novel approach that utilizes document passage empirical evaluation demonstrates the merit of the approach 
sponsored search is a form of online advertising where advertiser bid for placement next to search engine result for specific keywords a search engine compete for the growing share of online ad spend it becomes important for them to understand what keywords advertiser value most and what characteristic of keywords drive value in this paper we propose an approach to keyword value prediction that draw on advertiser bidding behavior across the term and campaign in an account we provide original insight into the structure of sponsored search account that motivate the use of a hierarchical modeling strategy we propose an economically meaningful loss function which allows u to implicitly fit a linear model for value given observables such a bid and click through rate the model draw on demographic and textual feature of keywords and take advantage of the hierarchical structure of sponsored search account it predictive quality is evaluated on several high revenue and high exposure advertising account on a major search engine besides the general evaluation of advertiser welfare our approach ha potential application to keyword and bid suggestion 
the maximum entropy method provides one technique for validating search engine effectiveness measure under this method the value of an effectiveness measure is used a a constraint to estimate the most likely distribution of relevant document under a maximum entropy assumption this inferred distribution may then be compared to the actual distribution to quantify the informativeness of the measure the inferred distribution may also be used to estimate value for other effectiveness measure previous work focused on traditional effectiveness measure such a average precision in this paper we extend the maximum entropy method to the newer cascade and intent aware effectiveness measure by considering the dependency of the document ranked in a result list these measure are intended to reflect the novelty and diversity of search result in addition to the traditional relevance our result indicate that intent aware measure based on the cascade model are informative in term of both inferring actual distribution and predicting the value of other retrieval measure 
we propose a new method for query biased multi document summarization based on sentence extraction the summary of multiple document is created in two step sentence are first clustered where each cluster corresponds to one of the main theme present in the collection inside each theme sentence are then ranked using a transductive learning to rank algorithm based on ranknet in order to better identify those which are relevant to the query the final summary contains the top ranked sentence of each theme our approach is validated on duc and duc datasets 
when searching for entity with a strong local character e g a museum people may also be interested in discovering proximal activity related entity e g a caf geographical proximity is a necessary but not sufficient qualifier for recommending other entity such that they are related in a useful manner e g interest in a fish market doe not imply interest in nearby bookshop but interest in other produce store is more likely we describe and evaluate method to identify such activity related local entity 
dynamic pruning strategy permit efficient retrieval by not fully scoring all posting of the document matching a query without degrading the retrieval effectiveness of the top ranked result however the amount of pruning achievable for a query can vary resulting in query taking different amount of time to execute knowing in advance the execution time of query would permit the exploitation of online algorithm to schedule query across replicated server in order to minimise the average query waiting and completion time in this work we investigate the impact of dynamic pruning strategy on query response time and propose a framework for predicting the efficiency of a query within this framework we analyse the accuracy of several query efficiency predictor across query submitted to in memory inverted index of a million document web crawl our result show that combining multiple efficiency predictor with regression can accurately predict the response time of a query before it is executed moreover using the efficiency predictor to facilitate online scheduling algorithm can result in a reduction in the mean waiting time experienced by query before execution and a reduction in the mean completion time experienced by user 
retrieving semi structured entity to answer keyword query is an increasingly important feature of many modern web application the fast growing linked open data lod movement make it possible to crawl and index very large amount of structured data describing hundred of million of entity however entity retrieval approach have yet to find efficient and effective way of ranking and navigating through those large data set in this paper we address the problem of ad hoc object retrieval over large scale lod data by proposing a hybrid approach that combine ir and structured search technique specifically we propose an architecture that exploit an inverted index to answer keyword query a well a a semi structured database to improve the search effectiveness by automatically generating query over the lod graph experimental result show that our ranking algorithm exploiting both ir and graph index outperform state of the art entity retrieval technique by up to over the bm baseline 
modern content sharing environment such a flickr or youtube contain a large amount of private resource such a photo showing wedding family holiday and private party these resource can be of a highly sensitive nature disclosing many detail of the user private sphere in order to support user in making privacy decision in the context of image sharing and to provide them with a better overview on privacy related visual content available on the web we propose technique to automatically detect private image and to enable privacy oriented image search to this end we learn privacy classifier trained on a large set of manually assessed flickr photo combining textual metadata of image with a variety of visual feature we employ the resulting classification model for specifically searching for private photo and for diversifying query result to provide user with a better coverage of private and public content large scale classification experiment reveal insight into the predictive performance of different visual and textual feature and a user evaluation of query result ranking demonstrates the viability of our approach 
a classifier that determines if a webpage is relevant to a specified set of topic comprises a key component for focused crawling can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant page in the face of changed content on the web we investigate this question in the context of researcher homepage crawling we show experimentally that classifier trained on existing datasets for homepage identification underperform while classifying irrelevant page on current day academic website a an alternative to obtaining datasets to retrain the classifier for the new content we propose to use effectively unlimited amount of unlabeled data readily available from these website in a co training scenario to this end we design novel url based feature and use them in conjunction with content based feature a complementary view of the data to obtain remarkable improvement in accurately identifying homepage from the current day university website in addition we propose a novel technique for learning a conforming pair of classifier using mini batch gradient descent our algorithm seek to minimize a loss objective function quantifying the difference in prediction from the two view afforded by co training we demonstrate that tuning the classifier so that they make similar prediction on unlabeled data strongly corresponds to the effect achieved by co training algorithm we argue that this loss formulation provides insight into understanding the co training process and can be used even in absence of a validation set 
time aware retrieval model exploit one of two time dimension namely a publication time or b content time temporal expression mentioned in document we show that the effectiveness for a temporal query e g illinois earthquake depends significantly on which time dimension is factored into ranking result motivated by this we propose a machine learning approach to select the most suitable time aware retrieval model for a given temporal query our method us three class of feature obtained from analyzing distribution over two time dimension a distribution over term and retrieval score within top k result document experiment on real world data with crowdsourced relevance assessment show the potential of our approach 
web application framework are a proven mean to accelerate the development of interactive web application however implementing collaborative real time application like google doc requires specific concurrency control service i e document synchronization and conflict resolution that are not included in prevalent general purpose framework like jquery or knockout hence developer have to get familiar with specific collaboration framework e g sharejs which substantially increase the development effort to ease the development of collaborative web application we propose a set of source code annotation representing a lightweight mechanism to introduce concurrency control service into mature web framework those annotation are interpreted at runtime by a dedicated collaboration engine to sync document and resolve conflict we enhanced the general purpose framework knockout with a collaboration engine and conducted a developer study comparing our approach to a traditional concurrency control library the evaluation result show that the effort to incorporate collaboration capability into a web application can be reduced by up to percent using the annotation based solution 
obtaining geographically tagged multimedia item from social web platform such a flickr is beneficial for a variety of application including the automatic creation of travelogue and personalized travel recommendation in order to take advantage of the large number of photo and video that do not contain gps based latitude longitude coordinate a number of approach have been proposed to estimate the geographic location where they were taken such location estimation method rely on existing geotagged multimedia item a training data across application and usage scenario it is commonly assumed that the available geotagged item contain reasonably accurate latitude longitude coordinate here we consider this assumption and investigate how accurate the provided location data is we conduct a study of flickr image and video and find that the accuracy of the geotag information is highly dependent on the popularity of the location image video taken at popular unpopular location are likely to be geotagged with a high low degree of accuracy with respect to the ground truth 
many query language for graph structured data are based on regular path expression which describe relation among pair of node we propose an extension that allows to retrieve group of node based on group structural characteristic and relation to other node or group it allows to express group selection query in a concise and natural style and can be integrated into any query language based on regular path query we present an efficient algorithm for evaluating group query in polynomial time from an input data graph evaluation using real world social network demonstrate the practical feasibility of our approach 
while more and more semantic data are published on the web an important question is how typical web user can access and exploit this body of knowledge although existing interaction paradigm in semantic search hide the complexity behind an easy to use interface they have not managed to cover common search need in this paper we present x en explore entity in search a web search application that enhances the classical keyword based web searching with semantic information a a mean to combine the pro of both semantic web standard and common web searching x en identifies entity of interest in the snippet of the top search result which can be further exploited in a faceted interaction scheme and thereby can help the user to limit the often very large search space to those hit that contain a particular piece of information moreover x en permit the exploration of the identified entity by exploiting semantic repository 
a crucial step in adding structure to unstructured data is to identify reference to entity and disambiguate them such disambiguated reference can help enhance readability and draw similarity across different piece of running text in an automated fashion previous research ha tackled this problem by first forming a catalog of entity from a knowledge base such a wikipedia and then using this catalog to disambiguate reference in unseen text however most of the previously proposed model either do not use all text in the knowledge base potentially missing out on discriminative feature or do not exploit word entity proximity to learn high quality catalog in this work we propose topic model that keep track of the context of every word in the knowledge base so that word appearing within the same context a an entity are more likely to be associated with that entity thus our topic model utilize all text present in the knowledge base and help learn high quality catalog our model also learn group of co occurring entity thus enabling collective disambiguation unlike most previous topic model our model are non parametric and do not require the user to specify the exact number of group present in the knowledge base in experiment performed on an extract of wikipedia containing almost reference our model outperform svm based baseline by a much a in term of disambiguation accuracy translating to an increment of almost correctly disambiguated reference 
to model time dependent user intent for web search this paper proposes a novel method using machine learning technique to exploit temporal feature for effective time sensitive search result re ranking we propose model to incorporate user click through information for query that are seen in the training data and then further extend the model to deal with unseen query considering the relationship between query experiment show significant improvement on search result ranking over original search output 
aggregating search result from a variety of heterogeneous source so called vertical such a news image and video into a single interface is a popular paradigm in web search current approach that evaluate the effectiveness of aggregated search system are based on rewarding system that return highly relevant vertical for a given query where this relevance is assessed under different assumption it is difficult to evaluate or compare those system without fully understanding the relationship between those underlying assumption to address this we present a formal analysis and a set of extensive user study to investigate the effect of various assumption made for assessing query vertical relevance a total of more than assessment on search task across vertical are collected through amazon mechanical turk and subsequently analysed our result provide insight into various aspect of query vertical relevance and allow u to explain in more depth a well a questioning the evaluation result published in the literature 
evaluating adaptive and personalized information retrieval tech niques is known to be a difficult endeavor the rapid evolution of novel technology in this scope raise additional challenge that further stress the need for new evaluation approach and method ology the bar workshop seek to provide a specific venue for work on novel personalization centric benchmarking approach to evaluate adaptive retrieval and recommender system 
while various model have been proposed for generating social friendship network graph the dynamic of user interaction through online social network osn based application remain largely unexplored we previously developed a growth model to capture static weekly snapshot of user activity graph uags using data from popular facebook gifting application this paper present a new continuous graph evolution model aimed to capture microscopic user level behavior that govern the growth of the uag and collectively define the overall graph structure we demonstrate the utility of our model by applying it to forecast the number of active user over time a the application transition from initial growth to peak mature and decline fatique phase using empirical evaluation we show that our model can accurately reproduce the evolution trend of active user population for gifting application or other osn application that employ similar growth mechanism we also demonstrate that the prediction from our model can guide the generation of synthetic graph that accurately represent empirical uag snapshot sampled at different evolution stage 
social networking service posse two feature capturing the social relationship among people represented by the social network and allowing user to express their preference on different kind of item e g photo celebrity page through endorsing button represented by a kind of endorsement bipartite graph in this work using such information we propose a novel recommendation method which leverage the viral marketing in the social network and the wisdom of crowd from endorsement network our recommendation consists of two part first given some query term describing user s preference we find a set of targeted influencers who have the maximum activation probability on those node related to the query term in the social network second based on the derived targeted influencers a key expert we recommend item via the endorsement network we conduct the experiment on dblp co authorship social network with author reference data a the endorsement network the result show our method can achieve effective recommendation 
social medium have been employed to ass public opinion on event market and policy most current work focus on either developing aggregated measure or opinion extraction method like sentiment analysis these approach suffer from unpredictable turnover in the participant and the information they react to making it difficult to distinguish meaningful shift from those that follow from known information we propose a novel approach to tame these source of uncertainty through the introduction of computational focus group to track opinion shift in social medium stream our approach us prior user behavior to detect user bias then group user with similar bias together we track the behavior stream from these like minded sub group and present time dependent collective measure of their opinion these measure control for the response rate and base attitude of the user making shift in opinion both easier to detect and easier to interpret we test the effectiveness of our system by tracking group twitter response to a common stimulus set the u s presidential election debate while our group behavior is consistent with their bias there are numerous moment and topic on which they behave out of character suggesting precise target for follow up inquiry we also demonstrate that tracking elite user with well established bias doe not yield such insight a they are insensitive to the stimulus and simply reproduce expected pattern the effectiveness of our system suggests a new direction both for researcher and data driven journalist interested in identifying opinion shifting process in real time 
query suggestion refers to the process of suggesting related query to search engine user most existing research have focused on improving the relevance of suggested query in this paper we introduce the concept of diversifying the content of the search result from suggested query while keeping the suggestion relevant our framework first retrieves a set of query candidate from search engine log using random walk and other technique we then re rank the suggested query by ranking them in the order which maximizes the diversification function that measure the difference between the original search result and the result from suggested query the diversification function we proposed includes feature like odp category url and domain similarity and so on one important outcome from our research which contradicts with most existing research is that with the increase of suggestion relevance the similarity between the query actually decrease experiment are conducted on a large set of human labeled data which is randomly sampled from a commercial search engine s log result indicate that the post ranking framework significantly improves the relevance of suggested query by comparing to existing model 
while existing test collection and evaluation conference effort may sufficiently support one s research one can easily find oneself wanting to solve problem no one else is solving yet but how can research in ir be done or be published without solid data and experiment not everyone can talk trec clef inex or ntcir into running a track to build a collection this tutorial aim to teach how to build a test collection using resource at hand how to measure the quality of that collection how to understand it limitation and how to communicate them the intended audience is advanced student who find themselves in need of a test collection or actually in the process of building a test collection to support their own research the goal of this tutorial is to lay out issue procedure pitfall and practical advice 
in this paper we introduce task trail a a new concept to understand user search behavior we define task to be an atomic user information need web search log have been studied mainly at session or query level where user may submit several query within one task and handle several task within one session although previous study have addressed the problem of task identification little is known about the advantage of using task over session and query for search application in this paper we conduct extensive analysis and comparison to evaluate the effectiveness of task trail in three search application determining user satisfaction predicting user search interest and query suggestion experiment are conducted on large scale datasets from a commercial search engine experimental result show that session and query are not a precise a task in determining user satisfaction task trail provide higher web page utility to user than other source task represent atomic user information need and therefore can preserve topic similarity between query pair task based query suggestion can provide complementary result to other model the finding in this paper verify the need to extract task trail from web search log and suggest potential application in search and recommendation system 
we present match the news a browser extension for real time news recommendation our extension work on the client side to recommend in real time recently published article that are relevant to the web page the user is currently visiting match the news is fed from google news r and applies syntactic matching to find the relevant article we implement an innovative weighting function to perform the keyword extraction task bm h with bm h we extract keywords not only relevant to currently browsed web page but also novel with respect to the user s recent browsing history the novelty feature in keyword extraction task result in meaningful news recommendation with regard to the web page the user currently visit moreover the extension offer a salient visualization of the term corresponding to the user recent browsing history making thus the extension a comprehensive tool for real time news recommendation and self assessment 
in this paper for the first time we study the problem of mapping keyword query to question on community based question answering cqa site mapping general web query to question enables search engine not only to discover explicit and specific information need question behind keywords query but also to find high quality information answer for answering keyword query in order to map query to question we propose a ranking algorithm containing three step candidate question selection candidate question ranking and candidate question grouping preliminary experimental result using query from search log of a commercial engine show that the presented approach can efficiently find the question which capture user s information need explicitly 
microblog service have emerged a an essential way to strengthen the communication among individual and organization these service promote timely and active discussion and comment towards product market a well a public event and have attracted a lot of attention from organization in particular emerging topic are of immediate concern to organization since they signal current concern of and feedback by their user two challenge must be tackled for effective emerging topic detection one is the problem of real time relevant data collection and the other is the ability to model the emerging characteristic of detected topic and identify them before they become hot topic to tackle these challenge we first design a novel scheme to crawl the relevant message related to the designated organization by monitoring multi aspect of microblog content including user the evolving keywords and their temporal sequence we then develop an incremental clustering framework to detect new topic and employ a range of content and temporal feature to help in promptly detecting hot emerging topic extensive evaluation on a representative real world dataset based on twitter data demonstrate that our scheme is able to characterize emerging topic well and detect them before they become hot topic 
in this paper we present a medical record search system which is useful for identifying cohort required in clinical study in particular we propose a query adaptive weighting method that can dynamically aggregate and score evidence in multiple medical report from different hospital department or from different test within the same department of a patient furthermore we explore several informative feature for learning our retrieval model 
time travel text search enriches standard text search by temporal predicate so that user of web archive can easily retrieve document version that are considered relevant to a given keyword query and existed during a given time interval different index structure have been proposed to efficiently support time travel text search none of them however can easily be updated a the web evolves and new document version are added to the web archive in this work we describe a novel index structure that efficiently support time travel text search and can be maintained incrementally a new document version are added to the web archive our solution us a sharded index organization bound the number of spuriously read index entry per shard and can be maintained using small in memory buffer and append only operation we present experiment on two large scale real world datasets demonstrating that maintaining our novel index structure is an order of magnitude more efficient than periodically rebuilding one of the existing index structure while query processing performance is not adversely affected 
a web search increasingly becomes reliant on social signal it is imperative for u to understand the effect of these signal on user behavior there are multiple way in which social signal can be used in search a to surface and rank important social content b to signal to user which result are more trustworthy and important by placing annotation on search result we focus on the latter problem of understanding how social annotation affect user behavior in previous work through eyetracking research we learned that user do not generally seem to fixate on social annotation when they are placed at the bottom of the search result block with probability of fixation a second eyetracking study showed that placing the annotation on top of the snippet block might mitigate this issue but this study wa conducted using mock ups and with expert searcher in this paper we describe a study conducted with a new eyetracking mix method using a live traffic search engine with the suggested design change on real user using the same experimental procedure the study comprised of subject with an average of task per subject using an eyetrace assisted retrospective think aloud protocol using a funnel analysis we found that user are indeed more likely to notice the annotation with a probability of fixation if the annotation wa in view moreover we found no learning effect across search session but found significant difference in query type with subject having a lower chance of fixating on annotation for query in the news category in the interview portion of the study user reported interesting wow moment a well a usefulness in recalling or re finding content previously shared by oneself or friend the result not only shed light on how social annotation should be designed in search engine but also how user make use of social annotation to make decision about which page are useful and potentially trustworthy 
commercial web search engine rely on very large compute infrastructure to be able to cope with the continuous growth of the web and user base achieving scalability and efficiency in such large scale search engine requires making careful architectural design choice while devising algorithmic performance optimization unfortunately most detail about the internal functioning of commercial web search engine remain undisclosed due to their financial value and the high level of competition in the search market the main objective of this tutorial is to provide an overview of the fundamental scalability and efficiency challenge in commercial web search engine bridging the existing gap between the industry and academia 
we perform a comprehensive measurement analysis of silk road an anonymous international online marketplace that operates a a tor hidden service and us bitcoin a it exchange currency we gather and analyze data over eight month between the end of and including daily crawl of the marketplace for nearly six month in we obtain a detailed picture of the type of good sold on silk road and of the revenue made both by seller and silk road operator through examining over separate item sold on the site we show that silk road is overwhelmingly used a a market for controlled substance and narcotic and that most item sold are available for le than three week the majority of seller disappears within roughly three month of their arrival but a core of seller ha been present throughout our measurement interval we evaluate the total revenue made by all seller from public listing to slightly over usd million per month this corresponds to about usd per month in commission for the silk road operator we further show that the marketplace ha been operating steadily with daily sale and number of seller overall increasing over our measurement interval we discus economic and policy implication of our analysis and result including ethical consideration for future research in this area 
current research on web search ha focused on optimizing and evaluating single query however a significant fraction of user query are part of more complex task which span multiple query across one or more search session an ideal search engine would not only retrieve relevant result for a user s particular query but also be able to identify when the user is engaged in a more complex task and aid the user in completing that task toward optimizing whole session or task relevance we characterize and address the problem of intrinsic diversity id in retrieval a type of complex task that requires multiple interaction with current search engine unlike existing work on extrinsic diversity that deal with ambiguity in intent across multiple user id query often have little ambiguity in intent but seek content covering a variety of aspect on a shared theme in such scenario the underlying need are typically exploratory comparative or breadth oriented in nature we identify and address three key problem for id retrieval identifying authentic example of id task from post hoc analysis of behavioral signal in search log learning to identify initiator query that mark the start of an id search task and given an initiator query predicting which content to prefetch and rank 
the advent of the internet brought parallel paradigm shift to both economics and computer science computer scientist realized that large scale performing system can emerge from the interaction of selfish agent and that incentive are a quintessential part of a good system design and economist saw that the default platform of economic transaction are computational and interconnected algorithmic game theory is a subdiscipline that emerged from this turmoil revisiting some of the most important problem in economics and game theory from a computational and network perspective this talk will survey some of the major theme result and challenge in this field 
in this paper a graph cut based tag enrichment approach is proposed we build a graph for each image with it initial tag the graph is with two terminal node of the graph are full connected with each other min cut max flow algorithm is utilized to find the relevant tag for the image experiment on flickr dataset demonstrate the effectiveness of the proposed graph cut based tag enrichment approach 
more and more today people are engaging in conversation via email blog discussion forum text messaging and other social medium a person may want to archive these conversation and later retrieve information about what wa discussed or analyze a conversation in real time what topic are covered in these conversation what opinion are people expressing have any decision been made have action item been assigned this tutorial will present various natural language processing nlp technique that can help answer these question thus creating numerous new and valuable application that can support people in more effectively participating in these conversation the tutorial is based on a book that we have recently published method for mining and summarizing text conversation 
in this paper we design and implement a benchmarking framework for fair and exhaustive comparison of entity annotation system the framework is based upon the definition of a set of problem related to the entity annotation task a set of measure to evaluate system performance and a systematic comparative evaluation involving all publicly available datasets containing text of various type such a news tweet and web page our framework is easily extensible with novel entity annotator datasets and evaluation measure for comparing system and it ha been released to the public a open source we use this framework to perform the first extensive comparison among all available entity annotator over all available datasets and draw many interesting conclusion upon their efficiency and effectiveness we also draw conclusion between academic versus commercial annotator 
ten blue link have defined web search result for the last fifteen year snippet of text combined with document title and url in this paper we establish the notion of enhanced search result that extend web search result to include multimedia object such a image and video intent specific key value pair and element that allow the user to interact with the content of a web page directly from the search result page we show that user express a preference for enhanced result both explicitly and when observed in their search behavior we also demonstrate the effectiveness of enhanced result in helping user to ass the relevance of search result lastly we show that we can efficiently generate enhanced result to cover a significant fraction of search result page 
the paper is focused on blogosphere research based on the trec blog distillation task and aim to explore unbiased and significant feature automatically and efficiently feedback from faceted feed is introduced to harvest relevant feature and information gain is used to select discriminative feature the evaluation result show that the selected feedback feature can greatly improve the performance and adapt well to the terabyte data 
developing and maintaining cascading style sheet cs is an important issue to web developer a they suffer from the lack of rigorous method most existing mean rely on validators that check syntactic rule and on runtime debugger that check the behavior of a cs style sheet on a particular document instance however the aim of most style sheet is to be applied to an entire set of document usually defined by some schema to this end a cs style sheet is usually written w r t a given schema while usual debugging tool help reducing the number of bug they do not ultimately allow to prove property over the whole set of document to which the style sheet is intended to be applied we propose a novel approach to fill this lack we introduce idea borrowed from the field of logic and compile time verification for the analysis of cs style sheet we present an original tool based on recent advance in tree logic the tool is capable of statically detecting a wide range of error such a empty cs selector and semantically equivalent selector a well a proving property related to set of document such a coverage of styling information in the presence or absence of schema information this new tool can be used in addition to existing runtime debugger to ensure a higher level of quality of cs style sheet 
we introduce the problem of evaluating graph constraint in content based publish subscribe pub sub system this problem formulation extends traditional content based pub sub system in the following manner publisher and subscriber are connected via a logical directed graph g with node and edge constraint which limit the set of valid path between them such graph constraint can be used to model a web advertising exchange where there may be restriction on how advertising network can connect advertiser and publisher and content delivery problem in social network where there may be restriction on how information can be shared via the social graph in this context we develop efficient algorithm for evaluating graph constraint over arbitrary directed graph g we also present experimental result that demonstrate the effectiveness and scalability of the proposed algorithm using a realistic dataset from yahoo s web advertising exchange 
the growth of online video ha spurred a concomitant increase in the storage reuse and remix of this content a we gain more experience with video content social norm about ownership have evolved accordingly spelling out what people think is appropriate use of content that is not necessarily their own we use a series of three study each centering on a different genre of recording to probe participant attitude toward video storage reuse and remix we also question participant about their own experience with online video the result allow u to characterize current practice and emerging social norm and to establish the relationship between the two hypothetical borrowed from legal research are used a the primary vehicle for testing attitude and for identifying boundary between socially acceptable and unacceptable behavior 
existing recommender system model user interest and the social influence independently in reality user interest may change over time and a the interest change new friend may be added while old friend grow apart and the new friendship formed may cause further interest change this complex interaction requires the joint modeling of user interest and social relationship over time in this paper we propose a probabilistic generative model called receptiveness over time model rtm to capture this interaction we design a gibbs sampling algorithm to learn the receptiveness and interest distribution among user over time the result of experiment on a real world dataset demonstrate that rtm based recommendation outperforms the state of the art recommendation method case study also show that rtm is able to discover the user interest shift and receptiveness change over time 
this paper present a novel method for enabling fast development and easy customization of interactive data intensive web application our approach is based on a high level hierarchical programming model that result in both a very clean semantics of the application while at the same time creating well defined interface for customization of application component a prototypical implementation of a conference management system show the efficacy of our approach 
did celebrity last longer in or we investigate the phenomenon of fame by mining a collection of news article that span the twentieth century and also perform a side study on a collection of blog post from the last year by analyzing mention of personal name we measure each person s time in the spotlight and watch the distribution change from a century ago to a year ago we expected to find a trend of decreasing duration of fame a news cycle accelerated and attention span became shorter instead we find a remarkable consistency through most of the period we study through a century of rapid technological and societal change through the appearance of twitter communication satellite and the internet we do not observe a significant change in typical duration of celebrity we also study the most famous of the famous and find different result depending on our method for measuring duration of fame with a method that may be thought of a measuring a spike of attention around a single narrow news story we see the same result a before story last a long now a they did in a second method which may be thought of a measuring the duration of public interest in a person indicates that famous people s presence in the news is becoming longer rather than shorter an effect most likely driven by the wider distribution and higher volume of medium in modern time similar study have been done with much shorter timescales specifically in the context of information spreading on twitter and similar social networking site however to the best of our knowledge this is the first massive scale study of this nature that span over a century of archived data thereby allowing u to track change across decade 
assessor frequently disagree on the topical relevance of document how much of this disagreement is due to ambiguity in assessment instruction we have two assessor ass trec legal track document for relevance some to a general topic description others to detailed assessment guideline we find that detailed guideline lead to no significant increase in agreement amongst assessor or between assessor and the official qrels 
the ntcir intent task comprises two subtasks em subtopic mining where system are required to return a ranked list of em subtopic string for each given query and em document ranking where system are required to return a diversified web search result for each given query this paper summarises the novel feature of the second intent task at ntcir and it main finding and pose some question for future diversified search evaluation 
each year many acm sig community will recognize an outstanding researcher through an award in honor of his or her profound impact and numerous research contribution this work is the first to investigate an automated mechanism to help in selecting future award winner we approach the problem a a researcher expertise ranking problem and propose a temporal probabilistic ranking model which combine content with citation network analysis experimental result based on real world citation data and historical awardees indicate that some kind of sig award are well modeled by this approach 
structural information retrieval is mostly based on hierarchy however in real life information is not purely hierarchical and structural element may overlap each other the most common example is a document with two distinct structural view where the logical view is section subsection paragraph and the physical view is page line each single structural view of this document is a hierarchy and the component are either disjoint or nested inside each other the overlapping issue arises when one structural element cannot be neatly nested into others for instance when a paragraph start in one page and terminates in the next page similar situation can appear in video and other multimedia content where temporal or spatial constituent of a medium file may overlap each other querying over overlapping structure is one of the challenge of large scale search engine for instance fsis fast search for internet site is a microsoft search platform which encounter overlap while analysing content of textual data fsis us a pipeline process to extract structure and semantic information of document the pipeline contains several component where each component writes annotation to the input data these annotation consist of structural element and some of them may overlap each other handling overlapping structure in search engine will add a novel capability of searching where user can ask query such a find all the word that overlap two line or find the music played during intro scene of avatar movie there are also other use case where the user of the search engine is not a person but is a specific program with complex non traditional information retrieval need this research attempt to index overlapping structure and provide efficient query processing for large scale search engine the current research on overlapping structure revolves around encoding and modelling data while indexing and query processing method need investigation moreover due to intrinsic complexity of overlap xml indexing and query processing technique cannot be used for overlapping structure hence my research on overlapping structure comprises three main part an indexing method that support both hierarchy and overlap a query processing method based on the indexing technique and a query language that is close to natural language and support both full text and structural query our approach for indexing overlap is to adapt the prepost xml indexing method to overlapping structure this method label each node with it start and end position and requires modest storage space however prepost indexing cannot be used for overlapping node to overcome this issue we need to define a data model for overlapping structure since hierarchy are not sufficient to describe overlapping component several data structure have been introduced by scholar one of the most interesting data model is goddag goddag is a tree like graph where node can have multiple parentage this model can support overlap a well a simple inheritance our proposed data model for indexing overlap is such a tree like structure where we can define overlapping parent child and ancestor descendant relationship 
internet advertising a form of advertising that utilizes the internet to deliver marketing message and attract customer ha seen exponential growth since it inception around twenty year ago it ha been pivotal to the success of the world wide web the dramatic growth of internet advertising pose great challenge to information retrieval machine learning data mining and game theory and it call for novel technology to be developed the main purpose of this workshop is to bring together researcher and practitioner in the area of internet advertising and enable them to share their latest research result to express their opinion and to discus future direction 
search result diversification ha gained momentum a a way to tackle ambiguous query an effective approach to this problem is to explicitly model the possible aspect underlying a query in order to maximise the estimated relevance of the retrieved document with respect to the different aspect however such aspect themselves may represent information need with rather distinct intent e g informational or navigational hence a diverse ranking could benefit from applying intent aware retrieval model when estimating the relevance of document to different aspect in this paper we propose to diversify the result retrieved for a given query by learning the appropriateness of different retrieval model for each of the aspect underlying this query thorough experiment within the evaluation framework provided by the diversity task of the trec and web track show that the proposed approach can significantly improve state of the art diversification approach 
in this work we study the notion of competing campaign in a social network and address the problem of influence limitation where a bad campaign start propagating from a certain node in the network and use the notion of limiting campaign to counteract the effect of misinformation the problem can be summarized a identifying a subset of individual that need to be convinced to adopt the competing or good campaign so a to minimize the number of people that adopt the bad campaign at the end of both propagation process we show that this optimization problem is np hard and provide approximation guarantee for a greedy solution for various definition of this problem by proving that they are submodular we experimentally compare the performance of the greedy method to various heuristic the experiment reveal that in most case inexpensive heuristic such a degree centrality compare well with the greedy approach we also study the influence limitation problem in the presence of missing data where the current state of node in the network are only known with a certain probability and show that prediction in this setting is a supermodular problem we propose a prediction algorithm that is based on generating random spanning tree and evaluate the performance of this approach the experiment reveal that using the prediction algorithm we are able to tolerate about missing data before the performance of the algorithm start degrading and even with large amount of missing data the performance degrades only to of the performance that would be achieved with complete data 
given two competing product or meme or virus etc spreading over a given network can we predict what will happen at the end that is which product will win in term of highest market share one may naively expect that the better product stronger virus will just have a larger footprint proportional to the quality ratio of the product or strength ratio of the virus however we prove the surprising result that under realistic condition for any graph topology the stronger virus completely wipe out the weaker one thus not merely winning but taking it all in addition to the proof we also demonstrate our result with simulation over diverse real graph topology including the social contact graph of the city of portland or about million edge and million node and internet a router graph finally we also provide real data about competing product from google insight like facebook myspace and we show again that they agree with our analysis 
we introduce the concept of keyqueries a dynamic content descriptor for document keyqueries are defined implicitly by the index and the retrieval model of a reference search engine keyqueries for a document are the minimal query that return the document in the top result rank besides application in the field of information retrieval and data mining keyqueries have the potential to form the basis of a dynamic classification system for future digital library the modern version of keywords for content description to determine the keyqueries for a document we present an exhaustive search algorithm along with effective pruning strategy for application where a small number of diverse keyqueries is sufficient two tailored search strategy are proposed our experiment emphasize the role of the reference search engine and show the potential of keyqueries a innovative document descriptor for large fast evolving body of digital content such a the web 
navigating information space is an essential part of our everyday life and in order to design efficient and user friendly information system it is important to understand how human navigate and find the information they are looking for we perform a large scale study of human wayfinding in which given a network of link between the concept of wikipedia people play a game of finding a short path from a given start to a given target concept by following hyperlink what distinguishes our setup from other study of human web browsing behavior is that in our case people navigate a graph of connection between concept and that the exact goal of the navigation is known ahead of time we study more than goal directed human search path and identify strategy people use when navigating information space we find that human wayfinding while mostly very efficient differs from shortest path in characteristic way most subject navigate through high degree hub in the early phase while their search is guided by content feature thereafter we also observe a trade off between simplicity and efficiency conceptually simple solution are more common but tend to be le efficient than more complex one finally we consider the task of predicting the target a user is trying to reach we design a model and an efficient learning algorithm such predictive model of human wayfinding can be applied in intelligent browsing interface 
the central problem in the emerging discipline of computational advertising is to find the best match between a given user in a given context and a suitable advertisement the context could be a user entering a query in a search engine sponsored search a user reading a web page content match and display ad a user streaming a movie and so on in some situation it is desirable to solve the dual optimization problem rather then find the best ad given a user in a context the goal is to identify the best audience i e the most receptive set of user and or the most suitable context for a given advertising campaign the information about the user can vary from scarily detailed to practically nil the number of potential advertisement might be in the billion thus depending on the definition of best match and best audience these problem lead to a variety of massive optimization problem with complicated constraint and challenging data representation and access issue in general the direct problem is solved in two stage first a rough filtering is used to identify a relatively small set of ad to be considered a potential match followed by a more sophisticated secondary ranking where economics consideration take center stage historically the filtering ha been conceived a a database selection problem and wa done using simple boolean formula for instance in sponsored search the filter could be all ad that provide a specific bid for the present query string or a subset of it similarly for the dual problem audience definition for say a sport car ad the filter could be all male in california aged or le this database approach for the direct problem ha been recently supplanted by an ir approach based on a similarity search between a carefully constructed query that capture the advertising opportunity and an annotated document corpus that represents the potential ad similarly in the dual problem the newer approach is to devise an efficient and effective representation of the user then form a query that represents a prototypical ideal user and finally find the user most similar to the prototype the aim of this talk is to discus the penetration of the ir paradigm in computational advertising and present some research challenge and opportunity in this area of enormous economic importance 
we present in this paper a contribution to ir modeling by proposing a new ranking function called sopra that considers the social dimension of the web this social dimension is any social information that surround document along with the social context of user currently our approach relies on folksonomies for extracting these social context but it can be extended to use any social meta data e g comment rating tweet etc the evaluation performed on our approach show it benefit for personalized search 
at present most major app marketplace perform ranking and recommendation based on search relevance feature or marketplace popularity statistic for instance they check similarity between app description and user search query or rank order the apps according to statistic such a number of downloads user rating etc ranking derived from such signal important a they are are insufficient to capture the dynamic of the apps ecosystem consider for example the question in a particular user context is app a more likely to be launched than app b or doe app c provide complementary functionality to app d answering these question requires identifying and analyzing the dependency between apps in the apps ecosystem ranking mechanism that reflect such interdependence are thus necessary in this paper we introduce the notion of interoperability ranking for mobile application intuitively apps with high rank are such apps which are inferred to be somehow important to other apps in the ecosystem we demonstrate how interoperability ranking can help answer the above question and also provide the basis for solving several problem which are rapidly attracting the attention of both researcher and the industry such a building personalized real time app recommender system or intelligent mobile agent we describe a set of method for computing interoperability rank and analyze their performance on real data from the window phone app marketplace 
recent research ha explored the increasingly important role of social medium by examining the dynamic of individual and group behavior characterizing pattern of information diffusion and identifying influential individual in this paper we suggest a measure of causal relationship between node based on the information theoretic notion of transfer entropy or information transfer this theoretically grounded measure is based on dynamic information capture fine grain notion of influence and admits a natural predictive interpretation network inferred by transfer entropy can differ significantly from static friendship network because most friendship link are not useful for predicting future dynamic we demonstrate through analysis of synthetic and real world data that transfer entropy reveals meaningful hidden network structure in addition to altering our notion of who is influential transfer entropy allows u to differentiate between weak influence over large group and strong influence over small group 
it is a common practice among web service to allow user to rate item on their site in this paper we first point out the flaw of the popular method for user rating based ranking of item and then argue that two well known information retrieval ir technique namely the probability ranking principle and statistical language modelling provide a simple but effective solution to this problem 
user of information retrieval system employ a variety of strategy when searching for information one factor that can directly influence how searcher go about their information finding task is the level of familiarity with a search topic we investigate how the search behavior of domain expert change based on their previous level of familiarity with a search topic reporting on a user study of biomedical expert searching for a range of domain specific material the result of our study show that topic familiarity can influence the number of query that are employed to complete a task the type of query that are entered and the overall number of query term our finding suggest that biomedical search system should enable searching through a variety of querying mode to support the different search strategy that user were found to employ depending on their familiarity with the information that they are searching for 
community based question answering cqa service have become a major venue for people s information seeking on the web however many study on cqa have focused on the prediction of the best answer for a given question this paper look into the formulation of effective question in the context of cqa in particular we looked at effect of contextual factor appended to a basic question on the performance of submitted answer this study analysed a total of answer returned in response to question that were formulated by participant the result show that adding a questionnaire s personal and social attribute to the question helped improve the perception of answer both in information seeking question and opinion seeking question 
measuring the causal effect of online advertising adfx on user behavior is important to the health of the www publishing industry in this paper using three controlled experiment we show that observational data frequently lead to incorrect estimate of adfx the reason which we label activity bias come from the surprising amount of time based correlation between the myriad activity that user undertake online in experiment user who are exposed to an ad on a given day are much more likely to engage in brand relevant search query a compared to their recent history for reason that had nothing do with the advertisement in experiment we show that activity bias occurs for page view across diverse website in experiment we track account sign ups at a competitor s of the advertiser website and find that many more people sign up on the day they saw an advertisement than on other day but that the true competitive effect wa minimal in all three experiment exposure to a campaign signal doing more of everything in given period of time making it difficult to find a suitable matched control using prior behavior in such case the match is fundamentally different from the exposed group and we show how and why observational method lead to a massive overestimate of adfx in such circumstance 
personalization of search result offer the potential for significant improvement in web search among the many observable user attribute approximate user location is particularly simple for search engine to obtain and allows personalization even for a first time web search user however acting on user location information is difficult since few web document include an address that can be interpreted a constraining the location where the document is relevant furthermore many web document such a local news story lottery result and sport team fan page may not correspond to physical address but the location of the user still play an important role in document relevance in this paper we show how to infer a more general location relevance which us not only physical location but a more general notion of location of interest for web page we compute this information using implicit user behavioral data characterize the most location centric page and show how location information can be incorporated into web search ranking our result show that a substantial fraction of web search query can be significantly improved by incorporating location based feature 
the live nugget extractor system provides user with a method of efficiently and accurately collecting relevant information for any web query rather than providing a simple ranked list of document the system utilizes an online learning procedure to infer relevance of unjudged document while extracting and ranking information from judged document this creates a set of judged and inferred relevance score for both document and text fragment which can be used for test collection summarization and other task where high accuracy and large collection with minimal human effort are needed 
integrating the extracted fact with an existing knowledge base ha raised an urgent need to address the problem of entity linking specifically entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base however this task is challenging due to name ambiguity textual inconsistency and lack of world knowledge in the knowledge base several method have been proposed to tackle this problem but they are largely based on the co occurrence statistic of term between the text around the entity mention and the document associated with the entity in this paper we propose linden a novel framework to link named entity in text with a knowledge base unifying wikipedia and wordnet by leveraging the rich semantic knowledge embedded in the wikipedia and the taxonomy of the knowledge base we extensively evaluate the performance of our proposed linden over two public data set and empirical result show that linden significantly outperforms the state of the art method in term of accuracy 
in ir research it is essential to know ir model research over the past year ha consolidated the foundation of ir model moreover relationship have been reported that help to use and position ir model knowing about the foundation and relationship of ir model can significantly improve building information management system the first part of this tutorial present an in depth consolidation of the foundation of the main ir model tf idf bm lm particular attention will be given to notation and probabilistic root the second part crystallises the relationship between model doe lm embody idf how heuristic is tf idf what are the probabilistic root how are lm and the probability of relevance related what are the component shared by the main ir model after the tutorial attendee will be familiar with a consolidated view on ir model the tutorial will be illustrative and interactive providing opportunity to exchange controversial issue and research challenge 
many area of study such a information retrieval collaborative filtering and social choice face the preference aggregation problem in which multiple preference over object must be combined into a consensus ranking preference over item can be expressed in a variety of form which make the aggregation problem difficult in this work we formulate a flexible probabilistic model over pairwise comparison that can accommodate all these form inference in the model is very fast making it applicable to problem with hundred of thousand of preference experiment on benchmark datasets demonstrate superior performance to existing method 
in support of our research project in information retrieval we have developed an integrated multi process software system that shepherd research data from induction through aggregation analysis and presentation we combine public domain code library with our own software to provide a flexible easilyconfigured modular system that expose data online for easier collaboration the goal is to create a single online infrastructure that allows colleague to submit process analyze and visualize data and discus and prioritize issue through a single integrated interface we demonstrate our system within the context of the large data set provided by the indexer s legacy project 
with the availability of cheap location sensor geotagging of message in online social network is proliferating for instance twitter facebook foursquare and google provide these service both explicitly by letting user choose their location or implicitly via a sensor this paper present an integrated generative model of location and message content that is we provide a model for combining distribution over location topic and over user characteristic both in term of location and in term of their content preference unlike previous work which modeled data in a flat pre defined representation our model automatically infers both the hierarchical structure over content and over the size and position of geographical location this affords significantly higher accuracy location uncertainty is reduced by relative to the best previous result achieved on location estimation from tweet we achieve this goal by proposing a new statistical model the nested chinese restaurant franchise ncrf a hierarchical model of tree distribution much statistical structure is shared between user that said each user ha his own distribution over interest and place the use of the ncrf allows u to capture the following effect we provide a topic model for tweet we obtain location specific topic we infer a latent distribution of location we provide a joint hierarchical model of topic and location we infer personalized preference over topic and location within the above model in doing so we are both able to obtain accurate estimate of the location of a user based on his tweet and to obtain a detailed estimate of a geographical language model 
search engine receive query with a broad range of different search intent however they do not perform equally well for all query understanding where search engine perform poorly is critical for improving their performance in this paper we present a method for automatically identifying poorly performing query group where a search engine may not meet searcher need this allows u to create coherent query cluster that help system design er generate actionable insight about necessary change and help learning to rank algorithm better learn relevance signal via spe cialized ranker the result is a framework capable of estimating dissatisfaction from web search log and learning to improve per formance for dissatisfied query through experimentation we show that our method yield good quality group that align with established retrieval performance metric we also show that we can significantly improve retrieval effectiveness via specialized ranker and that coherent grouping of underperforming query generated by our method is important in improving each group 
how to organize and present search result play a critical role in the utility of search engine due to the unprecedented scale of the web and diversity of search result the common strategy of ranked list ha become increasingly inadequate and clustering ha been considered a a promising alternative clustering divide a long list of disparate search result into a few topic coherent cluster allowing the user to quickly locate relevant result by topic navigation while many clustering algorithm have been proposed that innovate on the automatic clustering procedure we introduce clusteringwiki the first prototype and framework for personalized clustering that allows direct user editing of clustering result through a wiki interface the user can edit and annotate the membership structure and label of cluster for a personalized presentation in addition the edits and annotation can be shared among user a a mass collaborative way of improving search result organization and search engine utility 
in this paper we propose a novel top k learning to rank framework which involves labeling strategy ranking model and evaluation measure the motivation come from the difficulty in obtaining reliable relevance judgment from human assessor when applying learning to rank in real search system the traditional absolute relevance judgment method is difficult in both gradation specification and human assessing resulting in high level of disagreement on judgment while the pairwise preference judgment a a good alternative is often criticized for increasing the complexity of judgment from o n to n log n considering the fact that user mainly care about top ranked search result we propose a novel top k labeling strategy which adopts the pairwise preference judgment to generate the top k ordering item from n document i e top k ground truth in a manner similar to that of heapsort a a result the complexity of judgment is reduced to o n log k with the top k ground truth traditional ranking model e g pairwise or listwise model and evaluation measure e g ndcg no longer fit the data set therefore we introduce a new ranking model namely focusedrank which fully capture the characteristic of the top k ground truth we also extend the widely used evaluation measure ndcg and err to be applicable to the top k ground truth referred a ndcg and err respectively finally we conduct extensive experiment on benchmark data collection to demonstrate the efficiency and effectiveness of our top k labeling strategy and ranking model 
supervised learning to rank algorithm typically optimize for high relevance and ignore other facet of search quality such a freshness and diversity prior work on multi objective ranking trained ranker focused on using hybrid label that combine overall quality of document and implicitly incorporate multiple criterion into quantifying ranking risk however these hybrid score are usually generated based on heuristic without considering potential correlation between individual facet e g freshness versus relevance in this poster we empirically demonstrate that the correlation between objective facet in multi criterion ranking optimization may significantly influence the effectiveness of trained ranker with respect to each objective 
this paper study the problem of discovering and comparing geographical topic from gps associated document gps associated document become popular with the pervasiveness of location acquisition technology for example in flickr the geo tagged photo are associated with tag and gps location in twitter the location of the tweet can be identified by the gps location from smart phone many interesting concept including culture scene and product sale correspond to specialized geographical distribution in this paper we are interested in two question how to discover different topic of interest that are coherent in geographical region how to compare several topic across different geographical location to answer these question this paper proposes and compare three way of modeling geographical topic location driven model text driven model and a novel joint model called lgta latent geographical topic analysis that combine location and text to make a fair comparison we collect several representative datasets from flickr website including landscape activity manhattan national park festival car and food the result show that the first two method work in some datasets but fail in others lgta work well in all these datasets at not only finding region of interest but also providing effective comparison of the topic across different location the result confirm our hypothesis that the geographical distribution can help modeling topic while topic provide important cue to group different geographical region 
over the last fifteen year several type of attack against domain name and the company relying on them have been observed the well known cybersquatting of domain name gave way to typosquatting the abuse of a user s mistake when typing a url in her browser s address bar recently a new attack against domain name surfaced namely bitsquatting in bitsquatting an attacker leverage random bit error occurring in the memory of commodity computer and smartphones to redirect internet traffic to attacker controlled domain in this paper we report on a large scale experiment measuring the adoption of bitsquatting by the domain squatting community through the tracking of registration of bitsquatting domain targeting popular web site over a month period we show how new bitsquatting domain are registered daily and how attacker are trying to monetize their domain through the use of ad abuse of affiliate program and even malware installation lastly given the discovered prevalence of bitsquatting we review possible defense measure that company software developer and internet service provider can use to protect against it 
the information need of user and the document which answer it are frequently contingent on the different characteristic of user this is especially evident during natural disaster such a earthquake and violent weather incident which create a strong transient information need in this paper we investigate how the information need of user is affected by their physical detachment a estimated by their physical location in relation to that of the event and by their social detachment a quantified by the number of their acquaintance who may be affected by the event drawing on large scale data from three major event we show that social and physical detachment level of user are a major influence on their information need a manifested by their search engine query we demonstrate how knowing social and physical detachment level can assist in improving retrieval for two application identifying search query related to event and ranking result in response to event related query we find that the average precision in identifying relevant search query improves by approximately and that the average precision of ranking that us detachment information improves by 
online peer to peer p p lending service are a new type of social platform that enables individual borrow and lend money directly from one to another in this paper we study the dynamic of bidding behavior in a p p loan auction website prosper com we investigate the change of various attribute of loan requesting listing over time such a the interest rate and the number of bid we observe that there is herding behavior during bidding and for most of the listing the number of bid they receive reach spike at very similar time point we explain these phenomenon by showing that there are economic and social factor that lender take into account when deciding to bid on a listing we also observe that the profit the lender make are tied with their bidding preference finally we build a model based on the temporal progression of the bidding that reliably predicts the success of a loan request listing a well a whether a loan will be paid back or not 
we investigate the negative link feature of social network that allows user to tag other user a foe or a distrusted in addition to the usual friend and trusted link to answer the question whether negative link have an added value for an online social network we investigate the machine learning problem of predicting the negative link of such a network using only the positive link a a basis with the idea that if this problem can be solved with high accuracy then the negative link feature is redundant in doing so we also present a general methodology for assessing the added value of any new link type in online social network our evaluation is performed on two social network that allow negative link the technology news website slashdot and the product review site epinions in experiment with these two datasets we come to the conclusion that a combination of centrality based and proximity based link prediction function can be used to predict the negative edge in the network we analyse we explain this result by an application of the model of preferential attachment and balance theory to our learning problem and show that the negative link feature ha a small but measurable added value for these social network 
sometimes during a search task user may switch from one search engine to another for several reason e g dissatisfaction with the current search result or desire for broader topic coverage detecting the fact of switching is difficult but important for understanding user satisfaction with the search engine and the complexity of their search task leading to economic significance for search provider previous research on switching detection mainly focused on studying different signal useful for the task and particular reason for switching although it is known that switching is a personal choice of a user and different user have different search behavior little ha been done to understand how these difference could be used for switching detection in this paper we study the effectiveness of learning personal behavior pattern for switching detection and present a personalized approach which us user s session history containing session with and without switch experiment show that user personal habit and behavior pattern are indeed among the most informative signal our finding can be used by a search log analyzer for engine switching detection and potentially other log mining problem thus providing valuable signal for search provider to improve user experience 
in this demo we present mydj a karaoke recommendation system which recommends the song people are capable to sing different from the existing song recommendation system which recommend song people like to listen mydj can recommend proper song according to a subject s physical phonation area it consists of a singer profiler to analyze the subject s phonation character in addition the song profile for each song in database is extracted to learn a ranking function the learning to rank algorithm listnet is applied under a list of predefined feature extracted from each singer song profile pair in the result proper song which are suitable but challenging for the subject are recommended 
the emergence of human computation system including mechanical turk and game with a purpose ha made it feasible to distribute relevance judgment task to worker over the web most human computation system assign task to individual randomly and such assignment may match worker with task that they may be unqualified or unmotivated to perform we compare two group of worker those given a choice of query to judge versus those who are not in term of their self rated competence and their actual performance result show that when given a choice of task worker choose one for which they have greater expertise interest confidence and understanding 
while web search ha become increasingly effective over the last decade for many user need the required answer may be spread across many document or may not exist on the web at all yet many of these need could be addressed by asking people via popular community question answering cqa service such a baidu know quora or yahoo answer in this paper we perform the first large scale analysis of how searcher become asker for this we study the log of a major web search engine to trace the transformation of a large number of failed search into question posted on a popular cqa site specifically we analyze the characteristic of the query and of the pattern of search behavior that precede posting a question the relationship between the content of the attempted query and of the posted question and the subsequent action the user performs on the cqa site our work develops novel insight into searcher intent and behavior that lead to asking question to the community providing a foundation for more effective integration of automated web search and social information seeking 
we demonstrate the merit of using inter document similarity for federated search specifically we study a result merging method that utilizes information induced from cluster of similar document created across the list retrieved from the collection the method significantly outperforms state of the art result merging approach 
entity disambiguation is an important step in many information retrieval application this paper proposes new research for entity disambiguation with the focus of name disambiguation in digital library in particular pairwise similarity is first learned for publication that share the same author name string an and then a novel hierarchical agglomerative clustering approach with adaptive stopping criterion hacasc is proposed to adaptively cluster a set of publication that share a same an to individual cluster of publication with different author identity the hacasc approach utilizes a mixture of kernel ridge regression to intelligently determine the threshold in clustering this obtains more appropriate clustering granularity than non adaptive stopping criterion we conduct a large scale empirical study with a dataset of more than million publication record pair to demonstrate the advantage of the proposed hacasc approach 
this paper demonstrates treo a natural language query mechanism for linked data graph the approach us a distributional semantic vector space model to semantically match user query term with data supporting vocabulary independent or schema agnostic query over structured data 
many of user generated content in the web center around real world incident such a japanese tsunami or general concern such a recent economic downturn such type of information is always of interest to user for instance when a user read a news article about a tsunami in japan she want to see related flickr photo or more tweet about it conventional keyword based search is inappropriate since it is not always trivial to formulate ad hoc interest about the event and material in some case the user might want to explore emerging topic that dominate different source present system fail to connect topically document across medium and the user ha to examine individual source to infer the topic herself in this work we address a special type of user information need temporal topic which refers to any abstract matter active within some point or period of time a temporal topic can be a real world event e g the arab spring revolution but can also be a le conceivable subject e g the study of vacuum tube computer in s topic can also be recurrent such a the u presidency campaign there are extensive study on how to detect topic from a collection of document but little us temporal topic a part of user interest to retrieve document we believe that temporal topic based retrieval is a one solution to improve user experience of present ir system a well a to benefit other application e g topic sensitive online advertisement our research goal can be defined in three research question the first question involves finding latent temporal topic in a social medium stream where document are well equipped with meta data timestamps geo spatial data etc following mixture model such a lda we treat each document a a mix of different temporal topic model each model is incorporated with time a temporal topic consists of at least two type of attribute time and representing word a similar to the dynamic of temporal topic can be characterized in a timeline fashion or using hierarchical structure the challenge lie in devising a model flexible enough to diverse and rapidly changing data without many parameter assumption for this we see bayesian nonpara metric a one promising solution and will extend it to temporal dimension the second research question is how to retrieve and rank document from different social medium site based on their relevance to one or several given temporal topic we identify some following challenge the first one is representing temporal topic a query although there have been attempt using keywords and time window separately we aim to unify time and topical word in a single query model the second challenge is integrating temporal topic model into ranking model inspired by our previous work we will use language model to capture the relevance score between document and topic and investigate advanced method to index the score effectively our last question involves connecting a given document to document in other source data stream or corpus that shared one of it latent temporal topic this task doe not only provide unified insight into different social medium site but also help improve the quality of model by data in diverse source however formalizing the semantics of similarity for document in different setting based on temporal topcis is tricky one baseline method is to apply kullback leibler divergence on comparable feature tf idf n gram photo tag timestamps we can also use language model to construct a language model for each candidate document then estimate how likely it generates the document of interest within a given temporal topic 
affect how user interact with a search system microeconomic theory is used to generate the cost interaction hypothesis that state a the cost of querying increase user will pose fewer query and examine more document per query a between subject laboratory study with undergraduate subject wa conducted where subject were randomly assigned to use one of three search interface that varied according to the amount of physical cost required to query structured high cost standard medium cost and query suggestion low cost result show that subject who used the structured interface submitted significantly fewer query spent more time on search result page examined significantly more document per query and went to greater depth in the search result list result also showed that these subject spent longer generating their initial query saved more relevant document and rated their query a more successful these finding have implication for the usefulness of microeconomic theory a a way to model and explain search interaction a well a for the design of query facility 
improving query understanding is crucial for providing the user with information that suit her need to this end the retrieval system must be able to deal with several source of knowledge from which it could infer a topical context the use of external source of information for improving document retrieval ha been extensively studied improvement with either structured or large set of data have been reported however in these study resource are often used separately and rarely combined together we experiment in this paper a method that discount document based on their weighted divergence from a set of external resource we present an evaluation of the combination of four resource on two standard trec test collection our proposed method significantly outperforms a state of the art mixture of relevance model on one test collection while no significant difference are detected on the other one 
crowdsourcing allows to build hybrid online platform that combine scalable information system with the power of human intelligence to complete task that are difficult to tackle for current algorithm example include hybrid database system that use the crowd to fill missing value or to sort item according to subjective dimension such a picture attractiveness current approach to crowdsourcing adopt a pull methodology where task are published on specialized web platform where worker can pick their preferred task on a first come first served basis while this approach ha many advantage such a simplicity and short completion time it doe not guarantee that the task is performed by the most suitable worker in this paper we propose and extensively evaluate a different crowdsourcing approach based on a push methodology our proposed system carefully selects which worker should perform a given task based on worker profile extracted from social network worker and task are automatically matched using an underlying categorization structure that exploit entity extracted from the task description on one hand and category liked by the user on social platform on the other hand we experimentally evaluate our approach on task of varying complexity and show that our push methodology consistently yield better result than usual pull strategy 
recently answer for fact lookup query have appeared on major search engine for example for the query barack obama date of birth google directly show august above it regular result in this paper we describe facto an end to end system for answering fact lookup query for web search facto extract structured data from table on the web aggregate and clean such data and store them in a database given a web search query facto will decide if it asks for fact in this database and provides the most confident answer when possible facto achieves higher precision and comparable coverage comparing with the fact lookup engine by google and ask com although facto is developed by a very small team we present the challenge and our solution in developing every component of facto some solution are based on existing technology and many others are novel approach proposed by u 
the increasing number of image available online ha created a growing need for efficient way to search for relevant content text based query search is the most common approach to retrieve image from the web in this approach the similarity between the input query and the metadata of image is used to find relevant information however a the amount of available image grows the number of relevant image also increase all of them sharing very similar metadata but differing in other visual characteristic this paper study the influence of visual aesthetic quality in search result a a complementary attribute to relevance by considering aesthetic a new ranking parameter is introduced aimed at improving the quality at the top rank when large amount of relevant result exist two strategy for aesthetic rating inference are proposed one based on visual content another based on the analysis of user comment to detect opinion about the quality of image the result of a user study with participant show that the comment based aesthetic predictor outperforms the visual content based strategy and reveals that aesthetic aware ranking are preferred by user searching for photograph on the web 
in recent year new study concentrating on analyzing user personality and finding credible content in social medium have become quite popular most such work augments feature from textual content with feature representing the user s social tie and the tie strength social tie are crucial in understanding the network the people are a part of however textual content is extremely useful in understanding topic discussed and the personality of the individual we bring a new dimension to this type of analysis with method to compute the type of tie individual have and the strength of the tie in each dimension we present a new genre of behavioral feature that are able to capture the function of a specific relationship without the help of textual feature our novel feature are based on the statistical property of communication pattern between individual such a reciprocity assortativity attention and latency we introduce a new methodology for determining how such feature can be compared to textual feature and show using twitter data that our feature can be used to capture contextual information present in textual feature very accurately conversely we also demonstrate how textual feature can be used to determine social attribute related to an individual 
in crowdsourced relevance judging each crowd worker typically judge only a small number of example yielding a sparse and imbalanced set of judgment in which relatively few worker influence output consensus label particularly with simple consensus method like majority voting we show how probabilistic matrix factorization a standard approach in collaborative filtering can be used to infer missing worker judgment such that all worker influence output label given complete worker judgment inferred by pmf we evaluate impact in unsupervised and supervised scenario in the supervised case we consider both weighted voting and worker selection strategy based on worker accuracy experiment on crowd judgment from the trec relevance feedback track show promise of the pmf approach merit further investigation and analysis 
there is great interest in producing effectiveness measure that model user behavior in order to better model the utility of a system to it user these measure are often formulated a a sum over the product of a discount function of rank and a gain function mapping relevance assessment to numeric utility value we develop a conceptual framework for analyzing such effectiveness measure based on classifying member of this broad family of measure into four distinct family each of which reflects a different notion of system utility within this framework we can hypothesize about the property that such a measure should have and test those hypothesis against user and system data along the way we present a collection of novel result about specific measure and relationship between them 
the cyber physical system cps are envisioned a a class of real time system integrating the computing communication and storage facility with monitoring and control of the physical world one interesting cps application in the mobile internet is to provide web search on the spot regarding the physical world that a user see or literally wyriwys what you retrieve is what you see the objective of our work is to develop server browser software for supporting wyriwys search in our prototype cyber physical search engine a wyriwys search retrieves visible web object and rank them by their cyber physical relevance term visual spatial temporal etc this work is distinguished from previous lws a it provides quality web search geared with the physical world therefore it suggests a very promising solution to cyber physical web search 
the purpose of the contextual suggestion track an evaluation task at the trec conference is to suggest personalized tourist activity to an individual given a certain location and time in our content based approach we collected initial recommendation using the location context a search query in google place we first ranked the recommendation based on their textual similarity to the user profile in order to improve the ranking of popular sight we combined the initial ranking with ranking based on google search popularity and category finally we performed filtering based on the temporal context overall our system performed well above average and median and outperformed the baseline google place only run 
in web search user may remain unsatisfied for several reason the search engine may not be effective enough or the query might not reflect their intent year of research focused on providing the best user experience for the data available to the search engine however little ha been done to address the case in which relevant content for the specific user need ha not been posted on the web yet one obvious solution is to directly ask other user to generate the missing content using community question answering service such a yahoo answer or baidu zhidao however formulating a full fledged question after having issued a query requires some effort some previous work proposed to automatically generate natural language question from a given query but not for scenario in which a searcher is presented with a list of question to choose from we propose here to generate synthetic question that can actually be clicked by the searcher so a to be directly posted a question on a community question answering service this imposes new constraint a question will be actually shown to searcher who will not appreciate an awkward style or redundancy to this end we introduce a learning based approach that improves not only the relevance of the suggested question to the original query but also their grammatical correctness in addition since query are often underspecified and ambiguous we put a special emphasis on increasing the diversity of suggestion via a novel diversification mechanism we conducted several experiment to evaluate our approach by comparing it to prior work the experiment show that our algorithm improves question quality by over prior work and that adding diversification reduced redundancy by 
a growing set of on line application are generating data that can be viewed a very large collection of small dense social graph these range from set of social group event or collaboration project to the vast collection of graph neighborhood in large social network a natural question is how to usefully define a domain independent coordinate system for such a collection of graph so that the set of possible structure can be compactly represented and understood within a common space in this work we draw on the theory of graph homomorphism to formulate and analyze such a representation based on computing the frequency of small induced subgraphs within each graph we find that the space of subgraph frequency is governed both by it combinatorial property based on extremal result that constrain all graph a well a by it empirical property manifested in the way that real social graph appear to lie near a simple one dimensional curve through this space we develop flexible framework for studying each of these aspect for capturing empirical property we characterize a simple stochastic generative model a single parameter extension of erdos renyi random graph whose stationary distribution over subgraphs closely track the one dimensional concentration of the real social graph family for the extremal property we develop a tractable linear program for bounding the feasible space of subgraph frequency by harnessing a toolkit of known extremal graph theory together these two complementary framework shed light on a fundamental question pertaining to social graph what property of social graph are social property and what property are graph property we conclude with a brief demonstration of how the coordinate system we examine can also be used to perform classification task distinguishing between structure arising from different type of social graph 
ranking of image is difficult because many factor determine their importance e g popularity quality entertainment value context etc in social medium platform ranking also depends on social interaction and on the visibility of the image both inside and outside those platform in this context the application of standard ranking method is not clearly understood and neither are the subtlety associated with taking into account social interaction internal and external factor in this paper we use a large flickr dataset and investigate these factor by performing an in depth analysis of several ranking algorithm using both internal i e within flickr and external i e link from outside of flickr factor we analyze ranking given by common metric used in image retrieval e g number of favorite and compare them with metric based on page view e g time spent number of view in addition we represent user navigation by a graph and combine session model with some of these metric comparing with pagerank and browserank our experiment show significant difference between the ranking providing insight on the impact of social interaction internal and external factor in image ranking 
url shortening service have become extremely popular however it is still unclear whether they are an effective and reliable tool that can be leveraged to hide malicious url and to what extent these abuse can impact the end user with these question in mind we first analyzed existing countermeasure adopted by popular shortening service surprisingly we found such countermeasure to be ineffective and trivial to bypass this first measurement motivated u to proceed further with a large scale collection of the http interaction that originate when web user access live page that contain short url to this end we monitored distinct url shortening service between march and april and collected distinct short url with this large dataset we studied the abuse of short url despite short url are a significant new security risk in accordance with the report resulting from the observation of the overall phishing and spamming activity we found that only a relatively small fraction of user ever encountered malicious short url interestingly during the second year of measurement we noticed an increased percentage of short url being abused for drive by download campaign and a decreased percentage of short url being abused for spam campaign in addition to these security related finding our unique monitoring infrastructure and large dataset allowed u to complement previous research on short url and analyze these web service from the user s perspective 
a considerable portion of web content from wikis to collaboratively edited document to code posted online is revisioned we consider the problem of attributing authorship to such revisioned content and we develop scalable attribution algorithm that can be applied to very large body of revisioned content such a the english wikipedia since content can be deleted only to be later re inserted we introduce a notion of authorship that requires comparing each new revision with the entire set of past revision for each portion of content in the newest revision we search the entire history for content match that are statistically unlikely to occur spontaneously thus denoting common origin we use these match to compute the earliest possible attribution of each word or each token of the new content we show that this earliest plausible attribution can be computed efficiently via compact summary of the past revision history this lead to an algorithm that run in time proportional to the sum of the size of the most recent revision and the total amount of change edit work in the revision history this amount of change is typically much smaller than the total size of all past revision the resulting algorithm can scale to very large repository of revisioned content a we show via experimental data over the english wikipedia 
web search engine frequently show the same document repeatedly for different query within the same search session in essence forgetting when the same document were already shown to user depending on previous user interaction with the repeated result and the detail of the session we show that sometimes the repeated result should be promoted while some other time they should be demoted analysing search log from two different commercial search engine we find that result are repeated in about of multi query search session and that user engage differently with repeat than with result shown for the first time we demonstrate how statistic about result repetition within search session can be incorporated into ranking for personalizing search result our result on query log of two large scale commercial search engine suggest that we successfully promote document that are more likely to be clicked by the user in the future while maintaining performance over standard measure of non personalized relevance 
user modeling on the web ha rested on the fundamental assumption of markovian behavior a user s next action depends only on her current state and not the history leading up to the current state this form the underpinning of pagerank web ranking a well a a number of technique for targeting advertising to user in this work we examine the validity of this assumption using data from a number of web setting our main result invokes statistical order estimation test for markov chain to establish that web user are not in fact markovian we study the extent to which the markovian assumption is invalid and derive a number of avenue for further research 
we study the problem of labeling the edge of a social network graph e g acquaintance connection in facebook a either positive i e trust true friendship or negative i e distrust possible frenemy relation such signed relation provide much stronger signal in tying the behavior of online user than the unipolar homophily effect yet are largely unavailable a most social graph only contain unsigned edge we show the surprising fact that it is possible to infer signed social tie with good accuracy solely based on user behavior of decision making or using only a small fraction of supervision information via unsupervised and semi supervised algorithm this work hereby make it possible to turn an unsigned acquaintance network e g facebook myspace into a signed trust distrust network e g epinion slashdot our result are based on a mixed effect framework that simultaneously capture user behavior social interaction a well a the interplay between the two the framework includes a series of latent factor model and it also encodes the principle of balance and status from social psychology experiment on epinion and yahoo pulse network illustrate that signed social tie can be predicted with high accuracy even in fully unsupervised setting and the predicted signed tie are significantly more useful for social behavior prediction than simple homophily 
nowadays information retrieval ir system have to deal with multiple source of data available in different format datasets can consist of complex and heterogeneous object with relationship between them in addition information need can vary wildly and they can include different task a a result the importance of flexibility in ir system is rapidly growing this fact is specially important in environment where the information required at different moment is very different and it utility may be contingent on timely implementation in these case how quickly a new problem is solved is a important a how well you solve it current system are usually developed for specific case it implies that too much engineering effort is needed to adapt them when new knowledge appears or there are change in the requirement furthermore heterogeneous and linked data present greater challenge a well a the simultaneous application of different task this research proposes the usage of descriptive approach for three different purpose the modelling of the specific task of text classification tc focusing on knowledge and complex data exploitation the flexible application of model to different task and the simultaneously application of different ir task this investigation will contribute to the long term goal of achieving a descriptive and composable ir technology that provides a modular framework that knowledge engineer can compose into a task specific solution the ultimate goal is to develop a flexible framework that offer classifier retrieval model information extractor and other function in addition those functional block could be customised to satisfy user need descriptive approach allow a high level definition of algorithm which are in some case a compact a mathematical formulation one of the expected benefit is to make the implementation clearer and the knowledge transfer easier they allow model from different task to be defined a module that can be concatenated processing the information a a pipeline where some of the output of one module are the input of the following one this combination involves minimum engineering effort due to the paradigm s plug play capability offered by it functional syntax this solution provides the flexibility needed to customise and quickly combine different ir task and or model classification is a desired candidate for being part of a flexible ir framework because it can be required in several situation for different purpose in particular descriptive approach will improve it modelling with complex and heterogeneous object furthermore we aim to show how this approach allows to apply tc model for ad hoc retrieval and vice versa and their simultaneous application for complex information need the main hypothesis of this research is that a seamless approach for modelling tc and it integration with other ir task will provide a general framework for rapid prototyping and modelling of solution for specific user in addition it will allow new complex model that take into account relationship and inference from large ontology the importance of flexibility for information system and the exploitation of complex information and knowledge from heterogeneous source are the main point for discussion the main challenge are expressiveness and scalability abstraction improves flexibility and maintainability however it limit the modelling power a balance between abstraction and expressiveness ha to be reached on the other hand scalability ha been traditionally a challenge for descriptive modelling our goal is to prove the feasibility of our approach for real scale environment 
the explosion of web opinion data ha made essential the need for automatic tool to analyze and understand people s sentiment toward different topic in most sentiment analysis application the sentiment lexicon play a central role however it is well known that there is no universally optimal sentiment lexicon since the polarity of word is sensitive to the topic domain even worse in the same domain the same word may indicate different polarity with respect to different aspect for example in a laptop review large is negative for the battery aspect while being positive for the screen aspect in this paper we focus on the problem of learning a sentiment lexicon that is not only domain specific but also dependent on the aspect in context given an unlabeled opinionated text collection we propose a novel optimization framework that provides a unified and principled way to combine different source of information for learning such a context dependent sentiment lexicon experiment on two data set hotel review and customer feedback survey on printer show that our approach can not only identify new sentiment word specific to the given domain but also determine the different polarity of a word depending on the aspect in context in further quantitative evaluation our method is proved to be effective in constructing a high quality lexicon by comparing with a human annotated gold standard in addition using the learned context dependent sentiment lexicon improved the accuracy in an aspect level sentiment classification task 
web user are increasingly relying on social interaction to complete and validate the result of their search activity while search system are superior machine to get world wide information the opinion collected within friend and expert local community can ultimately determine our decision human curiosity and creativity is often capable of going much beyond the capability of search system in scouting interesting result or suggesting new unexpected search direction such personalized interaction occurs in most time aside of the search system and process possibly instrumented and mediated by a social network when such interaction is completed and user resort to the use of search system they do it through new query loosely related to the previous search or to the social interaction in this paper we propose crowdsearcher a novel search paradigm that embodies crowd a first class source for the information seeking process crowdsearcher aim at filling the gap between generalized search system which operate upon world wide information including fact and recommendation a crawled and indexed by computerized system with social system capable of interacting with real people in real time to capture their opinion suggestion emotion the technical contribution of this paper is the discussion of a model and architecture for integrating computerized search with human interaction by showing how search system can drive and encapsulate social system in particular we show how social platform such a facebook linkedin and twitter can be used for crowdsourcing search related task we demonstrate our approach with several prototype and we report on our experiment upon real user community 
the notion of relevance differs between assessor thus giving rise to assessor disagreement although assessor disagreement ha been frequently observed the factor leading to disagreement are still an open problem in this paper we study the relationship between assessor disagreement and various topic independent factor such a readability and cohesiveness we build a logistic model using reading level and other simple document feature to predict assessor disagreement and rank document by decreasing probability of disagreement we compare the predictive power of these document level feature with that of a meta search feature that aggregate a document s ranking across multiple retrieval run our feature are shown to be on a par with the meta search feature without requiring a large and diverse set of retrieval run to calculate surprisingly however we find that the reading level feature are negatively correlated with disagreement suggesting that they are detecting some other aspect of document content 
the complexity of medical terminology raise challenge when searching medical record for example cancer tumour and neoplasm which are synonym may prevent a traditional search system from retrieving relevant record that contain only synonym of the query term prior work use bag of concept approach to deal with this by representing medical term sharing the same meaning using concept from medical resource e g mesh the relevance score are then combined with a traditional bag of word representation when inferring the relevance of medical record even though the existing approach are effective the predicted retrieval effectiveness of either the bag of word or bag of concept representation which may be used to effectively model the score combination and hence improve retrieval performance is not taken into account in this paper we propose a novel learning framework that model the importance of the bag of word and the bag of concept representation combining their score on a per query basis our proposed framework leverage retrieval performance predictor such a the clarity score and avidf calculated on both representation a learning feature we evaluate our proposed framework using the trec medical record track s test collection a our proposed framework can significantly outperform an existing approach that linearly merges the relevance score we conclude that retrieval performance predictor can be effectively leveraged when combining the relevance score 
user increasingly rely on their mobile device to search local entity typically business while on the go even though recent work ha recognized that the ranking signal in mobile local search e g distance and customer rating score of a business are quite different from general web search they have mostly treated these signal a a black box to extract very basic feature e g raw distance value and rating score without going inside the signal to understand how exactly they affect the relevance of a business however a it ha been demonstrated in the development of general information retrieval model it is critical to explore the underlying behavior heuristic of a ranking signal to design more effective ranking feature in this paper we follow a data driven methodology to study the behavior of these ranking signal in mobile local search using a large scale query log our analysis reveals interesting heuristic that can be used to guide the exploitation of different signal for example user often take the mean value of a signal e g rating from the business result list a a pivot score and tend to demonstrate different click behavior on business with lower and higher signal value than the pivot the clickrate of a business generally is sublinearly decreasing with it distance to the user etc inspired by the understanding of these heuristic we further propose different transformation method to generate more effective ranking feature we quantify the improvement of the proposed new feature using real mobile local search log over a period of month and show that the mean average precision can be improved by over 
modeling user browsing behavior is an active research area with tangible real world application e g organization can adapt their online presence to their visitor browsing behavior with positive effect in user engagement and revenue we concentrate on online news agent and present a semi supervised method for predicting news article that a user will visit after reading an initial article our method tackle the problem using language intent model trained on historical data which can cope with unseen article we evaluate our method on a large set of article and in several experimental setting our result demonstrate the utility of language intent model for predicting user browsing behavior within online news site 
with the explosion of information on any topic the need for ranking is becoming very critical ranking typically depends on several aspect product for example have several aspect like price recency rating etc product ranking ha to bring the best product which is recent and highly rated hence ranking ha to satisfy multiple objective in this paper we explore multi objective ranking of comment using hodge decomposition while hodge decomposition produce a globally consistent ranking a globally inconsistent component is also present we propose an active learning strategy for the reduction of this component finally we develop technique for online hodge decomposition we experimentally validate the idea presented in this paper 
an optimally diverse ranking should achieve the maximum coverage of the aspect underlying an ambiguous or underspecified query with minimum redundancy with respect to the covered aspect although evaluation metric that reward coverage and penalise redundancy provide intuitive objective function for learning a diverse ranking it is unclear whether they are the most effective in this paper we contrast the suitability of relevance and diversity metric a objective function for learning a diverse ranking our result in the context of the diversity task of the trec and web track show that diversity metric are not necessarily better suited for guiding a learning approach moreover the suitability of these metric is compromised a they try to penalise redundancy during the learning process 
digg is a social news website that let people submit article to share their favorite web page e g blog posting or news article and vote the article posted by others digg service currently list the article in the front page by popularity without considering each user s preference to the topic in the article helping user to find the most interesting digg article tailored to each user s own interest will be very useful but it is not an easy task to classify the article according to their topic in order to recommend the article differently to each user in this paper we propose digtobi a personalized recommendation system for digg article using a novel probabilistic modeling our model considers the relevant article with low digg score important a well we show that our model can handle both warm start and cold start scenario seamlessly through a single model we next propose an em algorithm to learn the parameter of our probabilistic model our performance study with digg data confirms the effectiveness of digtobi compared to the traditional recommendation algorithm 
this paper study result diversification in collaborative filtering we argue that the diversification level in a recommendation list should be adapted to the target user individual situation and need different user may have different range of interest the preference of a highly focused user might include only few topic whereas that of the user with broad interest may encompass a wide range of topic thus the recommended item should be diversified according to the interest range of the target user such an adaptation is also required due to the fact that the uncertainty of the estimated user preference model may vary significantly between user to reduce the risk of the recommendation we should take the difference of the uncertainty into account a well in this paper we study the adaptive diversification problem theoretically we start with commonly used latent factor model and reformulate them using the mean variance analysis from the portfolio theory in text retrieval the resulting latent factor portfolio lfp model capture the user s interest range and the uncertainty of the user preference by employing the variance of the learned user latent factor it is shown that the correlation between item and thus the item diversity can be obtained by using the correlation between latent factor topical diversity which in return significantly reduce the computation load our mathematical derivation also reveals that diversification is necessary not only for risk averse system behavior non adpative but also for the target user individual situation adaptive which are represented by the distribution and the variance of the latent user factor our experiment confirm the theoretical insight and show that lfp succeeds in improving latent factor model by adaptively introducing recommendation diversity to fit the individual user s need 
many malicious activity on the web today make use of compromised web server because these server often have high pageranks and provide free resource attacker are therefore constantly searching for vulnerable server in this work we aim to understand how attacker find compromise and misuse vulnerable server specifically we present heat seeking honeypot that actively attract attacker dynamically generate and deploy honeypot page then analyze log to identify attack pattern over a period of three month our deployed honeypot despite their obscure location on a university network attracted more than attacker visit from close to distinct ip address by analyzing these visit we characterize attacker behavior and develop simple technique to identify attack traffic applying these technique to more than regular web server a an example we identified malicious query in almost all of their log 
we present a simple auction mechanism which extends the second price auction with reserve and is truthful in expectation this mechanism is particularly effective in private value environment where the distribution of valuation are irregular bidder can buy it now or alternatively take a chance where the top d bidder are equally likely to win the randomized take a chance allocation incentivizes high valuation bidder to buy it now we show that for a large class of valuation this mechanism achieves similar allocation and revenue a myerson s optimal mechanism and outperforms the second price auction with reserve in addition we present an evaluation of bid data from microsoft s adecn platform we find the valuation are irregular and counterfactual experiment suggest our bin tac mechanism would improve revenue by relative to an optimal second price mechanism with reserve 
the bag of visual word bow framework ha been widely used in query by example video retrieval to model the visual content by a set of quantized local feature descriptor in this paper we propose a novel technique to enhance bow by the selection of word of interest woi that utilizes the quantified temporal motion coherence of the visual word between the adjacent frame in the query example experiment carried out using trecvid datasets show that our technique improves the retrieval performance of the classical bow based approach 
in this paper we present a highly scalable algorithm for structurally clustering webpage for extraction we show that using only the url of the webpage and simple content feature it is possible to cluster webpage effectively and efficiently at the heart of our technique is a principled framework based on the principle of information theory that allows u to effectively leverage the url and combine them with content and structural property using an extensive evaluation over several large full website we demonstrate the effectiveness of our technique at a scale unattainable by previous technique 
a drive by download attack occurs when a user visit a webpage which attempt to automatically download malware without the user s consent attacker sometimes use a malware distribution network mdn to manage a large number of malicious webpage exploit and malware executables in this paper we provide a new method to determine these mdns from the secondary url and redirect chain recorded by a high interaction client honeypot in addition we propose a novel drive by download detection method instead of depending on the malicious content used by previous method our algorithm first identifies and then leverage the url of the mdn s central server where a central server is a common server shared by a large percentage of the drive by download attack in the same mdn a set of regular expression based signature are then generated based on the url of each central server this method allows additional malicious webpage to be identified which launched but failed to execute a successful drive by download attack the new drive by detection system named arrow ha been implemented and we provide a large scale evaluation on the output of a production drive by detection system the experimental result demonstrate the effectiveness of our method where the detection coverage ha been boosted by with an extremely low false positive rate 
the neighbourhood function ng t of a graph g give for each t n the number of pair of node x y such that y is reachable from x in le that t hop the neighbourhood function provides a wealth of information about the graph e g it easily allows one to compute it diameter but it is very expensive to compute it exactly recently the anf algorithm approximate neighbourhood function ha been proposed with the purpose of approximating ng t on large graph we describe a breakthrough improvement over anf in term of speed and scalability our algorithm called hyperanf us the new hyperloglog counter and combine them efficiently through broadword programming our implementation us talk decomposition to exploit multi core parallelism with hyperanf for the first time we can compute in a few hour the neighbourhood function of graph with billion of node with a small error and good confidence using a standard workstation then we turn to the study of the distribution of the distance between reachable node that can be efficiently approximated by mean of hyperanf and discover the surprising fact that it index of dispersion provides a clear cut characterisation of proper social network v web graph we thus propose the spid shortest path index of dispersion of a graph a a new informative statistic that is able to discriminate between the above two type of graph we believe this is the first proposal of a significant new non local structural index for complex network whose computation is highly scalable 
today s working world of knowledge worker is changing rapidly the available information that they need to process is ever growing in addition the characteristic of their work are changing a people can and do their work from home this ha resulted in the need to support knowledge worker in order to prevent burnouts the project swell http www swell project net target this by developing system that support user s mental and physical well being at work and at home in the phd project presented in this abstract we aim at maintaining well being at work through information support 
user generated content is the basic element of social medium website relatively few study have systematically analyzed the motivation to create and share content especially from the perspective of a common user in this paper we perform a comprehensive analysis of user posting behavior on a popular social medium website twitter specifically we assume that user behavior is mainly influenced by three factor breaking news post from social friend and user s intrinsic interest and propose a mixture latent topic model to combine all these factor we evaluated our model on a large scale twitter dataset from three different perspective the perplexity of held out content the performance of predicting retweets and the quality of generated latent topic the result were encouraging our model clearly outperformed it competitor 
many website encourage people to submit review of various product and service we present and evaluate a novel approach to efficiently model and analyze the text within user review to estimate how much reviewer care about different aspect of a product i e amenity food location room etc of a hotel our approach performs statistically quite similar to the best existing method however our method for computing aspect weight is a linear time method while the current state of the art solution requires cubic time at best 
this paper present two new document ranking model for web search based upon the method of semantic representation and the statistical translation based approach to information retrieval ir assuming that a query is parallel to the title of the document clicked on for that query large amount of query title pair are constructed from clickthrough data two latent semantic model are learned from this data one is a bilingual topic model within the language modeling framework it rank document for a query by the likelihood of the query being a semantics based translation of the document the semantic representation is language independent and learned from query title pair with the assumption that a query and it paired title share the same distribution over semantic topic the other is a discriminative projection model within the vector space modeling framework unlike latent semantic analysis and it variant the projection matrix in our model which is used to map from term vector into sematic space is learned discriminatively such that the distance between a query and it paired title both represented a vector in the projected semantic space is smaller than that between the query and the title of other document which have no click for that query these model are evaluated on the web search task using a real world data set result show that they significantly outperform their corresponding baseline model which are state of the art 
structured data in the form of entity and associated attribute ha been a rich web resource for search engine and knowledge database to efficiently extract structured data from enormous website in various vertical e g book restaurant much research effort ha been attracted but most existing approach either require considerable human effort or rely on strong feature that lack of flexibility we consider an ambitious scenario can we build a system that is general enough to handle any vertical without re implementation and requires only one labeled example site from each vertical for training to automatically deal with other site in the same vertical in this paper we propose a unified solution to demonstrate the feasibility of this scenario specifically we design a set of weak but general feature to characterize vertical knowledge including attribute specific semantics and inter attribute layout relationship such feature can be adopted in various vertical without redesign meanwhile they are weak enough to avoid overfitting of the learnt knowledge to seed site given a new unseen site the learnt knowledge is first applied to identify page level candidate attribute value while inevitably involve false positive to remove noise site level information of the new site is then exploited to boost up the true value the site level information is derived in an unsupervised manner without harm to the applicability of the solution promising experimental performance on website in distinct vertical demonstrated the feasibility and flexibility of the proposed solution 
virtually every modern search application either desktop web or mobile feature some kind of query auto completion in it basic form the problem consists in retrieving from a string set a small number of completion i e string beginning with a given prefix that have the highest score according to some static ranking in this paper we focus on the case where the string set is so large that compression is needed to fit the data structure in memory this is a compelling case for web search engine and social network where it is necessary to index hundred of million of distinct query to guarantee a reasonable coverage and for mobile device where the amount of memory is limited we present three different trie based data structure to address this problem each one with different space time complexity trade offs experiment on large scale datasets show that it is possible to compress the string set including the score down to space competitive with the gzip ed data while supporting efficient retrieval of completion at about a microsecond per completion 
human assessment of document relevance are needed for the construction of test collection for ad hoc evaluation and for training text classifier showing document to assessor in different ordering however may lead to different assessment outcome we examine the effect that defineterm threshold priming seeing varying degree of relevant document ha on people s calibration of relevance participant judged the relevance of a prologue of document containing highly relevant moderately relevant or non relevant ocuments followed by a common epilogue of document of mixed relevance we observe that participant exposed to only non relevant document in the prologue assigned significantly higher average relevance score to prologue and epilogue document than participant exposed to moderately or highly relevant document in the prologue we also examine how defineterm need for cognition an individual difference measure of the extent to which a person enjoys engaging in effortful cognitive activity impact relevance assessment high need for cognition participant had a significantly higher level of agreement with expert assessor than low need for cognition participant did our finding indicate that assessor should be exposed to document from multiple relevance level early in the judging process in order to calibrate their relevance threshold in a balanced way and that individual difference measure might be a useful way to screen assessor 
recommender system ha become an effective tool for information filtering which usually provides the most useful item to user by a top k ranking list traditional recommendation technique such a nearest neighbor nn and matrix factorization mf have been widely used in real recommender system however neither approach can well accomplish recommendation task since that most nn method leverage the neighbor s behavior for prediction which may suffer the severe data sparsity problem mf method are le sensitive to sparsity but neighbor influence on latent factor are not fully explored since the latent factor are often used independently to overcome the above problem we propose a new framework for recommender system called collaborative factorization it express the user a the combination of his own factor and those of the neighbor called collaborative latent factor and a ranking loss is then utilized for optimization the advantage of our approach is that it can both enjoy the merit of nn and mf method in this paper we take the logistic loss in ranknet and the likelihood loss in listmle a example and the corresponding collaborative factorization method are called cof net and cof mle our experimental result on three benchmark datasets show that they are more effective than several state of the art recommendation method 
this paper investigates how user cognitively coordinate multitasking web search across different information search problem the analysis suggests that multitasking is a prevalent web search behavior including both sequential multitasking and parallel multitasking multitasking is performed through a task switching process and such a process is supported and underpinned by cognitive coordination mechanism and strategy coordination 
nowadays micro blogging system like twitter have become one of the most important way for information sharing in twitter a user post a message tweet and the others can forward the message retweet mention is a new feature in micro blogging system by mentioning user in a tweet they will receive notification and their possible retweets may help to initiate large cascade diffusion of the tweet to enhance a tweet s diffusion by finding the right person to mention we propose in this paper a novel recommendation scheme named a whom to mention specifically we present an in depth study of mention mechanism and propose a recommendation scheme to solve the essential question of whom to mention in a tweet in this paper whom to mention is formulated a a ranking problem and we try to address several new challenge which are not well studied in the traditional information retrieval task by adopting feature including user interest match content dependent user relationship and user influence a machine learned ranking function is trained based on newly defined information diffusion based relevance the extensive evaluation using data gathered from real user demonstrates the advantage of our proposed algorithm compared with the traditional recommendation method 
we introduce the concept of a vertex collocation profile vcp for the purpose of topological link analysis and prediction vcps provide nearly complete information about the surrounding local structure of embedded vertex pair the vcp approach offer a new tool for domain expert to understand the underlying growth mechanism in their network and to analyze link formation mechanism in the appropriate sociological biological physical or other context the same resolution that give vcp it analytical power also enables it to perform well when used in supervised model to discriminate potential new link we first develop the theory mathematics and algorithm underlying vcps then we demonstrate vcp method performing link prediction competitively with unsupervised and supervised method across several different network family we conclude with timing result that introduce the comparative performance of several existing algorithm and the practicability of vcp computation on large network 
we consider the algorithmic challenge behind a novel interface that simplifies consumer research of online review by surfacing relevant comparable review bundle review for two or more of the item being researched all generated in similar enough circumstance to provide for easy comparison this can be review by the same reviewer or by the same demographic category of reviewer or review focusing on the same aspect of the item but such an interface will work only if the review ecosystem often ha comparable review bundle for common research task here we develop and evaluate practical algorithm for suggesting additional review target to reviewer to maximize comparable pair coverage the fraction of co researched pair of item that have both been reviewed by the same reviewer or more generally are comparable in one of several way we show the exact problem and many subcases to be intractable and give a greedy online linear time approximation for a very general setting and an offline approximation for a narrower setting we evaluate the algorithm on the google local review dataset yielding more than x gain in pair coverage from six month of simulated replacement of existing review by suggested review even allowing for of reviewer ignoring the suggestion the pair coverage grows more than x in the simulation to explore other part of the parameter space we also evaluate the algorithm on synthetic model 
mase provides a sandbox environment for high school student to create their own personalised search interface it ha been designed with two major goal in mind a a hand on tutorial for school child to excite them about programming and computing science through the development of a practical application and to enable child to design and create their own search interface without extensive programming knowledge or prior experience consequently mase provides a way to ascertain what child would like from a search engine interface in an exploratory and creative way a they can create a working prototype this approach contrast with previous work on exploring child s requirement of ir system which attempt to directly elicit user need through more traditional method i e survey interview focus group etc however we have attempted to incorporate the design guideline for child a identified by large into mase where we make use of bright colour large text font spell checking and the use of icon to represent search service a well a including a thematic experience a suggested by large with the use of a puppy avatar and puppy dog footprint 
query auto completion is known to provide poor prediction of the user s query when her input prefix is very short e g one or two character in this paper we show that context such a the user s recent query can be used to improve the prediction quality considerably even for such short prefix we propose a context sensitive query auto completion algorithm nearestcompletion which output the completion of the user s input that are most similar to the context query to measure similarity we represent query and context a high dimensional term weighted vector and resort to cosine similarity the mapping from query to vector is done through a new query expansion technique that we introduce which expands a query by traversing the query recommendation tree rooted at the query in order to evaluate our approach we performed extensive experimentation over the public aol query log we demonstrate that when the recent user s query are relevant to the current query she is typing then after typing a single character nearestcompletion s mrr is higher relative to the mrr of the standard mostpopularcompletion algorithm on average when the context is irrelevant however nearestcompletion s mrr is essentially zero to mitigate this problem we propose hybridcompletion which is a hybrid of nearestcompletion with mostpopularcompletion hybridcompletion is shown to dominate both nearestcompletion and mostpopularcompletion achieving a total improvement of in mrr relative to mostpopularcompletion on average 
the massive semantic data source linked in the web of data give new meaning to old feature like navigation introduce new challenge like semantic specification of web fragment and make it possible to specify action relying on semantic data in this paper we introduce a declarative language to face these challenge based on navigational feature it is designed to specify fragment of the web of data and action to be performed based on these data we implement it in a centralized fashion and show it power and performance finally we explore the same idea in a distributed setting showing their feasibility potentiality and challenge 
collaborative filtering cf is one of the most successful recommendation approach it typically associate a user with a group of like minded user based on their preference over all the item and recommends to the user those item enjoyed by others in the group however we find that two user with similar taste on one item subset may have totally different taste on another set in other word there exist many user item subgroup each consisting of a subset of item and a group of like minded user on these item it is more natural to make preference prediction for a user via the correlated subgroup than the entire user item matrix in this paper to find meaningful subgroup we formulate the multiclass co clustering mcoc problem and propose an effective solution to it then we propose an unified framework to extend the traditional cf algorithm by utilizing the subgroup information for improving their top n recommendation performance our approach can be seen a an extension of traditional clustering cf model systematic experiment on three real world data set have demonstrated the effectiveness of our proposed approach 
with the increasing popularity of mobile and hand held device automatic approach for adapting result to the limited screen size of mobile device are becoming more important traditional approach for reducing the length of textual result include summarization and snippet extraction in this study we investigate document rewriting technique which retain the meaning and readability of the original text evaluation on different document set show that i rewriting document considerably reduces document length and thus scrolling effort on device with limited screen size and ii the rewritten document have a higher readability 
user behavior provides many cue to improve the relevance of search result through personalization one aspect of user behavior that provides especially strong signal for delivering better relevance is an individual s history of query and clicked document previous study have explored how short term behavior or long term behavior can be predictive of relevance ours is the first study to ass how short term session behavior and long term historic behavior interact and how each may be used in isolation or in combination to optimally contribute to gain in relevance through search personalization our key finding include historic behavior provides substantial benefit at the start of a search session short term session behavior contributes the majority of gain in an extended search session and the combination of session and historic behavior out performs using either alone we also characterize how the relative contribution of each model change throughout the duration of a session our finding have implication for the design of search system that leverage user behavior to personalize the search experience 
popular internet service in recent year have shown that remarkable thing can be achieved by harnessing the power of the mass using crowd sourcing system however crowd sourcing system can also pose a real challenge to existing security mechanism deployed to protect internet service many of these security technique rely on the assumption that malicious activity is generated automatically by automated program thus they would perform poorly or be easily bypassed when attack are generated by real user working in a crowd sourcing system through measurement we have found surprising evidence showing that not only do malicious crowd sourcing system exist but they are rapidly growing in both user base and total revenue we describe in this paper a significant effort to study and understand these crowdturfing system in today s internet we use detailed crawl to extract data about the size and operational structure of these crowdturfing system we analyze detail of campaign offered and performed in these site and evaluate their end to end effectiveness by running active benign campaign of our own finally we study and compare the source of worker on crowdturfing site in different country our result suggest that campaign on these system are highly effective at reaching user and their continuing growth pose a concrete threat to online community both in the u and elsewhere 
in this paper we propose latent dirichlet allocation lda based document classification algorithm which doe not require any labeled dataset in our algorithm we construct a topic model using lda assign one topic to one of the class label aggregate all the same class label topic into a single topic using the aggregation property of the dirichlet distribution and then automatically assign a class label to each unlabeled document depending on it closeness to one of the aggregated topic we present an extension to our algorithm based on the combination of expectation maximization em algorithm and a naive bayes classifier we show effectiveness of our algorithm on three real world datasets 
one immediate challenge in searching the deep web database is source selection i e selecting the most relevant web database for answering a given query the existing database selection method both text and relational ass the source quality based on the query similarity based relevance assessment when applied to the deep web these method have two deficiency first is that the method are agnostic to the correctness trustworthiness of the source secondly the query based relevance doe not consider the importance of the result these two consideration are essential for the open collection like the deep web since a number of source provide answer to any query we conjuncture that the agreement between these answer are likely to be helpful in assessing the importance and the trustworthiness of the source we compute the agreement between the source a the agreement of the answer returned while computing the agreement we also measure and compensate for possible collusion between the source this adjusted agreement is modeled a a graph with source at the vertex on this agreement graph a quality score of a source that we call sourcerank is calculated a the stationary visit probability of a random walk we evaluate sourcerank in multiple domain including source in google base with size up to source we demonstrate that the sourcerank track source corruption further our relevance evaluation show that sourcerank improves precision by over the google base and the other baseline method sourcerank ha been implemented in a system called factal 
expanding a query with acronym or their corresponding long form ha not been shown to provide consistent improvement in the biomedical ir literature the major open issue with expanding acronym in a query is their inherent ambiguity a an acronym can refer to multiple long form at the same time a long form identified in a query can be expanded with it acronym s however some of these may be also ambiguous and lead to poor retrieval performance in this work we propose the use of the emim expected mutual information measure between a long form and it abbreviated acronym to measure ambiguity we experiment with expanding both acronym and long form identified in the query from the adhoc task of the trec genomics track our preliminary analysis show the potential of both acronym and long form expansion for biomedical ir 
this work explores the problem of cross lingual pairwise similarity where the task is to extract similar pair of document across two different language solution to this problem are of general interest for text mining in the multi lingual context and have specific application in statistical machine translation our approach take advantage of cross language information retrieval clir technique to project feature vector from one language into another and then us locality sensitive hashing lsh to extract similar pair we show that effective cross lingual pairwise similarity requires working with similarity threshold that are much lower than in typical monolingual application making the problem quite challenging we present a parallel scalable mapreduce implementation of the sort based sliding window algorithm which is compared to a brute force approach on german and english wikipedia collection our central finding can be summarized a no free lunch there is no single optimal solution instead we characterize effectiveness efficiency tradeoff in the solution space which can guide the developer to locate a desirable operating point based on applicationand resource specific constraint 
social hierarchy and stratification among human is a well studied concept in sociology the popularity of online social network present an opportunity to study social hierarchy for different type of network and at different scale we adopt the premise that people form connection in a social network based on their perceived social hierarchy a a result the edge direction in directed social network can be leveraged to infer hierarchy in this paper we define a measure of hierarchy in a directed online social network and present an efficient algorithm to compute this measure we validate our measure using ground truth including wikipedia notability score we use this measure to study hierarchy in several directed online social network including twitter delicious youtube flickr livejournal and curated list of several category of people based on different occupation and different organization our experiment on different online social network show how hierarchy emerges a we increase the size of the network this is in contrast to random graph where the hierarchy decrease a the network size increase further we show that the degree of stratification in a network increase very slowly a we increase the size of the graph 
the task of finding group is a natural extension of search task aimed at retrieving individual entity we introduce a group finding task given a query topic find knowledgeable group that have expertise on that topic we present four general strategy to this task the model are formalized using generative language model two of the model aggregate expertise score of the expert in the same group for the task one locates document associated with expert in the group and then determines how closely the document are associated with the topic whilst the remaining model directly estimate the degree to which a group is a knowledgeable group for a given topic we construct a test collection based on the trec and enterprise collection we find significant difference between different way of estimating the association between a topic and a group experiment show that our knowledgeable group finding model achieve high absolute score 
we developed a simple method of improving the accuracy of rating prediction using feature word extracted from customer review many rating predictor work well for a small and dense dataset of customer review however a practical dataset tends to be large and sparse because it often includes too many product for each customer to buy and evaluate data sparseness reduces prediction accuracy to improve accuracy we reduced the dimension of the feature vector using feature word extracted by analyzing the relationship between rating and accompanying review comment instead of using rating we applied our method to the pranking algorithm and evaluated it on a corpus of golf course review supplied by a japanese e commerce company we found that by successfully reducing data sparseness our method improves prediction accuracy a measured using rankloss 
how do we analyze sentiment over a set of opinionated twitter message this issue ha been widely studied in recent year with a prominent approach being based on the application of classification technique basically message are classified according to the implicit attitude of the writer with respect to a query term a major concern however is that twitter and other medium channel follows the data stream model and thus the classifier must operate with limited resource including labeled data for training classification model this imposes serious challenge for current classification technique since they need to be constantly fed with fresh training message in order to track sentiment drift and to provide up to date sentiment analysis we propose solution to this problem the heart of our approach is a training augmentation procedure which take a input a small training seed and then it automatically incorporates new relevant message to the training data classification model are produced on the fly using association rule which are kept up to date in an incremental fashion so that at any given time the model properly reflects the sentiment in the event being analyzed in order to track sentiment drift training message are projected on a demand driven basis according to the content of the message being classified projecting the training data offer a series of advantage including the ability to quickly detect trending information emerging in the stream we performed the analysis of major event in and we show that the prediction performance remains about the same or even increase a the stream pass and new training message are acquired this result hold for different language even in case where sentiment distribution change over time or in case where the initial training seed is rather small we derive lower bound for prediction performance and we show that our approach is extremely effective under diverse learning scenario providing gain that range from to 
in contrast to traditional web search where topical relevance is often the main selection criterion news search is characterized by the increased importance of freshness however the estimation of relevance and freshness and especially the relative importance of these two aspect are highly specific to the query and the time when the query wa issued in this work we propose a unified framework for modeling the topical relevance and freshness a well a their relative importance based on click log we use click statistic and content analysis technique to define a set of temporal feature which predict the right mix of freshness and relevance for a given query experimental result on both historical click data and editorial judgment demonstrate the effectiveness of the proposed approach 
wikipedia becomes one of the largest knowledge base on the web it ha attracted million page view per day in january however one critical issue for wikipedia is that article in different language are very unbalanced for example the number of article on wikipedia in english ha reached million while the number of chinese article is still le than half million and there are only thousand cross lingual link between article of the two language on the other hand there are more than million chinese wiki article on baidu baike and hudong com two popular encyclopedia in chinese one important question is how to link the knowledge entry distributed in different knowledge base this will immensely enrich the information in the online knowledge base and benefit many application in this paper we study the problem of cross lingual knowledge linking and present a linkage factor graph model feature are defined according to some interesting observation experiment on the wikipedia data set show that our approach can achieve a high precision of with a recall of the approach found new cross lingual link between english wikipedia and baidu baike 
motivated by application to word of mouth advertising we consider a game theoretic scenario in which competing advertiser want to target initial adopter in a social network each advertiser wish to maximize the resulting cascade of influence modeled by a general network diffusion process however competition between product may adversely impact the rate of adoption for any given firm the resulting framework give rise to complex preference that depend on the specific of the stochastic diffusion model and the network topology we study this model from the perspective of a central mechanism such a a social networking platform that can optimize seed placement a a service for the advertiser we ask given the reported demand of the competing firm how should a mechanism choose seed to maximize overall efficiency beyond the algorithmic problem competition raise issue of strategic behaviour rational agent should not be incentivized to underreport their budget demand we show that when there are two player the social welfare can be approximated by a polynomial time strategyproof mechanism our mechanism is defined recursively randomizing the order in which advertiser are allocated seed according to a particular greedy method for three or more player we demonstrate that under additional assumption satisfied by many existing model of influence spread there exists a simpler strategyproof e e approximation mechanism notably this second mechanism is not necessarily strategyproof when there are only two player 
the goal of this tutorial is to expose participant to current research on query performance prediction participant will become familiar with state of the art performance prediction method with common evaluation methodology of prediction quality and with potential application that can utilize performance predictor in addition some open issue and challenge in the field will be discussed this tutorial is an updated version of the sigir tutorial presented by david carmel and elad yom tov on the same subject this year we intend to expand on new result in the field in particular focusing on recently developed framework that provide a unified model for performance prediction 
community question answering cqa service such a yahoo answer and wikianswers have become popular with user a one of the central paradigm for satisfying user information need the task of question retrieval in cqa aim to resolve one s query directly by finding the most relevant question together with their answer from an archive of past question however a user can ask any question that they like a large number of question in cqa are not about objective factual knowledge but about subjective sentiment based opinion or social interaction the inhomogeneous nature of cqa lead to reduced performance of standard retrieval model to address this problem we present a hybrid approach that blend several language modelling technique for question retrieval namely the classic query likelihood language model the state of the art translation based language model and our proposed intent based language model the user intent of each candidate question objective subjective social is given by a probabilistic classifier which make use of both textual feature and metadata feature our experiment on two real world datasets show that our approach can significantly outperform existing one 
online social networking technology enable individual to simultaneously share information with any number of peer quantifying the causal effect of these medium on the dissemination of information requires not only identification of who influence whom but also of whether individual would still propagate information in the absence of social signal about that information we examine the role of social network in online information diffusion with a large scale field experiment that randomizes exposure to signal about friend information sharing among million subject in situ those who are exposed are significantly more likely to spread information and do so sooner than those who are not exposed we further examine the relative role of strong and weak tie in information propagation we show that although stronger tie are individually more influential it is the more abundant weak tie who are responsible for the propagation of novel information this suggests that weak tie may play a more dominant role in the dissemination of information online than currently believed 
online social network have become very popular in recent year and their number of user is already measured in many hundred of million for various commercial and sociological purpose an independent estimate of their size is important in this work algorithm for estimating the number of user in such network are considered the proposed scheme are also applicable for estimating the size of network sub population the suggested algorithm interact with the social network via their public apis only and rely on no other external information due to obvious traffic and privacy concern the number of such interaction is severely limited we therefore focus on minimizing the number of api interaction needed for producing good size estimate we adopt the abstraction of social network a undirected graph and use random node sampling by counting the number of collision or non unique node in the sample we produce a size estimate then we show analytically that the estimate error vanishes with high probability for smaller number of sample than those required by prior art algorithm moreover although our algorithm are provably correct for any graph they excel when applied to social network like graph the proposed algorithm were evaluated on synthetic a well real social network such a facebook imdb and dblp our experiment corroborated the theoretical result and demonstrated the effectiveness of the algorithm 
it is well recognized that user rely on social medium e g twitter or digg to fulfill two common need i e social need and informational need that is to keep in touch with their friend in the real world and to have access to information they are interested in traditional friend recommendation method in social medium mainly focus on a user s social need but seldom address their informational need i e suggesting friend that can provide information one may be interested in but have not been able to obtain so far in this paper we propose to recommend friend according to the informational utility which stand for the degree to which a friend satisfies the target user s unfulfilled informational need called informational friend recommendation in order to capture user informational need we view a post in social medium a an item and utilize collaborative filtering technique to predict the rating for each post the candidate friend are then ranked according to their informational utility for recommendation in addition we also show how to further consider diversity in such recommendation experiment on benchmark datasets demonstrate that our approach can significantly outperform the traditional friend recommendation method under informational evaluation measure 
we focus on the problem of selecting meaningful tweet given a user s interest the dynamic nature of user interest the sheer volume and the sparseness of individual message make this an challenging problem specifically we consider the task of time aware tweet summarization based on a user s history and collaborative social influence from social circle we propose a time aware user behavior model the tweet propagation model tpm in which we infer dynamic probabilistic distribution over interest and topic we then explicitly consider novelty coverage and diversity to arrive at an iterative optimization algorithm for selecting tweet experimental result validate the effectiveness of our personalized time aware tweet summarization method based on tpm 
entity linking el is the task of linking name mention in web text with their referent entity in a knowledge base traditional el method usually link name mention in a document by assuming them to be independent however there is often additional interdependence between different el decision i e the entity in the same document should be semantically related to each other in these case collective entity linking in which the name mention in the same document are linked jointly by exploiting the interdependence between them can improve the entity linking accuracy this paper proposes a graph based collective el method which can model and exploit the global interdependence between different el decision specifically we first propose a graph based representation called referent graph which can model the global interdependence between different el decision then we propose a collective inference algorithm which can jointly infer the referent entity of all name mention by exploiting the interdependence captured in referent graph the key benefit of our method come from the global interdependence model of el decision the purely collective nature of the inference algorithm in which evidence for related el decision can be reinforced into high probability decision experimental result show that our method can achieve significant performance improvement over the traditional el method 
the wide proliferation of powerful smart phone equipped with multiple sensor d graphical engine and g connection ha nurtured the creation of a new spectrum of visual mobile application these application require novel data retrieval technique which we call what you retrieve is what you see wyriwys however state of the art spatial retrieval method are mostly distance based and thus inapplicable for supporting wyriwys motivated by this problem we propose a novel query called spatio visual keyword svk query to support retrieving spatial web object that are both visually conspicuous and semantically relevant to the user to capture the visual feature of spatial web object with extent we introduce a novel visibility metric which computes object visibility in a cumulative manner we propose an incremental method called complete occlusion map based retrieval cor to answer svk query this method exploit effective heuristic to prune the search space and construct a data structure called occlusion map then the method adopts the best first strategy to return relevant object incrementally extensive experiment on real and synthetic data set suggest that our method is effective and efficient when processing svk query 
the data web contains a wealth of knowledge on a large number of domain question answering over interlinked data source is challenging due to two inherent characteristic first different datasets employ heterogeneous schema and each one may only contain a part of the answer for a certain question second constructing a federated formal query across different datasets requires exploiting link between the different datasets on both the schema and instance level we present a question answering system which transforms user supplied query i e natural language sentence or keywords into conjunctive sparql query over a set of interlinked data source the contribution of this paper is two fold firstly we introduce a novel approach for determining the most suitable resource for a user supplied query from different datasets disambiguation we employ a hidden markov model whose parameter were bootstrapped with different distribution function secondly we present a novel method for constructing a federated formal query using the disambiguated resource and leveraging the linking structure of the underlying datasets this approach essentially relies on a combination of domain and range inference a well a a link traversal method for constructing a connected graph which ultimately render a corresponding sparql query the result of our evaluation with three life science datasets and benchmark query demonstrate the effectiveness of our approach 
tweet have become a comprehensive repository for real time information however it is often hard for user to quickly get information they are interested in from tweet owing to the sheer volume of tweet a well a their noisy and informal nature we present quickview an nlp based tweet search platform to tackle this issue specifically it exploit a series of natural language processing technology such a tweet normalization named entity recognition semantic role labeling sentiment analysis tweet classification to extract useful information i e named entity event opinion etc from a large volume of tweet then non noisy tweet together with the mined information are indexed on top of which two brand new scenario are enabled i e categorized browsing and advanced search allowing user to effectively access either the tweet or fine grained information they are interested in 
our objective is to improve the performance of keyword based image search engine by re ranking their original result to this end we address three limitation of existing search engine in this paper first there is no straight forward fully automated way of going from textual query to visual feature image search engine therefore primarily rely on static and textual feature for ranking visual feature are mainly used for secondary task such a finding similar image second image ranker are trained on query image pair labeled with relevance judgment determined by human expert such label are well known to be noisy due to various factor including ambiguous query unknown user intent and subjectivity in human judgment this lead to learning a sub optimal ranker finally a static ranker is typically built to handle disparate user query the ranker is therefore unable to adapt it parameter to suit the query at hand which again lead to sub optimal result we demonstrate that all of these problem can be mitigated by employing a re ranking algorithm that leverage aggregate user click data we hypothesize that image clicked in response to a query are mostly relevant to the query we therefore re rank the original search result so a to promote image that are likely to be clicked to the top of the ranked list our re ranking algorithm employ gaussian process regression to predict the normalized click count for each image and combine it with the original ranking score our approach is shown to significantly boost the performance of the bing image search engine on a wide range of tail query 
ideally student in k grade level can turn to book recommenders to locate book that match their interest existing book recommenders however fail to take into account the readability level of their user and hence their recommendation may be unsuitable for the user to address this issue we introduce brek a recommender that target k user and prioritizes the reading level of it user in suggesting book of interest empirical study conducted using the bookcrossing dataset show that brek outperforms a number of existing recommenders developed for general user in identifying book appealing to k user 
the problem of answering sparql query over virtual sparql view is commonly encountered in a number of setting including while enforcing security policy to access rdf data or when integrating rdf data from disparate source we approach this problem by rewriting sparql query over the view to equivalent query over the underlying rdf data thus avoiding the cost entailed by view materialization and maintenance we show that sparql query rewriting combine the most challenging aspect of rewriting for the relational and xml case like the relational case sparql query rewriting requires synthesizing multiple view like the xml case the size of the rewritten query is exponential to the size of the query and the view in this paper we present the first native query rewriting algorithm for sparql for an input sparql query over a set of virtual sparql view the rewritten query resembles a union of conjunctive query and can be of exponential size we propose optimization over the basic rewriting algorithm to i minimize each conjunctive query in the union ii eliminate conjunctive query with empty result from evaluation and iii efficiently prune out big portion of the search space of empty rewriting the experiment performed on two rdf store show that our algorithm are scalable and independent of the underlying rdf store furthermore our optimization have order of magnitude improvement over the basic rewriting algorithm in both the rewriting size and evaluation time 
result clickthrough statistic and dwell time on clicked result have been shown valuable for inferring search result relevance but the interpretation of these signal can vary substantially for different task and user this paper show that that post click searcher behavior such a cursor movement and scrolling provides additional clue for better estimating document relevance to this end we identify pattern of examination and interaction behavior that correspond to viewing a relevant or non relevant document and design a new post click behavior pcb model to capture these pattern to our knowledge pcb is the first to successfully incorporate post click searcher interaction such a cursor movement and scrolling on a landing page for estimating document relevance we evaluate pcb on a dataset collected from a controlled user study that contains interaction gathered from hundred of unique query result click and page examination the experimental result show that pcb is significantly more effective than using page dwell time information alone both for estimating the explicit judgment of each user and for re ranking the result using the estimated relevance 
retrievability provides a different way to evaluate an information retrieval ir system a it focus on how easily document can be found it is intrinsically related to retrieval performance because a document need to be retrieved before it can be judged relevant in this paper we undertake an empirical investigation into the relationship between the retrievability of document the retrieval bias imposed by a retrieval system and the retrieval performance across different amount of document length normalization to this end two standard ir model are used on three trec test collection to show that there is a useful and practical link between retrievability and performance our finding show that minimizing the bias across the document collection lead to good performance though not the best performance possible we also show that past a certain amount of document length normalization the retrieval bias increase and the retrieval performance significantly and rapidly decrease these finding suggest that the relationship between retrievability and effectiveness may offer a way to automatically tune system 
typically every part in most coherent text ha some plausible reason for it presence some function that it performs to the overall semantics of the text rhetorical relation e g contrast cause explanation describe how the part of a text are linked to each other knowledge about this so called discourse structure ha been applied successfully to several natural language processing task this work study the use of rhetorical relation for information retrieval ir is there a correlation between certain rhetorical relation and retrieval performance can knowledge about a document s rhetorical relation be useful to ir we present a language model modification that considers rhetorical relation when estimating the relevance of a document to a query empirical evaluation of different version of our model on trec setting show that certain rhetorical relation can benefit retrieval effectiveness notably in mean average precision over a state of the art baseline 
in professional search environment such a patent search or legal search search task have unique characteristic user interactively issue several query for a topic and user are willing to examine many retrieval result i e there is typically an emphasis on recall recent survey have also verified that professional searcher continue to have a strong preference for boolean query because they provide a record of what document were searched to support this type of professional search we propose a novel boolean query suggestion technique specifically we generate boolean query by exploiting decision tree learned from pseudo labeled document and rank the suggested query using query quality predictor we evaluate our algorithm in simulated patent and medical search environment compared with a recent effective query generation system we demonstrate that our technique is effective and general 
most of the relevance feedback algorithm only use document term a feedback local feature in order to update the query and re rank the document to show to the user this approach is limited by the term of those document without any global context we propose to use statistical topic modeling technique in relevance feedback to incorporate a better estimate of context by including global information about the document this is particularly helpful for difficult query where learning the context from the interaction with the user is crucial we propose to use the topic mixture information obtained to characterize the document and learn their topic then we rank document incorporating positive and negative feedback by fitting a latent distribution for each class of document online and combining all the feature using bayesian logistic regression we show result using the ohsumed dataset for different variant and obtain higher performance up to in mean average precision map 
in information retrieval we are interested in the information that is not only relevant but also novel in this paper we study how to boost novelty for biomedical information retrieval through probabilistic latent semantic analysis we conduct the study based on trec genomics track data in trec genomics track each topic is considered to have an arbitrary number of aspect and the novelty of a piece of information retrieved called a passage is assessed based on the amount of new aspect it contains in particular the aspect performance of a ranked list is rewarded by the number of new aspect reached at each rank and penalized by the amount of irrelevant passage that are rated higher than the novel one therefore to improve aspect performance we should reach a many aspect a possible and a early a possible in this paper we make a preliminary study on how probabilistic latent semantic analysis can help capture different aspect of a ranked list and improve it performance by re ranking experiment indicate that the proposed approach can greatly improve the aspect level performance over baseline algorithm okapi bm 
we examine different tag ranking strategy for constructing tag cloud to represent collection of tagged object the proposed method are based on random walk on graph diversification and rank aggregation and they are empirically evaluated on a data set of tagged image from flickr 
lab based evaluation typically ass the quality of a retrieval system with respect to it ability to retrieve document that are relevant to the information need of an end user in a real time search task however user not only wish to retrieve the most relevant item but the most recent a well the current evaluation framework is not adequate to ass the ability of a system to retrieve both recent and relevant item and the one proposed in the recent trec microblog track ha certain flaw that quickly became apparent to the organizer in this poster we redefine the experiment for a real time ad hoc search task by setting new submission requirement for the submitted system run proposing metric to be used in evaluating the submission and suggesting a pooling strategy to be used to gather relevance judgment towards the computation of the described metric the proposed task can indeed ass the quality of a retrieval system with regard to retrieving both relevant and timely information 
the theory of structural hole suggests that individual would benefit from filling the hole called a structural hole spanner between people or group that are otherwise disconnected a few empirical study have verified that structural hole spanner play a key role in the information diffusion however there is still lack of a principled methodology to detect structural hole spanner from a given social network in this work we precisely define the problem of mining top k structural hole spanner in large scale social network and provide an objective quality function to formalize the problem two instantiation model have been developed to implement the objective function for the first model we present an exact algorithm to solve it and prove it convergence a for the second model the optimization is proved to be np hard and we design an efficient algorithm with provable approximation guarantee we test the proposed model on three different network coauthor twitter and inventor our study provides evidence for the theory of structural hole e g of twitter user who span structural hole control of the information diffusion on twitter we compare the proposed model with several alternative method and the result show that our model clearly outperform the comparison method our experiment also demonstrate that the detected structural hole spanner can help other social network application such a community kernel detection and link prediction to the best of our knowledge this is the first attempt to address the problem of mining structural hole spanner in large social network 
query biased search result summary or snippet help user decide whether a result is relevant for their information need and have become increasingly important for helping searcher with difficult or ambiguous search task previously published snippet generation algorithm have been primarily based on selecting document fragment most similar to the query which doe not take into account which part of the document the searcher actually found useful we present a new approach to improving result summary by incorporating post click searcher behavior data such a mouse cursor movement and scrolling over the result document to achieve this aim we develop a method for collecting behavioral data with precise association between searcher intent document examination behavior and the corresponding document fragment in turn this allows u to incorporate page examination behavior signal into a novel behavior biased snippet generation system bebs by mining searcher examination data bebs infers document fragment of most interest to user and combine this evidence with text based feature to select the most promising fragment for inclusion in the result summary our extensive experiment and analysis demonstrate that our method improves the quality of result summary compared to existing state of the art method we believe that this work open a new direction for improving search result presentation and we make available the code and the search behavior data used in this study to encourage further research in this area 
supervised learning applied to answer re ranking can highly improve on the overall accuracy of question answering qa system the key aspect is that the relationship and property of the question answer pair composed of a question and the supporting passage of an answer candidate can be efficiently compared with those captured by the learnt model in this paper we define novel supervised approach that exploit structural relationship between a question and their candidate answer passage to learn a re ranking model we model structural representation of both question and answer and their mutual relationship by just using an off the shelf shallow syntactic parser we encode structure in support vector machine svms by mean of sequence and tree kernel which can implicitly represent question and answer pair in huge feature space such model together with the latest approach to fast kernel based learning enabled the training of our rerankers on hundred of thousand of instance which previously rendered intractable for kernelized svms the result on two different qa datasets e g answerbag and jeopardy data show that our model deliver large improvement on passage re ranking task reducing the error in recall of bm baseline by about one of the key finding of this work is that despite it simplicity shallow syntactic tree allow for learning complex relational structure which exhibit a steep learning curve with the increase in the training size 
we model the strategic decision of web site in content market where site may reduce user search cost by aggregating content example aggregation include political news technology and other niche topic website we model this market scenario a an extensive form game of complete information where site choose a set of content to aggregate and user associate with site that are nearest to their interest thus our scenario is a location game in which site choose to aggregate content at a certain point in user preference space and our choice of distance metric jacquard distance induces a lattice structure on the game we provide two variant of this scenario one where user associate with the first site to enter amongst site of equal distance and a second where user choose uniformly between site at equal distance we show that subgame perfect nash equilibrium exist for both game while it appears to be computationally hard to compute equilibrium in both game we show a polynomial time satisficing strategy called frontier descent for the first game a satisficing strategy is not a best response but ensures that earlier site will have positive profit assuming all subsequent site also have positive profit by contrast we show that the second game ha no satisficing solution 
online reputation management is about monitoring and handling the public image of entity such a company on the web an important task in this area is identifying aspect of the entity of interest such a product service competitor key people etc given a stream of microblog post referring to the entity in this paper we compare different ir technique and opinion target identification method for automatically identifying aspect and find that i simple statistical method such a tf idf are a strong baseline for the task significantly outperforming opinion oriented method and ii only considering term tagged a noun improves the result for all the method analyzed 
ranking of retrieval system for focused task requires large number of relevance judgment we propose an approach that minimizes the number of relevance judgment where the performance measure are approximated using a monte carlo sampling technique partial measure are taken using relevance judgment whereas the remaining part of passage are annotated using a generated relevance probability distribution based on result rank we define two condition for stopping the assessment procedure when the ranking between system is stable 
many document with mathematical content are published on the web but conventional search engine that rely on keyword search only cannot fully exploit their mathematical information in particular keyword search is insufficient when expression in a document are not annotated with natural keywords or the user cannot describe her query with keywords retrieving document by querying their mathematical content directly is very appealing in various domain such a education digital library engineering patent document medical science etc capturing the relevance of mathematical expression also greatly enhances document classification in such domain unlike text retrieval where keywords carry enough semantics to distinguish text document and rank them math symbol do not contain much semantic information on their own in fact mathematical expression typically consist of few alphabetical symbol organized in rather complex structure hence the structure of an expression which describes the way such symbol are combined should also be considered unfortunately there is no standard testbed with which to evaluate the effectiveness of a mathematics retrieval algorithm in this paper we study the fundamental and challenging problem in mathematics retrieval that is how to capture the relevance of mathematical expression how to query them and how to evaluate the result we describe various search paradigm and propose retrieval system accordingly we discus the benefit and drawback of each approach and further compare them through an extensive empirical study 
e discovery is the requirement that the document and information in electronic form stored in corporate system be produced a evidence in litigation it ha posed great challenge for legal expert legal searcher have always looked to find any and all evidence for a given case thus a legal search system would essentially be a recall oriented system it ha been a common practice among expert searcher to formulate boolean query to represent their information need we want to work on three basic problem boolean query formulation our primary goal is to study boolean query formulation in the light of the e discovery task this will include automatic boolean query generation expansion and learning the effect of proximity operator in boolean search data fusion we would also like to explore the effectiveness of data fusion technique in improving recall error modeling finally we will work on error modeling method for noisy legal document 
result diversification is an effective method to reduce the risk that none of the returned result satisfies a user s query intention it ha been shown to decrease query abandonment substantially on the other hand computing an optimally diverse set is np hard for the usual objective existing greedy diversification algorithm require random access to the input set rendering them impractical in the context of large result set or continuous data to solve this issue we present a novel diversification approach which treat the input a a stream and process each element in an incremental fashion maintaining a near optimal diverse set at any point in the stream our approach exhibit a linear computation and constant memory complexity with respect to input size without significant loss of diversification quality in an extensive evaluation on several real world data set we show the applicability and efficiency of our algorithm for large result set a well a for continuous query scenario such a news stream subscription 
image tagging is a growing application on social medium website however the performance of many auto tagging method are often poor recent work ha exploited an image s context e g time and location in the tag recommendation process where tag which co occur highly within a given time interval or geographical area are promoted these model however fail to address how and when different image context can be combined in this paper we propose a weighted tag recommendation model building on an existing state of the art which varies the importance of time and location in the recommendation process based on a given set of input tag by retrieving more temporally and geographically relevant tag we achieve statistically significant improvement to recommendation accuracy when testing on k image collected from flickr the result of this paper is an important step towards more effective image annotation and retrieval system 
recently many popular website such a twitter and flickr expose their data through web service apis enabling third party organization to develop client application that provide function alities beyond what the original website offer these client appli cation should follow certain constraint in order to correctly in teract with the web service one common type of such constraint is dependency constraint on parameter given a web service operation o and it parameter pi pj these constraint describe the requirement on one parameter pi that is dependent on the condition of some other parameter s pj for example when requesting the twitter operation get status user timeline a user id parameter must be provided if a screen name parameter is not provided violation of such constraint can cause fatal error or incorrect result in the client application however these con straints are often not formally specified and thus not available for automatic verification of client application to address this issue we propose a novel approach called indicator to automatically infer dependency constraint on parameter for web service via a hybrid analysis of heterogeneous web service artifact including the service documentation the service sdks and the web service themselves to evaluate our approach we applied indicator to infer dependency constraint for four popular web service the result showed that indicator effectively infers constraint with an average precision of and recall of 
in this poster we introduce a web image search reranking approach with exploring multiple modality diff erent from the conventional method that build graph with one feature set for reranking our approach integrates multiple feature set that describe visual content from different aspect we simultaneously integrate the learning of relevance score the weighting of different feature set the distance metric and the scaling for each feature set into a unified scheme experimental result on a large data set that contains more than query and million image demonstrate the effectiveness of our approach 
in this paper we recast static index pruning a a model induction problem under the framework of kullback s principle of minimum cross entropy we show that static index pruning ha an approximate analytical solution in the form of convex integer program further analysis on computation feasibility suggests that one of it surrogate model can be solved efficiently this result ha led to the rediscovery of emph uniform pruning a simple yet powerful pruning method proposed in and later easily ignored by many of u to empirically verify this result we conducted experiment under a new design in which prune ratio is strictly controlled our result on standard ad hoc retrieval benchmark ha confirmed that uniform pruning is robust to high prune ratio and it performance is currently state of the art 
understanding social interaction within group is key to analyzing online community most current work focus on structural property who talk to whom and how such interaction form larger network structure the interaction themselves however generally take place in the form of natural language either spoken or written and one could reasonably suppose that signal manifested in language might also provide information about role status and other aspect of the group s dynamic to date however finding domain independent language based signal ha been a challenge here we show that in group discussion power differential between participant are subtly revealed by how much one individual immediately echo the linguistic style of the person they are responding to starting from this observation we propose an analysis framework based on linguistic coordination that can be used to shed light on power relationship and that work consistently across multiple type of power including a more static form of power based on status difference and a more situational form of power in which one individual experience a type of dependence on another using this framework we study how conversational behavior can reveal power relationship in two very different setting discussion among wikipedians and argument before the u s supreme court 
we report finding on how the user s perception of task difficulty change before and after searching for information to solve task we found that while in one type of task the dependent task this did not change in another the parallel task it did the finding have implication on designing system that can provide assistance to user with their search and task solving strategy 
we introduce a scheme for optimally allocating multiple bit per hyperplane for locality sensitive hashing lsh existing approach binarise lsh projection by thresholding at zero yielding a single bit per dimension we demonstrate that this is a sub optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space our proposed method dubbed neighbourhood preserving quantization npq assigns multiple bit per hyperplane based upon adaptively learned threshold npq exploit a pairwise affinity matrix to discretise each dimension such that nearest neighbour in the original feature space fall within the same quantisation threshold and are therefore assigned identical bit npq is not only applicable to lsh but can also be applied to any low dimensional projection scheme despite using half the number of hyperplanes npq is shown to improve lsh based retrieval accuracy by up to compared to the state of the art 
search personalization and diversification are often seen a opposing alternative to cope with query uncertainty where given an ambiguous query it is either preferable to adapt the search result to a specific aspect that may interest the user personalization or to regard multiple aspect in order to maximize the probability that some query aspect is relevant to the user diversification in this work we question this antagonistic view and hypothesize that these two direction may in fact be effectively combined and enhance each other we research the introduction of the user a an explicit random variable in state of the art diversification method thus developing a generalized framework for personalized diversification in order to evaluate our hypothesis we conduct an evaluation with real user using crowdsourcing service the obtained result suggest that the combination of personalization and diversification achieves competitive performance improving the base line plain personalization and plain diversification approach in term of both diversity and accuracy measure 
this paper proposes a novel product comparison approach the comparative relation between product are first mined from both user review on multiple review website and community based question answering pair containing product comparison information a unified graph model is then developed to integrate the resultant comparative relation for product comparison experiment on popular electronic product show that the proposed approach outperforms the state of the art method 
what other people think ha always been an important piece of information for most of u during the decision making process today people tend to make their opinion available to other people via the internet a a result the web ha become an excellent source of consumer opinion there are now numerous web resource containing such opinion e g product review forum discussion group and blog but it is really difficult for a customer to read all of the review and make an informed decision on whether to purchase the product it is also difficult for the manufacturer of the product to keep track and manage customer opinion also focusing on just user rating star is not a sufficient source of information for a user or the manufacturer to make decision therefore mining online review opinion mining ha emerged a an interesting new research direction extracting aspect and the corresponding rating is an important challenge in opinion mining an aspect is an attribute or component of a product e g zoom for a digital camera a rating is an intended interpretation of the user satisfaction in term of numerical value reviewer usually express the rating of an aspect by a set of sentiment e g great zoom in this tutorial we cover opinion mining in online product review with the focus on aspect based opinion mining this problem is a key task in the area of opinion mining and ha attracted a lot of researcher in the information retrieval community recently several opinion related information retrieval task can benefit from the result of aspect based opinion mining and therefore it is considered a a fundamental problem this tutorial cover not only general opinion mining and retrieval task but also state of the art method challenge application and also future research direction of aspect based opinion mining 
tenure is a critical factor for an individual to consider when making a job transition for instance software engineer make a job transition to senior software engineer in a span of year on average or it take for approximately year for realtor to switch to broker while most existing work on recommender system focus on finding what to recommend to a user this paper place emphasis on when to make appropriate recommendation and it impact on the item selection in the context of a job recommender system the approach we propose however is general and can be applied to any recommendation scenario where the decision making process is dependent on the tenure i e the time interval between successive decision our approach is inspired by the proportional hazard model in statistic it model the tenure between two successive decision and related factor we further extend the model with a hierarchical bayesian framework to address the problem of data sparsity the proposed model estimate the likelihood of a user s decision to make a job transition at a certain time which is denoted a the tenure based decision probability new and appropriate evaluation metric are designed to analyze the model s performance on deciding when is the right time to recommend a job to a user we validate the soundness of our approach by evaluating it with an anonymous job application dataset across industry on linkedin experimental result show that the hierarchical proportional hazard model ha better predictability of the user s decision time which in turn help the recommender system to achieve higher utility user satisfaction 
in recent year the web ha evolved substantially transforming from a place where we primarily find information to a place where we also leave share and keep it this present a fresh set of challenge for the management of personal information which include how to underpin greater awareness and more control over digital belonging and other personally meaningful content that is hosted online in the study reported here we follow up on research that suggests a sense of ownership and control can be reinforced by federating online content a a virtual single store we do this by conducting interview with individual about their web based content participant were asked to give the researcher a tour of online content that is personally meaningful to them to perform a search for themselves in order to uncover additional content and to respond to a series of design envisionments we examine whether there is any value in an integrated personal archive that would automatically update and serve firstly a a source of information regarding the content within it e g where it is stored who ha the right to it and secondly a a resource for crafting personal artefact such a scrapbook cv and gift for others our analysis lead u to reject the concept of a single archive instead we present a framework of five different type of online content each of which ha separate implication for personal information management 
a ever larger training set for learning to rank are created scalability of learning ha become increasingly important to achieving continuing improvement in ranking accuracy exploiting independence of summation form computation we show how each iteration in listnet gradient descent can benefit from parallel execution we seek to draw the attention of the ir community to use spark a newly introduced distributed cluster computing system for reducing training time of iterative learning to rank algorithm unlike mapreduce spark is especially suited for iterative and interactive algorithm our result show near linear reduction in listnet training time using spark on amazon ec cluster 
every day million of crowdsourcing task are performed in exchange for payment despite the important role pricing play in crowdsourcing campaign and the complexity of the market most platform do not provide requester appropriate tool for effective pricing and allocation of task in this paper we introduce a framework for designing mechanism with provable guarantee in crowdsourcing market the framework enables automating the process of pricing and allocation of task for requester in complex market like amazon s mechanical turk where worker arrive in an online fashion and requester face budget constraint and task completion deadline we present constant competitive incentive compatible mechanism for maximizing the number of task under a budget and for minimizing payment given a fixed number of task to complete to demonstrate the effectiveness of this framework we created a platform that enables applying pricing mechanism in market like mechanical turk the platform allows u to show that the mechanism we present here work well in practice a well a to give experimental evidence to worker strategic behavior in absence of appropriate incentive scheme 
peer to peer network are the most popular mechanism for the criminal acquisition and distribution of child pornography cp in this paper we examine observation of peer sharing known cp on the emule and gnutella network which were collected by law enforcement using forensic tool that we developed we characterize a year s worth of network activity and evaluate different strategy for prioritizing investigator limited resource the highest impact research in criminal forensics work within and is evaluated under the constraint and goal of investigation we follow that principle rather than presenting a set of isolated exploratory characterization of user first we focus on strategy for reducing the number of cp file available on the network by removing a minimal number of peer we present a metric for peer removal that is more effective than simply selecting peer with the largest library or the most day online second we characterize six aggressive peer subgroup including peer using tor peer that bridge multiple p p network and the top of peer contributing to file availability we find that these subgroup are more active in their trafficking having more known cp and more uptime than the average peer finally while in theory tor present a challenge to investigator we observe that in practice offender use tor inconsistently over of regular tor user send traffic from a non tor ip at least once after first using tor 
the judging of relevance ha been a subject of study in information retrieval for a long time especially in the creation of relevance judgment for test collection while the criterion by which assessor judge relevance ha been intensively studied little work ha investigated the process individual assessor go through to judge the relevance of a document in this paper we focus on the process by which relevance is judged and in particular the degree of effort a user must expend to judge relevance by better understanding this effort in isolation we may provide data which can be used to create better model of search we present the result of an empirical evaluation of the effort user must exert to judge the relevance of document investigating the effect of relevance level and document size result suggest that relevant document require more effort to judge when compared to highly relevant and not relevant document and that effort increase a document size increase 
knowing user view and demographic trait offer a great potential for personalizing web search result or related service such a query suggestion and query completion such signal however are often only available for a small fraction of search user namely those who log in with their social network account and allow it use for personalization of search result in this paper we offer a solution to this problem by showing how user demographic trait such a age and gender and even political and religious view can be efficiently and accurately inferred based on their search query history this is accomplished in two step we first train predictive model based on the publically available mypersonality dataset containing user facebook like and their demographic information we then match facebook like with search query using open directory project category finally we apply the model trained on facebook like to large scale query log of a commercial search engine while explicitly taking into account the difference between the trait distribution in both datasets we find that the accuracy of classifying age and gender expressed by the area under the roc curve auc are and respectively for prediction based on facebook like and only degrade to and when based on search query on a u state by state basis we find a pearson correlation of for political view between the predicted score and gallup data and for affiliation with judaism between predicted score and data from the u religious landscape survey we conclude that it is indeed feasible to infer important demographic data of user from their query history based on labelled like data and believe that this approach could provide valuable information for personalization and monetization even in the absence of demographic data 
in this paper we propose to explore the relevance between tag for image tag re ranking the key component is to define a global tag tag similarity matrix which is achieved by analysis in both semantic and visual aspect the text semantic relevance is explored by the latent semantic indexing lsi model for the visual information the tag relevance can be propagated by reconstructing exemplar image with visually and semantically consistent image based on our tag relevance matrix a random walk approach is leveraged to discover the significance of each tag finally all tag in an image are re ranked by their significance value extensive experiment show it effectiveness on an image dataset with a large tag vocabulary 
recent year show an increasing interest in vertical search searching within a particular type of information understanding what people search for in these vertical give direction to research and provides pointer for the search engine themselves in this paper we analyze the search log of one particular vertical people search engine based on an extensive analysis of the log of a search engine geared towards finding people we propose a classification scheme for people search at three level a query b session and c user for query we identify three type i event based high profile query people that become popular because of an event happening ii regular high profile query celebrity and iii low profile query other le known people we present experiment on automatic classification of query on the session level we observe five type i family session user looking for relative ii event session querying the main player of an event iii spotting session trying to spot different celebrity online iv polymerous session session without a clear relation between query and v repetitive session query refinement and copying finally for user we identify four type i monitor ii spotter iii follower and iv polymer our finding not only offer insight into search behavior in people search engine but they are also useful to identify future research direction and to provide pointer for search engine improvement 
category hierarchy often evolve at a much slower pace than the document reside in with newly available document kept adding into a hierarchy new topic emerge and document within the same category become le topically cohesive in this paper we propose a novel automatic approach to modifying a given category hierarchy by redistributing it document into more topically cohesive category the modification is achieved with three operation namely sprout merge and assign with reference to an auxiliary hierarchy for additional semantic information the auxiliary hierarchy cover a similar set of topic a the hierarchy to be modified our user study show that the modified category hierarchy is semantically meaningful a an extrinsic evaluation we conduct experiment on document classification using real data from yahoo answer and answerbag hierarchy and compare the classification accuracy obtained on the original and the modified hierarchy our experiment show that the proposed method achieves much larger classification accuracy improvement compared with several baseline method for hierarchy modification 
in this paper a novel approach based on recommendation model is proposed for automatic image annotation for any to be annotated image we first select some related image with tag from training dataset according to their visual similarity and then we estimate the initial rating for tag of the training image based on tag ranking method and construct a rating matrix we also construct a trust matrix based on visual similarity with a k nn strategy then a recommendation model is built on both matrix to rank candidate tag for the target image the proposed approach is evaluated using two benchmark image datasets and experimental result have indicated it effectiveness 
versioned document collection are collection that contain multiple version of each document important example are web archive wikipedia and other wikis or source code and document maintained in revision control system versioned document collection can become very large due to the need to retain past version but there is also a lot of redundancy between version that can be exploited thus versioned document collection are usually stored using special differential delta compression technique and a number of researcher have recently studied how to exploit this redundancy to obtain more succinct full text index structure in this paper we study index organization and compression technique for such versioned full text index structure in particular we focus on the case of positional index structure while most previous work ha focused on the non positional case building on earlier work in z redun we propose a framework for indexing and querying in versioned document collection that integrates non positional and positional index to enable fast top k query processing within this framework we define and study the problem of minimizing positional index size through optimal substring partitioning experiment on wikipedia and web archive data show that our technique achieve significant reduction in index size over previous work while supporting very fast query processing 
combating web spam is one of the greatest challenge for web search engine state of the art anti spam technique focus mainly on detecting variety of spam strategy such a content spamming and link based spamming although these anti spam approach have had much success they encounter problem when fighting against a continuous barrage of new type of spamming technique we attempt to solve the problem from a new perspective by noticing that query that are more likely to lead to spam page site have the following characteristic they are popular or reflect heavy demand for search engine user and there are usually few key resource or authoritative result for them from these observation we propose a novel method that is based on click through data analysis by propagating the spamicity score iteratively between query and url from a few seed page site once we obtain the seed page site we use the link structure of the click through bipartite graph to discover other page site that are likely to be spam experiment show that our algorithm is both efficient and effective in detecting web spam moreover combining our method with some popular anti spam technique such a trustrank achieves improvement compared with each technique taken individually 
labeling human face in image contained in web medium story enables enriching the user experience offered by medium site we propose a lightweight framework for automatic image annotation that exploit named entity mentioned in the article to significantly boost the accuracy of face recognition while previous work in the area labor to train comprehensive offline visual model for a pre defined universe of candidate our approach model the people mentioned in a given story on the y using a standard web image search engine a an image sampling mechanism we overcome multiple source of noise introduced by this ad hoc process to build a fast and robust end to end system from off the shelf error prone text analysis and machine vision component in experiment conducted on approximately face depicted in story from a major celebrity news website we were able to correctly label of the face while mislabeling of them 
our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environment we first develop parameterized family of max algorithm that take a input a set of item and output an item from the set that is believed to be the maximum such max algorithm could for instance select the best facebook profile that match a given person or the best photo that describes a given restaurant then we propose strategy that select appropriate max algorithm parameter our framework support various human error and cost model and we consider many of them for our experiment we evaluate under many metric both analytically and via simulation the tradeoff between three quantity quality monetary cost and execution time also we provide insight on the effectiveness of the strategy in selecting appropriate max algorithm parameter and guideline for choosing max algorithm and strategy for each application 
no one doubt that we have only scratched the surface of what is possible with the web the day is coming fast when the web will become almost a virtual mind reader your intent interest and need will be instantly perceived and the information you want will be promptly delivered whether you ask for it directly or not based on a deep understanding of the meaning of word in your query knowledge of your preference and pattern what others have done before you your location and more in this talk i will share some of my thought about where the web is heading and how search will be transformed to align to this new web laying out some specific behind microsoft s vision to empower people with knowledge 
this tutorial aim to provide a unifying account of current research on diversity and novelty in different ir domain namely in the context of search engine recommender system and data stream 
community question answering cqa platform contain a large number of question and associated answer answerer sometimes include url a part of the answer to provide further information this paper describes a novel way of building a test collection for web search by exploiting the link information from this type of social medium data we propose to build the test collection by regarding cqa question a query and the associated linked web page a relevant document to evaluate this approach we collect approximately ten thousand cqa query whose answer contained link to clueweb document after spam filtering experimental result using this collection show that the relative effectiveness between different retrieval model on the clueweb cqa query set is consistent with that on the trec web track query set confirming the reliability of our test collection further analysis show that the large number of query generated through this approach compensates for the sparse relevance judgment in determining significant difference 
we study the problem of online team formation we consider a setting in which people posse different skill and compatibility among potential team member is modeled by a social network a sequence of task arrives in an online fashion and each task requires a specific set of skill the goal is to form a new team upon arrival of each task so that i each team posse all skill required by the task ii each team ha small communication overhead and iii the workload of performing the task is balanced among people in the fairest possible way we propose efficient algorithm that address all these requirement our algorithm form team that always satisfy the required skill provide approximation guarantee with respect to team communication overhead and they are online competitive with respect to load balancing experiment performed on collaboration network among film actor and scientist confirm that our algorithm are successful at balancing these conflicting requirement this is the first paper that simultaneously address all these aspect previous work ha either focused on minimizing coordination for a single task or balancing the workload neglecting coordination cost 
in this paper we propose a method to rank and assign weight to query term according to their impact on the topic of the query we use search result overlap ratio sror to quantify the overlap of the search result of the full query and a shorten query after removing one term intuitively if the overlap is small it indicates a big topic shift and the removed term should be discriminative and important the sror could be used for measuring query term importance with a search engine automatically by this way learning based model could be trained based on a large number of automatically labeled instance and make prediction for future query efficiently 
the ability to aggregate huge volume of query over a large population of user allows search engine to build precise model for a variety of query assistance feature such a query recommendation correction etc yet no matter how much data is aggregated the long tail distribution implies that a large fraction of query are rare a a result most query assistance service perform poorly or are not even triggered on long tail query we propose a method to extend the reach of query assistance technique and in particular query recommendation to long tail query by reasoning about rule between query template rather than individual query transition a currently done in query flow graph model a a simple example if we recognize that montezuma is a city in the rare query montezuma surf and if the rule city surf beach ha been observed we are able to offer montezuma beach a a recommendation even if the two query were never observed in a same session we conducted experiment to validate our hypothesis first via traditional small scale editorial assessment but more interestingly via a novel automated large scale evaluation methodology our experiment show that general coverage can be relatively increased by using template without penalizing quality furthermore for of the m query in our query flow graph which have no out edge and thus could not be served recommendation we can now offer at least one recommendation in of the case 
deducing trip related information from web scale datasets ha received very large amount of attention recently identifying point of interest poi in geo tagged photo is one of these problem the problem can be viewed a a standard clustering problem of partitioning two dimensional object in this work we study spectral clustering which is the first attempt for the poi identification however there is no unified approach to assign the clustering parameter especially the feature of poi are immensely varying in different metropolitan and location to address this we are intent to study a self tuning technique which can properly assign the parameter for the clustering needed besides geographical information web photo inherently store rich information these information are mutually influenced each others and should be taken into trip related mining task to address this we study reinforcement which construct the relationship over multiple source by iterative learning at last we thoroughly demonstrate our finding by web scale datasets collected from flickr 
query spelling correction is a crucial component of modern search engine existing method in the literature for search query spelling correction have two major drawback first they are unable to handle certain important type of spelling error such a concatenation and splitting second they cannot efficiently evaluate all the candidate correction due to the complex form of their scoring function and a heuristic filtering step must be applied to select a working set of top k most promising candidate for final scoring leading to non optimal prediction in this paper we address both limitation and propose a novel generalized hidden markov model with discriminative training that can not only handle all the major type of spelling error including splitting and concatenation error in a single unified framework but also efficiently evaluate all the candidate correction to ensure the finding of a globally optimal correction experiment on two query spelling correction datasets demonstrate that the proposed generalized hmm is effective for correcting multiple type of spelling error the result also show that it significantly outperforms the current approach for generating top k candidate correction making it a better first stage filter to enable any other complex spelling correction algorithm to have access to a better working set of candidate correction a well a to cover splitting and concatenation error which no existing method in academic literature can correct 
blog post opinion retrieval is the problem of ranking blog post according to the likelihood that the post is relevant to the query and that the author wa expressing an opinion about the topic of the query a recent study ha proposed a method for finding the opinion density at query term position in a document which us the proximity of query term and opinion term a an indicator of their relatedness the maximum opinion density between different query position wa used a an opinion score of the whole document in this paper we investigate the effect of exploiting multiple opinion evidence of a document we propose using the ordered weighted averaging owa operator in order to combine the opinion score of different query position for a final score of a document in the proximity based opinion retrieval system 
triple store implementing the rl profile of owl are becoming increasingly popular in contrast to unrestricted owl the rl profile is known to enjoy favourable computational property for query answering and state of the art rl reasoner such a owlim and oracle s native inference engine of oracle spatial and graph have proved extremely successful in industry scale application the expressive restriction imposed by owl rl may however be problematical for some application in this paper we propose novel technique that allow u in many case to compute exact query answer using an off the shelf rl reasoner even when the ontology is outside the rl profile furthermore in the case where exact query answer cannot be computed we can still compute both lower and upper bound on the exact answer these bound allow u to estimate the degree of incompleteness of the rl reasoner on the given query and to optimise the computation of exact answer using a fully fledged owl reasoner a preliminary evaluation using the rdf semantic graph feature in oracle database ha shown very promising result with respect to both scalability and tightness of the bound 
we consider the task of suggesting related query to user after they issue their initial query to a web search engine we propose a machine learning approach to learn the probability that a user may find a follow up query both useful and relevant given his initial query our approach is based on a machine learning model which enables u to generalize to query that have never occurred in the log a well the model is trained on co occurrence mined from the search log with novel utility and relevance model and the machine learning step is done without any labeled data by human judge the learning step allows u to generalize from the past observation and generate query suggestion that are beyond the past co occurred query this brings significant gain in coverage while yielding modest gain in relevance both offline based on human judge and online based on million of user interaction evaluation demonstrate that our approach significantly outperforms strong baseline 
unlike a traditional social network service a microblogging network like twitter is a hybrid network combining aspect of both social network and information network understanding the structure of such hybrid network and to predict new link are important for many task such a friend recommendation community detection and network growth model in this paper by analyzing data collected over time we find that of new link are to people just two hop away and dynamic of friend acquisition are also related to user account age finally we compare two popular sampling method which are widely used for network analysis and find that forestfire doe not preserve property required for the link prediction task 
the availability of user check in data in large volume from the rapid growing location based social network lbsns enables many important location aware service to user point of interest poi recommendation is one of such service which is to recommend place where user have not visited before several technique have been recently proposed for the recommendation service however no existing work ha considered the temporal information for poi recommendation in lbsns we believe that time play an important role in poi recommendation because most user tend to visit different place at different time in a day eg visiting a restaurant at noon and visiting a bar at night in this paper we define a new problem namely the time aware poi recommendation to recommend poi for a given user at a specified time in a day to solve the problem we develop a collaborative recommendation model that is able to incorporate temporal information moreover based on the observation that user tend to visit nearby poi we further enhance the recommendation model by considering geographical information our experimental result on two real world datasets show that the proposed approach outperforms the state of the art poi recommendation method substantially 
twitter is currently one of the largest social hub for user to spread and discus news for most of the top news story happening there are corresponding discussion on social medium in this demonstration tweetmogaz is presented which is a platform for microblog search and filtering it creates a real time comprehensive report about what people discus and share around news happening in certain region tweetmogaz report the most popular tweet joke video image and news article that people share about top news story moreover it allows user to search for specific topic a scalable automatic technique for microblog filtering is used to obtain relevant tweet to a certain news category in a region tweetmogaz com demonstrates the effectiveness of our filtering technique for reporting public response toward news in different arabic region including egypt and syria in real time 
mobile browser is known to be slow because of the bottleneck in resource loading client only solution to improve resource loading are attractive because they are immediately deployable scalable and secure we present the first publicly known treatment of client only solution to understand how much they can improve mobile browser speed without infrastructure support leveraging an unprecedented set of web usage data collected from iphone user continuously over one year we examine the three fundamental orthogonal approach a client only solution can take caching prefetching and speculative loading speculative loading a is firstly proposed and studied in this work predicts and speculatively load the subresources needed to open a webpage once it url is given we show that while caching and prefetching are highly limited for mobile browsing speculative loading can be significantly more effective empirically we show that client only solution can improve the browser speed by about second on average for website visited by the iphone user we also report the design realization and evaluation of speculative loading in a webkit based browser called tempo on average tempo can reduce browser delay by second 
web search which take it root in the mature field of information retrieval evolved tremendously over the last year the field encountered it first revolution when it started to deal with huge amount of web page then a major step wa accomplished when engine started to consider the structure of the web graph and leveraged link analysis in both crawling and ranking finally a more discrete but no le critical step wa made when search engine started to monitor and exploit the numerous mostly implicit signal provided by user while interacting with the search engine in this tutorial we focus on this revolution of large scale usage data in the first part of this tutorial we focus on usage data which typically refers to any type of information provided by the user while interacting with the search engine it come first under it raw form a a set of individual signal but is typically mined after multiple signal have been aggregated and linked to the same interaction event the two major type of such data are query stream which include the query string that the user issued together with the time stamp of the query a user identifier possibly the ip of the machine on which the browser run and click data which include the reference to the element the user clicked on the page together with the timestamp user identifier possibly ip the rank of the link if it is a result etc exploiting usage data under it multiple form brought an unprecedented wealth of implicit information to web search we discus in the second part of this tutorial some of the key web search application that it made possible one such example is the query spelling correction feature embodied now in all search engine in fact after year of very sophisticated spell checking research simply counting similar query at a small edit distance would in most case surface the most popular spelling a the correct one a beautiful and simple demonstration of the wisdom of crowd principle 
the lack of sufficient labeled web page in many language especially for those uncommonly used one present a great challenge to traditional supervised classification method to achieve satisfactory web page classification performance to address this we propose a novel nonnegative matrix tri factorization nmtf based dual knowledge transfer dkt approach for cross language web page classification which is based on the following two important observation first we observe that web page for a same topic from different language usually share some common semantic pattern though in different representation form second we also observe that the association between word cluster and web page class are a more reliable carrier than raw word to transfer knowledge across language with these recognition we attempt to transfer knowledge from the auxiliary language in which abundant labeled web page are available to target language in which we want classify web page through two different path word cluster approximation and the association between word cluster and web page class due to the reinforcement between these two different knowledge transfer path our approach can achieve better classification accuracy we evaluate the proposed approach in extensive experiment using a real world cross language web page data set promising result demonstrate the effectiveness of our approach that is consistent with our theoretical analysis 
the collaborative filtering cf approach to recommender system ha received much attention recently however previous work mainly focus on improving the formula of rating prediction e g by adding user and item bias implicit feedback and time aware factor etc to reach a better prediction by minimizing an objective function however little effort ha been made on improving cf by incorporating additional regularization to the objective function regularization can further bound the searching range of predicted rating in this paper we improve the conventional rating based objective function by using ranking constraint a the supplementary regularization to restrict the searching of predicted rating in smaller and more likely range and develop a novel method called ranksvd based on the svd model experimental result show that ranksvd achieves better performance than existing main streaming method due to the addition of informative ranking based regularization the idea proposed here can also be easily incorporated to the other cf model 
in recent year a number of open database have emerged on the web providing web user with platform to collaboratively create structured information a these database are intended to accommodate heterogeneous information and knowledge they usually comprise a very large schema and billion of instance browsing and searching data on such a scale is not an easy task for a web user in this context interactive query construction offer an intuitive interface for novice user to retrieve information from database neither requiring any knowledge of structured query language nor any prior knowledge of the database schema however the existing mechanism do not scale well on large scale datasets this paper present a set of technique to boost the scalability of interactive query construction from the perspective of both user interaction cost and performance we connect an abstract ontology layer to the database schema to shorten the process of user computer interaction we also introduce a search mechanism to enable efficient exploration of query interpretation space over large scale data extensive experiment show that our approach scale well on freebase an open database containing more than relational table in more than domain 
in this paper we study the problem of online spelling correction for query completion misspelling is a common phenomenon among search engine query in order to help user effectively express their information need mechanism for automatically correcting misspelled query are required online spelling correction aim to provide spell corrected completion suggestion a a query is incrementally entered a latency is crucial to the utility of the suggestion such an algorithm need to be not only accurate but also efficient to tackle this problem we propose and study a generative model for input query based on a noisy channel transformation of the intended query utilizing spelling correction pair we train a markov n gram transformation model that capture user spelling behavior in an unsupervised fashion to find the top spell corrected completion suggestion in real time we adapt the a search algorithm with various pruning heuristic to dynamically expand the search space efficiently evaluation of the proposed method demonstrates a substantial increase in the effectiveness of online spelling correction over existing technique 
this workshop brings together researcher and practitioner from industry and academia to discus search and discovery in the medi cal domain the event focus on way to make medical and health information more accessible to laypeople including enhancement to ranking algorithm and search interface and how we can dis cover new medical fact and phenomenon from information sought online a evidenced in query stream and other source such a social medium this domain also offer many opportunity for appli cation that monitor and improve quality of life of those affected by medical condition by providing tool to support their health related information behavior 
all pair similarity search can be implemented in two stage the first stage is to partition the data and group potentially similar vector the second stage is to run a set of task where each task compare a partition of vector with other candidate partition because of data sparsity accessing feature vector in memory for runtime comparison in the second stage incurs significant overhead due to the presence of memory hierarchy this paper proposes a cache conscious data layout and traversal optimization to reduce the execution time through size controlled data splitting and vector coalescing it also provides an analysis to guide the optimal choice for the parameter setting our evaluation with several application datasets verifies the performance gain obtained by the optimization and show that the proposed scheme is upto x a fast a the cache oblivious baseline 
most online service provider offer free service to user and in part these service collect and monetize personally identifiable information pii primarily via targeted advertisement against this backdrop of economic exploitation of pii it is vital to understand the value that user put to their own pii although study have tried to discover how user value their privacy little is known about how user value their pii while browsing or the exploitation of their pii extracting valuation of pii from user is non trivial survey cannot be relied on a they do not gather information of the context where pii is being released thus reducing validity of answer in this work we rely on refined experience sampling a data collection method that probe user to valuate their pii at the time and place where it wa generated in order to minimize retrospective recall and hence increase measurement validity for obtaining an honest valuation of pii we use a reverse second price auction we developed a web browser plugin and had user living in spain install and use this plugin for week in order to extract valuation of pii in different context we found that user value item of their online browsing history for about usd and they give higher valuation to their offline pii such a age and address about or usd when it come to pii shared in specific online service user value information pertaining to financial transaction and social network interaction more than activity like search and shopping no significant distinction wa found between valuation of different quantity of pii e g one v search keywords but deviation wa found between type of pii e g photo v keywords finally the user preferred good for exchanging their pii included money and improvement in service followed by getting more free service and targeted advertisement 
the goal of the tutorial is to provide attendee with a comprehensive overview of the latest advance in the development of information retrieval evaluation measure and discus the current challenge in the area a number of topic are covered including background in traditional evaluation paradigm and traditional evaluation measure evaluation measure based on user model advanced model of user interaction with search engine measure based on these model measure for novelty and diversity and session based measure 
we demonstrate how a recent model of social network affiliation network offer powerful cue in local routing within social network a theme made famous by sociologist milgram s six degree of separation experiment this model posit the existence of an interest space that underlies a social network we prove that in network produced by this model not only do short path exist among all pair of node but natural local routing algorithm can discover them effectively specifically we show that local routing can discover path of length o log n to target chosen uniformly at random and path of length o to target chosen with probability proportional to their degree experiment on the co authorship graph derived from dblp data confirm our theoretical result and shed light into the power of one step of lookahead in routing algorithm for social network 
in recent year we have witnessed a rapid growth in the availability of digital multimedia on various application platform and domain consequently the problem of information overload ha become more and more serious in order to tackle the challenge various multimedia recommendation technology have been developed by different research community e g multimedia system information retrieval machine learning and computer version meanwhile many commercial web system e g flick youtube and last fm have successfully applied recommendation technique to provide user personalized content and service in a convenient and flexible way when looking back the information retrieval ir community ha a long history of studying and contributing recommender system design and related issue it ha been proven that the recommender system can effectively assist user in handling information overload and provide high quality personalization while several course were dedicated to multimedia retrieval in the recent decade to the best of our knowledge the tutorial is the first one specifically focusing on multimedia recommender system and their application on various domain and medium content we plan to summarize the research along this direction and provide an impetus for further research on this important topic 
in this paper we illustrate how user generated mobile location data ugmld like foursquare check in can be used in trade area analysis taa by introducing a new framework and corresponding analytic method three key process were created identifying the activity center of a mobile user profiling user based on their location history and modeling user preference probability extension to traditional taa are introduced including customer centric distance decay analysis and check in sequence analysis adopting the rich content and context of ugmld these method introduce new dimension to modeling and delineating trade area analyzing customer visit to a business in the context of their daily life shed new light on the nature and performance of the venue this work ha important business implication in the field of mobile computing 
web search is an integral part of our daily life recently there ha been a trend of personalization in web search where different user receive different result for the same search query the increasing personalization is leading to concern about filter bubble effect where certain user are simply unable to access information that the search engine algorithm decides is irrelevant despite these concern there ha been little quantification of the extent of personalization in web search today or the user attribute that cause it in light of this situation we make three contribution first we develop a methodology for measuring personalization in web search result while conceptually simple there are numerous detail that our methodology must handle in order to accurately attribute difference in search result to personalization second we apply our methodology to user on google web search we find that on average of result show difference due to personalization but that this varies widely by search query and by result ranking third we investigate the cause of personalization on google web search surprisingly we only find measurable personalization a a result of searching with a logged in account and the ip address of the searching user our result are a first step towards understanding the extent and effect of personalization on web search engine today 
after an end user ha partially input a query intelligent search engine can suggest possible completion of the partial query to help end user quickly express their information need all major web search engine and most proposed method that suggest query rely on search engine query log to determine possible query suggestion however for customized search system in the enterprise domain intranet search or personalized search such a email or desktop search or for infrequent query query log are either not available or the user base and the number of past user query is too small to learn appropriate model we propose a probabilistic mechanism for generating query suggestion from the corpus without using query log we utilize the document corpus to extract a set of candidate phrase a soon a a user start typing a query phrase that are highly correlated with the partial user query are selected a completion of the partial query and are offered a query suggestion our proposed approach is tested on a variety of datasets and is compared with state of the art approach the experimental result clearly demonstrate the effectiveness of our approach in suggesting query with higher quality 
result diversification ha gained a lot of attention a a way to answer ambiguous query and to tackle the redundancy problem in the result in the last decade diversification ha been applied on or integrated into the process of pagerankor eigenvector based method that run on various graph including social network collaboration network in academia web and product co purchasing graph for these application the diversification problem is usually addressed a a bicriteria objective optimization problem of relevance and diversity however such an approach is questionable since a query oblivious diversification algorithm that recommends most of it result without even considering the query may perform the best on these commonly used measure in this paper we show the deficiency of popular evaluation technique of diversification method and investigate multiple relevance and diversity measure to understand whether they have any correlation next we propose a novel measure called expanded relevance which combine both relevance and diversity into a single function in order to measure the coverage of the relevant part of the graph we also present a new greedy diversification algorithm called bestcoverage which optimizes the expanded relevance of the result set with e approximation with a rigorous experimentation on graph from various application we show that the proposed method is efficient and effective for many use case 
this paper present musubi a mobile social application platform that enables user to share any data type in real time feed created by any application on the phone musubi is unique in providing a disintermediated service to end user all communication is supported using public key encryption thus leaking no user information to a third party despite the heavy use of cryptography to provide user authentication and access control user found musubi simple to use we embed key exchange within familiar friending action and allow user to interact with any friend in their address book without requiring them to join a common network a priori our feed abstraction allows user to easily exercise access control all data reside on the phone granting user the freedom to apply application of their choice in addition to disintermediating personal messaging we have created an application platform to support multi party software with the same respect for personal data the socialkit library we created on top of musubi s trusted communication protocol facilitates the development of multi party application and integrates with musubi to provide a compelling group application experience socialkit allows developer to make social interactive privacy honoring application without needing to host their own server 
in recent year we have witnessed successful application of machine learning technique to a wide range of information retrieval problem including web search engine recommendation system online advertising etc it is thus critical for researcher in the information retrieval community to understand the core machine learning technique in order to accommodate audience with different level of understanding of machine learning we divide this tutorial into two session the first session will focus on basic machine learning concept and tool in the second session we will introduce more advanced topic in machine learning and will present recent development in machine learning and it application to information retrieval each season is self contained session core learning technology for information retrieval this session of the tutorial will cover the core machine learning method basic optimization technique and key information retrieval application in particular it includes core concept in machine learning such a supervised learning unsupervised learning bias and variance trade off and probabilistic model useful concept and algorithm in optimization including the first and second order gradient method and expectation and maximization the application of machine learning method to key information retrieval problem including text classification collaborative filtering clustering and learning to rank session emerging learning technology for information retrieval this session will cover more advanced machine learning technique that have started to be utilized in information retrieval application in particular it will cover advanced optimization technique including stochastic optimization and smooth minimization emerging learning technique such a multiple instance learning active learning and semi supervised learning the tutorial will benefit a large body of audience in the information retrieval community ranging from student who are new to machine learning to the seasoned researcher who would like to understand the recent advance in machine learning for information retrieval research this tutorial will also benefit the practitioner who apply learning technique to real world information retrieval system 
word cloud are popular for visualizing document but are not a useful for comparing document because identical word are not presented consistently across different cloud we introduce the concept of word storm a visualization tool for analyzing corpus of document a word storm is a group of word cloud in which each cloud represents a single document juxtaposed to allow the viewer to compare and contrast the document we present a novel algorithm that creates a coordinated word storm in which word that appear in multiple document are placed in the same location using the same color and orientation across cloud this ensures that similar document are represented by similar looking word cloud making them easier to compare and contrast visually we evaluate the algorithm using an automatic evaluation based on document classification and a user study the result confirm that a coordinated word storm allows for better visual comparison of document 
it is very challenging task to understand a short query especially if that query is considered in isolation luckily query do magically appear in a search box rather they are issued by real people trying to accomplish a task at a given point in time and space and this context can be used to aid query understanding traditionally search engine have returned the same result to everyone who asks the same question however using a single ranking for everyone in every context limit how well a search engine can do in this talk i outline a framework to quantify the potential for personalization that can be used to characterize the extent to which different people have the same or different intent for a query i then describe several example of how we represent and use different kind of context to improve search quality finally i conclude by highlighting some important challenge in developing such system at web scale including system optimization evaluation transparency and serendipity 
huge amount of search log data have been accumulated in various search engine currently a commercial search engine receives billion of query and collect tera byte of log data on any single day other than search log data browse log can be collected by client side browser plug in which record the browse information if user permission are granted such massive amount of search browse log data on the one hand provide great opportunity to mine the wisdom of crowd and improve web search result on the other hand designing effective and efficient method to clean model and process large scale log data also present great challenge in this tutorial we will focus on mining search and browse log data for search engine we will start with an introduction of search and browse log data and an overview of frequently used data summarization in log mining we will then elaborate how log mining application enhance the five major component of a search engine namely query understanding document understanding query document matching user understanding and monitoring and feedback for each aspect we will survey the major task fundamental principle and state of the art method finally we will discus the challenge and future trend of log data mining the goal of this tutorial is to provide a systematic survey on large scale search browse log mining to the ir community it may help ir researcher to get familiar with the core challenge and promising direction in log mining at the same time this tutorial may also serve the developer of web information retrieval system a a comprehensive and in depth reference to the advanced log mining technique 
expert finding is a task of finding knowledgeable people on a given topic state of the art expertise retrieval algorithm identify matching expert based on analysis of textual content of document expert are associated with while powerful these model ignore social structure that might be available in this paper we develop a bayesian hierarchical model for expert finding that account for both social relationship and content the model assumes that social link are determined by expertise similarity between candidate we demonstrate the improved retrieval performance of our model over the baseline on a realistic data set 
this paper present an application for medicinal plant prescription based on text classification technique the system receives a an input a free text describing the symptom of a user and retrieves a ranked list of medicinal plant related to those symptom in addition a set of link to wikipedia are also provided enriching the information about every medicinal plant presented to the user in order to improve the accessibility to the application the input can be written in six different language adapting the result accordingly the application interface can be accessed from different device and platform 
the web is far le usable and accessible for people with vision impairment than it is for sighted people web automation a process of automating browsing action on behalf of the user ha the potential to bridge the divide between the way sighted and people with vision impairment access the web specifically it can enable the latter to breeze through web browsing task that beforehand were slow hard or even impossible to accomplish typical web automation requires that the user record a macro a sequence of browsing step so that these step can be automated in the future by replaying the macro however for people with vision impairment automation with macro is not usable in this paper we propose a novel model based approach that facilitates web automation without having to either record or replay macro using the past browsing history and the current web page a the browsing context the proposed model can predict the most probable browsing action that the user can do the model construction is unsupervised more importantly the model is continuously and incrementally updated a history evolves thereby ensuring the prediction are not outdated we also describe a novel interface that let the user focus on the object associated with the most probable predicted browsing step e g clicking link and filling out form and facilitates automatic execution of the selected step a study with blind participant showed that the proposed approach dramatically reduced the interaction time needed to accomplish typical browsing task and the user interface wa perceived to be much more usable than the standard screen reading interface 
recommending product to consumer mean not only understanding their taste but also understanding their level of experience for example it would be a mistake to recommend the iconic film seven samurai simply because a user enjoys other action movie rather we might conclude that they will eventually enjoy it once they are ready the same is true for beer wine gourmet food or any product where user have acquired taste the best product may not be the most accessible thus our goal in this paper is to recommend product that a user will enjoy now while acknowledging that their taste may have changed over time and may change again in the future we model how taste change due to the very act of consuming more product in other word a user become more experienced we develop a latent factor recommendation system that explicitly account for each user s level of experience we find that such a model not only lead to better recommendation but also allows u to study the role of user experience and expertise on a novel dataset of fifteen million beer wine food and movie review 
various click model have been recently proposed a a principled approach to infer the relevance of document from the clickthrough data the inferred document relevance is potentially useful in evaluating the web retrieval system in practice it generally requires to acquire the accurate evaluation result within minimal user query submission this problem is important for speeding up search engine development and evaluation cycle and acquiring reliable evaluation result on tail query in this paper we propose a reordering framework for efficient evaluation problem in the context of clickthrough based web retrieval evaluation the main idea is to move up the document that contribute more for the evaluation task in this framework we propose four intuition and formulate them a an optimization problem both user study and trec data based experiment validate that the reordering framework result in much fewer query submission to get accurate evaluation result with only a little harm to the user utility 
an increasing amount of application build their functionality on the utilisation and manipulation of web resource consequently rest gain popularity with a resource centric interaction architecture that draw it flexibility from link between resource linked data offer a uniform data model for rest with self descriptive resource that can be leveraged to avoid a manual ad hoc development of web based application for declaratively specifying interaction between web resource we introduce data fu a lightweight declarative rule language with state transition system a formal grounding data fu enables the development of data driven application that facilitate the restful manipulation of read write linked data resource furthermore we describe an interpreter for data fu a a general purpose engine that allows to perform described interaction with web resource by order of magnitude faster than a comparable linked data processor 
it ha been recognized that when an information retrieval ir system achieves improvement in mean retrieval effectiveness e g mean average precision map over all the query the performance e g average precision ap of some individual query could be hurt resulting in retrieval instability some stability robustness metric have been proposed however they are often defined separately from the mean effectiveness metric consequently there is a lack of a unified formulation of effectiveness stability and overall retrieval quality considering both in this paper we present a unified formulation based on the bias variance decomposition correspondingly a novel evaluation methodology is developed to evaluate the effectiveness and stability in an integrated manner a case study applying the proposed methodology to evaluation of query language modeling illustrates the usefulness and analytical power of our approach 
a proposal summary for the eurohcir workshop at sigir 
cascading style sheet cs took a valuable step towards separating web content from presentation but html page still contain large amount of design scaffolding needed to hierarchically layer content for proper presentation this paper present cascading tree sheet ct a cs like language for separating this presentational html from real content with ct author can use standard cs selector to describe how to graft presentational scaffolding onto their pure content html this improved separation of content from presentation enables even naive author to incorporate rich layout including interactive javascript into their own page simply by linking to a tree sheet and adding some class name to their html 
different from a large body of research on social network that ha focused almost exclusively on positive relationship we study signed social network with both positive and negative link specifically we focus on how to reliably and effectively predict the sign of link in a newly formed signed social network called a target network since usually only a very small amount of edge sign information is available in such newly formed network this small quantity is not adequate to train a good classifier to address this challenge we need assistance from an existing mature signed network called a source network which ha abundant edge sign information we adopt the transfer learning approach to leverage the edge sign information from the source network which may have a different yet related joint distribution of the edge instance and their class label a there is no predefined feature vector for the edge instance in a signed network we construct generalizable feature that can transfer the topological knowledge from the source network to the target with the extracted feature we adopt an adaboost like transfer learning algorithm with instance weighting to utilize more useful training instance in the source network for model learning experimental result on three real large signed social network demonstrate that our transfer learning algorithm can improve the prediction accuracy by over baseline method 
user expectation and experience for web search and ecommerce product search are quite different product description are concise a compared to typical web document user expectation is more specific to find the right product the difference in the publisher and searcher vocabulary in case of product search the seller and the buyer vocabulary combined with the fact that there are fewer product to search over than web document result in observable number of search that return no result zero recall search in this paper we describe a study of zero recall search our study is focused on ecommerce search and us data from a leading ecommerce site s user click stream log there are main contribution of our study the cause of zero recall search a study of user s reaction and recovery from zero recall a study of difference in behavior of power user versus novice user to zero recall search 
result diversification deal with ambiguous or multi faceted query by providing document that cover a many subtopics of a query a possible various approach to subtopic modeling have been proposed subtopics have been extracted internally e g from retrieved document and externally e g from web resource such a query log internally modeled subtopics are often implicitly represented e g a latent topic while externally modeled subtopics are often explicitly represented e g a reformulated query we propose a framework that i combine both implicitly and explicitly represented subtopics and ii allows flexible combination of multiple external resource in a transparent and unified manner specifically we use a random walk based approach to estimate the similarity of the explicit subtopics mined from a number of heterogeneous resource click log anchor text and web n gram we then use these similarity to regularize the latent topic extracted from the top ranked document i e the internal implicit subtopics empirical result show that regularization with explicit subtopics extracted from the right resource lead to improved diversification result indicating that the proposed regularization with explicit external resource form better implicit topic model click log and anchor text are shown to be more effective resource than web n gram under current experimental setting combining resource doe not always lead to better result but achieves a robust performance this robustness is important for two reason it cannot be predicted which resource will be most effective for a given query and it is not yet known how to reliably determine the optimal model parameter for building implicit topic model 
this paper focus on exploring the feature of product review that satisfy user by which to improve the automatic helpfulness voting for the review on commercial website compared to the previous work which single mindedly adopts the textual feature to ass the review helpfulness we propose that user preference are more explicit clue to infer the opinion of user on the review helpfulness by using the user preference based feature we firstly implement a binary helpfulness based review classification system to divide helpful review and useless and on the basis we secondly build a ranking svm based automatic helpfulness voting system ahv which rank review based on their helpfulness experiment used a large scale dataset containing over review on product to test the system which achieves promising performance with accuracy of up to and ndcg of and at least accuracy improvement compared to the textual feature based helpfulness assessment 
conventional retrieval system view document a a unit and look at different retrieval type within a document we introduce proteus a frame work for seamlessly navigating book a dynamic collection which are defined on the fly proteus allows u to search various retrieval type navigable type include page book named person location and picture in a collection of book taken from the internet archive the demonstration show the value of multi type browsing in dynamic collection to peruse new data 
search engine train and apply a single ranking model across all user but searcher information need are diverse and cover a broad range of topic hence a single user independent ranking model is insufficient to satisfy different user result preference conventional personalization method learn separate model of user interest and use those to re rank the result from the generic model those method require significant user history information to learn user preference have low coverage in the case of memory based method that learn direct association between query url pair and have limited opportunity to markedly affect the ranking given that they only re order top ranked item in this paper we propose a general ranking model adaptation framework for personalized search using a given user independent ranking model trained offline and limited number of adaptation query from individual user the framework quickly learns to apply a series of linear transformation e g scaling and shifting over the parameter of the given global ranking model such that the adapted model can better fit each individual user s search preference extensive experimentation based on a large set of search log from a major commercial web search engine confirms the effectiveness of the proposed method compared to several state of the art ranking model adaptation method 
in this demo we present crowdterrier an infrastructure extension to the open source terrier ir platform that enables the semi automatic generation of relevance assessment for a variety of document ranking task using crowdsourcing the aim of crowdterrier is to reduce the time and expertise required to effectively crowdsource relevance assessment by abstracting away from the complexity of the crowdsourcing process it achieves this by automating the assessment process a much a possible via a close integration of the ir system that rank the document terrier and the crowdsourcing marketplace that is used to ass those document amazon s mechanical turk 
user behavior on the web change over time for example the query that people issue to search engine and the underlying informational goal behind the query vary over time in this paper we examine how to model and predict this temporal user behavior we develop a temporal modeling framework adapted from physic and signal processing that can be used to predict time varying user behavior using smoothing and trend we also explore other dynamic of web behavior such a the detection of periodicity and surprise we develop a learning procedure that can be used to construct model of user activity based on feature of current and historical behavior the result of experiment indicate that by using our framework to predict user behavior we can achieve significant improvement in prediction compared to baseline model that weight historical evidence the same for all query we also develop a novel learning algorithm that explicitly learns when to apply a given prediction model among a set of such model our improved temporal modeling of user behavior can be used to enhance query suggestion crawling policy and result ranking 
the evaluation of information retrieval ir system over special collection such a large book repository is out of reach of traditional method that rely upon editorial relevance judgment increasingly the use of crowdsourcing to collect relevance label ha been regarded a a viable alternative that scale with modest cost however crowdsourcing suffers from undesirable worker practice and low quality contribution in this paper we investigate the design and implementation of effective crowdsourcing task in the context of book search evaluation we observe the impact of aspect of the human intelligence task hit design on the quality of relevance label provided by the crowd we ass the output in term of label agreement with a gold standard data set and observe the effect of the crowdsourced relevance judgment on the resulting system ranking this enables u to observe the effect of crowdsourcing on the entire ir evaluation process using the test set and experimental run from the inex book track we find that varying the hit design and the pooling and document ordering strategy lead to considerable difference in agreement with the gold set label we then observe the impact of the crowdsourced relevance label set on the relative system ranking using four ir performance metric system ranking based on map and bpref remain le affected by different label set while the precision and ndcg lead to dramatically different system ranking especially for label acquired from hit with weaker quality control overall we find that crowdsourcing can be an effective tool for the evaluation of ir system provided that care is taken when designing the hit 
cellular network employ a specific radio resource management policy distinguishing them from wired and wi fi network a lack of awareness of this important mechanism potentially lead to resource inefficient mobile application we perform the first network wide large scale investigation of a particular type of application traffic pattern called periodic transfer where a handset periodically exchange some data with a remote server every t second using packet trace containing billion packet collected from a commercial cellular carrier we found that periodic transfer are very prevalent in today s smartphone traffic however they are extremely resource inefficient for both the network and end user device even though they predominantly generate very little traffic this somewhat counter intuitive behavior is a direct consequence of the adverse interaction between such periodic transfer pattern and the cellular network radio resource management policy for example for popular smartphone application such a facebook periodic transfer account for only of the overall traffic volume but contribute to of the total handset radio energy consumption we found periodic transfer are generated for various reason such a keep alive polling and user behavior measurement we further investigate the potential of various traffic shaping and resource control algorithm depending on their traffic pattern application exhibit disparate response to optimization strategy jointly using several strategy with moderate aggressiveness can eliminate almost all energy impact of periodic transfer for popular application such a facebook and pandora 
we present first result of a logfile analysis on web search engine for child the aim of this research is to analyse fundamental fact about how child s web search behaviour differs from that of adult we show difference to previous result which are often based on small lab experiment our large scale analysis suggests that child search query are more information oriented and shorter on average child indeed make a lot of spelling error and often repeat search and revisit web page 
logic based information retrieval ir model represent the retrieval decision a a logical implication d q between a document d and a query q where d and q are logical sentence however d q is a binary decision we thus need a measure to estimate the degree to which d implies q denoted p d q in this study we revisit the van rijsbergen s assumption about the logical implication is not the material one and p d q could be estimated by the conditional probability p q d more precisely we claim that the material implication is an appropriate implication for ir and also we mathematically prove that replacing p d q by p q d is a correct choice in order to prove the van rijsbergen s assumption we use the propositional logic and the lattice theory we also exploit the notion of degree of implication that is proposed by knuth 
browse with either web directory or social bookmark is an important complementation to search by keywords in web information retrieval to improve user browse experience and facilitate the web directory construction in this paper we propose a novel browse system called social web directory swd for short by integrating web directory and social bookmark in swd web page are automatically categorized to a hierarchical structure to be retrieved efficiently and the popular web page hottest tag and expert user in each category are ranked to help user find information more conveniently extensive experimental result demonstrate the effectiveness of our swd system 
kernel method km are powerful machine learning technique that can alleviate the data representation problem a they substitute scalar product between feature vector with similarity function kernel directly defined between data instance e g syntactic tree thus feature are not needed any longer this tutorial aim at introducing essential and simplified theory of support vector machine and km for the design of practical application it will describe effective kernel for easily engineering automatic classifier and learning to rank algorithm using structured data and semantic processing some example will be drawn from question answering passage re ranking short and long text categorization relation extraction named entity recognition co reference resolution moreover some practical demonstration will be given using the svm light tk tree kernel toolkit 
although context independent word based approach remain popular for cross language information retrieval many recent study have shown that integrating insight from modern statistical machine translation system can lead to substantial improvement in effectiveness in this paper we compare flat and hierarchical phrase based translation model for query translation both approach yield significantly better result than either a token based or a one best translation baseline on standard test collection the choice of model manifest interesting tradeoff in term of effectiveness efficiency and model compactness 
when browser report tl error they cannot distinguish between attack and harmless server misconfigurations hence they leave it to the user to decide whether continuing is safe however actual attack remain rare a a result user quickly become used to false positive that deplete their attention span making it unlikely that they will pay sufficient scrutiny when a real attack come along consequently browser vendor should aim to minimize the number of low risk warning they report to guide that process we perform a large scale measurement study of common tl warning using a set of passive network monitor located at different site we identify the prevalence of warning for a total population of about user over a nine month period we identify low risk scenario that consume a large chunk of the user attention budget and make concrete recommendation to browser vendor that will help maintain user attention in high risk situation we study the impact on end user with a data set much larger in scale than the data set used in previous tl measurement study a key novelty of our approach involves the use of internal browser code instead of generic tl library for analysis providing more accurate and representative result 
this paper investigates a framework of search based face annotation sbfa by mining weakly labeled facial image that are freely available on the world wide web www one challenging problem for search based face annotation scheme is how to effectively perform annotation by exploiting the list of most similar facial image and their weak label that are often noisy and incomplete to tackle this problem we propose an effective unsupervised label refinement ulr approach for refining the label of web facial image using machine learning technique we formulate the learning problem a a convex optimization and develop effective optimization algorithm to solve the large scale learning task efficiently to further speed up the proposed scheme we also propose a clustering based approximation algorithm which can improve the scalability considerably we have conducted an extensive set of empirical study on a large scale web facial image testbed in which encouraging result showed that the proposed ulr algorithm can significantly boost the performance of the promising sbfa scheme 
conventional search engine usually return a ranked list of web page in response to a query user have to visit several page to locate the relevant part a promising future search scenario should involve understanding user intent providing relevant information directly to satisfy searcher need a opposed to relevant page in this paper we present a search paradigm to summarize a query s information from different aspect query aspect could be aligned to user intent the generated summary for query aspect are expected to be both specific and informative so that user can easily and quickly find relevant information specifically we use a composite query for summarization method where a set of component query are used for providing additional information for the original query the system leverage the search engine to proactively gather information by submitting multiple component query according to the original query and it aspect in this way we could get more relevant information for each query aspect and roughly classify information by comparative mining the search result of different component query it is able to identify query dependent aspect word which help to generate more specific and informative summary the experimental result on two data set wikipedia and trec clueweb are encouraging our method outperforms two baseline method on generating informative summary 
result merging is an important step in federated search to merge the document returned from multiple source specific ranked list for a user query previous result merging method such a semi supervised learning ssl and sampleagglomerate fitting estimate safe use regression method to estimate global document score from document rank in individual ranked list ssl relies on overlapping document that exist in both individual ranked list and a centralized sample database safe go a step further by using both overlapping document with accurate rank information and document with estimated rank information for regression however existing method do not distinguish the accurate rank information from the estimated information furthermore all document are assigned equal weight in regression while intuitively document in the top should carry higher weight this paper proposes a weighted curve fitting method for result merging in federated search the new method explicitly model the importance of information from overlapping document over non overlapping one it also weight document at different position differently empirically result on two datasets clearly demonstrate the advantage of the proposed algorithm 
micro blogging service have become indispensable communication tool for online user for disseminating breaking news eyewitness account individual expression and protest group recently twitter along with other online social networking service such a foursquare gowalla facebook and yelp have started supporting location service in their message either explicitly by letting user choose their place or implicitly by enabling geo tagging which is to associate message with latitude and longitude this functionality allows researcher to address an exciting set of question how is information created and shared across geographical location how do spatial and linguistic characteristic of people vary across region and how to model human mobility although many attempt have been made for tackling these problem previous method are either complicated to be implemented or oversimplified that cannot yield reasonable performance it is a challenge task to discover topic and identify user interest from these geo tagged message due to the sheer amount of data and diversity of language variation used on these location sharing service in this paper we focus on twitter and present an algorithm by modeling diversity in tweet based on topical diversity geographical diversity and an interest distribution of the user furthermore we take the markovian nature of a user s location into account our model exploit sparse factorial coding of the attribute thus allowing u to deal with a large and diverse set of covariates efficiently our approach is vital for application such a user profiling content recommendation and topic tracking we show high accuracy in location estimation based on our model moreover the algorithm identifies interesting topic based on location and language 
there is a long history of developing efficient algorithm for set intersection which is a fundamental operation in information retrieval and database in this paper we describe a new data structure a cardinality filter to quickly compute an upper bound on the size of a set intersection knowing an upper bound of the size can be used to accelerate many application such a top k query processing in text mining given finite set a and b the expected computation time for the upper bound of the size of the intersection a cap b is o a b w where w is the machine word length this is much faster than the current best algorithm for the exact intersection which run in o a b w a cap b expected time our performance study show that our implementation of cardinality filter are from to time faster than existing set intersection algorithm and the time for a top k query in a text mining application can be reduced by half 
building test collection based on nugget is useful evaluating system that return document answer or summary however nugget construction requires a lot of manual work and is not feasible for large query set towards an efficient and scalable nugget based evaluation we study the applicability of semi automatic nugget extraction in the context of the ongoing ntcir one click access click task we compare manually extracted and semi automatically extracted japanese nugget to demonstrate the coverage and efficiency of the semi automatic nugget extraction our finding suggest that the manual nugget extraction can be replaced with a direct adaptation of the english semi automatic nugget extraction system especially for query for which the user desire broad answer from free form text 
it is well known that textual information such a video transcript and video review can significantly enhance the performance of video summarization algorithm unfortunately many video on the web such a those from the popular video sharing site youtube do not have useful textual information the goal of this paper is to propose a transfer learning framework for video summarization in the training process both the video feature and textual feature are exploited to train a summarization algorithm while for summarizing a new video only it video feature are utilized the basic idea is to explore the transferability between video and their corresponding textual information based on the assumption that video feature and textual feature are highly correlated with each other we can transfer textual information into knowledge on summarization using video information only in particular we formulate the video summarization problem a that of learning a mapping from a set of shot of a video to a subset of the shot using the general framework of svm based structured learning textual information is transferred by encoding them into a set of constraint used in the structured learning process which tend to provide a more detailed and accurate characterization of the different subset of shot experimental result show significant performance improvement of our approach and demonstrate the utility of textual information for enhancing video summarization 
this paper describes an approach for identifying translation of book in large scanned book collection with ocr error the method is based on the idea that although individual sentence do not necessarily preserve the word order when translated a book must preserve the linear progression of idea for it to be a valid translation consider two book in two different language say english and german the english book in the collection is represented by the sequence of word in the order they appear in the text which appear only once in the book similarly the book in german is represented by it sequence of word which appear only once an english german dictionary is used to transform the word sequence of the english book into german by translating individual word in place it is not necessary to translate all the word and this method work even with small dictionary both sequence are now in german and can therefore be aligned using a longest common subsequence lcs algorithm we describe two scoring function trans c and trans it which account for both the lcs length and the length of the original word sequence experiment demonstrate that trans it is particularly successful in finding translation of book and outperforms several baseline including metadata search based on matching title and author experiment performed on a europarl parallel corpus for four language pair english finnish english french english german english spanish and a scanned book collection of k english german book show that the proposed method retrieves translation of book with an average map score of and a speed of k book pair comparison per second on a single core 
the social customer relationship management crm landscape is attracting significant attention from customer and enterprise alike a a sustainable channel for tracking managing and improving customer relation enterprise are taking a hard look at this open unmediated platform because the community effect generated on this channel can have a telling effect on their brand image potential market opportunity and customer loyalty in this work we present our experience in building a system that mine conversation on social platform to identify and prioritize those post and message that are relevant to enterprise the system presented in this work aim to empower an agent or a representative in an enterprise to monitor track and respond to customer communication while also encouraging community participation 
multi document summarization aim to distill the most representative information from a set of document to generate a summary given a set of document a input most of existing multi document summarization approach utilize different sentence selection technique to extract a set of sentence from the document set a the summary the submodularity hidden in textual unit similarity motivates u to incorporate this property into our solution to multi document summarization task in this poster we propose a new principled and versatile framework for different multi document summarization task using the submodular function 
tweetspector is a tool for demonstrating entity based of retrieval of tweet the various feature of this tool include entity profile creation real time tweet classification active improvement of the created profile through user feedback and the dashboard displaying different metric 
how can web service that depend on user generated content discern fraudulent input by spammer from legitimate input in this paper we focus on the social network facebook and the problem of discerning ill gotten page like made by spammer hoping to turn a profit from legitimate page like our method which we refer to a copycatch detects lockstep page like pattern on facebook by analyzing only the social graph between user and page and the time at which the edge in the graph the like were created we offer the following contribution we give a novel problem formulation with a simple concrete definition of suspicious behavior in term of graph structure and edge constraint we offer two algorithm to find such suspicious lockstep behavior one provably convergent iterative algorithm and one approximate scalable mapreduce implementation we show that our method severely limit greedy attack and analyze the bound from the application of the zarankiewicz problem to our setting finally we demonstrate and discus the effectiveness of copycatch at facebook and on synthetic data a well a potential extension to anomaly detection problem in other domain copycatch is actively in use at facebook searching for attack on facebook s social graph of over a billion user many million of page and billion of page like 
there is a fundamental tradeoff between effectiveness and efficiency when designing retrieval model for large scale document collection effectiveness tends to derive from sophisticated ranking function such a those constructed using learning to rank while efficiency gain tend to arise from improvement in query evaluation and caching strategy given their inherently disjoint nature it is difficult to jointly optimize effectiveness and efficiency in end to end system to address this problem we formulate and develop a novel cascade ranking model which unlike previous approach can simultaneously improve both top k ranked effectiveness and retrieval efficiency the model construct a cascade of increasingly complex ranking function that progressively prune and refines the set of candidate document to minimize retrieval latency and maximize result set quality we present a novel boosting algorithm for learning such cascade to directly optimize the tradeoff between effectiveness and efficiency experimental result show that our cascade are faster and return higher quality result than comparable ranking model 
automatic document summarisation play a central role in the process of providing the user with a quick access to information application range from the generation of news headline to the aggregation of opinion extracted from review traditional topic based summarisation system are not always able to capture the sentiment expressed in a review major effort in sentiment analysis have been put in the task of mining and classifying review according to their polarity in this research we investigate the use of summarisation technique applied to review and we propose a knowledge based approach to summarisation in the context of sentiment analysis the proposed research is focused on three different aspect firstly we investigate the application of summarisation technique to sentiment classification capturing the key passage of a review can be beneficial for both a sentiment classifier and for a user who could potentially understand the polarity of a review without reading the full text secondly we investigate how to combine knowledge extracted from the review or integrated from external source with the purpose of producing opinion oriented summary thirdly we analyse the possibility of generating personalised user oriented or query biased opinion based summary 
searching is inherently an interactive process usually requiring numerous iteration of querying and assessing in order to find the desired amount of relevant information essentially the search process can be viewed a a combination of input query and assessment which are used to produce output relevance under this view it is possible to adapt microeconomic theory to analyze and understand the dynamic of interactive information retrieval in this paper we approach the search process a an economics problem and conduct extensive simulation on trec test collection analyzing various combination of input in the production of relevance the analysis reveals that the total cumulative gain output obtained during the course of a search session is functionally related to querying and assessing input and this can be characterized mathematically by the cobbs douglas production function further analysis using cost model that are grounded using cognitive load a the cost reveals which search strategy minimize the cost of interaction for a given level of output this paper demonstrates how economics can be applied to formally model the search process this development establishes the theoretical foundation of interactive information retrieval providing numerous direction for empirical experimentation that are motivated directly from theory 
in this paper we present a log based study on user search behavior comparison on three different platform desktop mobile and tablet we use three month search log in from a commercial search engine for our study our objective is to better understand how and to what extent mobile and tablet searcher behave differently than desktop user our study span a variety of aspect including query categorization query length search time distribution search location distribution user click pattern and so on from our data set we reveal that there are significant difference between user search pattern in these three platform and therefore use the same ranking system is not an optimal solution for all of them consequently we propose a framework that leverage a set of domain specific feature along with the training data from desktop search to further improve the search relevance for mobile and tablet platform experimental result demonstrate that by transferring knowledge from desktop search search relevance on mobile and tablet can be greatly improved 
a number of relevant information retrieval classification problem are one class classification problem at heart i e labeled data is only available for one class the so called target class and common discrimination based classification approach be them binary or multiclass are not applicable achieving a high effectiveness when solving one class problem is difficult anyway and it becomes even more challenging when the target class data is multimodal which is often the case to address these concern we propose a cluster based one class ensemble that consists of four step applying a clustering algorithm to the target class data training an individual one class classifier for each of the identified cluster aggregating the decision of the individual classifier and selecting the best fitting clustering model we evaluate our approach with four datasets an artificially generated dataset a dataset compiled from a known multiclass text corpus and two datasets related to one class problem that received much attention recently namely authorship verification and quality flaw prediction our approach outperforms a one class svm on all four datasets 
recently more and more short text e g ad tweet appear on the web classifying short text into a large taxonomy like odp or wikipedia category system ha become an important mining task to improve the performance of many application such a contextual advertising and topic detection for micro blogging in this paper we propose a novel multi stage classification approach to solve the problem first explicit semantic analysis is used to add more feature for both short text and category second we leverage information retrieval technology to fetch the most relevant category for an input short text from thousand of candidate finally a svm classifier is applied on only a few selected category to return the final answer our experimental result show that the proposed method achieved significant improvement on classification accuracy compared with several existing state of art approach 
sparql the standard query language for querying rdfprovides only limited navigational functionality although these feature are of fundamental importance for graph data format such a rdf this ha led the w c to include the property path feature in the upcoming version of the standard sparql we tested several implementation of sparql handling property path query and we observed that their evaluation method for this class of query have a poor performance even in some very simple scenario to formally explain this fact we conduct a theoretical study of the computational complexity of property path evaluation our result imply that the poor performance of the tested implementation is not a problem of these particular system but of the specification itself in fact we show that any implementation that adheres to the sparql specification a of november is doomed to show the same behavior the key issue being the need for counting solution imposed by the current specification we provide several intractability result that together with our empirical result provide strong evidence against the current semantics of sparql property path finally we put our result in perspective and propose a natural alternative semantics with tractable evaluation that we think may lead to a wide adoption of the language by practitioner developer and theoretician 
many technique for improving search result quality have been proposed typically these technique increase average effectiveness by devising advanced ranking feature and or by developing sophisticated learning to rank algorithm however while these approach typically improve average performance of search result relative to simple baseline they often ignore the important issue of robustness that is although achieving an average gain overall the new model often hurt performance on many query this limit their application in real world retrieval scenario given that robustness is an important measure that can negatively impact user satisfaction we present a unified framework for jointly optimizing effectiveness and robustness we propose an objective that capture the tradeoff between these two competing measure and demonstrate how we can jointly optimize for these two measure in a principled learning framework experiment indicate that ranking model learned this way significantly decreased the worst ranking failure while maintaining strong average effectiveness on par with current state of the art model 
it is well accepted that using high dimensional multi modal visual feature for image content representation and classifier training may achieve more sufficient characterization of the diverse visual property of the image and further result in higher discrimination power of the classifier however training the classifier in a high dimensional multi modal feature space requires a large number of labeled training image which will further result in the problem of curse of dimensionality to tackle this problem a hierarchical feature subset selection algorithm is proposed to enable more accurate image classification where the process for feature selection and classifier training are seamlessly integrated in a single framework first a feature hierarchy i e concept tree for automatic feature space partition and organization is used to automatically partition high dimensional heterogeneous multi modal visual feature into multiple low dimensional homogeneous single modal feature subset according to their certain physical meaning and each of them is used to characterize one certain type of the diverse visual property of the image second principal component analysis pca is performed on each homogeneous singlemodal feature subset to select the most representative feature dimension and a weak classifier is learned simultaneously after the weak classifier and their representative feature dimension are available for all these homogeneous single modal feature subset they are combined to generate an ensemble image classifier and achieve hierarchical feature subset selection our experiment on a specific domain of natural image have also obtained very positive result 
harnessing a crowd of user for the collection of mass data data sourcing ha recently become a wide spread practice one effective technique is based on game a a tool that attracts the crowd to contribute useful fact we focus here on the data management layer of such game and observe that the development of this layer involves challenge such a dealing with probabilistic data combined with recursive manipulation of this data these challenge are difficult to address using current declarative data management framework work and we thus propose here a novel such framework and demonstrate it usefulness in expressing different aspect in the data management of trivia like game we have implemented a system prototype with our novel data management framework at it core and we highlight key issue in the system design a well a our experimentation that indicate the usefulness and scalability of the approach 
a general characteristic of information retrieval ir and multilingual ir mir system is that if the same query wa submitted by different user the system would yield the same result regardless of the user on the other hand adaptive hypermedia ah system operate in a personalized manner where the service are adapted to the user personalized ir pir is motivated by the success in both area ir and ah ir system have the advantage of scalability and ah system have the advantage of satisfying individual user need the majority of study in pir literature have focused on monolingual ir and relatively little work ha been done concerning multilingual ir this phd research study aim to improve personalization in mir system by improving the relevance of multilingual search result with respect to the user and not just the query the study investigates how to model different aspect of a multilingual search user information about user can be demographic information such a language and country or information about the user s search interest this information can be gathered explicitly by asking the user to supply the required information or implicitly by inferring the information from the user s search history the study will then investigate how to exploit the modeled user information to personalize the user s multilingual search by performing query and result list adaptation the main research question that are addressed in this study are how to improve the relevance of search result with respect to individual user in pmir and how to construct profile that represent aspect and interest of a multilingual search user so far the work carried out for this study included a proposed framework for the delivery and evaluation of pmir and exploratory experiment with search history and collection result re ranking on a dataset of multilingual search log the next stage of experimentation will involve the investigation and development of algorithm for constructing multilingual user profile pre translation and post translation query expansion based on term from the user profile and result list re ranking based on the user s interest and preferred language two type of experiment will be conducted in an in lab setting with a group of user from different linguistic background in the first set of experiment user will be asked to use a baseline web search system for their daily search activity over a period of time the baseline system will be wrapped around one of the major search engine interaction with the system will be logged and part of this information will be used for training the system constructing user profile from text of query and clicked document the other part remaining query will be used for testing the effectiveness of the query adaptation and result list adaptation algorithm where the user will be asked to provide some personal relevance judgement in the second set of experiment the user will be asked to use the pmir system to fulfill a number of defined search task quantitative and qualitative technique will be used to evaluate different aspect of the experiment including retrieval effectiveness which can be measured using standard ir metric user s performance on search task which can be measured in term of time and number of action needed to fulfill the task user profile accuracy which can be assessed by questionnaire that indicate how well the user profile depicted the user search interest and usability and user satisfaction which can be assessed using standard system usability questionnaire 
distributing long tail content is an inherently difficult task due to the low amortization of bandwidth transfer cost a such content ha limited number of view two recent trend are making this problem harder first the increasing popularity of user generated content ugc and online social network osns create and reinforce such popularity distribution second the recent trend of geo replicating content across multiple pop spread around the world done for improving quality of experience qoe for user and for redundancy reason can lead to unnecessary bandwidth cost we build tailgate a system that exploit social relationship regularity in read access pattern and time zone difference to efficiently and selectively distribute long tail content across pop we evaluate tailgate using large trace from an osn and show that it can decrease wan bandwidth cost by a much a a well a reduce latency improving qoe we deploy tailgate on planetlab and show that even in the case when imprecise social information is available tailgate can still decrease the latency for accessing long tail youtube video by a factor of 
conventional classification method tend to focus on feature of individual object while missing out on potentially valuable pairwise feature that capture the relationship between object although recent development on graph regularization exploit this aspect existing work generally assume only a single kind of pairwise feature which is often insufficient we observe that multiple heterogeneous pairwise feature can often complement each other and are generally more robust in modeling the relationship between object furthermore a some object are easier to classify than others object with higher initial classification confidence should be weighed more towards classifying related but more ambiguous object an observation missing from previous graph regularization technique in this paper we propose a dirichlet based regularization framework that support the combination of heterogeneous pairwise feature with confidence aware prediction using limited labeled training data next we showcase a few application of our framework in information retrieval focusing on the problem of query intent classification finally we demonstrate through a series of experiment the advantage of our framework on a large scale real world dataset 
continuing advance in data storage and communication technology have led to an explosive growth in digital music collection to cope with their increasing scale we need effective music information retrieval mir capability like tagging concept search and clustering integral to mir is a framework for modelling music document and generating discriminative signature for them in this paper we introduce a multimodal layered learning framework called dmcm distinguished from the existing approach that encode music a an ensemble of order le feature vector our framework extract from each music document a variety of acoustic feature and translates them into low level encoding over the temporal dimension from them dmcm elucidates the concept dynamic in the music document representing them with a novel music signature scheme called stochastic music concept histogram smch that capture the probability distribution over all the concept experiment result with two large music collection confirm the advantage of the proposed framework over existing method on various mir task 
we study the problem of automatically assigning appropriate music piece to a picture or in general series of picture this task commonly referred to a soundtrack suggestion is non trivial a it requires a lot of human attention and a good deal of experience with master piece distinguished e g with the academy award for best original score we put forward picasso to solve this task in a fully automated way picasso make use of genuine sample obtained from first class contemporary movie hence the training set can be arbitrarily large and is also inexpensive to obtain but still provides an excellent source of information at query time picasso employ a three level algorithm first it selects for a given query image a ranking of the most similar screenshots taken and subsequently selects for each screenshot the most similar song to the music played in the movie when the screenshot wa taken last it issue a top k aggregation algorithm to find the overall best suitable song available we have created a large training set consisting of over image soundtrack sample obtained from movie and evaluated the suitability of picasso by mean of a user study 
conventional study of online information seeking behavior usually focus on the use of search engine or question answering q a website recently the fast growth of online social platform such a twitter and facebook ha made it possible for people to utilize them for information seeking by asking question to their friend or follower we anticipate a better understanding of web user information need by investigating research question about these question how are they distinctive from daily tweeted conversation how are they related to search query can user information need on one platform predict those on the other in this study we take the initiative to extract and analyze information need from billion of online conversation collected from twitter with an automatic text classifier we can accurately detect real question in tweet i e tweet conveying real information need we then present a comprehensive analysis of the large scale collection of information need we extracted we found that question being asked on twitter are substantially different from the topic being tweeted in general information need detected on twitter have a considerable power of predicting the trend of google query many interesting signal emerge through longitudinal analysis of the volume spike and entropy of question on twitter which provide insight to the understanding of the impact of real world event and user behavioral pattern in social platform 
a number of key information access task document retrieval clustering filtering and their combination can be seen a instance of a generic em document organization problem that establishes priority and relatedness relationship between document in other word a problem of forming and ranking cluster a far a we know no analysis ha been made yet on the evaluation of these task from a global perspective in this paper we propose two complementary evaluation measure reliability and sensitivity for the generic document organization task which are derived from a proposed set of formal constraint property that any suitable measure must satisfy in addition to be the first measure that can be applied to any mixture of ranking clustering and filtering task reliability and sensitivity satisfy more formal constraint than previously existing evaluation metric for each of the subsumed task besides their formal property it most salient feature from an empirical point of view is their strictness a high score according to the harmonic mean of reliability and sensitivity ensures a high score with any of the most popular evaluation metric in all the document retrieval clustering and filtering datasets used in our experiment 
fine grained search interaction in the desktop setting such a mouse cursor movement and scrolling have been shown valuable for understanding user intent attention and their preference for web search result a web search on smart phone and tablet becomes increasingly popular previously validated desktop interaction model have to be adapted for the available touch interaction such a pinching and swiping and for the different device form factor in this paper we present to our knowledge the first in depth study of modeling interaction on touch enabled device for improving web search ranking in particular we evaluate a variety of touch interaction on a smart phone a implicit relevance feedback and compare them with the corresponding fine grained interaction on a desktop computer with mouse and keyboard a the primary input device our experiment are based on a dataset collected from two user study with user in total using a specially instrumented version of a popular mobile browser to capture the interaction data we report a detailed analysis of the similarity and difference of fine grained search interaction between the desktop and the smart phone modality and identify novel pattern of touch interaction indicative of result relevance finally we demonstrate significant improvement to search ranking quality by mining touch interaction data 
diversification and personalization method are common ap proaches to deal with the one size fit all paradigm of web search engine we performed a user study with subject where we analyzed the effect of diversification and personalization method in a web search engine the obtained result suggest that our proposed combination of diversification and personalization factor may be a way to overcome the notion of intrusiveness in personalized approach 
we present a novel and innovative user interface for query by sketching based image retrieval that exploit emergent interactive paper and digital pen technology user can draw sketch with a digital pen on interactive paper in a user friendly way the pen is able to capture the stroke vector and to interactively stream them to the underlying content based image retrieval cbir system via the pen s bluetooth interface we present the integration of interactive paper digital pen technology with qbs our cbir system tailored to query by sketching and we demonstrate the use of the paper and pen interface together with qbs for three different collection mirflickr k a cartoon collection and a collection of medieval paper watermark 
many current effectiveness measure incorporate simplifying assumption about user behavior these assumption prevent the measure from reflecting aspect of the search process that directly impact the quality of retrieval result a experienced by the user in particular these measure implicitly model user a working down a list of retrieval result spending equal time assessing each document in reality even a careful user intending to identify a much relevant material a possible must spend longer on some document than on others aspect such a document length duplicate and summary all influence the time required in this paper we introduce a time biased gain measure which explicitly accommodates such aspect of the search process by conducting an appropriate user study we calibrate and validate the measure against the trec robust track test collection we examine property of the measure contrasting it to traditional effectiveness measure and exploring it extension to other aspect and environment a it primary benefit the measure allows u to evaluate system performance in human term while maintaining the simplicity and repeatability of system oriented test overall we aim to achieve a clearer connection between user oriented study and system oriented test allowing u to better transfer insight and outcome from one to the other 
an important aspect of communication in twitter and other social network is message propagation people creating post for others to share although there ha been work on modelling how tweet in twitter are propagated retweeted an untackled problem ha been who will retweet a message here we consider the task of finding who will retweet a message posted on twitter within a learning to rank framework we explore a wide range of feature such a retweet history follower status follower active time and follower interest we find that follower who retweeted or mentioned the author s tweet frequently before and have common interest are more likely to be retweeters 
content discovery is fast becoming the preferred tool for user engagement on the web discovery allows user to get educated and entertained about their topic of interest stumbleupon is the largest personalized content discovery engine on the web delivering more than billion personalized recommendation per month a a recommendation system one of the primary metric we track is whether the user return retention to use the product after their initial experience session with stumbleupon in this paper we attempt to address the problem of predicting user retention based on the user s previous session the paper first explores the different user and content feature that are helpful in predicting user retention this involved mapping the user and the user s recommendation stumble in a descriptive feature space such a the time spent by user number of stumble and content feature of the recommendation to model the diversity in user behaviour we also generated normalized feature that account for the user s speed of stumbling using these feature we built a decision tree classifier to predict retention we find that a model that us both the user and content feature achieves higher prediction accuracy than a model that us the two feature separately further we used information theoretical analysis to find a subset of recommendation that are most indicative of user retention a classifier trained on this subset of recommendation achieves the highest prediction accuracy this indicates that not every recommendation seen by the user is predictive of whether the user will be retained instead a subset of most informative recommendation is more useful in predicting retention 
with the rapid growth of online news service user can actively respond to online news by making comment user often express subjective emotion in comment such a sadness surprise and anger such emotion can help understand the preference and perspective of individual user and therefore may facilitate online publisher to provide user with more relevant service this paper tackle the task of predicting emotion for the comment of online news to the best of our knowledge this is the first research work for addressing the task in particular this paper proposes a novel meta classification approach that exploit heterogeneous information source such a the content of the comment and the emotion tag of news article generated by user the experiment on two datasets from online news service demonstrate the effectiveness of the proposed approach 
social network system such a facebook and youtube have played a significant role in capturing both explicit and implicit user preference for different item in the form of rating and tag this form a quaternary relationship among user item tag and rating existing system have utilized only ternary relationship such a user item rating or user item tag to derive their recommendation in this paper we show that ternary relationship are insufficient to provide accurate recommendation instead we model the quaternary relationship among user item tag and rating a a order tensor and cast the recommendation problem a a multi way latent semantic analysis problem a unified framework for user recommendation item recommendation tag recommendation and item rating prediction is proposed the result of extensive experiment performed on a real world dataset demonstrate that our unified framework outperforms the state of the art technique in all the four recommendation task 
natural graph such a social network email graph or instant messaging pattern have become pervasive through the internet these graph are massive often containing hundred of million of node and billion of edge while some theoretical model have been proposed to study such graph their analysis is still difficult due to the scale and nature of the data we propose a framework for large scale graph decomposition and inference to resolve the scale our framework is distributed so that the data are partitioned over a shared nothing set of machine we propose a novel factorization technique that relies on partitioning a graph so a to minimize the number of neighboring vertex rather than edge across partition our decomposition is based on a streaming algorithm it is network aware a it adapts to the network topology of the underlying computational hardware we use local copy of the variable and an efficient asynchronous communication protocol to synchronize the replicated value in order to perform most of the computation without having to incur the cost of network communication on a graph of million vertex and billion edge derived from an email communication network our algorithm retains convergence property while allowing for almost linear scalability in the number of computer 
today more and more product review become available on the internet e g product review forum discussion group and blog however it is almost impossible for a customer to read all of the different and possibly even contradictory opinion and make an informed decision therefore mining online review opinion mining ha emerged a an interesting new research direction extracting aspect and the corresponding rating is an important challenge in opinion mining an aspect is an attribute or component of a product e g screen for a digital camera it is common that reviewer use different word to describe an aspect e g lcd display screen a rating is an intended interpretation of the user satisfaction in term of numerical value reviewer usually express the rating of an aspect by a set of sentiment e g blurry screen in this paper we present three probabilistic graphical model which aim to extract aspect and corresponding rating of product from online review the first two model extend standard plsi and lda to generate a rated aspect summary of product review a our main contribution we introduce interdependent latent dirichlet allocation ilda model this model is more natural for our task since the underlying probabilistic assumption interdependency between aspect and rating are appropriate for our problem domain we conduct experiment on a real life dataset epinions com demonstrating the improved effectiveness of the ilda model in term of the likelihood of a held out test set and the accuracy of aspect and aspect rating 
it ha been previously noted that optimization of the n call k relevance objective i e a set based objective that is if at least n document in a set of k are relevant otherwise encourages more result set diversification for smaller n but this statement ha never been formally quantified in this work we explicitly derive the mathematical relationship between expected n call k and the relevance v diversity trade off through fortuitous cancellation in the resulting combinatorial optimization we show the trade off is a simple and intuitive function of n notably independent of the result set size k e n where diversification increase a n approach 
twitter is a popular social network service for sharing message among friend because twitter restricts the length of message many twitter user use url shortening service such a bit ly and goo gl to share long url with friend some url shortening service also provide click analytics of the shortened url including the number of click country platform browser and referrers to protect visitor privacy they do not reveal identifying information about individual visitor in this paper we propose a practical attack technique that can infer who click what shortened url on twitter unlike the conventional browser history stealing attack our attack method only need publicly available information provided by url shortening service and twitter evaluation result show that our attack technique can compromise twitter user privacy with high accuracy 
provider such a youtube offer easy access to multimedia content to million generating high bandwidth and storage demand on the content delivery network they rely upon more and more the diffusion of this content happens on online social network such a facebook and twitter where social cascade can be observed when user increasingly repost link they have received from others in this paper we describe how geographic information extracted from social cascade can be exploited to improve caching of multimedia file in a content delivery network we take advantage of the fact that social cascade can propagate in a geographically limited area to discern whether an item is spreading locally or globally this informs cache replacement policy which utilize this information to ensure that content relevant to a cascade is kept close to the user who may be interested in it we validate our approach by using a novel dataset which combine social interaction data with geographic information we track social cascade of youtube link over twitter and build a proof of concept geographic model of a realistic distributed content delivery network our performance evaluation show that we are able to improve cache hit with respect to cache policy without geographic and social information 
targeting interest to match a user with service e g news product game advertisement and predicting friendship to build connection among user are two fundamental task for social network system in this paper we show that the information contained in interest network i e user service interaction and friendship network i e user user connection is highly correlated and mutually helpful we propose a framework that exploit homophily to establish an integrated network linking a user to interested service and connecting different user with common interest upon which both friendship and interest could be efficiently propagated the proposed friendship interest propagation fip framework devise a factor based random walk model to explain friendship connection and simultaneously it us a coupled latent factor model to uncover interest interaction we discus the flexibility of the framework in the choice of loss objective and regularization penalty and benchmark different variant on the yahoo pulse social networking system experiment demonstrate that by coupling friendship with interest fip achieves much higher performance on both interest targeting and friendship prediction than system using only one source of information 
the signal used for ranking in local search are very different from web search in addition to textual relevance measure of geographic distance between the user and the search result a well a measure of popularity of the result are important for effective ranking depending on the query and search result different way to quantify these factor exist for example it is possible to use customer rating to quantify the popularity of restaurant whereas different measure are more appropriate for other type of business hence our approach is to capture the different notion of distance popularity relevant via a number of external data source e g log of customer rating driving direction request or site access in this paper we will describe the relevant signal contained in a number of such data source in detail and present method to integrate these external data source into the feature generation for local search ranking in particular we propose novel backoff method to alleviate the impact of skew noise or incomplete data in these log in a systematic manner we evaluate our technique on both human judged relevance data a well a click through data from a commercial local search engine 
we propose a new approach for social and personalized query expansion using social structure in the web while focusing on social tagging system the proposed approach considers i the semantic similarity between tag composing a query ii a social proximity between the query and the user profile and iii on the fly a strategy for expanding user query the proposed approach ha been evaluated using a large dataset crawled from del icio u 
the bane of one class collaborative filtering is interpreting and modelling the latent signal from the missing class in this paper we present a novel bayesian generative model for implicit collaborative filtering it form a core component of the xbox live architecture and unlike previous approach delineates the odds of a user disliking an item from simply being unaware of it the latent signal is treated a an unobserved random graph connecting user with item they might have encountered we demonstrate how large scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph sample a fine grained comparison is done against a state of the art baseline on real world data 
cross site request forgery csrf attack are one of the top threat on the web today these attack exploit ambient authority in browser eg cooky http authentication state turning them into confused deputy and causing undesired side effect on vulnerable web site existing defense against csrfs fall short in their coverage and or ease of deployment in this paper we present a browser server solution allowed referrer list arls that address the root cause of csrfs and remove ambient authority for participating web site that want to be resilient to csrf attack our solution is easy for web site to adopt and doe not affect any functionality on non participating site we have implemented our design in firefox and have evaluated it with real world site we found that arls successfully block csrf attack are simpler to implement than existing defense and do not significantly impact browser performance 
query segmentation is an important task toward understanding query accurately which is essential for improving search result existing segmentation model either use labeled data to predict the segmentation boundary for which the training data is expensive to collect or employ unsupervised strategy based on a large text corpus which might be inaccurate because of the lack of relevant information in this paper we propose a probabilistic model to exploit clickthrough data for query segmentation where the model parameter are estimated via an efficient em algorithm we further study how to properly interpret the segmentation result and utilize them to improve retrieval accuracy specifically we propose an integrated language model based on the standard bigram language model to exploit the probabilistic structure obtained through query segmentation experiment result on two datasets show that our segmentation model outperforms existing segmentation model furthermore extensive experiment on a large retrieval dataset reveals that the result of query segmentation can be leveraged to improve retrieval relevance by using the proposed integrated language model 
user generated content ugcs carry a huge amount of high quality information however the information overload and diversity of ugc source limit their potential us in this research we propose a framework to organize information from multiple ugc source by a topic hierarchy which is automatically generated and updated using the ugcs we explore the unique characteristic of ugcs like blog cqas microblogs etc and introduce a novel scheme to combine them we also propose a graph based method to enable incremental update of the generated topic hierarchy using the hierarchy user can easily obtain a comprehensive in depth and up to date picture of their topic of interest the experiment result demonstrate how information from multiple heterogeneous source improves the resultant topic hierarchy it also show that the proposed method achieves better f score in hierarchy generation a compared to the state of the art method 
verbose web query are often descriptive in nature where a term based search engine is unable to distinguish between the essential and noisy word which can result in a drift from the user intent we present a randomized query reduction technique that build on an earlier learning to rank based approach the proposed technique randomly pick only a small set of sample instead of the exponentially many sub query thus being fast enough to be useful for web search engine while still covering wide sub query space 
k nearest neighbor graph k nng construction is an important operation with many web related application including collaborative filtering similarity search and many others in data mining and machine learning existing method for k nng construction either do not scale or are specific to certain similarity measure we present nn descent a simple yet efficient algorithm for approximate k nng construction with arbitrary similarity measure our method is based on local search ha minimal space overhead and doe not rely on any shared global index hence it is especially suitable for large scale application where data structure need to be distributed over the network we have shown with a variety of datasets and similarity measure that the proposed method typically converges to above recall with each point comparing only to several percent of the whole dataset on average 
the use of information and communication technology and the web based product it provides is responsible for significant emission of greenhouse gas in order to enable the reduction of emission during the design of such product it is necessary to estimate a accurately a possible their carbon impact over the entire product system in this work we describe a new method which combine model of energy consumption during the use of digital medium with model of the behavior of the audience we apply this method to conduct an assessment of the annual carbon emission for the product suite of a major international news organization we then demonstrate it use for green design by evaluating the impact of five different intervention on the product suite we find that carbon footprint of the online newspaper amount to approximately tco e per year of which are caused by the user device among the evaluated scenario a significant uptake of ereaders in favor of pc ha the greatest reduction potential our result also show that even a significant reduction of data volume on a web page would only result in small overall energy saving 
wtf who to follow is twitter s user recommendation service which is responsible for creating million of connection daily between user based on shared interest common connection and other related factor this paper provides an architectural overview and share lesson we learned in building and running the service over the past few year particularly noteworthy wa our design decision to process the entire twitter graph in memory on a single server which significantly reduced architectural complexity and allowed u to develop and deploy the service in only a few month at the core of our architecture is cassovary an open source in memory graph processing engine we built from scratch for wtf besides powering twitter s user recommendation cassovary is also used for search discovery promoted product and other service a well we describe and evaluate a few graph recommendation algorithm implemented in cassovary including a novel approach based on a combination of random walk and salsa looking into the future we revisit the design of our architecture and comment on it limitation which are presently being addressed in a second generation system under development 
time travel query that couple temporal constraint with keyword query are useful in searching large scale archive of time evolving content such a the web archive or wikis typical approach for efficient evaluation of these query involve slicing either the entire collection or individual index list along the time axis both these method are not satisfactory since they sacrifice compactness of index for processing efficiency making them either too big or otherwise too slow we present a novel index organization scheme that shard each index list with almost zero increase in index size but still minimizes the cost of reading index entry during query processing based on the optimal sharding thus btained we develop a practically efficient sharding that take into account the different cost of random and sequential access our algorithm merges shard from the optimal solution to allow for a few extra sequential access while gaining significantly by reducing the number of random access we empirically establish the effectiveness of our sharding scheme with experiment over the revision history of the english wikipedia between approx gb and an archive of u k governmental web site approx gb our result demonstrate the feasibility of faster time travel query processing with no space overhead 
many website provide commenting facility for user to express their opinion or sentiment with regard to content item such a video news story blog post etc previous study have shown that user comment contain valuable information that can provide insight on web document and may be utilized for various task this work present a model that predicts for a given user suitable news story for commenting the model achieves encouraging result regarding the ability to connect user with story they are likely to comment on this provides ground for personalized recommendation of story to user who may want to take part in their discussion we combine a content based approach with a collaborative filtering approach utilizing user co commenting pattern in a latent factor modeling framework we experiment with several variation of the model s loss function in order to adjust it to the problem domain we evaluate the result on two datasets and show that employing co commenting pattern improves upon using content feature alone even with a few a two available comment per story finally we try to incorporate available social network data into the model interestingly the social data doe not lead to substantial performance gain suggesting that the value of social data for this task is quite negligible 
in this paper we propose a novel framework for modeling the uniqueness of the user preference for recommendation system user uniqueness is determined by learning to what extent the user s item preference deviate from those of an average user in the system based on this framework we suggest three different recommendation strategy that trade between uniqueness and conformity using two real item datasets we demonstrate the effectiveness of our uniqueness based recommendation framework 
discovering similar user with respect to their habit play an important role in a wide range of application such a collaborative filtering for recommendation user segmentation for market analysis etc recently the progressing ability to sense user context of smart mobile device make it possible to discover mobile user with similar habit by mining their habit from their mobile device however though some researcher have proposed effective method for mining user habit such a behavior pattern mining how to leverage the mined result for discovering similar user remains le explored to this end we propose a novel approach for conquering the sparseness of behavior pattern space and thus make it possible to discover similar mobile user with respect to their habit by leveraging behavior pattern mining to be specific first we normalize the raw context log of each user by transforming the location based context data and user interaction record to more general representation second we take advantage of a constraint based bayesian matrix factorization model for extracting the latent common habit among behavior pattern and then transforming behavior pattern vector to the vector of mined common habit which are in a much more dense space the experiment conducted on real data set show that our approach outperforms three baseline in term of the effectiveness of discovering similar mobile user with respect to their habit 
n gram representation of document may improve over a simple bag of word representation by relaxing the independence assumption of word and introducing context however this come at a cost of adding feature which are non descriptive and increasing the dimension of the vector space model exponentially we present new representation that avoid both pitfall they are based on sound theoretical notion of stringology and can be computed in optimal asymptotic time with algorithm using data structure from the suffix family while maximal repeat have been used in the past for similar task we show how another equivalence class of repeat largest maximal repeat obtain similar or better result with only a fraction of the feature this class act a a minimal generative basis of all repeated substring we also report their use for topic modeling showing easier to interpret model 
patent prior art search is a task in patent retrieval where the goal is to rank document which describe prior art work related to a patent application one of the main property of patent retrieval is that the query topic is a full patent application and doe not represent a focused information need this query by document nature of patent retrieval introduces new challenge and requires new investigation specific to this problem researcher have addressed this problem by considering different information resource for query reduction and query disambiguation however previous work ha not fully studied the effect of using proximity information and exploiting domain specific resource for performing query disambiguation in this paper we first reduce the query document by taking the first claim of the document itself we then build a query specific patent lexicon based on definition of the international patent classification ipc we study how to expand query by selecting expansion term from the lexicon that are focused on the query topic the key problem is how to capture whether an expansion term is focused on the query topic or not we address this problem by exploiting proximity information we assign high weight to expansion term appearing closer to query term based on the intuition that term closer to query term are more likely to be related to the query topic experimental result on two patent retrieval datasets show that the proposed method is effective and robust for query expansion significantly outperforming the standard pseudo relevance feedback prf and existing baseline in patent retrieval 
we give a fresh look into score normalization for merging result list isolating the problem from other component we focus on three of the simplest practical and widely used linear method which do not require any training data i e minmax sum and z score we provide theoretical argument on why and when the method work and evaluate them experimentally we find that minmax is the most robust under many circumstance and that sum is in contrast to previous literature the worst based on the insight gained we propose another three simple method which work a good or better than the baseline 
the success of a group depends on continued participation of it member through time we study the factor that affect continued user participation in the context of educational twitter chat to predict whether a user that attended her first session in a particular twitter chat group will return to the group we build f model that capture five different factor individual initiative group characteristic perceived receptivity linguistic affinity and geographical proximity through statistical data analysis of thirty twitter chat over a two year period a well a a survey study our work provides many insight about group dynamic in twitter chat we show similarity between twitter chat and traditional group such a the importance of social inclusion and linguistic similarity while also identifying important distinction such a the insignificance of geographical proximity we also show that informational support is more important than emotional support in educational twitter chat but this doe not reduce the sense of community a suggested in earlier study 
although a great deal of research ha been conducted about automatic technique for determining query quality there have been relatively few study about how people judge query quality this study investigated this topic through a laboratory experiment with subject subject were shown eight information problem five fact finding and three exploratory and asked to evaluate query for these problem according to several quality attribute subject then evaluated search engine result page serps for each query which were manipulated to exhibit different level of performance following this subject reevaluated the query were interviewed about their evaluation approach and repeated the rating procedure for two information problem result showed that for fact finding information problem longer query received higher rating both initial and post serp and that post serp query rating were more affected by the proportion of relevant document viewed to all document viewed rather than the rank of the relevant document for exploratory information problem subject rating were highly correlated with the number of relevant document in the serp a well a the proportion of relevant document viewed subject adopted several approach when evaluating query quality which led to different quality rating finally during the reliability check subject initial evaluation were fairly stable but their post serp evaluation significantly increased 
child often struggle with information retrieval task a searching for information often requires a developed vocabulary and strong categorisation skill neither of which are particularly developed in child under the age of in a study conducted by druin et al it wa found that in an experimental setting many child are often uninterested in searching for information online or are only interested in searching for information that is relevant to their personal interest consequently child who were unmotivated were the least successful in completing information retrieval task in their study it wa suggested that a more effective mean of engaging child participant in search study must be developed in order to gain further insight into the searching behaviour of child to this end we have developed a game called pagefetch which aim to engage child aged to in completing search task through a fun and interactive search like interface 
what price should be offered to a worker for a task in an online labor market how can one enable worker to express the amount they desire to receive for the task completion designing optimal pricing policy and determining the right monetary incentive is central to maximizing requester s utility and worker profit yet current crowdsourcing platform only offer a limited capability to the requester in designing the pricing policy and often rule of thumb are used to price task this limitation could result in inefficient use of the requester s budget or worker becoming disinterested in the task in this paper we address these question and present mechanism using the approach of regret minimization in online learning we exploit a link between procurement auction and multi armed bandit to design mechanism that are budget feasible achieve near optimal utility for the requester are incentive compatible truthful for worker and make minimal assumption about the distribution of worker true cost our main contribution is a novel no regret posted price mechanism bp ucb for budgeted procurement in stochastic online setting we prove strong theoretical guarantee about our mechanism and extensively evaluate it in simulation a well a on real data from the mechanical turk platform compared to the state of the art our approach lead to a increase in utility 
g ripple is a visualization of information flow that show user how public post are shared on google unlike other social network visualization ripple exists a a native visualization it is directly accessible from public post on google this unique position lead to both new constraint and new possibility for design we describe the visualization technique which is a new mix of node and link and circular treemap metaphor we then describe user reaction a well a some of the pattern of sharing that are made evident by the ripple visualization 
query performance prediction qpp is an important task in information retrieval ir in this paper we develop a new predictor based on the standard deviation of score in a variable length ranked list and we show that this new predictor outperforms state of the art approach without the need for tuning 
discretization is a standard technique used in click based graphical password for tolerating input variance so that approximately correct password are accepted by the system in this paper we show for the first time that two representative discretization scheme leak a significant amount of password information undermining the security of such graphical password we exploit such information leakage for successful dictionary attack on persuasive cued click point pccp which is to date the most secure click based graphical password scheme and wa considered to be resistant to such attack in our experiment our purely automated attack successfully guessed of the password when centered discretization wa used to implement pccp and of the password when robust discretization wa used each attack dictionary we used wa of approximately entry whereas the full password space wa of entry for centered discretization our attack still successfully guessed of the password when the dictionary size wa reduced to approximately entry our attack is also applicable to common implementation of other click based graphical password system such a passpoints and cued click point both have been extensively studied in the research community 
we provide a novel method of evaluating search result which allows u to combine existing editorial judgment with the relevance estimate generated by click based user browsing model there are evaluation method in the literature that use click and editorial judgment together but our approach is novel in the sense that it allows u to predict the impact of unseen search model without online test to collect click and without requesting new editorial data since we are only re using existing editorial data and click observed for previous result set configuration since the user browsing model and the pre existing editorial data cannot provide relevance estimate for all document for the selected set of query one important challenge is to obtain this performance estimation where there are a lot of ranked document with missing relevance value we introduce a query and rank based smoothing to overcome this problem we show that a hybrid of these smoothing technique performs better than both query and position based smoothing and despite the high percentage of missing judgment the resulting method is significantly correlated with dcg value evaluated using fully judged datasets and approach inter annotator agreement we show that previously published technique applicable to frequent query degrade when applied to a random sample of query with a correlation of only while our experiment focus on evaluation using dcg our method is also applicable to other commonly used metric 
stream of event appear increasingly today in various web application such a blog feed sensor data stream geospatial information on line financial data etc event processing ep is concerned with timely detection of compound event within stream of simple event state of the art ep provides on the fly analysis of event stream but cannot combine stream with background knowledge and cannot perform reasoning task on the other hand semantic tool can effectively handle background knowledge and perform reasoning thereon but cannot deal with rapidly changing data provided by event stream to bridge the gap we propose event processing sparql ep sparql a a new language for complex event and stream reasoning we provide syntax and formal semantics of the language and devise an effective execution model for the proposed formalism the execution model is grounded on logic programming and feature effective event processing and inferencing capability over temporal and static knowledge we provide an open source prototype implementation and present a set of test to show the usefulness and effectiveness of our approach 
traditional tool for information retrieval ir evaluation such a trec s trec eval have outdated command line interface with many unused feature or switch accumulated over the year they are usually seen a cumbersome application by new ir researcher steepening the learning curve we introduce a platform independent application for ir evaluation with a graphical easy to use interface the trec file evaluator the application support most of the standard measure used for evaluation in trec clef and elsewhere such a map p p and bpref a well a the averaged normalized modified retrieval rank anmrr proposed by mpeg for image retrieval evaluation additional feature include a batch mode and statistical significance testing of the result against a pre selected baseline 
in this paper we discus a very simple approach of combining content and link information in graph structure for the purpose of community discovery a fundamental task in network analysis our approach hinge on the basic intuition that many network contain noise in the link structure and that content information can help strengthen the community signal this enables one to eliminate the impact of noise false positive and false negative which is particularly prevalent in online social network and web scale information network specifically we introduce a measure of signal strength between two node in the network by fusing their link strength with content similarity link strength is estimated based on whether the link is likely with high probability to reside within a community content similarity is estimated through cosine similarity or jaccard coefficient we discus a simple mechanism for fusing content and link similarity we then present a biased edge sampling procedure which retains edge that are locally relevant for each graph node the resulting backbone graph can be clustered using standard community discovery algorithm such a metis and markov clustering through extensive experiment on multiple real world datasets flickr wikipedia and citeseer with varying size and characteristic we demonstrate the effectiveness and efficiency of our method over state of the art learning and mining approach several of which also attempt to combine link and content analysis for the purpose of community discovery specifically we always find a qualitative benefit when combining content with link analysis additionally our biased graph sampling approach realizes a quantitative benefit in that it is typically several order of magnitude faster than competing approach 
the effectiveness of various behavioural signal for implicit relevance feedback model ha been exhaustively studied despite the advantage of such technique for a real time information retrieval system most of the behavioural signal are noisy and therefore not reliable enough to be employed among many a combination of dwell time and task information ha been shown to be effective for relevance judgement prediction however the task information might not be available to the system at all time thus there is a need for other source of information which can be used a a substitute for task information recently affective and physiological signal have shown promise a a potential source of information for relevance judgement prediction however their accuracy is not high enough to be applicable on their own this paper investigates whether affective and physiological signal can be used a a complementary source of information for behavioural signal i e dwell time to create a reliable signal for relevance judgement prediction using a video retrieval system a a use case we study and compare the effectiveness of the affective and physiological signal on their own a well a in combination with behavioural signal for the relevance judgment prediction task across four different search intention seeking information re finding a particular information object and two different entertainment intention i e entertainment by adjusting arousal level and entertainment by adjusting mood our experimental result show that the effectiveness of studied signal varies across different search intention and when affective and physiological signal are combined with dwell time a significant improvement can be achieved overall these finding will help to implement better search engine in the future 
a we face an explosion of potential new application for the fundamental concept and technology of information retrieval ranging from ad ranking to social medium from collaborative recommending to question answering system many researcher are spending unnecessary time reinventing idea and relationship that are buried in the prehistory of information retrieval which for many researcher mean anything published before they entered graduate school a lot of the idea that surface a new in today s super heated research environment have very firm root in earlier development in field a diverse a citation analysis and pattern recognition the purpose of this tutorial is to survey those root and their relation to the contemporary fruit on the tree of information retrieval and to separate a much a is possible in an era of increasing secrecy about method the problem to be solved the algorithm for solving them and the heuristic that are the bread and butter of a working operation participant will become familiar with root in pattern analysis statistic information science and other source of key idea that reappear in the current development of information retrieval a it applies to search engine social medium and collaborative system they will be able to separate problem from algorithm and algorithm from heuristic in the application of these idea to their own research and or development activity course material will be made available on a web site two week prior to the tutorial they will include link to relevant software link to publication that will be discussed and mechanism for chat among the tutorial participant before during and after the tutorial 
identifying content for which a user may search ha a variety of application including ranking and recommendation in this poster we examine how pre search context can be used to predict content that the user will seek before they have even specified a search query we call this anticipatory search using a log based approach we compare different method for predicting the content to be searched using different attribute of the pre query context and behavioral signal from previous visitor to the most recent browse url each method cover different case and show promise for query free anticipatory search on the web 
in this paper we aim to provide a point of interest poi recommendation service for the rapid growing location based social network lbsns e g foursquare whrrl etc our idea is to explore user preference social influence and geographical influence for poi recommendation in addition to deriving user preference based on user based collaborative filtering and exploring social influence from friend we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check in activity of lbsns we argue that the geographical influence among poi play an important role in user check in behavior and model it by power law distribution accordingly we develop a collaborative recommendation algorithm based on geographical influence based on naive bayesian furthermore we propose a unified poi recommendation framework which fuse user preference to a poi with social influence and geographical influence finally we conduct a comprehensive performance evaluation over two large scale datasets collected from foursquare and whrrl experimental result with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approach 
enterprise search is challenging for several reason notably the dynamic terminology and jargon that are specific to the enterprise domain this challenge is partly addressed by having domain expert maintaining the enterprise search engine and adapting it to the domain specific those administrator commonly address user complaint about relevant document missing from the top match for that it ha been proposed to allow administrator to influence search result by crafting query rewrite rule each specifying how query of a certain pattern should be modified or augmented with additional query upon a complaint the administrator seek a semantically coherent rule that is capable of pushing the desired document up to the top match however the creation and maintenance of rewrite rule is highly tedious and time consuming our goal in this work is to ease the burden on search administrator by automatically suggesting rewrite rule this automation entail several challenge one major challenge is to select among many option rule that are natural from a semantic perspective e g corresponding to closely related and syntactically complete concept towards that we study a machine learning classification approach the second challenge is to accommodate the cross query effect of rule a rule introduced in the context of one query can eliminate the desired result for other query and the desired effect of other rule we present a formalization of this challenge a a generic computational problem a we show that this problem is highly intractable in term of complexity theory we present heuristic approach and optimization thereof in an experimental study within ibm intranet search those heuristic achieve near optimal quality and well scale to large data set 
collection containing a large number of short document are becoming increasingly common a these collection grow in number and size providing effective retrieval of brief text present a significant research problem we propose a novel approach to improving information retrieval ir for short text based on aggressive document expansion starting from the hypothesis that short document tend to be about a single topic we submit document a pseudo query and analyze the result to learn about the document themselves document expansion help in this context because short document yield little in the way of term frequency information however a we show the proposed technique help u model not only lexical property but also temporal property of document we present experimental result using a corpus of microblog twitter data and a corpus of metadata record from a federated digital library with respect to established baseline result of these experiment show that applying our proposed document expansion method yield significant improvement in effectiveness specifically our method improves the lexical representation of document and the ability to let time influence retrieval 
stochastic knapsack problem deal with selecting item with potentially random size and reward so a to maximize the total reward while satisfying certain capacity constraint a novel variant of this problem where item are worthless unless collected in bundle is introduced here this setup is similar to the groupon model where a deal is off unless a minimum number of user sign up for it since the optimal algorithm to solve this problem is not practical several adaptive greedy approach with reasonable time and memory requirement are studied in detail theoretically a well a experimentally worst case performance guarantee are provided for some of these greedy algorithm while result of experimental evaluation demonstrate that they are much closer to optimal than what the theoretical bound suggest application include optimizing for online advertising pricing model where advertiser pay only when certain goal in term of click or conversion are met we perform extensive experiment for the situation where there are between two and five ad for typical ad conversion rate the greedy policy of selecting item having the highest individual expected reward obtains a value within of optimal over of the time for a wide selection of parameter 
the goal of my research is to develop self learning search engine that can learn online i e directly from interaction with actual user such system can continuously adapt to user preference throughout their lifetime leading to better search performance in setting where expensive manual tuning is infeasible challenge that are addressed in my work include the development of effective online learning to rank algorithm for ir user aspect and evaluation 
building a federated search engine based on a large number existing web search engine is a challenge implementing the programming interface api for each search engine is an exacting and time consuming job in this demonstration we present searchresultfinder a browser plugin which speed up determining reusable xpaths for extracting search result item from html search result page based on a single search result page the tool present a ranked list of candidate extraction xpaths and allows highlighting to view the extraction result an evaluation with web search engine show that in of the case a correct xpath is suggested 
time is often important for understanding user intent during search activity especially for information need related to event driven topic diversity for multi faceted information need ensures that ranked document optimally cover multiple facet when a user s intent is uncertain effective diversity is reliant on method to i discover and represent facet and ii determine how likely each facet is the user s intent i e it popularity past work ha developed several technique addressing these issue however they have concentrated on static approach which do not consider the temporal nature of new and evolving intent and their popularity in many case what a user expects may change dramatically over time a event develop in this work we study the temporal variance of search intent for event driven information need using wikipedia first we model intent based upon the structure represented by the section hierarchy of wikipedia article closely related to the information need using this technique we investigate whether temporal change in the content structure i e in a section s text reflect the temporal popularity of the intent we map intent taken from a query log a ground truth to wikipedia article section and found that a large proportion are indeed reflected in topic related article structure by correlating the change activity of each section with the use of the intent query over time we found that section change activity doe reflect temporal popularity of many intent furthermore we show that popularity between intent change over time for event driven topic 
hashing is used to learn binary code representation for data with expectation of preserving the neighborhood structure in the original feature space due to it fast query speed and reduced storage cost hashing ha been widely used for efficient nearest neighbor search in a large variety of application like text and image retrieval most existing hashing method adopt hamming distance to measure the similarity neighborhood between point in the hashcode space however one problem with hamming distance is that it may destroy the neighborhood structure in the original feature space which violates the essential goal of hashing in this paper manhattan hashing mh which is based on manhattan distance is proposed to solve the problem of hamming distance based hashing the basic idea of mh is to encode each projected dimension with multiple bit of natural binary code nbc based on which the manhattan distance between point in the hashcode space is calculated for nearest neighbor search mh can effectively preserve the neighborhood structure in the data to achieve the goal of hashing to the best of our knowledge this is the first work to adopt manhattan distance with nbc for hashing experiment on several large scale image data set containing up to one million point show that our mh method can significantly outperform other state of the art method 
collaborative filtering cf is a major technique in recommender system to help user find their potentially desired item since the data sparsity problem is quite commonly encountered in real world scenario cross domain collaborative filtering cdcf hence is becoming an emerging research topic in recent year however due to the lack of sufficient dense explicit feedback and even no feedback available in user uninvolved domain current cdcf approach may not perform satisfactorily in user preference prediction in this paper we propose a generalized cross domain triadic factorization cdtf model over the triadic relation user item domain which can better capture the interaction between domain specific user factor and item factor in particular we devise two cdtf algorithm to leverage user explicit and implicit feedback respectively along with a genetic algorithm based weight parameter tuning algorithm to trade off influence among domain optimally finally we conduct experiment to evaluate our model and compare with other state of the art model by using two real world datasets the result show the superiority of our model against other comparative model 
we consider the impact of inter assessor disagreement on the maximum performance that a ranker can hope to achieve we demonstrate that even if a ranker were to achieve perfect performance with respect to a given assessor when evaluated with respect to a different assessor the measured performance of the ranker decrease significantly this decrease in performance may largely account for observed limit on the performance of learning to rank algorithm 
in this paper we consider the problem of devising blocking scheme for entity matching there is a lot of work on blocking technique for supporting various kind of predicate e g exact match fuzzy string similarity match and spatial match however given a complex entity matching function in the form of a boolean expression over several such predicate we show that it is an important and non trivial problem to combine the individual blocking technique into an efficient blocking scheme for the entity matching function a problem that ha not been studied previously in this paper we make fundamental contribution to this problem we consider an abstraction for modeling complex entity matching function a well a blocking scheme we present several result of theoretical and practical interest for the problem we show that in general the problem of computing the optimal blocking strategy is np hard in the size of the dnf formula describing the matching function we also present several algorithm for computing the exact optimal strategy with exponential complexity but often feasible in practice a well a fast approximation algorithm we experimentally demonstrate over commercially used rule based matching system over real datasets at yahoo a well a synthetic datasets that our blocking strategy can be an order of magnitude faster than the baseline method and our algorithm can efficiently find good blocking strategy 
prior search result diversification work focus on achieving topical variety in a ranked list typically equally across all aspect in this paper we diversify with sentiment according to an explicit bias we want to allow user to switch the result perspective to better grasp the polarity of opinionated content such a during a literature review for this we first infer the prior sentiment bias inherent in a controversial topic the topic sentiment then we utilize this information in different way to diversify result according to various sentiment bias equal diversification to achieve a balanced and unbiased representation of all sentiment on the topic diversification towards the topic sentiment in which the actual sentiment bias in the topic is mirrored to emphasize the general perception of the topic diversification against the topic sentiment in which document about the minority or outlying sentiment s are boosted and those with the popular sentiment are demoted since sentiment classification is an essential tool for this task we experiment by gradually degrading the accuracy of a perfect classifier down to and show which diversification approach prove most stable in this setting the result reveal that the proportionality based method and our scsf model considering sentiment strength and frequency in the diversified list yield the highest gain further in case the topic sentiment cannot be reliably estimated we show how performance is affected by equal diversification when actually an emphasis either towards or against the topic sentiment is desired in the former case an average of is lost across all evaluation measure whereas in the latter case this is confirming that bias specific sentiment diversification is crucial 
information retrieval in technical domain like physic is characterised by long and precise query whose meaning is strongly influenced by term context and domain we treat this a a disambiguation problem and present initial finding of a retrieval model that posit a higher probability of relevance for document matching disambiguated query term preliminary evaluation on a real life physic test collection show promising performance improvement 
we consider the problem of building online machine learned model for detecting auction fraud in e commence web site since the emergence of the world wide web online shopping and online auction have gained more and more popularity while people are enjoying the benefit from online trading criminal are also taking advantage to conduct fraudulent activity against honest party to obtain illegal profit hence proactive fraud detection moderation system are commonly applied in practice to detect and prevent such illegal and fraud activity machine learned model especially those that are learned online are able to catch fraud more efficiently and quickly than human tuned rule based system in this paper we propose an online probit model framework which take online feature selection coefficient bound from human knowledge and multiple instance learning into account simultaneously by empirical experiment on a real world online auction fraud detection data we show that this model can potentially detect more fraud and significantly reduce customer complaint compared to several baseline model and the human tuned rule based system 
click log from search engine provide a rich opportunity to acquire implicit feedback from user pattern derived from the time between a posted query and a click provide information on the ranking quality reflecting the perceived relevance of a retrieved url this paper applies the kaplan meier estimator to study click pattern the visualization of click curve demonstrates the interaction between the relevance and the rank position of url the observed result demonstrate the potential of using click curve to predict the quality of the top ranked result 
search engine are an essential component of the web but their web crawling agent can impose a significant burden on heavily loaded web server unfortunately blocking or deferring web crawler request is not a viable solution due to economic consequence we conduct a quantitative measurement study on the impact and cost of web crawling agent seeking optimization point for this class of request based on our measurement we present a practical caching approach for mitigating search engine overload and implement the two level cache scheme on a very busy web server our experimental result show that the proposed caching framework can effectively reduce the impact of search engine overload on service quality 
high quality relevance judgment are essential for the evaluation of information retrieval system traditional method of collecting relevance judgment are based on collecting binary or graded nominal judgment but such judgment are limited by factor such a inter assessor disagreement and the arbitrariness of grade previous research ha shown that it is easier for assessor to make pairwise preference judgment however unless the preference collected are largely transitive it is not clear how to combine them in order to obtain document relevance score another difficulty is that the number of pair that need to be assessed is quadratic in the number of document in this work we consider the problem of inferring document relevance score from pairwise preference judgment by analogy to tournament using the elo rating system we show how to combine a linear number of pairwise preference judgment from multiple assessor to compute relevance score for every document 
hierarchical taxonomy provide a multi level view of large document collection allowing user to rapidly drill down to fine grained distinction in topic of interest we show that automatically induced taxonomy can be made more robust by combining text with relational link the underlying mechanism is a bayesian generative model in which a latent hierarchical structure explains the observed data thus finding hierarchical group of document with similar word distribution and dense network connection a a nonparametric bayesian model our approach doe not require pre specification of the branching factor at each non terminal but find the appropriate level of detail directly from the data unlike many prior latent space model of network structure the complexity of our approach doe not grow quadratically in the number of document enabling application to network with more than ten thousand node experimental result on hypertext and citation network corpus demonstrate the advantage of our hierarchical multimodal approach 
a better understanding of strategy and behavior of successful searcher is crucial for improving the experience of all searcher however research of search behavior ha been struggling with the tension between the relatively small scale but controlled lab study and the large scale log based study where the searcher intent and many other important factor have to be inferred we present our solution for performing controlled yet realistic scalable and reproducible study of searcher behavior we focus on difficult informational task which tend to frustrate many user of the current web search technology first we propose a principled formalization of different type of success for informational search which encapsulate and sharpen previously proposed model second we present a scalable game like infrastructure for crowdsourcing search behavior study specifically targeted towards capturing and evaluating successful search strategy on informational task with known intent third we report our analysis of search success using these data which confirm and extends previous finding finally we demonstrate that our model can predict search success more effectively than the existing state of the art method on both our data and on a different set of log data collected from regular search engine session together our search success model the data collection infrastructure and the associated behavior analysis technique significantly advance the study of success in web search 
a an increasing amount of rdf data is published a linked data intuitive way of accessing this data become more and more important question answering approach have been proposed a a good compromise between intuitiveness and expressivity most question answering system translate question into triple which are matched against the rdf data to retrieve an answer typically relying on some similarity metric however in many case triple do not represent a faithful representation of the semantic structure of the natural language question with the result that more expressive query can not be answered to circumvent this problem we present a novel approach that relies on a parse of the question to produce a sparql template that directly mirror the internal structure of the question this template is then instantiated using statistical entity identification and predicate detection we show that this approach is competitive and discus case of question that can be answered with our approach but not with competing approach 
we propose a simple scalable and non parametric approach for short text classification leveraging the well studied and scalable information retrieval ir framework our approach mimic human labeling process for a piece of short text it first selects the most representative and topical indicative word from a given short text a query word and then search for a small set of labeled short text best matching the query word the predicted category label is the majority vote of the search result evaluated on a collection of more than k web snippet the proposed approach achieves comparable classification accuracy with the baseline maximum entropy classifier using a few a query word and top best matching search hit among the four query word selection scheme proposed and evaluated in our experiment term frequency together with clarity give the best classification accuracy 
an increasingly common feature of online community and social medium site is a mechanism for rewarding user achievement based on a system of badge badge are given to user for particular contribution to a site such a performing a certain number of action of a given type they have been employed in many domain including news site like the huffington post educational site like khan academy and knowledge creation site like wikipedia and stack overflow at the most basic level badge serve a a summary of a user s key accomplishment however experience with these site also show that user will put in non trivial amount of work to achieve particular badge and a such badge can act a powerful incentive thus far however the incentive structure created by badge have not been well understood making it difficult to deploy badge with an eye toward the incentive they are likely to create in this paper we study how badge can influence and steer user behavior on a site leading both to increased participation and to change in the mix of activity a user pursues on the site we introduce a formal model for reasoning about user behavior in the presence of badge and in particular for analyzing the way in which badge can steer user to change their behavior to evaluate the main prediction of our model we study the use of badge and their effect on the widely used stack overflow question answering site and find evidence that their badge steer behavior in way closely consistent with the prediction of our model finally we investigate the problem of how to optimally place badge in order to induce particular user behavior several robust design principle emerge from our framework that could potentially aid in the design of incentive for a broad range of site 
with the rapid growth of social medium website microblogging ha become a popular way to spread instant news and event due to the dynamic and social nature of microblogs extracting useful information from microblogs is more challenging than from the traditional news article in this paper we study the problem of summarizing the difference from microblogs given a collection of microblogs discussing an event topic we propose to generate a short summary delivering the difference among these microblogs such a the different point of view for a news topic and the change and evolution of an ongoing event 
current information retrieval system for digital cultural heritage support only the actual search aspect of the information seeking process this demonstration present the second path system which provides the exploration analysis and sense making feature to support the full information seeking process 
current approach for search result diversification have been categorized a either implicit or explicit the implicit approach assumes each document represents it own topic and promotes diversity by selecting document for different topic based on the difference of their vocabulary on the other hand the explicit approach model the set of query topic or aspect while the former approach is generally le effective the latter usually depends on a manually created description of the query aspect the automatic construction of which ha proven difficult this paper introduces a new approach term level diversification instead of modeling the set of query aspect which are typically represented a coherent group of term our approach us term without the grouping our result on the clueweb collection show that the grouping of topic term provides very little benefit to diversification compared to simply using the term themselves consequently we demonstrate that term level diversification with topic term identified automatically from the search result using a simple greedy algorithm significantly outperforms method that attempt to create a full topic structure for diversification 
we present turkalytics a novel analytics tool for human computation system turkalytics process and report logging event from worker in real time and ha been shown to scale to over one hundred thousand logging event per day we present a state model for worker interaction that cover the mechanical turk the scrap model and a data model that demonstrates the diversity of data collected by turkalytics we show that turkalytics is effective at data collection in spite of it being unobtrusive lastly we describe worker location browser environment activity information and other example of data collected by our tool 
most commercial search engine have a query suggestion feature which is designed to capture various possible search intent behind the user s original query however even though different search intent behind a given query may have been popular at different time period in the past existing query suggestion method neither utilize nor present such information in this study we propose time aware structured query suggestion tasqs which cluster query suggestion along a timeline so that the user can narrow down his search from a temporal point of view moreover when a suggested query is clicked tasqs present web page from query url bipartite graph after ranking them according to the click count within a particular time period our experiment using data from a commercial search engine log show that the time aware clustering and the time aware document ranking feature of tasqs are both effective 
query performance prediction is aimed at predicting the retrieval effectiveness that a query will achieve with respect to a particular ranking model in this paper we study query performance prediction for a ranking model that explicitly incorporates the time dimension into ranking different time based predictor are proposed a analogous to existing keyword based predictor in order to improve predicting performance we combine different predictor using linear regression and neural network extensive experiment are conducted using query and relevance judgment obtained by crowdsourcing 
in this paper we preliminarily learn the problem of reconstructing user life history based on the their twitter stream and proposed an unsupervised framework that create a chronological list for personal important event pie of individual by analyzing individual tweet collection we find that what are suitable for inclusion in the personal timeline should be tweet talking about personal a opposed to public and time specific a opposed to time general topic to further extract these type of topic we introduce a non parametric multi level dirichlet process model to recognize four type of tweet personal time specific personts personal time general persontg public time specific publicts and public time general publictg topic which in turn are used for further personal event extraction and timeline generation to the best of our knowledge this is the first work focused on the generation of timeline for individual from twitter data for evaluation we have built gold standard timeline that contain pie related event from ordinary twitter user and celebrity experimental result demonstrate that it is feasible to automatically extract chronological timeline for twitter user from their tweet collection 
timeline generation is an important research task which can help user to have a quick understanding of the overall evolution of any given topic it thus attracts much attention from research community in recent year nevertheless existing work on timeline generation often ignores an important factor the attention attracted to topic of interest hereafter termed social attention without taking into consideration social attention the generated timeline may not reflect user collective interest in this paper we study how to incorporate social attention in the generation of timeline summary in particular for a given topic we capture social attention by learning user collective interest in the form of word distribution from twitter which are subsequently incorporated into a unified framework for timeline summary generation we construct four evaluation set over six diverse topic we demonstrate that our proposed approach is able to generate both informative and interesting timeline our work shed light on the feasibility of incorporating social attention into traditional text mining task 
we investigated the problem of finding from a collection of string those similar to a given query string based on edit distance for which the critical operation is merging inverted list of gram generated from the collection of string we present an efficient algorithm to accelerate the merging operation 
the development of solution to scale the extraction of data from web source is still a challenging issue high accuracy can be achieved by supervised approach but the cost of training data i e annotation over a set of sample page limit their scalability crowd sourcing platform are making the manual annotation process more affordable however the task demanded to these platform should be extremely simple to be performed by non expert people and their number should be minimized to contain the cost we introduce a framework to support a supervised wrapper inference system with training data generated by the crowd training data are labeled value generated by mean of membership query the simplest form of query posed to the crowd we show that the cost of producing the training data are strongly affected by the expressiveness of the wrapper formalism and by the choice of the training set traditional supervised wrapper inference approach use a statically defined formalism assuming it is able to express the wrapper conversely we present an inference algorithm that dynamically chooses the expressiveness of the wrapper formalism and actively selects the training set while minimizing the number of membership query to the crowd we report the result of experiment on real web source to confirm the effectiveness and the feasibility of the approach 
malicious web page that host drive by download exploit have become a popular mean for compromising host on the internet and subsequently for creating large scale botnets in a drive by download exploit an attacker embeds a malicious script typically written in javascript into a web page when a victim visit this page the script is executed and attempt to compromise the browser or one of it plugins to detect drive by download exploit researcher have developed a number of system that analyze web page for the presence of malicious code most of these system use dynamic analysis that is they run the script associated with a web page either directly in a real browser running in a virtualized environment or in an emulated browser and they monitor the script execution for malicious activity while the tool are quite precise the analysis process is costly often requiring in the order of ten of second for a single page therefore performing this analysis on a large set of web page containing hundred of million of sample can be prohibitive one approach to reduce the resource required for performing large scale analysis of malicious web page is to develop a fast and reliable filter that can quickly discard page that are benign forwarding to the costly analysis tool only the page that are likely to contain malicious code in this paper we describe the design and implementation of such a filter our filter called prophiler us static analysis technique to quickly examine a web page for malicious content this analysis take into account feature derived from the html content of a page from the associated javascript code and from the corresponding url we automatically derive detection model that use these feature using machine learning technique applied to labeled datasets to demonstrate the effectiveness and efficiency of prophiler we crawled and collected million of page which we analyzed for malicious behavior our result show that our filter is able to reduce the load on a more costly dynamic analysis tool by more than with a negligible amount of missed malicious page 
cloud computing is an emerging paradigm which allows the on demand delivering of software hardware and data a service a cloud based service are more numerous and dynamic the development of efficient service provisioning policy become increasingly challenging game theoretic approach have shown to gain a thorough analytical understanding of the service provisioning problem in this paper we take the perspective of software a a service saas provider which host their application at an infrastructure a a service iaa provider each saas need to comply with quality of service requirement specified in service level agreement sla contract with the end user which determine the revenue and penalty on the basis of the achieved performance level saas provider want to maximize their revenue from slas while minimizing the cost of use of resource supplied by the iaa provider moreover saas provider compete and bid for the use of infrastructural resource on the other hand the iaa want to maximize the revenue obtained providing virtualized resource in this paper we model the service provisioning problem a a generalized nash game and we propose an efficient algorithm for the run time management and allocation of iaa resource to competing saass 
news clustering categorization and analysis are key component of any news portal they require algorithm capable of dealing with dynamic data to cluster interpret and to temporally aggregate news article these three task are often solved separately in this paper we present a unified framework to group incoming news article into temporary but tightly focused storyline to identify prevalent topic and key entity within these story and to reveal the temporal structure of story a they evolve we achieve this by building a hybrid clustering and topic model to deal with the available wealth of data we build an efficient parallel inference algorithm by sequential monte carlo estimation time and memory cost are nearly constant in the length of the history and the approach scale to hundred of thousand of document we demonstrate the efficiency and accuracy on the publicly available tdt dataset and data of a major internet news site 
query auto completion qac is a common feature in modern search engine high quality qac candidate enhance search experience by saving user time that otherwise would be spent on typing each character or word sequentially current qac method rank suggestion according to their past popularity however query popularity change over time and the ranking of candidate must be adjusted accordingly for instance while halloween might be the right suggestion after typing ha in october harry potter might be better any other time surprisingly despite the importance of qac a a key feature in most online search engine it temporal dynamic have been under studied in this paper we propose a time sensitive approach for query auto completion instead of ranking candidate according to their past popularity we apply time series and rank candidate according their forecasted frequency our experiment on k query and their daily frequency sampled over a period of year show that predicting the popularity of query solely based on their past frequency can be misleading and the forecast obtained by time series modeling are substantially more reliable our result also suggest that modeling the temporal trend of query can significantly improve the ranking of qac candidate 
information retrieval research ha made significant progress in the retrieval of text document and image however relatively little attention ha been given to the retrieval of information graphic non pictorial image such a bar chart and line graph despite their proliferation in popular medium such a newspaper and magazine our goal is to build a system for retrieving bar chart and line graph that reason about the content of the graphic itself in deciding it relevance to the user query this paper present the first step toward such a system with a focus on identifying the category of intended message of potentially relevant bar chart and line graph our learned model achieves accuracy higher than on a corpus of collected user query 
searching over heterogeneous structured data on the web is challenging due to vocabulary and structure mismatch among different data source in this paper we study two existing strategy and present a new approach to integrate additional data source into the search process the first strategy relies on data integration to mediate mismatch through upfront computation of mapping based on which query are rewritten to fit individual source the other extreme is keyword search which doe not require any up front investment but ignores structure information building on these strategy we present a hybrid approach which combine the advantage of both our approach doe not require any upfront data integration but also leverage the fine grained structure of the underlying data for a structured query adhering to the vocabulary of just one source the so called seed query we construct an entity relevance model erm which capture the content and the structure of the seed query result this erm is then aligned on the fly with keyword search result retrieved from other source and also used to rank these result the outcome of our experiment using large scale real world data set suggests that data integration lead to higher search effectiveness compared to keyword search and that our new hybrid approach consistently exceeds both strategy 
proactive link suggestion lead to improved user experience by allowing user to reach relevant information with fewer click fewer page to read or simply faster because the right page are prefetched just in time in this paper we tackle two new scenario for link suggestion which were not covered in prior work owing to scarcity of historical browsing data in the web search scenario we propose a method for generating quick link additional entry point into web site which are shown for top search result for navigational query for tail site for which little browsing statistic is available beyond web search we also propose a method for link suggestion in general web browsing effectively anticipating the next link to be followed by the user our approach performs clustering of web site in order to aggregate information across multiple site and enables relevant link suggestion for virtually any site including tail site and brand new site for which little historical data is available empirical evaluation confirms the validity of our method using editorially labeled data a well a real life search and browsing data from a major u search engine 
the number of image available online is growing steadily and current web search engine have indexed more than billion image approach to image retrieval are still often text based and operate on image annotation and caption image annotation i e image tag are typically short user generated and of varying quality which increase the mismatch problem between query term and image tag for example a user might enter the query wedding dress while all image are annotated with bridal gown or wedding gown this demonstration present an image search system using reduction and expansion of image annotation to overcome vocabulary mismatch problem by enriching the sparse set of image tag our image search application accepts a written query a input and produce a ranked list of result image and annotation i e image tag a output the system integrates method to reduce and expand the image tag set thus decreasing the effect of sparse image tag it build on different image collection such a the wikipedia image collection http www imageclef org wikidata and the microsoft office com clipart collection http office microsoft com but can be applied to social collection such a flickr a well our demonstration system run on pc tablet and smartphones making use of advanced user interface capability on mobile device 
good clustering can provide critical insight into potential location where congestion in a network may occur a natural measure of congestion for a collection of node in a graph is it cheeger ratio defined a the ratio of the size of it boundary to it volume spectral method provide effective mean to estimate the smallest cheeger ratio via the spectral gap of the graph laplacian here we compute the spectral gap of the truncated graph laplacian with the so called dirichlet boundary condition for the graph of a dozen communication network at the ip layer which are subgraphs of the much larger global ip layer network we show that i the dirichlet spectral gap of these network is substantially larger than the standard spectral gap and is therefore a better indicator of the true expansion property of the graph ii unlike the standard spectral gap the dirichlet spectral gap of progressively larger subgraphs converge to that of the global network thus allowing property of the global network to be efficiently obtained from them and iii the first two eigenvectors of the dirichlet graph laplacian can be used for spectral clustering with arguably better result than standard spectral clustering we first demonstrate these result analytically for finite regular tree we then perform spectral clustering on the ip layer network using dirichlet eigenvectors and show that it yield cut near the network core thus creating genuine single component cluster this is much better than traditional spectral clustering where several disjoint fragment near the network periphery are liable to be misleadingly classified a a single cluster since congestion in communication network is known to peak at the core due to large scale curvature and geometry identification of core congestion and it localization are important step in analysis and improved engineering of network thus spectral clustering with dirichlet boundary condition is seen to be more effective at finding bona fide bottleneck and congestion than standard spectral clustering 
the key issue of person name disambiguation is to discover different namesake in massive web document rather than simply cluster document by using textual feature in this paper we describe a novel person name disambiguation method based on social network to effectively identify namesake the social network snippet in each document are extracted then the namesake are identified via splicing the social network of each namesake by using the snippet a a bipartite graph experimental result show that our method achieves better result than the top performance of weps in identifying different namesake 
many important search task require multiple search session to complete task such a travel planning large purchase or job search can span hour day or even week inevitably life interferes requiring the searcher either to recover the state of the search manually most common or plan for interruption in advance unlikely the goal of this work is to better understand characterize and automatically detect search task that will be continued in the near future to this end we analyze a query log from the bing web search engine to identify the type of intent topic and search behavior pattern associated with long running task that are likely to be continued using our insight we develop an effective prediction algorithm that significantly outperforms both the previous state of the art method and even the ability of human judge to predict future task continuation potential application of our technique would allow a search engine to pre emptively save state for a searcher e g by caching search result perform more targeted personalization and otherwise better support the searcher experience for interrupted search task 
social group often exhibit a high degree of dynamism some group thrive while many others die over time modeling group stability dynamic and understanding whether when a group will remain stable or shrink over time can be important in a number of social domain in this paper we study two different type of social network a exemplar platform for modeling and predicting group stability dynamic we build model to predict if a group is going to remain stable or is likely to shrink over a period of time we observe that both the level of member diversity and social activity are critical in maintaining the stability of group we also find that certain prolific member play a more important role in maintaining the group stability our study show that group stability can be predicted with high accuracy and feature diversity is critical to prediction performance 
ensemble recommender system successfully enhance recom mendation accuracy by exploiting different source of user prefe rences such a rating and social contact in linear ensemble the optimal weight of each recommender strategy is commonly tuned empirically with limited guarantee that such weight are optimal afterwards we propose a self adjusting hybrid recommendation approach that alleviates the social cold start situation by weighting the recommender combination dynamically at recommendation time based on social network analysis algorithm we show empirical result where our approach outperforms the best static combination for different hybrid recommenders 
in addition to the main content most web page also contain navigation panel advertisement and copyright and disclaimer notice this additional content which is also known a noise is typically not related to the main subject and may hamper the performance of web data mining and hence need to be removed properly in this paper we present content extraction via text density cetd a fast accurate and general method for extracting content from diverse web page and using dom document object model node text density to preserve the original structure for this purpose we introduce two concept to measure the importance of node text density and composite text density in order to extract content intact we propose a technique called densitysum to replace data smoothing the approach wa evaluated with the cleaneval benchmark and with randomly selected page from well known website where various web domain and style are tested the average f score with our method were higher than the best score among several alternative method 
in many entity extraction application the entity to be recognized are constrained to be from a list of target entity in many case these target entity are i ad hoc i e do not exist in a knowledge base and ii homogeneous e g all the entity are it company we study the following novel disambiguation problem in this unique setting given the candidate mention of all the target entity determine which one are true mention of a target entity prior technique only consider target entity present in a knowledge base and or having a rich set of attribute in this paper we develop novel technique that require no knowledge about the entity except their name our main insight is to leverage the homogeneity constraint and disambiguate the candidate mention collectively across all document we propose a graph based model called mentionrank for that purpose furthermore if additional knowledge is available for some or all of the entity our model can leverage it to further improve quality our experiment demonstrate the effectiveness of our model to the best of our knowledge this is the first work on targeted entity disambiguation for ad hoc entity 
web search behaviour study including eye tracking study of search result examination have resulted in numerous insight to improve search result quality and presentation yet eye tracking study have been restricted in scale due to the expense and the effort required furthermore a the reach of the web expands it becomes increasingly important to understand how searcher around the world see and interact with the search result to address both challenge we introduce viewser a novel methodology for performing web search examination study remotely at scale and without requiring eye tracking equipment viewser operates by automatically modifying the appearance of a search engine result page to clearly show one search result at a time a if through a viewport while partially blurring the rest and allowing the participant to move the viewport naturally with a computer mouse or trackpad remarkably the resulting result viewing and clickthrough pattern agree closely with unrestricted viewing of result a measured by eye tracking equipment validated by a study with over participant we also explore application of viewser to practical search task such a analyzing the search result summary snippet attractiveness result re ranking and evaluating snippet quality these experiment could have only be done previously by tracking the eye movement for a small number of subject in the lab in contrast our study wa performed with over participant allowing u to reproduce and extend previous finding establishing viewser a a valuable tool for large scale search behavior experiment 
we present a large scale measurement of cluster of host sharing the same local dns server we analyze property of these ldns cluster from the perspective of content delivery network which commonly use dns for load distribution we found that not only ldns cluster differ widely in term of their size and geographical compactness but that the largest cluster are actually extremely compact this suggests potential benefit of a load distribution strategy with nuanced treatment of different ldns cluster based on the combination of their size and compactness we further observed interesting variation in ldns setup including a wide use of ldns pool which a we explain in the paper are different from setup where end host simply utilize multiple resolvers 
many real life graph such a social network and peer to peer network capture the relationship among the node by using trust score to label the edge important usage of such network includes trust prediction finding the most reliable or trusted node in a local subgraph etc for many of these application it is crucial to ass the prestige and bias of a node the bias of a node denotes it propensity to trust mistrust it neighbour and is closely related to truthfulness if a node trust all it neighbour it recommendation of another node a trustworthy is le reliable it is based on the idea that the recommendation of a highly biased node should weigh le in this paper we propose an algorithm to compute the bias and prestige of node in network where the edge weight denotes the trust score unlike most other graph based algorithm our method work even when the edge weight are not necessarily positive the algorithm is iterative and run in o km time where k is the number of iteration and m is the total number of edge in the network the algorithm exhibit several other desirable property it converges to a unique value very quickly also the error in bias and prestige value at any particular iteration is bounded further experiment show that our model conforms well to social theory such a the balance theory enemy of a friend is an enemy etc 
recently recommender system have fascinated researcher and benefited a variety of people s online activity enabling user to survive the explosive web information traditional collaborative filtering technique handle the general recommendation well however most such approach usually focus on long term preference to discover more short term factor influencing people s decision we propose a short term preference model implemented with implicit user feedback we conduct experiment comparing the performance of different short term model which show that our model outperforms significantly compared to those long term model 
web content quality assessment is a typical static ranking problem heuristic content and tfidf feature based statistical system have proven effective for web content quality assessment but they are all language dependent feature which are not suitable for cross language ranking in this paper we fuse a series of language independent feature including hostname feature domain registration feature two layer hyperlink analysis feature and third party web service feature to ass the web content quality the experiment on ecml pkdd discovery challenge cross language datasets show that the assessment is effective 
mobile device provide people with a conduit to the rich infor mation resource of the web with consent the device can also provide stream of information about search activity and location that can be used in population study and real time assistance we analyzed geotagged mobile query in a privacy sensitive study of potential transition from health information search to in world healthcare utilization we note difference in people s health infor mation seeking before during and after the appearance of evidence that a medical facility ha been visited we find that we can accu rately estimate statistic about such potential user engagement with healthcare provider the finding highlight the promise of using geocoded search for sensing and predicting activity in the world 
in information retrieval relevance judgment play an important role a they are required both for evaluating the quality of retrieval system and for training learning to rank algorithm in recent year numerous paper have been published using judgment obtained from a commercial search engine by researcher in industry a typically no information is provided about the quality of these judgment their reliability for evaluating training retrieval system remains questionable in this paper we analyze the reliability of such judgment for evaluating the quality of retrieval system by comparing them to judgment by nist judge at trec 
catch up or on demand access of previously broadcast tv content over the public internet constitutes a significant fraction of peak time network traffic this paper analysis consumption pattern of nearly million user of a nationwide deployment of a catch up tv service to understand the network support required we find that catch up ha certain natural scaling property compared to traditional tv the on demand nature spread load over time and user have much higher completion rate for content stream than previously reported user exhibit strong preference for serialised content and for specific genre exploiting this we design a speculative content offloading and recording engine score that predictively record a personalised set of show on user local storage and thereby offloads traffic that might result from subsequent catch up access evaluation show that even with a modest storage of gb an oracle with complete knowledge of user consumption can save up to of the energy and of the peak bandwidth compared to the current ip streaming based architecture in the best case optimising for energy consumption score can recover more than of the traffic and energy saving achieved by the oracle optimising purely for traffic rather than energy can reduce bandwith by an additional 
a key challenge in recommender system research is how to effectively profile new user a problem generally known a cold start recommendation recently the idea of progressively querying user response through an initial interview process ha been proposed a a useful new user preference elicitation strategy in this paper we present functional matrix factorization fmf a novel cold start recommendation method that solves the problem of initial interview construction within the context of learning user and item profile specifically fmf construct a decision tree for the initial interview with each node being an interview question enabling the recommender to query a user adaptively according to her prior response more importantly we associate latent profile for each node of the tree in effect restricting the latent profile to be a function of possible answer to the interview question which allows the profile to be gradually refined through the interview process based on user response we develop an iterative optimization algorithm that alternate between decision tree construction and latent profile extraction a well a a regularization scheme that take into account of the tree structure experimental result on three benchmark recommendation data set demonstrate that the proposed fmf algorithm significantly outperforms existing method for cold start recommendation 
crowdsourcing ha emerged in recent year a a promising new avenue for leveraging today s digitally connected diverse distributed workforce generally speaking crowdsourcing describes outsourcing of task to a large group of people instead of assigning such task to an in house employee or contractor crowdsourcing platform such a amazon mechanical turk and crowdflower have gained particular attention a active online market place for reaching and tapping into this still largely under utilized workforce crowdsourcing also offer intriguing new opportunity for accomplishing different kind of task or achieving broader participation than previously possible a well a completing standard task more accurately in le time and at lower cost unlocking the potential of crowdsourcing in practice however requires a tri partite understanding of principle platform and best practice we will introduce the opportunity and challenge of crowdsourcing while discussing the three issue above this will provide a basic foundation to begin crowdsourcing in the context of one s own particular task 
cache of result are critical component of modern web search engine since they enable lower response time to frequent query and reduce the load to the search engine backend result in long lived cache entry may become stale however a search engine continuously update their index to incorporate change to the web consequently it is important to provide mechanism that control the degree of staleness of cached result ideally enabling the search engine to always return fresh result in this paper we present a new mechanism that identifies and invalidates query result that have become stale in the cache online the basic idea is to evaluate at query time and against recent change if cache hit have had their result have changed for enhancing invalidation efficiency the generation time of cached query and their chronological order with respect to the latest index update are used to early prune unaffected query we evaluate the proposed approach using document that change over time and query log of the yahoo search engine we show that the proposed approach ensures good query result fewer stale result and high invalidation accuracy fewer unnecessary invalidation compared to a baseline approach that make invalidation decision off line more importantly the proposed approach induces le processing overhead ensuring an average throughput higher than that of the baseline approach 
the multimedia information retrieval community ha dedicated extensive research effort to the problem of content based image retrieval cbir however these system find their main limitation in the difficulty of creating pictorial query a a result few system offer the option of querying by visual example and rely on automatic concept detection and tagging technique to provide support for searching visual content using textual query this paper proposes and study a practical multimodal web search scenario where cbir fit intuitively to improve the retrieval of rich information query many online article contain useful know how knowledge about computer application these article tend to be richly illustrated by screenshots we present a system to search for such software know how article that leverage the visual correspondence between screenshots user can naturally create pictorial query simply by taking a screenshot of the application to retrieve a list of article containing a matching screenshot we build a prototype comprising k article that are classified into walkthrough book gallery and general category and provide a comprehensive evaluation of this system focusing on technical accuracy of cbir technique and usability perceived system usefulness aspect we also consider the study of added value feature of such a visual supported search including the ability to perform cross lingual query we find that the system is able to retrieve matching screenshots for a wide variety of program across language boundary and provide subjectively more useful result than keyword based web and image search engine 
the information need of search engine user vary in complexity depending on the task they are trying to accomplish some simple need can be satisfied with a single query whereas others require a series of query issued over a longer period of time while search engine effectively satisfy many simple need searcher receive little support when their information need span session boundary in this work we propose method for modeling and analyzing user search behavior that extends over multiple search session we focus on two problem i given a user query identify all of the related query from previous session that the same user ha issued and ii given a multi query task for a user predict whether the user will return to this task in the future we model both problem within a classification framework that us feature of individual query and long term user search behavior at different granularity experimental evaluation of the proposed model for both task indicates that it is possible to effectively model and analyze cross session search behavior our finding have implication for improving search for complex information need and designing search engine feature to support cross session search task 
we present greenmeter a tool for assessing the quality and recommending tag for web content it goal is to improve tag quality and the effectiveness of various information service e g search content recommendation that rely on tag a data source we demonstrate an implementation of greenmeter for the popular last fm application 
data center run many service that impact million of user daily in reality the latency of each service varies from one request to another existing tool allow to monitor service for performance glitch or service disruption but typically they do not help understanding the variation in latency we propose a general framework for understanding performance of arbitrary black box service we consider a stream of request to a given service with their monitored attribute a well a latency of serving each request we propose what we call the multi dimensional f measure that help for a given interval to identify the subset of monitored attribute that explains it we design algorithm that use this measure not only for a fixed latency interval but also to explain the entire range of latency of the service by segmenting it into smaller interval we perform a detailed experimental study with synthetic data a well a real data from a large search engine our experiment show that our method automatically identify significant latency interval together with request attribute that explain them and are robust 
online social network have become a major force in today s society and economy the largest of today s social network may have hundred of million to more than a billion user such network are too large to be downloaded or stored locally even if term of use and privacy policy were to permit doing so this limitation complicates even simple computational task one such task is computing the clustering coefficient of a network another task is to compute the network size number of registered user or a subpopulation size the clustering coefficient a classic measure of network connectivity come in two flavor global and network average in this work we provide efficient algorithm for estimating these measure which assume no prior knowledge about the network and access the network using only the publicly available interface more precisely this work provides three new estimation algorithm a the first external access algorithm for estimating the global clustering coefficient b an external access algorithm that improves on the accuracy of previous network average clustering coefficient estimation algorithm and c an improved external access network size estimation algorithm the main insight offered by this work is that only a relatively small number of public interface call are required to allow our algorithm to achieve a high accuracy estimation our approach is to view a social network a an undirected graph and use the public interface to retrieve a random walk to estimate the clustering coefficient the connectivity of each node in the random walk sequence is tested in turn we show that the error of this estimation drop exponentially in the number of random walk step another insight of this work is the fact that although the proposed algorithm can be used to estimate the clustering coefficient of any undirected graph they are particularly efficient on social network like graph to improve the network size prior art estimation algorithm we count node collision one step before they actually occur in our experiment we validate our algorithm on several publicly available social network datasets our result validate the theoretical claim and demonstrate the effectiveness of our algorithm 
understanding user intent from her sequential search behavior i e predicting the intent of each user query in a search session is crucial for modern web search engine however due to the huge number of user behavior variable and coarse level intent label defined by human editor it is very difficult to directly model user behavioral dynamic or user intent dynamic in user search session in this paper we propose a novel sparse hidden dynamic conditional random field shdcrf model for user intent learning from their search session through incorporating the proposed hidden state variable shdcrf aim to learn a substructure i e a set of related hidden variable for each intent label and they are used to model the intermediate dynamic between user intent label and user behavioral variable in addition shdcrf learns a sparse relation between the hidden variable and intent label to make the hidden state variable explainable extensive experiment result on real user search session from a popular commercial search engine show that the proposed shdcrf model significantly outperforms in term of intent prediction result that those classical solution such a support vector machine svm conditional random field crf and latnet dynamic conditional random field ldcrf 
people s belief and unconscious bias that arise from those belief influence their judgment decision making and action a is commonly accepted among psychologist bias can be observed in information retrieval in situation where searcher seek or are presented with information that significantly deviate from the truth there is little understanding of the impact of such bias in search in this paper we study search related bias via multiple probe an exploratory retrospective survey human labeling of the caption and result returned by a web search engine and a large scale log analysis of search behavior on that engine targeting yes no question in the critical domain of health search we show that web searcher exhibit their own bias and are also subject to bias from the search engine we clearly observe searcher favoring positive information over negative and more than expected given base rate based on consensus answer from physician we also show that search engine strongly favor a particular usually positive perspective irrespective of the truth importantly we show that these bias can be counterproductive and affect search outcome in our study around half of the answer that searcher settled on were actually incorrect our finding have implication for search engine design including the development of ranking algorithm that con sider the desire to satisfy searcher by validating their belief and providing accurate answer and properly considering base rate incorporating likelihood information into search is particularly important for consequential task such a those with a medical focus 
this paper present a framework called knowing camera for real time recognizing place of interest in smartphone photo with the availability of online geotagged image of such place we propose a probabilistic field of view model which capture the uncertainty in camera sensor data this model can be used to retrieve a set of candidate image the visual similarity computation of the candidate image relies on the sparse coding technique we also propose an ann filtering technique to speedup the sparse coding the final ranking combine an uncertain geometric relevance with the visual similarity our preliminary experiment conducted in an urban area of a large city show promising result the most distinguishing feature of our framework is it ability to perform well in contaminated real world online image database besides our framework is highly scalable a it doe not incur any complex data structure 
document expansion de in information retrieval ir involves modifying each document in the collection by introducing additional term into the document it is particularly useful to improve retrieval of short and noisy document where the additional term can improve the description of the document content existing approach to de assume that document to be expanded are from a single topic in the case of multi topic document this can lead to a topic bias in term selected for de and hence may result in poor retrieval quality due to the lack of coverage of the original document topic in the expanded document this paper proposes a new de technique providing a more uniform selection and weighting of de term from all constituent topic we show that our proposed method significantly outperforms the most recently reported relevance model based de method on a spoken document retrieval task for both manual and automatic speech recognition transcript 
to answer search query on a social network rich with user generated content it is desirable to give a higher ranking to content that is closer to the individual issuing the query query occur at node in the network document are also created by node in the same network and the goal is to find the document that match the query and is closest in network distance to the node issuing the query in this paper we present the partitioned multi indexing scheme which provides an approximate solution to this problem with m link in the network after an offline o m pre processing time our scheme allows for social index operation i e social search query a well a insertion and deletion of word into and from a document at any node all in time o further our scheme can be implemented on open source distributed streaming system such a yahoo s or twitter s storm so that every social index operation take o processing time and network query in the worst case and just two network query in the common case where the reverse index corresponding to the query keyword is much smaller than the memory available at any distributed compute node building on da sarma et al s approximate distance oracle the worst case approximation ratio of our scheme is o for undirected network our simulation on the social network twitter a well a synthetic network show that in practice the approximation ratio is actually close to for both directed and undirected network we believe that this work is the first demonstration of the feasibility of social search with real time text update at large scale 
the term online reputation address trust relationship amongst agent in dynamic open system these can appear a rating recommendation referral and feedback several reputation model and rating aggregation algorithm have been proposed however finding a trusted entity on the web is still an issue a all reputation system work individually the aim of this project is to introduce a global reputation system that aggregate people s opinion from different resource e g e commerce website and review with the help federated search technique a sentiment analysis approach is subsequently used to extract high quality opinion and inform how to increase trust in the search result 
reuse and remarketing of content and product is an integral part of the internet a e commerce ha grown online resale and secondary market form a significant part of the commerce space the intention and method for reselling are diverse in this paper we study an instance of such market that affords interesting data at large scale for mining purpose to understand the property and pattern of this online market a part of knowledge discovery of such a market we first formally propose criterion to reveal unseen resale behavior by elastic matching identification emi based on the account transfer and item similarity property of transaction then we present a large scale system that leverage mapreduce paradigm to mine million of online resale activity from petabyte scale heterogeneous e commerce data with the collected data we show that the number of resale activity lead to a power law distribution with a long tail where a significant share of user only resell in very low number and a large portion of resale come from a small number of highly active resellers we further conduct a comprehensive empirical study from different aspect of resale including the temporal spatial pattern user demographic reputation and the content of sale posting based on these observation we explore the feature related to successful resale transaction and evaluate if they can be predictable we also discus us of this information mining for business insight and user experience on a real world online marketplace 
researcher and social observer have both believed that hashtags a a new type of organizational object of information play a dual role in online microblogging community e g twitter on one hand a hashtag serf a a bookmark of content which link tweet with similar topic on the other hand a hashtag serf a the symbol of a community membership which bridge a virtual community of user are the real user aware of this dual role of hashtags is the dual role affecting their behavior of adopting a hashtag is hashtag adoption predictable we take the initiative to investigate and quantify the effect of the dual role on hashtag adoption we propose comprehensive measure to quantify the major factor of how a user selects content tag a well a join community experiment using large scale twitter datasets prove the effectiveness of the dual role where both the content measure and the community measure significantly correlate to hashtag adoption on twitter with these measure a feature a machine learning model can effectively predict the future adoption of hashtags that a user ha never used before 
we propose a kernel based model to automatically extract social relation such a economic relation and political relation between two people from news article to determine whether two people are structurally associated with each other the proposed model us an svm support vector machine tree kernel based on trigram of head dependent relation between them in the experiment with the automatic content extraction ace corpus and a korean news corpus the proposed model outperformed the previous system based on svm tree kernel even though it used more shallow linguistic knowledge 
voice search offer user with a new search experience instead of typing user can vocalize their search query however due to voice input error such a speech recognition error and improper system interruption user need to frequently reformulate query to handle the incorrectly recognized query we conducted user experiment with native english speaker on their query reformulation behavior in voice search and found that user often reformulate query with both lexical and phonetic change to previous query in this paper we first characterize and analyze typical voice input error in voice search and user corresponding reformulation strategy then we evaluate the impact of typical voice input error on user search progress and the effectiveness of different reformulation strategy on handling these error this study provides a clearer picture on how to further improve current voice search system 
seasonal event such a halloween and christmas repeat every year and initiate several temporal information need the impact of such event on user is often reflected in search log in form of seasonal spike in the frequency of related query e g halloween costume where is santa many seasonal query such a sigir conference mainly target fresh page e g sigir org that have le usage data such a click and anchor text compared to older alternative e g sigir org thus it is important for search engine to correctly identify seasonal query and make sure that their result are temporally reordered if necessary in this poster we focus on detecting seasonal query using time series analysis we demonstrate that the seasonality of a query can be determined with high accuracy according to it historical frequency distribution 
nutrition is a key factor in people s overall health hence understanding the nature and dynamic of population wide dietary preference over time and space can be valuable in public health to date study have leveraged small sample of participant via food intake log or treatment data we propose a complementary source of population data on nutrition obtained via web log our main contribution is a spatiotemporal analysis of population wide dietary preference through the lens of log gathered by a widely distributed web browser add on using the access volume of recipe that user seek via search a a proxy for actual food consumption we discover that variation in dietary preference a expressed via recipe access ha two main periodic component one yearly and the other weekly and that there exist characteristic regional difference in term of diet within the united state in a second study we identify user who show evidence of having made an acute decision to lose weight we characterize the shift in interest that they express in their search query and focus on change in their recipe query in particular last we correlate nutritional time series obtained from recipe query with time aligned data on hospital admission aimed at understanding how behavioral data captured in web log might be harnessed to identify potential relationship between diet and acute health problem in this preliminary study we focus on pattern of sodium identified in recipe over time and pattern of admission for congestive heart failure a chronic illness that can be exacerbated by increase in sodium intake 
clinical information retrieval ir present several challenge including terminology mismatch and granularity mismatch one of the main objective in clinical ir is to fill the semantic gap among the query and document and go beyond keywords matching to address these issue in this paper we attempt to use semantic information to improve the performance of clinical ir system by representing query in an expressive and meaningful context to model a query context initially we model and develop query domain ontology the query domain ontology represents concept closely related with query concept query context represents concept extracted from query domain ontology and weighted according to their semantic relatedness to query concept s the query context is then exploited in query expansion and patient record re ranking for improving clinical retrieval performance we evaluate our approach on the trec medical record dataset result show that our proposed approach significantly improves the retrieval performance compare to classic keyword based ir model 
the web provides a wealth of information about medical symptom and disorder although this content is often valuable to consumer study have found that interaction with web content may heighten anxiety and stimulate healthcare utilization we present a longitudinal log based study of medical search and browsing behavior on the web we characterize how user focus on particular medical concern and how concern persist and influence future behavior including change in focus of attention in searching and browsing for health information we build and evaluate model that predict transition from search on symptom to search on health condition and escalation from symptom to serious illness we study the influence that the prior onset of concern may have on future behavior including sudden shift back to searching on the concern amidst other search our finding have implication for refining web search and retrieval to support people pursuing diagnostic information 
this paper us the url word breaking task a an example to elaborate what we identify a crucial in designing statistical natural language processing nlp algorithm for web scale application rudimentary multilingual capability to cope with the global nature of the web multi style modeling to handle diverse language style seen in the web content fast adaptation to keep pace with the dynamic change of the web minimal heuristic assumption for generalizability and robustness and possibility of efficient implementation and minimal manual effort for processing massive amount of data at a reasonable cost we first show that the state of the art word breaking technique can be unified and generalized under the bayesian minimum risk bmr framework that using a web scale n gram can meet the first three requirement we discus how the existing technique can be viewed a introducing additional assumption to the basic bmr framework and describe a generic yet efficient implementation called word synchronous beam search testing the framework and it implementation on a series of large scale experiment reveals the following first the language style used to build the model play a critical role in the word breaking task and the most suitable for the url word breaking task appears to be that of the document title where the best performance is obtained model created from other language style such a from document body anchor text and even query exhibit varying degree of mismatch although all style benefit from increasing modeling power which in our experiment corresponds to the use of a higher order n gram the gain is most recognizable for the title model the heuristic proposed by the prior art do contribute to the word breaking performance for mismatched or le powerful model but are le effective and in many case lead to poorer performance than the matched model with minimal assumption for the matched model based on document title an accuracy rate of can already be achieved using simple trigram without any heuristic 
in the era when facebook and twitter dominate the market for social medium google ha introduced google g and reported a significant growth in it size while others called it a ghost town this begs the question that whether g can really attract a significant number of connected and active user despite the dominance of facebook and twitter this paper tackle the above question by presenting a detailed characterization of g based on large scale measurement we identify the main component of g structure characterize the key feature of their user and their evolution over time we then conduct detailed analysis on the evolution of connectivity and activity among user in the largest connected component lcc of g structure and compare their characteristic with other major osns we show that despite the dramatic growth in the size of g the relative size of lcc ha been decreasing and it connectivity ha become le clustered while the aggregate user activity ha gradually increased only a very small fraction of user exhibit any type of activity to our knowledge our study offer the most comprehensive characterization of g based on the largest collected data set 
exploratory search is a complex iterative information seeking activity that involves running multiple query and finding and examining many document we designed a query preview control that visualizes the distribution of newly retrieved and re retrieved document prior to running the query when evaluating the preview control with a control condition we found effect on both people s information seeking behavior and improved retrieval performance people spent more time formulating a query and were more likely to explore search result more deeply retrieved a more diverse set of document and found more different relevant document when using the preview 
in this paper we study how the network of agent adopting a particular technology relates to the structure of the underlying network over which the technology adoption spread we develop a model and show that the network of agent adopting a particular technology may have characteristic that differ significantly from the social network of agent over which the technology spread for example the network induced by a cascade may have a heavy tailed degree distribution even if the original network doe not this provides evidence that online social network created by technology adoption over an underlying social network may look fundamentally different from social network and indicates that using data from many online social network may mislead u if we try to use it to directly infer the structure of social network our result provide an alternate explanation for certain property repeatedly observed in data set for example heavy tailed degree distribution network densification shrinking diameter and network community profile these property could be caused by a sort of sampling bias rather than by attribute of the underlying social structure by generating network using cascade over traditional network model that do not themselves contain these property we can nevertheless reliably produce network that contain all these property an opportunity for interesting future research is developing new method that correctly infer underlying network structure from data about a network that is generated via a cascade spread over the underlying network 
in this paper we study how to discover the evolution of topic over time in a time stamped document collection our approach is uniquely designed to capture the rich topology of topic evolution inherent in the corpus instead of characterizing the evolving topic at fixed time point we conceptually define a topic a a quantized unit of evolutionary change in content and discover topic with the time of their appearance in the corpus discovered topic are then connected to form a topic evolution graph using a measure derived from the underlying document network our approach allows inhomogeneous distribution of topic over time and doe not impose any topological restriction in topic evolution graph we evaluate our algorithm on the acm corpus the topic evolution graph obtained from the acm corpus provide an effective and concrete summary of the corpus with remarkably rich topology that are congruent to our background knowledge in a finer resolution the graph reveal concrete information about the corpus that were previously unknown to u suggesting the utility of our approach a a navigational tool for the corpus 
a significant portion of web search is performed in mobile setting we explore the link between user query on mobile device and their location and movement with a focus on interpreting query about address we find that user tend to have a primary location likely corresponding to home or workplace and that a user s location relative to this primary location systematically influence the pattern of address search we apply our finding to construct a statistical model that can predict with high accuracy whether a user will be soon observed at an address that had been recently retrieved via search such an ability to predict that a user will transition to a location can be harnessed for multiple us including provision of direction and traffic information the rendering of competitive advertising and guiding the opportunistic completion of pending task that can be accomplished en route to a target location 
in this work we describe the result of a large scale study on the effect of the distribution of label across the different grade of relevance in the training set on the performance of trained ranking function in a controlled experiment we generate a large number of training datasets wih different label distribution and employ three learning to rank algorithm over these datasets we investigate the effect of these distribution on the accuracy of obtained ranking function to give an insight into the manner training set should be constructed 
a point of interest poi is a focused geographic entity such a a landmark a school an historical building or a business point of interest are the basis for most of the data supporting location based application in this paper we propose to curate poi from online source by bootstrapping training data from web snippet seeded by poi gathered from social medium this large corpus is used to train a sequential tagger to recognize mention of poi in text using wikipedia data a the training data we can identify poi in free text with an accuracy that is better than the state of the art poi identifier in term of precision and better in term of recall we show that using foursquare and gowalla checkins a seed to bootstrap training data from web snippet we can improve precision between and and recall between and over the state of the art the name of a poi is not sufficient a the poi must also be associated with a set of geographic coordinate our method increase the number of poi that can be localized nearly three fold from to in a sample of with a median localization accuracy of le than one kilometer 
news reporting ha seen a shift toward fast paced online reporting in new source such a social medium web search engine that support a news vertical have historically relied upon article published by major newswire provider when serving news related query in this paper we investigate to what extent real time content from newswire blog twitter and wikipedia source are useful to return to the user in the current fast paced news search setting in particular we perform a detailed user study using the emerging medium of crowdsourcing to determine when and where integrating news related content from these various source can better serve the user s news need we sampled approximately news related search query using google trend and bitly data in real time for two time period for these query we have crowdsourced worker compare web search ranking for each with similar ranking integrating real time news content from source such a twitter or the blogosphere our result show that user exhibited a preference for ranking integrating newswire article for only half of our query indicating that relying solely on newswire provider for news related content is now insufficient moreover our result show that user preferred ranking that integrate tweet more often than those that integrate newswire article showing the potential of using social medium to better serve news query 
the problem we tackle in this work is given a present news event to generate a plausible future event that can be caused by the given event we present a new methodology for modeling and predicting such future news event using machine learning and data mining technique our pundit algorithm generalizes example of causality pair to infer a causality predictor to obtain precise labeled causality example we mine year of news article and apply semantic natural language modeling technique to title containing certain predefined causality pattern for generalization the model us a vast amount of world knowledge ontology mined from linkeddata containing datasets with approximately billion relation empirical evaluation on real news article show that our pundit algorithm reach a human level performance 
search query are often ambiguous and or underspecified to accomodate different user need search result diversification ha received attention in the past few year accordingly several new metric for evaluating diversification have been proposed but their property are little understood we compare the property of existing metric given the premise that query may have multiple intent the likelihood of each intent given a query is available and graded relevance assessment are available for each intent we compare a wide range of traditional and diversified ir metric after adding graded relevance assessment to the trec web track diversity task test collection which originally had binary relevance assessment our primary criterion is discriminative power which represents the reliability of a metric in an experiment our result show that diversified ir experiment with a given number of topic can be a reliable a traditional ir experiment with the same number of topic provided that the right metric are used moreover we compare the intuitiveness of diversified ir metric by closely examining the actual ranked list from trec we show that a family of metric called d measure have several advantage over other metric such a ndcg and intent aware metric 
in this paper we provide an overview of the msr bing web scale speller challenge of we describe the motivation and outline the algorithmic and engineering challenge posed by this activity the design and the evaluation method are also reviewed and the online resource that will remain publicly available to the community are also described the challenge will culminate in a workshop after the time of the writing where the top prize winner will publish their approach the main finding and the lesson learned will be summarized and shared in the industry track presentation accompanying this paper 
with the growing amount of information available online recommender system are starting to provide a viable alternative and complement to search engine in helping user to find object of interest method based on matrix factorization mf model are the state of the art in recommender system the input to mf is user feedback in the form of a rating matrix however user can be engaged in interaction with multiple type of entity across different context leading to multiple rating matrix in other word user can have interaction in a heterogeneous information network generally in a heterogeneous network entity from any two entity type can have interaction with a weight rating indicating the level of endorsement collective matrix factorization cmf ha been proposed to address the recommendation problem in heterogeneous network however a main issue with cmf is that entity share the same latent factor across different context this is particularly problematic in two case latent factor for entity that are cold start in a context will be learnt mainly based on the data from other context where these entity are not cold start and therefore the factor are not properly learned for the cold start context also if a context ha more data compared to another context then the dominant context will dominate the learning process for the latent factor for entity shared in these two context in this paper we propose a context dependent matrix factorization model heteromf that considers a general latent factor for entity of every entity type and context dependent latent factor for every context in which the entity are involved we learn a general latent factor for every entity and transfer matrix for every context to convert the general latent factor into a context dependent latent factor experiment on two real life datasets from epinions and flixster demonstrate that heteromf substantially outperforms cmf particularly for cold start entity and for context where interaction in one context are dominated by other context 
in this paper we discus how we can extend probabilistic topic model to analyze the relationship graph of popular social network data so that we can group or label the edge and node in the graph based on their topic similarity in particular we first apply the well known latent dirichlet allocation lda model and it existing variant to the graph labeling task and argue that the existing model do not handle popular node node with many incoming edge in the graph very well we then propose possible extension to this model to deal with popular node our experiment show that the proposed extension are very effective in labeling popular node showing significant improvement over the existing method our proposed method can be used for providing for instance more relevant friend recommendation within a social network 
today plenty of data is emerging from various city system beyond the classical web resource large amount of data are retrieved from sensor device social network governmental application or service network in such a diversity of information answering specific information need of city inhabitant requires holistic ir technique capable of harnessing different type of city data and turned it into actionable insight to answer different query this tutorial will present deep insight challenge opportunity and technique to make heterogeneous city data searchable and show how emerging ir technique model can be employed to retrieve relevant information for the citizen 
we propose a mathematical framework for query selection a a mechanism for reducing the cost of constructing information retrieval test collection in particular our mathematical formulation explicitly model the uncertainty in the retrieval effectiveness metric that is introduced by the absence of relevance judgment since the optimization problem is computationally intractable we devise an adaptive query selection algorithm referred to a adaptive that provides an approximate solution adaptive selects query iteratively and assumes that no relevance judgment are available for the query under consideration once a query is selected the associated relevance assessment are acquired and then used to aid the selection of subsequent query we demonstrate the effectiveness of the algorithm on two trec test collection a well a a test collection of an online search engine with query our experimental result show that the query chosen by adaptive produce reliable performance ranking of system the ranking is better correlated with the actual system ranking than the ranking produced by query that were selected using the considered baseline method 
fast nearest neighbor search is necessary for a variety of large scale web application such a information retrieval nearest neighbor classification and nearest neighbor regression recently a number of machine learning algorithm have been proposed for representing the data to be searched a short bit vector and then using hashing to do rapid search these algorithm have been limited in their applicability in that they are suited for only one type of task e g spectral hashing learns bit vector representation for retrieval but not say classification in this paper we present a unified approach to learning bit vector representation for many application that use nearest neighbor search the main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand this broadens the usefulness of bit vector representation to task beyond just conventional retrieval we propose a learning to rank formulation to learn the bit vector representation of the data lambdarank algorithm is used for learning a function that computes a task specific bit vector from an input data vector our approach outperforms state of the art nearest neighbor method on a number of real world text and image classification and retrieval datasets it is scalable and learns a bit representation on million training case in two day 
online social network osns have become a popular new vector for distributing malware and spam which we refer to a socware unlike email spam which is sent by spammer directly to intended victim socware cascade through osns a compromised user spread it to their friend in this paper we analyze data from the wall of roughly million facebook user over five month with the goal of developing a better understanding of socware cascade we study socware cascade to understand a their spatio temporal property b the underlying motivation and mechanism and c the social engineering trick used to con user first we identify an evolving trend in which cascade appear to be throttling their rate of growth to evade detection and thus lasting longer second our forensic investigation into the infrastructure that support these cascade show that surprisingly facebook seems to be inadvertently enabling most cascade of cascade are disseminated via facebook application at the same time we observe large group of synergistic facebook apps more than group of size or more that collaborate to support multiple cascade lastly we find that hacker rely on two social engineering trick in equal measure luring user with free product and appealing to user social curiosity to enable socware cascade our finding present several promising avenue towards reducing socware on facebook but also highlight associated challenge 
our meta dex software suite extract content and index text from a corpus of pdf file and generates a meta index that reference entry across an entire domain we provide tool to analyze the individual and integrated index and visualize entry and book within the meta index the suite is scalable to very large data set 
an object on the semantic web is likely to be denoted with multiple uris by different party object coreference resolution is to identify equivalent uris that denote the same object driven by the linking open data lod initiative million of uris have been explicitly linked with owl sameas statement but potentially coreferent one are still considerable existing approach address the problem mainly from two direction one is based upon equivalence inference mandated by owl semantics which find semantically coreferent uris but probably omits many potential one the other is via similarity computation between property value pair which is not always accurate enough in this paper we propose a self training approach for object coreference resolution on the semantic web which leverage the two class of approach to bridge the gap between semantically coreferent uris and potential candidate for an object uri we firstly establish a kernel that consists of semantically coreferent uris based on owl sameas inverse functional property and max cardinality and then extend such kernel iteratively in term of discriminative property value pair in the description of uris in particular the discriminability is learnt with a statistical measurement which not only exploit key characteristic for representing an object but also take into account the matchability between property from pragmatic in addition frequent property combination are mined to improve the accuracy of the resolution we implement a scalable system and demonstrate that our approach achieves good precision and recall for resolving object coreference on both benchmark and large scale datasets 
a the volume of ai problem involving human knowledge are likely to soar crowdsourcing ha become essential in a wide range of world wide web application one of the biggest challenge of crowdsourcing is aggregating the answer collected from crowd worker and thus many aggregate technique have been proposed however given a new application it is difficult for user to choose the best suited technique a well a appropriate parameter value since each of these technique ha distinct performance characteristic depending on various factor e g worker expertise question difficulty in this paper we develop a benchmarking tool that allows to i simulate the crowd and ii evaluate aggregate technique in different aspect accuracy sensitivity to spammer etc we believe that this tool will be able to serve a a practical guideline for both researcher and software developer while researcher can use our tool to ass existing or new technique developer can reuse it component to reduce the development complexity 
latent dirichlet allocation lda is a popular topic modeling technique for exploring document collection because of the increasing prevalence of large datasets there is a need to improve the scalability of inference for lda in this paper we introduce a novel and flexible large scale topic modeling package in mapreduce mr lda a opposed to other technique which use gibbs sampling our proposed framework us variational inference which easily fit into a distributed environment more importantly this variational implementation unlike highly tuned and specialized implementation based on gibbs sampling is easily extensible we demonstrate two extension of the model possible with this scalable framework informed prior to guide topic discovery and extracting topic from a multilingual corpus we compare the scalability of mr lda against mahout an existing large scale topic modeling package mr lda out performs mahout both in execution speed and held out likelihood 
we develop and make publicly available an entity search test collection based on the dbpedia knowledge base this includes a large number of query and corresponding relevance judgment from previous benchmarking campaign covering a broad range of information need ranging from short keyword query to natural language question further we present baseline result for this collection with a set of retrieval model based on language modeling and bm finally we perform an initial analysis to shed light on certain characteristic that make this data set particularly challenging 
the continuing increase in the volume of information available in our daily life is creating ever greater challenge for people to find personally useful information one approach used to addressing this problem is personalized information retrieval pir pir system collect a user s personal information from both implicit and explicit source to build a user profile with the objective of giving retrieval result which better meet their individual user information need than a standard information retrieval ir system however in many situation there may be no opportunity to learn about the specific interest of a user and build a personal model when this user is querying on a new topic e g when a user visit a museum or exhibition which is unrelated to their normal search interest under this condition the experience and behaviour of other previous user who have made similar query could be used to build a model of user behavior in this domain my phd proposes to focus on the development of new and innovative method of domain specific ir my work seek to combine recommender algorithm trained using previous search behaviour from different searcher with a standard ranked ir method to form a domain specific ir model to improve the search effectiveness for a user entering a query without personal prior search history on this topic the challenge for my work are how to provide user better result how to train and evaluate the method proposed in my work 
with the rapid growing of web people spend more time on social network such a facebook and twitter in order to know the people they are interacting with finding the web appearance of them will help the social network user to a great extent we propose a novel and effective latent factor model to find web appearance of target social network user our method solves the name ambiguity problem by simultaneously exploring the link structure of social network and the web experiment on real world data show the superiority of our method over several baseline 
in this poster we address the problem of location disambiguation for geotagged web photo resource we propose an approach for analyzing and partitioning large geotagged photo collection using geographic and semantic information by organizing the dataset in a structural scheme we resolve the location ambiguity and clutter problem yield by massive volume of geotagged photo 
ranking function performance reached a plateau in the reason for this is investigated first the performance of bm is measured a the proportion of query satisfied on the first page of result it performs well the performance is then compared to human performance they perform comparably the conclusion is there isn t much room for ranking function improvement 
the bag of visual word bovw paradigm is fast becoming a popular image representation for content based image retrieval cbir mainly because of it better retrieval effectiveness over global feature representation on collection with image being near duplicate to query in this experimental study we demonstrate that this advantage of bovw is diminished when visual diversity is enhanced by using a secondary modality such a text to pre filter image the top surf descriptor is evaluated against compact composite descriptor on a two stage image retrieval setup which first us a text modality to rank the collection and then perform cbir only on the top k item 
nowadays more and more people tend to make decision based on the opinion information from the internet in addition to recommendation from offline friend or parent for example we may browse the resume and comment on election candidate to determine if one candidate is qualified or consult the consumer report or review on special e commercial website to decide which brand of computer is suitable for one s need though opinion information is rich on the internet point out that of american internet user deem that online information is irretrievable confusing or conflicting with each other early work on opinion mining help to classify opinion polarity to extract specific opinion and to summarize opinion text however all these work are usually based on plain text review comment or news article with the explosion of web application especially social network application like blog discussion forum micro blog the massive individual user go to the major medium website which lead to much more opinion material posted on the internet by user shared experience or view these opinion rich and social network based application bring new perspective for opinion mining a well first in addition to plain text review newswire in traditional opinion mining we see new type of cyber based text like personal diary blog cyber sm tweet second if we regard the opinion in plain text a static the dynamic change of opinion in the social network is a new promising area and catch increasing attention of worldwide researcher in the social network the opinion held by one individual is not static but change which can be influenced by others a serial of change among different user form the opinion propagation or diffusion in the network this paper and my doctoral work focus on the opinion influence and diffusion in the social network which explore the detailed process of one to one influence and the opinion diffusion process in the social network the significance of this work is it can benefit many other related research like information maximum viral marketing now some pioneering work have been conducted to investigate the role of social network in information diffusion and influencers in the social network these work are usually based on information diffusion model like the cascade model cm or epidemic model em however we argue that it is not enough to simply apply these model to opinion influence and diffusion for both cm and em status shift is along specific direction from inactive to active cm or from susceptible to infectious and then to recovered em but opinion influence is more complex 
online content rating service allow user to find and share content ranging from news article digg to video youtube to business yelp generally these site allow user to create account declare friendship upload and rate content and locate new content by leveraging the aggregated rating of others these service are becoming increasingly popular yelp alone ha over million review unfortunately this popularity is leading to increasing level of malicious activity including multiple identity sybil attack and the buying of rating from user in this paper we present iolaus a system that leverage the underlying social network of online content rating system to defend against such attack iolaus us two novel technique a weighing rating to defend against multiple identity attack and b relative rating to mitigate the effect of bought rating an evaluation of iolaus using microbenchmarks synthetic data and real world content rating data demonstrates that iolaus is able to outperform existing approach and serve a a practical defense against multiple identity and rating buying attack 
moboq is a location based real time social question answering service deployed in the field in china using moboq people can ask temporal and geo sensitive question such a how long is the line at a popular business right now and then receive answer that crowdsourced from other user in a timely fashion to obtain answer for question the system analyzes the live stream from public microblogging service sina weibo to identify people who are likely to currently be at the place that is associated with a question and sends them the unsolicited question through the microblogging service from which they were identified moboq wa deployed in china at the beginning of until october of the same year it wa used to ask question by registered user and it gathered answer of the question received at least one answer received a first response within minute and of the question got first answer within minute in total of the question successfully found at least one answer candidate and they were sent to microblogging service user we analyze the usage pattern and behavior of the real world end user discus the lesson learned and outline the future direction and possible application that could be built on top of moboq 
the twitter real time information network is the subject of research for information retrieval task such a real time search however so far reproducible experimentation on twitter data ha been impeded by restriction imposed by the twitter term of service in this paper we detail a new methodology for legally building and distributing twitter corpus developed through collaboration between the text retrieval conference trec and twitter in particular we detail how the first publicly available twitter corpus referred to a tweet wa distributed via list of tweet identifier and specialist tweet crawling software furthermore we analyse whether this distribution approach remains robust over time a tweet in the corpus are removed either by user or twitter itself tweet wa successfully used by participating group for the trec microblog track while our result attest to the robustness of the crawling methodology over time 
internet search engine typically compute a relevance score for webpage given the query term and then rank the page by decreasing relevance score the popular search engine do not however present the relevance score that were computed during this process we suggest that these relevance score may contain information that can help user make conscious decision in this paper we evaluate in a user study how user react to the display of such score the result indicate that user understand graphical display of relevance and make decision based on these score our result suggest that in the context of exploratory search relevance score may cause user to explore more search result 
the explosion of social medium service present a great opportunity to understand the sentiment of the public via analyzing it large scale and opinion rich data in social medium it is easy to amass vast quantity of unlabeled data but very costly to obtain sentiment label which make unsupervised sentiment analysis essential for various application it is challenging for traditional lexicon based unsupervised method due to the fact that expression in social medium are unstructured informal and fast evolving emoticon and product rating are example of emotional signal that are associated with sentiment expressed in post or word inspired by the wide availability of emotional signal in social medium we propose to study the problem of unsupervised sentiment analysis with emotional signal in particular we investigate whether the signal can potentially help sentiment analysis by providing a unified way to model two main category of emotional signal i e emotion indication and emotion correlation we further incorporate the signal into an unsupervised learning framework for sentiment analysis in the experiment we compare the proposed framework with the state of the art method on two twitter datasets and empirically evaluate our proposed framework to gain a deep understanding of the effect of emotional signal 
the distribution of password chosen by user ha implication for site security password handling algorithm and even how user are permitted to select password using password list from four different web site we investigate if zipf s law is a good description of the frequency with which password are chosen we use a number of standard statistic which measure the security of password distribution to see if modelling the data using a simple distribution is effective we then consider how much the password distribution from each site have in common using password cracking a a metric this show that these distribution have enough high frequency password in common to provide effective speed ups for cracking password finally a an alternative to a deterministic banned list we will show how to stochastically shape the distribution of password by occasionally asking user to choose a different password 
in medical record negative qualifier e g no or without are commonly used by health practitioner to identify the absence of a medical condition without considering whether the term occurs in a negative or positive context the sole presence of a query term in a medical record is insufficient to imply that the record is relevant to the query in this paper we show how to effectively handle such negation within a medical record information retrieval system in particular we propose a term representation that tackle negated language in medical record which is further extended by considering the dependence of negated query term we evaluate our negation handling technique within the search task provided by the trec medical record track our result which show a significant improvement upon a system that doe not consider negated context within record attest the importance of handling negation 
the development of web technology ha led to huge economic benefit and challenge for both e commerce website and online shopper one core technology to increase sale and consumer satisfaction is the use of recommender system existing product recommender system consider the order of item purchased by user to obtain a list of recommended item however they do not consider the time interval between the product purchased for example there is often an interval of month between the purchase of printer ink cartridge or refill thus recommending appropriate ink cartridge one week before the user need to replace the depleted ink cartridge would increase the likelihood of a purchase decision in this paper we propose to utilize the purchase interval information to improve the performance of the recommender system for e commerce we design an efficient algorithm to compute the purchase interval between product pair from user purchase history and integrate this information into the marginal utility model we evaluate our approach on a real world ecommerce dataset experimental result demonstrate that our approach significantly improves the conversion rate and temporal diversity compared to state of the art algorithm 
cloud computing is the latest computing paradigm that delivers hardware and software resource a virtualized service in which user are free from the burden of worrying about the low level system administration detail migrating web application to cloud service and integrating cloud service into existing computing infrastructure is non trivial it lead to new challenge that often require innovation of paradigm and practice at all level technical cultural legal regulatory and social the key problem in mapping web application to virtualized cloud service is selecting the best and compatible mix of software image e g web server image and infrastructure service to ensure that quality of service qos target of an application are achieved the fact that when selecting cloud service engineer must consider heterogeneous set of criterion and complex dependency between infrastructure service and software image which are impossible to resolve manually is a critical issue to overcome these challenge we present a framework called cloudgenius which automates the decision making process based on a model and factor specifically for web server migration to the cloud cloudgenius leverage a well known multi criterion decision making technique called analytic hierarchy process to automate the selection process based on a model factor and qos parameter related to an application an example application demonstrates the applicability of the theoretical cloudgenius approach moreover we present an implementation of cloudgenius that ha been validated through experiment 
expert search ha made rapid progress in modeling algorithm and evaluation in the recent year however there is very few work on analyzing how user interact with expert search system in this paper we conduct analysis of an expert search query log the aim is to understand the special characteristic of expert search usage to the best of our knowledge this is one of the earliest work on expert search query log analysis we find that expert search user generally issue shorter query more common query and use more advanced search feature with fewer query in a session than general web search user do this study explores a new research direction in expert search by analyzing and exploiting query log 
wikipedia ha become one of the most important source of information available all over the world however the categorization of wikipedia article is not standardized and the search are mainly performed on keywords rather than concept in this paper we present an application that build a hierarchical structure to organize all wikipedia entry so that medical article can be reached from general to particular using the well known medical subject heading mesh thesaurus moreover the language link between article will allow using the directory created in different language the final system can be packed and ported to mobile device a a standalone offline application 
we present a model of competition between web search algorithm and study the impact of such competition on user welfare in our model search provider compete for customer by strategically selecting which search result to display in response to user query customer in turn have private preference over search result and will tend to use search engine that are more likely to display page satisfying their demand our main question is whether competition between search engine increase the overall welfare of the user i e the likelihood that a user find a page of interest when search engine derive utility only from customer to whom they show relevant result we show that they differentiate their result and every equilibrium of the resulting game achieves at least half of the welfare that could be obtained by a social planner this bound also applies whenever the likelihood of selecting a given engine is a convex function of the probability that a user s demand will be satisfied which includes natural markovian model of user behavior on the other hand when search engine derive utility from all customer independent of search result relevance and the customer demand function are not convex there are instance in which the unique equilibrium involves no differentiation between engine and a high degree of randomness in search result this can degrade social welfare by a factor of numpages relative to the social optimum where numpages is the number of webpage these bad equilibrium persist even when search engine can extract only small but non zero expected revenue from dissatisfied user and much higher revenue from satisfied one 
we introduce an entity centric search experience called active object in which entity bearing query are paired with action that can be performed on the entity for example given a query for a specific flashlight we aim to present action such a reading review watching demo video and finding the best price online in an annotation study conducted over a random sample of user query session we found that a large proportion of query in query log involve action on entity calling for an automatic approach to identifying relevant action for entity bearing query in this paper we pose the problem of finding action that can be performed on entity a the problem of probabilistic inference in a graphical model that capture how an entity bearing query is generated we design model of increasing complexity that capture latent factor such a entity type and intended action that determine how a user writes a query in a search box and the url that they click on given a large collection of real world query and click from a commercial search engine the model are learned efficiently through maximum likelihood estimation using an em algorithm given a new query probabilistic inference enables recommendation of a set of pertinent action and host we propose an evaluation methodology for measuring the relevance of our recommended action and show empirical evidence of the quality and the diversity of the discovered action 
existing community question answering forum usually provide only textual answer however for many question pure text cannot provide intuitive information while image or video content are more appropriate in this paper we introduce a scheme that is able to enrich text answer with image and video information our scheme investigates a rich set of technique including question answer classification query generation image and video search reranking etc given a question and the community contributed answer our approach is able to determine which type of medium information should be added and then automatically collect data from internet to enrich the textual answer different from some effort that attempt to directly answer question with image and video data our approach is built based on the community contributed textual answer and thus it is more feasible and able to deal with more complex question we have conducted empirical study on more than qa pair and the result demonstrate the effectiveness of our approach 
we propose a method for finding impressive creator in online social network site sn many user are actively engaged in publishing their own work sharing visual content on site such a youtube or flickr in this paper we focus on the japanese illustration sharing sn pixiv we implement an illustrator search system based on user impression category the impression of illustrator are estimated from clue in the crowdsourced social tag annotation on their illustration we evaluated our system in term of normalized discounted cumulative gain and found that using feedback on motif and impression for illustration of relevant illustrator improved illustrator search by 
we demonstrate rdf xpress a search engine that enables user to effectively retrieve information from large rdf knowledge base or linked data source rdf xpress provides a search interface where user can combine triple pattern with keywords to form query moreover rdf xpress support automatic query relaxation and return a ranked list of diverse query result 
a dynamic pruning strategy such a wand enhances retrieval efficiency without degrading effectiveness to a given rank k known a safe to rank k however it is also possible for wand to obtain more efficient but unsafe retrieval without actually significantly degrading effectiveness on the other hand in a modern search engine setting dynamic pruning strategy can be used to efficiently obtain the set of document to be re ranked by the application of a learned model in a learning to rank setting no work ha examined the impact of safeness on the effectiveness of the learned model in this work we investigate the impact of wand safeness through experiment using trec web track topic we find that unsafe wand is biased towards document with lower docids thereby impacting effectiveness 
the generalized second price gsp auction is the primary auction used for selling sponsored search advertisement in this paper we consider the revenue of this auction at equilibrium we prove that if agent value are drawn from identical regular distribution then the gsp auction paired with an appropriate reserve price generates a constant fraction th of the optimal revenue in the full information game we show that at any nash equilibrium of the gsp auction obtains at least half of the revenue of the vcg mechanism excluding the payment of a single participant this bound hold also with any reserve price and is tight finally we consider the tradeoff between maximizing revenue and social welfare we introduce a natural convexity assumption on the click through rate and show that it implies that the revenue maximizing equilibrium of gsp in the full information model will necessarily be envy free in particular it is always possible to maximize revenue and social welfare simultaneously when click through rate are convex without this convexity assumption however we demonstrate that revenue may be maximized at a non envy free equilibrium that generates a socially inefficient allocation 
it is typically expected that when people work together they can often accomplish goal that are difficult or even impossible for individual we consider this notion of the group achieving more than the sum of all individual achievement to be the synergic effect in collaboration similar expectation exists for people working in collaboration for information seeking task we however lack a methodology and appropriate evaluation metric for studying and measuring the synergic effect in this paper we demonstrate how to evaluate this effect and discus what it mean to various collaborative information seeking ci situation we present a user study with four different condition single user pair of user at the same computer pair of user at different computer and co located and pair of user remotely located each of these individual or pair wa given the same task of information seeking and usage for the same amount of time we then combined the output of single independent user to form artificial pair and compared against the real pair not surprisingly participant using different computer co located or remotely located were able to cover more information source than those using a single computer single user or a pair but more interestingly we found that real pair with their own computer co located or remotely located were able to cover more unique and useful information than that of the artificially created pair this indicates that those working in collaboration achieved something greater and better than what could be achieved by adding independent user thus demonstrating the synergic effect remotely located real team were also able to formulate a wider range of query than those pair that were co located or artificially created this show that the collaborator working remotely were able to achieve synergy while still being able to think and work independently through the experiment and measurement presented here we have also contributed a unique methodology and an evaluation metric for ci 
a highly structured document with rich metadata such a product movie etc become increasingly prevalent searching those document ha become an important ir problem unfortunately existing work on document summarization especially in the context of search ha been mainly focused on unstructured document and little attention ha been paid to highly structured document due to the different characteristic of structured and unstructured document the ideal approach for document summarization might be different in this paper we study the problem of summarizing highly structured document in a search context we propose a new summarization approach based on query specific facet selection our approach aim to discover the important facet hidden behind a query using a machine learning approach and summarizes retrieved document based on those important facet in addition we propose to evaluate summarization approach based on a utility function that measure how well the summary assist user in interacting with the search result furthermore we develop a game on mechanical turk to evaluate different summarization approach the experimental result show that the new summarization approach significantly outperforms two existing one 
in modern search engine an increasing number of search result page serps are federated from multiple specialized search engine called vertical such a image or video a an effective approach to interpret user click through behavior a feedback information most click model were designed to reduce the position bias and improve ranking performance of ordinary search result which have homogeneous appearance however when vertical result are combined with ordinary one significant difference in presentation may lead to user behavior bias and thus failure of state of the art click model with the help of a popular commercial search engine in china we collected a large scale log data set which contains behavior information on both vertical and ordinary result we also performed eye tracking analysis to study user s real world examining behavior according these analysis we found that different result appearance may cause different behavior bias both for vertical result local effect and for the whole result list global effect these bias include examine bias for vertical result especially those with multimedia component trust bias for result list with vertical result and a higher probability of result revisitation for vertical result based on these finding a novel click model considering these bias besides position bias wa constructed to describe interaction with serps containing vertical experimental result show that the new vertical aware click model vcm is better at interpreting user click behavior on federated search in term of both log likelihood and perplexity than existing model 
with the growing pervasiveness of the internet online search for product and service is constantly increasing most product search engine are based on adaptation of theoretical model devised for information retrieval however the decision mechanism that underlies the process of buying a product is different than the process of locating relevant document or object we propose a theory model for product search based on expected utility theory from economics specifically we propose a ranking technique in which we rank highest the product that generate the highest surplus after the purchase in a sense the top ranked product are the best value for money for a specific user our approach build on research on demand estimation from economics and present a solid theoretical foundation on which further research can build on we build algorithm that take into account consumer demographic heterogeneity of consumer preference and also account for the varying price of the product we show how to achieve this without knowing the demographic or purchasing history of individual consumer but by using aggregate demand data we evaluate our work by applying the technique on hotel search our extensive user study using more than user provided ranking comparison demonstrate an overwhelming preference for the ranking generated by our technique compared to a large number of existing strong state of the art baseline 
traditionally collaborative filtering assumes that similar user have similar response to similar item however human activity exhibit heterogenous feature across multiple domain such that user own similar taste in one domain may behave quite differently in other domain moreover highly sparse data present crucial challenge in preference prediction intuitively if user interested domain are captured first the recommender system is more likely to provide the enjoyed item while filter out those uninterested one therefore it is necessary to learn preference profile from the correlated domain instead of the entire user item matrix in this paper we propose a unified framework toprec which detects topical community to construct interpretable domain for domain specific collaborative filtering in order to mine community a well a the corresponding topic a semi supervised probabilistic topic model is utilized by integrating user guidance with social network experimental result on real world data from epinions and ciao demonstrate the effectiveness of the proposed framework 
collaborative filtering based recommendation algorithm have achieved widespread success on the web but little work ha been performed to investigate appropriate user item relationship structure of rating matrix this paper present a novel and general collaborative filtering framework based on approximate bordered block diagonal form structure of user item rating matrix we show formally that matrix in a bbdf structure correspond to community detection on the corresponding bipartite graph and they reveal relationship among user and item intuitionally in recommendation task by this framework general and special interest of a user are distinguished which help to improve prediction accuracy in collaborative filtering task experimental result on four real world datasets including the yahoo music dataset which is currently the largest show that the proposed framework help many traditional collaborative filtering algorithm such a user based item based svd and nmf approach to make more accurate rating prediction moreover by leveraging smaller and denser submatrices to make prediction this framework contributes to the scalability of recommender system 
web based advertising and electronic commerce combined with the key role of search engine in driving visitor to ad monetized and e commerce web site ha given rise to the phenomenon of web spam web page that are of little value to visitor but that are created mainly to mislead search engine into driving traffic to target web site a large fraction of spam web page is automatically generated and some portion of these page is generated by stitching together part sentence or paragraph of other web page this paper present a scalable algorithm for detecting such quilted web page previous work by the author and his collaborator introduced a sampling based algorithm that wa capable of detecting some but by far not all quilted web page in a collection by contrast the algorithm presented in this work identifies all quilted web page and it is scalable to very large corpus we tested the algorithm on the half billion page english language subset of the clueweb collection and evaluated it effectiveness in detecting web spam by manually inspecting small sample of the detected quilted page this manual inspection guided u in iteratively refining the algorithm to be more efficient in detecting real world spam 
this short paper proposes a method to classify music video clip uploaded to a video sharing service into music mood category such a cheerful wistful and aggressive the method leverage viewer comment posted to the music video clip for the music mood classification it extract specific feature from the comment adjective in comment lengthened word in comment and comment in chorus section our experimental result classifying video clip into six mood category showed that our method outperformed the baseline in term of macro and micro averaged f measure in addition our method outperformed the existing approach that utilize lyric and audio signal of song 
we investigate interpreting coordination e g word sequence connected with coordinating conjunction such a and and or a logical disjunction of term to generate a set of disjunctionfree query variant for information retrieval ir query in addition so called hyphen coordination are resolved by generating full compound form and rephrasing the original query e g rice im and export is transformed into rice import and export query variant are then processed separately and retrieval result are merged using a standard data fusion technique we evaluate the approach on german standard ir benchmarking data the result show that i our proposed approach to generate compound from hyphen coordination produce the correct result for all test topic ii our proposed heuristic to identify coordination and generate query variant based on shallow natural language processing nlp technique is highly accurate on the topic and doe not rely on parsing or part of speech tagging iii using query variant to produce multiple retrieval result and merging the result decrease precision at top rank however in combination with blind relevance feedback brf this approach can show significant improvement over the standard brf baseline using the original query 
user engagement in search refers to the frequency for user re using the search engine to accomplish their task among factor that affected user visit frequency relevance of search result is believed to play a pivotal role while multiple work in the past ha demonstrated the correlation between search success and user engagement based on longitudinal analysis we examine this problem from a different perspective in this work specifically we carefully designed a large scale controlled experiment on user of a large commercial web search engine in which user were separated into control and treatment group where user in treatment group were presented with search result which are deliberate degraded in relevance we studied user response to the relevance degradation through tracking several behavioral metric such a query per user click per session over an extended period of time both during and following the experiment by quantifying the relationship between user engagement and search relevance we observe significant difference between user s short term search behavior and long term engagement change by leveraging some of the key finding from the experiment we developed a machine learning model to predict the long term impact of relevance degradation on user engagement overall our model achieves over of accuracy in predicting user engagement drop besides our model is also capable of predicting engagement change for low frequency user with very few user signal we believe that insight from this study can be leveraged by search engine company to detect and intervene search relevance degradation and to prevent long term user engagement drop 
previous research ha suggested the permutation test a the theoretically optimal statistical significance test for ir evaluation and advocated for the discontinuation of the wilcoxon and sign test we present a large scale study comprising nearly million system comparison showing that in practice the bootstrap t test and wilcoxon test outperform the permutation test under different optimality criterion we also show that actual error rate seem to be lower than the theoretically expected further confirming that we may actually be underestimating significance 
from a mute but eloquent alphabet of character emerges a complex biological literature whose highest expression is human existence the rapidly advancing technology of nextgen sequencing will soon make it possible to inexpensively acquire and store the character of our complete personal genetic instruction set and make it available for health assessment and disease management this uniquely personal form of big data brings with it challenge that will be discussed in this keynote presentation topic will include a brief introduction to the linguistic challenge of biology a literature the impact of personal molecular variation on traditional approach to disease prevention diagnosis and treatment and the challenge of information retrieval when a large volume of primary observation is made that is associated with an evanescent and rapidly changing corpus of scientific interpretation of those primary observation experience with extracting high quality pheonotypes from electronic medical record ha shown that natural language processing capability is an essential information extraction function for correlation of clinical event with personal genetic variation any powerful set of information can be used or misused and put those who depend upon it in jeopardy these issue and a lesson from the long running jeopardy tv series will be discussed 
web search is an interactive process that involves action from web search user and response from the search engine many research effort have been made to address the problem of understanding search behavior in general some of this work focused on predicting whether a particular user ha succeeded in achieving her search goal or not most of these study have faced the problem of the lack of reliable labeled data to learn from unlike labeled data unlabeled data recording behavioral signal in web search is widely available in search log in this work we study the plausibility of using labeled and unlabeled data to learn better model of user behavior that can be used to predict search success more effectively we present a semi supervised approach to modeling web search satisfaction the proposed approach can use either labeled data only or both labeled and unlabeled data we show that the proposed model outperforms previous method for modeling search success using labeled data we also show that adding unlabeled data improves the effectiveness of the proposed model and that the proposed method outperforms other strong semi supervised baseline 
traditional web search engine do not use the image in the html page to find relevant document for a given query instead they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page in this paper we study whether the content of the picture appearing in a web page can be used to enrich the semantic description of an html document and consequently boost the performance of a keyword based search engine we present a web scalable system that exploit a pure text based search engine to find an initial set of candidate document for a given query then the candidate set is reranked using semantic information extracted from the image contained in the page the resulting system retains the computational efficiency of traditional text based search engine with only a small additional storage cost needed to encode the visual information we test our approach on the trec million query track where we show that our use of visual content yield improvement in accuracy for two distinct text based search engine including the system with the best reported performance on this benchmark 
most popular ir metric are parameterized usually parameter of these metric are chosen on the basis of general consideration and not adjusted by experiment with real user particularly the parameter of the expected reciprocal rank measure are the normalized parameter of the dcg metric and the latter are chosen in an ad hoc manner we suggest an approach for adjusting parameter of the err metric that allows to reach maximum agreement with the real user behavior more exactly we optimized the parameter by maximizing pearson weighted correlation between err and several online click metric for each click metric we managed to find the parameter of err that result into it higher correlation with the given online click metric 
amazon cloudsearch is a new hosted search service built on top of many cloud based aws service and based on the same technology that power search on amazon s retail site because of it ease of configuration and scalability cloudsearch represents the next step in the democratization of information retrieval this democratization process increasing access to search for both end user and potential search provider ha continued over several decade through technology like early online metered search service enterprise search software web search and open source search tool cloudsearch further reduces barrier to entry allowing a person or organization to basically say make my content searchable and have it happen automatically cloudsearch may also offer an opportunity to overcome the stagnation that ha occurred in search user experience over the past year when you no longer need to be a search expert to make your content available you re not stuck with ten blue link instead you can focus on providing the kind of interaction that make sense for your application and your user cloudsearch enables a flowering of search application that need not be tied to the web and an opportunity to explore new way of interacting with information retrieval technology 
we consider an interactive information retrieval task in which the user is interested in finding several to many relevant document with minimal effort given an initial document ranking user interaction with the system produce relevance feedback rf which the system then us to revise the ranking this interactive process repeat until the user terminates the search to maximize accuracy relative to user effort we propose an active learning strategy at each iteration the document whose relevance is maximally uncertain to the system is slotted high into the ranking in order to obtain user feedback for it simulated feedback on the robust trec collection show our active learning approach dominates several standard rf baseline relative to the amount of feedback provided by the user evaluation on robust under noisy feedback and on letor collection further demonstrate the effectiveness of active learning a well a value of negative feedback in this task scenario 
until recently the lack of user activity on search result wa perceived a a sign of user dissatisfaction from retrieval performance however recent study have reported that some query might not be followed by click to the content of the retrieved result because the search task can be satisfied in the list of retrieved result the user view without the need to click through them in this paper we propose a method for evaluating user satisfaction from the result of search that are not followed by clickthrough activity to the retrieved result we found that there is a strong association between some implicit measure of user activity and user s explicit satisfaction judgment moreover we developed a predictive model of user satisfaction based on implicit measure achieving accuracy up to 
personalization of search result offer the potential for significant improvement in information retrieval performance user interaction with the system and document during information seeking session provide a wealth of information about user preference and their task goal in this paper we propose method for analyzing and modeling user search behavior in search session to predict document usefulness and then using information to personalize search result we generate prediction model of document usefulness from behavior data collected in a controlled lab experiment with participant each completing uncontrolled searching for task in the web the generated model are then tested with another data set of user search session in radically different search task and constrains the document predicted useful and not useful by the model are used to modify the query in each search session using a standard relevance feedback technique the result show that application of the model led to consistently improved performance over a baseline that did not take account of user interaction information these finding have implication for designing system for personalized search and improving user search experience 
an emerging need in information retrieval is to identify a set of document conforming to an abstract description this task present two major challenge to existing method of document retrieval and classification first similarity based on overall content is le effective because there may be great variance in both content and subject of document produced for similar function e g a presidential speech or a government ministry white paper second the function of the document can be defined based on user interest or the specific data set through a set of existing example which cannot be described with standard category additionally the increasing volume and complexity of document collection demand new scalable computational solution we conducted a case study using web archived data from the latin american government document archive lagda to illustrate these problem and challenge we propose a new hybrid approach based on na ve bayes inference that us mixed n gram model obtained from a training set to classify document in the corpus the approach ha been developed to exploit parallel processing for large scale data set the preliminary work show promising result with improved accuracy for this type of retrieval problem 
uncovering the nature of the connection between a set of entity e g passenger on a flight and organization on a watchlist can be viewed a a multi source multi destination msmd path query problem on labeled graph data model such a rdf using existing graph navigational path finding technique to solve msmd problem will require query to be decomposed into multiple single source or destination path subqueries each of which is solved independently navigational technique on disk resident graph typically generate very poor i o access pattern for large disk resident graph and for msmd path query such poor access pattern may be repeated if common graph exploration step exist across subqueries in this paper we propose an optimization technique for general msmd path query that generalizes an efficient algebraic approach for solving a variety of single source path problem the generalization enables holistic evaluation of msmd path query without the need for query decomposition we present a conceptual framework for sharing computation in the algebraic framework that is based on suffix equivalence suffix equivalence amongst subqueries capture the fact that multiple subqueries with different prefix can share a suffix and a such share the computation of shared suffix which allows prefix path computation to share common suffix path computation this approach offer order of magnitude better performance than current existing technique a demonstrated by a comprehensive experimental evaluation over real and synthetic datasets 
recently researcher have shown interest in the use of preference judgment for evaluation in ir literature although preference judgment have several advantage over absolute judgment one of the major disadvantage is that the number of judgment needed increase polynomially a the number of document in the pool increase we propose a novel method using pagerank to minimize the number of judgment required to evaluate system using preference judgment we test the proposed hypothesis using the trec to terabyte dataset to show that it is possible to reduce the evaluation cost considerably further we study the susceptibility of the method due to assessor error 
the assumption of information seeker being independent and ir problem being individual ha been challenged often in the recent past with an argument that the next big leap in search and retrieval will come through incorporating social and collaborative aspect of information seeking this half day tutorial will introduce the student to theory methodology and tool that focus on information retrieval seeking in collaboration the student will have an opportunity to learn about the social aspect of ir with a focus on collaborative information seeking ci situation system and evaluation technique the course is intended for those interested in social and collaborative aspect of ir from both academia and industry and requires only a general understanding of ir system and evaluation 
distributed event are collection of event taking place within a small area over the same time period and relating to a single topic there are often a large number of event on offer and the time in which they can be visited are heavily constrained therefore the task of choosing event to visit and in which order can be very difficult in this work we investigate how visitor can be assisted by mean of a recommender system via large scale naturalistic study n and n we show that a recommender system can influence user to select event that result in tighter and more compact route thus allowing user to spend le time travelling and more time visiting event 
this work address the task of recommending relevant tag to a target object by jointly exploiting three dimension of the problem i term co occurrence with tag pre assigned to the target object ii term extracted from multiple textual feature and iii several metric of tag relevance in particular we propose several new heuristic method which extend state of the art strategy by including new metric that try to capture how accurately a candidate term describes the object s content we also exploit two learning to rank l r technique namely ranksvm and genetic programming for the task of generating ranking function that combine multiple metric to accurately estimate the relevance of a tag to a given object we evaluate all proposed method in various scenario for three popular web application namely lastfm youtube and yahoovideo we found that our new heuristic greatly outperform the method on which they are based producing gain in precision of up to a well a another state of the art technique with improvement in precision of up to over the best baseline in any scenario further improvement can also be achieved with the new l r strategy which have the additional advantage of being quite flexible and extensible to exploit other aspect of the tag recommendation problem 
session search is the information retrieval ir task that performs document retrieval for a search session during a session a user constantly modifies query in order to find relevant document that fulfill the information need this paper proposes a novel query change retrieval model qcm which utilizes syntactic editing change between adjacent query a well a the relationship between query change and previously retrieved document to enhance session search we propose to model session search a a markov decision process mdp we consider two agent in this mdp the user agent and the search engine agent the user agent s action are query change that we observe and the search agent s action are proposed in this paper experiment show that our approach is highly effective and outperforms top session search system in trec and 
widespread growth of open wireless hotspot ha made it easy to carry out man in the middle attack and impersonate web site although http can be used to prevent such attack it universal adoption is hindered by it performance cost and it inability to leverage caching at intermediate server such a cdn server and caching proxy while maintaining end to end security to complement http we revive an old idea from shttp a protocol that offer end to end web integrity without confidentiality we name the protocol httpi and give it an efficient design that is easy to deploy for today s web in particular we tackle several previously unidentified challenge such a supporting progressive page loading on the client s browser handling mixed content and defining access control policy among http httpi and http content from the same domain our prototyping and evaluation experience show that httpi incurs negligible performance overhead over http can leverage existing web infrastructure such a cdns or caching proxy without any modification to them and can make many of the mixed content problem in existing http web site easily go away based on this experience we advocate browser and web server vendor to adopt httpi 
recommender system associated with social network often use social explanation e g x y and friend like this to support the recommendation we present a study of the effect of these social explanation in a music recommendation context we start with an experiment with user in which we show explanation with varying level of social information and analyze their effect on user decision we distinguish between two key decision the likelihood of checking out the recommended artist and the actual rating of the artist based on listening to several song we find that while the explanation do have some influence on the likelihood there is little correlation between the likelihood and actual listening rating for the same artist based on these insight we present a generative probabilistic model that explains the interplay between explanation and background information on music preference and how that lead to a final likelihood rating for an artist acknowledging the impact of explanation we discus a general recommendation framework that model external informational element in the recommendation interface in addition to inherent preference of user 
a a tremendous number of mobile application apps are readily available user have difficulty in identifying apps that are relevant to their interest recommender system that depend on previous user rating i e collaborative filtering or cf can address this problem for apps that have sufficient rating from past user but for apps that are newly released cf doe not have any user rating to base recommendation on which lead to the cold start problem in this paper we describe a method that account for nascent information culled from twitter to provide relevant recommendation in such cold start situation we use twitter handle to access an app s twitter account and extract the id of their twitter follower we create pseudo document that contain the id of twitter user interested in an app and then apply latent dirichlet allocation to generate latent group at test time a target user seeking recommendation is mapped to these latent group by using the transitive relationship of latent group to apps we estimate the probability of the user liking the app we show that by incorporating information from twitter our approach overcomes the difficulty of cold start app recommendation and significantly outperforms other state of the art recommendation technique by up to 
intonow is a mobile application that provides a second screen experience to television viewer intonow us the microphone of the companion device to sample the audio coming from the tv set and compare it against a database of tv show in order to identify the program being watched the system we demonstrate is activated by intonow for specific type of show it retrieves information related to the program the user is watching by using closed caption which are provided by each broadcasting network along the tv signal it then match the stream of closed caption in real time against multiple source of content more specifically during news program it display link to online news article and the profile of people and organization in the news and during music show it display link to song the matching model are machine learned from editorial judgment and tuned to achieve approximately precision 
many on line service allow user to describe their opinion about a product or a service through a review in order to help other user to find out the major opinion about a given topic without the effort to read several review multi document summarisation is required this research proposes an approach for extractive summarisation supporting different scoring technique such a cosine similarity or divergence a a method for finding representative sentence the main contribution of this paper is the definition of an algorithm for sentence removal developed to maximise the score between the summary and the original document instead of ranking the sentence and selecting the most important one the algorithm iteratively remove unimportant sentence until a desired compression rate is reached experimental result show that variation of the sentence removal algorithm provide good performance 
ambiguous query constitute a significant fraction of search instance and pose real challenge to web search engine with current approach the top result for these query tend to be homogeneous making it difficult for user interested in le popular aspect to find relevant document while existing research in search diversification offer several solution for introducing variety into the result the majority of such work is predicated implicitly or otherwise on the assumption that a single relevant document will fulfill a user s information need making them inadequate for many informational query in this paper we present a search diversification algorithm particularly suitable for informational query by explicitly modeling that the user may need more than one page to satisfy their need this modeling enables our algorithm to make a well informed tradeoff between a user s desire for multiple relevant document probabilistic information about an average user s interest in the subtopics of a multifaceted query and uncertainty in classifying document into those subtopics we evaluate the effectiveness of our algorithm against commercial search engine result and other modern ranking strategy demonstrating notable improvement in multiple document scenario 
traditional probabilistic relevance framework for informational retrieval refrain from taking positional information into account due to the hurdle of developing a sound model while avoiding an explosion in the number of parameter nonetheless the well known bm f extension of the successful okapi ranking function can be seen a an embryonic attempt in that direction in this paper we proceed along the same line defining the notion of virtual region a virtual region is a part of the document that like a bm f field can provide a larger or smaller depending on a tunable weighting parameter evidence of relevance of the document differently from bm f field though virtual region are generated implicitly by applying suitable usually but not necessarily positional aware operator to the query this technique fit nicely in the eliteness model behind bm and provides a principled explanation to bm f it specializes to bm f for some trivial operator but ha a much more general appeal our experiment both on standard collection such a trec and on web like repertoire show that the use of virtual region is beneficial for retrieval effectiveness 
in general centrality based retrieval model treat all element of the retrieval space equally which may reduce their effectiveness in the specific context of extractive summarization or important passage retrieval this mean that these model do not take into account that information source often contain lateral issue which are hardly a important a the description of the main topic or are composed by mixture of topic we present a new two stage method that start by extracting a collection of key phrase that will be used to help centrality a relevance retrieval model we explore several approach to the integration of the key phrase in the centrality model the proposed method is evaluated using different datasets that vary in noise noisy v clean and language portuguese v english result show that the best variant achieves relative performance improvement of about in clean data and in noisy data 
patent prior art query are full patent application which are much longer than standard web search topic such query are composed of hundred of term and do not represent a focused information need one way to make the query more focused is to select a group of key term a representative existing work show that such a selection to reduce patent query is a challenging task mainly because of the presence of ambiguous term given this setup we present a query modeling approach where we utilize patent specific characteristic to generate more precise query we propose to automatically disambiguate query term by employing noun phrase that are extracted using the global analysis of the patent collection we further introduce a method for predicting whether expansion using noun phrase would improve the retrieval effectiveness our experiment show that we can obtain almost improvement by performing query expansion using the true importance of the noun phrase query based on this observation we introduce various feature that can be used to estimate the importance of the noun phrase query we evaluated the effectiveness of the proposed method on the patent prior art search collection clef ip our experimental result indicate that the proposed feature make good predictor of the noun phrase importance and selective application of noun phrase query using the importance predictor outperforms existing query generation method 
this paper present finding from a field study of individual who kept diary of their web use across device and location for a period of four day our focus wa on how the web wa used for non work purpose with a view to understanding how this is intertwined with everyday life while our initial aim wa to update existing framework of web activity such a those described by sellen et al and kellar et al our data lead u to suggest that the notion of web activity is only partially useful for an analytic understanding of what it is that people do when they go online instead our analysis lead u to present five mode of web use which can be used to frame and enrich interpretation of activity these are respite orienting opportunistic use purposeful use and lean back internet we then consider two property of the web that enable it to be tailored to these different mode persistence and temporality and close by suggesting way of drawing upon these quality in order to inform design 
service oriented computing soc enables the composition of loosely coupled service provided with varying quality of service qos level selecting a near optimal set of service for a composition in term of qos is crucial when many functionally equivalent service are available with the advent of cloud computing both the number of such service and their distribution across the network are rising rapidly increasing the impact of the network on the qos of such composition despite this current approach do not differentiate between the qos of service themselves and the qos of the network therefore the computed latency differs substantially from the actual latency resulting in suboptimal qos for service composition in the cloud thus we propose a network aware approach that handle the qos of service and the qos of the network independently first we build a network model in order to estimate the network latency between arbitrary service and potential user our selection algorithm then leverage this model to find composition that will result in a low latency given an employed execution policy in our evaluation we show that our approach efficiently computes composition with much lower latency than current approach 
clinical narrative in the medical record provides perhaps the most detailed account of a patient s history however this information is documented in free text which make it challenging to analyze effort to index unstructured clinical narrative often focus on identifying predefined concept from clinical terminology le studied is the problem of analyzing the text a a whole to create temporal index that capture relationship between learned clinical event topic model provide a method for analyzing large corpus of text to discover semantically related cluster of word this work present a topic model tailored to the clinical reporting environment that allows for individual patient timeline result show the model is able to identify pattern of clinical event in a cohort of brain cancer patient 
predicting searcher success and satisfaction is a key problem in web search which is essential for automatic evaluating and improving search engine performance this problem ha been studied actively in the desktop search setting but not specifically for mobile search despite many known difference between the two modality a mobile device become increasingly popular for searching the web improving the searcher experience on such device is becoming crucially important in this paper we explore the possibility of predicting searcher success and satisfaction in mobile search with a smart phone specifically we investigate client side interaction signal including the number of browsed page and touch screen specific action such a zooming and sliding exploiting this information with machine learning technique result in nearly accuracy for predicting searcher success significantly outperforming the previous model 
the main goal of knowledge base population kbp is to distill entity information e g fact of a person from multiple unstructured and semi structured data source and incorporate the information into a knowledge base kb in this work we intend to release an open source kbp toolkit that is publicly available for research purpose 
we consider the task of assigning category e g howto cooking sport basketball pet dog to youtube video from video and text signal we show that two complementary view on the data from the video and text perspective complement each other and refine prediction the contribution of the paper are threefold we show that a text based classifier trained on imperfect prediction of the weakly supervised video content based classifier is not redundant we demonstrate that a simple model which combine the prediction made by the two classifier outperforms each of them taken independently we analyse such source of text information a video title description user tag and viewer comment and show that each of them provides valuable clue to the topic of the video 
the exploration navigation and retrieval of information in cultural heritage workshop enrich offer a forum to discus the challenge and opportunity in information retrieval research in the area of cultural heritage encourage collaboration between researcher engaged in work in this specialist area of information retrieval and to foster the formation of a research community and identify a set of action which the community should undertake to progress the research agenda the workshop will foster a new stream of information retrieval research and support the design of search tool that can help end user fully exploit the wonderful cultural heritage material that is available across the globe 
the goal of the facebook recommendation engine is to compare and rank heterogeneous type of content in order to find the most relevant recommendation based on user preference and page context the challenge for such a recommendation engine include several aspect the online query being processed are at very large scale with new content type and new user generated content constantly added to the system the candidate object set and underlying data distribution change rapidly different type of content usually have very distinct characteristic which make generic feature engineering difficult and unlike a search engine that can capture intention of user based on their search query our recommendation engine need to focus more on user profile and interest past behavior and current action in order to infer their cognitive state in this presentation we would like to introduce an effective scalable online machine learning framework we developed in order to address the aforementioned challenge we also want to discus the insight approach and experience we have accumulated during our research and development process 
given a task t a set of expert v with multiple skill and a social network g v w reflecting the compatibility among the expert team formation is the problem of identifying a team c v that is both competent in performing the task t and compatible in working together existing method for this problem make too restrictive assumption and thus cannot model practical scenario the goal of this paper is to consider the team formation problem in a realistic setting and present a novel formulation based on densest subgraphs our formulation allows modeling of many natural requirement such a i inclusion of a designated team leader and or a group of given expert ii restriction of the size or more generally cost of the team iii enforcing locality of the team e g in a geographical sense or social sense etc the proposed formulation lead to a generalized version of the classical densest subgraph problem with cardinality constraint dsp which is an np hard problem and ha many application in social network analysis in this paper we present a new method for approximately solving the generalized dsp gdsp our method forte is based on solving an equivalent continuous relaxation of gdsp the solution found by our method ha a quality guarantee and always satisfies the constraint of gdsp experiment show that the proposed formulation gdsp is useful in modeling a broader range of team formation problem and that our method produce more coherent and compact team of high quality we also show with the help of an lp relaxation of gdsp that our method give close to optimal solution to gdsp 
diversity a a relevant dimension of retrieval quality is receiving increasing attention in the information retrieval and recommender system r field the problem ha nonetheless been approached under different view and formulation in ir and r respectively giving rise to different model methodology and metric with little convergence between both field in this poster we explore the adaptation of diversity metric technique and principle from ad hoc ir to the recommendation task by introducing the notion of user profile aspect a an analogue of query intent a a particular approach user aspect are automatically extracted from latent item feature empirical result support the proposed approach and provide further insight 
we demonstrate ssnetviz that is developed for integrating visualizing and querying heterogeneous semantic social network obtained from multiple information source a semantic social network refers to a social network graph with multi typed node and link we demonstrate various innovative feature of ssnetviz with social network from three information source covering a similar set of entity and relationship in terrorism domain 
we consider the problem of information retrieval evaluation and the method and metric used for such evaluation we propose a probabilistic framework for evaluation which we use to develop new information theoretic evaluation metric we demonstrate that these new metric are powerful and generalizable enabling evaluation heretofore not possible we introduce four preliminary us of our framework a measure of conditional rank correlation information tau a powerful meta evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation a new evaluation measure relevance information correlation which is correlated with traditional evaluation measure and can be used to evaluate a collection of system simultaneously which provides a natural upper bound on metasearch performance and a measure of the similarity between ranker on judged document information difference which allows u to determine whether system with similar performance are in fact different 
query processing with precomputed term pair list can improve efficiency for some query but suffers from the quadratic number of index list that need to be read we present a novel hybrid index structure that aim at decreasing the number of index list retrieved at query processing time trading off a reduced number of index list for an increased number of byte to read our experiment demonstrate significant cold cache performance gain of almost on standard benchmark query 
in this paper we present a longitudinal naturalistic study of email behavior n and describe our effort at isolating re finding behavior in the log through various qualitative and quantitative analysis the presented work underline the methodological challenge faced with this kind of research but demonstrates that it is possible to isolate re finding behavior from email interaction log with reasonable accuracy using the approach developed we uncover interesting aspect of email re finding behavior that have so far been impossible to study such a how various feature of email client are used in re finding and the difficulty people encounter when using these we explain how our finding could influence the design of email client and outline our thought on how future more in depth analysis can build on the work presented here to achieve a fuller understanding of email behavior and the support that people need 
crowdtracker is a community based web monitoring system optimized for real time web stream like twitter facebook and google buzz in this demo summary we provide an overview of the system and architecture and outline the demonstration plan 
recommender system are commonly evaluated by trying to predict known withheld rating for a set of user measure such a the root mean square error are used to estimate the quality of the recommender algorithm this process doe however not acknowledge the inherent rating inconsistency of user in this paper we present the first result from a noise measurement user study for estimating the magic barrier of recommender system conducted on a commercial movie recommendation community the magic barrier is the expected squared error of the optimal recommendation algorithm or the lowest error we can expect from any recommendation algorithm our result show that the barrier can be estimated by collecting the opinion of user on already rated item 
the evolution of the internet ha manifested itself in many way the traffic characteristic the interconnection topology and the business relationship among the autonomous component it is important to understand why and how this evolution came about and how the interplay of these dynamic may affect future evolution and service we propose a network aware macroscopic model that capture the characteristic and interaction of the application and network provider and show how it lead to a market equilibrium of the ecosystem by analyzing the driving force and the dynamic of the market equilibrium we obtain some fundamental understanding of the cause and effect of the internet evolution which explain why some historical and recent evolution have happened furthermore by projecting the likely future evolution our model can help application and network provider to make informed business decision so a to succeed in this competitive ecosystem 
the situation in which a choice is made is an important information for recommender system context aware recommenders take this information into account to make prediction so far the best performing method for context aware rating prediction in term of predictive accuracy is multiverse recommendation based on the tucker tensor factorization model however this method ha two drawback it model complexity is exponential in the number of context variable and polynomial in the size of the factorization and it only work for categorical context variable on the other hand there is a large variety of fast but specialized recommender method which lack the generality of context aware method we propose to apply factorization machine fm to model contextual information and to provide context aware rating prediction this approach result in fast context aware recommendation because the model equation of fm can be computed in linear time both in the number of context variable and the factorization size for learning fm we develop an iterative optimization method that analytically find the least square solution for one parameter given the other one finally we show empirically that our approach outperforms multiverse recommendation in prediction quality and runtime 
modern day federated search engine aggregate heterogeneous type of result from multiple vertical search engine and compose a single search engine result page serp the search engine aggregate the result and produce one ranked list constraining the vertical result to specific slot on the serp the usual way to compare two ranking algorithm is to first fix their operating point internal threshold and then run an online experiment that last multiple week online user engagement metric are then compared to decide which algorithm is better however this method doe not characterize and compare the behavior over the entire span of operating point furthermore this time consuming approach is not practical if we have to conduct the experiment over numerous operating point in this paper we propose a method of characterizing the performance of model that allows u to predict answer to what if question about online user engagement using click log over the entire span of feasible operating point we audition vertical at various slot on the serp and generate click log this log is then used to create operating curve between variable of interest for example between result quality and click through the operating point for the system then can be chosen to achieve a specific trade off between the variable we apply this methodology to predict i the online performance of two different model ii the impact of changing internal quality threshold on clickthrough iii the behavior of introducing a new feature iv which machine learning loss function will give better online engagement v the impact of sampling distribution of head and tail query in the training process the result are reported on a well known federated search engine we validate the prediction with online experiment 
multiview learning ha been shown to be a natural and efficient framework for supervised or semi supervised learning of multilingual document categorizers the state of the art co regularization approach relies on alternate minimization of a combination of language specific categorization error and a disagreement between the output of the monolingual text categorizers this is typically solved by repeatedly training categorizers on each language with the appropriate regularizer we extend and improve this approach by introducing an on line learning scheme where language specific update are interleaved in order to iteratively optimize the global cost in one pas our experimental result show that this produce similar performance a the batch approach at a fraction of the computational cost 
recommending news article ha become a promising research direction a the internet provides fast access to real time information from multiple source around the world traditional news recommendation system strive to adapt their service to individual user by virtue of both user and news content information however the latent relationship among different news item and the special property of new article such a short shelf life and value of immediacy render the previous approach inefficient in this paper we propose a scalable two stage personalized news recommendation approach with a two level representation which considers the exclusive characteristic e g news content access pattern named entity popularity and recency of news item when performing recommendation also a principled framework for news selection based on the intrinsic property of user interest is presented with a good balance between the novelty and diversity of the recommended result extensive empirical experiment on a collection of news article obtained from various news website demonstrate the efficacy and efficiency of our approach 
many network based ranking approach have been proposed to rank object according to different criterion including relevance prestige and diversity however existing approach either only aim at one or two of the criterion or handle them with additional heuristic in multiple step inspired by divrank we propose a unified ranking model decayed divrank ddrank to meet the three criterion simultaneously empirical experiment on paper citation network show that ddrank can outperform existing algorithm in capturing relevance diversity and prestige simultaneously in ranking 
expert retrieval ha been widely studied especially after the introduction of expert finding task in the trec s enterprise track in this track provided two different test collection crawled from two organization public facing website and internal email which led to the development of many state of the art algorithm on expert retrieval until recently these datasets were considered good representative of the information resource available within enterprise however the recent growth of social medium also influenced the work environment and social medium became a common communication and collaboration tool within organization according to a recent survey by mckinsey global institute of the company use at least one social medium tool for matching their employee to task and of them ass their employee performance by using social medium this show that intra organizational social medium became an important resource to identify expertise within organization in recent year in addition to the intra organizational social medium public social medium tool like twitter facebook linkedin also became common environment for searching expertise these tool provide an opportunity for their user to show their specific skill to the world which motivates recruiter to look for talented job candidate on social medium or writer and reporter to find expert for consulting on specific topic they are working on with these motivation in mind in this work we propose to develop expert retrieval algorithm for intra organizational and public social medium tool social medium datasets have both challenge and advantage in term of challenge they do not always contain context on one specific domain instead one social medium tool may contain discussion on technical stuff hobby or news concurrently they may also contain spam post or advertisement compared to well edited enterprise document they are much more informal in language furthermore depending on the social medium platform they may have limit on the number of character used in post even though they include the challenge stated above they also bring some unique authority signal such a vote comment follower following information which can be useful in estimating expertise furthermore compared to previously used enterprise document social medium provides clear association between document and candidate in the context of authorship information in this work we propose to develop expert retrieval approach which will handle these challenge while making use of the advantage expert retrieval is a very useful application by itself furthermore it can be a step towards improving other social medium application social medium is different than other web based tool mainly because it is dependent on it user in social medium user are not just content consumer but they are also the primary and sometimes the only content creator therefore the quality of any user generated content in social medium depends on it creator in this thesis we propose to use expertise of user in order to improve the existing application so that they can estimate the relevancy of a content not just based on the content but also based on the expertise of the content creator by using expertise of the content generator we also hope to boost content that are more reliable we propose to apply this user s expertise information in order to improve ad hoc search and question answering application in social medium in this work previous trec enterprise datasets available intra organizational social medium and public social medium datasets will be used to test the proposed algorithm 
web search engine have historically focused on connecting people with information resource for example if a person wanted to know when their flight to hyderabad wa leaving a search engine might connect them with the airline where they could find flight status information however search engine have recently begun to try to meet people s search need directly providing for example flight status information in response to query that include an airline and a flight number in this paper we use large scale query log analysis to explore the challenge a search engine face when trying to meet an information need directly in the search result page we look at how people s interaction behavior change when inline content is returned finding that such content can cannibalize click from the algorithmic result we see that in the absence of interaction behavior an individual s repeat search behavior can be useful in understanding the content s value we also discus some of the way user behavior can be used to provide insight into when inline answer might better trigger and what type of additional information might be included in the result 
in service oriented architecture soa independently developed web service can be dynamically composed however the composition is prone to producing semantically conflicting interaction among the service for example in an interdepartmental business collaboration through web service the decision by the marketing department to clear out the inventory might be inconsistent with the decision by the operation department to increase production resolving semantic conflict is challenging especially when service are loosely coupled and their interaction are not carefully governed to address this problem we propose a novel distributed service choreography framework we deploy safety constraint to prevent conflicting behavior and enforce reliable and efficient service interaction via federated publish subscribe messaging along with strategic placement of distributed choreography agent and coordinator to minimize runtime overhead experimental result show that our framework prevents semantic conflict with negligible overhead and scale better than a centralized approach by up to 
one of the most popular user activity on the web is watching video service like youtube vimeo and hulu host and stream million of video providing content that is on par with tv while some of this content is popular all over the globe some video might be only watched in a confined local region in this work we study the relationship between popularity and locality of online youtube video we investigate whether youtube video exhibit geographic locality of interest with view arising from a confined spatial area rather than from a global one our analysis is done on a corpus of more than million youtube video uploaded over one year from different region we find that about of the video have more than of their view in a single region by relating locality to viralness we show that social sharing generally widens the geographic reach of a video if however a video cannot carry it social impulse over to other mean of discovery it get stuck in a more confined geographic region finally we analyze how the geographic property of a video s view evolve on a daily basis during it lifetime providing new insight on how the geographic reach of a video change a it popularity peak and then fade away our result demonstrate how despite the global nature of the web online video consumption appears constrained by geographic locality of interest this ha a potential impact on a wide range of system and application spanning from delivery network to recommendation and discovery engine providing new direction for future research 
the psycholinguistic theory of communication accommodation account for the general observation that participant in conversation tend to converge to one another s communicative behavior they coordinate in a variety of dimension including choice of word syntax utterance length pitch and gesture in it almost forty year of existence this theory ha been empirically supported exclusively through small scale or controlled laboratory study here we address this phenomenon in the context of twitter conversation undoubtedly this setting is unlike any other in which accommodation wa observed and thus challenging to the theory it novelty come not only from it size but also from the non real time nature of conversation from the character length restriction from the wide variety of social relation type and from a design that wa initially not geared towards conversation at all given such constraint it is not clear a priori whether accommodation is robust enough to occur given the constraint of this new environment to investigate this we develop a probabilistic framework that can model accommodation and measure it effect we apply it to a large twitter conversational dataset specifically developed for this task this is the first time the hypothesis of linguistic style accommodation ha been examined and verified in a large scale real world setting furthermore when investigating concept such a stylistic influence and symmetry of accommodation we discover a complexity of the phenomenon which wa never observed before we also explore the potential relation between stylistic influence and network feature commonly associated with social status 
matrix factorization on user item rating matrix ha achieved significant success in collaborative filtering based recommendation task however it also encounter the problem of data sparsity and scalability when applied in real world recommender system in this paper we present the localized matrix factorization lmf framework which attempt to meet the challenge of sparsity and scalability by factorizing block diagonal form bdf matrix in the lmf framework a large sparse matrix is first transformed into recursive bordered block diagonal form rbbdf which is an intuitionally interpretable structure for user item rating matrix smaller and denser submatrices are then extracted from this rbbdf matrix to construct a bdf matrix for more effective collaborative prediction we show formally that the lmf framework is suitable for matrix factorization and that any decomposable matrix factorization algorithm can be integrated into this framework it ha the potential to improve prediction accuracy by factorizing smaller and denser submatrices independently which is also suitable for parallelization and contributes to system scalability at the same time experimental result based on a number of real world public access benchmark show the effectiveness and efficiency of the proposed lmf framework 
consumer purchase decision are increasingly influenced by user generated online review accordingly there ha been growing concern about the potential for posting deceptive opinion spam fictitious review that have been deliberately written to sound authentic to deceive the reader but while this practice ha received considerable public attention and concern relatively little is known about the actual prevalence or rate of deception in online review community and le still about the factor that influence it we propose a generative model of deception which in conjunction with a deception classifier we use to explore the prevalence of deception in six popular online review community expedia hotel com orbitz priceline tripadvisor and yelp we additionally propose a theoretical model of online review based on economic signaling theory in which consumer review diminish the inherent information asymmetry between consumer and producer by acting a a signal to a product s true unknown quality we find that deceptive opinion spam is a growing problem overall but with different growth rate across community these rate we argue are driven by the different signaling cost associated with deception for each review community e g posting requirement when measure are taken to increase signaling cost e g filtering review written by first time reviewer deception prevalence is effectively reduced 
we introduce a general information access evaluation framework that can potentially handle summary ranked document list and even multi query session seamlessly our framework first build a trailtext which represents a concatenation of all the text read by the user during a search session and then computes an evaluation metric called u measure over the trailtext instead of discounting the value of a retrieved piece of information based on rank u measure discount it based on it position within the trailtext u measure take the document length into account just like time biased gain tbg and ha the diminishing return property it is therefore more realistic than rank based metric furthermore it is arguably more flexible than tbg a it is free from the linear traversal assumption i e that the user scan the ranked list from top to bottom and can handle information access task other than ad hoc retrieval this paper demonstrates the validity and versatility of the u measure framework our main conclusion are a for ad hoc retrieval u measure is at least a reliable a tbg in term of rank correlation with traditional metric and discriminative power b for diversified search our diversity version of u measure are highly correlated with state of the art diversity metric c for multi query session u measure is highly correlated with session ndcg and d unlike rank based metric such a dcg u measure can quantify the difference between linear and nonlinear traversal in session we argue that our new framework is useful for understanding the user s search behaviour and for comparison across different information access style e g examining a direct answer v examining a ranked list of web page 
recent advance in information extraction have led to huge knowledge base kb which capture knowledge in a machine readable format inductive logic programming ilp can be used to mine logical rule from the kb these rule can help deduce and add missing knowledge to the kb while ilp is a mature field mining logical rule from kb is different in two aspect first current rule mining system are easily overwhelmed by the amount of data state of the art system cannot even run on today s kb second ilp usually requires counterexample kb however implement the open world assumption owa meaning that absent data cannot be used a counterexample in this paper we develop a rule mining model that is explicitly tailored to support the owa scenario it is inspired by association rule mining and introduces a novel measure for confidence our extensive experiment show that our approach outperforms state of the art approach in term of precision and coverage furthermore our system amie mine rule order of magnitude faster than state of the art approach 
a diversified search result for an underspecified query generally contains web page in which there are answer that are relevant to different aspect of the query in order to help the user locate such relevant answer we propose a simple extension to the standard search engine result page serp interface called aspectiles in addition to presenting a ranked list of url with their title and snippet aspectiles visualizes the relevance degree of a document to each aspect by mean of colored square tile to compare aspectiles with the standard serp interface in term of usefulness we conducted a user study involving search task designed based on the trec web diversity task topic a well a participant our result show that aspectiles ha some advantage in term of search performance user behavior and user satisfaction first aspectiles enables the user to gather relevant information significantly more efficiently than the standard serp interface for task where the user considers several different aspect of the query to be important at the same time multi aspect task second aspectiles affect the user s information seeking behavior with this interface we observed significantly fewer query reformulations shorter query and deeper examination of ranked list in multi aspect task third participant of our user study found aspectiles significantly more useful for finding relevant information and easy to use than the standard serp interface these result suggest that simple interface like aspectiles can enhance the search performance and search experience of the user when their query are underspecified 
naive bayes nb classifier are simple probabilistic classifier still widely used in supervised learning due to their tradeoff between efficient model training and good empirical result one of the drawback of these classifier is that in situation of data sparsity i e when the size of training set is small the maximum likelihood estimation of the probability of unseen feature in these situation is equal to zero causing arithmetic anomaly to prevent this undesirable behavior a number of smoothing technique have been proposed among these the bayesian approach incorporates smoothing in term of prior knowledge about the parameter of the model usually called hyper parameter our research question is can a visualization tool help researcher to quickly ass the goodness of the performance of nb classifier by setting optimal smoothing parameter 
this paper address the problem of long term language change in information retrieval ir system ir research ha often ignored lexical drift but in the emerging domain of massive digitized book collection the risk of vocabulary mismatch due to language change is high collection such a google book and the hathi trust contain text written in the vernacular of many century with respect to ir change in vocabulary and orthography make th century english qualitatively different from st century english this challenge retrieval model that rely on keyword matching with this challenge in mind we ask given a query written in contemporary english how can we retrieve relevant document that were written in early english we argue that search in historically diverse corpus is similar to cross language retrieval clir by considering modern english and archaic english a distinct language clir technique can improve what we call cross temporal ir ctir we focus on way to combine evidence to improve ctir effectiveness proposing and testing several way to handle language change during book search we find that a principled combination of three source of evidence during relevance feedback yield strong ctir performance 
text search engine are a fundamental tool nowadays their efficiency relies on a popular and simple data structure the inverted index currently inverted index can be represented very efficiently using index compression scheme recent investigation also study how an optimized document ordering can be used to assign document identifier docids to the document database this yield important improvement in index compression and query processing time in this paper we follow this line of research yet from a different perspective we propose a docid reassignment method that allows one to focus on a given subset of inverted list to improve their performance we then use run length encoding to compress these list a many consecutive s are generated we show that by using this approach not only the performance of the particular subset of inverted list is improved but also that of the whole inverted index our experimental result indicate a reduction of about in the space usage of the whole index docid reassignment wa focused also decompression speed is up to time faster if the run must be explicitly decompressed and up to time faster if implicit decompression of run is allowed finally we also improve the document at a time query processing time of and query by up to wand query by up to and full non ranked or query by up to 
browser have become mature execution platform enabling web application to rival their desktop counterpart an important class of such application is interactive multimedia game animation and interactive visualization unlike many early web application these application are latency sensitive and processing cpu and graphic intensive when demand exceed available resource application quality e g frame rate diminishes because it is hard to balance timeliness and utilization the quality of ambitious web application is also limited by single threaded execution prevalent in the web application need to scale their quality and thereby scale processing load based on the resource that are available we refer to this a scalable quality doha is an execution layer written entirely in javascript to enable scalable quality in web application doha favor important computation with more influence over quality based on hint from application specific adaptation policy to utilize widely available multi core resource doha augments html web worker with mechanism to facilitate state management and load balancing we evaluate doha with an award winning web based game when resource are limited the modified game ha better timing and overall quality more importantly quality scale linearly with a small number of core and the game is playable in challenging scenario that are beyond the scope of the original game 
we use modern feature of web browser to develop a secure login system from an untrusted terminal the system called session juggler requires no server side change and no special software on the terminal beyond a modern web browser this important property make adoption much easier than with previous proposal with session juggler user never enter their long term credential on the untrusted terminal instead user log in to a web site using a smartphone app and then transfer the entire session including cooky and all other session state to the untrusted terminal we show that session juggler work on all the alexa top site except eight of those eight five failure were due to the site enforcing ip session binding we also show that session juggler work flawlessly with facebook connect beyond login session juggler also provides a secure logout mechanism where the trusted phone is used to kill the session to validate the session juggling concept we conducted a number of web site survey that are of independent interest first we survey how web site bind a session token to a specific device and show that most use fairly basic technique that are easily defeated second we survey how web site handle logout and show that many popular site surprisingly do not properly handle logout request 
the assumption underlying the probability ranking principle prp have led to a number of alternative approach that cater or compensate for the prp s limitation in this poster we focus on the interactive prp iprp which reject the assumption of independence between document made by the prp although the theoretical framework of the iprp is appealing no instantiation ha been proposed and investigated in this poster we propose a possible instantiation of the principle performing the first empirical comparison of the iprp against the prp for document diversification our result show that the iprp is significantly better than the prp and comparable to or better than other method such a modern portfolio theory 
document at a time daat dynamic pruning strategy for information retrieval system such a maxscore and wand can increase querying efficiency without decreasing effectiveness both work on posting list sorted by ascending document identifier docid the order in which docids are assigned and hence the order of posting in the posting list is known to have a noticeable impact on posting list compression however the resulting impact on dynamic pruning strategy is not well understood in this poster we examine the impact on the efficiency of these strategy across different docid ordering by experimenting using the trec clueweb corpus we find that while the number of posting scored by dynamic pruning strategy do not markedly vary for different docid ordering the ordering still ha a marked impact on mean query response time moreover when docids are assigned by lexicographical url ordering the benefit to response time for is more pronounced for wand than for maxscore 
in the past there have been dozen of study on automatic authorship classification and many of these study concluded that the writing style is one of the best indicator for original authorship from among the hundred of feature which were developed syntactic feature were best able to reflect an author s writing style however due to the high computational complexity for extracting and computing syntactic feature only simple variation of basic syntactic feature such a function word po part of speech tag and rewrite rule were considered in this paper we propose a new feature set of k embedded edge subtree pattern that hold more syntactic information than previous feature set we also propose a novel approach to directly mining them from a given set of syntactic tree we show that this approach reduces the computational burden of using complex syntactic structure a the feature set comprehensive experiment on real world datasets demonstrate that our approach is reliable and more accurate than previous study 
many recent and highly effective retrieval model for long query use query reformulation method that jointly optimize term weight and term selection these method learn using word context and global context but typically fail to capture query context in this paper we present a novel term ranking algorithm phrank that extends work on markov chain framework for query expansion to select compact and focused term from within a query itself this focus query so that one to five term in an unweighted model achieve better retrieval effectiveness than weighted term selection model that use up to term phrank term are also typically compact and contain word compared to competing model that use query subset up to word long phrank capture query context with an affinity graph constructed using word co occurrence in pseudo relevant document a random walk of the graph is used for term ranking in combination with discrimination weight empirical evaluation using newswire and web collection demonstrates that performance of reformulated query is significantly improved for long query and at least a good for short keyword query compared to highly competitive information retrieval ir model 
novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to user evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre identified subtopics which may be disambiguation of the query facet of an information need or nugget of information alternately when expressing a emph preference for document a or document b user may implicitly take subtopics into account but may also take into account other factor such a recency readability length and so on each of which may have more or le importance depending on user a emph user profile contains information about the extent to which each factor including subtopic relevance play a role in the user s preference for one document over another a preference based evaluation can then take this user profile information into account to better model utility to the space of user in this work we propose an evaluation framework that not only can consider implicit factor but also handle difference in user preference due to varying underlying information need our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gain some utility from every document that is relevant their information need thus we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgment and define evaluation measure based on the same we validate our framework by comparing it to existing measure such a alpha ndcg err ia and subtopic recall that require explicit subtopic judgment we show that our proposed measure correlate well with existing measure while having the potential to capture various other factor when real data is used we also show that the proposed measure can easily handle relevance assessment against multiple user profile and that they are robust to noisy and incomplete judgment 
we explore the notion put forward by cormack lynam and robertson that we should consider a document collection used for cranfield style experiment a a sample from some larger population of document in this view any per topic metric such a average precision should be regarded a an estimate of that metric s true value for that topic in the full population and therefore a carrying it own per topic variance or estimate precision or noise a in the two mentioned paper we explore this notion by simulating other sample from the same large population we investigate different way of performing this simulation one use of this analysis is to refine the notion of statistical significance of a difference between two system in most such analysis each per topic measurement is treated a equally precise we propose a mixed effect model method to measure significance and compare it experimentally with the traditional t test 
recent study have shown that boosting provides excellent predictive performance across a wide variety of task in learning to rank boosted model such a rankboost and lambdamart have been shown to be among the best performing learning method based on evaluation on public data set in this paper we show how the combination of bagging a a variance reduction technique and boosting a a bias reduction technique can result in very high precision and low variance ranking model we perform thousand of parameter tuning experiment for lambdamart to achieve a high precision boosting model then we show that a bagged ensemble of such lambdamart boosted model result in higher accuracy ranking model while also reducing variance a much a we report our result on three public learning to rank data set using four metric bagged lamdbamart outperforms all previously reported result on ten of the twelve comparison and bagged lambdamart outperforms non bagged lambdamart on all twelve comparison for example wrapping bagging around lambdamart increase ndcg from to on the mq data set the best prior result in the literature for this data set is by rankboost 
we propose a classification model of tweet stream in twitter which are representative of document stream whose statistical property will change over time our model solves several problem that hinder the classification of tweet in particular the problem that the probability of word occurrence change at different rate for different word our model switch between two probability estimate based on full and recent data for each word when detecting change in word probability this switching enables our model to achieve both accurate learning of stationary word and quick response to bursty word we then explain how to implement our model by using a word suffix array which is a full text search index using the word suffix array allows our model to handle the temporal attribute of word n gram effectively experiment on three tweet data set demonstrate that our model offer statistically significant higher topic classification accuracy than conventional temporally aware classification model 
republished article finding is the task of identifying instance of article that have been published in one source and republished more or le verbatim in another source which is often a social medium source we address this task a an ad hoc retrieval problem using the source article a a query our approach is based on language modeling we revisit the assumption underlying the unigram language model taking into account the fact that in our setup query are a long a complete news article we argue that in this case the underlying generative assumption of sampling word from a document with replacement i e the multinomial modeling of document produce le accurate query likelihood estimate to make up for this discrepancy we consider distribution that emerge from sampling without replacement the central and non central hypergeometric distribution we present two retrieval model that build on top of these distribution a log odds model and a bayesian model where document parameter are estimated using the dirichlet compound multinomial distribution we analyse the behavior of our new model using a corpus of news article and blog post and find that for the task of republished article finding where we deal with query whose length approach the length of the document to be retrieved model based on distribution associated with sampling without replacement outperform traditional model based on multinomial distribution 
in the internet music scene where recommendation technology is key for navigating huge collection large market player enjoy a considerable advantage accessing a wider pool of user feedback lead to an increasingly more accurate analysis of user taste effectively creating a rich get richer effect this work aim at significantly lowering the entry barrier for creating music recommenders through a paradigm coupling a public data source and a new collaborative filtering cf model we claim that internet radio station form a readily available resource of abundant fresh human signal on music through their playlist which are essentially cohesive set of related track in a way our model rely on the knowledge of a diverse group of expert in lieu of the commonly used wisdom of crowd over several week we aggregated publicly available playlist of thousand of internet radio station resulting in a dataset encompassing million of play and hundred of thousand of track and artist this provides the large scale ground data necessary to mitigate the cold start problem of new item at both mature and emerging service furthermore we developed a new probabilistic cf model tailored to the internet radio resource the success of the model wa empirically validated on the collected dataset moreover we tested the model at a cross source transfer learning manner the same model trained on the internet radio data wa used to predict behavior of yahoo music user this demonstrates the ability to tap the internet radio signal in other music recommendation setup based on encouraging empirical result our hope is that the proposed paradigm will make quality music recommendation accessible to all interested party in the community 
form are our gate to the web they enable u to access the deep content of web site automatic form understanding unlocks this content for application ranging from crawler to meta search engine and is essential for improving usability and accessibility of the web form understanding ha received surprisingly little attention other than a component in specific application such a crawler no comprehensive approach to form understanding exists and previous work disagree even in the definition of the problem in this paper we present opal the first comprehensive approach to form understanding we identify form labeling and form interpretation a the two main task involved in form understanding on both problem opal push the state of the art for form labeling it combine signal from the text structure and visual rendering of a web page yielding robust characterisation of common design pattern in extensive experiment on the icq and tel benchmark and a set of modern web form opal outperforms previous approach by a significant margin for form interpretation we introduce a template language to describe frequent form pattern these two part of opal combined yield form understanding with near perfect accuracy 
we propose to explicitly exploit issue related to novelty and diversity in tag recommendation task an unexplored research avenue only relevance issue have been investigated so far in order to improve user experience and satisfaction we propose new tag recommendation strategy to cover these issue and highlight the involved challenge 
large web search engine process billion of query each day over ten of billion of document with often very stringent requirement for a user s search experience in particular low latency and highly relevant search result index generation and serving are key to satisfying both these requirement for example the load to search engine can vary drastically when popular event happen around the world in the case when the load is exceeding what the search engine can serve query will get dropped this result in an ungraceful degradation in search quality another example that could increase the query load and affect the user s search experience are ambiguous query which often result in the execution of multiple query alteration in the back end in this paper we look into the problem of designing robust indexing strategy i e strategy that allow for a graceful degradation of search quality in both the above scenario we study the problem of index generation and serving using the notion of document allocation server selection and document replication we explore the space of efficient algorithm for these problem and empirically corroborate with existing theory that it is hard to optimally solve the alocation and selection problem without any replication we propose a greedy replication algorithm and study it performance under different choice of allocation and selection further we show hat under random selection and allocation our algorithm is optimal 
decompounding ha been found to improve information retrieval ir effectiveness in general domain for language such a german or dutch we investigate if cross language patent retrieval can profit from decompounding this pose two challenge i there may be few resource such a parallel corpus available for training an machine translation system for a compounding language ii patent have a specific writing style and vocabulary patentese which may affect the performance of decompounding and translation method experiment on data from the clef ip task show that decompounding patent for translation can overcome out of vocabulary problem oov and that decompounding improves ir performance significantly for small training corpus 
we reveal that the okapi bm retrieval function tends to overly penalize very long document to address this problem we present a simple yet effective extension of bm namely bm l which shift the term frequency normalization formula to boost score of very long document our experiment show that bm l with the same computation cost is more effective and robust than the standard bm 
random walk with restart rwr ha become an appealing measure of node proximity in emerging application eg recommender system and automatic image captioning in practice a real graph is typically large and is frequently updated with small change it is often cost inhibitive to recompute proximity from scratch via emph batch algorithm when the graph is updated this paper focus on the incremental computation of rwr in a dynamic graph whose edge often change over time the prior attempt of rwr deploys kdash to find top k highest proximity node for a given query which involves a strategy to incrementally emph estimate upper proximity bound however due to it aim to prune needle calculation such an incremental strategy is emph approximate in o time for each node the main contribution of this paper is to devise an emph exact and fast incremental algorithm of rwr for edge update our solution irwr can incrementally compute any node proximity in o time for each edge update without loss of exactness the empirical evaluation show the high efficiency and exactness of irwr for computing proximity on dynamic network against it batch counterpart 
online forum are becoming a popular way of finding useful information on the web search over forum for existing discussion thread so far is limited to keyword based search due to the minimal effort required on part of the user however it is often not possible to capture all the relevant context in a complex query using a small number of keywords example based search that retrieves similar discussion thread given one exemplary thread is an alternate approach that can help the user provide richer context and vastly improve forum search result in this paper we address the problem of finding similar thread to a given thread towards this we propose a novel methodology to estimate similarity between discussion thread our method exploit the thread structure to decompose thread in to set of weighted overlapping component it then estimate pairwise thread similarity by quantifying how well the information in the thread are mutually contained within each other using lexical similarity between their underlying component we compare our proposed method on real datasets against state of the art thread retrieval mechanism wherein we illustrate that our technique outperform others by large margin on popular retrieval evaluation measure such a ndcg map precision k and mrr in particular consistent improvement of up to are observed on all evaluation measure 
crowdsourcing is a market of steadily growing importance upon which both academia and industry increasingly rely however this market appears to be inherently infested with a significant share of malicious worker who try to maximise their profit through cheating or sloppiness this serf to undermine the very merit crowdsourcing ha come to represent based on previous experience a well a psychological insight we propose the use of a game in order to attract and retain a larger share of reliable worker to frequently requested crowdsourcing task such a relevance assessment and clustering in a large scale comparative study conducted using recent trec data we investigate the performance of traditional hit design and a game based alternative that is able to achieve high quality at significantly lower pay rate facing fewer malicious submission 
synchronous social q a system exist on the web and in the enterprise to connect people with question to people with answer in real time in such system asker desire for quick answer is in tension with cost associated with interrupting numerous candidate answerer per question supporting user of synchronous social q a system at various point in the question lifecycle from conception to answer help asker make informed decision about the likelihood of question success and help answerer face fewer interruption for example predicting that a question will not be well answered may lead the asker to rephrase or retract the question similarly predicting that an answer is not forthcoming during the dialog can prompt system behavior such a finding other answerer to join the conversation a another example prediction of asker satisfaction can be assigned to completed conversation and used for later retrieval in this paper we use data from an instant messaging based synchronous social q a service deployed to an online community of over two thousand user to study the prediction of i whether a question will be answered ii the number of candidate answerer that the question will be sent to and iii whether the asker will be satisfied by the answer received prediction are made at many point of the question lifecycle e g when the question is entered when the answerer is located halfway through the asker answerer dialog etc the finding from our study show that we can learn capable model for these task using a broad range of feature derived from user profile system interaction question setting and the dialog between asker and answerer our research can lead to more sophisticated and more useful real time q a support 
we conduct a study of the spatio temporal dynamic of twitter hashtags through a sample of billion geo tagged tweet in our analysis we i examine the impact of location time and distance on the adoption of hashtags which is important for understanding meme diffusion and information propagation ii examine the spatial propagation of hashtags through their focus entropy and spread and iii present two method that leverage the spatio temporal propagation of hashtags to characterize location based on this study we find that although hashtags are a global phenomenon the physical distance between location is a strong constraint on the adoption of hashtags both in term of the hashtags shared between location and in the timing of when these hashtags are adopted we find both spatial and temporal locality a most hashtags spread over small geographical area but at high speed we also find that hashtags are mostly a local phenomenon with long tailed life span these and other finding have important implication for a variety of system and application including targeted advertising location based service social medium search and content delivery network 
query suggestion which enables the user to revise a query with a single click ha become one of the most fundamental feature of web search engine however it is often difficult for the user to choose from a list of query suggestion and to understand the relation between an input query and suggested one in this paper we propose a new method to present query suggestion to the user which ha been designed to help two popular query reformulation action namely specialization e g from nikon to nikon camera and parallel movement e g from nikon camera to canon camera using a query log collected from a popular commercial web search engine our prototype called sparqs classifies query suggestion into automatically generated category and generates a label for each category moreover sparqs present some new entity a alternative to the original query e g canon in response to the query nikon together with their query suggestion classified in the same way a the original query s suggestion we conducted a task based user study to compare sparqs with a traditional flat list query suggestion interface our result show that the sparqs interface enables subject to search more successfully than the flat list case even though query suggestion presented were exactly the same in the two interface in addition the subject found the query suggestion more helpful when they were presented in the sparqs interface rather than in a flat list 
type ahead search can on the fly find answer a a user type in a keyword query a main challenge in this search paradigm is the high efficiency requirement that query must be answered within millisecond in this paper we study how to answer top k query in this paradigm i e a a user type in a query letter by letter we want to efficiently find the k best answer instead of inventing completely new algorithm from scratch we study challenge when adopting existing top k algorithm in the literature that heavily rely on two basic list access method random access and sorted access we present two algorithm to support random access efficiently we develop novel technique to support efficient sorted access using list pruning and materialization we extend our technique to support fuzzy type ahead search which allows minor error between query keywords and answer we report our experimental result on several real large data set to show that the proposed technique can answer top k query efficiently in type ahead search 
the result cache is a vital component for efficiency of large scale web search engine and maintaining the freshness of cached query result is the current research challenge a a remedy to this problem our work proposes a new mechanism to identify query whose cached result are stale the basic idea behind our mechanism is to maintain and compare generation time of query result with update time of posting list and document to decide on staleness of query result the proposed technique is evaluated using a wikipedia document collection with real update information and a real life query log we show that our technique ha good prediction accuracy relative to a baseline based on the time to live mechanism moreover it is easy to implement and incurs le processing overhead on the system relative to a recently proposed more sophisticated invalidation mechanism 
semi supervised learning is to exploit the vast amount of unlabeled data in the world this paper proposes a scalable graph based technique leveraging the distributed computing power of the mapreduce programming model for a higher quality of learning the paper also present a multi layer learning structure to unify both visual and textual information of image data during the learning process experimental result show the effectiveness of the proposed method 
it s well known that the transitivity of friendship is a popular sociological principle in social network however it s still unknown that to what extent people s friend making behavior follow this principle and to what extent it can benefit the link prediction task in this paper we try to adopt this sociological principle to explain the evolution of network and study the latent friendship propagation unlike traditional link prediction approach we model link formation a result of individual friend making behavior combined with personal interest we propose the latent friendship propagation network lfpn which depicts the evolution progress of one s egocentric network and reveals future growth potential driven by the transitivity of friendship based on personal interest we model individual social behavior using the latent friendship propagation model lfpm a probabilistic generative model from which the lfpn can be learned effectively to evaluate the power of the friendship propagation in link prediction we design lfpn rw which model the friend making behavior a a random walk upon the lfpn naturally and capture the co influence effect of the friend circle a well a personal interest to provide more accurate prediction experimental result on real world datasets show that lfpn rw outperforms the state of the art approach this convinces that the transitivity of friendship actually play important role in the evolution of social network 
we present a new unsupervised topic discovery model for a collection of text document in contrast to the majority of the state of the art topic model our model doe not break the document s structure such a paragraph and sentence in addition it preserve word order in the document a a result it can generate two level of topic of different granularity namely segment topic and word topic in addition it can generate n gram word in each topic we also develop an approximate inference scheme using gibbs sampling method we conduct extensive experiment using publicly available data from different collection and show that our model improves the quality of several text mining task such a the ability to support fine grained topic with n gram word in the correlation graph the ability to segment a document into topically coherent section document classification and document likelihood estimation 
the amount of semantic data on the web ha been growing rapidly in recent year one of the key challenge triggered by this growth is the ad hoc querying i e the ability to retrieve answer from semantic resource using natural language query this facilitates interaction with semantic resource for the user so they can benefit from the knowledge covered by semantic data without the complexity of semantic query language in this paper we focus on semantic query where the aim is to retrieve object belonging to a set of semantically related entity an example of such an ad hoc type query is apollo astronaut who walked on the moon in order to address the task we propose the semsets retrieval model that exploit and combine traditional document based information retrieval link structure of the semantic data and entity membership in semantic set in order to provide the answer the novelty of the approach lie in the utilization of semantic set i e group of semantically related entity we propose two approach to identify such semantic set from the knowledge base the first one requires involvement of an expert user knowledgeable of the data set structure the second one is fully automatic and provides result that are comparable with those delivered by the expert user a demonstrated in the experimental evaluation the proposed model ha the state of the art performance on the semsearch data set which ha been designed especially for the semantic list search evaluation 
community discovery on large scale linked document corpus ha been a hot research topic for decade there are two type of link the first one which we call d d link indicates connectiveness among different document such a blog reference and research paper citation the other one which we call u u link represents co occurrence or simultaneous participation of different user in one document and typically each document from u u link corpus ha more than one user author example of u u link data cover email archive and research paper co authorship network community discovery in d d link data ha achieved much success while method for that in u u link data either make no use of the textual content of the document or make oversimplified assumption about the user and the textual content in this paper we propose a general approach of community discovery for u u link data i e multiple user data by placing topical variable on multiple author participation in document experiment on a research proceeding co authorship corpus and a new york time news corpus show the effectiveness of our model 
the primary problem confronting any new kind of search task is how to boot strap a reliable and repeatable evaluation campaign and a crowd sourcing approach provides many advantage however can these crowd sourced evaluation be repeated over long period of time in a reliable manner to demonstrate we investigate creating an evaluation campaign for the semantic search task of keyword based ad hoc object retrieval in contrast to traditional search over web page object search aim at the retrieval of information from factual assertion about real world object rather than searching over web page with textual description using the first large scale evaluation campaign that specifically target the task of ad hoc web object retrieval over a number of deployed system we demonstrate that crowd sourced evaluation campaign can be repeated over time and still maintain reliable result furthermore we show how these result are comparable to expert judge when ranking system and that the result hold over different evaluation and relevance metric this work provides empirical support for scalable reliable and repeatable search system evaluation using crowdsourcing 
a adult we take for granted our capacity to express our information need verbally and textually however young child also have preference and information need but are just learning to be able to express themselves effectively consequently they encounter many barrier when trying to spell type and communicate their need to a faceless search engine text box junior search juse is an interface that enables preschooler and young child to search and find consumable online content such a game for kid video etc through adaptable picture dictionary inspired by educational child s toy rather than search engine designed for adult juse incorporates a learning element by combining audio visual and textual cue to improve written word recognition and vocabulary skill juse provides an interactive learning environment that allows parent to introduce new word and concept into the child s lexicon a well a controlling the content and search query 
web search component such a ranking and query suggestion analyze the user data provided in query and click log while this data is easy to collect and provides information about user behavior it omits user interaction with the search engine that do not hit the server these log omit search data such a user cursor movement just a click provide signal for relevance in search result cursor hovering and scrolling can be additional implicit signal in this work we demonstrate a technique to extend model of the user s search result examination state to infer document relevance we start by exploring recorded user interaction with the search result both qualitatively and quantitatively we find that cursor hovering and scrolling are signal telling u which search result were examined and we use these interaction to reveal latent variable in searcher model to more accurately compute document attractiveness and satisfaction accuracy is evaluated by computing how well our model using these parameter can predict future click for a particular query we are able to improve the click prediction compared to a basic searcher model for higher ranked search result using the additional log data 
term weighting scheme are central to the study of information retrieval system this article proposes a novel tf idf term weighting scheme that employ two different within document term frequency normalization to capture two different aspect of term saliency one component of the term frequency is effective for short query while the other performs better on long query the final weight is then measured by taking a weighted combination of these component which is determined on the basis of the length of the corresponding query experiment conducted on a large number of trec news and web collection demonstrate that the proposed scheme almost always outperforms five state of the art retrieval model with remarkable significance and consistency the experimental result also show that the proposed model achieves significantly better precision than the existing model 
the search for entity is the most common search behavior on the web especially in social medium community where entity such a image video people location and tag are highly heterogeneous and correlated while previous research usually deal with these social medium entity separately we are investigating in this paper a unified multi level and correlative entity graph to represent the unstructured social medium data through which various application e g friend suggestion personalized image search image tagging etc can be realized more effectively in one single framework we regard the social medium object equally a entity and all of these application a entity search problem which search for entity with different type we first construct a multi level graph which organizes the heterogeneous entity into multiple level with one type of entity a vertex in each level the edge between graph pairwisely connect the entity weighted by intra relation in the same level and inter link across two different level distilled from the social behavior e g tagging commenting and joining community to infer the strength of intra relation we propose a circular propagation scheme which reinforces the mutual exchange of information across different entity type in a cyclic manner based on the constructed unified graph we explicitly formulate entity search a a global optimization problem in a unified bayesian framework in which various application are efficiently realized empirically we validate the effectiveness of our unified entity graph for various social medium application on million scale real world dataset 
existing enterprise calendaring system have suffered from problem like rigidity lack of transparency and poor integration with social network we present the system design and rationale for a novel social coordination mechanism called suggestion that address these issue our system integrates idea drawn from design of lightweight polling system and one s social network into an open calendar tool providing a space for user to coordinate socialize around or negotiate the what and the when of their event suggestion wa released inside a large enterprise setting where initial interview revealed user thought on transparent scheduling reaching wider audience and task appropriateness and suggested way to improve our design 
reading congressional legislation also known a bill is often tedious because bill tend to be long and written in complex language in ibm many bill an interactive web based visualization of legislation user of different background can browse bill and quickly explore part that are of interest to them one task user have is to be able to locate section that don t seem to fit with the overall topic of the bill in this paper we present novel technique to determine which section within a bill are likely to be outlier by employing approach from information retrieval the most promising technique first detect the most topically relevant part of a bill by ranking it section followed by a comparison between these topically relevant part and the remaining section in the bill to compare section we use various dissimilarity metric based on kullback leibler divergence the result indicate that these technique are more successful than a classification based approach finally we analyze how the dissimilarity metric succeed in discriminating between section that are strong outlier versus those that are milder outlier 
the common tag given by multiple user to a particular document are often semantically relevant to the document and each tag represents a specific topic in this paper we attempt to emulate human tagging behavior to recommend tag by considering the concept contained in document specifically we represent each document using a few most relevant concept contained in the document where the concept space is derived from wikipedia tag are then recommended based on the tag concept model derived from the annotated document of each tag evaluated on a delicious dataset of more than k document the proposed technique achieved comparable tag recommendation accuracy a the state of the art while yielding an order of magnitude speed up 
traditional recommendation algorithm often select product with the highest predicted rating to recommend however earlier research in economics and marketing indicates that a consumer usually make purchase decision s based on the product s marginal net utility i e the marginal utility minus the product price utility is defined a the satisfaction or pleasure user u get when purchasing the corresponding product a rational consumer chooses the product to purchase in order to maximize the total net utility in contrast to the predicted rating the marginal utility of a product depends on the user s purchase history and change over time according to the law of diminishing marginal utility many product have the decreasing marginal utility with the increase of purchase count such a cell phone computer and so on user are not likely to purchase the same or similar product again in a short time if they already purchased it before on the other hand some product such a pet food baby diaper would be purchased again and again to better match user purchase decision in the real world this paper explores how to recommend product with the highest marginal net utility in e commerce site inspired by the cobb douglas utility function in consumer behavior theory we propose a novel utility based recommendation framework the framework can be utilized to revamp a family of existing recommendation algorithm to demonstrate the idea we use singular value decomposition svd a an example and revamp it with the framework we evaluate the proposed algorithm on an e commerce shop com data set the new algorithm significantly improves the base algorithm largely due to it ability to recommend both product that are new to the user and product that the user is likely to re purchase 
uncovering the topic within short text such a tweet and instant message ha become an important task for many content analysis application however directly applying conventional topic model e g lda and plsa on such short text may not work well the fundamental reason lie in that conventional topic model implicitly capture the document level word co occurrence pattern to reveal topic and thus suffer from the severe data sparsity in short document in this paper we propose a novel way for modeling topic in short text referred a biterm topic model btm specifically in btm we learn the topic by directly modeling the generation of word co occurrence pattern i e biterms in the whole corpus the major advantage of btm are that btm explicitly model the word co occurrence pattern to enhance the topic learning and btm us the aggregated pattern in the whole corpus for learning topic to solve the problem of sparse word co occurrence pattern at document level we carry out extensive experiment on real world short text collection the result demonstrate that our approach can discover more prominent and coherent topic and significantly outperform baseline method on several evaluation metric furthermore we find that btm can outperform lda even on normal text showing the potential generality and wider usage of the new topic model 
versioned textual collection are collection that retain multiple version of a document a it evolves over time important large scale example are wikipedia and the web collection of the internet archive search query over such collection often use keywords a well a temporal constraint most commonly a time range of interest in this paper we study how to support such temporal range query over versioned text our goal is to process these query faster than the corresponding keyword only query by exploiting the additional constraint a simple approach might partition the index into different time range and then access only the relevant part however specialized inverted index compression technique are crucial for large versioned collection and a naive partitioning can negatively affect index size and query throughput we show how to achieve high query throughput by using smart index partitioning technique that take index compression into account experiment on over million version of wikipedia article show that query can be executed in a few millisecond on memory based index structure and only slightly more time on disk based structure we also show how to efficiently support the recently proposed stable top k search primitive on top of our scheme 
people nowadays can obtain information on current news event through medium outlet social medium and by actively seeking information using search engine in this paper we investigate the temporal relationship between news coverage by medium outlet social medium and query log and show that social medium frequently precedes other information source additionally we demonstrate that there is strong negative correlation between the probability for reporting of an event and the distance of the information source from the event 
a search page are becoming increasingly complex with image and nonlinear page layout understanding how user examine the page is important we present a lab study on the effect of a rich informational panel to the right of the search result column on eye and mouse behavior using eye and mouse data we show that the flow of user attention on nonlinear page layout is different from the widely believed top down linear examination order of search result we further demonstrate that the mouse like the eye is sensitive to two key attribute of page element their position layout and their relevance to the user s task we identify mouse measure that are strongly correlated with eye movement and develop model to predict user attention eye gaze from mouse activity these finding show that mouse tracking can be used to infer user attention and information flow pattern on search page potential application include ranking search page optimization and ui evaluation 
in recent year personalized web search pws ha demonstrated effectiveness in improving the quality of search service on the internet unfortunately the need for collecting private information in pws ha become a major barrier for it wide proliferation we study privacy protection in pws engine which capture personality in user profile we propose a pws framework called ups that can generalize profile in for each query according to user specified privacy requirement two predictive metric are proposed to evaluate the privacy breach risk and the query utility for hierarchical user profile we develop two simple but effective generalization algorithm for user profile allowing for query level customization using our proposed metric we also provide an online prediction mechanism based on query utility for deciding whether to personalize a query in ups extensive experiment demonstrate the efficiency and effectiveness of our framework 
with more than million message that are posted on twitter every day the amount of duplicate content a well a the demand for appropriate duplicate detection mechanism is increasing tremendously yet there exists little research that aim at detecting near duplicate content on microblogging platform we investigate the problem of near duplicate detection on twitter and introduce a framework that analyzes the tweet by comparing i syntactical characteristic ii semantic similarity and iii contextual information our framework provides different duplicate detection strategy that among others make use of external web resource which are referenced from microposts machine learning is exploited in order to learn pattern that help identifying duplicate content we put our duplicate detection framework into practice by integrating it into twinder a search engine for twitter stream an in depth analysis show that it allows twinder to diversify search result and improve the quality of twitter search we conduct extensive experiment in which we evaluate the quality of different strategy for detecting duplicate analyze the impact of various feature on duplicate detection investigate the quality of strategy that classify to what exact level two microposts can be considered a duplicate and optimize the process of identifying duplicate content on twitter our result prove that semantic feature which are extracted by our framework can boost the performance of detecting duplicate 
automatic query expansion technology have been proven to be effective in many information retrieval task most existing approach are based on the assumption that the most informative term in top retrieved document can be viewed a context of the query and thus can be used for query expansion one problem with these approach is that some of the expansion term extracted from feedback document are irrelevant to the query and thus may hurt the retrieval performance in social annotation user provide different keywords describing the respective web page from various aspect these feature may be used to boost ir performance however to date the potential of social annotation for this task ha been largely unexplored in this paper we explore the possibility and potential of social annotation a a new resource for extracting useful expansion term in particular we propose a term ranking approach based on social annotation resource the proposed approach consists of two phase in the first phase we propose a term dependency method to choose the most likely expansion term in the second phase we develop a machine learning method for term ranking which is learnt from the statistic of the candidate expansion term using listnet experimental result on three trec test collection show that the retrieval performance can be improved when the term ranking method is used in addition we also demonstrate that term selected by the term dependency method from social annotation resource are beneficial to improve the retrieval performance 
the importance of information retrieval ir in audio visual recording ha been increasing with steeply growing number of audio visual document available on line compared to traditional ir method this task requires specific technique such a passage retrieval which can accelerate the search process by retrieving the exact relevant passage of a recording instead of the full document in passage retrieval full recording are divided into shorter segment which serve a individual document for the further ir setup this technique also allows normalizing document length and applying positional information it wa shown that it can even improve retrieval result in this work we examine two general strategy for passage retrieval blind segmentation into overlapping regular length passage and segmentation into variable length passage based on semantics of their content time based segmentation wa already shown to improve retrieval of textual document and audio visual recording our experiment performed on the test collection used in the search subtask of the search and hyperlinking task in mediaeval benchmarking confirm those finding and show that parameter segment length and shift tuning for a specific test collection can further improve the result our best result on this collection were achieved by using second long segment with second shift semantic based segmentation can be divided into three type similarity based producing segment with high intra similarity and low inter similarity lexical chain based producing segment with frequent lexically connected word and feature based combining various feature which signalize a segment break in a machine learning setting in this work we mainly focus on feature based segmentation which allows exploiting various feature from all modality of the data including segment length in a single trainable model and produce segment which can eventually overlap our preliminary result show that even simple semantic based segmentation outperforms regular segmentation our model is a decision tree incorporating the following feature shot segment output of texttiling algorithm cue word well thanks so i now sentence break and the length of the silence after the previous word in term of the masp the relative improvement over regular segmentation is more than 
news article typically drive a lot of traffic in the form of comment posted by user on a news site such user generated content tends to carry additional information such a entity and sentiment in general when article are recommended to user only popularity e g most shared and most commented recency and sometimes manual editor pick based on daily hot topic are considered we formalize a novel recommendation problem where the goal is to find the closest most diverse article to the one the user is currently browsing our diversity measure incorporates entity and sentiment extracted from comment given the real time nature of our recommendation we explore the applicability of nearest neighbor algorithm to solve the problem our user study on real opinion article from aljazeera net and reuters com validates the use of entity and sentiment extracted from article and their comment to achieve news diversity when compared to content based diversity finally our performance experiment show the real time feasibility of our solution 
existing adaptive filtering system learn user profile based on user relevance judgment on document in some case user have some prior knowledge about what feature are important for a document to be relevant for example a spanish speaker may only want news written in spanish and thus a relevant document should contain the feature language spanish a researcher working on hiv know an article with the medical subject subject aid is very likely to be interesting to him her semi structured document with rich faceted metadata are increasingly prevalent over the internet motivated by the commonly used faceted search interface in e commerce we study whether user prior knowledge about faceted feature could be exploited for filtering semi structured document we envision two faceted feedback solicitation mechanism and propose a novel user profile learning algorithm that can incorporate user feedback on feature to evaluate the proposed work we use two data set from the trec filtering track and conduct a user study on amazon mechanical turk our experimental result show that user feedback on faceted feature is useful for filtering the new user profile learning algorithm can effectively learn from user feedback on faceted feature and performs better than several other method adapted from the feature based feedback technique proposed for retrieval and text classification task in previous work 
relevance is the central concept of information retrieval although it important role is unanimously accepted among researcher numerous different definition of the term have emerged over the year considerable effort ha been put into creating consistent and universally applicable description of relevance in the form of relevance framework across these various formal system of relevance a wide range of relevance criterion ha been identified the probably most frequently used single criterion that in some application even becomes a synonym for relevance is topicality it express a document s topical overlap with the user s information need for textual resource it is often estimated based on term co occurrence between query and document there is however a significant number of further noteworthy relevance criterion prominent specimen are currency determines how recent and up to date the document is outdated information may have become invalid over time availability express how easy it is to obtain the document user might not want to invest more than a threshold amount of resource e g disk space downloading time or money to get the document readability describes the document s readability and understandability a document with a high topical relevance towards a given information need can become irrelevant if the user is not able to extract the desired information from it credibility contains criterion such a the document author s expertise the publication s reputation and the document s general trustworthiness novelty describes the document s contribution to satisfying an information need with respect to the user s context e g previous search result or general knowledge about the domain it is evident that these criterion can have very different scope some of them are static characteristic of the document or the author others depend on the concrete information need at hand or even the user s search context currently state of the art retrieval model often treat relevance regardless which interpretation of the term wa chosen a an atomic concept that can be expressed through topical overlap between document and query or a plain linear combination of multiple score considering the broad audience a web search engine ha to serve such a method doe not seem optimal a the concrete composition of relevance will vary from person to person depending on social and educational context furthermore each individual can be expected to have situational preference for certain combination of relevance facet depending on the information need at hand we investigate combination scheme which respect the dimension specific relevance distribution in particular we developed a risk aware method of combining relevance criterion inspired by the economic portfolio theory a a first stage we applied this method for result set diversification across dimension 
in the field of music information retrieval mir multi label genre classification is the problem of assigning one or more genre label to a music piece in this work we propose a set of ensemble technique which are specific to the task of multi label genre classification our goal is to enhance classification performance by combining multiple classifier in addition we also investigate some existing ensemble technique from machine learning the effectiveness of these technique is demonstrated through a set of empirical experiment and various related issue are discussed to the best of our knowledge there ha been limited work on applying ensemble technique to multi label genre classification in the literature and we consider the result in this work a our initial effort toward this end the significance of our work ha two fold proposing a set of ensemble technique specific to music genre classification and shedding light on further research along this direction 
the textual context of an element structurally contains trace of evidence utilizing this context in scoring is called contextualization in this study we hypothesize that the context of an xml element originated from it textit preceding and textit following element in the sequential ordering of a document improves the quality of retrieval in the tree form of the document s structure textit kinship contextualization mean contextualization based on the horizontal and vertical element in the textit kinship tree or element in closer to a wider structural kinship we have tested several variant of kinship contextualization and verified notable improvement in comparison with the baseline system and gold standard in the retrieval of focused element 
there is a widespread intuitive sense that different kind of information spread differently on line but it ha been difficult to evaluate this question quantitatively since it requires a setting where many different kind of information spread in a shared environment here we study this issue on twitter analyzing the way in which token known a hashtags spread on a network defined by the interaction among twitter user we find significant variation in the way that widely used hashtags on different topic spread our result show that this variation is not attributable simply to difference in stickiness the probability of adoption based on one or more exposure but also to a quantity that could be viewed a a kind of persistence the relative extent to which repeated exposure to a hashtag continue to have significant marginal effect we find that hashtags on politically controversial topic are particularly persistent with repeated exposure continuing to have unusually large marginal effect on adoption this provides to our knowledge the first large scale validation of the complex contagion principle from sociology which posit that repeated exposure to an idea are particularly crucial when the idea is in some way controversial or contentious among other finding we discover that hashtags representing the natural analogue of twitter idiom and neologism are particularly non persistent with the effect of multiple exposure decaying rapidly relative to the first exposure we also study the subgraph structure of the initial adopter for different widely adopted hashtags again finding structural difference across topic we develop simulation based and generative model to analyze how the adoption dynamic interact with the network structure of the early adopter on which a hashtag spread 
query recommendation ha been considered a an effective way to help search user in their information seeking activity traditional approach mainly focused on recommending alternative query with close search intent to the original query however to only take relevance into account may generate redundant recommendation to user it is better to provide diverse a well a relevant query recommendation so that we can cover multiple potential search intent of user and minimize the risk that user will not be satisfied besides previous query recommendation approach mostly relied on measuring the relevance or similarity between query in the euclidean space however there is no convincing evidence that the query space is euclidean it is more natural and reasonable to assume that the query space is a manifold in this paper therefore we aim to recommend diverse and relevant query based on the intrinsic query manifold we propose a unified model named manifold ranking with stop point for query recommendation by turning ranked query into stop point on the query manifold our approach can generate query recommendation by simultaneously considering both diversity and relevance in a unified way empirical experimental result on a large scale query log of a commercial search engine show that our approach can effectively generate highly diverse a well a closely related query recommendation 
deliberate degradation of search result is a common tool in user experiment we degrade high quality search result by inserting non relevant document at different rank the effect of these manipulation on a number of commonly used metric is counter intuitive the discount function implicit in p k mrr ndcg and others do not account for the true relationship between rank and value to the user we propose an alternative based on visibility data 
we model the economics of incentivizing high quality user generated content ugc motivated by setting such a online review forum question answer site and comment on news article and blog we provide a game theoretic model within which to study the problem of incentivizing high quality ugc in which contributor are strategic and motivated by exposure our model ha the feature that both the quality of contribution a well a the extent of participation is determined endogenously in a free entry nash equilibrium the model predicts a observed in practice that if exposure is independent of quality there will be a flood of low quality contribution in equilibrium an ideal mechanism in this context would elicit both high quality and high participation in equilibrium with near optimal quality a the available attention diverges and should be easily implementable in practice we consider a very simple elimination mechanism which subject each contribution to rating by some number a of viewer and eliminates any contribution that are not uniformly rated positively we construct and analyze free entry nash equilibrium for this mechanism and show that a can be chosen to achieve quality that tends to optimal along with diverging participation a the number of viewer diverges 
search is generally a mean to the end of finishing a task while the current search engine are useful to user for finding relevant information they offer little help to user for further digesting and analyzing the overwhelming found information needed for finishing a complex task in this talk i will discus how statistical topic model can be used to help user analyze and digest the found relevant information and turn search result into actionable knowledge needed to complete a task i will present several general statistical topic model for extracting and analyzing topic and their pattern in text and show sample application of such model in task such a opinion integration comparative summarization contextual topic trend analysis and event impact analysis the talk will conclude with a discussion of novel challenge raised in extending a search engine to an analysis engine that can go beyond search to provide more complete support for user to finish their task 
one potential disadvantage of social tagging system is that due to the lack of a centralized vocabulary a crowd of user may never manage to reach a consensus on the description of resource e g book user or song on the web yet previous research ha provided interesting evidence that the tag distribution of resource may become semantically stable over time a more and more user tag them at the same time previous work ha raised an array of new question such a i how can we ass the semantic stability of social tagging system in a robust and methodical way ii doe semantic stabilization of tag vary across different social tagging system and ultimately iii what are the factor that can explain semantic stabilization in such system in this work we tackle these question by i presenting a novel and robust method which overcomes a number of limitation in existing method ii empirically investigating semantic stabilization process in a wide range of social tagging system with distinct domain and property and iii detecting potential cause for semantic stabilization specifically imitation behavior shared background knowledge and intrinsic property of natural language our result show that tagging stream which are generated by a combination of imitation dynamic and shared background knowledge exhibit faster and higher semantic stability than tagging stream which are generated via imitation dynamic or natural language phenomenon alone 
we present a practical off path tcp injection attack for connection between current non buggy browser and web server the attack allows web cache poisoning with malicious object these object can be cached for long time period exposing any user of that cache to x csrf and phishing attack in contrast to previous tcp injection attack we assume neither vulnerability such a client malware nor predictable choice of client port or ip id we only exploit subtle detail of http and tcp specification and feature of legitimate and common browser implementation an empirical evaluation of our technique with current version of browser show that connection with popular website are vulnerable our attack is modular and it module may improve other off path attack on tcp communication we present practical patch against the attack however the best defense is surely adoption of tl that ensures security even against the stronger man in the middle attacker 
relevance assessment are a key component for test collection based evaluation of information retrieval system this paper report on a feature of such collection that is used a a form of ground truth data to allow analysis of human assessment error a wide range of test collection are retrospectively examined to determine how accurately assessor judge the relevance of document our result demonstrate a high level of inconsistency across the collection studied the level of irregularity is shown to vary across topic with some showing a very high level of assessment error we investigate possible influence on the error and demonstrate that inconsistency in judging increase with time while the level of detail in a topic specification doe not appear to influence the error that assessor make judgement are significantly affected by the decision made on previously seen similar document assessor also display an assessment inertia alternate approach to generating relevance judgement appear to reduce error a further investigation of the way that retrieval system are ranked using set of relevance judgement produced early and late in the judgement process reveals a consistent influence measured across the majority of examined test collection we conclude that there is a clear value in examining even inserting ground truth data in test collection and propose way to help minimise the source of inconsistency when creating future test collection 
in this paper we present a contribution to ir modeling we propose an approach that computes on the fly a personalized social document representation psdr of each document per user based on his social activity the psdrs are used to rank document with respect to a query this approach ha been intensively evaluated on a large public dataset showing significant benefit for personalized search 
there usually exist many news article written in different language about a hot news event the news article in different language are written in different way to reflect different standpoint for example the chinese news agency and the western news agency have published many article to report the same news of liu xiaobo s nobel prize in chinese and english language respectively the chinese news article and the english news article share something about the news fact in common but they focus on different aspect in order to reflect different standpoint about the event in this paper we investigate the task of multilingual news summarization for the purpose of finding and summarizing the major difference between the news article about the same event in the chinese and english language we propose a novel constrained co ranking c corank method for addressing this special task the c corank method add the constraint between the difference score and the common score of each sentence to the co ranking process evaluation result on the manually labeled test set with news topic show the effectiveness of our proposed method and the constrained co ranking method can outperform a few baseline and the typical co ranking method 
in this paper we tackle the problem of top n context aware recommendation for implicit feedback scenario we frame this challenge a a ranking problem in collaborative filtering cf much of the past work on cf ha not focused on evaluation metric that lead to good top n recommendation list in designing recommendation model in addition previous work on context aware recommendation ha mainly focused on explicit feedback data i e rating we propose tfmap a model that directly maximizes mean average precision with the aim of creating an optimally ranked list of item for individual user under a given context tfmap us tensor factorization to model implicit feedback data e g purchase click with contextual information the optimization of map in a large data collection is computationally too complex to be tractable in practice to address this computational bottleneck we present a fast learning algorithm that exploit several intrinsic property of average precision to improve the learning efficiency of tfmap and to ensure it scalability we experimentally verify the effectiveness of the proposed fast learning algorithm and demonstrate that tfmap significantly outperforms state of the art recommendation approach 
aggregated search interface provide user with an overview of result from various source two general type of display exist tabbed with access to each source in a separate tab and blended which combine multiple source into a single result page multi session search task e g a research project consist of multiple stage each with it own sub task several factor involved in multi session search task have been found to influence user search behavior we investigate whether user preference for source presentation change during a multi session search task the dynamic nature of multi session search task make the design of a controlled experiment a non trivial challenge we adopt a methodology based on triangulation and conduct two type of observational study a longitudinal study and a laboratory study in the longitudinal study we follow the use of tabbed and blended display by student during a project we find that while a tabbed display is used more than a blended display subject repeatedly switch between display during the project use of the tabbed display is motivated by a need to zoom in on a specific source while the blended display is used to explore available material across source whenever the information need change in a laboratory study student completed a multi session search task composed of three sub task the first with a tabbed display the second and third with blended display the task were manipulated by either providing three task about the same topic or about three different topic we find that a stable information need over multiple sub task negatively influence perceived usability of the blended display while we do not find an influence when the information need change 
in this paper we use query level regression a the loss function the regression loss function ha been used in pointwise method however pointwise method ignore the query boundary and treat the data equally across query and thus the effectiveness is limited we show that regression is an effective loss function for learning to rank when used in query level we propose a method namely listreg to use neural network to model the ranking function and gradient descent for optimization experimental result show that listreg significantly outperforms pointwise regression and the state of the art listwise method in most case 
many website encourage user participation via the use of virtual reward like badge while badge typically have no explicit value they act a symbol of social status within a community in this paper we study how to design virtual incentive mechanism that maximize total contribution to a website when user are motivated by social status we consider a game theoretic model where user exert costly effort to make contribution and in return are awarded with badge the value of a badge is determined endogenously by the number of user who earn an equal or higher badge a more user earn a particular badge the value of that badge diminishes for all user we show that among all possible mechanism for assigning status driven reward the optimal mechanism is a leaderboard with a cutoff user that contribute le than a certain threshold receive nothing while the remaining are ranked by contribution we next study the necessary feature of approximately optimal mechanism and find that approximate optimality is influenced by the the convexity of status valuation when status valuation are concave any approximately optimal mechanism must contain a coarse status partition i e a partition of user into status class whose size will grow a the population grows conversely when status valuation are convex we prove that fine partitioning that is a partition of user into status class whose size stay constant a the population grows is necessary for approximate optimality 
