a consultant system s main task is to provided helpful advice to the user consultant system should not only find solution to user problem but should also inform the user of potential problem with these solution expressing such potential caveat is a difficult process due to the many potential plan failure for each particular plan in a particular planning situation a commonsense planner called kip knowledge intensive planner is described kip is the planner for the unix consultant system kip detect potential plan failure using a new knowledge structure termed a concern concern allow kip to detects plan failure due to unsatisfied condition or goal conflict kip s concern algorithm also is able to provide information to the expression mechanism regarding potential plan failure concern information is passed to the expression mechanism when kip s selected plan might not work in this case kip pass information regarding both the suggested plan and the potential caveat in that plan to the expression mechanism this is an efficient approach since kip must make such decision in the context of it planning process a concern s declarative structure make it easier to express than procedural description of plan failure used by earlier system 
we present a semantic and pragmatic account of the anaphoric property of past and perfect that improves on previous work by integrating discourse structure aspectual type surface structure and commonsense knowledge a novel aspect of our account is that we distinguish between two kind of temporal interval in the interpretation of temporal operator discourse reference interval and event interval this distinction make it possible to develop an analogy between centering and temporal centering which operates on discourse reference interval our is a defeasible inference rule on the logical form along with lexical and causal reasoning it play a role in incrementally resolving underspecified aspect of the event structure representation of an utterance against the current context 
we describe a series of three experiment in which supervised learning technique were used to acquire three different type of grammar for english news story the acquired grammar type were context free context dependent and probabilistic context free training data were derived from university of pennsylvania treebank par of wall street journal article in each case the system started with essentially no grammatical knowledge and learned a set of grammar rule exclusively from the training data performance for each grammar type wa then evaluated on an independent set of test sentence using parseval a standard measure of parsing accuracy these experimental result yield a direct quantitative comparison between each of the three method 
hobbs j r m e stickel d e appelt and p martin interpretation a abduction artificial intelligence abduction is inference to the best explanation in the tacitus project at sri we have developed an approach to abductive inference called weighted abduction that ha resulted in a significant simplification of how the problem of interpreting text is conceptualized the interpretation of a text is the minimal explanation of why the text would be true more precisely to interpret a text one must prove the logical form of the text from what is already mutually known allowing for coercion merging redundancy where possible and making assumption where necessary it is shown how such local pragmatic problem a reference resolution the interpretation of compound nominal the resolution of syntactic ambiguity and metonymy and schema recognition can be solved in this manner moreover this approach of interpretation a abduction can be combined with the older view of parsing a deduction to produce an elegant and thorough integration of syntax semantics and pragmatic one that span the range of linguistic phenomenon from phonology to discourse structure finally we discus mean for making the abduction process efficient possibility for extending the approach to other pragmatic phenomenon and the semantics of the weight and cost in the abduction scheme 
morphotactics and allomorphy are usually modeled in different component leading to interface problem to describe both uniformly we define finite automaton fa for allomorphy in the same feature description language used for morphotactics nonphonologically conditioned allomorphy is problematic in fa model but submits readily to treatment in a uniform formalism 
this paper compare the account based on a simple feature extension to lambek categorial grammar lcg we show that the lcg treatment account for construction that have been recognized a problematic for unification based treatment 
dtg are designed to share some of the advantage of tag while overcoming some of it limitation dtg involve two composition operation called subsertion and sister adjunction the most distinctive feature of dtg is that unlike tag there is complete uniformity in the way that the two dtg operation relate lexical item subsertion always corresponds to complementation and sister adjunction to modification furthermore dtg unlike tag can provide a uniform analysis for element in kashmiri appears in sentence second position and not sentence initial position a in english 
this paper introduces to the calculus of regular expression a replace operator and defines a set of replacement expression that concisely encode alternate variation of the operation replace expression denote regular relation defined in term of other regular expression operator the basic case is unconditional obligatory replacement we develop several version of conditional replacement that allow the operation to be constrained by context 
this paper describes a method of detecting speech repair that us a part of speech tagger the tagger is given knowledge about category transition for speech repair and so is able to mark a transition either a a likely repair or a fluent speech other contextual clue such a editing term word fragment and word matchings are also factored in by modifying the transition probability 
it is genermly agreed that coherent li ourse consists of segment tha t are related to one another a number of researcher have a rgm d lbr the use of rhetorica l ri or coherence relation i iol and the rhetorical relation specified by r s i mt have i e used in structuring text tlov mp in this l a l el we exa mine rh q ori al relation in the cold xt of dia logue ra tlmr than single speaker te t we argue that rea s millg about relational l r lmsit ion is necessary but not sufficient ior structuring lialogtm oiht out s v q al prol h m of a pplyiug rst to dialogue a nd argue tbr the necessity of recognizillg the intention underlying utteratlces a lld i he rich relationshil s among these intention our research on recognizing expression of hml t and interpreting ilmirect reply provides evidence that what moore a nd polla ck call i lfformal itmal level rela ti m ml play a n illlporta llt role in identil ing intention ill lia h gue r llsi l l s me ontiilua l i s of i h iblh wing liah gm sequence 
this paper describes a heuristic based approach to word sense disambiguation the heuristic that are applied to disambiguate a word depend on it part of speech and on it relationship to neighboring salient word in the text part of speech are found through a tagger and related neighboring word are identified by a phrase extractor operating on the tagged text to suggest possible sens each heuristic draw on semantic relation extracted from a webster s dictionary and the semantic thesaurus wordnet for a given word all applicable heuristic are tried and those sens that are rejected by all heuristic are discarded in all the disambiguator us heuristic based on relationship 
a consultant system s main task is to provided helpful advice to the user consultant system should not only find solution to user problem but should also inform the user of potential problem with these solution expressing such potential caveat is a difficult process due to the many potential plan failure for each particular plan in a particular planning situation a commonsense planner called kip knowledge intensive planner is described kip is the planner for the unix consultant system kip detect potential plan failure using a new knowledge structure termed a concern concern allow kip to detects plan failure due to unsatisfied condition or goal conflict kip s concern algorithm also is able to provide information to the expression mechanism regarding potential plan failure concern information is passed to the expression mechanism when kip s selected plan might not work in this case kip pass information regarding both the suggested plan and the potential caveat in that plan to the expression mechanism this is an efficient approach since kip must make such decision in the context of it planning process a concern s declarative structure make it easier to express than procedural description of plan failure used by earlier system 
we present a semantic and pragmatic account of the anaphoric property of past and perfect that improves on previous work by integrating discourse structure aspectual type surface structure and commonsense knowledge a novel aspect of our account is that we distinguish between two kind of temporal interval in the interpretation of temporal operator discourse reference interval and event interval this distinction make it possible to develop an analogy between centering and temporal centering which operates on discourse reference interval our is a defeasible inference rule on the logical form along with lexical and causal reasoning it play a role in incrementally resolving underspecified aspect of the event structure representation of an utterance against the current context 
we describe a series of three experiment in which supervised learning technique were used to acquire three different type of grammar for english news story the acquired grammar type were context free context dependent and probabilistic context free training data were derived from university of pennsylvania treebank par of wall street journal article in each case the system started with essentially no grammatical knowledge and learned a set of grammar rule exclusively from the training data performance for each grammar type wa then evaluated on an independent set of test sentence using parseval a standard measure of parsing accuracy these experimental result yield a direct quantitative comparison between each of the three method 
hobbs j r m e stickel d e appelt and p martin interpretation a abduction artificial intelligence abduction is inference to the best explanation in the tacitus project at sri we have developed an approach to abductive inference called weighted abduction that ha resulted in a significant simplification of how the problem of interpreting text is conceptualized the interpretation of a text is the minimal explanation of why the text would be true more precisely to interpret a text one must prove the logical form of the text from what is already mutually known allowing for coercion merging redundancy where possible and making assumption where necessary it is shown how such local pragmatic problem a reference resolution the interpretation of compound nominal the resolution of syntactic ambiguity and metonymy and schema recognition can be solved in this manner moreover this approach of interpretation a abduction can be combined with the older view of parsing a deduction to produce an elegant and thorough integration of syntax semantics and pragmatic one that span the range of linguistic phenomenon from phonology to discourse structure finally we discus mean for making the abduction process efficient possibility for extending the approach to other pragmatic phenomenon and the semantics of the weight and cost in the abduction scheme 
morphotactics and allomorphy are usually modeled in different component leading to interface problem to describe both uniformly we define finite automaton fa for allomorphy in the same feature description language used for morphotactics nonphonologically conditioned allomorphy is problematic in fa model but submits readily to treatment in a uniform formalism 
this paper compare the account based on a simple feature extension to lambek categorial grammar lcg we show that the lcg treatment account for construction that have been recognized a problematic for unification based treatment 
dtg are designed to share some of the advantage of tag while overcoming some of it limitation dtg involve two composition operation called subsertion and sister adjunction the most distinctive feature of dtg is that unlike tag there is complete uniformity in the way that the two dtg operation relate lexical item subsertion always corresponds to complementation and sister adjunction to modification furthermore dtg unlike tag can provide a uniform analysis for element in kashmiri appears in sentence second position and not sentence initial position a in english 
this paper introduces to the calculus of regular expression a replace operator and defines a set of replacement expression that concisely encode alternate variation of the operation replace expression denote regular relation defined in term of other regular expression operator the basic case is unconditional obligatory replacement we develop several version of conditional replacement that allow the operation to be constrained by context 
this paper describes a method of detecting speech repair that us a part of speech tagger the tagger is given knowledge about category transition for speech repair and so is able to mark a transition either a a likely repair or a fluent speech other contextual clue such a editing term word fragment and word matchings are also factored in by modifying the transition probability 
it is genermly agreed that coherent li ourse consists of segment tha t are related to one another a number of researcher have a rgm d lbr the use of rhetorica l ri or coherence relation i iol and the rhetorical relation specified by r s i mt have i e used in structuring text tlov mp in this l a l el we exa mine rh q ori al relation in the cold xt of dia logue ra tlmr than single speaker te t we argue that rea s millg about relational l r lmsit ion is necessary but not sufficient ior structuring lialogtm oiht out s v q al prol h m of a pplyiug rst to dialogue a nd argue tbr the necessity of recognizillg the intention underlying utteratlces a lld i he rich relationshil s among these intention our research on recognizing expression of hml t and interpreting ilmirect reply provides evidence that what moore a nd polla ck call i lfformal itmal level rela ti m ml play a n illlporta llt role in identil ing intention ill lia h gue r llsi l l s me ontiilua l i s of i h iblh wing liah gm sequence 
this paper describes a heuristic based approach to word sense disambiguation the heuristic that are applied to disambiguate a word depend on it part of speech and on it relationship to neighboring salient word in the text part of speech are found through a tagger and related neighboring word are identified by a phrase extractor operating on the tagged text to suggest possible sens each heuristic draw on semantic relation extracted from a webster s dictionary and the semantic thesaurus wordnet for a given word all applicable heuristic are tried and those sens that are rejected by all heuristic are discarded in all the disambiguator us heuristic based on relationship 
this paper present a computational model of how conversational participant collaborate in order to make a referring action successful the model is based on the view of language a goal directed behavior we propose that the content of a referring expression can be accounted for by the planning paradigm not only doe this approach allow the process of building referring expression and identifying their referent to be captured by plan construction and plan inference it also allows u to account for how participant clarify a referring expression by using meta action that reason about and manipulate the plan derivation that corresponds to the referring expression to account for how clarification goal arise and how inferred clarification plan affect the agent we propose that the agent are in a certain state of mind and that this state includes an intention to achieve the goal of referring and a plan that the agent are currently considering it is this mental state that sanction the adoption of goal and the acceptance of inferred plan and so act a a link between understanding and generation 
semantic cluster of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation several attempt have been made to extract the semantic cluster of a domain by probabilistic or taxonomic technique however not much progress ha been made in evaluating the obtained semantic cluster this paper focus on an evaluation mechanism that can be used to evaluate semantic cluster produced by a system against those provided by human expert 
some linguistic constraint cannot be effectively resolved during parsing at the location in which they are most naturally introduced this paper show how constraint can be propagated in a memoizing parser such a a chart parser in much the same way that variable binding are providing a general treatment of constraint coroutining in memoization prolog code for a simple application of our technique to bouma and van noord s categorial grammar analysis of dutch is provided 
lexicalized context free grammar lcfg is an attractive compromise between the parsing efficiency of context free grammar cfg and the elegance and lexical sensitivity of lexicalized tree adjoining grammar ltag lcfg is a restricted form of ltag that can only generate context free language and can be parsed in cubic time however lcfg support much of the elegance of ltag s analysis of english and share with ltag the ability to lexicalize cfgs without changing the tree generated 
when human tutor engage in dialogue they freely exploit all aspect of the mutually known context including the previous discourse utterance that do not draw on previous discourse seem awkward unnatural or even incoherent previous discourse must be taken into account in order to relate new information effectively to recently conveyed material and to avoid repeating old material that would distract the student from what is new producing a system that display such behavior involves finding an efficient way to identify which previous explanation if any are relevant to the current explanation task thus we are implementing a system that us a case based reasoning approach to identify previous situation and explanation that could potentially affect the explanation being constructed we have identified heuristic for constructing explanation that exploit this information in way similar to what we have observed in human human tutorial dialogue 
we propose a plan based approach for respondingto user query in a collaborative environment weargue that in such an environment the system shouldnot accept the user s query automatically but shouldconsider it a proposal open for negotiation in this paperwe concentrate on case in which the system anduser disagree and discus how this disagreement canbe detected negotiated and how final modificationsshould be made to the existing plan introductionin task oriented 
this paper present a model for generating prosodically appropriate synthesized response to database query using combinatory categorial grammar ccg cf a formalism which easily integrates the notion of syntactic constituency prosodic phrasing and information structure the model determines accent location within phrase on the basis of contrastive set derived from the discourse structure and a domain independent knowledge base 
we propose a distinction between two kind of metonymy referential metonymy in which the referent of an np is shifted and predicative metonymy in which the referent of the np is unchanged and the argument place of the predicate is shifted instead example are respectively the hamburger is waiting for his check and which airline fly from boston to denver we also show that complication arise for both type of metonymy when multiple coercing predicate are considered finally we present implemented algorithm handling these complexity that generate both type of metonymic reading a well a criterion for choosing one type of metonymic reading over another 
we propose an algorithm ha been proposed the algorithm presented in this paper improves the run time of the recent result using an entirely different approach 
we extract from sentence a superstructure made of argumentative operator and connective applying to the remaining set of terminal sub sentence we found the argumentative interpretation of utterance on a semantics defined at the linguistic level we describe the computation of this particular semantics based on the constraint that the superstructure impels to the argumentative power of terminal subsentences 
the functionality of system that extract information from text can be specified quite simply the input is a stream of text and the output is some representation of the information to be extracted hence the problem of template design is an instance of the problem of knowledge representation in particular it is the problem of representing essential fact about situation in a way that can mediate between text that describe those situation and a variety of application that involve reasoning about them the research on which we report here is directed at elucidating principle of template design and at compiling these with example in a manual for template designer 
abstract this paper present texttiling a method for partitioning full length text document into coherent multiparagraph unit the layout of text tile is meant to reflect the pattern of subtopics contained in an expository text the approach us lexical analysis based on tf idf an information retrieval measurement to determine the extent of the tile incorporating thesaural information via a statistical disambiguation algorithm the tile have been found to correspond well to human judgement of the major subtopic boundary of science magazine article 
interpreting fully natural speech is an important goal for spoken language understanding system however while corpus study have shown that about of spontaneous utterance contain self correction or repair little is known about the extent to which cue in the speech signal may facilitate repair processing we identify several cue based on acoustic and prosodic analysis of repair in a corpus of spontaneous speech and propose method for exploiting these cue to detect and correct repair we test our acoustic prosodic cue with other lexical cue to repair identification and find that precision rate of and recall of can be achieved depending upon the cue employed from a prosodically labeled corpus 
in this paper we describe a fast algorithm for aligning sentence with their translation in a bilingual corpus existing efficient algorithm ignore word identity and only consider sentence length brown et al b gale and church our algorithm construct a simple statistical word to word translation model on the fly during alignment we find the alignment that maximizes the probability of generating the corpus with this translation model we have achieved an error rate of approximately on canadian hansard data which is a significant improvement over previous result the algorithm is language independent 
we present an implemented unification based parser for relational grammar developed within the stratified feature grammar sfg framework which generalizes kasper round logic to handle relational grammar analysis we first introduce the key aspect of sfg and a lexicalized graph based variant of the framework suitable for implementing relational grammar we then describe a head driven chart parser for lexicalized sfg the basic parsing operation is essentially ordinary feature structure unification augmented with an operation of label unification to build the stratified feature characteristic of sfg 
suppose we have a feature system and we wish to add default value in a well defined way we might start with kasper round logic and use reiter s example to form it into a default logic giving a node a default value would be equivalent to saying if it is consistent for this node to have that value then it doe then we could use default theory to describe feature structure the particular feature structure described would be the structure that support the extension of the default theory this is in effect what the theory of nonmonotonic sort give you this paper describes how that theory derives from what is described above 
we examine the consistency problem for description of tree based on remote dominance and present a consistency checking algorithm which is polynomial in the number of node in the description despite disjunction inherent in the theory of tree the resulting algorithm allows for description which go beyond set of atomic formula to allow certain type of disjunction and negation 
we present a program for segmenting text according to the separate event they describe a modular architecture is described that allows u to examine the contribution made by particular aspect of natural language to event structuring this is applied in the context of terrorist news article and a technique is suggested for evaluating the resulting segmentation we also examine the usefulness of various heuristic in forming these segmentation 
motivation for including relational constraint other than equality within grammatical formalism ha come from discontinuous constituency and partially free word order for natural language a well a from the need to define combinatory operation at the most basic level for language with a two dimensional syntax e g mathematical notation chemical equation and various diagramming language this paper present f patr a generalization of the patr ii unification based formalism which incorporates relational constraint expressed a user defined function an operational semantics is given for unification that is an adaptation and extension of the approach taken by ait kaci and nasr it is designed particularly for unification based formalism implemented in functional programming environment such a lisp the application of unification in a chart parser for relational set language is discussed briefly 
we describe a method for evaluating a grammar checking application with hand bracketed par a randomly selected set of sentence wa submitted to a grammar checker in both bracketed and unbracketed format a comparison of the resulting error report illuminates the relationship between the underlying performance of the parser grammar system and the error critique presented to the user 
we apply smith s theory of aspect to german a language without any aspectual marker in particular we try to shed more light on the effect aspect can have on discourse structure and show how english and german behave differently in this respect we furthermore describe how smith s notion of a neutral viewpoint can be helpful for the analysis of discourse in german it turned out that proposal claiming that the german preterite cover the progressive a well a the simple aspect can not sufficiently explain the data presented in this paper b uerle finally we give a situation theoretic approach to formalize smith s intuition following glasbey incorporating allen s interval calculus allen 
an automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank a new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research community the simple algorithm achieves conversion accuracy of when tested on sentence between two major grammar revision of a large mt system 
in this paper we discus the different strategy used in comet coordinated multimedia explanation testbed for selecting word with which the user is familiar when picture cannot be used to disambiguate a word or phrase comet ha four strategy for avoiding unknown word we give example for each of these strategy and show how they are implemented in comet 
the penn treebank ha recently implemented a new syntactic annotation scheme designed to highlight aspect of predicate argument structure this paper discus the implementation of crucial aspect of this new annotation scheme it incorporates a more consistent treatment of a wide range of grammatical phenomenon provides a set of coindexed null element in what can be thought of a underlying position for phenomenon such a wh movement passive and the subject of infinitival construction provides some non context free annotational mechanism to allow the structure of discontinuous constituent to be easily recovered and allows for a clear concise tagging system for some semantic role 
in this paper we present a method to group adjective according to their meaning a a first step towards the automatic identification of adjectival scale we discus the property of adjectival scale and of group of semantically related adjective and how they imply source of linguistic knowledge in text corpus we describe how our system exploit this linguistic knowledge to compute a measure of similarity between two adjective using statistical technique and without having access to any semantic information about the adjective we also show how a clustering algorithm can use these similarity to produce the group of adjective and we present result produced by our system for a sample set of adjective we conclude by presenting evaluation method for the task at hand and analyzing the significance of the result obtained 
we present an efficient procedure for cost based abduction which is based on the idea of using chart parser a proof procedure we discus in detail three feature of our algorithm goal driven bottom up derivation tabulation of the partial result and agenda control mechanism and report the result of the preliminary experiment which show how these feature improve the computational efficiency of cost based abduction 
this paper present a method for inducing the part of speech of a language and part of speech label for individual word from a large text corpus vector representation for the part of speech of a word are formed from entry of it near lexical neighbor a dimensionality reduction creates a space representing the syntactic category of unambiguous word a neural net trained on these spatial representation classifies individual context of occurrence of ambiguous word the method classifies both ambiguous and unambiguous word correctly with high accuracy 
in the first part of the paper i present a new treatment of the imperfective paradox dowty for the restricted case of trajectory of motion event this treatment extends and refines those of moens and steedman and jackendoff in the second part i describe an implemented algorithm based on this treatment which determines whether a specified sequence of such event is or is not possible under certain situationally supplied constraint and restrictive assumption 
various feature description are being employed in logic programming language and constraint based grammar formalism the common notational primitive of these description are functional attribute called feature the description considered in this paper are the possibly quantified first order formula obtained from a signature of binary and unary predicate called feature and sort respectively we establish a first order theory ft by mean of three axiom scheme show it completeness and construct three elementarily equivalent model one of the model consists of the so called feature graph a data structure common in computational linguistics the other two model consist of the so called feature tree a recordlike data structure generalizing the tree corresponding to first order term our completeness proof exhibit a terminating simplification system deciding validity and satisfiability of possibly quantified feature description 
various feature description are being employed in constrained based grammar formalism the common notational primitive of these description are functional attribute called feature the description considered in this paper are the possibly quantified first order formula obtained from a signature of feature and sort we establish a complete first order theory by mean of three axiom scheme and construct three elementarily equivalent model one of the model consists of so called feature graph a data structure common in computational linguistics the other two model consist of so called feature tree a record like data structure generalizing the tree corresponding to first order term our completeness proof exhibit a terminating simplification system deciding validity and satisfiability of possibly quantified feature description 
there is a need to develop a suitable computational grammar formalism for free word order language for two reason first a suitably designed formalism is likely to be more efficient second such a formalism is also likely to be linguistically more elegant and satisfying in this paper we describe such a formalism called the paninian framework that ha been successfully applied to indian language this paper show that the paninian framework applied to modern indian language give an elegant account of the relation between surface form vibhakti and semantic karaka role the mapping is elegant and compact the same basic account also explains active passive and complex sentence this suggests that the solution is not just adhoc but ha a deeper underlying unity a constraint based parser is described for the framework the constraint problem reduces to bipartite graph matching problem because of the nature of constraint efficient solution are known for these problem it is interesting to observe that such a parser designed for free word order language compare well in asymptotic time complexity with the parser for context free grammar cfgs which are basically designed for positional language 
the paper demonstrates that exponential complexity with respect to grammar size and input length have little impact on the performance of three unification based parsing algorithm using a wide coverage grammar the result imply that the study and optimisation of unification based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parser 
this paper present an unsupervised learning algorithm for sense disambiguation that when trained on unannotated english text rival the performance of supervised technique that require time consuming hand annotation the algorithm is based on two powerful constraint that word tend to have one sense per discourse and one sense per collocation exploited in an iterative bootstrapping procedure tested accuracy exceeds 
a variety of statistical method for nouncompound analysis are implemented andcompared the result support two mainconclusions first the use of conceptualassociation not only enables a broad coverage but also improves the accuracy second an analysis model based on dependencygrammar is substantially more accuratethan one based on deepest constituent even though the latter is more preva lent in the literature 
we describe the design of comlex syntax a computational lexicon providing detailed syntactic information for approximately english headword we consider the type of error which arise in creating such a lexicon and how such error can be measured and controlled 
the purpose of this paper is to suggest that quantifier in natural language do not have a fixed truth functional meaning a ha long been held in logical semantics instead we suggest that quantifier can best be modeled a complex inference procedure that are highly dynamic and sensitive to the linguistic context a well a time and memory constraint 
we attemped to improve recognition accuracy by reducing the inadequacy of the lexicon and language model specifically we address the following three problem the best size for the lexicon conditioning written text for spoken language recognition and using additional training outside the text distribution we found that increasing the lexicon word to word reduced the percentage of word outside the vocabulary from over to just thereby decreasing the error rate substantially the error rate on word already in the vocabulary did not increase substantially we modified the language model training text by applying rule to simulate the difference between the training text and what people actually said finally we found that using another three year of training text even without the appropriate preprocessing substantially improved the language model we also tested these approach on spontaneous news dictation and found similar improvement 
i propose that the characteristic of the scope disambiguation process observed in the literature can be explained in term of the way in which the model of the situation described by a sentence is built the model construction procedure i present build an event structure by identifying the situation associated with the operator in the sentence and their mutual dependency relation a well a the relation between these situation and other situation in the context the procedure take into account lexical semantics and the result of various discourse interpretation procedure such a definite description interpretation and doe not require a complete disambiguation to take place 
this paper describes the problem faced while using kimmo s two level model to describe certain indian language such a tamil and hindi the two level model is shown to be descriptively inadequate to address these problem a simple extension to the basic two level model is introduced which allows conflicting phonological rule to coexist the computational complexity of the extension is the same a kimmo s two level model 
we apply smith s theory of aspect to german a language without any aspectual marker in particular we try to shed more light on the effect aspect can have on approach to formalize smith s intuition following glasbey incorporating allen s interval calculus allen 
this paper present a method for learning phonological rule from sample pair of underlyingand surface form without negative evidence the learned rule are represented asfinite state transducer that accept underlying form a input and generate surface form asoutput the algorithm for learning them is an extension of the ostia algorithm for learninggeneral subsequential finite state transducer although ostia is capable of learningarbitrary s f s t s in the limit large dictionary 
most probabilistic classifier used for word sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependency among multiple contextual feature in this paper a different approach to formulating a probabilistic model is presented along with a case study of the performance of model produced in this manner for the disambiguation of the noun interest we describe a method for formulating probabilistic model that use multiple contextual feature for word sense disambiguation without requiring untested assumption regarding the form of the model using this approach the joint distribution of all variable is described by only the most systematic variable interaction thereby limiting the number of parameter to be estimated supporting computational efficiency and providing an understanding of the data 
an automatic compound retrieval method is proposed to extract compound within a text message it us n gram mutual information relative frequency count and part of speech a the feature for compound extraction the problem is modeled a a two class classification problem based on the distributional characteristic of n gram token in the compound and the non compound cluster the recall and precision using the proposed approach are and for bigram compound and and for trigram compound for a testing corpus of word a significant cutdown in processing time ha been observed 
we describe one approach to build an automatically trainable anaphora resolution system in this approach we use japanese newspaper article tagged with discourse information a training example for a machine learning algorithm which employ the c decision tree algorithm by quinlan quinlan then we evaluate and compare the result of several variant of the machine learning based approach with those of our existing anaphora resolution system which us manually designed knowledge source finally we compare our algorithm with existing theory of anaphora in particular japanese zero pronoun 
eric brill ha recently proposed a simple and powerful corpus based language modeling approach that can be applied to various task including part of speech tagging and building phrase structure tree the method learns a series of symbolic transformational rule which can then be applied in sequence to a test corpus to produce prediction the learning process only requires counting match for a given set of rule template allowing the method to survey a very large space of possible contextual factor this paper analysis brill s approach a an interesting variation on existing decision tree method based on experiment involving part of speech tagging for both english and ancient greek corpus in particular the analysis throw light on why the new mechanism seems surprisingly resistant to overtraining a fast incremental implementation and a mechanism for recording the dependency that underlie the resulting rule sequence are also described 
a number of grammatical formalism were introduced to define the syntax of natural language among them are parallel multiple context free grammar pmcfg s and lexical functional grammar lfg s pmcfg s and their subclass called multiple context free grammar mcfg s are natural extension of cfg s and pmcfg s are known to be recognizable in polynomial time some subclass of lfg s have been proposed but they were shown to generate an np complete language finite state translation system ft were introduced a a computational model of transformational grammar in this paper three subclass of lfg s called nc lfg s dc lfg s and fc lfg s are introduced and the generative capacity of the above mentioned grammatical formalism are investigated first we show that the generative capacity of ft is equal to that of nc lfg s a relation among subclass of those formalism it is shown that the generative capacity of deterministic ft dc lfg s and pmcfg s are equal to each other and the generative capacity of fc lfg s is equal to that of mcfg s it is also shown that at least one np complete language is generated by ft consequently deterministic ft dc lfg s and fc lfg s can be recognized in polynomial time however ft and nc lfg s cannot if p np 
we propose an o m n time algorithm for the recognition of tree adjoining language tals where n is the size of the input string and m k is the time needed to multiply two k x k boolean matrix tree adjoining grammar tag are formalism suitable for natural language processing and have received enormous attention in the past among not only natural language processing researcher but also algorithm designer the first polynomial time algorithm for tal parsing wa proposed in and had a run time of o n quite recently an o n m n algorithm ha been proposed the algorithm presented in this paper improves the run time of the recent result using an entirely different approach 
to achieve reasonable accuracy in large vocabulary speech recognition system it is important to use detailed acoustic model together with good long span language model for example in the wall street journal wsj task both cross word triphones and a trigram language model are necessary to achieve state of the art performance however when using these model the size of a pre compiled recognition network can make a standard viterbi search infeasible and hence either multiple pas or asynchronous stack decoding scheme are typically used in this paper we show that time synchronous one pas decoding using cross word triphones and a trigram language model can be implemented using a dynamically built tree structured network this approach avoids the compromise inherent in using fast match or preliminary pass and is relatively efficient in implementation it wa included in the htk large vocabulary speech recognition system used for the arpa wsj evaluation and experimental result are presented for that task 
this paper describes a new discourse module within our multilingual nlp system because of it unique data driven architecture the discourse module is language independent moreover the use of hierarchically organized multiple knowledge source make the module robust and trainable using discourse tagged corpus separating discourse phenomenon from knowledge source make the discourse module easily extensible to additional phenomenon 
this paper present a massively parallel parser that predicts critical attachment behavior of the human sentence processor without the use of explicit preference heuristic or revision strategy the processing of a syntactic ambiguity is modeled a an active distributed competition among the potential attachment for a phrase computationally motivated constraint on the competitive mechanism provide a principled and uniform account of a range of human attachment preference and garden path phenomenon 
we propose a novel approach to extraposition in german within an alternative conception of syntax in which syntactic structure and linear order are mediated not via encoding of hierarchical relation but instead via order domain at the heart of our proposal is a new kind of domain formation which affords analysis of extraposition construction that are linguistically more adequate than those previously suggested in the literature 
the computational lexicalization of a grammar is the optimization of the link between lexicalized rule and lexical item in order to improve the quality of the bottom up filtering during parsing this problem is np complete and untractable on large grammar an approximation algorithm is presented the quality of the suboptimal solution is evaluated on real world grammar a well a on randomly generated one 
this paper deal with the automatic translation of route description into graphic sketch we discus some general problem implied by such inter mode transcription we propose a model for an automatic text to image translator with a two stage intermediate representation in which the linguistic representation of a route description precedes the creation of it conceptual representation 
the goal of information extraction task is to identify categorize classify relate and normalize specific information of interest found in free text and to make that information available to a back end data base data fusion or other application a data structure referred to a a that have emerged these desideratum feed into the discussion of design element and a procedural review of the design process design iteration use of those linguistic analysis tool etc 
this paper present a new model of anaphoric processing that utilizes the establishment of coherence relation between clause in a discourse we survey data that comprises a currently stalemated argument over whether vp ellipsis is an inherently syntactic or inherently semantic phenomenon and show that the data can be handled within a uniform discourse processing architecture this architecture which revise the dichotomy between ellipsis v model interpretive anaphora given by sag and hankamer is also able to accommodate divergent theory and data for pronominal reference resolution the resulting architecture serf a a baseline system for modeling the role of cohesive device in natural language 
we present a new approach to disambiguating syntactically ambiguous word in context based on variable memory markov vmm model in contrast to fixed length markov model which predict based on fixed lenth history variable memory markov model dynamically adapt their history length based on the training data and hence may use fewer parameter in a test of a vmm based tagger on the brown corpus of token are correctly classified 
in this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it a the basis for an algorithm to track discourse context and bind pronoun a described in gjw the process of centering attention on entity in the discourse give rise to the intersentential transitional state of we propose an extension to these state which handle some additional case of multiple ambiguous pronoun the algorithm ha been implemented in an hpsg natural language system which serf a the interface to a database query application 
the paper describes an algorithm that employ english and french text tagger to associate noun phrase in an aligned bilingual corpus the tagger provide part of speech category which are used by finite state recognizers to extract simple noun phrase for both language noun phrase are then mapped to each other using an iterative re estimation algorithm that bear similarity to the baum welch algorithm which is used for training the tagger the algorithm provides an alternative to other approach for finding word correspondence with the advantage that linguistic structure is incorporated improvement to the basic algorithm are described which enable context to be accounted for when constructing the noun phrase mapping 
this paper address the alignment issue in the framework of exploitation of large bimultilingual corpus for translation purpose a generic alignment scheme is proposed that can meet varying requirement of different application depending on the level at which alignment is sought appropriate surface linguistic information is invoked coupled with information about possible unit delimiters each text unit sentence clause or phrase is represented by the sum of it content tag the result are then fed into a dynamic programming framework that computes the optimum alignment of unit the proposed scheme ha been tested at sentence level on parallel corpus of the celex database the success rate exceeded the next step of the work concern the testing of the scheme s efficiency at lower level endowed with necessary bilingual information about potential delimiters 
in the minimalist program chomsky it is assumed that there are different type of projection lexical and functional and therefore different type of head this paper explains why functional head are not treated a head corner by the minimalist head corner parser described here 
evans and gazdar evans and gazdar a evans and gazdar b introduced datr a a simple non monotonic language for representing natural language lexicon although a number of implementation of datr exist the full language ha until now lacked an explicit declarative semantics this paper rectifies the situation by providing a mathematical semantics for datr we present a view of datr a a language for defining certain kin of partial function by case the formal model provides a transparent treatment of datr s notion of global context it is shown that datr s default mechanism can be accounted for by interpreting value descriptor a family of value indexed by path 
we present an implemented compilation algorithm that translates hpsg into lexicalized feature based tag relating concept of the two theory while hpsg ha a more elaborated principle based theory of possible phrase structure tag provides the mean to represent lexicalized structure more explicitly our objective are met by giving clear definition that determine the projection of structure from the lexicon and identify maximal projection auxiliary tree and foot node 
a system wolfie that acquires a mapping of word to their semantic representation is presented and a preliminary evaluation is performed tree least general generalization tlggs of the representation of input sentence are performed to assist in determining the representation of individual word in the sentence the best guess for a meaning of a word is the tlgg which overlap with the highest percentage of sentence representation in which that word appears some promising experimental result on a non artificial data set are presented 
we describe and evaluate experimentally a method for clustering word according to their distribution in particular syntactic context word are represented by the relative frequency distribution of context in which they appear and relative entropy between those distribution is used a the similarity measure for clustering cluster are represented by average context distribution derived from the given word according to their probability of cluster membership in many case the cluster can be thought of a encoding coarse sense distinction deterministic annealing is used to find lowest distortion set of cluster a the annealing parameter increase existing cluster become unstable and subdivide yielding a hierarchical soft clustering of the data cluster are used a the basis for class model of word coocurrence and the model evaluated with respect to held out test data 
we describe and evaluate experimentally a method for clustering word according to their distribution in particular syntactic context word are represented by the relative frequency distribution of context in which they appear and relative entropy between those distribution is used a the similarity measure for clustering cluster are represented by average context distribution derived from the given word according to their probability of cluster membership in many case the cluster can be thought of a encoding coarse sense distinction deterministic annealing is used to find lowest distortion set of cluster a the annealing parameter increase existing cluster become unstable and subdivide yielding a hierarchical soft clustering of the data cluster are used a the basis for class model of word coocurrence and the model evaluated with respect to held out test data 
interactive spoken dialog provides many new challenge for spoken language system one of the most critical is the prevalence of speech repair this paper present an algorithm that detects and corrects speech repair based on finding the repair pattern the repair pattern is built by finding word match and word replacement and identifying fragment and editing term rather than using a set of prebuilt template we build the pattern on the fly in a the fair test our method when combined with a statistical model to filter possible repair wa successful at detecting and correcting of the repair without using prosodic information or a parser 
this paper examines the current performance of the stochastic tagger part church in handling phrasal verb describes a problem that arises from the statistical model used and suggests a way to improve the tagger s performance the solution involves a change in the definition of what count a a word for the purpose of tagging phrasal verb 
chinese sentence are written with no special delimiters such a space to indicate word boundary existing chinese nlp system therefore employ preprocessors to segment sentence into word contrary to the conventional wisdom of separating this issue from the task of sentence understanding we propose an integrated model that performs word boundary identification in lockstep with sentence understanding in this approach there is no distinction between rule for word boundary identification and rule for sentence understanding these two function are combined word boundary ambiguity are detected especially the fallacious one when they block the primary task of discovering the inter relationship among the various constituent of a sentence which essentially is the essence of the understanding process in this approach statistical information is also incorporated providing the system a quick and fairly reliable starting ground to carry out the primary task of relationship building 
overgeneration is the main source of computational complexity in previous principle based parser this paper present a message passing algorithm for principle based parsing that avoids the overgeneration problem this algorithm ha been implemented in c and successfully tested with example sentence from van riemsdijk and williams 
the goal of the pangloss project are to investigate and develop a new generation knowledge based interlingual machine translation system combining symbolic and statistical technique the system is to translate newspaper text in arbitrary domain though a specific financial domain is given preference to a high quality a possible using a little human intervention a possible 
our goal is to identify the feature that predict cue selection and placement in order to devise strategy for automatic text generation much previous work in this area ha relied on ad hoc method our coding scheme for the exhaustive analysis of discourse allows a systematic evaluation and refinement of hypothesis concerning cue we report two result based on this analysis a comparison of the distribution of since and because in our corpus and the impact of embeddedness on cue selection 
we present a corpus based study of method that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjective solution to this problem are applicable to the more general task of selecting the positive term from the pair using automatically collected data the accuracy and applicability of each method is quantified and a statistical analysis of the significance of the result is performed we show that some simple method are indeed good indicator for the answer to the problem while other proposed method fail to perform better than would be attributable to chance in addition one of the simplest method text frequency dominates all others we also apply two generic statistical learning method for combining the indication of the individual method and compare their performance to the simple method the most sophisticated complex learning method offer a small but statistically significant improvement over the original test 
we show that feature logic extended by functional uncertainty is decidable even if one admits cyclic description we present an algorithm which solves feature description containing functional uncertainty in two phase both phase using a set of deterministic and non deterministic rewrite rule we then compare our algorithm with the one of kaplan and maxwell that doe not cover cyclic feature description 
a the first step in an automated text summarization algorithm this work present a new method for automatically identifying the central idea in a text based on a knowledge based concept counting paradigm to represent and generalize concept we use the hierarchical concept taxonomy wordnet by setting appropriate cutoff value for such parameter a concept generality and child to parent frequency ratio we control the amount and level of generality of concept extracted from the text 
we describe a grammarless method for simultaneously bracketing both half of a parallel text and giving word alignment assuming only a translation lexicon for the language pair we introduce inversion invariant transduction grammar which serve a generative model for parallel bilingual sentence with weak order constraint focusing on transduction grammar for bracketing we formulate a normal form and a stochastic version amenable to a maximum likelihood bracketing algorithm several extension and experiment are discussed 
there have been a number of recent paper on aligning parallel text at the sentence level e g brown et al gale and church to appear isabelle kay and r senschein to appear simard et al warwick armstrong and russell on clean input such a the canadian hansard these method have been very successful at least correct by sentence unfortunately if the input is noisy due to ocr and or unknown markup convention then these method tend to break down because the noise can make it difficult to find paragraph boundary let alone sentence this paper describes a new program char align that aligns text at the character level rather than at the sentence paragraph level based on the cognate approach proposed by simard et al 
the relevance of context in disambiguating natural language input ha been widely acknowledged in the literature however most attempt at formalising the intuitive notion of context tend to treat the word and it context symmetrically we demonstrate here that traditional measure such a mutual information score are likely to overlook a significant fraction of all co occurrence phenomenon in natural language we also propose metric for measuring directed lexical influence and compare performance 
this paper outline the linguistic semantic commitment underlying an application which automatically construct depiction of verbal spatial description our approach draw on the ideational view of linguistic semantics developed by ronald langacker in his theory of cognitive grammar and the conceptual representation of physical object from the two level semantics of bierwisch and lang in particular the dimension of the process of conventional imagery are used a a metric for the design of our own conceptual representation 
we describe and evaluate hidden understanding model a statistical learning approach to natural language understanding given a string of word hidden understanding model determine the most likely meaning for the string we discus the problem of representing meaning in this framework the structure of the statistical model the process of training the model and the process of understanding using the model finally we give experimental result including result on an arpa evaluation 
this paper describes a nethod for finding structuralmatching between parallel sentence of twolauguages such a japanese and english par allel sentence are analyzed based on unificationgrammars and structural matching is performedby making use of a similarity measure of word pairsin the two language syntactic ambiguity are resolvedsimultaneously in the matching process theresults serve a a useful source for extracting linguisticand lexical knowledge 
this paper describes a method for finding structural matching between parallel sentence of two language such a japanese and english parallel sentence are analyzed based on unification grammar and structural matching is performed by making use of a similarity measure of word pair in the two language syntactic ambiguity are resolved simultaneously in the matching process the result serve a a useful source for extracting linguistic and lexical knowledge 
this paper present a cooperative consultation system on a restricted domain the system build hypothesis on the user s plan and avoids misunderstanding with consequent repair dialogue through clarification dialogue in case of ambiguity the role played by constraint in the generation of the answer is characterized in order to limit the case of ambiguity requiring a clarification dialogue the answer of the system are generated at different level of detail according to the user s competence in the domain 
in this paper we discus the result of experiment which use a context essentially an ordered set of lexical item a the seed from which to build a network representing statistically important relationship among lexical item in some corpus a metric is then applied to the node in the network in order to discover those pair of item related by high index of similarity the goal of this research is to instantiate a class of item corresponding to each item in the priming context we believe that this instantiation process is ultimately a special case of abstraction over the entire network in this abstraction similar node are collapsed into metanodes which may then function a if they were single lexical item 
this paper report work in progress on a sentence generation model which attempt to emulate certain language output pattern of child between the age of one and one half and three year in particular the model address the issue of why missing or phonetically null subject appear a often a they do in the speech of young english speaking child it will also be used to examine why other pattern of output appear in the speech of child learning language such a italian and chinese initial finding are that an output generator successfully approximates the null subject output pattern found in english speaking child by using a processing overload metric alone however reference to several parameter related to discourse orientation and agreement morphology is necessary in order to account for the differing pattern of null argument appearing cross linguistically based on these finding it is argued that the null subject phenomenon is due to the combined effect of limited processing capacity and early accurate parameter setting 
current mt system whatever translation method they at present employ do not reach an optimum output on free text our hypothesis for the experiment reported in this paper is that if an mt environment can use the best result from a variety of mt system working simultaneously on the same text the overall quality will improve using this novel approach to mt in the latest version of the pangloss mt project we submit an input text to a battery of machine translation system engine collect their possibly incomplete result in a joint chart like data structure and select the overall best translation using a set of simple heuristic this paper describes the simple mechanism we use for combining the finding of the various translation engine 
combinatory categorial grammar ccgs steedman have been shown by weir and joshi to generate the same class of language a tree adjoining grammar tag head grammar hg and linear indexed grammar lig in this paper i will discus the effect of using variable in lexical category assignment in ccgs it will be shown that using variable in lexical category can increase the weak generative capacity of ccgs beyond the class of grammar listed above 
machine translation mt ha recently been formulated in term of constraint based knowledge representation and unification theory but it is becoming more and more evident that it is not possible to design a practical mt system without an adequate method of handling mismatch between semantic representation in the source and target language in this paper we introduce the idea of information based mt which is considerably more flexible than interlingual mt or the conventional transfer based mt 
this paper we take this argument one sma step further a nd suggest a way these two levelsmight be organized into a stratified strltcture our discussion here ha s a very ha trow tbcus a nd doesnot attempt to answer such iml orta nt question a s whether the list collection of l resenta tionalrelations is exhaustive and adequate lbr describing a possible intentiola l structure 
in this paper we describe a new technique for parsing free text a transformational grammar is automatically learned that is capable of accurately parsing text into binary branching syntactic tree with nonterminals unlabelled the algorithm work by beginning in a very naive state of knowledge about phrase structure by repeatedly comparing the result of bracketing in the current state to proper bracketing provided in the training corpus the system learns a set of simple structural transformation that can be applied to reduce error after describing the algorithm we present result and compare these result to other recent result in automatic grammar induction 
this paper show how to formally characterize language learning in a finite parameter space a a markov structure important new language learning result follow directly explicitly calculated sample complexity learning time under different input distribution assumption inclding childes database language input and learning regime we also briefly describe a new way to formally model rapid diachronic syntax change 
we describe a computational framework for a grammar architecture in which different linguistic domain such a morphology syntax and semantics are treated not a separate component but compositional domain the framework is based on combinatory categorial grammar and it us the morpheme a the basic building block of the categorial lexicon 
this paper present an algorithm for learning the probability of optional phonological rule from corpus the algorithm is based on using a speech recognition system to discover the surface pronunciation of word in speech corpus using an automatic system obviates expensive phonetic labeling by hand we describe the detail of our algorithm and show the probability the system ha learned for ten common phonological rule which model reduction and coarticulation effect these probability were derived from a corpus of sentence of read speech from the wall street journal and are shown to be a reasonably close match to probability from phonetically hand transcribed data timit finally we analyze the probability difference between rule use in male versus female speech and suggest that the difference are caused by differing average rate of speech 
in this paper we present some novel application of explanation based learning ebl technique to parsing lexicalized tree adjoining grammar the novel aspect are a immediate generalization of par in the training set b generalization over recursive structure and c representation of generalized par a finite state transducer a highly impoverished parser called a stapler ha also been introduced we present experimental result using ebl for different corpus and architecture to show the effectiveness of our approach 
this paper proposes a new indicator of text structure called the lexical cohesion profile lcp which locates segment boundary in a text a text segment is a coherent scene the word in a segment are linked together via lexical cohesion relation lcp record mutual similarity of word in a sequence of text the similarity of word which represents their cohesiveness is computed using a semantic network comparison with the text segment marked by a number of subject show that lcp closely correlate with the human judgment lcp may provide valuable information for resolving anaphora and ellipsis 
thematic knowledge is a basis of semantic interpretation in this paper we propose an acquisition method to acquire thematic knowledge by exploiting syntactic clue from training sentence the syntactic clue which may be easily collected by most existing syntactic processor reduce the hypothesis space of the thematic role the ambiguity may be further resolved by the evidence either from a trainer or from a large corpus a set of heuristic based on linguistic constraint is employed to guide the ambiguity resolution process when a trainer is available the system generates new sentence whose thematic validity can be justified by the trainer when a large corpus is available the thematic validity may be justified by observing the sentence in the corpus using this way a syntactic processor may become a thematic recognizer by simply deriving it thematic knowledge from it own syntactic knowledge 
this research characterizes the spontaneous spoken disfluency typical of human computer interaction and present a predictive model accounting for their occurrence data were collected during three empirical study in which people spoke or wrote to a highly interactive simulated system the study involved within subject factorial design in which input modality and presentation format were varied spoken disfluency rate during human computer interaction were documented to be substantially lower than rate typically observed during comparable human human speech two separate factor both associated with increased planning demand were statistically related to increased speech disfluency rate length of utterance and lack of structure in the presentation format regression technique revealed that a linear model based simply on utterance length account for over of the variability in spoken disfluency therefore design technique capable of channeling user speech into briefer sentence potentially could eliminate most spoken disfluency in addition the degree of structure in the presentation format wa manipulated in a manner that successfully eliminated to of all disfluent speech the long term goal of this research is to provide empirical guidance for the design of robust spoken language technology 
an approach is described for supplying selectional restriction to parser in natural language interface nlis to database by extracting the selectional restriction from semantic description of those nlis automating the process of finding selectional restriction reduces nli development time and may avoid error introduced by hand coding selectional restriction 
efficient natural language generation ha been successfully demonstrated using highly compiled knowledge about speech act and their related social action a design and prototype implementation of a parser which utilizes this same pragmatic knowledge to efficiently guide parsing is presented such guidance is shown to prune the search space and thus avoid needle processing of pragmatically unlikely constituent structure 
project listen is developing a novel weapon against illiteracy an automated reading coach that display a story on a computer screen listens to a child read it aloud and help where needed the coach provides a combination of reading and listening in which the child read wherever possible and the coach help wherever necessary we demonstrated a prototype of this coach at the arpa workshop on human language technology in march a short video show the coach in action 
this paper present a new method for producing a dictionary of subcategorization frame from unlabelled text corpus it is shown that statistical filtering of the result of a finite state parser running on the output of a stochastic tagger produce high quality result despite the error rate of the tagger and the parser further it is argued that this method can be used to learn all subcategorization frame whereas previous method are not extensible to a general solution to the problem 
corpus based sense disambiguation method like most other statistical nlp approach suffer from the problem of data sparseness in this paper we describe an approach which overcomes this problem using dictionary definition using the definition based conceptual co occurrence data collected from the relatively small brown corpus our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information 
