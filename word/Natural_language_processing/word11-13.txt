in this work we tackle the task of machine translation mt without parallel training data we frame the mt problem a a decipherment task treating the foreign text a a cipher for english and present novel method for training translation model from non parallel text 
we offer a simple effective and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking herbrich et al unlike the popular mert algorithm och our pairwise ranking optimization pro method is not limited to a handful of parameter and can easily handle system with thousand of feature moreover unlike recent approach built upon the mira algorithm of crammer and singer watanabe et al chiang et al b pro is easy to implement it us off the shelf linear binary classifier software and can be built on top of an existing mert framework in a matter of hour we establish pro s scalability and effectiveness by comparing it to mert and mira and demonstrate parity on both phrase based and syntax based system in a variety of language pair using large scale data scenario 
writing comment about news article blog or review have become a popular activity in social medium in this paper we analyze reader comment about review analyzing review comment is important because review only tell the experience and evaluation of reviewer about the reviewed product or service comment on the other hand are reader evaluation of review their question and concern clearly the information in comment is valuable for both future reader and brand this paper proposes two latent variable model to simultaneously model and extract these key piece of information the result also enable classification of comment accurately experiment using amazon review comment demonstrate the effectiveness of the proposed model 
in this paper we describe a novel approach to cascaded learning and inference on sequence we propose a weakly joint learning model on cascaded inference on sequence called multilayer sequence labeling in this model inference on sequence is modeled a cascaded decision however the decision on a sequence labeling sequel to other decision utilizes the feature on the preceding result a marginalized by the probabilistic model on them it is not novel itself but our idea central to this paper is that the probabilistic model on succeeding labeling are viewed a indirectly depending on the probabilistic model on preceding analysis we also propose two type of efficient dynamic programming which are required in the gradient based optimization of an objective function one of the dynamic programming algorithm resembles back propagation algorithm for multilayer feed forward neural network the other is a generalized version of the forward backward algorithm we also report experiment of cascaded part of speech tagging and chunking of english sentence and show effectiveness of the proposed method 
we present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guideline for parsing several type of transformation pattern tp are designed to capture the systematic annotation inconsistency among different tree bank based on such tps we design quasi synchronous grammar feature to augment the baseline parsing model our approach can significantly advance the state of the art parsing accuracy on two widely used target tree bank penn chinese treebank and using the chinese dependency treebank a the source treebank the improvement are respectively and with automatic part of speech tag moreover an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion 
most statistical machine translation system rely on composed rule rule that can be formed out of smaller rule in the grammar though this practice improves translation by weakening independence assumption in the translation model it nevertheless result in huge redundant grammar making both training and decoding inefficient here we take the opposite approach where we only use minimal rule those that cannot be formed out of other rule and instead rely on a rule markov model of the derivation history to capture dependency between minimal rule large scale experiment on a state of the art tree to string translation system show that our approach lead to a slimmer model a faster decoder yet the same translation quality measured using b a composed rule 
in active dual supervision not only informative example but also feature are selected for labeling to build a high quality classifier with low cost however how to measure the informativeness for both example and feature on the same scale ha not been well solved in this paper we propose a non negative matrix factorization based approach to address this issue we first extend the matrix factorization framework to explicitly model the corresponding relationship between feature class and example class then by making use of the reconstruction error we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled empirical result demonstrate the effectiveness of our proposed method 
with a few exception discriminative training in statistical machine translation smt ha been content with tuning weight for large feature set on small development data evidence from machine learning indicates that increasing the training sample size result in better prediction the goal of this paper is to show that this common wisdom can also be brought to bear upon smt we deploy local feature for scfg based smt that can be read off from rule at runtime and present a learning algorithm that applies l l regularization for joint feature selection over distributed stochastic learning process we present experiment on learning on million training sentence and show significant improvement over tuning discriminative model on small development set 
recent work on bilingual word sense disambiguation wsd ha shown that a resource deprived language l can benefit from the annotation work done in a resource rich language l via parameter projection however this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible instead we focus on the situation where there are two resource deprived language both having a very small amount of seed annotated data and a large amount of untagged data we then use bilingual bootstrapping wherein a model trained using the seed annotated data of l is used to annotate the untagged data of l and vice versa using parameter projection the untagged instance of l and l which get annotated with high confidence are then added to the seed data of the respective language and the above process is repeated our experiment show that such a bilingual bootstrapping algorithm when evaluated on two different domain with small seed size using hindi l and marathi l a the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost 
stanford dependency are widely used in natural language processing a a semantically oriented representation commonly generated either by i converting the output of a constituent parser or ii predicting dependency directly previous comparison of the two approach for english suggest that starting from constituent yield higher accuracy in this paper we re evaluate both method for chinese using more accurate dependency parser than in previous work our comparison of performance and efficiency across seven popular open source parser four constituent and three dependency show by contrast that recent higher order graph based technique can be more accurate though somewhat slower than constituent parser we demonstrate also that n way jackknifing is a useful technique for producing automatic rather than gold part of speech tag to train chinese dependency parser finally we analyze the relation produced by both kind of parsing and suggest which specific parser to use in practice 
if unsupervised morphological analyzer could approach the effectiveness of supervised one they would be a very attractive choice for improving mt performance on low resource inflected language in this paper we compare performance gain for state of the art supervised v unsupervised morphological analyzer using a state of the art arabic to english mt system we apply maximum marginal decoding to the unsupervised analyzer and show that this yield the best published segmentation accuracy for arabic while also making segmentation output more stable our approach give an relative bleu gain for levantine dialectal arabic furthermore it give higher gain for modern standard arabic msa a measured on nist mt than doe mada habash and rambow a leading supervised msa segmenter 
preposition and conjunction are two of the largest remaining bottleneck in parsing across various existing parser these two category have the lowest accuracy and mistake made have consequence for down stream application preposition and conjunction are often assumed to depend on lexical dependency for correct resolution a lexical statistic based on the training set only are sparse unlabeled data can help ameliorate this sparsity problem by including unlabeled data feature into a factorization of the problem which match the representation of preposition and conjunction we achieve a new state of the art for english dependency with correct attachment on the current standard furthermore conjunction are attached with an accuracy of and preposition with an accuracy of 
spelling correction for keyword search query is challenging in restricted domain such a personal email or desktop search due to the scarcity of query log and due to the specialized nature of the domain for that task this paper present an algorithm that is based on statistic from the corpus data rather than the query log this algorithm which employ a simple graph based approach can incorporate different type of data source with different level of reliability e g email subject v email body and can handle complex spelling error like splitting and merging of word an experimental study show the superiority of the algorithm over existing alternative in the email domain 
this paper present a higher order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree experiment on english and chinese treebanks confirm it advantage over it first order version it achieves it best f score of and on the two language respectively and further push them to and via combination with other high performance parser 
in this paper we encode topic dependency in hierarchical multi label text categorization tc by mean of rerankers we represent reranking hypothesis with several innovative kernel considering both the structure of the hierarchy and the probability of node additionally to better investigate the role of category relationship we consider two interesting case i traditional scheme in which node father include all the document of their child category and ii more general scheme in which child can include document not belonging to their father the extensive experimentation on reuters corpus volume show that our rerankers inject effective structural semantic dependency in multi classifier and significantly outperform the state of the art 
in social psychology it is generally accepted that one discloses more of his her personal information to someone in a strong relationship we present a computational framework for automatically analyzing such self disclosure behavior in twitter conversation our framework us text mining technique to discover topic emotion sentiment lexical pattern a well a personally identifiable information pii and personally embarrassing information pei our preliminary result illustrate that in relationship with high relationship strength twitter user show significantly more frequent behavior of self disclosure 
automated grammar correction technique have seen improvement over the year but there is still much room for increased performance current correction technique mainly focus on identifying and correcting a specific type of error such a verb form misuse or preposition misuse which restricts the correction to a limited scope we introduce a novel technique based on a noisy channel model which can utilize the whole sentence context to determine proper correction we show how to use the em algorithm to learn the parameter of the noise model using only a data set of erroneous sentence given the proper language model this free u from the burden of acquiring a large corpus of corrected sentence we also present a cheap and efficient way to provide automated evaluation result for grammar correction by using bleu and meteor in contrast to the commonly used manual evaluation 
this paper present a two step approach to compress spontaneous spoken utterance in the first step we use a sequence labeling method to determine if a word in the utterance can be removed and generate n best compressed sentence in the second step we use a discriminative training approach to capture sentence level global information from the candidate and rerank them for evaluation we compare our system output with multiple human reference our result show that the new feature we introduced in the first compression step improve performance upon the previous work on the same data set and reranking is able to yield additional gain especially when training is performed to take into account multiple reference 
a central topic in natural language processing is the design of lexical and syntactic feature suitable for the target application in this paper we study convolution dependency tree kernel for automatic engineering of syntactic and semantic pattern exploiting lexical similarity we define efficient and powerful kernel for measuring the similarity between dependency structure whose surface form of the lexical node are in part or completely different the experiment with such kernel for question classification show an unprecedented result e g of error reduction of the former state of the art additionally semantic role classification confirms the benefit of semantic smoothing for dependency kernel 
in this work we tackle the task of machine translation mt without parallel training data we frame the mt problem a a decipherment task treating the foreign text a a cipher for english and present novel method for training translation model from non parallel text 
we offer a simple effective and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking herbrich et al unlike the popular mert algorithm och our pairwise ranking optimization pro method is not limited to a handful of parameter and can easily handle system with thousand of feature moreover unlike recent approach built upon the mira algorithm of crammer and singer watanabe et al chiang et al b pro is easy to implement it us off the shelf linear binary classifier software and can be built on top of an existing mert framework in a matter of hour we establish pro s scalability and effectiveness by comparing it to mert and mira and demonstrate parity on both phrase based and syntax based system in a variety of language pair using large scale data scenario 
writing comment about news article blog or review have become a popular activity in social medium in this paper we analyze reader comment about review analyzing review comment is important because review only tell the experience and evaluation of reviewer about the reviewed product or service comment on the other hand are reader evaluation of review their question and concern clearly the information in comment is valuable for both future reader and brand this paper proposes two latent variable model to simultaneously model and extract these key piece of information the result also enable classification of comment accurately experiment using amazon review comment demonstrate the effectiveness of the proposed model 
in this paper we describe a novel approach to cascaded learning and inference on sequence we propose a weakly joint learning model on cascaded inference on sequence called multilayer sequence labeling in this model inference on sequence is modeled a cascaded decision however the decision on a sequence labeling sequel to other decision utilizes the feature on the preceding result a marginalized by the probabilistic model on them it is not novel itself but our idea central to this paper is that the probabilistic model on succeeding labeling are viewed a indirectly depending on the probabilistic model on preceding analysis we also propose two type of efficient dynamic programming which are required in the gradient based optimization of an objective function one of the dynamic programming algorithm resembles back propagation algorithm for multilayer feed forward neural network the other is a generalized version of the forward backward algorithm we also report experiment of cascaded part of speech tagging and chunking of english sentence and show effectiveness of the proposed method 
we present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guideline for parsing several type of transformation pattern tp are designed to capture the systematic annotation inconsistency among different tree bank based on such tps we design quasi synchronous grammar feature to augment the baseline parsing model our approach can significantly advance the state of the art parsing accuracy on two widely used target tree bank penn chinese treebank and using the chinese dependency treebank a the source treebank the improvement are respectively and with automatic part of speech tag moreover an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion 
most statistical machine translation system rely on composed rule rule that can be formed out of smaller rule in the grammar though this practice improves translation by weakening independence assumption in the translation model it nevertheless result in huge redundant grammar making both training and decoding inefficient here we take the opposite approach where we only use minimal rule those that cannot be formed out of other rule and instead rely on a rule markov model of the derivation history to capture dependency between minimal rule large scale experiment on a state of the art tree to string translation system show that our approach lead to a slimmer model a faster decoder yet the same translation quality measured using b a composed rule 
in active dual supervision not only informative example but also feature are selected for labeling to build a high quality classifier with low cost however how to measure the informativeness for both example and feature on the same scale ha not been well solved in this paper we propose a non negative matrix factorization based approach to address this issue we first extend the matrix factorization framework to explicitly model the corresponding relationship between feature class and example class then by making use of the reconstruction error we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled empirical result demonstrate the effectiveness of our proposed method 
with a few exception discriminative training in statistical machine translation smt ha been content with tuning weight for large feature set on small development data evidence from machine learning indicates that increasing the training sample size result in better prediction the goal of this paper is to show that this common wisdom can also be brought to bear upon smt we deploy local feature for scfg based smt that can be read off from rule at runtime and present a learning algorithm that applies l l regularization for joint feature selection over distributed stochastic learning process we present experiment on learning on million training sentence and show significant improvement over tuning discriminative model on small development set 
recent work on bilingual word sense disambiguation wsd ha shown that a resource deprived language l can benefit from the annotation work done in a resource rich language l via parameter projection however this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible instead we focus on the situation where there are two resource deprived language both having a very small amount of seed annotated data and a large amount of untagged data we then use bilingual bootstrapping wherein a model trained using the seed annotated data of l is used to annotate the untagged data of l and vice versa using parameter projection the untagged instance of l and l which get annotated with high confidence are then added to the seed data of the respective language and the above process is repeated our experiment show that such a bilingual bootstrapping algorithm when evaluated on two different domain with small seed size using hindi l and marathi l a the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost 
stanford dependency are widely used in natural language processing a a semantically oriented representation commonly generated either by i converting the output of a constituent parser or ii predicting dependency directly previous comparison of the two approach for english suggest that starting from constituent yield higher accuracy in this paper we re evaluate both method for chinese using more accurate dependency parser than in previous work our comparison of performance and efficiency across seven popular open source parser four constituent and three dependency show by contrast that recent higher order graph based technique can be more accurate though somewhat slower than constituent parser we demonstrate also that n way jackknifing is a useful technique for producing automatic rather than gold part of speech tag to train chinese dependency parser finally we analyze the relation produced by both kind of parsing and suggest which specific parser to use in practice 
if unsupervised morphological analyzer could approach the effectiveness of supervised one they would be a very attractive choice for improving mt performance on low resource inflected language in this paper we compare performance gain for state of the art supervised v unsupervised morphological analyzer using a state of the art arabic to english mt system we apply maximum marginal decoding to the unsupervised analyzer and show that this yield the best published segmentation accuracy for arabic while also making segmentation output more stable our approach give an relative bleu gain for levantine dialectal arabic furthermore it give higher gain for modern standard arabic msa a measured on nist mt than doe mada habash and rambow a leading supervised msa segmenter 
preposition and conjunction are two of the largest remaining bottleneck in parsing across various existing parser these two category have the lowest accuracy and mistake made have consequence for down stream application preposition and conjunction are often assumed to depend on lexical dependency for correct resolution a lexical statistic based on the training set only are sparse unlabeled data can help ameliorate this sparsity problem by including unlabeled data feature into a factorization of the problem which match the representation of preposition and conjunction we achieve a new state of the art for english dependency with correct attachment on the current standard furthermore conjunction are attached with an accuracy of and preposition with an accuracy of 
spelling correction for keyword search query is challenging in restricted domain such a personal email or desktop search due to the scarcity of query log and due to the specialized nature of the domain for that task this paper present an algorithm that is based on statistic from the corpus data rather than the query log this algorithm which employ a simple graph based approach can incorporate different type of data source with different level of reliability e g email subject v email body and can handle complex spelling error like splitting and merging of word an experimental study show the superiority of the algorithm over existing alternative in the email domain 
this paper present a higher order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree experiment on english and chinese treebanks confirm it advantage over it first order version it achieves it best f score of and on the two language respectively and further push them to and via combination with other high performance parser 
in this paper we encode topic dependency in hierarchical multi label text categorization tc by mean of rerankers we represent reranking hypothesis with several innovative kernel considering both the structure of the hierarchy and the probability of node additionally to better investigate the role of category relationship we consider two interesting case i traditional scheme in which node father include all the document of their child category and ii more general scheme in which child can include document not belonging to their father the extensive experimentation on reuters corpus volume show that our rerankers inject effective structural semantic dependency in multi classifier and significantly outperform the state of the art 
in social psychology it is generally accepted that one discloses more of his her personal information to someone in a strong relationship we present a computational framework for automatically analyzing such self disclosure behavior in twitter conversation our framework us text mining technique to discover topic emotion sentiment lexical pattern a well a personally identifiable information pii and personally embarrassing information pei our preliminary result illustrate that in relationship with high relationship strength twitter user show significantly more frequent behavior of self disclosure 
automated grammar correction technique have seen improvement over the year but there is still much room for increased performance current correction technique mainly focus on identifying and correcting a specific type of error such a verb form misuse or preposition misuse which restricts the correction to a limited scope we introduce a novel technique based on a noisy channel model which can utilize the whole sentence context to determine proper correction we show how to use the em algorithm to learn the parameter of the noise model using only a data set of erroneous sentence given the proper language model this free u from the burden of acquiring a large corpus of corrected sentence we also present a cheap and efficient way to provide automated evaluation result for grammar correction by using bleu and meteor in contrast to the commonly used manual evaluation 
this paper present a two step approach to compress spontaneous spoken utterance in the first step we use a sequence labeling method to determine if a word in the utterance can be removed and generate n best compressed sentence in the second step we use a discriminative training approach to capture sentence level global information from the candidate and rerank them for evaluation we compare our system output with multiple human reference our result show that the new feature we introduced in the first compression step improve performance upon the previous work on the same data set and reranking is able to yield additional gain especially when training is performed to take into account multiple reference 
a central topic in natural language processing is the design of lexical and syntactic feature suitable for the target application in this paper we study convolution dependency tree kernel for automatic engineering of syntactic and semantic pattern exploiting lexical similarity we define efficient and powerful kernel for measuring the similarity between dependency structure whose surface form of the lexical node are in part or completely different the experiment with such kernel for question classification show an unprecedented result e g of error reduction of the former state of the art additionally semantic role classification confirms the benefit of semantic smoothing for dependency kernel 
unsupervised word representation are very useful in nlp task both a input to learning algorithm and a extra word feature in nlp system however most of these model are built with only local context and one representation per word this is problematic because word are often polysemous and global context can also provide useful information for learning word meaning we present a new neural network architecture which learns word embeddings that better capture the semantics of word by incorporating both local and global document context and account for homonymy and polysemy by learning multiple embeddings per word we introduce a new dataset with human judgment on pair of word in sentential context and evaluate our model on it showing that our model outperforms competitive baseline and other neural language model 
we present a probabilistic topic model for jointly identifying property and attribute of social medium review snippet our model simultaneously learns a set of property of a product and capture aggregate user sentiment towards these property this approach directly enables discovery of highly rated or inconsistent property of a product our model admits an efficient variational mean field inference algorithm which can be parallelized and run on large snippet collection we evaluate our model on a large corpus of snippet from yelp review to ass property and attribute prediction we demonstrate that it outperforms applicable baseline by a considerable margin 
the importance of inference rule to semantic application ha long been recognized and extensive work ha been carried out to automatically acquire inference rule resource however evaluating such resource ha turned out to be a non trivial task slowing progress in the field in this paper we suggest a framework for evaluating inference rule resource our framework simplifies a previously proposed instance based evaluation method that involved substantial annotator training making it suitable for crowdsourcing we show that our method produce a large amount of annotation with high inter annotator agreement for a low cost at a short period of time without requiring training expert annotator 
argumentation scheme are structure or template for various kind of argument given the text of an argument with premise and conclusion identified we classify it a an instance of one of five common scheme using feature specific to each scheme we achieve accuracy of in one against others classification and in pairwise classification baseline in both case 
bootstrapping a classifier from a small set of seed rule can be viewed a the propagation of label between example via feature shared between them this paper introduces a novel variant of the yarowsky algorithm based on this view it is a bootstrapping learning method which us a graph propagation algorithm with a well defined objective function the experimental result show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data set 
there ha been recent interest in the problem of decoding letter substitution cipher using technique inspired by natural language processing we consider a different type of classical encoding scheme known a the running key cipher and propose a search solution using gibbs sampling with a word language model we evaluate our method on synthetic cipher text of different length and find that it outperforms previous work that employ viterbi decoding with character based model 
our research aim at building computational model of word meaning that are perceptually grounded using computer vision technique we build visual and multimodal distributional model and compare them to standard textual model our result show that while visual model with state of the art computer vision technique perform worse than textual model in general task accounting for semantic relatedness they are a good or better model of the meaning of word with visual correlate such a color term even in a nontrivial task that involves nonliteral us of such word moreover we show that visual and textual information are tapping on different aspect of meaning and indeed combining them in multimodal model often improves performance 
accurate prediction of demographic attribute from social medium and other informal online content is valuable for marketing personalization and legal investigation this paper describes the construction of a large multilingual dataset labeled with gender and investigates statistical model for determining the gender of uncharacterized twitter user we explore several different classifier type on this dataset we show the degree to which classifier accuracy varies based on tweet volume a well a when various kind of profile metadata are included in the model we also perform a large scale human assessment using amazon mechanical turk our method significantly out perform both baseline model and almost all human on the same task 
automatically produced text e g translation or summary are usually evaluated with n gram based measure such a bleu or rouge while the wide set of more sophisticated measure that have been proposed in the last year remains largely ignored for practical purpose in this paper we first present an in depth analysis of the state of the art in order to clarify this issue after this we formalize and verify empirically a set of property that every text evaluation measure based on similarity to human produced reference satisfies these property imply that corroborating system improvement with additional measure always increase the overall reliability of the evaluation process in addition the greater the heterogeneity of the measure which is measurable the higher their combined reliability these result support the use of heterogeneous measure in order to consolidate text evaluation result 
in the last few year the interest of the research community in micro blog and social medium service such a twitter is growing exponentially yet so far not much attention ha been paid on a key characteristic of micro blog the high level of information redundancy the aim of this paper is to systematically approach this problem by providing an operational definition of redundancy we cast redundancy in the framework of textual entailment recognition we also provide quantitative evidence on the pervasiveness of redundancy in twitter and describe a dataset of redundancy annotated tweet finally we present a general purpose system for identifying redundant tweet an extensive quantitative evaluation show that our system successfully solves the redundancy detection task improving over baseline system with statistical significance 
we present a system for the large scale induction of cognate group our model explains the evolution of cognate a a sequence of mutation and innovation along a phylogeny on the task of identifying cognate from over word in different language from the oceanic language family our model achieves a cluster purity score over while maintaining pairwise recall over 
learning for sentence re writing is a fundamental task in natural language processing and information retrieval in this paper we propose a new class of kernel function referred to a string re writing kernel to address the problem a string re writing kernel measure the similarity between two pair of string each pair representing re writing of a string it can capture the lexical and structural similarity between two pair of sentence without the need of constructing syntactic tree we further propose an instance of string re writing kernel which can be computed efficiently experimental result on benchmark datasets show that our method can achieve better result than state of the art method on two sentence re writing learning task paraphrase identification and recognizing textual entailment 
while it is ha often been observed that the product of translation is somehow different than non translated text scholar have emphasized two distinct base for such difference some have noted interference from the source language spilling over into translation in a source language specific way while others have noted general effect of the process of translation that are independent of source language using a series of text categorization experiment we show that both these effect exist and that moreover there is a continuum between them there are many effect of translation that are consistent among text translated from a given source language some of which are consistent even among text translated from family of source language significantly we find that even for widely unrelated source language and multiple genre difference between translated text and non translated text are sufficient for a learned classifier to accurately determine if a given text is translated or original 
although much work on relation extraction ha aimed at obtaining static fact many of the target relation are actually fluents a their validity is naturally anchored to a certain time period this paper proposes a methodological approach to temporally anchored relation extraction our proposal performs distant supervised learning to extract a set of relation from a natural language corpus and anchor each of them to an interval of temporal validity aggregating evidence from document supporting the relation we use a rich graph based document level representation to generate novel feature for this task result show that our implementation for temporal anchoring is able to achieve a of the upper bound performance imposed by the relation extraction step compared to the state of the art the overall system achieves the highest precision reported 
one of the major challenge facing statistical machine translation is how to model difference in word order between language although a great deal of research ha focussed on this problem progress is hampered by the lack of reliable metric most current metric are based on matching lexical item in the translation and the reference and their ability to measure the quality of word order ha not been demonstrated this paper present a novel metric the lrscore which explicitly measure the quality of word order by using permutation distance metric we show that the metric is more consistent with human judgement than other metric including the bleu score we also show that the lrscore can successfully be used a the objective function when training translation model parameter training with the lrscore lead to output which is preferred by human moreover the translation incur no penalty in term of bleu score 
ccgs are directly compatible with binary branching bottom up parsing algorithm in particular cky and shift reduce algorithm while the chart based approach ha been the dominant approach for ccg the shift reduce method ha been little explored in this paper we develop a shift reduce ccg parser using a discriminative model and beam search and compare it strength and weakness with the chart based c c parser we study different error made by the two parser and show that the shift reduce parser give competitive accuracy compared to c c considering our use of a small beam and given the high ambiguity level in an automatically extracted grammar and the amount of information in the ccg lexical category which form the shift action this is a surprising result 
large e commerce enterprise feature million of item entered daily by a large variety of seller while some seller provide rich structured description of their item a vast majority of them provide unstructured natural language description in the paper we present a step method for structuring item into descriptive property the first step consists in unsupervised property discovery and extraction the second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm we evaluate our method on a year worth of e commerce data and show that it achieves excellent precision with good recall 
the automatic extraction of comparative information is an important text mining problem and an area of increasing interest in this paper we study how to build a korean comparison mining system our work is composed of two consecutive task classifying comparative sentence into different type and mining comparative entity and predicate we perform various experiment to find relevant feature and learning technique a a result we achieve outstanding performance enough for practical use 
bayesian approach have been shown to reduce the amount of overfitting that occurs when running the em algorithm by placing prior probability on the model parameter we apply one such bayesian technique variational bayes to the ibm model of word alignment for statistical machine translation we show that using variational bayes improves the performance of the widely used giza software a well a improving the overall performance of the moses machine translation system in term of bleu score 
a re scoring strategy is proposed that make it feasible to capture more long distance dependency in the natural language two pas strategy have become popular in a number of recognition task such a asr automatic speech recognition mt machine translation and ocr optical character recognition the first pas typically applies a weak language model n gram to a lattice and the second pas applies a stronger language model to n best list the stronger language model is intended to capture more long distance dependency the proposed method us rnn lm recurrent neural network language model which is a long span lm to re score word lattice in the second pas a hill climbing method iterative decoding is proposed to search over island of confusability in the word lattice an evaluation based on broadcast news show speedup of over basic n best re scoring and word error rate reduction of relative on a highly competitive setup 
the notion of infix probability ha been introduced in the literature a a generalization of the notion of prefix or initial substring probability motivated by application in speech recognition and word error correction for the case where a probabilistic context free grammar is used a language model method for the computation of infix probability have been presented in the literature based on various simplifying assumption here we present a solution that applies to the problem in it full generality 
we present a novel method for record extraction from social stream such a twitter unlike typical extraction setup these environment are characterized by short one sentence message with heavily colloquial speech to further complicate matter individual message may not express the full relation to be uncovered a is often assumed in extraction task we develop a graphical model that address these problem by learning a latent set of record and a record message alignment simultaneously the output of our model is a set of canonical record the value of which are consistent with aligned message we demonstrate that our approach is able to accurately induce event record from twitter message evaluated against event from a local city guide our method achieves significant error reduction over baseline method 
supervised classification need large amount of annotated training data that is expensive to create two approach that reduce the cost of annotation are active learning and crowdsourcing however these two approach have not been combined successfully to date we evaluate the utility of active learning in crowdsourcing on two task named entity recognition and sentiment detection and show that active learning outperforms random selection of annotation example in a noisy crowdsourcing scenario 
we present a quasi synchronous dependency grammar smith and eisner for machine translation in which the leaf of the tree are phrase rather than word a in previous work gimpel and smith this formulation allows u to combine structural component of phrase based and syntax based mt in a single model we describe a method of extracting phrase dependency from parallel text using a target side dependency parser for decoding we describe a coarse to fine approach based on lattice dependency parsing of phrase lattice we demonstrate performance improvement for chinese english and urdu english translation over a phrase based baseline we also investigate the use of unsupervised dependency parser reporting encouraging preliminary result 
this paper describes dualist an active learning annotation paradigm which solicits and learns from label on both feature e g word and instance e g document we present a novel semi supervised training algorithm developed for this setting which is fast enough to support real time interactive speed and at least a accurate a preexisting method for learning with mixed feature and instance label human annotator in user study were able to produce near state of the art classifier on several corpus in a variety of application domain with only a few minute of effort 
we describe a new approach to disambiguating semantic frame evoked by lexical predicate previously unseen in a lexicon or annotated data our approach make use of large amount of unlabeled data in a graph based semi supervised learning framework we construct a large graph where vertex correspond to potential predicate and use label propagation to learn possible semantic frame for new one the label propagated graph is used within a frame semantic parser and for unknown predicate result in over absolute improvement in frame identification accuracy and over absolute improvement in full frame semantic parsing f score on a blind test set over a state of the art supervised baseline 
we present a statistical model for canonicalizing named entity mention into a table whose row represent entity and whose column are attribute or part of attribute the model is novel in that it incorporates entity context surface feature first order dependency among attribute part and a notion of noise transductive learning from a few seed and a collection of mention token combine bayesian inference and conditional estimation we evaluate our model and it component on two datasets collected from political blog and sport news finding that it outperforms a simple agglomerative clustering approach and previous work 
log linear parsing model are often trained by optimizing likelihood but we would prefer to optimise for a task specific metric like f measure softmax margin is a convex objective for such model that minimises a bound on expected risk for a given loss function but it na ve application requires the loss to decompose over the predicted structure which is not true of f measure we use softmax margin to optimise a log linear ccg parser for a variety of loss function and demonstrate a novel dynamic programming algorithm that enables u to use it with f measure leading to substantial gain in accuracy on ccg bank when we embed our loss trained parser into a larger model that includes supertagging feature incorporated via belief propagation we obtain further improvement and achieve a labelled unlabelled dependency f measure of on gold part of speech tag and on automatic part of speech tag the best reported result for this task 
method that measure compatibility between mention pair are currently the dominant approach to coreference however they suffer from a number of drawback including difficulty scaling to large number of mention and limited representational power a these drawback become increasingly restrictive the need to replace the pairwise approach with a more expressive highly scalable alternative is becoming urgent in this paper we propose a novel discriminative hierarchical model that recursively partition entity into tree of latent sub entity these tree succinctly summarize the mention providing a highly compact information rich structure for reasoning about entity and coreference uncertainty at massive scale we demonstrate that the hierarchical model is several order of magnitude faster than pairwise allowing u to perform coreference on six million author mention in under four hour on a single cpu 
we address the creation of cross lingual textual entailment corpus by mean of crowd sourcing our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotator without resorting to preprocessing tool or already annotated monolingual datasets in line with recent work emphasizing the need of large scale annotation effort for textual entailment our work aim to i tackle the scarcity of data available to train and evaluate system and ii promote the recourse to crowdsourcing a an effective way to reduce the cost of data collection without sacrificing quality we show that a complex data creation task for which even expert usually feature low agreement score can be effectively decomposed into simple subtasks assigned to non expert annotator the resulting dataset obtained from a pipeline of different job routed to amazon mechanical turk contains more than aligned pair for each combination of text hypothesis in english italian and german 
we describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domain designated a the source domain we automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domain to find the association between word that express similar sentiment in different domain the created thesaurus is then used to expand feature vector to train a binary classifier unlike previous cross domain sentiment classification method our method can efficiently learn from multiple source domain our method significantly outperforms numerous baseline and return result that are better than or comparable to previous cross domain sentiment classification method on a benchmark dataset containing amazon user review for different type of product 
this paper present a generative model for the automatic discovery of relation between entity in electronic medical record the model discovers relation instance and their type by determining which context token express the relation additionally the valid semantic class for each type of relation are determined we show that the model produce cluster of relation trigger word which better correspond with manually annotated relation than several existing clustering technique the discovered relation reveal some of the implicit semantic structure present in patient record 
predicate argument structure contains rich semantic information of which statistical machine translation hasn t taken full advantage in this paper we propose two discriminative feature based model to exploit predicate argument structure for statistical machine translation a predicate translation model and an argument reordering model the predicate translation model explores lexical and semantic context surrounding a verbal predicate to select desirable translation for the predicate the argument reordering model automatically predicts the moving direction of an argument relative to it predicate after translation using semantic feature the two model are integrated into a state of the art phrase based machine translation system and evaluated on chinese to english translation task with large scale training data experimental result demonstrate that the two model significantly improve translation accuracy 
text mining and data harvesting algorithm have become popular in the computational linguistics community they employ pattern that specify the kind of information to be harvested and usually bootstrap either the pattern learning or the term harvesting process or both in a recursive cycle using data learned in one step to generate more seed for the next they therefore treat the source text corpus a a network in which word are the node and relation linking them are the edge the result of computational network analysis especially from the world wide web are thus applicable surprisingly these result have not yet been broadly introduced into the computational linguistics community in this paper we show how various result apply to text mining how they explain some previously observed phenomenon and how they can be helpful for computational linguistics application 
we present an automatic method which leverage word lengthening to adapt a sentiment lexicon specifically for twitter and similar social messaging network the contribution of the paper are a follows first we call attention to lengthening a a widespread phenomenon in microblogs and social messaging and demonstrate the importance of handling it correctly we then show that lengthening is strongly associated with subjectivity and sentiment finally we present an automatic method which leverage this association to detect domain specific sentimentand emotion bearing word we evaluate our method by comparison to human judgment and analyze it strength and weakness our result are of interest to anyone analyzing sentiment in microblogs and social network whether for research or commercial purpose 
in this work we address the problem of unsupervised part of speech induction by bringing together several strand of research into a single model we develop a novel hidden markov model incorporating sophisticated smoothing using a hierarchical pitman yor process prior providing an elegant and principled mean of incorporating lexical characteristic central to our approach is a new type based sampling algorithm for hierarchical pitman yor model in which we track fractional table count in an empirical evaluation we show that our model consistently out performs the current state of the art across language 
in this work we propose method to label probabilistic synchronous context free grammar pscfg rule using only word tag generated by either part of speech analysis or unsupervised word class induction the proposal range from simple tag combination scheme to a phrase clustering model that can incorporate an arbitrary number of feature our model improve translation quality over the single generic label approach of chiang and perform on par with the syntactically motivated approach from zollmann and venugopal on the nist large chinese to english translation task these result persist when using automatically learned word tag suggesting broad applicability of our technique across diverse language pair for which syntactic resource are not available 
we introduce an approach to optimize a machine translation mt system on multiple metric simultaneously different metric e g bleu ter focus on different aspect of translation quality our multi objective approach leverage these diverse aspect to improve overall quality our approach is based on the theory of pareto optimality it is simple to implement on top of existing single objective optimization method e g mert pro and outperforms ad hoc alternative based on linear combination of metric we also discus the issue of metric tunability and show that our pareto approach is more effective in incorporating new metric from mt evaluation for mt optimization 
this paper present a new algorithm for linear text segmentation it is an adaptation of affinity propagation a state of the art clustering algorithm in the framework of factor graph affinity propagation for segmentation or aps receives a set of pairwise similarity between data point and produce segment boundary and segment centre data point which best describe all other data point within the segment aps iteratively pass message in a cyclic factor graph until convergence each iteration work with information on all available similarity resulting in high quality result aps scale linearly for realistic segmentation task we derive the algorithm from the original affinity propagation formulation and evaluate it performance on topical text segmentation in comparison with two state of the art segmenters the result suggest that aps performs on par with or outperforms these two very competitive baseline 
an ideal summarization system should produce summary that have high content coverage and linguistic quality many state of the art summarization system focus on content coverage by extracting content dense sentence from source article a current research focus is to process these sentence so that they read fluently a a whole the current aesop task encourages research on evaluating summary on content readability and overall responsiveness in this work we adapt a machine translation metric to measure content coverage apply an enhanced discourse coherence model to evaluate summary readability and combine both in a trained regression model to evaluate overall responsiveness the result show significantly improved performance over aesop submitted metric 
we propose a novel approach to improve smt via paraphrase rule which are automatically extracted from the bilingual training data without using extra paraphrase resource we acquire the rule by comparing the source side of the parallel corpus with the target to source translation of the target side besides the word and phrase paraphrase the acquired paraphrase rule mainly cover the structured paraphrase on the sentence level these rule are employed to enrich the smt input for translation quality improvement the experimental result show that our proposed approach achieves significant improvement of point of bleu in the oral domain and point in the news domain 
convolution kernel support the modeling of complex syntactic information in machine learning task however such model are highly sensitive to the type and size of syntactic structure used it is therefore an important challenge to automatically identify high impact sub structure relevant to a given task in this paper we present a systematic study investigating combination of sequence and convolution kernel using different type of substructure in document level sentiment classification we show that minimal sub structure extracted from constituency and dependency tree guided by a polarity lexicon show point absolute improvement in accuracy over a bag of word classifier on a widely used sentiment corpus 
this paper proposes a new method for approximate string search specifically candidate generation in spelling error correction which is a task a follows given a misspelled word the system find word in a dictionary which are most similar to the misspelled word the paper proposes a probabilistic approach to the task which is both accurate and efficient the approach includes the use of a log linear model a method for training the model and an algorithm for finding the top k candidate the log linear model is defined a a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word the learning method employ the criterion in candidate generation a loss function the retrieval algorithm is efficient and is guaranteed to find the optimal k candidate experimental result on large scale data show that the proposed approach improves upon existing method in term of accuracy in different setting 
we propose a novel technique of learning how to transform the source parse tree to improve the translation quality of syntax based translation model using synchronous context free grammar we transform the source tree phrasal structure into a set of simpler structure expose such decision to the decoding process and find the least expensive transformation operation to better model word reordering in particular we integrate synchronous binarizations verb regrouping removal of redundant parse node and incorporate a few important feature such a translation boundary we learn the structural preference from the data in a generative framework the syntax based translation system integrating the proposed technique outperforms the best arabic english unconstrained system in nist evaluation by absolute bleu which is statistically significant 
we discus and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max norm constraint with respect to an observed distribution this setting generalizes the classical maximum entropy problem a it relaxes the standard constraint on the observed value we tackle the problem by introducing a re parametrization in which the unknown distribution is distilled to a single scalar we then describe a homotopy between the relaxation parameter and the distribution characterizing parameter the homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution we then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path our derivation are based on a compact geometric view of the relaxation path a a piecewise linear function in a two dimensional space of the relaxation characterization parameter we demonstrate the usability of our approach by applying the problem to zipfian distribution over a large alphabet 
this paper present an unsupervised method for deriving inference axiom by composing semantic relation the method is independent of any particular relation inventory it relies on describing semantic relation using primitive and manipulating these primitive according to an algebra the method wa tested using a set of eight semantic relation yielding inference axiom which were evaluated over propbank 
we propose method for estimating the probability that an entity from an entity database is associated with a web search query association is modeled using a query entity click graph blending general query click log with vertical query click log smoothing technique are proposed to address the inherent data sparsity in such graph including interpolation using a query synonymy model a large scale empirical analysis of the smoothing technique over a year click graph collected from a commercial search engine show significant reduction in modeling error the association model are then applied to the task of recommending product to web query by annotating query with product from a large catalog and then mining query product association through web search session analysis experimental analysis show that our smoothing technique improve coverage while keeping precision stable and overall that our top performing model affect of general web query with precision 
searching document that are similar to a query document is an important component in modern information retrieval some existing hashing method can be used for efficient document similarity search however unsupervised hashing method cannot incorporate prior knowledge for better hashing although some supervised hashing method can derive effective hash function from prior knowledge they are either computationally expensive or poorly discriminative this paper proposes a novel semi supervised hashing method named semi supervised simhash s h for high dimensional data similarity search the basic idea of s h is to learn the optimal feature weight from prior knowledge to relocate the data such that similar data have similar hash code we evaluate our method with several state of the art method on two large datasets all the result show that our method get the best performance 
we describe a joint model for understanding user action in natural language utterance our multi layer generative approach us both labeled and unlabeled utterance to jointly learn aspect regarding utterance s target domain e g movie intention e g finding a movie along with other semantic unit e g movie name we inject information extracted from unstructured web search query log a prior information to enhance the generative process of the natural language utterance understanding model using utterance from five domain our approach show up to improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model which requires fully labeled data 
we present a discriminative learning method to improve the consistency of translation in phrase based statistical machine translation smt system our method is inspired by translation memory tm system which are widely used by human translator in industrial setting we constrain the translation of an input sentence using the most similar translation example retrieved from the tm differently from previous research which used simple fuzzy match threshold these constraint are imposed using discriminative learning to optimise the translation performance we observe that using this method can benefit the smt system by not only producing consistent translation but also improved translation output we report a point improvement in term of bleu score on english chinese technical document 
treebanks are not large enough to reliably model precise lexical phenomenon this deficiency provokes attachment error in the parser trained on such data we propose in this paper to compute lexical affinity on large corpus for specific lexico syntactic configuration that are hard to disambiguate and introduce the new information in a parser experiment on the french treebank showed a relative decrease of the error rate of labeled accuracy score yielding the best parsing result on this treebank 
cross document coreference the task of grouping all the mention of each entity in a document collection arises in information extraction and automated knowledge base construction for large collection it is clearly impractical to consider all possible grouping of mention into distinct entity to solve the problem we propose two idea a a distributed inference technique that us parallelism to enable large scale processing and b a hierarchical model of coreference that represents uncertainty over multiple granularity of entity to facilitate more effective approximate inference to evaluate these idea we constructed a labeled corpus of million disambiguated mention in web page by selecting link anchor referring to wikipedia entity we show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy with error reduction of on this large dataset demonstrating the scalability of our approach 
this paper address the search problem in textual inference where system need to infer one piece of text from another a prominent approach to this task is attempt to transform one text into the other through a sequence of inference preserving transformation a k a a proof while estimating the proof s validity this raise a search challenge of finding the best possible proof we explore this challenge through a comprehensive investigation of prominent search algorithm and propose two novel algorithmic component specifically designed for textual inference a gradient style evaluation function and a local lookahead node expansion method evaluation using the open source system biutee show the contribution of these idea to search efficiency and proof quality 
the large combined search space of joint word segmentation and part of speech po tagging make efficient decoding very hard a a result effective high order feature representing rich context are inconvenient to use in this work we propose a novel stacked subword model for this task concerning both efficiency and effectiveness our solution is a two step process first one word based segmenter one character based segmenter and one local character classifier are trained to produce coarse segmentation and po information second the output of the three predictor are merged into sub word sequence which are further bracketed and labeled with po tag by a fine grained sub word tagger the coarse to fine search scheme is efficient while in the sub word tagging step rich contextual feature can be approximately derived evaluation on the penn chinese tree bank show that our model yield improvement over the best system reported in the literature 
comprehending action precondition and effect is an essential step in modeling the dynamic of the world in this paper we express the semantics of precondition relation extracted from text in term of planning operation the challenge of modeling this connection is to ground language at the level of relation this type of grounding enables u to create high level plan based on language abstraction our model jointly learns to predict precondition relation from text and to perform high level planning guided by those relation we implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempt when applied to a complex virtual world and text describing that world our relation extraction technique performs on par with a supervised baseline yielding an f measure of compared to the baseline s additionally we show that a high level planner utilizing these extracted relation significantly outperforms a strong text unaware baseline successfully completing of planning task a compared to for the baseline 
aspect extraction is a central problem in sentiment analysis current method either extract aspect without categorizing them or extract and categorize them using unsupervised topic modeling by categorizing we mean the synonymous aspect should be clustered into the same category in this paper we solve the problem in a different setting where the user provides some seed word for a few aspect category and the model extract and cluster aspect term into category simultaneously this setting is important because categorizing aspect is a subjective task for different application purpose different categorization may be needed some form of user guidance is desired in this paper we propose two statistical model to solve this seeded problem which aim to discover exactly what the user want our experimental result show that the two proposed model are indeed able to perform the task effectively 
part of speech po is an indispensable feature in dependency parsing current research usually model po tagging and dependency parsing independently this may suffer from error propagation problem our experiment show that parsing accuracy drop by about when using automatic po tag instead of gold one to solve this issue this paper proposes a solution by jointly optimizing po tagging and dependency parsing in a unique model we design several joint model and their corresponding decoding algorithm to incorporate different feature set we further present an effective pruning strategy to reduce the search space of candidate po tag leading to significant improvement of parsing speed experimental result on chinese penn treebank show that our joint model significantly improve the state of the art parsing accuracy by about detailed analysis show that the joint method is able to choose such po tag that are more helpful and discriminative from parsing viewpoint this is the fundamental reason of parsing accuracy improvement 
augmented and alternative communication aac device enable user with certain communication disability to participate in everyday conversation such device often rely on statistical language model to improve text entry by offering word prediction these prediction can be improved if the language model is trained on data that closely reflects the style of the user intended communication unfortunately there is no large dataset consisting of genuine aac message in this paper we demonstrate how we can crowd source the creation of a large set of fictional aac message we show that these message model conversational aac better than the currently used datasets based on telephone conversation or newswire text we leverage our crowdsourced message to intelligently select sentence from much larger set of twitter blog and usenet data compared to a model trained only on telephone transcript our best performing model reduced perplexity on three test set of aac like communication by relative this translated to a potential keystroke saving in a predictive keyboard interface of 
this paper describes a novel technique for incorporating syntactic knowledge into phrase based machine translation through incremental syntactic parsing bottom up and top down parser typically require a completed string a input this requirement make it difficult to incorporate them into phrase based translation which generates partial hypothesized translation from left to right incremental syntactic language model score sentence in a similar left to right fashion and are therefore a good mechanism for incorporating syntax into phrase based translation we give a formal definition of one such lineartime syntactic language model detail it relation to phrase based decoding and integrate the model with the moses phrase based translation system we present empirical result on a constrained urdu english translation task that demonstrate a significant bleu score improvement and a large decrease in perplexity 
topic model have been successfully applied to many document analysis task to discover topic embedded in text however existing topic model generally cannot capture the latent topical structure in document since language are intrinsically cohesive and coherent modeling and discovering latent topical transition structure within document would be beneficial for many text analysis task in this work we propose a new topic model structural topic model which simultaneously discovers topic and reveals the latent topical structure in text through explicitly modeling topical transition with a latent first order markov chain experiment result show that the proposed structural topic model can effectively discover topical structure in text and the identified structure significantly improve the performance of task such a sentence annotation and sentence ordering 
this paper present a novel method to suggest long word reordering to a phrase based smt decoder we address language pair where long reordering concentrate on few pattern and use fuzzy chunk based rule to predict likely reordering for these phenomenon then we use reordered n gram lm to rank the resulting permutation and select the n best for translation finally we encode these reordering by modifying selected entry of the distortion cost matrix on a per sentence basis in this way we expand the search space by a much finer degree than if we simply raised the distortion limit the proposed technique are tested on arabic english and german english using well known smt benchmark 
grapheme to phoneme conversion g p of name is an important and challenging problem the correct pronunciation of a name is often reflected in it transliteration which are expressed within a different phonological inventory we investigate the problem of using transliteration to correct error produced by state of the art g p system we present a novel re ranking approach that incorporates a variety of score and n gram feature in order to leverage transliteration from multiple language our experiment demonstrate significant accuracy improvement when re ranking is applied to n best list generated by three different g p program 
we present a novel model to represent and ass the discourse coherence of text our model assumes that coherent text implicitly favor certain type of discourse relation transition we implement this model and apply it towards the text ordering ranking task which aim to discern an original text from a permuted ordering of it sentence the experimental result demonstrate that our model is able to significantly outperform the state of the art coherence model by barzilay and lapata reducing the error rate of the previous approach by an average of over three data set against human upper bound we further show that our model is synergistic with the previous approach demonstrating an error reduction of when the feature from both model are combined for the task 
to adapt a translation model trained from the data in one domain to another previous work paid more attention to the study of parallel corpus while ignoring the in domain monolingual corpus which can be obtained more easily in this paper we propose a novel approach for translation model adaptation by utilizing in domain monolingual topic information instead of the in domain bilingual corpus which incorporates the topic information into translation probability estimation our method establishes the relationship between the out of domain bilingual corpus and the in domain monolingual corpus via topic mapping and phrase topic distribution probability estimation from in domain monolingual corpus experimental result on the nist chinese english translation task show that our approach significantly outperforms the baseline system 
class instance label propagation algorithm have been successfully used to fuse information from multiple source in order to enrich a set of unlabeled instance with class label yet nobody ha explored the relationship between the instance themselves to enhance an initial set of class instance pair we propose two graph theoretic method centrality and regularization which start with a small set of labeled class instance pair and use the instance instance network to extend the class label to all instance in the network we carry out a comparative study with state of the art knowledge harvesting algorithm and show that our approach can learn additional class label while maintaining high accuracy we conduct a comparative study between class instance and instance instance graph used to propagate the class label and show that the latter one achieves higher accuracy 
the computation of logical form ha been proposed a an intermediate step in the translation of sentence to logic logical form encodes the resolution of scope ambiguity in this paper we describe experiment on a modest sized corpus of regulation annotated with a novel variant of logical form called abstract syntax tree asts the main step in computing asts is to order scope taking operator a learning model for ranking is adapted fortius ordering we design feature by studying the problem of comparing the scope of one operator to another the scope comparison are used to compute asts with an f score of on the set of ordering decisons 
we show that category induced by unsupervised word clustering can surpass the performance of gold part of speech tag in dependency grammar induction unlike classic clustering algorithm our method allows a word to have different tag in different context in an ablative analysis we first demonstrate that this context dependence is crucial to the superior performance of gold tag requiring a word to always have the same part of speech significantly degrades the performance of manual tag in grammar induction eliminating the advantage that human annotation ha over unsupervised tag we then introduce a sequence modeling technique that combine the output of a word clustering algorithm with context colored noise to allow word to be tagged differently in different context with these new induced tag a input our state of the art dependency grammar inducer achieves directed accuracy on section all sentence of the wall street journal wsj corpus higher than using gold tag 
we propose an automatic method of extracting paraphrase from definition sentence which are also automatically acquired from the web we observe that a huge number of concept are defined in web document and that the sentence that define the same concept tend to convey mostly the same information using different expression and thus contain many paraphrase we show that a large number of paraphrase can be automatically extracted with high precision by regarding the sentence that define the same concept a parallel corpus experimental result indicated that with our method it wa possible to extract about paraphrase from x web document with a precision rate of about 
due to it explicit modeling of the grammaticality of the output via target side syntax the string to tree model ha been shown to be one of the most successful syntax based translation model however a major limitation of this model is that it doe not utilize any useful syntactic information on the source side in this paper we analyze the difficulty of incorporating source syntax in a string to tree model we then propose a new way to use the source syntax in a fuzzy manner both in source syntactic annotation and in rule matching we further explore three algorithm in rule matching matching likelihood matching and deep similarity matching our method not only guarantee grammatical output with an explicit target tree but also enables the system to choose the proper translation rule via fuzzy use of the source syntax our extensive experiment have shown significant improvement over the state of the art string to tree system 
we present a novel answer summarization method for community question answering service cqas to address the problem of incomplete answer i e the best answer of a complex multi sentence question miss valuable information that is contained in other answer in order to automatically generate a novel and non redundant community answer summary we segment the complex original multi sentence question into several sub question and then propose a general conditional random field crf based answer summary method with group l regularization various textual and non textual qa feature are explored specifically we explore four different type of contextual factor namely the information novelty and non redundancy modeling for local and non local sentence interaction under question segmentation to further unleash the potential of the abundant cqa feature we introduce the group l regularization for feature learning experimental result on a yahoo answer dataset show that our proposed method significantly outperforms state of the art method on cqa summarization task 
it is often assumed that grounded learning task are beyond the scope of grammatical inference technique in this paper we show that the grounded task of learning a semantic parser from ambiguous training data a discussed in kim and mooney can be reduced to a probabilistic context free grammar learning task in a way that give state of the art result we further show that additionally letting our model learn the language s canonical word order improves it performance and lead to the highest semantic parsing f score previously reported in the literature 
we propose a new approach for the creation of child language development metric a set of linguistic feature is computed on child speech sample and used a input in two age prediction experiment in the first experiment we learn a child specific metric and predicts the age at which speech sample were produced we then learn a more general developmental index by applying our method across child predicting relative temporal ordering of speech sample in both case we compare our result with established measure of language development showing improvement in age prediction performance 
model of word alignment built a sequence of link have limited expressive power but are easy to decode word aligners that model the alignment matrix can express arbitrary alignment but are difficult to decode we propose an alignment matrix model a a correction algorithm to an underlying sequence based aligner then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation improved alignment performance is shown for all nine language pair tested the improved alignment also improved translation quality from chinese to english and english to italian 
in recent year the amount of user generated opinionated text e g review user comment continues to grow at a rapid speed featured news story on a major event easily attract thousand of user comment on a popular online news service how to consume subjective information of this volume becomes an interesting and important research question in contrast to previous work on review analysis that tried to filter or summarize information for a generic average user we explore a different direction of enabling personalized recommendation of such information for each user our task is to rank the comment associated with a given article according to personalized user preference i e whether the user is likely to like or dislike the comment to this end we propose a factor model that incorporates rater comment and rater author interaction simultaneously in a principled way our full model significantly outperforms strong baseline a well a related model that have been considered in previous work 
learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language it is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context recent work by chen and mooney introduced a lexicon learning method that deal with ambiguous relational data by taking intersection of graph while the algorithm produced good lexicon for the task of learning to interpret navigation instruction it only work in batch setting and doe not scale well to large datasets in this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the state of the art result we show that by changing the grammar of the formal meaning representation language and training on additional data collected from amazon s mechanical turk we can further improve the result we also include experimental result on a chinese translation of the training data to demonstrate the generality of our approach 
it ha been widely recognized that one of the most difficult and intriguing problem in natural language processing nlp is how to cope with idiosyncratic multiword expression this paper present an overview of the comprehensive dictionary jdmwe of japanese multiword expression the jdmwe is characterized by a large notational syntactic and semantic diversity of contained expression a well a a detailed description of their syntactic function structure and flexibility the dictionary contains about expression potentially expression this paper show that the jdmwe s validity can be supported by comparing the dictionary with a large scale japanese n gram frequency dataset namely the ldc t generated by google inc kudo et al 
statistical machine translation is often faced with the problem of combining training data from many diverse source into a single translation model which then ha to translate sentence in a new domain we propose a novel approach ensemble decoding which combine a number of translation system dynamically at the decoding step in this paper we evaluate performance on a domain adaptation setting where we translate sentence from the medical domain our experimental result show that ensemble decoding outperforms various strong baseline including mixture model the current state of the art for domain adaptation in machine translation 
information retrieval ir and figurative language processing flp could scarcely be more different in their treatment of language and meaning ir view language a an open ended set of mostly stable sign with which text can be indexed and retrieved focusing more on a text s potential relevance than it potential meaning in contrast flp view language a a system of unstable sign that can be used to talk about the world in creative new way there is another key difference ir is practical scalable and robust and in daily use by million of casual user flp is neither scalable nor robust and not yet practical enough to migrate beyond the lab this paper thus present a mutually beneficial hybrid of ir and flp one that enriches ir with new operator to enable the non literal retrieval of creative expression and which also transplant flp into a robust scalable framework in which practical application of linguistic creativity can be implemented 
in this work we present two extension to the well known dynamic programming beam search in phrase based statistical machine translation smt aiming at increased efficiency of decoding by minimizing the number of language model computation and hypothesis expansion our result show that language model based pre sorting yield a small improvement in translation quality and a speedup by a factor of two look ahead method are shown to further increase translation speed by a factor of without changing the search space and a factor of with the side effect of some additional search error we compare our approach with moses and observe the same performance but a substantially better trade off between translation quality and speed at a speed of roughly word per second moses reach bleu whereas our approach yield with identical model 
we describe an exact decoding algorithm for syntax based statistical translation the approach us lagrangian relaxation to decompose the decoding problem into tractable sub problem thereby avoiding exhaustive dynamic programming the method recovers exact solution with certificate of optimality on over of test example it ha comparable speed to state of the art decoder 
during early language acquisition infant must learn both a lexicon and a model of phonetics that explains how lexical item can vary in pronunciation for instance the might be realized a i or previous model of acquisition have generally tackled these problem in isolation yet behavioral evidence suggests infant acquire lexical and phonetic knowledge simultaneously we present a bayesian model that cluster together phonetic variant of the same lexical item while learning both a language model over lexical item and a log linear model of pronunciation variability based on articulatory feature the model is trained on transcribed surface pronunciation and learns by bootstrapping without access to the true lexicon we test the model using a corpus of child directed speech with realistic phonetic variation and either gold standard or automatically induced word boundary in both case modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item ha a unique pronunciation 
classically training relation extractor relies on high quality manually annotated training data which can be expensive to obtain to mitigate this cost nlu researcher have considered two newly available source of le expensive but potentially lower quality labeled data from distant supervision and crowd sourcing there is however no study comparing the relative impact of these two source on the precision and recall of post learning answer to fill this gap we empirically study how state of the art technique are affected by scaling these two source we use corpus size of up to million document and ten of thousand of crowd source labeled example our experiment show that increasing the corpus size for distant supervision ha a statistically significant positive impact on quality f score in contrast human feedback ha a positive and statistically significant but lower impact on precision and recall 
in this paper we give an overview of the knowledge base population kbp track at the text analysis conference the main goal of kbp is to promote research in discovering fact about entity and augmenting a knowledge base kb with these fact this is done through two task entity linking linking name in context to entity in the kb and slot filling adding information about an entity to the kb a large source collection of newswire and web document is provided from which system are to discover information attribute slot derived from wikipedia infoboxes are used to create the reference kb in this paper we provide an overview of the technique which can serve a a basis for a good kbp system lay out the remaining challenge by comparison with traditional information extraction ie and question answering qa task and provide some suggestion to address these challenge 
the amount of labeled sentiment data in english is much larger than that in other language such a disproportion arouse interest in cross lingual sentiment classification which aim to conduct sentiment classification in the target language e g chinese using labeled data in the source language e g english most existing work relies on machine translation engine to directly adapt labeled data from the source language to the target language this approach suffers from the limited coverage of vocabulary in the machine translation result in this paper we propose a generative cross lingual mixture model clmm to leverage unlabeled bilingual parallel data by fitting parameter to maximize the likelihood of the bilingual parallel data the proposed model learns previously unseen sentiment word from the large bilingual parallel data and improves vocabulary coverage significantly experiment on multiple data set show that clmm is consistently effective in two setting labeled data in the target language are unavailable and labeled data in the target language are also available 
we explore efficient domain adaptation for the task of statistical machine translation based on extracting sentence from a large general domain parallel corpus that are most relevant to the target domain these sentence may be selected with simple cross entropy based method of which we present three a these sentence are not themselves identical to the in domain data we call them pseudo in domain subcorpora these subcorpora the size of the original can then used to train small domain adapted statistical machine translation smt system which outperform system trained on the entire corpus performance is further improved when we use these domain adapted model in combination with a true in domain model the result show that more training data is not always better and that best result are attained via proper domain relevant data selection a well a combining inand general domain system during decoding 
this paper present a probabilistic framework that combine multiple knowledge source for haptic voice recognition hvr a multi modal input method designed to provide efficient text entry on modern mobile device hvr extends the conventional voice input by allowing user to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition this paper investigates the use of the initial letter of the word in the utterance a the partial lexical information in addition to the acoustic and language model used in automatic speech recognition system hvr us the haptic and partial lexical model a additional knowledge source to reduce the recognition search space and suppress confusion experimental result show that both the word error rate and runtime factor can be reduced by a factor of two using hvr 
most nlp system use tokenization a part of preprocessing generally tokenizers are based on simple heuristic and do not recognize multi word unit mwus like hot dog or black hole unless a precompiled list of mwus is available in this paper we propose a new cascaded model for detecting mwus of arbitrary length for tokenization focusing on noun phrase in the physic domain we adopt a classification approach because unlike other work on mwus tokenization requires a completely automatic approach we achieve an accuracy of for recognizing non compositional mwus and show that our mwu recognizer improves retrieval performance when used a part of an information retrieval system 
tweet represent a critical source of fresh information in which named entity occur frequently with rich variation we study the problem of named entity normalization nen for tweet two main challenge are the error propagated from named entity recognition ner and the dearth of information in a single tweet we propose a novel graphical model to simultaneously conduct ner and nen on multiple tweet to address these challenge particularly our model introduces a binary random variable for each pair of word with the same lemma across similar tweet whose value indicates whether the two related word are mention of the same entity we evaluate our method on a manually annotated data set and show that our method outperforms the baseline that handle these two task separately boosting the f from to for ner and the accuracy from to for nen respectively 
based on analysis of on line review corpus we observe that most sentence have complicated opinion structure and they cannot be well represented by existing method such a frame based and feature based one in this work we propose a novel graph based representation for sentence level sentiment an integer linear programming based structural learning method is then introduced to produce the graph representation of input sentence experimental evaluation on a manually labeled chinese corpus demonstrate the effectiveness of the proposed approach 
the automatic coding of clinical document is an important task for today s healthcare provider though it can be viewed a multi label document classification the coding problem ha the interesting property that most code assignment can be supported by a single phrase found in the input document we propose a lexically triggered hidden markov model lt hmm that leverage these phrase to improve coding accuracy the lt hmm work in two stage first a lexical match is performed against a term dictionary to collect a set of candidate code for a document next a discriminative hmm selects the best subset of code to assign to the document by tagging candidate a present or absent by confirming code proposed by a dictionary the lt hmm can share feature across code enabling strong performance even on rare code in fact we are able to recover code that do not occur in the training set at all our approach achieves the best ever performance on the medical nlp challenge test set with an f measure of 
we propose an algorithm allowing to efficiently retrieve example treelet in a parsed tree database in order to allow on the fly extraction of syntactic translation rule we also propose improvement of this algorithm allowing several kind of flexible matchings 
for sentence compression we propose new semantic constraint to directly capture the relation between a predicate and it argument whereas the existing approach have focused on relatively shallow linguistic property such a lexical and syntactic information these constraint are based on semantic role and superior to the constraint of syntactic dependency our empirical evaluation on the written news compression corpus clarke and lapata demonstrates that our system achieves result comparable to other state of the art technique 
in this paper we adopt an n best rescoring scheme using pitch accent pattern to improve automatic speech recognition asr performance the pitch accent model is decoupled from the main asr system thus allowing u to develop it independently n best hypothesis from recognizers are rescored by additional score that measure the correlation of the pitch accent pattern between the acoustic signal and lexical cue to test the robustness of our algorithm we use two different data set and recognition setup the first one is english radio news data that ha pitch accent label but the recognizer is trained from a small amount of data and ha high error rate the second one is english broadcast news data using a state of the art sri recognizer our experimental result demonstrate that our approach is able to reduce word error rate relatively by about this gain is consistent across the two different test showing promising future direction of incorporating prosodic information to improve speech recognition 
the named entity disambiguation task is to resolve the many to many correspondence between ambiguous name and the unique real world entity this task can be modeled a a classification problem provided that positive and negative example are available for learning binary classifier high quality sense annotated data however are hard to be obtained in streaming environment since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream on the other hand few positive example plus large amount of unlabeled data may be easily acquired producing binary classifier directly from this data however lead to poor disambiguation performance thus we propose to enhance the quality of the classifier using finer grained variation of the well known expectation maximization em algorithm we conducted a systematic evaluation using twitter streaming data and the result show that our classifier are extremely effective providing improvement ranging from to when compared to the current state of the art biased svms being more than time faster 
disambiguating named entity in natural language text map mention of ambiguous name onto canonical entity like people or place registered in a knowledge base such a dbpedia or yago this paper present a robust method for collective disambiguation by harnessing context from knowledge base and using a new form of coherence graph it unifies prior approach into a comprehensive framework that combine three measure the prior probability of an entity being mentioned the similarity between the context of a mention and a candidate entity a well a the coherence among candidate entity for all mention together the method build a weighted graph of mention and candidate entity and computes a dense subgraph that approximates the best joint mention entity mapping experiment show that the new method significantly outperforms prior method in term of accuracy with robust behavior across a variety of input 
this work present a first step to a general implementation of the semantic script theory of humor ssth of the scarce amount of research in computational humor no research had focused on humor generation beyond simple pun and punning riddle we propose an algorithm for mining simple humorous script from a semantic network concept net by specifically searching for dual script that jointly maximize overlap and incongruity metric in line with raskin s semantic script theory of humor initial result show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddle we evaluate the said metric through a user assessed quality of the generated two liner 
we present a novel approach to grammatical error correction based on alternating structure optimization a part of our work we introduce the nu corpus of learner english nucle a fully annotated one million word corpus of learner english available for research purpose we conduct an extensive evaluation for article and preposition error using various feature set our experiment show that our approach outperforms two baseline trained on non learner text and learner text respectively our approach also outperforms two commercial grammar checking software package 
we consider a new subproblem of unsupervised parsing from raw text unsupervised partial parsing the unsupervised version of text chunking we show that addressing this task directly using probabilistic finite state method produce better result than relying on the local prediction of a current best unsu pervised parser seginer s ccl these finite state model are combined in a cascade to produce more general full sentence constituent structure doing so outperforms ccl by a wide margin in unlabeled parseval score for english german and chinese finally we address the use of phrasal punctuation a a heuristic indicator of phrasal boundary both in our system and in ccl 
joint sentiment topic jst model wa previously proposed to detect sentiment and topic simultaneously from text the only supervision required by jst model learning is domain independent polarity word prior in this paper we modify the jst model by incorporating word polarity prior through modifying the topic word dirichlet prior we study the polarity bearing topic extracted by jst and show that by augmenting the original feature space with polarity bearing topic the in domain supervised classifier learned from augmented feature representation achieve the state of the art performance of on the movie review data and an average of on the multi domain sentiment dataset furthermore using feature augmentation and selection according to the information gain criterion for cross domain sentiment classification our proposed approach performs either better or comparably compared to previous approach nevertheless our approach is much simpler and doe not require difficult parameter tuning 
the language mix consists of all string over the three letter alphabet a b c that contain an equal number of occurrence of each letter we prove joshi s conjecture that mix is not a tree adjoining language 
we present a joint model for chinese word segmentation and new word detection we present high dimensional new feature including word based feature and enriched edge label transition feature for the joint modeling a we know training a word segmentation system on large scale datasets is already costly in our case adding high dimensional new feature will further slow down the training speed to solve this problem we propose a new training method adaptive online gradient descent based on feature frequency information for very fast online training of the parameter even given large scale datasets with high dimensional feature compared with existing training method our training method is an order magnitude faster in term of training time and can achieve equal or even higher accuracy the proposed fast training method is a general purpose optimization method and it is not limited in the specific task discussed in this paper 
it is popular for user in web era to freely annotate online resource with tag to ease the annotation process it ha been great interest in automatic tag suggestion we propose a method to suggest tag according to the text description of a resource by considering both the description and tag of a given resource a summary to the resource written in two language we adopt word alignment model in statistical machine translation to bridge their vocabulary gap based on the translation probability between the word in description and the tag estimated on a large set of description tag pair we build a word trigger method wtm to suggest tag according to the word in a resource description experiment on real world datasets show that wtm is effective and robust compared with other method moreover wtm is relatively simple and efficient which is practical for web application 
we present a simple objective function that when optimized yield accurate solution to both decipherment and cognate pair identification problem the objective simultaneously score a matching between two alphabet and a matching between two lexicon each in a different language we introduce a simple coordinate descent procedure that efficiently find effective solution to the resulting combinatorial optimization problem our system requires only a list of word in both language a input yet it competes with and surpasses several state of the art system that are both substantially more complex and make use of more information 
smt ha been used in paraphrase generation by translating a source sentence into another pivot language and then back into the source the resulting sentence can be used a candidate paraphrase of the source sentence existing work that us two independently trained smt system cannot directly optimize the paraphrase result paraphrase criterion especially the paraphrase rate is not able to be ensured in that way in this paper we propose a joint learning method of two smt system to optimize the process of paraphrase generation in addition a revised bleu score called ibleu which measure the adequacy and diversity of the generated paraphrase sentence is proposed for tuning parameter in smt system our experiment on nist testing data with automatic evaluation a well a human judgment suggest that the proposed method is able to enhance the paraphrase quality by adjusting between semantic equivalency and surface dissimilarity 
in this paper we propose a novel method of reducing the size of translation model for hierarchical phrase based machine translation system previous approach try to prune infrequent entry or unreliable entry based on statistic but cause a problem of reducing the translation coverage on the contrary the proposed method try to prune only ineffective entry based on the estimation of the information redundancy encoded in phrase pair and hierarchical rule and thus preserve the search space of smt decoder a much a possible experimental result on chinese to english machine translation task show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance 
syntactic analysis of search query is important for a variety of information retrieval task however the lack of annotated data make training query analysis model difficult we propose a simple efficient procedure in which part of speech tag are transferred from retrieval result snippet to query at training time unlike previous work our final model doe not require any additional resource at run time compared to a state of the art approach we achieve more than relative error reduction additionally we annotate a corpus of search query with part of speech tag providing a resource for future work on syntactic query analysis 
polarity classification of word is important for application such a opinion mining and sentiment analysis a number of sentiment word sense dictionary have been manually or semi automatically constructed the dictionary have substantial inaccuracy besides obvious instance where the same word appears with different polarity in different dictionary the dictionary exhibit complex case which cannot be detected by mere manual inspection we introduce the concept of polarity consistency of word sens in sentiment dictionary in this paper we show that the consistency problem is np complete we reduce the polarity consistency problem to the satisfiability problem and utilize a fast sat solver to detect inconsistency in a sentiment dictionary we perform experiment on four sentiment dictionary and wordnet 
we propose a novel heuristic algorithm for cube pruning running in linear time in the beam size empirically we show a gain in running time of a standard machine translation system at a small loss in accuracy 
comparing one thing with another is a typical part of human decision making process however it is not always easy to know what to compare and what are the alternative in this paper we present a novel way to automatically mine comparable entity from comparative question that user posted online to address this difficulty to ensure high precision and high recall we develop a weakly supervised bootstrapping approach for comparative question identification and comparable entity extraction by leveraging a large collection of online question archive the experimental result show our method achieves f measure of percent in comparative question identification and percent in comparable entity extraction both significantly outperform an existing state of the art method additionally our ranking result show highly relevance to user s comparison intent in web 
this paper develops a framework for syntactic dependency parse correction dependency in an input parse tree are revised by selecting for a given dependent the best governor from within a small set of candidate we use a discriminative linear ranking model to select the best governor from a group of candidate for a dependent and our model includes a rich feature set that encodes syntactic structure in the input parse tree the parse correction framework is parser agnostic and can correct attachment using either a generic model or specialized model tailored to difficult attachment type like coordination and pp attachment our experiment show that parse correction combining a generic model with specialized model for difficult attachment type can successfully improve the quality of predicted parse tree output by several representative state of the art dependency parser for french 
this paper proposes a data driven method for concept to text generation the task of automatically producing textual output from non linguistic input a key insight in our approach is to reduce the task of content selection what to say and surface realization how to say into a common parsing problem we define a probabilistic context free grammar that describes the structure of the input a corpus of database record and text describing some of them and represent it compactly a a weighted hypergraph the hypergraph structure encodes exponentially many derivation which we rerank discriminatively using local and global feature we propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting experimental evaluation on the atis domain show that our model outperforms a competitive discriminative system both using bleu and in a judgment elicitation study 
this paper present grammar error correction for japanese particle that us discriminative sequence conversion which corrects erroneous particle by substitution insertion and deletion the error correction task is hindered by the difficulty of collecting large error corpus we tackle this problem by using pseudo error sentence generated automatically furthermore we apply domain adaptation the pseudo error sentence are from the source domain and the real error sentence are from the target domain experiment show that stable improvement is achieved by using domain adaptation 
we investigate an important and challenging problem in summary generation i e evolutionary trans temporal summarization etts which generates news timeline from massive data on the internet etts greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity given the collection of time stamped web document related to the evolving news etts aim to return news evolution along the timeline consisting of individual but correlated summary on each date existing summarization algorithm fail to utilize trans temporal characteristic among these component summary we propose to model trans temporal correlation among component summary for timeline using inter date and intra date sentence dependency and present a novel combination we develop experimental system to compare rival algorithm on instinctively different datasets which amount to document evaluation result in rouge metric indicate the effectiveness of the proposed approach based on trans temporal information 
in this paper we propose a method to automatically label multi lingual data with named entity tag we build on prior work utilizing wikipedia metadata and show how to effectively combine the weak annotation stemming from wikipedia metadata with information obtained through english foreign language parallel wikipedia sentence the combination is achieved using a novel semi crf model for foreign sentence tagging in the context of a parallel english sentence the model outperforms both standard annotation projection method and method based solely on wikipedia metadata 
this paper present a supervised pronoun anaphora resolution system based on factorial hidden markov model fhmms the basic idea is that the hidden state of fhmms are an explicit short term memory with an antecedent buffer containing recently described referent thus an observed pronoun can find it antecedent from the hidden buffer or in term of a generative model the entry in the hidden buffer generate the corresponding pronoun a system implementing this model is evaluated on the ace corpus with promising performance 
we exploit sketch technique especially the count min sketch a memory and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself these method use hashing to deal with massive amount of streaming text we apply count min sketch to approximate word pair count and exhibit their effectiveness on three important nlp task our experiment demonstrate that on all of the three task we get performance comparable to exact word pair count setting and state of the art system our method scale to gb of unzipped web data using bounded space of billion counter gb memory 
lot of chinese character are very productive in that they can form many structured word either a prefix or a suffix previous research in chinese word segmentation mainly focused on identifying only the word boundary without considering the rich internal structure of many word in this paper we argue that this is unsatisfying in many way both practically and theoretically instead we propose that word structure should be recovered in morphological analysis an elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction our probability model is trained with the penn chinese treebank and actually is able to parse both word and phrase structure in a unified way 
a one of the most popular micro blogging service twitter attracts million of user producing million of tweet daily shared information through this service spread faster than would have been possible with traditional source however the proliferation of user generation content pose challenge to browsing and finding valuable information in this paper we propose a graph theoretic model for tweet recommendation that present user with item they may have an interest in our model rank tweet and their author simultaneously using several network the social network connecting the user the network connecting the tweet and a third network that tie the two together tweet and author entity are ranked following a co ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweet and their author that could be reflected in the ranking we show that this framework can be parametrized to take into account user preference the popularity of tweet and their author and diversity experimental evaluation on a large dataset show that our model outperforms competitive approach by a large margin 
previous research ha conflicting conclusion on whether word sense disambiguation wsd system can improve information retrieval ir performance in this paper we propose a method to estimate sense distribution for short query together with the sens predicted for word in document we propose a novel approach to incorporate word sens into the language modeling approach to ir and also exploit the integration of synonym relation our experimental result on standard trec collection show that using the word sens tagged by a supervised wsd system we obtain significant improvement over a state of the art ir system 
we present a method for generating colloquial egyptian arabic cea from morphologically disambiguated modern standard arabic msa when used in po tagging this process improves the accuracy from to on unseen cea text and reduces the percentage of out of vocabulary word from to the process hold promise for any nlp task targeting the dialectal variety of arabic e g this approach may provide a cheap way to leverage msa data and morphological resource to create resource for colloquial arabic to english machine translation it can also considerably speed up the annotation of arabic dialect 
a rumor is commonly defined a a statement whose true value is unverifiable rumor may spread misinformation false information or disinformation deliberately false information on a network of people identifying rumor is crucial in online social medium where large amount of information are easily spread across a large network by source with unverified authority in this paper we address the problem of rumor detection in microblogs and explore the effectiveness of category of feature content based network based and microblog specific meme for correctly identifying rumor moreover we show how these feature are also effective in identifying disinformers user who endorse a rumor and further help it to spread we perform our experiment on more than manually annotated tweet collected from twitter and show how our retrieval model achieves more than in mean average precision map finally we believe that our dataset is the first large scale dataset on rumor detection it can open new dimension in analyzing online misinformation and other aspect of microblog conversation 
context dependent word similarity can be measured over multiple cross cutting dimension for example lung and breath are similar thematically while authoritative and superficial occur in similar syntactic context but share little semantic similarity both of these notion of similarity play a role in determining word meaning and hence lexical semantic model must take them both into account towards this end we develop a novel model multi view mixture mvm that represents word a multiple overlapping clustering mvm find multiple data partition based on different subset of feature subject to the marginal constraint that feature subset are distributed according to latent dirichlet allocation intuitively this constraint favor feature partition that have coherent topical semantics furthermore mvm us soft feature assignment hence the contribution of each data point to each clustering view is variable isolating the impact of data only to view where they assign the most feature through a series of experiment we demonstrate the utility of mvm a an inductive bias for capturing relation between word that are intuitive to human outperforming related model such a latent dirichlet allocation 
this paper describes a novel approach towards the empirical approximation of discourse relation between different utterance in text following the idea that every pair of event come with preference regarding the range and frequency of discourse relation connecting both part the paper investigates whether these preference are manifested in the distribution of relation word that serve to signal these relation experiment on two large scale english web corpus show that significant correlation between pair of adjacent event and relation word exist that they are reproducible on different data set and for three relation word that their distribution corresponds to theory based assumption 
this paper introduces a psycholinguistic model of sentence processing which combine a hidden markov model noun phrase chunker with a co reference classifier both model are fully incremental and generative giving probability of lexical element conditional upon linguistic structure this allows u to compute the information theoretic measure of surprisal which is known to correlate with human processing effort we evaluate our surprisal prediction on the dundee corpus of eye movement data show that our model achieve a better fit with human reading time than a syntax only model which doe not have access to co reference information 
we predict entity type distribution in web search query via probabilistic inference in graphical model that capture how entity bearing query are generated we jointly model the interplay between latent user intent that govern query and unobserved entity type leveraging observed signal from query formulation and document click we apply the model to resolve entity type in new query and to assign prior type distribution over an existing knowledge base our model are efficiently trained using maximum likelihood estimation over million of real world web search query we show that modeling user intent significantly improves entity type resolution for head query over the state of the art on several metric without degradation in tail query performance 
the chinese comma signal the boundary of discourse unit and also anchor discourse relation between adjacent text span in this work we propose a discourse structure oriented classification of the comma that can be automatically extracted from the chinese treebank based on syntactic pattern we then experimented with two supervised learning method that automatically disambiguate the chinese comma based on this classification the first method integrates comma classification into parsing and the second method adopts a post processing approach that extract feature from automatic par to train a classifier the experimental result show that the second approach compare favorably against the first approach 
the role of search query a available within query session or in isolation from one another in examined in the context of ranking the class label e g brazilian city business center hilly site extracted from web document for various instance e g rio de janeiro the co occurrence of a class label and an instance in the same query or within the same query session is used to reinforce the estimated relevance of the class label for the instance experiment over evaluation set of instance associated with web search query illustrate the higher quality of the query based re ranked class label relative to ranking baseline using document based count 
interpreting news requires identifying it constituent event event are complex linguistically and ontologically so disambiguating their reference is challenging we introduce event linking which canonically label an event reference with the article where it wa first reported this implicitly relaxes coreference to co reporting and will practically enable augmenting news archive with semantic hyperlink we annotate and analyse a corpus of document extracting link to a news archive with reasonable inter annotator agreement 
in this paper we present a new method for learning to finding translation and transliteration on the web for a given term the approach involves using a small set of term and translation to obtain mixed code snippet from a search engine and automatically annotating the snippet with tag and feature for training a conditional random field model at run time the model is used to extracting translation candidate for a given term preliminary experiment and evaluation show our method cleanly combining various feature resulting in a system that outperforms previous work 
this paper present an extension of chiang s hierarchical phrase based hpb model called head driven hpb hd hpb which incorporates head information in translation rule to better capture syntax driven information a well a improved reordering between any two neighboring non terminal at any stage of a derivation to explore a larger reordering search space experiment on chinese english translation on four nist mt test set show that the hd hpb model significantly outperforms chiang s model with average gain of point absolute in bleu 
we design a class of submodular function meant for document summarization task these function each combine two term one which encourages the summary to be representative of the corpus and the other which positively reward diversity critically our function are monotone nondecreasing and submodular which mean that an efficient scalable greedy optimization scheme ha a constant factor guarantee of optimality when evaluated on duc corpus we obtain better than existing state of art result in both generic and query focused document summarization lastly we show that several well established method for document summarization correspond in fact to submodular function optimization adding further evidence that submodular function are a natural fit for document summarization 
n gram language model are a major resource bottleneck in machine translation in this paper we present several language model implementation that are both highly compact and fast to query our fastest implementation is a fast a the widely used srilm while requiring only of the storage our most compact representation can store all billion n gram and associated count for the google n gram corpus in bit per n gram the most compact lossless representation to date and even more compact than recent lossy compression technique we also discus technique for improving query speed during decoding including a simple but novel language model caching technique that improves the query speed of our language model and srilm by up to 
in this paper we introduce the novel task of word epoch disambiguation defined a the problem of identifying change in word usage over time through experiment run using word usage example collected from three major period of time we show that the task is feasible and significant difference can be observed between occurrence of word in different period of time 
this paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation the model extends hidden semi markov chain model by using factored output node and special structure for it conditional probability distribution it relies on morpho syntactic and lexical source side information part of speech morphological segmentation while learning a morpheme segmentation over the target language our model outperforms a competitive word alignment system in alignment quality used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state of the art model on three arabic and hebrew datasets 
we present a method that paraphrase a given sentence by first generating candidate paraphrase and then ranking or classifying them the candidate are generated by applying existing paraphrasing rule extracted from parallel corpus the ranking component considers not only the overall quality of the rule that produced each candidate but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence a well a the degree to which the candidate differs from the input we experimented with both a maximum entropy classifier and an svr ranker experimental result show that incorporating feature from an existing paraphrase recognizer in the ranking component improves performance and that our overall method compare well against a state of the art paraphrase generator when paraphrasing rule apply to the input sentence we also propose a new methodology to evaluate the ranking component of generate and rank paraphrase generator which evaluates them across different combination of weight for grammaticality meaning preservation and diversity the paper is accompanied by a paraphrasing dataset we constructed for evaluation of this kind 
part of speech language modeling is commonly used a a component in statistical machine translation system but there is mixed evidence that it usage lead to significant improvement we argue that it limited effectiveness is due to the lack of lexicalization we introduce a new approach that build a separate local language model for each word and part of speech pair the resulting model lead to more context sensitive probability distribution and we also exploit the fact that different local model are used to estimate the language model probability of each word during decoding our approach is evaluated for arabicand chinese to english translation we show that it lead to statistically significant improvement for multiple test set and also across different genre when compared against a competitive baseline and a system using a part of speech model 
in this paper we address the issue for learning better translation consensus in machine translation mt research and explore the search of translation consensus from similar rather than the same source sentence or their span unlike previous work on this topic we formulate the problem a structured labeling over a much smaller graph and we propose a novel structured label propagation for the task we convert such graph based translation consensus from similar source string into useful feature both for n best output re ranking and for decoding algorithm experimental result show that our method can significantly improve machine translation performance on both iwslt and nist data compared with a state of the art baseline 
this paper present a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control application such a game our ultimate goal is to enrich a stochastic player with high level guidance expressed in text our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategy guided by the selected text our method operates in the monte carlo search framework and learns both text analysis and game strategy based only on environment feedback we apply our approach to the complex strategy game civilization ii using the official game manual a the text guide our result show that a linguistically informed game playing agent significantly outperforms it language unaware counterpart yielding a absolute improvement and winning over of game when playing against the built in ai of civilization ii 
reranking model have been successfully applied to many task of natural language processing however there are two aspect of this approach that need a deeper investigation i assessment of hypothesis generated for reranking at classification phase baseline model generate a list of hypothesis and these are used for reranking without any assessment ii detection of case where reranking model provide a worst result the best hypothesis provided by the reranking model is assumed to be always the best result in some case the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct especially when baseline model are accurate in this paper we propose solution for these two aspect i a semantic inconsistency metric to select possibly more correct n best hypothesis from a large set generated by an slu basiline model the selected hypothesis are reranked applying a state of the art model based on partial tree kernel which encode slu hypothesis in support vector machine with complex structured feature ii finally we apply a decision strategy based on confidence value to select the final hypothesis between the first ranked hypothesis provided by the baseline slu model and the first ranked hypothesis provided by the re ranker we show the effectiveness of these solution presenting comparative result obtained reranking hypothesis generated by a very accurate conditional random field model we evaluate our approach on the french medium corpus the result show significant improvement with respect to current state of the art and previous re ranking model 
there are several task where is preferable not responding than responding incorrectly this idea is not new but despite several previous attempt there isn t a commonly accepted measure to ass non response we study here an extension of accuracy measure with this feature and a very easy to understand interpretation the measure proposed c ha a good balance of discrimination power stability and sensitivity property we show also how this measure is able to reward system that maintain the same number of correct answer and at the same time decrease the number of incorrect one by leaving some question unanswered this measure is well suited for task such a reading comprehension test where multiple choice per question are given but only one is correct 
we present a novel approach to data oriented parsing dop like other dop model our parser utilizes syntactic fragment of arbitrary size from a treebank to analyze new sentence but crucially it us only those which are encountered at least twice this criterion allows u to work with a relatively small but representative set of fragment which can be employed a the symbolic backbone of several probabilistic generative model for parsing we define a transform backtransform approach that allows u to use standard pcfg technology making our result easily replicable according to standard parseval metric our best model is on par with many state of the art parser while offering some complementary benefit a simple generative probability model and an explicit representation of the larger unit of grammar 
an a c bilingual dictionary can be inferred by merging a b and b c dictionary using b a pivot however polysemous pivot word often produce wrong translation candidate this paper analyzes two method for pruning wrong candidate one based on exploiting the structure of the source dictionary and the other based on distributional similarity computed from comparable corpus a both method depend exclusively on easily available resource they are well suited to le resourced language we studied whether these two technique complement each other given that they are based on different paradigm we also researched combining them by looking for the best adequacy depending on various application scenario 
learning by reading lbr aim at enabling machine to acquire knowledge from and reason about textual input this requires knowledge about the domain structure such a entity class and action in order to do inference we present a method to infer this implicit knowledge from unlabeled text unlike previous approach we use automatically extracted class with a probability distribution over entity to allow for context sensitive labeling from a corpus of m sentence we learn about k simple proposition about american football in the form of predicate argument structure like quarterback throw pass to receiver using several statistical measure we show that our model is able to generalize and explain the data statistically significantly better than various baseline approach human subject judged up to of the resulting proposition to be sensible the class and probabilistic model can be used in textual enrichment to improve the performance of lbr end to end system 
most information extraction ie system identify fact that are explicitly stated in text however in natural language some fact are implicit and identifying them requires reading between the line human reader naturally use common sense knowledge to infer such implicit information from the explicitly stated fact we propose an approach that us bayesian logic program blps a statistical relational model combining firstorder logic and bayesian network to infer additional implicit information from extracted fact it involves learning uncertain common sense knowledge in the form of probabilistic first order rule from natural language text by mining a large corpus of automatically extracted fact these rule are then used to derive additional fact from extracted information using blp inference experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach 
we report on empirical result in extreme extraction it is extreme in that from receipt of the ontology specifying the target concept and relation development is limited to one week and that relatively little training data is assumed we are able to surpass human recall and achieve an f of on a question answering task with le than hour of effort using a hybrid approach that mix active learning bootstrapping and limited hour manual rule writing we compare the performance of three system extraction with handwritten rule bootstrapped extraction and a combination we show that while the recall of the handwritten rule surpasses that of the learned system the learned system is able to improve the overall recall and f 
in the present paper we propose the effective usage of function word to generate generalized translation rule for forest based translation given aligned forest string pair we extract composed tree to string translation rule that account for multiple interpretation of both aligned and unaligned target function word in order to constrain the exhaustive attachment of function word we limit to bind them to the nearby syntactic chunk yielded by a target dependency parser therefore the proposed approach can not only capture source tree to target chunk correspondence but can also use forest structure that compactly encode an exponential number of parse tree to properly generate target function word during decoding extensive experiment involving large scale english to japanese translation revealed a significant improvement of point in bleu score a compared with a strong forest to string baseline system 
in this paper we develop an rst style text level discourse parser based on the hilda discourse parser hernault et al b we significantly improve it tree building step by incorporating our own rich linguistic feature we also analyze the difficulty of extending traditional sentence level discourse parsing to text level parsing by comparing discourse parsing performance under different discourse condition 
this research study the text genre of message board forum which contain a mixture of expository sentence that present factual information and conversational sentence that include communicative act between the writer and reader our goal is to create sentence classifier that can identify whether a sentence contains a speech act and can recognize sentence containing four different speech act class commissives directive expressives and representative we conduct experiment using a wide variety of feature including lexical and syntactic feature speech act word list from external resource and domain specific semantic class feature we evaluate our result on a collection of message board post in the domain of veterinary medicine 
one of the key task for analyzing conversational data is segmenting it into coherent topic segment however most model of topic segmentation ignore the social aspect of conversation focusing only on the word used we introduce a hierarchical bayesian nonparametric model speaker identity for topic segmentation sits that discovers the topic used in a conversation how these topic are shared across conversation when these topic shift and a person specific tendency to introduce new topic we evaluate against current unsupervised segmentation model to show that including person specific information improves segmentation performance on meeting corpus and on political debate moreover we provide evidence that sits capture an individual s tendency to introduce new topic in political context via analysis of the u presidential debate and the television program crossfire 
we propose a novel unsupervised method for separating out distinct authorial component of a document in particular we show that given a book artificially munged from two thematically similar biblical book we can separate out the two constituent book almost perfectly this allows u to automatically recapitulate many conclusion reached by bible scholar over century of research one of the key element of our method is exploitation of difference in synonym choice by different author 
this paper us an unsupervised model of grounded language acquisition to study the role that social cue play in language acquisition the input to the model consists of orthographically transcribed child directed utterance accompanied by the set of object present in the non linguistic context each object is annotated by social cue indicating e g whether the caregiver is looking at or touching the object we show how to model the task of inferring which object are being talked about and which word refer to which object a standard grammatical inference and describe pcfg based unigram model and adaptor grammar based collocation model for the task exploiting social cue improves the performance of all model our model learn the relative importance of each social cue jointly with word object mapping and collocation structure consistent with the idea that child could discover the importance of particular social information source during word learning 
in this paper we present a method for unsupervised semantic role induction which we formalize a a graph partitioning problem argument instance of a verb are represented a vertex in a graph whose edge weight quantify their role semantic similarity graph partitioning is realized with an algorithm that iteratively assigns vertex to cluster based on the cluster assignment of neighboring vertex our method is algorithmically and conceptually simple especially with respect to how problem specific knowledge is incorporated into the model experimental result on the conll benchmark dataset demonstrate that our model is competitive with other unsupervised approach in term of f whilst attaining significantly higher cluster purity 
we present a simple method for transferring dependency parser from source language with labeled training data to target language without labeled training data we first demonstrate that delexicalized parser can be directly transferred between language producing significantly higher accuracy than unsupervised parser we then use a constraint driven learning algorithm where constraint are drawn from parallel corpus to project the final parser unlike previous work on projecting syntactic resource we show that simple method for introducing multiple source language can significantly improve the overall quality of the resulting parser the projected parser from our system result in state of the art performance when compared to previously studied unsupervised and projected parsing system across eight different language 
we present a holistic data driven approach to image description generation exploiting the vast amount of noisy parallel image data and associated natural language description available on the web more specifically given a query image we retrieve existing human composed phrase used to describe visually similar image then selectively combine those phrase to generate a novel description for the query image we cast the generation process a constraint optimization problem collectively incorporating multiple interconnected aspect of language composition for content planning surface realization and discourse structure evaluation by human annotator indicates that our final system generates more semantically correct and linguistically appealing description than two nontrivial baseline 
the text analysis conference tac rank summarization system by their average score over a collection of document set we investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation system 
we introduce a novel bayesian approach for deciphering complex substitution cipher our method us a decipherment model which combine information from letter n gram language model a well a word dictionary bayesian inference is performed on our model using an efficient sampling technique we evaluate the quality of the bayesian decipherment output on simple and homophonic letter substitution cipher and show that unlike a previous approach our method consistently produce almost accurate decipherment the new method can be applied on more complex substitution cipher and we demonstrate it utility by cracking the famous zodiac cipher in a fully automated fashion which ha never been done before 
most previous study of morphological disambiguation and dependency parsing have been pursued independently morphological tagger operate on n gram and do not take into account syntactic relation parser use the pipeline approach assuming that morphological information ha been separately obtained however in morphologically rich language there is often considerable interaction between morphology and syntax such that neither can be disambiguated without the other in this paper we propose a discriminative model that jointly infers morphological property and syntactic structure in evaluation on various highly inflected language this joint model outperforms both a baseline tagger in morphological disambiguation and a pipeline parser in head selection 
translating compound is an important problem in machine translation since many compound have not been observed during training they pose a challenge for translation system previous decompounding method have often been restricted to a small set of language a they cannot deal with more complex compound forming process we present a novel and unsupervised method to learn the compound part and morphological operation needed to split compound into their compound part the method us a bilingual corpus to learn the morphological operation required to split a compound into it part furthermore monolingual corpus are used to learn and filter the set of compound part candidate we evaluate our method within a machine translation task and show significant improvement for various language to show the versatility of the approach 
we present a novel computational formulation of speaker authority in discourse this notion which focus on how speaker position themselves relative to each other in discourse is first developed into a reliable coding scheme agreement between human annotator we also provide a computational model for automatically annotating text using this coding scheme using supervised learning enhanced by constraint implemented with integer linear programming we show that this constrained model s analysis of speaker authority correlate very strongly with expert human judgment r coefficient of 
we analyze collective discourse a collective human behavior in content generation and show that it exhibit diversity a property of general collective system using extensive analysis we propose a novel paradigm for designing summary generation system that reflect the diversity of perspective seen in reallife collective summarization we analyze set of summary written by human about the same story or artifact and investigate the diversity of perspective across these summary we show how different summary use various phrasal information unit i e nugget to express the same atomic semantic unit called factoid finally we present a ranker that employ distributional similarity to build a network of word and capture the diversity of perspective by detecting community in this network our experiment show how our system outperforms a wide range of other document ranking system that leverage diversity 
polarity classification of opinionated sentence with both positive and negative sentiment is a key challenge in sentiment analysis this paper present a novel unsupervised method for discovering intra sentence level discourse relation for eliminating polarity ambiguity firstly a discourse scheme with discourse constraint on polarity wa defined empirically based on rhetorical structure theory rst then a small set of cuephrase based pattern were utilized to collect a large number of discourse instance which were later converted to semantic sequential representation ssrs finally an unsupervised method wa adopted to generate weigh and filter new ssrs without cue phrase for recognizing discourse relation experimental result showed that the proposed method not only effectively recognized the defined discourse relation but also achieved significant improvement by integrating discourse information in sentence level polarity classification 
we explore the contribution of morphological feature both lexical and inflectional to dependency parsing of arabic a morphologically rich language using controlled experiment we find that definiteness person number gender and the undiacritzed lemma are most helpful for parsing on automatically tagged input we further contrast the contribution of form based and functional feature and show that functional gender and number e g broken plural and the related rationality feature improve over form based feature it is the first time functional morphological feature are used for arabic nlp 
most previous work on multilingual sentiment analysis ha focused on method to adapt sentiment resource from resource rich language to resource poor language we present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data we rely on the intuition that the sentiment label for parallel sentence should be similar and present a model that jointly learns improved monolingual sentiment classifier for each language experiment on multiple data set show that the proposed approach outperforms the monolingual baseline significantly improving the accuracy for both language by outperforms two standard approach for leveraging unlabeled data and produce albeit smaller performance gain when employing pseudo parallel data from machine translation engine 
we propose a principled and efficient phrase to phrase alignment model useful in machine translation a well a other related natural language processing problem in a hidden semi markov model word to phrase and phrase to word translation are modeled directly by the system agreement between two directional model encourages the selection of parsimonious phrasal alignment avoiding the overfitting commonly encountered in unsupervised training with multi word unit expanding the state space to include gappy phrase such a french ne pa make the alignment space more symmetric thus it allows agreement between discontinuous alignment the resulting system show substantial improvement in both alignment quality and translation quality over word based hidden markov model while maintaining asymptotically equivalent runtime 
unsupervised vector based approach to semantics can model rich lexical meaning but they largely fail to capture sentiment information that is central to many word meaning and important for a wide range of nlp task we present a model that us a mix of unsupervised and supervised technique to learn word vector capturing semantic term document information a well a rich sentiment content the proposed model can leverage both continuous and multi dimensional sentiment information a well a non sentiment annotation we instantiate the model to utilize the document level sentiment polarity annotation present in many online document e g star rating we evaluate the model using small widely used sentiment and subjectivity corpus and find it out performs several previously introduced method for sentiment classification we also introduce a large dataset of movie review to serve a a more robust benchmark for work in this area 
in this paper we describe an unsupervised method for semantic role induction which hold promise for relieving the data acquisition bottleneck associated with supervised role labelers we present an algorithm that iteratively split and merges cluster representing semantic role thereby leading from an initial clustering to a final clustering of better quality the method is simple surprisingly effective and allows to integrate linguistic knowledge transparently by combining role induction with a rule based component for argument identification we obtain an unsupervised end to end semantic role labeling system evaluation on the conll benchmark dataset demonstrates that our method outperforms competitive unsupervised approach by a wide margin 
when designing grammar of natural language typically more than one formal analysis can account for a given phenomenon moreover because analysis interact the choice made by the engineer influence the possibility available in further grammar development the order in which phenomenon are treated may therefore have a major impact on the resulting grammar this paper proposes to tackle this problem by using metagrammar development a a methodology for grammar engineering i argue that metagrammar engineering a an approach facilitates the systematic exploration of grammar through comparison of competing analysis the idea is illustrated through a comparative study of auxiliary structure in hpsg based grammar for german and dutch auxiliary form a central phenomenon of german and dutch and are likely to influence many component of the grammar this study show that a special auxiliary verb construction significantly improves efficiency compared to the standard argument composition analysis for both parsing and generation 
we propose a probabilistic generative model for unsupervised semantic role induction which integrates local role assignment decision and a global role ordering decision in a unified model the role sequence is divided into interval based on the notion of primary role and each interval generates a sequence of secondary role and syntactic constituent using local feature the global role ordering consists of the sequence of primary role only thus making it a partial ordering 
unsupervised word alignment is most often modeled a a markov process that generates a sentence f conditioned on it translation e a similar model generating e from f will make different alignment prediction statistical machine translation system combine the prediction of two directional model typically using heuristic combination procedure like grow diag final this paper present a graphical model that embeds two directional aligners into a single model inference can be performed via dual decomposition which reuses the efficient inference algorithm of the directional model our bidirectional model enforces a one to one phrase constraint while accounting for the uncertainty in the underlying directional model the resulting alignment improve upon baseline combination heuristic in word level and phrase level evaluation 
building on earlier work that integrates different factor in language modeling we view i backing off to a shorter history and ii class based generalization a two complementary mechanism of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation this view entail that the class in a language model should be learned from rare event only and should be preferably applied to rare event we construct such a model and show that both training on rare event and preferable application to rare event improve perplexity when compared to a simple direct interpolation of class based with standard language model 
negation is present in all human language and it is used to reverse the polarity of part of statement that are otherwise affirmative by default a negated statement often carry positive implicit meaning but to pinpoint the positive part from the negative part is rather difficult this paper aim at thoroughly representing the semantics of negation by revealing implicit positive meaning the proposed representation relies on focus of negation detection for this new annotation over propbank and a learning algorithm are proposed 
in recent year error mining approach were developed to help identify the most likely source of parsing failure in parsing system using handcrafted grammar and lexicon however the technique they use to enumerate and count n gram build on the sequential nature of a text corpus and do not easily extend to structured data in this paper we propose an algorithm for mining tree and apply it to detect the most likely source of generation failure we show that this tree mining algorithm permit identifying not only error in the generation system grammar lexicon but also mismatch between the structure contained in the input and the input structure expected by our generator a well a a few idiosyncrasy error in the input data 
metonymic language is a pervasive phenomenon metonymic type shifting or argument type coercion result in a selectional restriction violation where the argument s semantic class differs from the class the predicate expects in this paper we present an un supervised method that learns the selectional restriction of argument and enables the detection of argument coercion this method also generates an enhanced probabilistic resolution of logical metonymy the experimental result indicate substantial improvement the detection of coercion and the ranking of metonymic interpretation 
dependency parsing is a central nlp task in this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotation we show that for three leading unsupervised parser klein and manning cohen and smith spitkovsky et al a a small set of parameter can be found whose modification yield a significant improvement in standard evaluation measure these parameter correspond to local case where no linguistic consensus exists a to the proper gold annotation therefore the standard evaluation doe not provide a true indication of algorithm quality we present a new measure neutral edge direction ned and show that it greatly reduces this undesired phenomenon 
we seek to automatically estimate typical duration for event and habit described in twitter tweet a corpus of more than million tweet containing temporal duration information wa collected these tweet were classified a to their habituality status using a bootstrapped decision tree for each verb lemma associated duration information wa collected for episodic and habitual us of the verb summary statistic for verb lemma and their typical habit and episode duration ha been compiled and made available this automatically generated duration information is broadly comparable to hand annotation 
microblogs such a twitter reflect the general public s reaction to major event bursty topic from microblogs reveal what event have attracted the most online attention although bursty event detection from text stream ha been studied before previous work may not be suitable for microblogs because compared with other text stream such a news article and scientific publication microblog post are particularly diverse and noisy to find topic that have bursty pattern on microblogs we propose a topic model that simultaneously capture two observation post published around the same time are more likely to have the same topic and post published by the same user are more likely to have the same topic the former help find event driven post while the latter help identify and filter out personal post our experiment on a large twitter dataset show that there are more meaningful and unique bursty topic in the top ranked result returned by our model than an lda baseline and two degenerate variation of our model we also show some case study that demonstrate the importance of considering both the temporal information and user personal interest for bursty topic detection from microblogs 
in this paper we introduce a connotation lexicon a new type of lexicon that list word with connotative polarity i e word with positive connotation e g award promotion and word with negative connotation e g cancer war connotation lexicon differ from much studied sentiment lexicon the latter concern word that express sentiment while the former concern word that evoke or associate with a specific polarity of sentiment understanding the connotation of word would seem to require common sense and world knowledge however we demonstrate that much of the connotative polarity of word can be inferred from natural language text in a nearly unsupervised manner the key linguistic insight behind our approach is selectional preference of connotative predicate we present graph based algorithm using pagerank and hit that collectively learn connotation lexicon together with connotative predicate our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicon 
we describe a discourse annotation scheme for chinese and report on the preliminary result our scheme inspired by the penn discourse treebank pdtb adopts the lexically grounded approach at the same time it make adaptation based on the linguistic and statistical characteristic of chinese text annotation result show that these adaptation work well in practice our scheme taken together with other pdtb style scheme e g for english turkish hindi and czech affords a broader perspective on how the generalized lexically grounded approach can flesh itself out in the context of cross linguistic annotation of discourse relation 
this paper describes movie dic a movie dialogue corpus recently collected for research and development purpose the collected dataset comprises dialogue containing a total of turn that have been extracted from movie detail on how the data collection ha been created and how it is structured are provided along with it main statistic and characteristic 
we present an online learning algorithm for training parser which allows for the inclusion of multiple objective function the primary example is the extension of a standard supervised parsing objective function with additional loss function either based on intrinsic parsing quality or task specific extrinsic measure of quality our empirical result show how this approach performs for two dependency parsing algorithm graph based and transition based parsing and how it achieves increased performance on multiple target task including reordering for machine translation and parser adaptation 
we propose a sentence generation strategy that describes image by predicting the most likely noun verb scene and preposition that make up the core sentence structure the input are initial noisy estimate of the object and scene detected in the image using state of the art trained detector a predicting action from still image directly is unreliable we use a language model trained from the english gigaword corpus to obtain their estimate together with probability of co located noun scene and preposition we use these estimate a parameter on a hmm that model the sentence generation process with hidden node a sentence component and image detection a the emission experimental result show that our strategy of combining vision and language produce readable and descriptive sentence compared to naive strategy that use vision alone 
while it is generally accepted that many translation phenomenon are correlated with linguistic structure employing linguistic syntax for translation ha proven a highly non trivial task the key assumption behind many approach is that translation is guided by the source and or target language parse employing rule extracted from the parse tree or performing tree transformation these approach enforce strict constraint and might overlook important translation phenomenon that cross linguistic constituent we propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side our method induces joint probability synchronous grammar and estimate their parameter by selecting and weighing together linguistically motivated rule according to an objective function directly targeting generalisation over future data we obtain statistically significant improvement across different language pair with english a source mounting up to bleu for chinese a target 
we propose an improved bottom up method for converting ccg derivation into ptb style phrase structure tree in contrast with past work clark and curran which used simple transduction on category pair our approach us richer transduction attached to single category our conversion preserve more sentence under round trip conversion v and is more robust in particular unlike past method ours doe not require ad hoc rule over non local feature and so can be easily integrated into a parser 
this paper compare several translation representation for a synchronous context free grammar parse including cfgs hypergraphs finite state automaton fsa and pushdown automaton pda the representation choice is shown to determine the form and complexity of target lm intersection and shortest path algorithm that follow intersection shortest path fsa expansion and rtn replacement algorithm are presented for pda chinese to english translation experiment using hifst and hipdt fsa and pda based decoder are presented using admissible or exact search possible for hifst with compact scfg rulesets and hipdt with compact lm for large rulesets with large lm we introduce a two pas search strategy which we then analyze in term of search error and translation performance 
we consider the problem of correcting error made by english a a second language esl writer and address two issue that are essential to making progress in esl error correction algorithm selection and model adaptation to the first language of the esl learner a variety of learning algorithm have been applied to correct esl mistake but often comparison were made between incomparable data set we conduct an extensive fair comparison of four popular learning method for the task reversing conclusion from earlier evaluation our result hold for different training set genre and feature set a second key issue in esl error correction is the adaptation of a model to the first language of the writer error made by non native speaker exhibit certain regularity and a we show model perform much better when they use knowledge about error pattern of the non native writer we propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation method 
previous work on classifying information status nissim rahman and ng is restricted to coarse grained classification and focus on conversational dialogue we here introduce the task of classifying fine grained information status and work on written text we add a fine grained information status layer to the wall street journal portion of the ontonotes corpus we claim that the information status of a mention depends not only on the mention itself but also on other mention in the vicinity and solve the task by collectively classifying the information status of all mention our approach strongly outperforms reimplementations of previous work 
we present a hierarchical chunk to string translation model which can be seen a a compromise between the hierarchical phrase based model and the tree to string model to combine the merit of the two model with the help of shallow parsing our model learns rule consisting of word and chunk and meanwhile introduce syntax cohesion under the weighed synchronous context free grammar defined by these rule our model search for the best translation derivation and yield target translation simultaneously our experiment show that our model significantly outperforms the hierarchical phrase based model and the tree to string model on english chinese translation task 
real document collection do not fit the independence assumption asserted by most statistical topic model but how badly do they violate them we present a bayesian method for measuring how well a topic model fit a corpus our approach is based on posterior predictive checking a method for diagnosing bayesian model in user defined way our method can identify where a topic model fit the data where it fall short and in which direction it might be improved 
we investigate the consistency of human assessor involved in summarization evaluation to understand it effect on system ranking and automatic evaluation technique using text analysis conference data we measure annotator consistency based on human scoring of summary for responsiveness readability and pyramid scoring we identify inconsistency in the data and measure to what extent these inconsistency affect the ranking of automatic summarization system finally we examine the stability of automatic metric rouge and classy with respect to the inconsistent assessment 
we investigate the problem of acoustic modeling in which prior language specific knowledge and transcribed data are unavailable we present an unsupervised model that simultaneously segment the speech discovers a proper set of sub word unit e g phone and learns a hidden markov model hmm for each induced acoustic unit our approach is formulated a a dirichlet process mixture model in which each mixture is an hmm that represents a sub word unit we apply our model to the timit corpus and the result demonstrate that our model discovers sub word unit that are highly correlated with english phone and also produce better segmentation than the state of the art unsupervised baseline we test the quality of the learned acoustic model on a spoken term detection task compared to the baseline our model improves the relative precision of top hit by at least and outperforms a language mismatched acoustic model 
many machine translation evaluation metric have been proposed after the seminal bleu metric and many among them have been found to consistently outperform bleu demonstrated by their better correlation with human judgment it ha long been the hope that by tuning machine translation system against these new generation metric advance in automatic machine translation evaluation can lead directly to advance in automatic machine translation however to date there ha been no unambiguous report that these new metric can improve a state of the art machine translation system over it bleu tuned baseline in this paper we demonstrate that tuning joshua a hierarchical phrase based statistical machine translation system with the tesla metric result in significantly better human judged translation quality than the bleu tuned baseline tesla m in particular is simple and performs well in practice on large datasets we release all our implementation under an open source license it is our hope that this work will encourage the machine translation community to finally move away from bleu a the unquestioned default and to consider the new generation metric when tuning their system 
we learn a joint model of sentence extraction and compression for multi document summarization our model score candidate summary according to a combined linear model whose feature factor over the n gram type in the summary and the compression used we train the model using a margin based objective whose loss capture end summary quality because of the exponentially large set of candidate summary we use a cutting plane algorithm to incrementally detect and add active constraint efficiently inference in our model can be cast a an ilp and thereby solved in reasonable time we also present a fast approximation scheme which achieves similar performance our jointly extracted and compressed summary outperform both unlearned baseline and our learned extraction only system on both rouge and pyramid without a drop in judged linguistic quality we achieve the highest published rouge result to date on the tac data set 
we present an approach for detecting salient important date in text in order to automatically build event timeline from a search query e g the name of an event or person etc this work wa carried out on a corpus of newswire text in english provided by the agence france presse afp in order to extract salient date that warrant inclusion in an event timeline we first recognize and normalize temporal expression in text and then use a machine learning approach to extract salient date that relate to a particular topic we focused only on extracting the date and not the event to which they are related 
this paper show that full abstraction can be accomplished in the context of guided summarization we describe a work in progress that relies on information extraction statistical content selection and natural language generation early result already demonstrate the effectiveness of the approach 
tree to string translation is syntax aware and efficient but sensitive to parsing error forest to string translation approach mitigate the risk of propagating parser error into translation error by considering a forest of alternative tree a generated by a source language parser we propose an alternative approach to generating forest that is based on combining sub tree within the first best parse through binarization provably our binarization forest can cover any non consitituent phrase in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small for the purpose of reducing search error we apply the synchronous binarization technique to forest to string decoding combining the two technique we show that using a fast shift reduce parser we can achieve significant quality gain in nist english to chinese track bleu point over a phrase based system bleu point over a hierarchical phrase based system consistent and significant gain are also shown in wmt in the english to german french spanish and czech track 
conversation provide rich opportunity for interactive continuous learning when something go wrong a system can ask for clarification rewording or otherwise redirect the interaction to achieve it goal in this paper we present an approach for using conversational interaction of this type to induce semantic parser we demonstrate learning without any explicit annotation of the meaning of user utterance instead we model meaning with latent variable and introduce a loss function to measure how well potential meaning match the conversation this loss drive the overall learning approach which induces a weighted ccg grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system experiment on darpa communicator conversational log demonstrate effective learning despite requiring no explicit meaning annotation 
naively collecting translation by crowd sourcing the task to non professional translator yield disfluent low quality result if no quality control is exercised we demonstrate a variety of mechanism that increase the translation quality to near professional level specifically we solicit redundant translation and edits to them and automatically select the best output among them we propose a set of feature that model both the translation and the translator such a country of residence lm perplexity of the translation edit rate from the other translation and optionally calibration against professional translator using these feature to score the collected translation we are able to discriminate between acceptable and unacceptable translation we recreate the nist urdu to english evaluation set with mechanical turk and quantitatively show that our model are able to select translation within the range of quality that we expect from professional translator the total cost is more than an order of magnitude lower than professional translation 
state of the art tree structure prediction technique rely on bottom up decoding these approach allow the use of context free feature and bottom up feature we discus the limitation of mainstream technique in solving common natural language processing task then we devise a new framework that go beyond bottom up decoding and that allows a better integration of contextual feature furthermore we design a system that address these issue and we test it on hierarchical machine translation a well known tree structure prediction problem the structure of the proposed system allows the incorporation of non bottom up feature and relies on a more sophisticated decoding approach we show that the proposed approach can find better translation using a smaller portion of the search space 
word boundary within noun compound are not marked by white space in a number of language unlike in english and it is beneficial for various nlp application to split such noun compound in the case of japanese noun compound made up of katakana word i e transliterated foreign word are particularly difficult to split because katakana word are highly productive and are often out of vocabulary to overcome this difficulty we propose using monolingual and bilingual paraphrase of katakana noun compound for identifying word boundary experiment demonstrated that splitting accuracy is substantially improved by extracting such paraphrase from unlabeled textual data the web in our case and then using that information for constructing splitting model 
discriminative training for machine translation ha been well studied in the recent past a limitation of the work to date is that it relies on the availability of high quality in domain bilingual text for supervised training we present an unsupervised discriminative training framework to incorporate the usually plentiful target language monolingual data by using a rough reverse translation system intuitively our method strives to ensure that probabilistic round trip translation from a target language sentence to the source language and back will have low expected loss theoretically this may be justified a discriminatively minimizing an imputed empirical risk empirically we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both iwslt and nist task 
this paper describes a novel probabilistic approach for generating natural language sentence from their underlying semantics in the form of typed lambda calculus the approach is built on top of a novel reduction based weighted synchronous context free grammar formalism which facilitates the transformation process from typed lambda calculus into natural language sentence sentence can then be generated based on such grammar rule with a log linear model to acquire such grammar rule automatically in an unsupervised manner we also propose a novel approach with a generative model which map from sub expression of logical form to word sequence in natural language sentence experiment on benchmark datasets for both english and chinese generation task yield significant improvement over result obtained by two state of the art machine translation model in term of both automatic metric and human evaluation 
most traditional summarization method treat their output a static and plain text which fail to capture user interest during summarization because the generated summary are the same for different user however user have individual preference on a particular source document collection and obviously a universal summary for all user might not always be satisfactory hence we investigate an important and challenging problem in summary generation i e interactive personalized summarization ip which generates summary in an interactive and personalized manner given the source document ip capture user interest by enabling interactive click and incorporates personalization by modeling captured reader preference we develop experimental system to compare rival algorithm on instinctively different datasets which amount to document evaluation result in rouge metric indicate the comparable performance between ip and the best competing system but ip produce summary with much more user satisfaction according to evaluator rating besides low rouge consistency among these user preferred summary indicates the existence of personalization 
this paper introduces an attribute selection task a a way to characterize the inherent meaning of property denoting adjective in adjective noun phrase such a e g hot in hot summer denoting the attribute temperature rather than taste we formulate this task in a vector space model that represents adjective and noun a vector in a semantic space defined over possible attribute the vector incorporate latent semantic information obtained from two variant of lda topic model our lda model outperform previous approach on a small set of attribute with considerable gain on sparse representation which highlight the strong smoothing power of lda model for the first time we extend the attribute selection task to a new data set with more than class we observe that large scale attribute selection is a hard problem but a subset of attribute performs robustly on the large scale a well again the lda model outperform the vsm baseline 
we propose a method for automatically labelling topic learned via lda topic model we generate our label candidate set from the top ranking topic term title of wikipedia article containing the top ranking topic term and sub phrase extracted from the wikipedia article title we rank the label candidate using a combination of association measure and lexical feature optionally fed into a supervised ranking model our method is shown to perform strongly over four independent set of topic significantly better than a benchmark method 
most previous graph based parsing model increase decoding complexity when they use high order feature due to exact inference decoding in this paper we present an approach to enriching high order feature representation for graph based dependency parsing model using a dependency languagemodel and beam search the dependency language model is built on a large amount of additional auto parsed data that is processed by a baseline parser based on the dependency language model we represent a set of feature for the parsing model finally the feature are efficiently integrated into the parsing model during decoding using beam search our approach ha two advantage firstly we utilize rich high order feature defined over a view of large scope and additional large raw corpus secondly our approach doe not increase the decoding complexity we evaluate the proposed approach on english and chinese data the experimental result show that our new parser achieves the best accuracy on the chinese data and comparable accuracy with the best known system on the english data 
in this paper we present a unified model for the automatic induction of word sens from text and the subsequent disambiguation of particular word instance using the automatically extracted sense inventory the induction step and the disambiguation step are based on the same principle word and context are mapped to a limited number of topical dimension in a latent semantic word space the intuition is that a particular sense is associated with a particular topic so that different sens can be discriminated through their association with particular topical dimension in a similar vein a particular instance of a word can be disambiguated by determining it most important topical dimension the model is evaluated on the semeval word sense induction and disambiguation task on which it reach state of the art result 
although discriminative training guarantee to improve statistical machine translation by incorporating a large amount of overlapping feature it is hard to scale up to large data due to decoding complexity we propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivation that exactly yield the reference translation with million of feature trained on k sentence in second per sentence our system achieves significant improvement by bleu over the baseline system on the nist chinese english test set 
we introduce synchronous tree adjoining grammar tag into tree to string translation which convert a source tree to a target string without reconstructing tag derivation explicitly our rule extraction algorithm directly learns tree to string rule from aligned treebank style tree a tree to string translation cast decoding a a tree parsing problem rather than parsing the decoder still run fast when adjoining is included le than time slower the adjoining tree to string system improves translation quality by bleu over the baseline system only allowing for tree substitution on nist chinese english test set 
we examine some of the frequently disregarded subtlety of tokenization in penn treebank style and present a new rule based preprocessing toolkit that not only reproduces the treebank tokenization with unmatched accuracy but also maintains exact stand off pointer to the original text and allows flexible configuration to diverse use case e g to genre or domain specific idiosyncrasy 
traditional approach to relation extraction from text require manually defining the relation to be extracted we propose here an approach to automatically discovering relevant relation given a large text corpus plus an initial ontology defining hundred of noun category e g athlete musician instrument our approach discovers frequently stated relation between pair of these category using a two step process for each pair of category e g musician and instrument it first co cluster the text context that connect known instance of the two category generating a candidate relation for each resulting cluster it then applies a trained classifier to determine which of these candidate relation is semantically valid our experiment apply this to a text corpus containing approximately million web page and an ontology containing category from the nell system carlson et al b producing a set of proposed candidate relation approximately half of which are semantically valid we conclude this is a useful approach to semi automatic extension of the ontology for large scale information extraction system such a nell 
this paper present a domain assisted approach to organize various aspect of a product into a hierarchy by integrating domain knowledge e g the product specification a well a consumer review based on the derived hierarchy we generate a hierarchical organization of consumer review on various product aspect and aggregate consumer opinion on these aspect with such organization user can easily grasp the overview of consumer review furthermore we apply the hierarchy to the task of implicit aspect identification which aim to infer implicit aspect of the review that do not explicitly express those aspect but actually comment on them the experimental result on popular product in four domain demonstrate the effectiveness of our approach 
resolving polysemy and synonymy is required for high quality information extraction we present conceptresolver a component for the never ending language learner nell carlson et al that handle both phenomenon by identifying the latent concept that noun phrase refer to conceptresolver performs both word sense induction and synonym resolution on relation extracted from text using an ontology and a small amount of labeled data domain knowledge the ontology guide concept creation by defining a set of possible semantic type for concept word sense induction is performed by inferring a set of semantic type for each noun phrase synonym detection exploit redundant information to train several domain specific synonym classifier in a semi supervised fashion when conceptresolver is run on nell s knowledge base of the word sens it creates correspond to real world concept and of noun phrase that it suggests refer to the same concept are indeed synonym 
nested event structure are a common occurrence in both open domain and domain specific extraction task e g a crime event can cause a investigation event which can lead to an arrest event however most current approach address event extraction with highly local model that extract each event and argument independently we propose a simple approach for the extraction of such structure by taking the tree of event argument relation and using it directly a the representation in a reranking dependency parser this provides a simple framework that capture global property of both nested and flat event structure we explore a rich feature space that model both the event to be parsed and context from the original supporting text our approach obtains competitive result in the extraction of biomedical event from the bionlp shared task with a f score of in development and in testing 
in citation based summarization text written by several researcher is leveraged to identify the important aspect of a target paper previous work on this problem focused almost exclusively on it extraction aspect i e selecting a representative set of citation sentence that highlight the contribution of the target paper meanwhile the fluency of the produced summary ha been mostly ignored for example diversity readability cohesion and ordering of the sentence included in the summary have not been thoroughly considered this resulted in noisy and confusing summary in this work we present an approach for producing readable and cohesive citation based summary our experiment show that the proposed approach outperforms several baseline in term of both extraction quality and fluency 
lightweight semantic annotation of text call for a simple representation ideally without requiring a semantic lexicon to achieve good coverage in the language and domain in this paper we repurpose wordnet s supersense tag for annotation developing specific guideline for nominal expression and applying them to arabic wikipedia article in four topical domain the resulting corpus ha high coverage and wa completed quickly with reasonable inter annotator agreement 
this paper study the problem of sentence level semantic coherence by answering sat style sentence completion question these question test the ability of algorithm to distinguish sense from nonsense based on a variety of sentence level phenomenon we tackle the problem with two approach method that use local lexical information such a the n gram of a classical language model and method that evaluate global coherence such a latent semantic analysis we evaluate these method on a suite of practice sat question and on a recently released sentence completion task based on data taken from five conan doyle novel we find that by fusing local and global information we can exceed on this task chance baseline is and we suggest some avenue for further research 
current approach for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain this supervision bottleneck is one of the major difficulty in scaling up semantic parsing we argue that a semantic parser can be trained effectively without annotated data and introduce an unsupervised learning algorithm the algorithm take a self training approach driven by confidence estimation evaluated over geoquery a standard dataset for this task our system achieved accuracy compared to of it fully supervised counterpart demonstrating the promise of unsupervised approach for this task 
we present a novel approach for building verb subcategorization lexicon using a simple graphical model in contrast to previous method we show how the model can be trained without parsed input or a predefined subcategorization frame inventory our method outperforms the state of the art on a verb clustering task and is easily trained on arbitrary domain this quantitative evaluation is complemented by a qualitative discussion of verb and their frame we discus the advantage of graphical model for this task in particular the ease of integrating semantic information about verb and argument in a principled fashion we conclude with future work to augment the approach 
we argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representation specifically we consider unsupervised induction of semantic role from sentence annotated with automatically predicted syntactic dependency representation and use a state of the art generative bayesian non parametric model at inference time instead of only seeking the model which explains the monolingual data available for each language we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentence we propose a simple approximate learning algorithm for our set up which result in efficient inference when applied to german english parallel data our method obtains a substantial improvement over a model trained without using the agreement signal when both are tested on non parallel sentence 
we describe an annotation tool developed to assist in the creation of multimodal action communication corpus from on line massively multi player game or mmgs mmgs typically involve group of player who control their avatar perform various activity questing competing fighting etc and communicate via chat or speech using assumed screen name we collected a corpus of group quest in second life that jointly involved player who generated over message in quasi synchronous chat during approximately hour of recorded action multiple level of coordinated annotation of this corpus dialogue movement touch gaze wear etc are required in order to support development of automated predictor of selected real life social and demographic characteristic of the player the annotation tool presented in this paper wa developed to enable efficient and accurate annotation of all dimension simultaneously 
while world knowledge ha been shown to improve learning based coreference resolvers the improvement were typically obtained by incorporating world knowledge into a fairly weak baseline resolver hence it is not clear whether these benefit can carry over to a stronger baseline moreover since there ha been no attempt to apply different source of world knowledge in combination to coreference resolution it is not clear whether they offer complementary benefit to a resolver we systematically compare commonly used and under investigated source of world knowledge for coreference resolution by applying them to two learning based coreference model and evaluating them on document annotated with two different annotation scheme 
to address semantic ambiguity in coreference resolution we use web n gram feature that capture a range of world knowledge in a diffuse but robust way specifically we exploit short distance cue to hypernymy semantic compatibility and semantic context a well a general lexical co occurrence when added to a state of the art coreference baseline our web feature give significant gain on multiple datasets ace and ace and metric muc and b resulting in the best result reported to date for the end to end task of coreference resolution 
online discussion forum are a valuable mean for user to resolve specific information need both interactively for the participant and statically for user who search browse over historical thread data however the complex structure of forum thread can make it difficult for user to extract relevant information the discourse structure of web forum thread in the form of labelled dependency relationship between post ha the potential to greatly improve information access over web forum archive in this paper we present the task of parsing user forum thread to determine the labelled dependency between post three method including a dependency parsing approach are proposed to jointly classify the link relationship between post and the dialogue act type of each link the proposed method significantly surpass an informed baseline we also experiment with in situ classification of evolving thread and establish that our best method are able to perform equivalently well over partial thread a complete thread 
this paper present a novel top down head driven parsing algorithm for data driven projective dependency analysis this algorithm handle global structure such a clause and coordination better than shift reduce or other bottom up algorithm experiment on the english penn treebank data and the chinese conll data show that the proposed algorithm achieves comparable result with other data driven dependency parsing algorithm 
when translating among language that differ substantially in word order machine translation mt system benefit from syntactic pre ordering an approach that us feature from a syntactic parse to permute source word into a target language like order this paper present a method for inducing parse tree automatically from a parallel corpus instead of using a supervised parser trained on a tree bank these induced par are used to pre order source sentence we demonstrate that our induced parser is effective it not only improves a state of the art phrase based system with integrated reordering but also approach the performance of a recent pre ordering method based on a supervised parser these result show that the syntactic structure which is relevant to mt pre ordering can be learned automatically from parallel text thus establishing a new application for unsupervised grammar induction 
we present a data driven approach to generating response to twitter status post based on phrase based statistical machine translation we find that mapping conversational stimulus onto response is more difficult than translating between language due to the wider range of possible response the larger fraction of unaligned word phrase and the presence of large phrase pair whose alignment cannot be further decomposed after addressing these challenge we compare approach based on smt and information retrieval in a human evaluation we show that smt outperforms ir on this task and it output is preferred over actual human response in of case a far a we are aware this is the first work to investigate the use of phrase based smt to directly translate a linguistic stimulus into an appropriate response 
information oriented document labeling is a special document multi labeling task where the target label refer to a specific information instead of the topic of the whole document these kind of task are usually solved by looking up indicator phrase and analyzing their local context to filter false positive match here we introduce an approach for machine learning local content shifter which detects irrelevant local context using just the original document level training label we handle content shifter in general instead of learning a particular language phenomenon detector e g negation or hedging and form a single system for document labeling and content shift detection our empirical result achieved error reduction compared to supervised baseline method on three document labeling task 
the challenge of named entity recognition ner for tweet lie in the insufficient information in a tweet and the unavailability of training data we propose to combine a k nearest neighbor knn classifier with a linear conditional random field crf model under a semi supervised learning framework to tackle these challenge the knn based classifier conduct pre labeling to collect global coarse evidence across tweet while the crf model conduct sequential labeling to capture fine grained information encoded in a tweet the semi supervised learning plus the gazetteer alleviate the lack of training data extensive experiment show the advantage of our method over the baseline a well a the effectiveness of knn and semi supervised learning 
this paper proposes the use of local histogram lh over character n gram for authorship attribution aa lh are enriched histogram representation that preserve sequential information in document they have been successfully used for text categorization and document visualization using word histogram in this work we explore the suitability of lh over n gram at the character level for aa we show that lh are particularly helpful for aa because they provide useful information for uncovering to some extent the writing style of author we report experimental result in aa data set that confirm that lh over character n gram are more helpful for aa than the usual global histogram yielding result far superior to state of the art approach we found that lh are even more advantageous in challenging condition such a having imbalanced and small training set our result motivate further research on the use of lh for modeling the writing style of author for related task such a authorship verification and plagiarism detection 
this paper extends the training and tuning regime for phrase based statistical machine translation to obtain fluent translation into morphologically complex language we build an english to finnish translation system our method use unsupervised morphology induction unlike previous work we focus on morphologically productive phrase pair our decoder can combine morpheme across phrase boundary morpheme in the target language may not have a corresponding morpheme or word in the source language therefore we propose a novel combination of post processing morphology prediction with morpheme based translation we show using both automatic evaluation score and linguistically motivated analysis of the output that our method outperform previously proposed one and provide the best known result on the english finnish europarl translation task our method are mostly language independent so they should improve translation into other target language with complex morphology 
this paper proposes a novel reordering model for statistical machine translation smt by mean of modeling the translation order of the source language collocation the model is learned from a word aligned bilingual corpus where the collocated word in source sentence are automatically detected during decoding the model is employed to softly constrain the translation order of the source language collocation so a to constrain the translation order of those source phrase containing these collocated word the experimental result show that the proposed method significantly improves the translation quality achieving the absolute improvement of bleu score over the baseline method 
event extraction is the task of detecting certain specified type of event that are mentioned in the source language data the state of the art research on the task is transductive inference e g cross event inference in this paper we propose a new method of event extraction by well using cross entity inference in contrast to previous inference method we regard entity type consistency a key feature to predict event mention we adopt this inference method to improve the traditional sentence level event extraction system experiment show that we can get gain in trigger event identification and more than gain for argument role classification in ace event extraction 
variant of naive bayes nb and support vector machine svm are often used a baseline method for text classification but their performance varies greatly depending on the model variant feature used and task dataset we show that i the inclusion of word bigram feature give consistent gain on sentiment analysis task ii for short snippet sentiment task nb actually doe better than svms while for longer document the opposite result hold iii a simple but novel svm variant using nb log count ratio a feature value consistently performs well across task and datasets based on these observation we identify simple nb and svm variant which outperform most published result on sentiment analysis datasets sometimes providing a new state of the art performance level 
we propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of eisner s generative model in our framework we define two kind of generative model for reranking one is learned from training data offline and the other from a forest generated by a baseline parser on the fly the final prediction in the reranking stage is performed using linear interpolation of these model and discriminative model in order to efficiently train the model from and decode on a hypergraph data structure representing a forest we apply extended inside outside and viterbi algorithm experimental result show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approach 
we present an accurate word alignment algorithm that heavily exploit source and target language syntax using a discriminative framework and an efficient bottom up search algorithm we train a model of hundred of thousand of syntactic feature our new model help u to very accurately model syntactic transformation between language is language independent and with automatic feature extraction assist system developer in obtaining good word alignment performance off the shelf when tackling new language pair we analyze the impact of our feature describe inference under the model and demonstrate significant alignment and translation quality improvement over already powerful baseline trained on very large corpus we observe translation quality improvement corresponding to and bleu for arabic english and chinese english respectively 
this paper describes a novel approach to the semantic relation detection problem instead of relying only on the training instance for a new relation we leverage the knowledge learned from previously trained relation detector specifically we detect a new semantic relation by projecting the new relation s training instance onto a lower dimension topic space constructed from existing relation detector through a three step process first we construct a large relation repository of more than relation from wikipedia second we construct a set of non redundant relation topic defined at multiple scale from the relation repository to characterize the existing relation similar to the topic defined over word each relation topic is an interpretable multinomial distribution over the existing relation third we integrate the relation topic in a kernel function and use it together with svm to construct detector for new relation the experimental result on wikipedia and ace data have confirmed that background knowledge based topic generated from the wikipedia relation repository can significantly improve the performance over the state of the art relation detection approach 
we evaluate several popular model of local discourse coherence for domain and task generality by applying them to chat disentanglement using experiment on synthetic multiparty conversation we show that most model transfer well from text to dialogue coherence model improve result overall when good par and topic model are available and on a constrained task for real chat data 
we present an inference algorithm that organizes observed word token into structured inflectional paradigm type it also naturally predicts the spelling of unobserved form that are missing from these paradigm and discovers inflectional principle grammar that generalize to wholly unobserved word our bayesian generative model of the data explicitly represents token type inflection paradigm and locally conditioned string edits it assumes that inflected word token are generated from an infinite mixture of inflectional paradigm string tuples each paradigm is sampled all at once from a graphical model whose potential function are weighted finite state transducer with language specific parameter to be learned these assumption naturally lead to an elegant empirical bayes inference procedure that exploit monte carlo em belief propagation and dynamic programming given seed paradigm adding a million word corpus reduces prediction error for morphological inflection by up to 
we present an unsupervised model for joint phrase alignment and extraction using non parametric bayesian method and inversion transduction grammar itgs the key contribution is that phrase of many granularity are included directly in the model through the use of a novel formulation that memorizes phrase generated not only by terminal but also non terminal symbol this allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase based machine translation task directly from unaligned sentence pair experiment on several language pair demonstrate that the proposed model match the accuracy of traditional two step word alignment phrase extraction approach while reducing the phrase table to a fraction of the original size 
we address the issue of consuming heterogeneous annotation data for chinese word segmentation and part of speech tagging we empirically analyze the diversity between two representative corpus i e penn chinese treebank ctb and pku s people s daily ppd on manually mapped data and show that their linguistic annotation are systematically different and highly compatible the analysis is further exploited to improve processing accuracy by integrating system that are respectively trained on heterogeneous annotation to reduce the approximation error and re training model with high quality automatically converted data to reduce the estimation error evaluation on the ctb and ppd data show that our novel model achieves a relative error reduction of over the best reported result in the literature 
people tweet more than million time daily yielding a noisy informal but sometimes informative corpus of character message that mirror the zeitgeist in an unprecedented manner the performance of standard nlp tool is severely degraded on tweet this paper address this issue by re building the nlp pipeline beginning with part of speech tagging through chunking to named entity recognition our novel t ner system double f score compared with the stanford ner system t ner leverage the redundancy inherent in tweet to achieve this performance using labeledlda to exploit freebase dictionary a a source of distant supervision labeledlda outperforms co training increasing f by over ten common entity type our nlp tool are available at http github com aritter twitter nlp 
we propose a novel approach to translating from a morphologically complex language unlike previous research which ha targeted word inflection and concatenation we focus on the pairwise relationship between morphologically related word which we treat a potential paraphrase and handle using paraphrasing technique at the word phrase and sentence level an important advantage of this framework is that it can cope with derivational morphology which ha so far remained largely beyond the capability of statistical machine translation system our experiment translating from malay whose morphology is mostly derivational into english show significant improvement over rivaling approach based on five automatic evaluation measure for sentence pair million english word token 
the availability of learner corpus especially those which have been manually error tagged or shallow parsed is still limited this mean that researcher do not have a common development and test set for natural language processing of learner english such a for grammatical error detection given this background we created a novel learner corpus that wa manually error tagged and shallow parsed this corpus is available for research and educational purpose on the web in this paper we describe it in detail together with it data collection method and annotation scheme another contribution of this paper is that we take the first step toward evaluating the performance of existing po tagging chunking technique on learner corpus using the created corpus these contribution will facilitate further research in related area such a grammatical error detection and automated essay scoring 
metalanguage is an essential linguistic mechanism which allows u to communicate explicit information about language itself however it ha been underexamined in research in language technology to the detriment of the performance of system that could exploit it this paper describes the creation of the first tagged and delineated corpus of english metalanguage accompanied by an explicit definition and a rubric for identifying the phenomenon in text this resource will provide a basis for further study of metalanguage and enable it utilization in language technology 
in this paper we present a novel approach to entity linking based on a statistical language model based information retrieval with query expansion we use both local context and global world knowledge to expand query language model we place a strong emphasis on named entity in the local context and explore a positional language model to weigh them differently based on their distance to the query our experiment on the tac kbp data show that incorporating such contextual information indeed aid in disambiguating the named entity and consistently improves the entity linking performance compared with the official result from kbp participant our system show competitive performance 
dialogue act classification is a central challenge for dialogue system although the importance of emotion in human dialogue is widely recognized most dialogue act classification model make limited or no use of affective channel in dialogue act classification this paper present a novel affect enriched dialogue act classifier for task oriented dialogue that model facial expression of user in particular facial expression related to confusion the finding indicate that the affect enriched classifier perform significantly better for distinguishing user request for feedback and grounding dialogue act within textual dialogue the result point to way in which dialogue system can effectively leverage affective channel to improve dialogue act classification 
dual decomposition ha been recently proposed a a way of combining complementary model with a boost in predictive power however in case where lightweight decomposition are not readily available e g due to the presence of rich feature or logical constraint the original subgradient algorithm is inefficient we sidestep that difficulty by adopting an augmented lagrangian method that accelerates model consensus by regularizing towards the averaged vote we show how first order logical constraint can be handled efficiently even though the corresponding subproblems are no longer combinatorial and report experiment in dependency parsing with state of the art result 
in this paper we propose innovative representation for automatic classification of verb according to mainstream linguistic theory namely verbnet and framenet first syntactic and semantic structure capturing essential lexical and syntactic property of verb are defined then we design advanced similarity function between such structure i e semantic tree kernel function for exploiting distributional and grammatical information in support vector machine the extensive empirical analysis on verbnet class and frame detection show that our model capture meaningful syntactic semantic structure which allows for improving the state of the art 
in order to obtain a fine grained evaluation of parser accuracy over naturally occurring text we study example each of ten reasonably frequent linguistic phenomenon randomly selected from a parsed version of the english wikipedia we construct a corresponding set of gold standard target dependency for these sentence operationalize mapping to these target from seven state of the art parser and evaluate the parser against this data to measure their level of success in identifying these dependency 
recent work ha shown how a parallel corpus can be leveraged to build syntactic parser for a target language by projecting automatic source parse onto the target sentence using word alignment the projected target dependency par are not always fully connected to be useful for training traditional dependency parser in this paper we present a greedy non directional parsing algorithm which doesn t need a fully connected parse and can learn from partial par by utilizing available structural and syntactic information in them our parser achieved statistically significant improvement over a baseline system that train on only fully connected par for bulgarian spanish and hindi it also gave a significant improvement over previously reported result for bulgarian and set a benchmark for hindi 
long distance word reordering is a major challenge in statistical machine translation research previous work ha shown using source syntactic tree is an effective way to tackle this problem between two language with substantial word order difference in this work we further extend this line of exploration and propose a novel but simple approach which utilizes a ranking model based on word order precedence in the target language to reposition node in the syntactic parse tree of a source sentence the ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical feature we evaluated our approach on large scale japanese english and english japanese machine translation task and show that it can significantly outperform the baseline phrase based smt system 
traditional computational approach to referring expression generation operate in a deliberate manner choosing the attribute to be included on the basis of their ability to distinguish the intended referent from it distractors however work in psycholinguistics suggests that speaker align their referring expression with those used previously in the discourse implying le deliberate choice and more subconscious reuse this raise the question a to which is a more accurate characterisation of what people do using a corpus of dialogue containing referring expression we explore this question via the generation of subsequent reference in shared visual scene we use a machine learning approach to referring expression generation and demonstrate that incorporating feature that correspond to the computational tradition doe not match human referring behaviour a well a using feature corresponding to the process of alignment the result support the view that the traditional model of referring expression generation that is widely assumed in work on natural language generation may not in fact be correct our analysis may also help explain the oft observed redundancy found in human produced referring expression 
when automatically translating from a weakly inflected source language like english to a target language with richer grammatical feature such a gender and dual number the output commonly contains morpho syntactic agreement error to address this issue we present a target side class based agreement model agreement is promoted by scoring a sequence of fine grained morpho syntactic class that are predicted during decoding for each translation hypothesis for english to arabic translation our model yield a bleu average improvement over a state of the art baseline the model doe not require bitext or phrase table annotation and can be easily implemented a a feature in many phrase based decoder 
some statistical machine translation system never see the light because the owner of the appropriate training data cannot release them and the potential user of the system cannot disclose what should be translated we propose a simple and practical encryption based method addressing this barrier 
we propose a relaxed correspondence assumption for cross lingual projection of constituent syntax which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse such a relaxed assumption fundamentally tolerates the syntactic non isomorphism between language and enables u to learn the target language specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax based on this assumption a novel constituency projection method is also proposed in order to induce a projected constituent tree bank from the source parsed bilingual corpus experiment show that the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parser 
sequential modeling ha been widely used in a variety of important application including named entity recognition and shallow parsing however a more and more real time large scale tagging application arise decoding speed ha become a bottleneck for existing sequential tagging algorithm in this paper we propose best a best iterative a k best a and k best iterative viterbi a algorithm for sequential decoding we show the efficiency of these proposed algorithm for five nlp tagging task in particular we show that iterative viterbi a decoding can be several time or order of magnitude faster than the state of the art algorithm for tagging task with a large number of label this algorithm make real time large scale tagging application with thousand of label feasible 
this paper describes an algorithm for exact decoding of phrase based translation model based on lagrangian relaxation the method recovers exact solution with certificate of optimality on over of test example the method is much more efficient than approach based on linear programming lp or integer linear programming ilp solver these method are not feasible for anything other than short sentence we compare our method to moses koehn et al and give precise estimate of the number and magnitude of search error that moses make 
we present minimum bayes risk system combination a method that integrates consensus decoding and system combination into a unified multi system minimum bayes risk mbr technique unlike other mbr method that re rank translation of a single smt system mbr system combination us the mbr decision rule and a linear combination of the component system probability distribution to search for the minimum risk translation among all the finite length string over the output vocabulary we introduce expected bleu an approximation to the bleu score that allows to efficiently apply mbr in these condition mbr system combination is a general method that is independent of specific smt model enabling u to combine system with heterogeneous structure experiment show that our approach bring significant improvement to single system based mbr decoding and achieves comparable result to different state of the art system combination method 
responding to the need for semantic lexical resource in natural language processing application we examine method to acquire noun compound nc e g orange juice together with suitable fine grained semantic interpretation e g squeezed from which are directly usable a paraphrase we employ bootstrapping and web statistic and utilize the relationship between nc and paraphrasing pattern to jointly extract nc and such pattern in multiple alternating iteration in evaluation we found that having one compound noun fixed yield both a higher number of semantically interpreted nc and improved accuracy due to stronger semantic restriction 
we present the first algorithm to automatically identify explicit discourse connective and the relation they signal for arabic text first we show that for arabic news most adjacent sentence are connected via explicit connective in contrast to english making the treatment of explicit discourse connective for arabic highly important we also show that explicit arabic discourse connective are far more ambiguous than english one making their treatment challenging in the second part of the paper we present supervised algorithm to address automatic discourse connective identification and discourse relation recognition our connective identifier based on gold standard syntactic feature achieves almost human performance in addition an identifier based solely on simple lexical and automatically derived morphological and po feature performs with high reliability essential for language that do not have high quality parser yet our algorithm for recognizing discourse relation performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation 
we propose a new approach to characterizing the timeline of a text temporal dependency structure where all the event of a narrative are linked via partial ordering relation like before after overlap and identity we annotate a corpus of child s story with temporal dependency tree achieving agreement krippendorff s alpha of on the event word on the link between event and of on the ordering relation label we compare two parsing model for temporal dependency structure and show that a deterministic non projective dependency parser outperforms a graph based maximum spanning tree parser achieving labeled attachment accuracy of and labeled tree edit distance of our analysis of the dependency parser error give some insight into future research direction 
online forum are becoming a popular resource in the state of the art question answering qa system because of it nature a an online community it contains more updated knowledge than other place however going through tedious and redundant post to look for answer could be very time consuming most prior work focused on extracting only question answering sentence from user conversation in this paper we introduce the task of sentence dependency tagging finding dependency structure can not only help find answer quickly but also allow user to trace back how the answer is concluded through user conversation we use linear chain conditional random field crf for sentence type tagging and a d crf to label the dependency relation between sentence our experimental result show that our proposed approach performs well for sentence dependency tagging this dependency information can benefit other task such a thread ranking and answer summarization in online forum 
a lack of standard datasets and evaluation metric ha prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last year we address both problem by presenting a novel data collection framework that produce highly parallel text data relatively inexpensively and on a large scale the highly parallel nature of this data allows u to use simple n gram comparison to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidate in addition to being simple and efficient to compute experiment show that these metric correlate highly with human judgment 
this paper explores approach to sentiment classification of u s congressional floor debate transcript collective classification technique are used to take advantage of the informal citation structure present in the debate we use a range of method based on local and global formulation and introduce novel approach for incorporating the output of machine learner into collective classification algorithm our experimental evaluation show that the mean field algorithm obtains the best result for the task significantly outperforming the benchmark technique 
we present a novel approach to discovering relation and their instantiation from a collection of document in a single domain our approach learns relation type by exploiting meta constraint that characterize the general quality of a good relation in any domain these constraint state that instance of a single relation should exhibit regularity at multiple level of linguistic structure including lexicography syntax and document level context we capture these regularity via the structure of our probabilistic model a well a a set of declaratively specified constraint enforced during posterior inference across two domain our approach successfully recovers hidden relation structure comparable to or outperforming previous state of the art approach furthermore we find that a small set of constraint is applicable across the domain and that using domain specific constraint can further improve performance 
we present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in goldwater by adding rejuvenation to a particle filter we are able to considerably improve it performance both in term of finding higher probability and higher accuracy solution 
we propose a non parametric bayesian model for unsupervised semantic parsing following poon and domingo we consider a semantic parsing setting where the goal is to decompose the syntactic dependency tree of a sentence into fragment assign each of these fragment to a cluster of semantically equivalent syntactic structure and predict predicate argument relation between the fragment we use hierarchical pitman yor process to model statistical dependency between meaning representation of predicate and those of their argument a well a the cluster of their syntactic realization we develop a modification of the metropolis hastings split merge sampler resulting in an efficient inference algorithm for the model the method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain 
decision tree have been applied to a variety of nlp task including language modeling for their ability to handle a variety of attribute and sparse context space moreover forest collection of decision tree have been shown to substantially outperform individual decision tree in this work we investigate method for combining tree in a forest a well a method for diversifying tree for the task of syntactic language modeling we show that our tree interpolation technique outperforms the standard method used in the literature and that on this particular task restricting tree context in a principled way produce smaller and better forest with the best achieving an relative reduction in word error rate over an n gram baseline 
latent variable model have the potential to add value to large document collection by discovering interpretable low dimensional subspace in order for people to use such model however they must trust them unfortunately typical dimensionality reduction method for text such a latent dirichlet allocation often produce low dimensional subspace topic that are obviously flawed to human domain expert the contribution of this paper are threefold an analysis of the way in which topic can be flawed an automated evaluation metric for identifying such topic that doe not rely on human annotator or reference collection outside the training data a novel statistical topic model based on this metric that significantly improves topic quality in a large scale document collection from the national institute of health nih 
we present an ilp based model of zero anaphora detection and resolution that build on the joint determination of anaphoricity and coreference model proposed by denis and baldridge but revise it and extends it into a three way ilp problem also incorporating subject detection we show that this new model outperforms several baseline and competing model a well a a direct translation of the denis baldridge model for both italian and japanese zero anaphora we incorporate our model in complete anaphoric resolvers for both italian and japanese showing that our approach lead to improved performance also when not used in isolation provided that separate classifier are used for zero and for explicitly realized anaphor 
this paper develops a minimally supervised approach based on focused distributional similarity method and discourse connective for identifying of causality relation between event in context while it ha been shown that distributional similarity can help identifying causality we observe that discourse connective and the particular discourse relation they evoke in context provide additional information towards determining causality between event we show that combining discourse relation prediction and distributional similarity method in a global inference procedure provides additional improvement towards determining event causality 
dependency parser are critical component within many nlp system however currently available dependency parser each exhibit at least one of several weakness including high running time limited accuracy vague dependency label and lack of non projectivity support furthermore no commonly used parser provides additional shallow semantic interpretation such a preposition sense disambiguation and noun compound interpretation in this paper we present a new dependency tree conversion of the penn treebank along with it associated fine grain dependency label and a fast accurate parser trained on it we explain how a non projective extension to shift reduce parsing can be incorporated into non directional easy first parsing the parser performs well when evaluated on the standard test section of the penn treebank outperforming several popular open source dependency parser it is to the best of our knowledge the first dependency parser capable of parsing more than sentence per second at over accuracy 
metaphor is ubiquitous in text even in highly technical text correct inference about textual entailment requires computer to distinguish the literal and metaphorical sens of a word past work ha treated this problem a a classical word sense disambiguation task in this paper we take a new approach based on research in cognitive linguistics that view metaphor a a method for transferring knowledge from a familiar well understood or concrete domain to an unfamiliar le understood or more abstract domain this view lead to the hypothesis that metaphorical word usage is correlated with the degree of abstractness of the word s context we introduce an algorithm that us this hypothesis to classify a word sense in a given context a either literal denotative or metaphorical connotative we evaluate this algorithm with a set of adjective noun phrase e g in dark comedy the adjective dark is used metaphorically in dark hair it is used literally and with the trofi trope finder example base of literal and nonliteral usage for fifty verb we achieve state of the art performance on both datasets 
in this paper we observe that there exists a second dimension to the relation extraction re problem that is orthogonal to the relation type dimension we show that most of these second dimensional structure are relatively constrained and not difficult to identify we propose a novel algorithmic approach to re that start by first identifying these structure and then within these identifying the semantic type of the relation in the real re problem where relation argument need to be identified exploiting these structure also allows reducing pipelined propagated error we show that this re framework provides significant improvement in re performance 
long distance reordering remains one of the biggest challenge facing machine translation we derive soft constraint from the source dependency parsing to directly address the reordering problem for the hierarchical phrase based model our approach significantly improves chinese english machine translation on a large scale task by bleu point on average moreover when we switch the tuning function from bleu to the lrscore which promotes reordering we observe total improvement of bleu lrscore and ter over the baseline on average our approach improves reordering precision and recall by and absolute point respectively and is found to be especially effective for long distance reodering 
efficient decoding for syntactic parsing ha become a necessary research area a statistical grammar grow in accuracy and size and a more nlp application leverage syntactic analysis we review prior method for pruning and then present a new framework that unifies their strength into a single approach using a log linear model we learn the optimal beam search pruning parameter for each cyk chart cell effectively predicting the most promising area of the model space to explore we demonstrate that our method is faster than coarse to fine pruning exemplified in both the charniak and berkeley parser by empirically comparing our parser to the berkeley parser using the same grammar and under identical operating condition 
mining retrospective event from text stream ha been an important research topic classic text representation model i e vector space model cannot model temporal aspect of document to address it we proposed a novel burst based text representation model denoted a burstvsm burstvsm corresponds dimension to bursty feature instead of term which can capture semantic and temporal information meanwhile it significantly reduces the number of non zero entry in the representation we test it via scalable event detection and experiment in a year news archive show that our method are both effective and efficient 
this paper present a novel method for the computation of word meaning in context we make use of a factorization model in which word together with their window based context word and their dependency relation are linked to latent dimension the factorization model allows u to determine which dimension are important for a particular context and adapt the dependency based feature vector of the word accordingly the evaluation on a lexical substitution task carried out for both english and french indicates that our approach is able to reach better result than state of the art method in lexical substitution while at the same time providing more accurate meaning representation 
we propose to directly measure the importance of query in the source domain to the target domain where no rank label of document are available which is referred to a query weighting query weighting is a key step in ranking model adaptation a the learning object of ranking algorithm is divided by query instance we argue that it s more reasonable to conduct importance weighting at query level than document level we present two query weighting scheme the first compress the query into a query feature vector which aggregate all document instance in the same query and then conduct query weighting based on the query feature vector this method can efficiently estimate query importance by compressing query data but the potential risk is information loss resulted from the compression the second measure the similarity between the source query and each target query and then combine these fine grained similarity value for it importance estimation adaptation experiment on letor data set demonstrate that query weighting significantly outperforms document instance weighting method 
we develop a novel approach to the semantic analysis of short text segment and demonstrate it utility on a large corpus of web search query extracting meaning from short text segment is difficult a there is little semantic redundancy between term hence method based on shallow semantic analysis may fail to accurately estimate meaning furthermore search query lack explicit syntax often used to determine intent in question answering in this paper we propose a hybrid model of semantic analysis combining explicit class label extraction with a latent class pcfg this class label correlation clc model admits a robust parallel approximation allowing it to scale to large amount of query data we demonstrate it performance in term of it predicted label accuracy on polysemous query and it ability to accurately chunk query into base constituent 
this paper proposes a semi supervised relation acquisition method that doe not rely on extraction pattern e g x cause y for causal relation but instead learns a combination of indirect evidence for the target relation semantic word class and partial pattern this method can extract long tail instance of semantic relation like causality from rare and complex expression in a large japanese web corpus in extreme case pattern that occur only once in the entire corpus such pattern are beyond the reach of current pattern based method we show that our method performs on par with state of the art pattern based method and maintains a reasonable level of accuracy even for instance acquired from infrequent pattern this ability to acquire long tail instance is crucial for risk management and innovation where an exhaustive database of high level semantic relation like causation is of vital importance 
we propose a simple generative syntactic language model that condition on overlapping window of tree context or treelet in the same way that n gram language model condition on overlapping window of linear context we estimate the parameter of our model by collecting count from automatically parsed text using standard n gram language model estimation technique allowing u to train a model on over one billion token of data using a single machine in a matter of hour we evaluate on perplexity and a range of grammaticality task and find that we perform a well or better than n gram model and other generative baseline our model even competes with state of the art discriminative model hand designed for the grammaticality task despite training on positive data alone we also show fluency improvement in a preliminary machine translation experiment 
marking up search query with linguistic annotation such a part of speech tag capitalization and segmentation is an important part of query processing and understanding in information retrieval system due to their brevity and idiosyncratic structure search query pose a challenge to existing nlp tool to address this challenge we propose a probabilistic approach for performing joint query annotation first we derive a robust set of unsupervised independent annotation using query and pseudo relevance feedback then we stack additional classifier on the independent annotation and exploit the dependency between them to further improve the accuracy even with a very limited amount of available training data we evaluate our method using a range of query extracted from a web search log experimental result verify the effectiveness of our approach for both short keyword query and verbose natural language query 
the integration of multiword expression in a parsing procedure ha been shown to improve accuracy in an artificial context where such expression have been perfectly pre identified this paper evaluates two empirical strategy to integrate multiword unit in a real constituency parsing context and show that the result are not a promising a ha sometimes been suggested firstly we show that pre grouping multiword expression before parsing with a state of the art recognizer improves multiword recognition accuracy and unlabeled attachment score however it ha no statistically significant impact in term of f score a incorrect multiword expression recognition ha important side effect on parsing secondly integrating multiword expression in the parser grammar followed by a reranker specific to such expression slightly improves all evaluation metric 
we propose the first joint model for word segmentation po tagging and dependency parsing for chinese based on an extension of the incremental joint model for po tagging and dependency parsing hatori et al we propose an efficient character based decoding method that can combine feature from state of the art segmentation po tagging and dependency parsing model we also describe our method to align comparable state in the beam and how we can combine feature of different characteristic in our incremental framework in experiment using the chinese treebank ctb we show that the accuracy of the three task can be improved significantly over the baseline model particularly by for po tagging and for dependency parsing we also perform comparison experiment with the partially joint model 
we present a novel approach to the task of word lemmatisation we formalise lemmatisation a a category tagging task by describing how a word to lemma transformation rule can be encoded in a single label and how a set of such label can be inferred for a specific language in this way a lemmatisation system can be trained and tested using any supervised tagging model in contrast to previous approach the proposed technique allows u to easily integrate relevant contextual information we test our approach on eight language reaching a new state of the art level for the lemmatisation task 
blog and forum are widely adopted by online community to debate about various issue however a user that want to cut in on a debate may experience some difficulty in extracting the current accepted position and can be discouraged from interacting through these application in our paper we combine textual entailment with argumentation theory to automatically extract the argument from debate and to evaluate their acceptability 
multiword expression mwe a known nuisance for both linguistics and nlp blur the line between syntax and semantics previous work on mwe identification ha relied primarily on surface statistic which perform poorly for longer mwes and cannot model discontinuous expression to address these problem we show that even the simplest parsing model can effectively identify mwes of arbitrary length and that tree substitution grammar achieve the best result our experiment show a f absolute improvement for french over an n gram surface statistic baseline currently the predominant method for mwe identification our model are useful for several nlp task in which mwe pre grouping ha improved accuracy 
we propose a latent variable model to enhance historical analysis of large corpus this work extends prior work in topic modelling by incorporating metadata and the interaction between the component in metadata in a general way to test this we collect a corpus of slavery related united state property law judgement sampled from the year to we study the language use in these legal case with a special focus on shift in opinion on controversial topic across different region because this is a longitudinal data set we are also interested in understanding how these opinion change over the course of decade we show that the joint learning scheme of our sparse mixed effect model improves on other state of the art generative and discriminative model on the region and time period identification task experiment show that our sparse mixed effect model is more accurate quantitatively and qualitatively interesting and that these improvement are robust across different parameter setting 
a key factor of high quality word segmentation for japanese is a high coverage dictionary but it is costly to manually build such a lexical resource although external lexical resource for human reader are potentially good knowledge source they have not been utilized due to difference in segmentation criterion to supplement a morphological dictionary with these resource we propose a new task of japanese noun phrase segmentation we apply non parametric bayesian language model to segment each noun phrase in these resource according to the statistical behavior of it supposed constituent in text for inference we propose a novel block sampling procedure named hybrid type based sampling which ha the ability to directly escape a local optimum that is not too distant from the global optimum experiment show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer 
we present a novel approach to the automatic acquisition of a verbnet like classification of french verb which involves the use i of a neural clustering method which associate cluster with feature ii of several supervised and unsupervised evaluation metric and iii of various existing syntactic and semantic lexical resource we evaluate our approach on an established test set and show that it outperforms previous related work with an f measure of 
we present a novel machine translation model which model translation by a linear sequence of operation in contrast to the n gram model this sequence includes not only translation but also reordering operation key idea of our model are i a new reordering approach which better restricts the position to which a word or phrase can be moved and is able to handle short and long distance re ordering in a unified way and ii a joint sequence model for the translation and reordering probability which is more flexible than standard phrase based mt we observe statistically significant improvement in bleu over moses for german to english and spanish to english task and comparable result for a french to english task 
the dominant practice of statistical machine translation smt us the same chinese word segmentation specification in both alignment and translation rule induction step in building chinese english smt system which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation to tackle this we propose a framework that us two different segmentation specification for alignment and translation respectively we use chinese character a the basic unit for alignment and then convert this alignment to conventional word alignment for translation rule induction experimentally our approach outperformed two baseline fully word based system using word for both alignment and translation and fully character based system in term of alignment quality and translation performance 
we develop a general dynamic programming technique for the tabulation of transition based dependency parser and apply it to obtain novel polynomial time algorithm for parsing with the arc standard and arc eager model we also show how to reverse our technique to obtain new transition based dependency parser from existing tabular method additionally we provide a detailed discussion of the condition under which the feature model commonly used in transition based parsing can be integrated into our algorithm 
previous work using topic model for statistical machine translation smt explore topic information at the word level however smt ha been advanced from word based paradigm to phrase rule based paradigm we therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase based translation we associate each synchronous rule with a topic distribution and select desirable rule according to the similarity of their topic distribution with given document we show that our model significantly improves the translation performance over the baseline on nist chinese to english translation experiment our model also achieves a better performance and a faster speed than previous approach that work at the word level 
in this paper we show how to train statistical machine translation system on real life task using only non parallel monolingual data from two language we present a modification of the method shown in ravi and knight that is scalable to vocabulary size of several thousand word on the task shown in ravi and knight we obtain better result with only of the computational effort when running our method with an n gram language model the efficiency improvement of our method allows u to run experiment with vocabulary size of around word such a a non parallel version of the verbmobil corpus we also report result using data from the monolingual french and english gigaword corpus 
the goal of our research is to improve event extraction by learning to identify secondary role filler context in the absence of event keywords we propose a multi layered event extraction architecture that progressively zoom in on relevant information our extraction model includes a document genre classifier to recognize event narrative two type of sentence classifier and noun phrase classifier to extract role filler these module are organized a a pipeline to gradually zero in on event related information we present result on the muc event extraction data set and show that this model performs better than previous system 
we propose an architecture for expressing various linguistically motivated feature that help identify multi word expression in natural language text the architecture combine various linguistically motivated classification feature in a bayesian network we introduce novel way for computing many of these feature and manually define linguistically motivated interrelationship among them which the bayesian network model our methodology is almost entirely unsupervised and completely language independent it relies on few language resource and is thus suitable for a large number of language furthermore unlike much recent work our approach can identify expression of various type and syntactic construction we demonstrate a significant improvement in identification accuracy compared with le sophisticated baseline 
we present a named entity recognition ner system for extracting product attribute and value from listing title information extraction from short listing title present a unique challenge with the lack of informative context and grammatical structure in this work we combine supervised ner with bootstrapping to expand the seed list and output normalized result focusing on listing from ebay s clothing and shoe category our bootstrapped ner system is able to identify new brand corresponding to spelling variant and typographical error of the known brand a well a identifying novel brand among the top new brand predicted our system achieves precision to output normalized attribute value we explore several string comparison algorithm and found n gram substring matching to work well in practice 
we propose a novel model to automatically extract transliteration pair from parallel corpus our model is efficient language pair independent and mine transliteration pair in a consistent fashion in both unsupervised and semi supervised setting we model transliteration mining a an interpolation of transliteration and non transliteration sub model we evaluate on news shared task data and on parallel corpus with competitive result 
we introduce a novel machine learning framework based on recursive autoencoders for sentence level prediction of sentiment label distribution our method learns vector space representation for multi word phrase in sentiment prediction task these representation outperform other state of the art approach on commonly used datasets such a movie review without using any pre defined sentiment lexica or polarity shifting rule we also evaluate the model s ability to predict sentiment distribution on a new dataset based on confession from the experience project the dataset consists of personal user story annotated with multiple label which when aggregated form a multinomial distribution that capture emotional reaction our algorithm can more accurately predict distribution over such label compared to several competitive baseline 
this paper present hypothesis mixture decoding hm decoding a new decoding scheme that performs translation reconstruction using hypothesis generated by multiple translation system hm decoding involves two decoding stage first each component system decodes independently with the explored search space kept for use in the next step second a new search space is constructed by composing existing hypothesis produced by all component system using a set of rule provided by the hm decoder itself and a new set of model independent feature are used to seek the final best translation from this new search space few assumption are made by our approach about the underlying component system enabling u to leverage smt model based on arbitrary paradigm we compare our approach with several related technique and demonstrate significant bleu improvement in large scale chinese to english translation task 
we address a core aspect of the multilingual content synchronization task the identification of novel more informative or semantically equivalent piece of information in two document about the same topic this can be seen a an application oriented variant of textual entailment recognition where i t and h are in different language and ii entailment relation between t and h have to be checked in both direction using a combination of lexical syntactic and semantic feature to train a cross lingual textual entailment system we report promising result on different datasets 
this paper present a comparative study of target dependency structure yielded by several state of the art linguistic parser our approach is to measure the impact of these non isomorphic dependency structure to be used for string to dependency translation besides using traditional dependency parser we also use the dependency structure transformed from pcfg tree and predicate argument structure pas which are generated by an hpsg parser and a ccg parser the experiment on chinese to english translation show that the hpsg parser s pas achieved the best dependency and translation accuracy 
this paper investigates novel method for incorporating syntactic information in probabilistic latent variable model of lexical choice and contextual similarity the resulting model capture the effect of context on the interpretation of a word and in particular it effect on the appropriateness of replacing that word with a potentially related one evaluating our technique on two datasets we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitute 
standard algorithm for template based information extraction ie require predefined template schema and often labeled data to learn to extract their slot filler e g an embassy is the target of a bombing template this paper describes an approach to template based ie that remove this requirement and performs extraction without knowing the template structure in advance our algorithm instead learns the template structure automatically from raw text inducing template schema a set of linked event e g bombing include detonate set off and destroy event associated with semantic role we also solve the standard ie task using the induced syntactic pattern to extract role filler from specific document we evaluate on the muc terrorism dataset and show that we induce template structure very similar to hand created gold structure and we extract role filler with an f score of approaching the performance of algorithm that require full knowledge of the template 
conventional automated essay scoring aes measure may cause severe problem when directly applied in scoring automatic speech recognition asr transcription a they are error sensitive and unsuitable for the characteristic of asr transcription therefore we introduce a framework of finite state transducer fst to avoid the shortcoming compared with the latent semantic analysis with support vector regression lsa svr method stand for the conventional measure our fst method show better performance especially towards the asr transcription in addition we apply the synonym similarity to expand the fst model the final scoring performance reach an acceptable level of which is only lower than the correlation between human raters 
we propose symbol refined tree substitution grammar sr tsgs for syntactic parsing an sr tsg is an extension of the conventional tsg model where each nonterminal symbol can be refined subcategorized to fit the training data we aim to provide a unified model where tsg rule and symbol refinement are learned from training data in a fully automatic and consistent fashion we present a novel probabilistic sr tsg model based on the hierarchical pitman yor process to encode backoff smoothing from a fine grained sr tsg to simpler cfg rule and develop an efficient training method based on markov chain monte carlo mcmc sampling our sr tsg parser achieves an f score of in the wall street journal wsj english penn treebank parsing task which is a point improvement over a conventional bayesian tsg parser and better than state of the art discriminative reranking parser 
this paper present a joint model for template filling where the goal is to automatically specify the field of target relation such a seminar announcement or corporate acquisition event the approach model mention detection unification and field extraction in a flexible feature rich model that allows for joint modeling of interdependency at all level and across field such an approach can for example learn likely event duration and the fact that start time should come before end time while the joint inference space is large we demonstrate effective learning with a perceptron style approach that us simple greedy beam decoding empirical result in two benchmark domain demonstrate consistently strong performance on both mention detection and template filling task 
we present a general learning based approach for phrase level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature thus we can model the compositional effect required for accurate assignment of phrase level sentiment for example combining an adverb e g very with a positive polar adjective e g good produce a phrase very good with increased polarity over the adjective alone inspired by recent work on distributional approach to compositionality we model each word a a matrix and combine word using iterated matrix multiplication which allows for the modeling of both additive and multiplicative semantic effect although the multiplication based matrix space framework ha been shown to be a theoretically elegant way to model composition rudolph and giesbrecht training such model ha to be done carefully the optimization is non convex and requires a good initial starting point this paper present the first such algorithm for learning a matrix space model for semantic composition in the context of the phrase level sentiment analysis task our experimental result show statistically significant improvement in performance over a bag of word model 
many researcher have attempted to predict the enron corporate hierarchy from the data this work however ha been hampered by a lack of data we present a new large and freely available gold standard hierarchy using our new gold standard we show that a simple lower bound for social network based system outperforms an upper bound on the approach taken by current nlp system 
we present a system for cross lingual parse disambiguation exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different language have different ambiguity we simultaneously reduce ambiguity in multiple language in a fully automatic way evaluation show that the system reliably discard dispreferred par from the raw parser output which result in a pre selection that can speed up manual treebanking 
in this paper we propose a novel topic model based on incorporating dictionary definition traditional topic model treat word a surface string without assuming predefined knowledge about word meaning they infer topic only by observing surface word co occurrence however the co occurred word may not be semantically related in a manner that is relevant for topic coherence exploiting dictionary definition explicitly in our model yield a better understanding of word semantics leading to better text modeling we exploit wordnet a a lexical resource for sense definition we show that explicitly modeling word definition help improve performance significantly over the baseline for a text categorization task 
this paper present an attempt at building a large scale distributed composite language model that simultaneously account for local word lexical information mid range sentence syntactic structure and long span document semantic content under a directed markov random field paradigm the composite language model ha been trained by performing a convergent n best list approximate em algorithm that ha linear time complexity and a follow up em algorithm to improve word prediction power on corpus with up to a billion token and stored on a supercomputer the large scale distributed composite language model give drastic perplexity reduction over n gram and achieves significantly better translation quality measured by the bleu score and readability when applied to the task of re ranking the n best list from a state of the art parsing based machine translation system 
we present novel metric for parse evaluation in joint segmentation and parsing scenario where the gold sequence of terminal is not known in advance the protocol us distance based metric defined for the space of tree over lattice our metric allow u to precisely quantify the performance gap between non realistic parsing scenario assuming gold segmented and tagged input and realistic one not assuming gold segmentation and tag our evaluation of segmentation and parsing for modern hebrew shed new light on the performance of the best parsing system to date in the different scenario 
we consider the problem of predicting measurable response to scientific article based primarily on their text content specifically we consider paper in two field economics and computational linguistics and make prediction about downloads and within community citation our approach is based on generalized linear model allowing interpretability a novel extension that capture first order temporal effect is also presented we demonstrate that text feature significantly improve accuracy of prediction over metadata feature like author topical category and publication venue 
in this paper we present a structural learning model for joint sentiment classification and aspect analysis of text at various level of granularity our model aim to identify highly informative sentence that are aspect specific in online custom review the primary advantage of our model are two fold first it performs document level and sentence level sentiment polarity classification jointly second it is able to find informative sentence that are closely related to some respect in a review which may be helpful for aspect level sentiment analysis such a aspect oriented summarization the proposed method wa evaluated with chinese restaurant review preliminary experiment demonstrate that our model obtains promising performance 
an entity in a dialogue may be old new or mediated inferrable with respect to the hearer s belief knowing the information status of the entity participating in a dialogue can therefore facilitate it interpretation we address the under investigated problem of automatically determining the information status of discourse entity specifically we extend nissim s machine learning approach to information status determination with lexical and structured feature and exploit learned knowledge of the information status of each discourse entity for coreference resolution experimental result on a set of switchboard dialogue reveal that incorporating our proposed feature into nissim s feature set enables our system to achieve state of the art performance on information status classification and the resulting information can be used to improve the performance of learning based coreference resolvers 
argumentative zoning az analysis of the argumentative structure of a scientific paper ha proved useful for a number of information access task current approach to az rely on supervised machine learning ml requiring large amount of annotated data these approach are expensive to develop and port to different domain and task a potential solution to this problem is to use weakly supervised ml instead we investigate the performance of four weakly supervised classifier on scientific abstract data annotated for multiple az class our best classifier based on the combination of active learning and self training outperforms our best supervised classifier yielding a high accuracy of when using just of the labeled data this result suggests that weakly supervised learning could be employed to improve the practical applicability and portability of az across different information access task 
preordering of source side sentence ha proved to be useful in improving statistical machine translation most work ha used a parser in the source language along with rule to map the source language word order into the target language word order the requirement to have a source language parser is a major drawback which we seek to overcome in this paper instead of using a parser and then using rule to order the source side sentence we learn a model that can directly reorder source side sentence to match target word order using a small parallel corpus with high quality word alignment our model learns pairwise cost of a word immediately preceding another word we use the lin kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering we show gain in translation performance based on our reordering model for translating from hindi to english urdu to english with a public dataset and english to hindi for english to hindi we show that our technique achieves better performance than a method that us rule applied to the source side english parse 
sociolinguist have long argued that social context influence language use in all manner of way resulting in lects this paper explores a text classification problem we will call lect modeling an example of what ha been termed computational sociolinguistics in particular we use machine learning technique to identify social power relationship between member of a social network based purely on the content of their interpersonal communication we rely on statistical method a opposed to language specific engineering to extract feature which represent vocabulary and grammar usage indicative of social power lect we then apply support vector machine to model the social power lects representing superior subordinate communication in the enron email corpus our result validate the treatment of lect modeling a a text classification problem albeit a hard one and constitute a case for future research in computational sociolinguistics 
we propose a novel way of incorporating dependency parse and word co occurrence information into a state of the art web scale n gram model for spelling correction the syntactic and distributional information provides extra evidence in addition to that provided by a web scale n gram corpus and especially help with data sparsity problem experimental result show that introducing syntactic feature into n gram based model significantly reduces error by up to over the current state of the art the word co occurrence information show potential but only improves overall accuracy slightly 
in many natural language application there is a need to enrich syntactical parse tree we present a statistical tree annotator augmenting node with additional information the annotator is generic and can be applied to a variety of application we report such application in this paper predicting function tag predicting null element and predicting whether a tree constituent is projectable in machine translation our function tag prediction system outperforms significantly published result 
mapping document into an interlingual representation can help bridge the language barrier of cross lingual corpus many existing approach are based on word co occurrence extracted from aligned training data represented a a covariance matrix in theory such a covariance matrix should represent semantic equivalence and should be highly sparse unfortunately the presence of noise lead to dense covariance matrix which in turn lead to suboptimal document representation in this paper we explore technique to recover the desired sparsity in covariance matrix in two way first we explore word association measure and bilingual dictionary to weigh the word pair later we explore different selection strategy to remove the noisy pair based on the association score our experimental result on the task of aligning comparable document show the efficacy of sparse covariance matrix on two data set from two different language pair 
this paper address a data driven surface realisation model based on a large scale reversible grammar of german we investigate the relationship between the surface realisation performance and the character of the input to generation i e it degree of underspecification we extend a syntactic surface realisation system which can be trained to choose among word order variant such that the candidate set includes active and passive variant this allows u to study the interaction of voice and word order alternation in realistic german corpus data we show that with an appropriately underspecified input a linguistically informed realisation model trained to regenerate string from the underlying semantic representation achieves accuracy over a baseline of in the prediction of the original voice 
via an oracle experiment we show that the upper bound on accuracy of a ccg parser is significantly lowered when it search space is pruned using a supertagger though the supertagger also prune many bad par inspired by this analysis we design a single model with both supertagging and parsing feature rather than separating them into distinct model chained together in a pipeline to overcome the resulting increase in complexity we experiment with both belief propagation and dual decomposition approach to inference the first empirical comparison of these algorithm that we are aware of on a structured natural language processing problem on ccgbank we achieve a labelled dependency f measure of on gold po tag and on automatic part of speeoch tag the best reported result for this task 
information extraction ie hold the promise of generating a large scale knowledge base from the web s natural language text knowledge based weak supervision using structured data to heuristically label a training corpus work towards this goal by enabling the automated learning of a potentially unbounded number of relation extractor recently researcher have developed multi instance learning algorithm to combat the noisy training data that can come from heuristic labeling but their model assume relation are disjoint for example they cannot extract the pair founded job apple and ceo of job apple this paper present a novel approach for multi instance learning with overlapping relation that combine a sentence level extraction model with a simple corpus level component for aggregating the individual fact we apply our model to learn extractor for ny time text using weak supervision from free base experiment show that the approach run quickly and yield surprising gain in accuracy at both the aggregate and sentence level 
in this paper we address the problem of question recommendation from large archive of community question answering data by exploiting the user information need our experimental result indicate that question based on the same or similar information need can provide excellent question recommendation we show that translation model can be effectively utilized to predict the information need given only the user s query question experiment show that the proposed information need prediction approach can improve the performance of question recommendation 
we investigate whether wording stylistic choice and online behavior can be used to predict the age category of blog author our hypothesis is that significant change in writing style distinguish pre social medium blogger from post social medium blogger through experimentation with a range of year we found that the birth date of student in college at the time when social medium such a aim sm text messaging myspace and facebook first became popular enable accurate age prediction we also show that internet writing characteristic are important feature for age prediction but that lexical content is also needed to produce significantly more accurate result our best result allow for accuracy 
most previous research on verb clustering ha focussed on acquiring flat classification from corpus data although many manually built classification are taxonomic in nature also natural language processing nlp application benefit from taxonomic classification because they vary in term of the granularity they require from a classification we introduce a new clustering method called hierarchical graph factorization clustering hgfc and extend it so that it is optimal for the task our result show that hgfc outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from verbnet and that it yield state of the art performance also on a flat test set we demonstrate how the method can be used to acquire novel classification a well a to extend existing one on the basis of some prior knowledge about the classification 
temporal reasoner for document understanding typically assume that a document s creation date is known algorithm to ground relative time expression and order event often rely on this timestamp to assist the learner unfortunately the timestamp is not always known particularly on the web this paper address the task of automatic document timestamping presenting two new model that incorporate rich linguistic feature about time the first is a discriminative classifier with new feature extracted from the text s time expression e g since this model alone improves on previous generative model by the second model learns probabilistic constraint between time expression and the unknown document time imposing these learned constraint on the discriminative model further improves it accuracy finally we present a new experiment design that facilitates easier comparison by future work 
we propose a method to improve the accuracy of parsing bilingual text bitexts with the help of statistical machine translation smt system previous bitext parsing method use human annotated bilingual treebanks that are hard to obtain instead our approach us an auto generated bilingual treebank to produce bilingual constraint however because the auto generated bilingual treebank contains error the bilingual constraint are noisy to overcome this problem we use large scale unannotated data to verify the constraint and design a set of effective bilingual feature for parsing model based on the verified result the experimental result show that our new parser significantly outperform state of the art baseline moreover our approach is still able to provide improvement when we use a larger monolingual treebank that result in a much stronger baseline especially notable is that our approach can be used in a purely monolingual setting with the help of smt 
we present new training method that aim to mitigate local optimum and slow convergence in unsupervised training by using additional imperfect objective in it simplest form lateen em alternate between the two objective of ordinary soft and hard expectation maximization em algorithm switching objective when stuck can help escape local optimum we find that applying a single such alternation already yield state of the art result for english dependency grammar induction more elaborate lateen strategy track both objective with each validating the move proposed by the other disagreement can signal earlier opportunity to switch or terminate saving iteration de emphasizing fixed point in these way eliminates some guesswork from tuning em an evaluation against a suite of unsupervised dependency parsing task for a variety of language showed that lateen strategy significantly speed up training of both em algorithm and improve accuracy for hard em 
we present a novel approach for automatic collocation error correction in learner english which is based on paraphrase extracted from parallel corpus our key assumption is that collocation error are often caused by semantic similarity in the first language l language of the writer an analysis of a large corpus of annotated learner english confirms this assumption we evaluate our approach on real world learner data and show that l induced paraphrase outperform traditional approach based on edit distance homophone and wordnet synonym 
we study the problem of finding the best head driven parsing strategy for linear context free rewriting system production a head driven strategy must begin with a specified righthand side nonterminal the head and add the remaining nonterminals one at a time in any order we show that it is np hard to find the best head driven strategy in term of either the time or space complexity of parsing 
we propose an approach that bias machine translation system toward relevant translation based on topic specific context where topic are induced in an unsupervised way using topic model this can be thought of a inducing subcorpora for adaptation without any human annotation we use these topic distribution to compute topic dependent lexical weighting probability and directly incorporate them into our translation model a feature conditioning lexical probability on the topic bias translation toward topic relevant output resulting in significant improvement of up to bleu and ter on chinese to english translation over a strong baseline 
we propose a language independent method for the automatic extraction of transliteration pair from parallel corpus in contrast to previous work our method us no form of supervision and doe not require linguistically informed preprocessing we conduct experiment on data set from the news shared task on transliteration mining and achieve an f measure of up to outperforming most of the semi supervised system that were submitted we also apply our method to english hindi and english arabic parallel corpus and compare the result with manually built gold standard which mark transliterated word pair finally we integrate the transliteration module into the giza word aligner and evaluate it on two word alignment task achieving improvement in both precision and recall measured against gold standard word alignment 
the rapid and continuous growth of social networking site ha led to the emergence of many community of communicating group many of these group discus ideological and political topic it is not uncommon that the participant in such discussion split into two or more subgroup the member of each subgroup share the same opinion toward the discussion topic and are more likely to agree with member of the same subgroup and disagree with member from opposing subgroup in this paper we propose an unsupervised approach for automatically detecting discussant subgroup in online community we analyze the text exchanged between the participant of a discussion to identify the attitude they carry toward each other and towards the various aspect of the discussion topic we use attitude prediction to construct an attitude vector for each discussant we use clustering technique to cluster these vector and hence determine the subgroup membership of each participant we compare our method to text clustering and other baseline and show that our method achieves promising result 
state of the art statistical machine translation mt system have made significant progress towards producing user acceptable translation output however there is still no efficient way for mt system to inform user which word are likely translated correctly and how confident it is about the whole sentence we propose a novel framework to predict word level and sentence level mt error with a large number of novel feature experimental result show that the mt error prediction accuracy is increased from to in f score the pearson correlation between the proposed confidence measure and the human targeted translation edit rate hter is improvement between and ter reduction are obtained with the n best list reranking task using the proposed confidence measure also we present a visualization prototype of mt error at the word and sentence level with the objective to improve post editor productivity 
this paper introduces chart inference ci an algorithm for deriving a ccg category for an unknown word from a partial parse chart it is shown to be faster and more precise than a baseline brute force method and to achieve wider coverage than a rule based system in addition we show the application of ci to a domain adaptation task for question word which are largely missing in the penn treebank when used in combination with self training ci increase the precision of the baseline statccg parser over subject extraction question by an error analysis show that ci contributes to the increase by expanding the number of category type available to the parser while self training adjusts the count 
linear model have enjoyed great success in structured prediction in nlp while a lot of progress ha been made on efficient training with several loss function the problem of endowing learner with a mechanism for feature selection is still unsolved common approach employ ad hoc filtering or l regularization both ignore the structure of the feature space preventing practicioners from encoding structural prior knowledge we fill this gap by adopting regularizers that promote structured sparsity along with efficient algorithm to handle them experiment on three task chunking entity recognition and dependency parsing show gain in performance compactness and model interpretability 
most previous study in computerized deception detection have relied only on shallow lexico syntactic pattern this paper investigates syntactic stylometry for deception detection adding a somewhat unconventional angle to prior literature over four different datasets spanning from the product review to the essay domain we demonstrate that feature driven from context free grammar cfg parse tree consistently improve the detection performance over several baseline that are based only on shallow lexico syntactic feature our result improve the best published result on the hotel review data ott et al reaching accuracy with error reduction 
summarizing and analyzing twitter content is an important and challenging task in this paper we propose to extract topical keyphrases a one way to summarize twitter we propose a context sensitive topical pagerank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking we evaluate our proposed method on a large twitter data set experiment show that these method are very effective for topical keyphrase extraction 
sentiment analysis on twitter data ha attracted much attention recently in this paper we focus on target dependent twitter sentiment classification namely given a query we classify the sentiment of the tweet a positive negative or neutral according to whether they contain positive negative or neutral sentiment about that query here the query serf a the target of the sentiment the state of the art approach for solving this problem always adopt the target independent strategy which may assign irrelevant sentiment to the given target moreover the state of the art approach only take the tweet to be classified into consideration when classifying the sentiment they ignore it context i e related tweet however because tweet are usually short and more ambiguous sometimes it is not enough to consider only the current tweet for sentiment classification in this paper we propose to improve target dependent twitter sentiment classification by incorporating target dependent feature and taking related tweet into consideration according to the experimental result our approach greatly improves the performance of target dependent sentiment classification 
natural language question have become popular in web search however various question can be formulated to convey the same information need which pose a great challenge to search system in this paper we automatically mined w h question reformulation pattern from large scale search log data the question reformulations generated from these pattern are further incorporated into the retrieval model experiment show that using question reformulation pattern can significantly improve the search performance of natural language question 
many machine translation mt evaluation metric have been shown to correlate better with human judgment than bleu in principle tuning on these metric should yield better system than tuning on bleu however due to issue such a speed requirement for linguistic resource and optimization difficulty they have not been widely adopted for tuning this paper present port a new mt evaluation metric which combine precision recall and an ordering metric and which is primarily designed for tuning mt system port doe not require external resource and is quick to compute it ha a better correlation with human judgment than bleu we compare port tuned mt system to bleu tuned baseline in five experimental condition involving four language pair port tuning achieves consistently better performance than bleu tuning according to four automated metric including bleu and to human evaluation in comparison of output from source sentence human judge preferred the port tuned output of the time v bleu tuning preference and tie 
we describe a method for prediction of linguistic structure in a language for which only unlabeled data is available using annotated data from a set of one or more helper language our approach is based on a model that locally mix between supervised model from the helper language parallel data is not used allowing the technique to be applied even in domain where human translated text are unavailable we obtain state of the art performance for two task of structure prediction unsupervised part of speech tagging and unsupervised dependency parsing 
a the number of learner of english is constantly growing automatic error correction of esl learner writing is an increasingly active area of research however most research ha mainly focused on error concerning article and preposition even though tense aspect error are also important one of the main reason why tense aspect error correction is difficult is that the choice of tense aspect is highly dependent on global context previous research on grammatical error correction typically us pointwise prediction that performs classification on each word independently and thus fails to capture the information of neighboring label in order to take global information into account we regard the task a sequence labeling each verb phrase in a document is labeled with tense aspect depending on surrounding label our experiment show that the global context make a moderate contribution to tense aspect error correction 
large vocabulary speech recognition system fail to recognize word beyond their vocabulary many of which are information rich term like named entity or foreign word hybrid word sub word system solve this problem by adding sub word unit to large vocabulary word based system new word can then be represented by combination of sub word unit previous work heuristically created the sub word lexicon from phonetic representation of text using simple statistic to select common phone sequence we propose a probabilistic model to learn the subword lexicon optimized for a given task we consider the task of out of vocabulary oov word detection which relies on output from a hybrid model a hybrid model with our learned sub word lexicon reduces error by and absolute at a false alarm rate on an english broadcast news and mit lecture task respectively 
minimum error rate training is a crucial component to many state of the art nlp application such a machine translation and speech recognition however common evaluation function such a bleu or word error rate are generally highly non convex and thus prone to search error in this paper we present lp mert an exact search algorithm for minimum error rate training that reach the global optimum using a series of reduction to linear programming given a set of n best list produced from s input sentence this algorithm find a linear model that is globally optimal with respect to this set we find that this algorithm is polynomial in n and in the size of the model but exponential in s we present extension of this work that let u scale to reasonably large tuning set e g one thousand sentence by either searching only promising region of the parameter space or by using a variant of lp mert that relies on a beam search approximation experimental result show improvement over the standard och algorithm 
we propose several technique for improving statistical machine translation between closely related language with scarce resource we use character level translation trained on n gram character aligned bitexts and tuned using word level bleu which we further augment with character based transliteration at the word level and combine with a word level translation model the evaluation on macedonian bulgarian movie subtitle show an improvement of bleu point over a phrase based word level baseline 
extractive method for multi document summarization are mainly governed by information overlap coherence and content constraint we present an unsupervised probabilistic approach to model the hidden abstract concept across document a well a the correlation between these concept to generate topically coherent and non redundant summary based on human evaluation our model generate summary with higher linguistic quality in term of coherence readability and redundancy compared to benchmark system although our system is unsupervised and optimized for topical coherence we achieve a rouge on the duc test set roughly in the range of state of the art supervised model 
this paper describes the creation of the first large scale corpus containing draft and final version of essay written by non native speaker with the sentence aligned across different version furthermore the sentence in the draft are annotated with comment from teacher the corpus is intended to support research on textual revision by language learner and how it is influenced by feedback this corpus ha been converted into an xml format conforming to the standard of the text encoding initiative tei 
in this paper we study unsupervised word sense disambiguation wsd based on sense definition we learn low dimensional latent semantic vector of concept definition to construct a more robust sense similarity measure wmfvec experiment on four all word wsd data set show significant improvement over the baseline wsd system and lda based similarity measure achieving result comparable to state of the art wsd system 
we address the problem of learning the mapping between word and their possible pronunciation in term of sub word unit most previous approach have involved generative modeling of the distribution of pronunciation usually trained to maximize likelihood we propose a discriminative feature rich approach using large margin learning this approach allows u to optimize an objective closely related to a discriminative task to incorporate a large number of complex feature and still do inference efficiently we test the approach on the task of lexical access that is the prediction of a word given a phonetic transcription in experiment on a subset of the switchboard conversational speech corpus our model thus far improve classification error rate from a previously published result of to about we find that large margin approach outperform conditional random field learning and that the passive aggressive algorithm for large margin learning is faster to converge than the pegasos algorithm 
long span feature such a syntax can improve language model for task such a speech recognition and machine translation however these language model can be difficult to use in practice because of the time required to generate feature for rescoring a large hypothesis set in this work we propose substructure sharing which save duplicate work in processing hypothesis set with redundant hypothesis structure we apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedup and further improve the accuracy of these tool through up training when using these improved tool in a language model for speech recognition we obtain significant speed improvement with both n best and hill climbing rescoring and show that up training lead to wer reduction 
we introduce a discriminatively trained globally normalized log linear variant of the lexical translation model proposed by brown et al in our model arbitrary non independent feature may be freely incorporated thereby overcoming the inherent limitation of generative model which require that feature be sensitive to the conditional independency of the generative process however unlike previous work on discriminative modeling of word alignment which also permit the use of arbitrary feature the parameter in our model are learned from unannotated parallel sentence rather than from supervised word alignment using a variety of intrinsic and extrinsic measure including translation performance we show our model yield better alignment than generative baseline in a number of language pair 
in this work we present a novel approach to the generation task of ordering prenominal modifier we take a maximum entropy reranking approach to the problem which admits arbitrary feature on a permutation of modifier exploiting hundred of thousand of feature in total we compare our error rate to the state of the art and to a strong google n gram count baseline we attain a maximum error reduction of and average error reduction across all test set of compared to the state of the art and a maximum error reduction of and average error reduction across all test set of compared to our google n gram count baseline 
text simplification aim to rewrite text into simpler version and thus make information accessible to a broader audience most previous work simplifies sentence using handcrafted rule aimed at splitting long sentence or substitute difficult word using a predefined dictionary this paper present a data driven model based on quasi synchronous grammar a formalism that can naturally capture structural mismatch and complex rewrite operation we describe how such a grammar can be induced from wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrite generated by the grammar we show experimentally that our method creates simplification that significantly reduce the reading difficulty of the input while maintaining grammaticality and preserving it meaning 
parallel data in the domain of interest is the key resource when training a statistical machine translation smt system for a specific purpose since ad hoc manual translation can represent a significant investment in time and money a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful in this work we show how to predict what the learning curve would look like if we were to manually translate increasing amount of data we consider two scenario monolingual sample in the source and target language are available and an additional small amount of parallel corpus is also available we propose method for predicting learning curve in both these scenario 
reordering is a difficult task in translating between widely different language such a japanese and english we employ the post ordering framework proposed by sudoh et al b for japanese to english translation and improve upon the reordering method the existing post ordering method reorder a sequence of target language word in a source language word order via smt while our method reorder the sequence by parsing the sequence to obtain syntax structure similar to a source language structure and transferring the obtained syntax structure into the syntax structure of the target language 
mining of transliteration from comparable or parallel text can enhance natural language processing application such a machine translation and cross language information retrieval this paper present an enhanced transliteration mining technique that us a generative graph reinforcement model to infer mapping between source and target character sequence an initial set of mapping are learned through automatic alignment of transliteration pair at character sequence level then these mapping are modeled using a bipartite graph a graph reinforcement algorithm is then used to enrich the graph by inferring additional mapping during graph reinforcement appropriate link reweighting is used to promote good mapping and to demote bad one the enhanced transliteration mining technique is tested in the context of mining transliteration from parallel wikipedia title in alphabet based language pair namely english arabic english russian english hindi and english tamil the improvement in f measure over the baseline system were and basis point for the four language pair respectively the result herein outperform the best reported result in the literature by and basis point for the four language pair respectively 
we use multiple view for cross domain document classification the main idea is to strengthen the view consistency for target data with source training data by identifying the correlation of domain specific feature from different domain we present an information theoretic multi view adaptation model imam based on a multi way clustering scheme where word and link cluster can draw together seemingly unrelated domain specific feature from both side and iteratively boost the consistency between document clustering based on word and link view experiment show that imam significantly outperforms state of the art baseline 
extracting sentiment and topic lexicon is important for opinion mining previous work have showed that supervised learning method are superior for this task however the performance of supervised method highly relies on manually labeled training data in this paper we propose a domain adaptation framework for sentimentand topiclexicon co extraction in a domain of interest where we do not require any labeled data but have lot of labeled data in another related domain the framework is twofold in the first step we generate a few high confidence sentiment and topic seed in the target domain in the second step we propose a novel relational adaptive bootstrapping rap algorithm to expand the seed in the target domain by exploiting the labeled source domain data and the relationship between topic and sentiment word experimental result show that our domain adaptation framework can extract precise lexicon in the target domain without any annotation 
this paper focus on identifying extracting and evaluating feature related to syntactic complexity of spontaneous spoken response a part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspect considered important in the construct of communicative competence our goal is to find effective feature selected from a large set of feature proposed previously and some new feature designed in analogous way from a syntactic complexity perspective that correlate well with human rating of the same spoken response and to build automatic scoring model based on the most promising feature by using machine learning method on human transcription with manually annotated clause and sentence boundary our best scoring model achieves an overall pearson correlation with human rater score of r on an unseen test set whereas correlation of model using sentence or clause boundary from automated classifier are around r 
we consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage we show that a soft inference procedure based on a combination of constrained weighted random walk through the knowledge base graph can be used to reliably infer new belief for the knowledge base more specifically we show that the system can learn to infer different target relation by tuning the weight associated with random walk that follow different path through the graph using a version of the path ranking algorithm lao and cohen b we apply this approach to a knowledge base of approximately belief extracted imperfectly from the web by nell a never ending language learner carlson et al this new system improves significantly over nell s earlier horn clause learning and inference method it obtains nearly double the precision at rank and the new learning method is also applicable to many more inference task 
this paper focus on mining the hyponymy or is a relation from large scale open domain web document a nonlinear probabilistic model is exploited to model the correlation between sentence in the aggregation of pattern matching result based on the model we design a set of evidence combination and propagation algorithm these significantly improve the result quality of existing approach experimental result conducted on million web page and hypernym label for term show over performance improvement in term of p map and r precision 
the web and digitized text source contain a wealth of information about named entity such a politician actor company or cultural landmark extracting this information ha enabled the automated construction of large knowledge base containing hundred million of binary relationship or attribute value about these named entity however in reality most knowledge is transient i e change over time requiring a temporal dimension in fact extraction in this paper we develop a methodology that combine label propagation with constraint reasoning for temporal fact extraction label propagation aggressively gather fact candidate and an integer linear program is used to clean out false hypothesis that violate temporal constraint our method is able to improve on recall while keeping up with precision which we demonstrate by experiment with biography style wikipedia page and a large corpus of news article 
we propose a simple training regime that can improve the extrinsic performance of a parser given only a corpus of sentence and a way to automatically evaluate the extrinsic quality of a candidate parse we apply our method to train parser that excel when used a part of a reordering component in a statistical machine translation system we use a corpus of weakly labeled reference reordering to guide parser training our best parser contribute significant improvement in subjective translation quality while their intrinsic attachment score typically regress 
this paper present an exponential model for translation into highly inflected language which can be scaled to very large datasets a in other recent proposal it predicts target side phrase and can be conditioned on source side context however crucially for the task of modeling morphological generalization it estimate feature parameter from the entire training set rather than a a collection of separate classifier we apply it to english czech translation using a variety of feature capturing potential predictor for case number and gender and one of the largest publicly available parallel data set we also describe generation and modeling of inflected form unobserved in training data and decoding procedure for a model with non local target side feature dependency 
we propose an efficient way to train maximum entropy language model melm and neural network language model nnlm the advantage of the proposed method come from a more robust and efficient subsampling technique the original multi class language modeling problem is transformed into a set of binary problem where each binary classifier predicts whether or not a particular word will occur we show that the binarized model is a powerful a the standard model and allows u to aggressively subsample negative training example without sacrificing predictive performance empirical result show that we can train melm and nnlm at of the standard complexity with no loss in performance 
in this work we address the task of computerassisted assessment of short student answer we combine several graph alignment feature with lexical semantic similarity measure using machine learning technique and show that the student answer can be more accurately graded than if the semantic measure were used in isolation we also present a first attempt to align the dependency graph of the student and the instructor answer in order to make use of a structural component in the automatic grading of student answer 
in this paper we demonstrate that accurate machine translation is possible without the concept of word treating mt a a problem of transformation between character string we achieve this result by applying phrasal inversion transduction grammar alignment technique to character string to train a character based translation model and using this in the phrase based mt framework we also propose a look ahead parsing algorithm and substring informed prior probability to achieve more effective and efficient alignment in an evaluation we demonstrate that character based translation can achieve result that compare to word based system while effectively translating unknown and uncommon word over several language pair 
in relation extraction distant supervision seek to extract relation between entity from text by using a knowledge base such a freebase a a source of supervision when a sentence and a knowledge base refer to the same entity pair this approach heuristically label the sentence with the corresponding relation in the knowledge base however this heuristic can fail with the result that some sentence are labeled wrongly this noisy labeled data cause poor extraction performance in this paper we propose a method to reduce the number of wrong label we present a novel generative model that directly model the heuristic labeling process of distant supervision the model predicts whether assigned label are correct or wrong via it hidden variable our experimental result show that this model detected wrong label with higher performance than baseline method in the experiment we also found that our wrong label reduction boosted the performance of relation extraction 
we demonstrate how supervised discriminative machine learning technique can be used to automate the assessment of english a a second or other language esol examination script in particular we use rank preference learning to explicitly model the grade relationship between script a number of different feature are extracted and ablation test are used to investigate their contribution to overall performance a comparison between regression and rank preference model further support our method experimental result on the first publically available dataset show that our system can achieve level of performance close to the upper bound for the task a defined by the agreement between human examiner on the same corpus finally using a set of outlier text we test the validity of our model and identify case where the model s score diverge from that of a human examiner 
a system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise belief about previous input under some circumstance such an error correction capability might induce comprehenders to adopt grammatical analysis that are inconsistent with the true input here we present a formal model of how such input unfaithful garden path may be adopted and the difficulty incurred by their subsequent disconfirmation combining a rational noisy channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty we also present a behavioral experiment confirming the key empirical prediction of the theory 
the last decade ha seen many interesting application of question answering qa technology the jeopardy quiz show is certainly one of the most fascinating from the viewpoint of both it broad domain and the complexity of it language in this paper we study kernel method applied to syntactic semantic structure for accurate classification of jeopardy definition question our extensive empirical analysis show that our classification model largely improve on classifier based on word language model such classifier are also used in the state of the art qa pipeline constituting watson the ibm jeopardy system our experiment measuring their impact on watson show enhancement in qa accuracy and a consequent increase in the amount of money earned in game based evaluation 
open information extraction ie is the task of extracting assertion from massive corpus without requiring a pre specified vocabulary this paper show that the output of state of the art open ie system is rife with uninformative and incoherent extraction to overcome these problem we introduce two simple syntactic and lexical constraint on binary relation expressed by verb we implemented the constraint in the reverb open ie system which more than double the area under the precision recall curve relative to previous extractor such a textrunner and woepos more than of reverb s extraction are at precision or higher compared to virtually none for earlier system the paper concludes with a detailed analysis of reverb s error suggesting direction for future work 
this paper present a model that extends semantic role labeling existing approach independently analyze relation expressed by verb predicate or those expressed a nominalizations however sentence express relation via other linguistic phenomenon a well furthermore these phenomenon interact with each other thus restricting the structure they articulate in this paper we use this intuition to define a joint inference model that capture the inter dependency between verb semantic role labeling and relation expressed using preposition the scarcity of jointly labeled data present a crucial technical challenge for learning a joint model the key strength of our model is that we use existing structure predictor a black box by enforcing consistency constraint between their prediction we show improvement in the performance of both task without retraining the individual model 
the local multi bottom up tree transducer is introduced and related to the non contiguous synchronous tree sequence substitution grammar it is then shown how to obtain a weighted local multi bottom up tree transducer from a bilingual and biparsed corpus finally the problem of non preservation of regularity is addressed three property that ensure preservation are introduced and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled 
this paper brings a marriage of two seemly unrelated topic natural language processing nlp and social network analysis sna we propose a new task in sna which is to predict the diffusion of a new topic and design a learning based framework to solve this problem we exploit the latent semantic information among user topic and social connection a feature for prediction our framework is evaluated on real data collected from public domain the experiment show auc improvement over baseline method the source code and dataset are available at http www csie ntu edu tw d diffusion 
we consider the problem of learning factored probabilistic ccg grammar for semantic parsing from data containing sentence paired with logical form meaning representation traditional ccg lexicon list lexical item that pair word and phrase with syntactic and semantic content such lexicon can be inefficient when word appear repeatedly with closely related lexical content in this paper we introduce factored lexicon which include both lexeme to model word meaning and template to model systematic variation in word usage we also present an algorithm for learning factored ccg lexicon along with a probabilistic parse selection model evaluation on benchmark datasets demonstrate that the approach learns highly accurate parser whose generalization performance benefit greatly from the lexical factoring 
we describe the use of a hierarchical topic model for automatically identifying syntactic and lexical pattern that explicitly state ontological relation we leverage distant supervision using relation from the knowledge base freebase but do not require any manual heuristic nor manual seed list selection result show that the learned pattern can be used to extract new relation with good precision 
with the recent rise in popularity and scale of social medium a growing need exists for system that can extract useful information from huge amount of data we address the issue of detecting influenza epidemic first the proposed system extract influenza related tweet using twitter api then only tweet that mention actual influenza patient are extracted by the support vector machine svm based classifier the experiment result demonstrate the feasibility of the proposed approach correlation to the gold standard especially at the outbreak and early spread early epidemic stage the proposed method show high correlation correlation which outperforms the state of the art method this paper describes that twitter text reflect the real world and that nlp technique can be applied to extract only tweet that contain useful information 
in this paper we present a new ranking scheme collaborative ranking cr in contrast to traditional non collaborative ranking scheme which solely relies on the strength of isolated query and one stand alone ranking algorithm the new scheme integrates the strength from multiple collaborator of a query and the strength from multiple ranking algorithm we elaborate three specific form of collaborative ranking namely micro collaborative ranking micr macro collaborative ranking macr and micro macro collaborative ranking mimacr experiment on entity linking task show that our proposed scheme is indeed effective and promising 
previous work ha shown that high quality phrasal paraphrase can be extracted from bilingual parallel corpus however it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrase which are more obviously learnable from monolingual parallel corpus we extend bilingual paraphrase extraction to syntactic paraphrase and demonstrate it ability to learn a variety of general paraphrastic transformation including passivization dative shift and topicalization we discus how our model can be adapted to many text generation task by augmenting it feature set development data and parameter estimation routine we illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve result competitive with state of the art compression system 
we present a method to discover robust and interpretable sociolinguistic association from raw geotagged text data using aggregate demographic statistic about the author geographic community we solve a multi output regression problem between demographic and lexical frequency by imposing a composite ell regularizer we obtain structured sparsity driving entire row of coefficient to zero we perform two regression study first we use term frequency to predict demographic attribute our method identifies a compact set of word that are strongly associated with author demographic next we conjoin demographic attribute into feature which we use to predict term frequency the composite regularizer identifies a small number of feature which correspond to community of author united by shared demographic and linguistic property 
we present a bayesian nonparametric model for estimating tree insertion grammar tig building upon recent work in bayesian inference of tree substitution grammar tsg via dirichlet process under our general variant of tig grammar are estimated via the metropolis hastings algorithm that us a context free grammar transformation a a proposal which allows for cubic time string parsing a well a tree wide joint sampling of derivation in the spirit of cohn and blunsom we use the penn treebank for our experiment and find that our proposal bayesian tig model not only ha competitive parsing performance but also find compact yet linguistically rich tig representation of the data 
we introduce a spectral learning algorithm for latent variable pcfgs petrov et al under a separability singular value condition we prove that the method provides consistent parameter estimate 
recently it wa shown kuhlmann satta tree adjoining grammar are not closed under strong lexicalization comput linguist that finitely ambiguous tree adjoining grammar cannot be transformed into a normal form preserving the generated tree language in which each production contains a lexical symbol a more powerful model the simple context free tree grammar admits such a normal form it can be effectively constructed and the maximal rank of the nonterminals only increase by thus simple context free tree grammar strongly lexicalize tree adjoining grammar and themselves 
two decade after their invention the ibm word based translation model widely available in the giza toolkit remain the dominant approach to word alignment and an integral part of many statistical translation system although many model have surpassed them in accuracy none have supplanted them in practice in this paper we propose a simple extension to the ibm model an l prior to encourage sparsity in the word to word translation model we explain how to implement this extension efficiently for large scale data also released a a modification to giza and demonstrate in experiment on czech arabic chinese and urdu to english translation significant improvement over ibm model in both word alignment up to f and translation quality up to b 
all type of part of speech po tagging error have been equally treated by existing tagger however the error are not equally important since some error affect the performance of subsequent natural language processing nlp task seriously while others do not this paper aim to minimize these serious error while retaining the overall performance of po tagging two gradient loss function are proposed to reflect the different type of error they are designed to assign a larger cost to serious error and a smaller one to minor error through a set of po tagging experiment it is shown that the classifier trained with the proposed loss function reduces serious error compared to state of the art po tagger in addition the experimental result on text chunking show that fewer serious error help to improve the performance of subsequent nlp task 
measuring semantic relatedness between word or concept is a crucial process to many natural language processing task exiting method exploit semantic evidence from a single knowledge source and are predominantly evaluated only in the general domain this paper introduces a method of harnessing different knowledge source under a uniform model for measuring semantic relatedness between word or concept using wikipedia and wordnet a example and evaluated in both the general and biomedical domain it successfully combine strength from both knowledge source and outperforms state of the art on many datasets 
we re investigate the rationale for and the effectiveness of adopting the notion of depth and density in wordnet based semantic similarity measure we show that the intuition for including these notion in wordnet based similarity measure doe not always stand up to empirical examination in particular the traditional definition of depth and density a ordinal integer value in the hierarchical structure of wordnet doe not always correlate with human judgment of lexical semantic similarity which imposes strong limitation on their contribution to an accurate similarity measure we thus propose several novel definition of depth and density which yield significant improvement in degree of correlation with similarity when used in wordnet based semantic similarity measure the new definition consistently improve performance on a task of correlating with human judgment 
in this paper we dedicate to the topic of aspect ranking which aim to automatically identify important product aspect from online consumer review the important aspect are identified according to two observation a the important aspect of a product are usually commented by a large number of consumer and b consumer opinion on the important aspect greatly influence their overall opinion on the product in particular given consumer review of a product we first identify the product aspect by a shallow dependency parser and determine consumer opinion on these aspect via a sentiment classifier we then develop an aspect ranking algorithm to identify the important aspect by simultaneously considering the aspect frequency and the influence of consumer opinion given to each aspect on their overall opinion the experimental result on popular product in four domain demonstrate the effectiveness of our approach we further apply the aspect ranking result to the application of document level sentiment classification and improve the performance significantly 
to discover relation type from text most method cluster shallow or syntactic pattern of relation mention but consider only one possible sense per pattern in practice this assumption is often violated in this paper we overcome this issue by inducing cluster of pattern sens from feature representation of pattern in particular we employ a topic model to partition entity pair associated with pattern into sense cluster using local and global feature we merge these sense cluster into semantic relation using hierarchical agglomerative clustering we compare against several baseline a generative latent variable model a clustering method that doe not disambiguate between path sens and our own approach but with only local feature experimental result show our proposed approach discovers dramatically more accurate cluster than model without sense disambiguation and that incorporating global feature such a the document theme is crucial 
there are a growing number of popular web site where user submit and review instruction for completing task a varied a building a table and baking a pie in addition to providing their subjective evaluation reviewer often provide actionable refinement these refinement clarify correct improve or provide alternative to the original instruction however identifying and reading all relevant review is a daunting task for a user in this paper we propose a generative model that jointly identifies user proposed refinement in instruction review at multiple granularity and aligns them to the appropriate step in the original instruction labeled data is not readily available for these task so we focus on the unsupervised setting in experiment in the recipe domain our model provides f for predicting refinement at the review level and f for predicting refinement segment within review 
resolving coordination ambiguity is a classic hard problem this paper look at co ordination disambiguation in complex noun phrase np parser trained on the penn treebank are reporting impressive number these day but they don t do very well on this problem we explore system trained using three type of corpus annotated e g the penn treebank bitexts e g europarl and unannotated monolingual e g google n gram size matter is a million word is potentially billion of word and is potentially trillion of word the unannotated monolingual data is helpful when the ambiguity can be resolved through association among the lexical item the bilingual data is helpful when the ambiguity can be resolved by the order of word in the translation we train separate classifier with monolingual and bilingual feature and iteratively improve them via co training the co trained classifier achieves close to accuracy on treebank data and make fewer error than a supervised system trained with treebank annotation 
dependency structure a a first step towards semantics is believed to be helpful to improve translation quality however previous work on dependency structure based model typically resort to insertion operation to complete translation which make it difficult to specify ordering information in translation rule in our model of this paper we handle this problem by directly specifying the ordering information in head dependent rule which represent the source side a head dependent relation and the target side a string the head dependent rule require only substitution operation thus our model requires no heuristic or separate ordering model of the previous work to control the word order of translation large scale experiment show that our model performs well on long distance reordering and outperforms the state of the art constituency to string model bleu on average and hierarchical phrase based model bleu on average on two chinese english nist test set without resort to phrase or parse forest for the first time a source dependency structure based model catch up with and surpasses the state of the art translation model 
information published in online stock investment message board and more recently in stock microblogs is considered highly valuable by many investor previous work focused on aggregation of sentiment from all user however in this work we show that it is beneficial to distinguish expert user from non expert we propose a general framework for identifying expert investor and use it a a basis for several model that predict stock rise from stock microblogging message stock tweet in particular we present two method that combine expert identification and per user unsupervised learning these method were shown to achieve relatively high precision in predicting stock rise and significantly outperform our baseline in addition our work provides an in depth analysis of the content and potential usefulness of stock tweet 
this paper investigates improving supervised word segmentation accuracy with unlabeled data both large scale in domain data and small scale document text are considered we present a unified solution to include feature derived from unlabeled data to a discriminative learning model for the large scale data we derive string statistic from gigaword to assist a character based segmenter in addition we introduce the idea about transductive document level segmentation which is designed to improve the system recall for out of vocabulary oov word which appear more than once inside a document novel feature result in relative error reduction of and in term of f score and the recall of oov word respectively 
traditional approach to sentiment classification rely on lexical feature syntax based feature or a combination of the two we propose semantic feature using word sens for a supervised document level sentiment classifier to highlight the benefit of sense based feature we compare word based representation of document with a sense based representation where wordnet sens of the word are used a feature in addition we highlight the benefit of sens by presenting a part of speech wise effect on sentiment classification finally we show that even if a wsd engine disambiguates between a limited set of word in a document a sentiment classifier still performs better than what it doe in absence of sense annotation since word sens used a feature show promise we also examine the possibility of using similarity metric defined on wordnet to address the problem of not finding a sense in the training corpus we perform experiment using three popular similarity metric to mitigate the effect of unknown synset in a test corpus by replacing them with similar synset from the training corpus the result show promising improvement with respect to the baseline 
we introduce a novel method for grammatical error correction with a number of small corpus to make the best use of several corpus with different characteristic we employ a meta learning with several base classifier trained on different corpus this research focus on a grammatical error correction task for article error a series of experiment is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpus 
attempt to profile author according to their characteristic extracted from textual data including native language have drawn attention in recent year via various machine learning approach utilising mostly lexical feature drawing on the idea of contrastive analysis which postulate that syntactic error in a text are to some extent influenced by the native language of an author this paper explores the usefulness of syntactic feature for native language identification we take two type of parse substructure a feature horizontal slice of tree and the more general feature schema from discriminative parse reranking and show that using this kind of syntactic feature result in an accuracy score in classification of seven native language of around an error reduction of more than 
this paper present an unsupervised approach to learning translation span alignment from parallel data that improves syntactic rule extraction by deleting spurious word alignment link and adding new valuable link based on bilingual translation span correspondence experiment on chinese english translation demonstrate improvement over standard method for tree to string and tree to tree translation 
in this paper we present an unsupervised framework that bootstrap a complete coreference resolution core system from word association mined from a large unlabeled corpus we show that word association are useful for core e g the strong association between obama and president is an indicator of likely coreference association information ha so far not been used in core because it is sparse and difficult to learn from small labeled corpus since unlabeled text is readily available our unsupervised approach address the sparseness problem in a self training framework we train a decision tree on a corpus that is automatically labeled using word association we show that this unsupervised system ha better core performance than other learning approach that do not use manually labeled data 
we explore unsupervised approach to relation extraction between two named entity for instance the semantic bornin relation between a person and location entity concretely we propose a series of generative probabilistic model broadly similar to topic model each which generates a corpus of observed triple of entity mention pair and the surface syntactic dependency path between them the output of each model is a clustering of observed relation tuples and their associated textual expression to underlying semantic relation type our proposed model exploit entity type constraint within a relation a well a feature on the dependency path between entity mention we examine effectiveness of our approach via multiple evaluation and demonstrate error reduction in precision over a state of the art weakly supervised baseline 
we present a method for the computation of prefix probability for synchronous context free grammar our framework is fairly general and relies on the combination of a simple novel grammar transformation and standard technique to bring grammar into normal form 
arabic handwriting recognition hr is a challenging problem due to arabic s connected letter form consonantal diacritic and rich morphology in this paper we isolate the task of identification of erroneous word in hr from the task of producing correction for these word we consider a variety of linguistic morphological and syntactic and non linguistic feature to automatically identify these error our best approach achieves a roughly absolute increase in f score over a simple but reasonable baseline a detailed error analysis show that linguistic feature such a lemma i e citation form model help improve hr error detection precisely where we expect them to semantically incoherent error word 
we investigate the potential of tree substitution grammar a a source of feature for native language detection the task of inferring an author s native language from text in a different language we compare two state of the art method for tree substitution grammar induction and show that feature from both method outperform previous state of the art result at native language detection furthermore we contrast these two induction algorithm and show that the bayesian approach produce superior classification result with a smaller feature set 
since we can spin word and concept to suit our affective need context is a major determinant of the perceived affect of a word or concept we view this re profiling a a selective emphasis or de emphasis of the quality that underpin our shared stereotype of a concept or a word meaning and construct our model of the affective lexicon accordingly we show how a large body of affective stereotype can be acquired from the web and also show how these are used to create and interpret affective metaphor 
in this paper we consider the problem of unsupervised morphological analysis from a new angle past work ha endeavored to design unsupervised learning method which explicitly or implicitly encode inductive bias appropriate to the task at hand we propose instead to treat morphological analysis a a structured prediction problem where language with labeled data serve a training example for unlabeled language without the assumption of parallel data we define a universal morphological feature space in which every language and it morphological analysis reside we develop a novel structured nearest neighbor prediction method which seek to find the morphological analysis for each unlabeled language which lie a close a possible in the feature space to a training language we apply our model to eight inflecting language and induce nominal morphology with substantially higher accuracy than a traditional mdl based approach our analysis indicates that accuracy continues to improve substantially a the number of training language increase 
from the perspective of structural linguistics we explore paradigmatic and syntagmatic lexical relation for chinese po tagging an important and challenging task for chinese language processing paradigmatic lexical relation are explicitly captured by word clustering on large scale unlabeled data and are used to design new feature to enhance a discriminative tagger syntagmatic lexical relation are implicitly captured by constituent parsing and are utilized via system combination experiment on the penn chinese treebank demonstrate the importance of both paradigmatic and syntagmatic relation our linguistically motivated approach yield a relative error reduction of in total over a state of the art baseline 
machine produced text often lack grammaticality and fluency this paper study grammaticality improvement using a syntax based algorithm based on ccg the goal of the search problem is to find an optimal parse tree among all that can be constructed through selection and ordering of the input word the search problem which is significantly harder than parsing is solved by guided learning for best first search in a standard word ordering task our system give a bleu score of higher than the previous result of achieved by a dependency based system 
due to arabic s morphological complexity arabic retrieval benefit greatly from morphological analysis particularly stemming however the best known stemming doe not handle linguistic phenomenon such a broken plural and malformed stem in this paper we propose a model of character level morphological transformation that is trained using wikipedia hypertext to page title link the use of our model yield statistically significant improvement in arabic retrieval over the use of the best statistical stemming technique the technique can potentially be applied to other language 
disambiguating concept and entity in a context sensitive way is a fundamental problem in natural language processing the comprehensiveness of wikipedia ha made the online encyclopedia an increasingly popular target for disambiguation disambiguation to wikipedia is similar to a traditional word sense disambiguation task but distinct in that the wikipedia link structure provides additional information about which disambiguation are compatible in this work we analyze approach that utilize this information to arrive at coherent set of disambiguation for a given document which we call global approach and compare them to more traditional local approach we show that previous approach for global disambiguation can be improved but even then the local disambiguation provides a baseline which is very hard to beat 
we consider a semi supervised setting for domain adaptation where only unlabeled data is available for the target domain one way to tackle this problem is to train a generative model with latent variable on the mixture of data from the source and target domain such a model would cluster feature in both domain and ensure that at least some of the latent variable are predictive of the label on the source domain the danger is that these predictive cluster will consist of feature specific to the source domain only and consequently a classifier relying on such cluster would perform badly on the target domain we introduce a constraint enforcing that marginal distribution of each cluster i e each latent variable do not vary significantly across domain we show that this constraint is effective on the sentiment classification task pang et al resulting in score similar to the one obtained by the structural correspondence method blitzer et al without the need to engineer auxiliary task 
linking entity with knowledge base entity linking is a key issue in bridging the textual data with the structural knowledge base due to the name variation problem and the name ambiguity problem the entity linking decision are critically depending on the heterogenous knowledge of entity in this paper we propose a generative probabilistic model called entity mention model which can leverage heterogenous entity knowledge including popularity knowledge name knowledge and context knowledge for the entity linking task in our model each name mention to be linked is modeled a a sample generated through a three step generative story and the entity knowledge is encoded in the distribution of entity in document p e the distribution of possible name of a specific entity p s e and the distribution of possible context of a specific entity p c e to find the referent entity of a name mention our method combine the evidence from all the three distribution p e p s e and p c e experimental result show that our method can significantly outperform the traditional method 
in this paper we present a novel approach which incorporates the web derived selectional preference to improve statistical dependency parsing conventional selectional preference learning method have usually focused on word to class relation e g a verb selects a it subject a given nominal class this paper extends previous work to word to word selectional preference by using web scale data experiment show that web scale data improves statistical dependency parsing particularly for long dependency relationship there is no data like more data performance improves log linearly with the number of parameter unique n gram more importantly when operating on new domain we show that using web derived selectional preference is essential for achieving robust performance 
unrehearsed spoken language often contains disfluency in order to correctly interpret a spoken utterance any such disfluency must be identified and removed or otherwise dealt with operating on transcript of speech which contain disfluency we study the effect of language model and loss function on the performance of a linear reranker that rescores the best output of a noisy channel model we show that language model trained on large amount of non speech data improve performance more than a language model trained on a more modest amount of speech data and that optimising f score rather than log loss improves disfluency detection performance our approach us a log linear reranker operating on the top n analysis of a noisy channel model we use large language model introduce new feature into this reranker and examine different optimisation strategy we obtain a disfluency detection f score of which improves upon the current state of the art 
this paper present a comparative study of spelling error that are corrected a you type v those that remain uncorrected first we generate naturally occurring online error correction data by logging user keystroke and by automatically deriving preand post correction string from them we then perform an analysis of this data against the error that remain in the final text a well a across language our analysis show a clear distinction between the type of error that are generated and those that remain uncorrected a well a across language 
although researcher have conducted extensive study on relation extraction in the last decade supervised approach are still limited because they require large amount of training data to achieve high performance to build a relation extractor without significant annotation effort we can exploit cross lingual annotation projection which leverage parallel corpus a external resource for supervision this paper proposes a novel graph based projection approach and demonstrates the merit of it by using a korean relation extraction system based on projected dataset from an english korean parallel corpus 
previous approach to instruction interpretation have required either extensive domain adaptation or manually annotated corpus this paper present a novel approach to instruction interpretation that leverage a large amount of unannotated easy to collect data from human interacting with a virtual world we compare several algorithm for automatically segmenting and discretizing this data into utterance reaction pair and training a classifier to predict reaction given the next utterance our empirical analysis show that the best algorithm achieves accuracy on this task with no manual annotation required 
we use search engine result to address a particularly difficult cross domain language processing task the adaptation of named entity recognition ner from news text to web query the key novelty of the method is that we submit a token with context to a search engine and use similar context in the search result a additional information for correctly classifying the token we achieve strong gain in ner performance on news in domain and out of domain and on web query 
social medium language contains huge amount and wide variety of nonstandard token created both intentionally and unintentionally by the user it is of crucial importance to normalize the noisy nonstandard token before applying other nlp technique a major challenge facing this task is the system coverage i e for any user created nonstandard term the system should be able to restore the correct word within it top n output candidate in this paper we propose a cognitively driven normalization system that integrates different human perspective in normalizing the nonstandard token including the enhanced letter transformation visual priming and string phonetic similarity the system wa evaluated on both wordand message level using four sm and twitter data set result show that our system achieves over word coverage across all data set a absolute increase compared to state of the art the broad word coverage can also successfully translate into message level performance gain yielding absolute increase compared to the best prior approach 
statistical machine translation system are usually trained on a large amount of bilingual sentence pair and translate one sentence at a time ignoring document level information in this paper we propose a cache based approach to document level translation since cache mainly depend on relevant data to supervise subsequent decision it is critical to fill the cache with highly relevant data of a reasonable size in this paper we present three kind of cache to store relevant document level information a dynamic cache which store bilingual phrase pair from the best translation hypothesis of previous sentence in the test document a static cache which store relevant bilingual phrase pair extracted from similar bilingual document pair i e source document similar to the test document and their corresponding target document in the training parallel corpus a topic cache which store the target side topic word related with the test document in the source side in particular three new feature are designed to explore various kind of document level information in above three kind of cache evaluation show the effectiveness of our cache based approach to document level translation with the performance improvement of in blue score over moses especially detailed analysis and discussion are presented to give new insight to document level translation 
we propose a general method to watermark and probabilistically identify the structured output of machine learning algorithm our method is robust to local editing operation and provides well defined trade offs between the ability to identify algorithm output and the quality of the watermarked output unlike previous work in the field our approach doe not rely on controlling the input to the algorithm and provides probabilistic guarantee on the ability to identify collection of result from one s own algorithm we present an application in statistical machine translation where machine translated output is watermarked at minimal loss in translation quality and detected with high recall 
learning entailment rule is fundamental in many semantic inference application and ha been an active field of research in recent year in this paper we address the problem of learning transitive graph that describe entailment rule between predicate termed entailment graph we first identify that entailment graph exhibit a tree like property and are very similar to a novel type of graph termed forest reducible graph we utilize this property to develop an iterative efficient approximation algorithm for learning the graph edge where each iteration take linear time we compare our approximation algorithm to a recently proposed state of the art exact algorithm and show that it is more efficient and scalable both theoretically and empirically while it output quality is close to that given by the optimal solution of the exact algorithm 
many semantic parsing model use tree transformation to map between natural language and meaning representation however while tree transformation are central to several state of the art approach little use ha been made of the rich literature on tree automaton this paper make the connection concrete with a tree transducer based semantic parsing model and suggests that other model can be interpreted in a similar framework increasing the generality of their contribution in particular this paper further introduces a variational bayesian inference algorithm that is applicable to a wide class of tree transducer producing state of the art semantic parsing result while remaining applicable to any domain employing probabilistic tree transducer 
in this paper with a belief that a language model that embrace a larger context provides better prediction ability we present two extension to standard n gram language model in statistical machine translation a backward language model that augments the conventional forward language model and a mutual information trigger model which capture long distance dependency that go beyond the scope of standard n gram language model we integrate the two proposed model into phrase based statistical machine translation and conduct experiment on large scale training data to investigate their effectiveness our experimental result show that both model are able to significantly improve translation quality and collectively achieve up to bleu point over a competitive baseline 
we describe a generative model for non projective dependency parsing based on a simplified version of a transition system that ha recently appeared in the literature we then develop a dynamic programming parsing algorithm for our model and derive an inside outside algorithm that can be used for unsupervised learning of non projective dependency tree 
this paper present a pilot study of opinion summarization on conversation we create a corpus containing extractive and abstractive summary of speaker s opinion towards a given topic using telephone conversation we adopt two method to perform extractive summarization the first one is a sentence ranking method that linearly combine score measured from different aspect including topic relevance subjectivity and sentence importance the second one is a graph based method which incorporates topic and sentiment information a well a additional information about sentence to sentence relation extracted based on dialogue structure our evaluation result show that both method significantly outperform the baseline approach that extract the longest utterance in particular we find that incorporating dialogue structure in the graph based method contributes to the improved system performance 
we present disputant relation based method for classifying news article on contentious issue we observe that the disputant of a contention are an important feature for understanding the discourse it performs unsupervised classification on news article based on disputant relation and help reader intuitively view the article through the opponent based frame the reader can attain balanced understanding on the contention free from a specific biased view we applied a modified version of hit algorithm and an svm classifier trained with pseudo relevant data for article analysis 
in this paper we propose a novel approach to automatic generation of aspect oriented summary from multiple document we first develop an event aspect lda model to cluster sentence into aspect we then use extended lexrank algorithm to rank the sentence in each cluster we use integer linear programming for sentence selection key feature of our method include automatic grouping of semantically related sentence and sentence ranking based on extension of random walk model also we implement a new sentence compression algorithm which use dependency tree instead of parser tree we compare our method with four baseline method quantitative evaluation based on rouge metric demonstrates the effectiveness and advantage of our method 
in this paper we present an unsupervized segmentation system tested on mandarin chinese following harris s hypothesis in kempe and tanaka ishii s reformulation we base our work on the variation of branching entropy we improve on jin and tanaka ishii by adding normalization and viterbi decoding this enable u to remove most of the threshold and parameter from their model and to reach near state of the art result wang et al with a simpler system we provide evaluation on different corpus available from the segmentation bake off ii emerson and define a more precise topline for the task using cross trained supervized system available off the shelf zhang and clark zhao and kit huang and zhao 
community based question answer q a ha become an important issue due to the popularity of q a archive on the web this paper is concerned with the problem of question retrieval question retrieval in q a archive aim to find historical question that are semantically equivalent or relevant to the queried question in this paper we propose a novel phrase based translation model for question retrieval compared to the traditional word based translation model the phrase based translation model is more effective because it capture contextual information in modeling the translation of phrase a a whole rather than translating single word in isolation experiment conducted on real q a data demonstrate that our proposed phrase based translation model significantly outperforms the state of the art word based translation model 
we present llccm a log linear variant of the constituent context model ccm of grammar induction llccm retains the simplicity of the original ccm but extends robustly to long sentence on sentence of up to length llccm outperforms ccm by bracketing f and outperforms a right branching baseline in regime where ccm doe not 
we investigate how novel english derived word anglicism are used in a german language internet hip hop forum and what factor contribute to their uptake 
count from large corpus like the web can be powerful syntactic cue past work ha used web count to help resolve isolated ambiguity such a binary noun verb pp attachment and noun compound bracketings in this work we first present a method for generating web count feature that address the full range of syntactic attachment these feature encode both surface evidence of lexical affinity a well a paraphrase based cue to syntactic structure we then integrate our feature into full scale dependency and constituent parser we show relative error reduction of over the second order dependency parser of mcdonald and pereira over the constituent parser of petrov et al and over a non local constituent reranker 
extracting biomedical event from literature ha attracted much recent attention the best performing system so far have been pipeline of simple subtask specific local classifier a natural drawback of such approach are cascading error introduced in early stage of the pipeline we present three joint model of increasing complexity designed to overcome this problem the first model performs joint trigger and argument extraction and lends itself to a simple efficient and exact inference algorithm the second model capture correlation between event while the third model ensures consistency between argument of the same event inference in these model is kept tractable through dual decomposition the first two model outperform the previous best joint approach and are very competitive with respect to the current state of the art the third model yield the best result reported so far on the bionlp shared task the bionlp genia task and the bionlp infectious disease task 
twitter provides access to large volume of data in real time but is notoriously noisy hampering it utility for nlp in this paper we target out of vocabulary word in short text message and propose a method for identifying and normalising ill formed word our method us a classifier to detect ill formed word and generates correction candidate based on morphophonemic similarity both word similarity and context are then exploited to select the most probable correction candidate for the word the proposed method doesn t require any annotation and achieves state of the art performance over an sm corpus and a novel dataset based on twitter 
this paper present the problem within hittite and ancient near eastern study of fragmented and damaged cuneiform text and proposes to use well known text classification metric in combination with some fact about the structure of hittite language cuneiform text to help classify a number of fragment of clay cuneiform script tablet into more complete text in particular i propose using sumerian and akkadian ideogrammatic sign within hittite text to improve the performance of naive bayes and maximum entropy classifier the performance in some case is improved and in some case very much not suggesting that the variable frequency of occurrence of these ideogram in individual fragment make considerable difference in the ideal choice for a classification method further complexity of the writing system and the digital availability of hittite text complicate the problem 
we investigate the problem of ordering medical event in unstructured clinical narrative by learning to rank them based on their time of occurrence we represent each medical event a a time duration with a corresponding start and stop and learn to rank the start stop based on their proximity to the admission date such a representation allows u to learn all of allen s temporal relation between medical event interestingly we observe that this methodology performs better than a classification based approach for this domain but worse on the relationship found in the timebank corpus this finding ha important implication for style of data representation and resource used for temporal relation learning clinical narrative may have different language attribute corresponding to temporal ordering relative to timebank implying that the field may need to look at a wider range of domain to fully understand the nature of temporal ordering 
extensive knowledge base of entailment rule between predicate are crucial for applied semantic inference in this paper we propose an algorithm that utilizes transitivity constraint to learn a globally optimal set of entailment rule for typed predicate we model the task a a graph learning problem and suggest method that scale the algorithm to larger graph we apply the algorithm over a large data set of extracted predicate instance from which a resource of typed entailment rule ha been recently released schoenmackers et al our result show that using global transitivity information substantially improves performance over this resource and several baseline and that our scaling method allow u to increase the scope of global learning of entailment rule graph 
we show for both english po tagging and chinese word segmentation that with proper representation large number of deterministic constraint can be learned from training example and these are useful in constraining probabilistic inference for tagging learned constraint are directly used to constrain viterbi decoding for segmentation character based tagging constraint can be learned with the same template however they are better applied to a word based model thus an integer linear programming ilp formulation is proposed for both problem the corresponding constrained solution have advantage in both efficiency and accuracy 
this paper present a novel way of improving po tagging on heterogeneous data first two separate model are trained generalized and domain specific from the same data set by controlling lexical item with different document frequency during decoding one of the model is selected dynamically given the cosine similarity between each sentence and the training data this dynamic model selection approach coupled with a one pas left to right po tagging algorithm is evaluated on corpus from seven different genre even with this simple tagging algorithm our system show comparable result against other state of the art system and give higher accuracy when evaluated on a mixture of the data furthermore our system is able to tag about k token per second we believe that this model selection approach can be applied to more sophisticated tagging algorithm and improve their robustness even further 
we describe a novel approach for inducing unsupervised part of speech tagger for language that have no labeled training data but have translated text in a resource rich language our method doe not assume any knowledge about the target language in particular no tagging dictionary is assumed making it applicable to a wide array of resource poor language we use graph based label propagation for cross lingual knowledge transfer and use the projected label a feature in an unsupervised model berg kirkpatrick et al across eight european language our approach result in an average absolute improvement of over a state of the art baseline and over vanilla hidden markov model induced with the expectation maximization algorithm 
it is well known that parsing accuracy suffers when a model is applied to out of domain data it is also known that the most beneficial data to parse a given domain is data that match the domain sekine gildea hence an important task is to select appropriate domain however most previous work on domain adaptation relied on the implicit assumption that domain are somehow given a more and more data becomes available automatic way to select data that is beneficial for a new unknown target domain are becoming attractive this paper evaluates various way to automatically acquire related training data for a given test set the result show that an unsupervised technique based on topic model is effective it outperforms random data selection on both language examined english and dutch moreover the technique work better than manually assigned label gathered from meta data that is available for english 
we present a systematic comparison and combination of two orthogonal technique for efficient parsing of combinatory categorial grammar ccg first we consider adaptive supertagging a widely used approximate search technique that prune most lexical category from the parser s search space using a separate sequence model next we consider several variant on a a classic exact search technique which to our knowledge ha not been applied to more expressive grammar formalism like ccg in addition to standard hardware independent measure of parser effort we also present what we believe is the first evaluation of a parsing on the more realistic but more stringent metric of cpu time by itself a substantially reduces parser effort a measured by the number of edge considered during parsing but we show that for ccg this doe not always correspond to improvement in cpu time over a cky baseline combining a with adaptive supertagging decrease cpu time by for our best model 
we present a simple semi supervised relation extraction system with large scale word clustering we focus on systematically exploring the effectiveness of different cluster based feature we also propose several statistical method for selecting cluster at an appropriate level of granularity when training on different size of data our semi supervised approach consistently outperformed a state of the art supervised baseline system 
in this work we introduce the tesla celab metric translation evaluation of sentence with linear programming based analysis character level evaluation for language with ambiguous word boundary for automatic machine translation evaluation for language such a chinese where word usually have meaningful internal structure and word boundary are often fuzzy tesla celab acknowledges the advantage of character level evaluation over word level evaluation by reformulating the problem in the linear programming framework tesla celab address several drawback of the character level metric in particular the modeling of synonym spanning multiple character we show empirically that tesla celab significantly outperforms character level bleu in the english chinese translation evaluation task 
optimising for one grammatical representation but evaluating over a different one is a particular challenge for parser and n best ccg parsing we find that this mismatch cause many n best ccg par to be semantically equivalent and describe a hashing technique that eliminates this problem improving oracle n best f score by and reranking accuracy by we also present a comprehensive analysis of error made by the c c ccg parser providing the first breakdown of the impact of implementation decision such a supertagging on parsing accuracy 
we present a first known result of high precision rare word bilingual extraction from comparable corpus using aligned comparable document and supervised classification we incorporate two feature a context vector similarity and a co occurrence model between word in aligned document in a machine learning approach we test our hypothesis on different pair of language and corpus we obtain very high f measure between and for recognizing and extracting correct translation for rare term from to occurrence moreover we show that our system can be trained on a pair of language and test on a different pair of language obtaining a f measure of for the classification of chinese english translation using a training corpus of spanish french our method is therefore even potentially applicable to low resource language without training data 
this paper explores the use of bilingual parallel corpus a a source of lexical knowledge for cross lingual textual entailment we claim that in spite of the inherent difficulty of the task phrase table extracted from parallel data allow to capture both lexical relation between single word and contextual information useful for inference we experiment with a phrasal matching method in order to i build a system portable across language and ii evaluate the contribution of lexical knowledge in isolation without interaction with other inference mechanism result achieved on an english spanish corpus obtained from the rte dataset support our claim with an overall accuracy above average score reported by rte participant on monolingual data finally we show that using parallel corpus to extract paraphrase table reveals their potential also in the monolingual setting improving the result achieved with other source of lexical knowledge 
we present a novel algorithm for multilingual dependency parsing that us annotation from a diverse set of source language to parse a new unannotated language our motivation is to broaden the advantage of multilingual learning to language that exhibit significant difference from existing resource rich language the algorithm learns which aspect of the source language are relevant for the target language and tie model parameter accordingly the model factorizes the process of generating a dependency tree into two step selection of syntactic dependent and their ordering being largely language universal the selection component is learned in a supervised fashion from all the training language in contrast the ordering decision are only influenced by language with similar property we systematically model this cross lingual sharing using typological feature in our experiment the model consistently outperforms a state of the art multi lingual parser the largest improvement is achieved on the non indo european language yielding a gain of 
in this paper we describe a method for simplifying sentence using phrase based machine translation augmented with a re ranking heuristic based on dissimilarity and trained on a monolingual parallel corpus we compare our system to a word substitution baseline and two state of the art system all trained and tested on paired sentence from the english part of wikipedia and simple wikipedia human test subject judge the output of the different system analysing the judgement show that by relatively careful phrase based paraphrasing our model achieves similar simplification result to state of the art system while generating better formed output we also argue that text readability metric such a the flesch kincaid grade level should be used with caution when evaluating the output of simplification system 
the problem addressed in this paper is to segment a given multilingual document into segment for each language and then identify the language of each segment the problem wa motivated by an attempt to collect a large amount of linguistic data for non major language from the web the problem is formulated in term of obtaining the minimum description length of a text and the proposed solution find the segment and their language through dynamic programming empirical result demonstrating the potential of this approach are presented for experiment using text taken from the universal declaration of human right and wikipedia covering more than language 
this paper present a novel sequence labeling model based on the latent variable semi markov conditional random field for jointly extracting argument role of event from text the model take in coarse mention and type information and predicts argument role for a given event template this paper address the event extraction problem in a primarily unsupervised setting where no labeled training instance are available our key contribution is a novel learning framework called structured preference modeling pm that allows arbitrary preference to be assigned to certain structure during the learning procedure we establish and discus connection between this framework and other existing work we show empirically that the structured preference are crucial to the success of our task our model trained without annotated data and with a small number of structured preference yield performance competitive to some baseline supervised approach 
the state of the art system combination method for machine translation mt is based on confusion network constructed by aligning hypothesis with regard to word similarity we introduce a novel system combination framework in which hypothesis are encoded a a confusion forest a packed forest representing alternative tree the forest is generated using syntactic consensus among parsed hypothesis first mt output are parsed second a context free grammar is learned by extracting a set of rule that constitute the parse tree third a packed forest is generated starting from the root symbol of the extracted grammar through non terminal rewriting the new hypothesis is produced by searching the best derivation in the forest experimental result on the wmt system combination shared task yield comparable performance to the conventional confusion network based method with smaller space 
