frequent itemset mining ha been the subject of a lot of work in data mining research ever since association rule were introduced in this paper we address a problem with frequent itemsets that they only count row where all their attribute are present and do not allow for any noise we show that generalizing the concept of frequency while preserving the performance of mining algorithm is nontrivial and introduce a generalization of frequent itemsets dense itemsets dense itemsets do not require all attribute to be present at the same time instead the itemset need to define a sufficiently large submatrix that exceeds a given density threshold of attribute present we consider the problem of computing all dense itemsets in a database we give a levelwise algorithm for this problem and also study the top k variation i e finding the k densest set with a given support or the k best supported set with a given density these algorithm select the other parameter automatically which simplifies mining dense itemsets in an explorative way we show that the concept capture natural facet of data set and give extensive empirical result on the performance of the algorithm combining the concept of dense itemsets with set cover idea we also show that dense itemsets can be used to obtain succinct description of large datasets we also discus some variation of dense itemsets 
a sensible use of classifier must be based on the estimated reliability of their prediction a cautious classifier would delegate the difficult or uncertain prediction to other possibly more specialised classifier in this paper we analyse and develop this idea of delegating classifier in a systematic way first we design a two step scenario where a first classifier chooses which example to classify and delegate the difficult example to train a second classifier secondly we present an iterated scenario involving an arbitrary number of chained classifier we compare these scenario to classical ensemble method such a bagging and boosting we show experimentally that our approach is not far behind these method in term of accuracy but with several advantage i improved efficiency since each classifier learns from fewer example than the previous one ii improved comprehensibility since each classification derives from a single classifier and iii the possibility to simplify the overall multi classifier by removing the part that lead to delegation 
essentially all data mining algorithm assume that the datagenerating process is independent of the data miner s activity however in many domain including spam detection intrusion detection fraud detection surveillance and counter terrorism this is far from the case the data is actively manipulated by an adversary seeking to make the classier produce false negative in these domain the performance of a classier can degrade rapidly after it is deployed a the adversary learns to defeat it currently the only solution to this is repeated manual ad hoc reconstruction of the classier in this paper we develop a formal framework and algorithm for this problem we view classication a a game between the classier and the adversary and produce a classier that is optimal given the adversary s optimal strategy experiment in a spam detection domain show that this approach can greatly outperform a classier learned in the standard way and within the parameter of the problem automatically adapt the classier to the adversary s evolving manipulation 
we present a novel method for approximate inference in bayesian model and regularized risk functionals it is based on the propagation of mean and variance derived from the laplace approximation of conditional probability in factorizing distribution much akin to minka s expectation propagation in the jointly normal case it coincides with the latter and belief propagation whereas in the general case it provides an optimization strategy containing support vector chunking the bayes committee machine and gaussian process chunking a special case 
spectral method for nonlinear dimensionality reduction nldr impose a neighborhood graph on point data and compute eigenfunctions of a quadratic form generated from the graph we introduce a more general and more robust formulation of nldr based on the singular value decomposition svd in this framework most spectral nldr principle can be recovered by taking a subset of the constraint in a quadratic form built from local nullspaces on the manifold the minimax formulation also open up an interesting class of method in which the graph is decorated with information at the vertex offering discrete or continuous map reduced computational complexity and immunity to some solution instability of eigenfunction approach apropos we show almost all nldr method based on eigenvalue decomposition evd have a solution instability that increase faster than problem size this pathology can be observed and corrected via the minimax formulation in problem a small a n point 
we propose a gossip based distributed algorithm for gaussian mixture learning newscast em the algorithm operates on network topology where each node observes a local quantity and can communicate with other node in an arbitrary point to point fashion the mai n difference between newscast em and the standard em algorithm is that the m step in our case is implemented in a decentralized manner random pair of node repeatedly exchange their local parameter estimat e and combine them by weighted averaging we provide theoretical evidence and demonstrate experimentally that under this protocol nod e converge exponentially fast to the correct estimate in each m step of t he em algorithm 
low rank approximation technique are widespread in pattern recognition research they include latent semantic analysis lsa probabilistic lsa principal component analysus pca the generative aspect model and many form of bibliometric analysis all make use of a low dimensional manifold onto which data are projected such technique are generally unsupervised which allows them to model data in the absence of label or category with many practical problem however some prior knowledge is available in the form of context in this paper i describe a principled approach to incorporating such information and demonstrate it application to pca based approximation of several data set 
rao blackwellization is an approximation technique for probabilistic inference that flexibly combine exact inference with sampling it is useful in model where conditioning on some of the variable leaf a simpler inference problem that can be solved tractably this paper present sample propagation an efficient implementation of rao blackwellized approximate inference for a large class of model sample propagation tightly integrates sampling with message passing in a junction tree and is named for it simple appealing structure it walk the cluster of a junction tree sampling some of the current cluster s variable and then passing a message to one of it neighbor we discus the application of sample propagation to conditional gaussian inference problem such a switching linear dynamical system 
a cocluster of a m x n matrix x is a submatrix determined by a subset of the row and a subset of the column the problem of finding coclusters with specific property is of interest in particular in the analysis of microarray experiment in that case the entry of the matrix x are the expression level of m gene in each of n tissue sample one goal of the analysis is to extract a subset of the sample and a subset of the gene such that the expression level of the chosen gene behave similarly across the subset of the sample presumably reflecting an underlying regulatory mechanism governing the expression level of the gene we propose to base the similarity of the gene in a cocluster on a simple biological model in which the strength of the regulatory mechanism in sample j is hj and the response strength of gene i to the regulatory mechanism is gi in other word every two gene participating in a good cocluster should have expression value in each of the participating sample whose ratio is a constant depending only on the two gene noise in the expression level of gene is taken into account by allowing a deviation from the model measured by a relative error criterion the sleeve width of the cocluster reflects the extent to which entry i j in the cocluster is allowed to deviate relatively from being expressed a the product gihj we present a polynomial time monte carlo algorithm which output a list of coclusters whose sleeve width do not exceed a prespecified value moreover we prove that the list includes with fixed probability a cocluster which is near optimal in it dimension extensive experimentation with synthetic data show that the algorithm performs well 
we introduce a general family of kernel based on weighted transducer or rational relation rational kernel that can be used for analysis of variable length sequence or more generally weighted automaton in application such a computational biology or speech recognition we show that rational kernel can be computed efficiently using a general algorithm of composition of weighted transducer and a general single source shortest distance algorithm we also describe several general family of positive definite symmetric rational kernel these general kernel can be combined with support vector machine to form efficient and powerful technique for spoken dialog classification highly complex kernel become easy to design and implement and lead to substantial improvement in the classification accuracy we also show that the string kernel considered in application to computational biology are all specific instance of rational kernel 
we present a trainable sequential inference technique for process with large state and observation space and relational structure our method assumes reliable observation i e that each process state persists long enough to be reliably inferred from the observation it generates we introduce the idea of a state inference function from observation sequence to underlying hidden state for representing knowledge about a process and develop an efficient sequential inference algorithm utilizing this function that is correct for process that generate reliable observation consistent with the state inference function we describe a representation for state inference function in relational domain and give a corresponding supervised learning algorithm experiment in relational video interpretation show that our technique provides significantly improved accuracy and speed relative to a variety of recent hand coded non trainable system 
we investigate bayesian alternative to classical monte carlo method for evaluating integral bayesian monte carlo bmc allows the incorporation of prior knowledge such a smoothness of the integrand into the estimation in a simple problem we show that this outperforms any classical importance sampling method we also attempt more challenging multidimensional integral involved in computing marginal likelihood of statistical model a k a partition function and model evidence we find that bayesian monte carlo outperformed anne aled importance sampling although for very high dimensional problem or problem with massive multimodality bmc may be le adequate one advantage of the bayesian approach to monte carlo is that sample can be drawn from any distribution this allows for the possibil ity of active design of sample point so a to maximise information gain 
several author have suggested viewing boosting a a gradient descent search for a good fit in function space we apply gradient based boostin g methodology to the unsupervised learning problem of density estimation we show convergence property of the algorithm and prove that a strength of weak learnability property applies to this problem a well we illustrate the poten tial of this approach through experiment with boosting bayesian network to learn density model 
we propose a family of kernel based on the binet cauchy theorem and it extension to fredholm operator this includes a special case all currently known kernel derived from the behavioral framework diffusion process marginalized kernel kernel on graph and the kernel on set arising from the subspace angle approach many of these kernel can be seen a the extremum of a new continuum of kernel function which lead to numerous new special case a an application we apply the new class of kernel to the problem of clustering of video sequence with encouraging result 
motivated by the interest in relational reinforcement learning we introduce a novel relational bellman update operator called rebel it employ a constraint logic programming language to compactly represent markov decision process over relational domain using rebel a novel value iteration algorithm is developed in which abstraction over state and action play a major role this framework provides new insight into relational reinforcement learning convergence result a well a experiment are presented 
we construct a nonlinear mapping from a high dimensional sample space to a low dimensional vector space effectively recovering a cartesian coordinate system for the manifold from which the data is sampled the mapping preserve local geometric relation in the manifold and is pseudo invertible we show how to estimate the intrinsic dimensionality of the manifold from sample decompose the sample data into locally linear low dimensional patch merge these patch into a single lowdimensional coordinate system and compute forward and reverse mapping between the sample and coordinate space the objective function are convex and their solution are given in closed form nonlinear dimensionality reduction nldr by charting charting is the problem of assigning a low dimensional coordinate system to data point in a high dimensional sample space it is presumed that the data lie on or near a lowdimensional manifold embedded in the sample space and that there exists a to smooth nonlinear transform between the manifold and a low dimensional vector space the datamodeler s goal is to estimate smooth continuous mapping between the sample and coordinate space often this analysis will shed light on the intrinsic variable of the datagenerating phenomenon for example revealing perceptual or configuration space our goal is to find a mapping expressed a a kernel based mixture of linear projection that minimizes information loss about the density and relative location of sample point this constraint is expressed in a posterior that combine a standard gaussian mixture model gmm likelihood function with a prior that penalizes uncertainty due to inconsistent projection in the mixture section develops a special case where this posterior is unimodal and maximizable in closed form yielding a gmm whose covariance reveal a patchwork of overlapping locally linear subspace that cover the manifold section show that for this or any gmm and a choice of reduced dimension d there is a unique closed form solution for a minimally distorting merger of the subspace into a d dimensional coordinate space a well a an reverse mapping defining the surface of the manifold in the sample space the intrinsic dimensionality d of the data manifold can be estimated from the growth process of point to point distance in analogy to differential geometry we call the subspace chart and their merger the connection section considers example problem where these method are used to untie knot unroll and untwist sheet and visualize video data background topology neutral nldr algorithm can be divided into those that compute mapping and 
we investigate improvement of adaboost that can exploit the fact that the weak hypothesis are one sided i e either all it positive or negative prediction are correct in particular for any set of m labeled example consistent with a disjunction of k literal which are one sided in this case adaboost construct a consistent hypothesis by using o k log m iteration on the other hand a greedy set covering algorithm find a consistent hypothesis of size o k log m our primary question is whether there is a simple boosting algorithm that performs a well a the greedy set covering we first show that infoboost a modification of adaboost proposed by aslam for a different purpose doe perform a well a the greedy set covering algorithm we then show that adaboost requires k log m iteration for learning k literal disjunction we achieve this with an adversary construction and a well a in simple experiment based on artificial data further we give a variant called semiboost that can handle the degenerate case when the given example all have the same label we conclude by showing that semiboost can be used to produce small conjunction a well 
abstract recently there have been several advance in the machine learning and pattern recognition community for developing manifold learning algo rithms to construct nonlinear low dimensional manifold from sample data point embedded in high dimensional space in this paper we de velop algorithm that address two key issue in manifold learning the adaptive selection of the neighborhood size and better fitting the local geometric structure to account for the variation in the curvature of the manifold and it interplay with the sampling density of the data set we also illustrate the effectiveness of our method on some synthetic data set 
abstract we consider the learning problem ofnding a dependency betweena general class of object and another possibly dierent generalclass of object the object can be for example vector image string tree or graph such a task is made possible by employingsimilarity measure in both input and output space using kernelfunctions thus embedding the object into vector space weexperimentally validate our approach on several task mappingstrings to string pattern 
linear model allow the description of a continuous symmetric response in term of a linear combination of predictor variable generalized linear model extend this framework to a wider range of response type including categorical binary and skewed continuous response this allows a common approach to statistical inference in the fitting and testing of such model a well a diagnostic checking 
a new family of kernel for statistical learning is introduc ed that exploit the geometric structure of statistical model base d on the heat equation on the riemannian manifold defined by the fisher inf ormation metric information diffusion kernel generalize the gaussian kernel of euclidean space and provide a natural way of combining generative statistical modeling with non parametric discriminative learning a a special case the kernel give a new approach to applying kernel based learning algorithm to discrete data bound on covering number for the new kernel are proved using spectral theory in differen tial geometry and experimental result are presented for text classificat ion 
we introduce a probabilistic model that generalizesclassical linear discriminant analysisand give an interpretation for the componentsas informative or relevant componentsof data the component maximize thepredictability of class distribution which isasymptotically equivalent to i maximizingmutual information with the class and ii nding principal component in the so calledlearning or fisher metric the fisher metricmeasures only distance that are relevantto the 
in this paper we consider tipping s relevance vector machi ne rvm and formalize an incremental training strategy a a vari ant of the expectation maximization em algorithm that we call subspace em working with a subset of active basis function the sparsit y of the rvm solution will ensure that the number of basis function and t hereby the computational complexity is kept low we also introduce a mean field approach to the intractable classification model that is exp ected to give a very good approximation to exact bayesian inference and contains the laplace approximation a a special case we test the algorithm on two large data set with example the result indicate that bayesian learning of large data set e g the mnist database is realistic 
we consider loopy belief propagation for approximate inference in probabilistic graphical model a limitation of the standard algorithm is that clique marginals are computed a if there were no loop in the graph to overcome this limitation we introduce fractional belief propagation fractional belief propagation is formulated in term of a family of approximate free energy which includes the bethe free energy and the naive mean field free a special case using the linear response correction of the clique marginals the scale parameter can be tuned simulation result illustrate the potential merit of the approach 
high dimensional collection of data occur in many application the attribute in such data set are typically considered to be unordered however in many case there is a natural total or partial order pr underlying the variable of the data set example of variable for which such order exist include term in document course in enrollment data and paleontological site in fossil data collection the observation in such application are flat unordered set however the data set respect the underlying ordering of the variable by this we mean that if a pr b pr c are three variable respecting the underlying ordering pr and both of variable a and c appear in an observation then up to noise level variable b also appears in this observation similarly if a pr a pr pr al pr ai is a longer sequence of variable we do not expect to see many observation for which there are index i j k such that ai and ak occur in the observation but aj doe not in this paper we study the problem of discovering fragment of order of variable implicit in collection of unordered observation we define measure that capture how well a given order agrees with the observed data we describe a simple and efficient algorithm for finding all the fragment that satisfy certain condition we also discus the sometimes necessary postprocessing for selecting only the best fragment of order also we relate our method with a sequencing approach that us a spectral algorithm and with the consecutive one problem we present experimental result on some real data set author list of database paper exam result data and paleontological data 
data cleaning method are used for finding duplicate within a file or across set of file this overview provides background on the fellegi sunter model of record linkage the fellegi sunter model provides an optimal theoretical classification rule fellegi and sunter introduced method for automatically estimating optimal parameter without training data that we extend to many real world situation 
current psychological theory of human causal learning and judgment focus primarily on long run prediction two by estimating parameter of a causal bayes net though for different parameterizations and a third through structural learning this paper focus on people s short run behavior by examining dynamical version of these three theory and comparing their prediction to a real world dataset 
due to the advance in positioning technology the real time information of moving object becomes increasingly available which ha posed new challenge to the database research a a long standing technique to identify overall distribution pattern in data clustering ha achieved brilliant success in analyzing static datasets in this paper we study the problem of clustering moving object which could catch interesting pattern change during the motion process and provide better insight into the essence of the mobile data point in order to catch the spatial temporal regularity of moving object and handle large amount of data micro clustering is employed efficient technique are proposed to keep the moving micro cluster geographically small important event such a the collision among moving micro cluster are also identified in this way high quality moving micro cluster are dynamically maintained which lead to fast and competitive clustering result at any given time instance we validate our approach with a through experimental evaluation where order of magnitude improvement on running time is observed over normal k mean clustering method 
this paper introduces correlated q ce q learning a multiagent q learning algorithm based on the correlated equilibrium ce solution concept ce q generalizes both nashq and friend and foe q in general sum game the set of correlated equilibrium contains the set of nash equilibrium in constantsum game the set of correlated equilibrium contains the set of minimax equilibrium this paper describes experiment with four variant of ce q demonstrating empirical convergence to equilibrium policy on a testbed of general sum markov game 
we present several new algorithm for multiagent reinforcementlearning a commonfeatureof these algorithm is a parameterized structured representation of a policy or value function this structure is leveraged in an approach we call coordinated reinforcement learning by which agent coordinate both their action selection activity and their parameter update within the limit of our parametric representation the agent will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space our method differ from many previous reinforcement learning approach to multiagent coordination in that structured communication and coordination between agent appears at the core of both the learning algorithm and the execution architecture our experimental result comparing our approach to other rl method illustrate both the quality of the policy obtained and the additional benefit of coordination 
principal component analysis pca is one of the most widely used technique in machine learning and data mining minor component analysis mca is le well known but can also play an important role in the presence of constraint on the data distribution in this paper we present a probabilistic model for extreme component analysis xca which at the maximum likelihood solution extract an optimal combination of principal and minor component for a given number of component the log likelihood of the xca model is guaranteed to be larger or equal than that of the probabilistic model for pca and mca we describe an efficient algorithm to solve for the globally optimal solution for log convex spectrum we prove that the solution consists of principal component only while for log concave spectrum the solution consists of minor component in general the solution admits a combination of both in experiment we explore the property of xca on some synthetic and real world datasets 
we introduce the community to a new construction principle whose practical implication are very broad central to this research is the idea of improving the presentation of algorithm in the literature and making them more appealing we dene a new notion of capacity for data set and derive a methodology for selecting from them our experiment demonstrate that even not so good algorithm can be shown signicantly better than competitor we present some experimental result which are very promising 
gaussian process are usually parameterised in term of their covariance function however this make it difficult to deal with multiple output because ensuring that the covariance matrix is positive definite is problematic an alternative formulation is to treat gaussian process a white noise source convolved with smoothing kernel and to parameterise the kernel instead using this we extend gaussian process to handle multiple coupled output 
abstract variable selection the process of identifying input variable that are relevant to a particular learning problem ha received much attention in the learning community method that employ a learning algorithm a a part of the selection process wrapper have been shown to outperform method that select variable independently from the learning algorithm filter but only at great compu tational expense we present a randomized wrapper algorithm whose computational requirement are within a constant factor of simply learning in the presence of all input variable provided that the number of relevant variable is small and known in advance we then show how to remove the latter assumption and demonstrate performance on several problem 
we formulate the problem of graph inference where part of the graph is known a a supervised learning problem and propose an algorithm to solve it the method involves the learning of a mapping of the vertex to a euclidean space where the graph is easy to infer and can be formulated a an optimization problem in a reproducing kernel hilbert space we report encouraging result on the problem of metabolic network reconstruction from genomic data 
we describe a probabilistic approach to the task of placing object described by high dimensional vector or by pairwise dissimilarity in a low dimensional space in a way that preserve neighbor identity a gaussian is centered on each object in the high dimensional space and the density under this gaussian or the given dissimilarity are used to define a probability distribution over all the potential neighbor of the object the aim of the embedding is to approximate this distribution a well a possible when the same operation is performed on the low dimensional image of the object a natural cost function is a sum of kullback leibler divergence one per object which lead to a simple gradient for adjusting the position of the low dimensional image unlike other dimensionality reduction method this probabilistic framework make it easy to represent each object by a mixture of widely separated low dimensional image this allows ambiguous object like the document count vector for the word bank to have version close to the image of both river and finance without forcing the image of outdoor concept to be located close to those of corporate concept 
why are sensory modality segregated the way they are in this paper we show that sensory modality are well designed for self supervised cross modal learning using the minimizing disagreement algorithm on an unsupervised speech categorization task with visual moving lip and auditory sound signal input we show that very informative auditory dimension actually harm performance when moved to the visual side of the network it is better to throw them away than to consider them part of the visual input we explain this finding in term of the statistical structure in sensory input 
boosting algorithm and successful application thereof abound for classification and regression learning problem but not for unsupervised learning we propose a sequential approach to adding feature to a random field model by training them to improve classification performance between the data and an equal sized sample of negative example generated from the model s current estimate of the data density training in each boosting round proceeds in three stage first we sample negative example from the model s current boltzmann distribution next a feature is trained to improve classification performance between data and negative example finally a coefficient is learned which determines the importance of this feature relative to one already in the pool negative example only need to be generated once to learn each new feature the validity of the approach is demonstrated on binary digit and continuous synthetic data 
considerable progress wa recently achieved on semi supervised learning which diers from the traditional supervised learning by additionally exploring the information of the unlabelled example however a disadvantage of many existing method is that it doe not generalize to unseen input this paper investigates learning method that eectively make use of both labelled and unlabelled data to build predictive function which are defined on not just the seen input but the whole space a a nice property the proposed method allows ecient training and can easily handle new test point we validate the method based on both toy data and real world data set 
we consider the problem of recovering an underwater image distorted by surface wave a large amount of video data of the distorted image is acquired the problem is posed in term of finding an undistor ted image patch at each spatial location this challenging reconstruction task can be formulated a a manifold learning problem such that the center of the manifold is the image of the undistorted patch to compute the center we present a new technique to estimate global distance on the manifold our technique achieves robustness through convex flow computation and solves the leakage problem inherent in recent manifold embedding technique 
we show two related thing given a classier which consists of a weighted sum of featureswith a large margin we can construct a stochastic classier withnegligibly larger training error rate the stochastic classier hasa future error rate bound that depends on the margin distributionand is independent of the size of the base hypothesis class 
in this paper we propose a novel method for learning a mahalanobis distance measure to be used in the knn classification algorit hm the algorithm directly maximizes a stochastic variant of the le ave one out knn score on the training set it can also learn a low dimensional linear embedding of labeled data that can be used for data visualization and fast classification unlike other method our classific ation model is non parametric making no assumption about the shape of the class distribution or the boundary between them the performance of the method is demonstrated on several data set both for metric learning and linear dimensionality reduction 
many problem in information processing involve some form of dimensionality reduction in this paper we introduce locality preserving projection lpp these are linear projective map that arise by solving a variational problem that optimally preserve the neighborhood structure of the data set lpp should be seen a an alternative to principal component analysis pca a classical linear technique that project the data along the direction of maximal variance when the high dimensional data lie on a low dimensional manifold embedded in the ambient space the locality preserving projection are obtained by finding the optimal linear approximation to the eigenfunctions of the laplace beltrami operator on the manifold a a result lpp share many of the data representation property of nonlinear technique such a laplacian eigenmaps or locally linear embedding yet lpp is linear and more crucially is defined everywhere in ambient space rather than just on the training data point this is borne out by illustrative example on some high dimensional data set 
spectral clustering refers to a class of technique which re ly on the eigenstructure of a similarity matrix to partition point into di sjoint cluster with point in the same cluster having high similarity and point in different cluster having low similarity in this paper we der ive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem minimizing this cost function with respect to the partition lead to a new spectral clustering algorithm minimizing with respect to the similarity matrix lead to an algorithm f or learning the similarity matrix we develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors 
we address in this paper the question of how the knowledge of the marginal distribution p x can be incorporated in a learning algorithm we suggest three theoretical method for taking into account this distribution for regularization and provide link to existing graph based semi supervised learning algorithm we also propose practical implementation 
the similarity between object is a fundamental element of many learning algorithm most non parametric method take this similarity to be fixed but much recent work ha shown the advantage of learni ng it in particular to exploit the local invariance in the data or to capture the possibly non linear manifold on which most of the data lie we propose a new non parametric kernel density estimation method which capture the local structure of an underlying manifold through the le ading eigenvectors of regularized local covariance matrix experim ents in density estimation show significant improvement with respect to pa rzen density estimator the density estimator can also be used within bayes classifier yielding classification rate similar to svms and much superior to the parzen classifier 
we describe a way of using multiple different type of similarity relationship to learn a low dimensional embedding of a dataset our method chooses different possibly overlapping representation of similarity by individually reweighting the dimension of a common underlying latent space when applied to a single similarity relation that is based on euclidean distance between the input data point the method reduces to simple dimensionality reduction if additional information is available about the dataset or about subset of it we can use this information to clean up or otherwise improve the embedding we demonstrate the potential usefulness of this form of semi supervised dimensionality reduction on some simple example 
monaural speech separation ha been studied in previous system that incorporate auditory scene analysis principle a major problem for these system is their inability to deal with speech in the highfrequency range psychoacoustic evidence suggests that different perceptual mechanism are involved in handling resolved and unresolved harmonic motivated by this we propose a model for monaural separation that deal with low frequency and highfrequency signal differently for resolved harmonic our model generates segment based on temporal continuity and cross channel correlation and group them according to periodicity for unresolved harmonic the model generates segment based on amplitude modulation am in addition to temporal continuity and group them according to am repetition rate derived from sinusoidal modeling underlying the separation process is a pitch contour obtained according to psychoacoustic constraint our model is systematically evaluated and it yield substantially better performance than previous system especially in the high frequency range 
learning from structured data is becoming increasingly important however most prior work on kernel method ha focused on learning from attribute value data only recently research started investigating kernel for structured data this paper considers kernel for multi instance problem a class of concept on individual represented by set the main result of this paper is a kernel on multi instance data that can be shown to separate positive and negative set under natural assumption this kernel compare favorably with state of the art multi instance learning algorithm in an empirical study finally we give some concluding remark and propose future work that might further improve the result 
we propose a new method for clustering based on finding maximum mar gin hyperplanes through data by reformulating the problem in term of the implied equivalence relation matrix we can pose the problem a a convex integer program although this still yield a difficult com putational problem the hard clustering constraint can be relaxed to a soft clustering formulation which can be feasibly solved with a semidef inite program since our clustering technique only depends on the data through the kernel matrix we can easily achieve nonlinear clustering in the same manner a spectral clustering experimental result show that our maximum margin clustering technique often obtains more accurate result than conventional clustering method the real benefit of our ap proach however is that it lead naturally to a semi supervised training method for support vector machine by maximizing the margin simul taneously on labeled and unlabeled training data we achieve state of the art performance by using a single integrated learning principle 
we generalise the gaussian process gp framework for regression by learning a nonlinear transformation of the gp output this allows for non gaussian process and non gaussian noise the learning algorithm chooses a nonlinear transformation such that transformed data is well modelled by a gp this can be seen a including a preprocessing transformation a an integral part of the probabilistic modelling problem rather than a an ad hoc step we demonstrate on several real regression problem that learning the transformation can lead to significantly better performance than using a regular gp or a gp with a fix ed transformation 
a general linear response method for deriving improved estimate of correlation in the variational bayes framework is presented three application are given and it is discussed how to use linear response a a general principle for improving mean field approximation 
we describe a visualization technique that us brushed parallel histogram to aid in understanding concept drift in multidimensional problem space this technique illustrates the relationship between change in distribution of multiple antecedent feature value and the outcome distribution we can also observe effect on the relative utilization of predictive rule our parallel histogram technique solves the over plotting difficulty of parallel coordinate graph and the difficulty of comparing distribution of brushed and original data we demonstrate our technique s usefulness in understanding concept drift in power demand and stock investment return 
in we introduced a linear statistical model of joint color change in image due to variation in lighting and certain non geometric camera parameter we did this by measuring the mapping of color in one image of a scene to color in another image of the same scene under different lighting condition here we increase the flexibility of this color flow model by allowing flow coefficient to vary according to a low order polynomial over the image this allows u to better fit smoothly varying lighting condition a well a curved surface without endowing our model with too much capacity we show result on image matching and shadow removal and detection addressing the variability of image due to these photic parameter ha been an important problem in machine vision we distinguish photic parameter from geometric parameter such a camera orientation or blurring that affect which part of the scene a particular pixel represents we also note that photic parameter are more general than lighting parameter and include anything which affect the final rgb value in an image given that the geometric parameter and the object in the scene have been fixed we present a statistical linear model of color change space that is learned by observing how the color in static image change jointly under common naturally occurring lighting change such a model can be used for a number of task including synthesis of image of new object under different lighting condition image matching and shadow detection result for each of these task will be reported several aspect of our model merit discussion first it is obtained from video data in a completely unsupervised fashion the model us no prior knowledge of lighting condition surface reflectance or other parameter during data collection and modeling it also ha no built in knowledge of the physic of image acquisition or typical image color 
although discriminatively trained classifier are usually more accurate when labeled training data is abundant previous work ha shown that when training data is limited generative classifier can ou t perform them this paper describes a hybrid model in which a high dimensional subset of the parameter are trained to maximize generative likelihood and another small subset of parameter are discriminatively trained to maximize conditional likelihood we give a sample complexity bound showing that in order to fit the discriminative parameter we ll the number of training example required depends only on the logarithm of the number of feature occurrence and feature set size experimental result show that hybrid model can provide lower test error and can produce better accuracy coverage curve than either their purely g enerative or purely discriminative counterpart we also discus several advantage of hybrid model and advocate further work in this area 
this work present an architecture based on perceptrons to recognize phrase structure and an online learning algorithm to train the perceptrons together and dependently the recognition strategy applies learning in two layer a filtering layer which reduces the search space by identifying plausible phrase candidate and a ranking layer which recursively build the optimal phrase structure we provide a recognition based feedback rule which reflects to each local function it committed error from a global point of view and allows to train them together online a perceptrons experimentation on a syntactic parsing problem the recognition of clause hierarchy improves state of the art result and evinces the advantage of our global training method over optimizing each function locally and independently 
abstract we consider a learning setting in which thereare well dened relation that exist amonginstances of certain class in particular we consider the domain of predicting varioustypes of gene regulation element in bacterialgenomes given instance of one class wecan often acquire weakly labeled quot trainingdata for another class by taking advantage ofknown relationship that exist between thetwo class the example are weakly labeledin that either the class label is 
we present an algorithm for unsupervised learning and semantic classification of name and term given a small number of seed example and an unlabeled training corpus the algorithm learns pattern that identify more example in a bootstrapping cycle multiple class are learned simultaneously including negative class that serve to provide negative example for the target class we apply the algorithm to text from several domain in english and chinese 
detection of near duplicate document is an important problem in many data mining and information filtering application when faced with massive quantity of data traditional duplicate detection technique relying on direct inter document similarity computation e g using the cosine measure are often not feasible given the time and memory performance constraint on the other hand fingerprint based method such a i match are very attractive computationally but may be brittle with respect to small change to document content we focus on approach to near replica detection that are based upon large collection statistic and present a general technique of increasing their robustness via multiple lexicon randomization in experiment with large web page and spam email datasets the proposed method is shown to consistently outperform traditional i match with the relative improvement in duplicate document recall reaching a high a the large gain in detection accuracy are offset by only small increase in computational requirement 
we consider an mdp setting in which the reward function is allowed to change during each time step of play possibly in an adversarial manner yet the dynamic remain fixed similar to the expert setting we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time we provide efficient algorithm which have regret bound with no dependenceon the size of state space instead these bound depend only on a certain horizon time of the process and logarithmically on the number of action we also show that in the case that the dynamic change over time the problem becomes computationally hard 
collaborative and content based filtering are two paradigm that have been applied in the context of recommender system and user preference prediction this paper proposes a novel unified approach that systematically integrates all available training information such a past user item rating a well a attribute of item or user to learn a prediction function the key ingredient of our method is the design of a suitable kernel or similarity function between user item pair that allows simultaneous generalization across the user and item dimension we propose an on line algorithm jrank that generalizes perceptron learning experimental result on the eachmovie data set show significant improvement over standard approach 
cross entropy and mean squared error are typical cost function used to optimize classifier performance the goal of the optimization is usually to achieve the best correct classification rate however for many two class real world problem the roc curve is a more meaningful performance measure we demonstrate that minimizing cross entropy or mean squared error doe not necessarily maximize the area under the roc curve auc we then consider alternative objective function for training a 
we e xplore dynamic shaping to integrate our prior belief of the final policy into a conventional reinforcement learning system shaping provides a positive or negative artificial increment t o the native task reward in order to encourage or discourage behavior previously shaping function have been static the additional reward do not vary with experience but some prior knowledge ca nnot be e xpressed a s tatic shaping we take an explanation based approach in which the specific shaping function emerges from initial experience with the world we compare no shaping static shaping and dynamic shaping in the task of learning bipedal walking on a simulator we e mpirically evaluate the convergence rate a nd final performance a mong these c onditions while varying the acc uracy of the prior knowledge we c onclude that i n the appropriate context dynamic shaping can greatly improve the learning of action policy 
hidden variable evolving over time appear in multiple setting where it is valuable to recover them typically from observed sum our driving application is network tomography where we need to estimate the origin destination od traffic flow to determine e g who is communicating with whom in a local area network this information allows network engineer and manager to solve problem in design routing configuration debugging monitoring and pricing unfortunately the direct measurement of the od traffic is usually difficult or even impossible instead we can easily measure the load on every link that is sum of desirable od flow in this paper we propose i filter a method to solve this problem which improves the state of the art by a introducing explicit time dependence and by b using realistic non gaussian marginals in the statistical model for the traffic flow a never attempted before we give experiment on real data where i filter scale linearly with new observation and out performs the best existing solution in a wide variety of setting specifically on real network traffic measured at cmu and at at t i filter reduced the estimation error between and in all case 
the banking industry regularly mount campaign to improve customer value by offering new product to existing customer in recent year this approach ha gained significant momentum because of the increasing availability of customer data and the improved analysis capability in data mining typically response model based on historical data are used to estimate the probability of a customer purchasing an additional product and the expected return from that additional purchase even with these computational improvement and accurate model of customer behavior the problem of efficiently using marketing resource to maximize the return on marketing investment is a challenge this problem is compounded because of the capability to launch multiple campaign through several distribution channel over multiple time period the combination of alternative creates a complicated array of possible action this paper present a solution that answer the question of what product if any to offer to each customer in a way that maximizes the marketing return on investment the solution is an improvement over the usual approach of picking the customer that have the largest expected value for a particular product because it is a global maximization from the viewpoint of the bank and allows for the effective implementation of business constraint across customer and business unit the approach account for limited resource multiple sequential campaign and other business constraint furthermore the solution provides insight into the cost of these constraint in term of decreased profit and thus is an effective tool for both tactical campaign execution and strategic planning 
in this paper we address the problem of statistical learnin g for multitopic text categorization mtc whose goal is to choose all relevant topic a label from a given set of topic the proposed algorithm maximal margin labeling mml treat all possible label a independent class and learns a multi class classifier on the induced mu lti class categorization problem to cope with the data sparseness caused by the huge number of possible label mml combine some prior knowledge about label prototype and a maximal margin criterion in a novel way experiment with multi topic web page show that mml outperforms existing learning algorithm including support vector machine 
is it feasible to train classifier to decode the cognitive state of a human subject based on single episode fmri data if so these trained classifier could be used a virtual sensor to detect hidden cognitive state of a subject providing a key tool for experimental research in cognitive science and in diagnosis of mental process in patient with brain injury whereas much work ha been done on fmri data analysis method that average together data collected from repeated stimulus over multiple episode little is known about the feasibility of training classifier to decode cognitive state from single episode this paper present several case study in which we have successfully trained such classifier we explore the technical issue involved in training such single episode classifier and discus area for future research these case study include training a classifier to determine which of twelve semantic category of word is being read by a human subject e g a word describing animal or one describing building whether or not a subject find a sentence ambiguous and whether the subject is looking at a picture or at a sentence describing a picture 
de novo sequencing of peptide is a challenging task in proteome research while there exist reliable dna sequencing method the highthroughput de novo sequencing of protein by mass spectrometry is still an open problem current approach suffer from a lack in precision to detect mass peak in the spectrogram in this paper we present a novel method for de novo peptide sequencing based on a hidden markov model experiment effectively demonstrate that this new method significantly outperforms standard approach in matching quality 
many automated learning procedure lack interpretability operating effectively a a black box providing a prediction tool but no explanation of the underlying dynamic that drive it a common approach to interpretation is to plot the dependence of a learned function on one or two predictor we present a method that seek not to display the behavior of a function but to evaluate the importance of non additive interaction within any set of variable should the function be close to a sum of low dimensional component these component can be viewed and even modeled parametrically alternatively the work here provides an indication of where intrinsically high dimensional behavior take place the calculation used in this paper correspond closely with the functional anova decomposition a well developed construction in statistic in particular the proposed score of interaction importance measure the loss associated with the projection of the prediction function onto a space of additive model the algorithm run in linear time and we present display of the output a a graphical model of the function for interpretation purpose 
segmentation and recognition have long been treated a two separate process we propose a mechanism based on spectral graph partitioning that readily combine the two process into one a part based recognition system detects object patch supply their partial segmentation and knowledge about the spatial configuration of the object the goal of patch grouping is to find a set of patch that conform best to the object configuration this process is integrated with the pixel grouping based on low level feature similarity through pixel patch interaction and patch competition that is encoded a constraint in the solution space the globally optimal partition is obtained by solving a constrained eigenvalue problem we demonstrate that the resulting object segmentation eliminates local false positive at the high level of part detection while overcoming occlusion and weak contour at the low level of edge detection 
many inference and optimization task in machine learning can be solved by sampling approach such a markov chain monte carlo mcmc and simulated annealing these method can be slow if a single target density query requires many run of a simulation or a complete sweep of a training data set we introduce a hierarchy of mcmc sampler that allow most step to be taken in the solution space using only a small sample of simulation run or training example this is shown to accelerate learning in a policy search optimization task 
the basic tool of machine learning appear in the inner loop of most reinforcement learning algorithm typically in the form of monte carlo method or function approximation technique to a large extent however current reinforcement learning algorithm draw upon machine learning technique that are at least ten year old and with a few exception very little ha been done to exploit recent advance in classification learning for the purpose of reinforcement learning we use a variant of approximate policy iteration based on rollouts that allows u to use a pure classification learner such a a support vector machine svm in the inner loop of the algorithm we argue that the use of svms particularly in combination with the kernel trick can make it easier to apply reinforcement learning a an outof the box technique without extensive feature engineering our approach open the door to modern classification method but doe not preclude the use of classical method we present experimental result in the pendulum balancing and bicycle riding domain using both svms and neural network for classifier 
is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in taking the case of space for example is there a way for this algorithm to realize that it body is in a three dimensional world is it possible for this algorithm to discover how to move in a straight line and more basically do these question make any sense at all given that the algorithm only ha access to the very high dimensional data consisting of it sensory input and motor output we demonstrate in this article how these question can be given a positive answer we show that it is possible to make an algorithm that by analyzing the law that link it motor output to it sensory input discovers information about the structure of the world regardless of the device constituting the body it is linked to we present result from simulation demonstrating a way to issue motor order resulting in fundamental movement of the body a regard the structure of the physical world 
thanks to it increasing availability electronic literature can now be a major source of information when developing complex statistical model where data is scarce or contains much noise this raise the question of how to integrate information from domain literature with statistical data because quantifying similarity or dependency between variable is a basic building block in knowledge discovery we consider here the following question which vector representation of text and which statistical score of similarity or dependency support best the use of literature in statistical model for the text source we assume to have annotation for the domain variable a short free text description and optionally to have a large literature repository from which we can further expand the annotation for evaluation we contrast the variable similarity or dependency obtained from text using different annotation source and vector representation with those obtained from measurement data or expert assessment specifically we consider two learning problem clustering and bayesian network learning firstly we report performance against an expert reference for clustering yeast gene from textual annotation secondly we ass the agreement between text based and data based score of variable dependency when learning bayesian network substructure for the task of modeling the joint distribution of clinical measurement of ovarian tumor 
in applying hidden markov model to the analysis of massive data stream it is often necessary to use an artiflcially reduced set of state this is due in large part to the fact that the basic hmm estimation algorithm have a quadratic dependence on the size of the state set we present algorithm that reduce this computational bottleneck to linear or near linear time when the state can be embedded in an underlying grid of parameter this type of state representation arises in many domain in particular we show an application to tra c analysis at a high volume web site 
this paper investigates the effect of kernel principal component analysis kpca within the classification framework essentially the regularization property of this dimensionality reduction method kpca ha been previously used a a pre processing step before applying an svm but we point out that this method is somewhat redundant from a regularization point of view and we propose a new algorithm called kernel projection machine to avoid this redundancy based on an analogy with the statistical framework of regression for a gaussian white noise model preliminary experimental result show that this algorithm reach the same performance a an svm 
motor control depends on sensory feedback in multiple modality with different latency in this paper we consider within the framework of reinforcement learning how different sensory modality can be combined and selected for real time optimal movement control we propose an actor critic architecture with multiple module whose output are combined using a softmax function we tested our architecture in a simulation of a sequential reaching task reaching wa initially guided by visual feedback with a long latency our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory in simulation with different latency for visual and somatosensory feedback we found that the agent depended more on feedback with shorter latency 
in many real world classification problem the input contains a large number of potentially irrelevant feature this paper proposes a new bayesian framework for determining the relevance of input feature this approach extends one of the most successful bayesian method for feature selection and sparse learning known a automatic relevance determination ard ard find the relevance of feature by optimizing the model marginal likelihood also known a the evidence we show that this can lead to overfitting to address this problem we propose predictive ard based on estimating the predictive performance of the classifier while the actual leave one out predictive performance is generally very costly to compute the expectation propagation ep algorithm proposed by minka provides an estimate of this predictive performance a a side effect of it iteration we exploit this in our algorithm to do feature selection and to select data point in a sparse bayesian kernel classifier moreover we provide two other improvement to previous algorithm by replacing laplace s approximation with the generally more accurate ep and by incorporating the fast optimization algorithm proposed by faul and tipping our experiment show that our method based on the ep estimate of predictive performance is more accurate on test data than relevance determination by optimizing the evidence 
procedure for collective inference make simultaneous statistical judgment about the same variable for a set of related data instance for example collective inference could be used to simultaneously classify a set of hyperlinked document or infer the legitimacy of a set of related financial transaction several recent study indicate that collective inference can significantly reduce classification error when compared with traditional inference technique we investigate the underlying mechanism for this error reduction by reviewing past work on collective inference and characterizing different type of statistical model used for making inference in relational data we show important difference among these model and we characterize the necessary and sufficient condition for reduced classification error based on experiment with real and simulated data 
the detection and pose estimation of people in image and video is made challenging by the variability of human appearance the complexity of natural scene and the high dimensionality of articulated body model to cope with these problem we represent the d human body a a graphical model in which the relationship between the body part are represented by conditional probability distribution we formulate the pose estimation problem a one of probabilistic inference over a graphical model where the random variable correspond to the individual limb parameter position and orientation because the limb are described by dimensional vector encoding pose in space discretization is impractical and the random variable in our model must be continuousvalued to approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter this framework facilitates the automatic initialization of the body model from low level cue and is robust to occlusion of body part and scene clutter 
this paper present a novel hidden markov model architecture to model the joint probability of pair of asynchronous sequence describing the same event it is based on two other markovian model namely asynchronous input output hidden markov model and pair hidden markov model an em algorithm to train the model is presented a well a a viterbi decoder that can be used to obtain the optimal state sequence a well a the alignment between the two sequence the model ha been tested on an audio visual speech recognition task using the m vt database and yielded robust performance under various noise condition 
certain simple image are known to trigger a percept of transparency the input image i is perceived a the sum of two image i x y i x y i x y this percept is puzzling first why dowechoosethe morecomplicated descriptionwithtwoimages ratherthanthe simpler explanation i x y i x y second giventheinflnitenumberofwaystoexpress i asasumoftwo image how do we compute the best decomposition herewesuggestthattransparencyistherationalperceptofasystemthatisadaptedtothestatisticsofnaturalscenes wepresent a probabilistic model of image based on the qualitative statistic of derivative fllters and corner detector in natural scene and usethismodeltoflndthemostprobabledecompositionofanovel image the optimization is performed using loopy belief propagation we show that our model computes perceptually correct decomposition on synthetic image and discus it application to real image 
classifier learning method commonly assume that the training data consist of randomly drawn example from the same distribution a the test example about which the learned model is expected to make prediction in many practical situation however this assumption is violated in a problem known in econometrics a sample selection bias in this paper we formalize the sample selection bias problem in machine learning term and study analytically and experimentally how a number of well known classifier learning method are affected by it we also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias 
statistical language model estimate the probability of a word occurring in a given context the most common language model rely on a discrete enumeration of predictive context e g n gram and consequently fail to capture and exploit statistical regularity across these context in this paper we show how to learn hierarchical distributed representation of word context that maximize the predictive value of a statistical language model the representation are initialized by unsupervised algorithm for linear and nonlinear dimensionality reduction then fed a input into a hierarchical mixture of expert where each expert is a multinomial distribution over predicted word while the distributed representation in our model are inspired by the neural probabilistic language model of bengio et al our particular architecture enables u to work with significantly larger vocabulary and training corpus for example on a large scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentence we demonstrate consistent improvement over class based bigram model we also discus extension of our approach to longer multiword context 
the standard model of supervised learning assumes that training and test data are drawn from the same underlying distribution this paper explores an application in which a second auxiliary source of data is available drawn from a different distribution this auxiliary data is more plentiful but of significantly lower quality than the training and test data in the svm framework a training example ha two role a a a data point to constrain the learning process and b a a candidate support vector that can form part of the definition of the classifier the paper considers using the auxiliary data in either or both of these role this auxiliary data framework is applied to a problem of classifying image of leaf of maple and oak tree using a kernel derived from the shape of the leaf experiment show that when the training data set is very small training with auxiliary data can produce large improvement in accuracy even when the auxiliary data is significantly different from the training and test data the paper also introduces technique for adjusting the kernel score of the auxiliary data point to make them more comparable to the training data point 
support vector machine and other kernel method have proven to be very effective for nonlinear inference practical issue are how to select the type of kernel including any parameter and how to deal with the computational issue caused by the fact that the kernel matrix grows quadratically with the data inspired by ensemble and boosting method like mart we propose the multiple additive regression kernel mark algorithm to address these issue mark considers a large potentially infinite library of kernel matrix formed by different kernel function and parameter using gradient boosting column generation mark construct column of the heterogeneous kernel matrix the base hypothesis on the fly and then add them into the kernel ensemble regularization method such a used in svm kernel ridge regression and mart are used to prevent overfitting we investigate how mark is applied to heterogeneous kernel ridge regression the resulting algorithm is simple to implement and efficient kernel parameter selection is handled within mark sampling and weak kernel are used to further enhance the computational efficiency of the resulting additive algorithm the user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernel mark compare very favorably with svm and kernel ridge regression on several benchmark datasets 
we describe method for taking into account unlabeled data in the training of a kernel based classifier such a a support vector machine svm we propose two approach utilizing unlabeled point in the vicinity of labeled one both of the approach effectively modify the metric of the pattern space either by using nonspherical gaussian density estimate which are determined using em or by modifying the kernel function using displacement vector computed from pair of unlabeled and labeled point the latter is linked to technique for training invariant svms we present experimental result indicating that the proposed technique can lead to substantial improvement of classification accuracy 
while clustering is usually an unsupervised operation there are circumstance in which we believe with varying degree of certainty that item a and b should be assigned to the same cluster while item a and c should not we would like such pairwise relation to influence cluster assignment of out of sample data in a manner consistent with the prior knowledge expressed in the training set our starting point is probabilistic clustering based on gaussian mixture model gmm of the data distribution we express clustering preference in the prior distribution over assignment of data point to cluster this prior penalizes cluster assignment according to the degree with which they violate the preference we fit the model parameter with em experiment on a variety of data set show that ppc can consistently improve clustering result 
decision tree construction is a well studied problem in data mining recently there ha been much interest in mining streaming data domingo and hulten have presented a one pas algorithm for decision tree construction their work us hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed in this paper we revisit this problem we make the following two contribution we present a numerical interval pruning nip approach for efficiently processing numerical attribute our result show an average of reduction in execution time we exploit the property of the gain function entropy and gini to reduce the sample size required for obtaining a given bound on the accuracy our experimental result show a reduction in the number of data instance required 
data mining with bayesian network learning ha two important characteristic under condition learned edge between variable correspond to casual influence and second for every variable t in the network a special subset markov blanket identifiable by the network is the minimal variable set required to predict t however all known algorithm learning a complete bn do not scale up beyond a few hundred variable on the other hand all known sound algorithm learning a local region of the network require an exponential number of training instance to the size of the learned region the contribution of this paper is two fold we introduce a novel local algorithm that return all variable with direct edge to and from a target variable t a well a a local algorithm that return the markov blanket of t both algorithm i are sound ii can be run efficiently in datasets with thousand of variable and iii significantly outperform in term of approximating the true neighborhood previous state of the art algorithm using only a fraction of the training size required by the existing method a fundamental difference between our approach and existing one is that the required sample depends on the generating graph connectivity and not the size of the local region this yield up to exponential saving in sample relative to previously known algorithm the result presented here are promising not only for discovery of local causal structure and variable selection for classification but also for the induction of complete bns 
in this paper we propose a new information theoretic divisive algorithm for word clustering applied to text classification in previous work such distributional clustering of feature ha been found to achieve improvement over feature selection in term of classification accuracy especially at lower number of feature however the existing clustering technique are agglomerative in nature and result in i sub optimal word cluster and ii high computational cost in order to explicitly capture the optimality of word cluster in an information theoretic framework we first derive a global criterion for feature clustering we then present a fast divisive algorithm that monotonically decrease this objective function value thus converging to a local minimum we show that our algorithm minimizes the within cluster jensen shannon divergence while simultaneously maximizing the between cluster jensen shannon divergence in comparison to the previously proposed agglomerative strategy our divisive algorithm achieves higher classification accuracy especially at lower number of feature we further show that feature clustering is an effective technique for building smaller class model in hierarchical classification we present detailed experimental result using naive bayes and support vector machine on the newsgroups data set and a level hierarchy of html document collected from dmoz open directory 
we study the problem of modeling specie geographic distribution a critical problem in conservation biology we propose the use of maximum entropy technique for this problem specifically sequential update algorithm that can handle a very large number of feature we describe experiment comparing maxent with a standard distribution modeling tool called garp on a dataset containing observation data for north american breeding bird we also study how well maxent performs a a function of the number of training example and training time analyze the use of regularization to avoid overfitting when the number of example is small and explore the interpretability of model constructed using maxent 
neural network are successful in acquiring hidden knowledge in datasets their biggest weakness is that the knowledge they acquire is represented in a form not understandable to human researcher tried to address this problem by extracting rule from trained neural network most of the proposed rule extraction method required specialized type of neural network some required binary input and some were computationally expensive craven proposed extracting mofn type decision tree from neural network we believe mofn type decision tree are only good for mofn type problem and tree created for regular high dimensional real world problem may be very complex in this paper we introduced a new method for extracting regular c like decision tree from trained neural network we showed that the new method dectext is effective in extracting high fidelity tree from trained network we also introduced a new discretization technique to make dectext be able to handle continuous feature and a new pruning technique for finding simplest tree with the highest fidelity 
this paper address the problem of finding a small and coherent subset of point in a given data this problem sometimes referred to a one class or set covering requires to find a small radius ball that cover a many data point a possible it rise naturally in a wide range of application from finding gene module to extracting document topic where many data point are irrelevant to the task at hand or in application where only positive example are available most previous approach to this problem focus on identifying and discarding a possible set of outlier in this paper we adopt an opposite approach which directly aim to find a small set of coherently structured region by using a loss function that focus on local property of the data we formalize the learning task a an optimization problem using the information bottleneck principle an algorithm to solve this optimization problem is then derived and analyzed experiment on gene expression data and a text document corpus demonstrate the merit of our approach 
we describe a sparse bayesian regression method for recovering d human body motion directly from silhouette extracted from monocular video sequence no detailed body shape model is needed and realism is ensured by training on real human motion capture data the tracker estimate d body pose by using relevance vector machine regression to combine a learned autoregressive dynamical model with robust shape descriptor extracted automatically from image silhouette we studied several different combination method the most effective being to learn a nonlinear observation update correction based on joint regression with respect to the predicted state and the observation we demonstrate the method on a parameter full body pose model both quantitatively using motion capture based test sequence and qualitatively on a test video sequence 
surrogate maximization or minimization sm algorithm are a family of algorithm that can be regarded a a generalization of expectation maximization em algorithm there are three major approach to the construction of surrogate function all relying on the convexity of some function in this paper we solve the boosting problem by proposing sm algorithm for the corresponding optimization problem specifically for adaboost we derive an sm algorithm that can be shown to be identical to the algorithm proposed by collins et al based on bregman distance more importantly for logitboost or logistic boosting we use several method to construct different surrogate function which result in different sm algorithm by combining multiple method we are able to derive an sm algorithm that is also the same a an algorithm derived by collins et al our approach based on sm algorithm is much simpler and convergence result follow naturally 
frequent itemset mining ha been the subject of a lot of work in data mining research ever since association rule were introduced in this paper we address a problem with frequent itemsets that they only count row where all their attribute are present and do not allow for any noise we show that generalizing the concept of frequency while preserving the performance of mining algorithm is nontrivial and introduce a generalization of frequent itemsets dense itemsets dense itemsets do not require all attribute to be present at the same time instead the itemset need to define a sufficiently large submatrix that exceeds a given density threshold of attribute present we consider the problem of computing all dense itemsets in a database we give a levelwise algorithm for this problem and also study the top k variation i e finding the k densest set with a given support or the k best supported set with a given density these algorithm select the other parameter automatically which simplifies mining dense itemsets in an explorative way we show that the concept capture natural facet of data set and give extensive empirical result on the performance of the algorithm combining the concept of dense itemsets with set cover idea we also show that dense itemsets can be used to obtain succinct description of large datasets we also discus some variation of dense itemsets 
a sensible use of classifier must be based on the estimated reliability of their prediction a cautious classifier would delegate the difficult or uncertain prediction to other possibly more specialised classifier in this paper we analyse and develop this idea of delegating classifier in a systematic way first we design a two step scenario where a first classifier chooses which example to classify and delegate the difficult example to train a second classifier secondly we present an iterated scenario involving an arbitrary number of chained classifier we compare these scenario to classical ensemble method such a bagging and boosting we show experimentally that our approach is not far behind these method in term of accuracy but with several advantage i improved efficiency since each classifier learns from fewer example than the previous one ii improved comprehensibility since each classification derives from a single classifier and iii the possibility to simplify the overall multi classifier by removing the part that lead to delegation 
essentially all data mining algorithm assume that the datagenerating process is independent of the data miner s activity however in many domain including spam detection intrusion detection fraud detection surveillance and counter terrorism this is far from the case the data is actively manipulated by an adversary seeking to make the classier produce false negative in these domain the performance of a classier can degrade rapidly after it is deployed a the adversary learns to defeat it currently the only solution to this is repeated manual ad hoc reconstruction of the classier in this paper we develop a formal framework and algorithm for this problem we view classication a a game between the classier and the adversary and produce a classier that is optimal given the adversary s optimal strategy experiment in a spam detection domain show that this approach can greatly outperform a classier learned in the standard way and within the parameter of the problem automatically adapt the classier to the adversary s evolving manipulation 
we present a novel method for approximate inference in bayesian model and regularized risk functionals it is based on the propagation of mean and variance derived from the laplace approximation of conditional probability in factorizing distribution much akin to minka s expectation propagation in the jointly normal case it coincides with the latter and belief propagation whereas in the general case it provides an optimization strategy containing support vector chunking the bayes committee machine and gaussian process chunking a special case 
spectral method for nonlinear dimensionality reduction nldr impose a neighborhood graph on point data and compute eigenfunctions of a quadratic form generated from the graph we introduce a more general and more robust formulation of nldr based on the singular value decomposition svd in this framework most spectral nldr principle can be recovered by taking a subset of the constraint in a quadratic form built from local nullspaces on the manifold the minimax formulation also open up an interesting class of method in which the graph is decorated with information at the vertex offering discrete or continuous map reduced computational complexity and immunity to some solution instability of eigenfunction approach apropos we show almost all nldr method based on eigenvalue decomposition evd have a solution instability that increase faster than problem size this pathology can be observed and corrected via the minimax formulation in problem a small a n point 
we propose a gossip based distributed algorithm for gaussian mixture learning newscast em the algorithm operates on network topology where each node observes a local quantity and can communicate with other node in an arbitrary point to point fashion the mai n difference between newscast em and the standard em algorithm is that the m step in our case is implemented in a decentralized manner random pair of node repeatedly exchange their local parameter estimat e and combine them by weighted averaging we provide theoretical evidence and demonstrate experimentally that under this protocol nod e converge exponentially fast to the correct estimate in each m step of t he em algorithm 
low rank approximation technique are widespread in pattern recognition research they include latent semantic analysis lsa probabilistic lsa principal component analysus pca the generative aspect model and many form of bibliometric analysis all make use of a low dimensional manifold onto which data are projected such technique are generally unsupervised which allows them to model data in the absence of label or category with many practical problem however some prior knowledge is available in the form of context in this paper i describe a principled approach to incorporating such information and demonstrate it application to pca based approximation of several data set 
rao blackwellization is an approximation technique for probabilistic inference that flexibly combine exact inference with sampling it is useful in model where conditioning on some of the variable leaf a simpler inference problem that can be solved tractably this paper present sample propagation an efficient implementation of rao blackwellized approximate inference for a large class of model sample propagation tightly integrates sampling with message passing in a junction tree and is named for it simple appealing structure it walk the cluster of a junction tree sampling some of the current cluster s variable and then passing a message to one of it neighbor we discus the application of sample propagation to conditional gaussian inference problem such a switching linear dynamical system 
a cocluster of a m x n matrix x is a submatrix determined by a subset of the row and a subset of the column the problem of finding coclusters with specific property is of interest in particular in the analysis of microarray experiment in that case the entry of the matrix x are the expression level of m gene in each of n tissue sample one goal of the analysis is to extract a subset of the sample and a subset of the gene such that the expression level of the chosen gene behave similarly across the subset of the sample presumably reflecting an underlying regulatory mechanism governing the expression level of the gene we propose to base the similarity of the gene in a cocluster on a simple biological model in which the strength of the regulatory mechanism in sample j is hj and the response strength of gene i to the regulatory mechanism is gi in other word every two gene participating in a good cocluster should have expression value in each of the participating sample whose ratio is a constant depending only on the two gene noise in the expression level of gene is taken into account by allowing a deviation from the model measured by a relative error criterion the sleeve width of the cocluster reflects the extent to which entry i j in the cocluster is allowed to deviate relatively from being expressed a the product gihj we present a polynomial time monte carlo algorithm which output a list of coclusters whose sleeve width do not exceed a prespecified value moreover we prove that the list includes with fixed probability a cocluster which is near optimal in it dimension extensive experimentation with synthetic data show that the algorithm performs well 
we introduce a general family of kernel based on weighted transducer or rational relation rational kernel that can be used for analysis of variable length sequence or more generally weighted automaton in application such a computational biology or speech recognition we show that rational kernel can be computed efficiently using a general algorithm of composition of weighted transducer and a general single source shortest distance algorithm we also describe several general family of positive definite symmetric rational kernel these general kernel can be combined with support vector machine to form efficient and powerful technique for spoken dialog classification highly complex kernel become easy to design and implement and lead to substantial improvement in the classification accuracy we also show that the string kernel considered in application to computational biology are all specific instance of rational kernel 
we present a trainable sequential inference technique for process with large state and observation space and relational structure our method assumes reliable observation i e that each process state persists long enough to be reliably inferred from the observation it generates we introduce the idea of a state inference function from observation sequence to underlying hidden state for representing knowledge about a process and develop an efficient sequential inference algorithm utilizing this function that is correct for process that generate reliable observation consistent with the state inference function we describe a representation for state inference function in relational domain and give a corresponding supervised learning algorithm experiment in relational video interpretation show that our technique provides significantly improved accuracy and speed relative to a variety of recent hand coded non trainable system 
we investigate bayesian alternative to classical monte carlo method for evaluating integral bayesian monte carlo bmc allows the incorporation of prior knowledge such a smoothness of the integrand into the estimation in a simple problem we show that this outperforms any classical importance sampling method we also attempt more challenging multidimensional integral involved in computing marginal likelihood of statistical model a k a partition function and model evidence we find that bayesian monte carlo outperformed anne aled importance sampling although for very high dimensional problem or problem with massive multimodality bmc may be le adequate one advantage of the bayesian approach to monte carlo is that sample can be drawn from any distribution this allows for the possibil ity of active design of sample point so a to maximise information gain 
several author have suggested viewing boosting a a gradient descent search for a good fit in function space we apply gradient based boostin g methodology to the unsupervised learning problem of density estimation we show convergence property of the algorithm and prove that a strength of weak learnability property applies to this problem a well we illustrate the poten tial of this approach through experiment with boosting bayesian network to learn density model 
we propose a family of kernel based on the binet cauchy theorem and it extension to fredholm operator this includes a special case all currently known kernel derived from the behavioral framework diffusion process marginalized kernel kernel on graph and the kernel on set arising from the subspace angle approach many of these kernel can be seen a the extremum of a new continuum of kernel function which lead to numerous new special case a an application we apply the new class of kernel to the problem of clustering of video sequence with encouraging result 
motivated by the interest in relational reinforcement learning we introduce a novel relational bellman update operator called rebel it employ a constraint logic programming language to compactly represent markov decision process over relational domain using rebel a novel value iteration algorithm is developed in which abstraction over state and action play a major role this framework provides new insight into relational reinforcement learning convergence result a well a experiment are presented 
we construct a nonlinear mapping from a high dimensional sample space to a low dimensional vector space effectively recovering a cartesian coordinate system for the manifold from which the data is sampled the mapping preserve local geometric relation in the manifold and is pseudo invertible we show how to estimate the intrinsic dimensionality of the manifold from sample decompose the sample data into locally linear low dimensional patch merge these patch into a single lowdimensional coordinate system and compute forward and reverse mapping between the sample and coordinate space the objective function are convex and their solution are given in closed form nonlinear dimensionality reduction nldr by charting charting is the problem of assigning a low dimensional coordinate system to data point in a high dimensional sample space it is presumed that the data lie on or near a lowdimensional manifold embedded in the sample space and that there exists a to smooth nonlinear transform between the manifold and a low dimensional vector space the datamodeler s goal is to estimate smooth continuous mapping between the sample and coordinate space often this analysis will shed light on the intrinsic variable of the datagenerating phenomenon for example revealing perceptual or configuration space our goal is to find a mapping expressed a a kernel based mixture of linear projection that minimizes information loss about the density and relative location of sample point this constraint is expressed in a posterior that combine a standard gaussian mixture model gmm likelihood function with a prior that penalizes uncertainty due to inconsistent projection in the mixture section develops a special case where this posterior is unimodal and maximizable in closed form yielding a gmm whose covariance reveal a patchwork of overlapping locally linear subspace that cover the manifold section show that for this or any gmm and a choice of reduced dimension d there is a unique closed form solution for a minimally distorting merger of the subspace into a d dimensional coordinate space a well a an reverse mapping defining the surface of the manifold in the sample space the intrinsic dimensionality d of the data manifold can be estimated from the growth process of point to point distance in analogy to differential geometry we call the subspace chart and their merger the connection section considers example problem where these method are used to untie knot unroll and untwist sheet and visualize video data background topology neutral nldr algorithm can be divided into those that compute mapping and 
we investigate improvement of adaboost that can exploit the fact that the weak hypothesis are one sided i e either all it positive or negative prediction are correct in particular for any set of m labeled example consistent with a disjunction of k literal which are one sided in this case adaboost construct a consistent hypothesis by using o k log m iteration on the other hand a greedy set covering algorithm find a consistent hypothesis of size o k log m our primary question is whether there is a simple boosting algorithm that performs a well a the greedy set covering we first show that infoboost a modification of adaboost proposed by aslam for a different purpose doe perform a well a the greedy set covering algorithm we then show that adaboost requires k log m iteration for learning k literal disjunction we achieve this with an adversary construction and a well a in simple experiment based on artificial data further we give a variant called semiboost that can handle the degenerate case when the given example all have the same label we conclude by showing that semiboost can be used to produce small conjunction a well 
abstract recently there have been several advance in the machine learning and pattern recognition community for developing manifold learning algo rithms to construct nonlinear low dimensional manifold from sample data point embedded in high dimensional space in this paper we de velop algorithm that address two key issue in manifold learning the adaptive selection of the neighborhood size and better fitting the local geometric structure to account for the variation in the curvature of the manifold and it interplay with the sampling density of the data set we also illustrate the effectiveness of our method on some synthetic data set 
abstract we consider the learning problem ofnding a dependency betweena general class of object and another possibly dierent generalclass of object the object can be for example vector image string tree or graph such a task is made possible by employingsimilarity measure in both input and output space using kernelfunctions thus embedding the object into vector space weexperimentally validate our approach on several task mappingstrings to string pattern 
linear model allow the description of a continuous symmetric response in term of a linear combination of predictor variable generalized linear model extend this framework to a wider range of response type including categorical binary and skewed continuous response this allows a common approach to statistical inference in the fitting and testing of such model a well a diagnostic checking 
a new family of kernel for statistical learning is introduc ed that exploit the geometric structure of statistical model base d on the heat equation on the riemannian manifold defined by the fisher inf ormation metric information diffusion kernel generalize the gaussian kernel of euclidean space and provide a natural way of combining generative statistical modeling with non parametric discriminative learning a a special case the kernel give a new approach to applying kernel based learning algorithm to discrete data bound on covering number for the new kernel are proved using spectral theory in differen tial geometry and experimental result are presented for text classificat ion 
we introduce a probabilistic model that generalizesclassical linear discriminant analysisand give an interpretation for the componentsas informative or relevant componentsof data the component maximize thepredictability of class distribution which isasymptotically equivalent to i maximizingmutual information with the class and ii nding principal component in the so calledlearning or fisher metric the fisher metricmeasures only distance that are relevantto the 
in this paper we consider tipping s relevance vector machi ne rvm and formalize an incremental training strategy a a vari ant of the expectation maximization em algorithm that we call subspace em working with a subset of active basis function the sparsit y of the rvm solution will ensure that the number of basis function and t hereby the computational complexity is kept low we also introduce a mean field approach to the intractable classification model that is exp ected to give a very good approximation to exact bayesian inference and contains the laplace approximation a a special case we test the algorithm on two large data set with example the result indicate that bayesian learning of large data set e g the mnist database is realistic 
we consider loopy belief propagation for approximate inference in probabilistic graphical model a limitation of the standard algorithm is that clique marginals are computed a if there were no loop in the graph to overcome this limitation we introduce fractional belief propagation fractional belief propagation is formulated in term of a family of approximate free energy which includes the bethe free energy and the naive mean field free a special case using the linear response correction of the clique marginals the scale parameter can be tuned simulation result illustrate the potential merit of the approach 
high dimensional collection of data occur in many application the attribute in such data set are typically considered to be unordered however in many case there is a natural total or partial order pr underlying the variable of the data set example of variable for which such order exist include term in document course in enrollment data and paleontological site in fossil data collection the observation in such application are flat unordered set however the data set respect the underlying ordering of the variable by this we mean that if a pr b pr c are three variable respecting the underlying ordering pr and both of variable a and c appear in an observation then up to noise level variable b also appears in this observation similarly if a pr a pr pr al pr ai is a longer sequence of variable we do not expect to see many observation for which there are index i j k such that ai and ak occur in the observation but aj doe not in this paper we study the problem of discovering fragment of order of variable implicit in collection of unordered observation we define measure that capture how well a given order agrees with the observed data we describe a simple and efficient algorithm for finding all the fragment that satisfy certain condition we also discus the sometimes necessary postprocessing for selecting only the best fragment of order also we relate our method with a sequencing approach that us a spectral algorithm and with the consecutive one problem we present experimental result on some real data set author list of database paper exam result data and paleontological data 
data cleaning method are used for finding duplicate within a file or across set of file this overview provides background on the fellegi sunter model of record linkage the fellegi sunter model provides an optimal theoretical classification rule fellegi and sunter introduced method for automatically estimating optimal parameter without training data that we extend to many real world situation 
current psychological theory of human causal learning and judgment focus primarily on long run prediction two by estimating parameter of a causal bayes net though for different parameterizations and a third through structural learning this paper focus on people s short run behavior by examining dynamical version of these three theory and comparing their prediction to a real world dataset 
due to the advance in positioning technology the real time information of moving object becomes increasingly available which ha posed new challenge to the database research a a long standing technique to identify overall distribution pattern in data clustering ha achieved brilliant success in analyzing static datasets in this paper we study the problem of clustering moving object which could catch interesting pattern change during the motion process and provide better insight into the essence of the mobile data point in order to catch the spatial temporal regularity of moving object and handle large amount of data micro clustering is employed efficient technique are proposed to keep the moving micro cluster geographically small important event such a the collision among moving micro cluster are also identified in this way high quality moving micro cluster are dynamically maintained which lead to fast and competitive clustering result at any given time instance we validate our approach with a through experimental evaluation where order of magnitude improvement on running time is observed over normal k mean clustering method 
this paper introduces correlated q ce q learning a multiagent q learning algorithm based on the correlated equilibrium ce solution concept ce q generalizes both nashq and friend and foe q in general sum game the set of correlated equilibrium contains the set of nash equilibrium in constantsum game the set of correlated equilibrium contains the set of minimax equilibrium this paper describes experiment with four variant of ce q demonstrating empirical convergence to equilibrium policy on a testbed of general sum markov game 
we present several new algorithm for multiagent reinforcementlearning a commonfeatureof these algorithm is a parameterized structured representation of a policy or value function this structure is leveraged in an approach we call coordinated reinforcement learning by which agent coordinate both their action selection activity and their parameter update within the limit of our parametric representation the agent will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space our method differ from many previous reinforcement learning approach to multiagent coordination in that structured communication and coordination between agent appears at the core of both the learning algorithm and the execution architecture our experimental result comparing our approach to other rl method illustrate both the quality of the policy obtained and the additional benefit of coordination 
principal component analysis pca is one of the most widely used technique in machine learning and data mining minor component analysis mca is le well known but can also play an important role in the presence of constraint on the data distribution in this paper we present a probabilistic model for extreme component analysis xca which at the maximum likelihood solution extract an optimal combination of principal and minor component for a given number of component the log likelihood of the xca model is guaranteed to be larger or equal than that of the probabilistic model for pca and mca we describe an efficient algorithm to solve for the globally optimal solution for log convex spectrum we prove that the solution consists of principal component only while for log concave spectrum the solution consists of minor component in general the solution admits a combination of both in experiment we explore the property of xca on some synthetic and real world datasets 
we introduce the community to a new construction principle whose practical implication are very broad central to this research is the idea of improving the presentation of algorithm in the literature and making them more appealing we dene a new notion of capacity for data set and derive a methodology for selecting from them our experiment demonstrate that even not so good algorithm can be shown signicantly better than competitor we present some experimental result which are very promising 
gaussian process are usually parameterised in term of their covariance function however this make it difficult to deal with multiple output because ensuring that the covariance matrix is positive definite is problematic an alternative formulation is to treat gaussian process a white noise source convolved with smoothing kernel and to parameterise the kernel instead using this we extend gaussian process to handle multiple coupled output 
abstract variable selection the process of identifying input variable that are relevant to a particular learning problem ha received much attention in the learning community method that employ a learning algorithm a a part of the selection process wrapper have been shown to outperform method that select variable independently from the learning algorithm filter but only at great compu tational expense we present a randomized wrapper algorithm whose computational requirement are within a constant factor of simply learning in the presence of all input variable provided that the number of relevant variable is small and known in advance we then show how to remove the latter assumption and demonstrate performance on several problem 
we formulate the problem of graph inference where part of the graph is known a a supervised learning problem and propose an algorithm to solve it the method involves the learning of a mapping of the vertex to a euclidean space where the graph is easy to infer and can be formulated a an optimization problem in a reproducing kernel hilbert space we report encouraging result on the problem of metabolic network reconstruction from genomic data 
we describe a probabilistic approach to the task of placing object described by high dimensional vector or by pairwise dissimilarity in a low dimensional space in a way that preserve neighbor identity a gaussian is centered on each object in the high dimensional space and the density under this gaussian or the given dissimilarity are used to define a probability distribution over all the potential neighbor of the object the aim of the embedding is to approximate this distribution a well a possible when the same operation is performed on the low dimensional image of the object a natural cost function is a sum of kullback leibler divergence one per object which lead to a simple gradient for adjusting the position of the low dimensional image unlike other dimensionality reduction method this probabilistic framework make it easy to represent each object by a mixture of widely separated low dimensional image this allows ambiguous object like the document count vector for the word bank to have version close to the image of both river and finance without forcing the image of outdoor concept to be located close to those of corporate concept 
why are sensory modality segregated the way they are in this paper we show that sensory modality are well designed for self supervised cross modal learning using the minimizing disagreement algorithm on an unsupervised speech categorization task with visual moving lip and auditory sound signal input we show that very informative auditory dimension actually harm performance when moved to the visual side of the network it is better to throw them away than to consider them part of the visual input we explain this finding in term of the statistical structure in sensory input 
boosting algorithm and successful application thereof abound for classification and regression learning problem but not for unsupervised learning we propose a sequential approach to adding feature to a random field model by training them to improve classification performance between the data and an equal sized sample of negative example generated from the model s current estimate of the data density training in each boosting round proceeds in three stage first we sample negative example from the model s current boltzmann distribution next a feature is trained to improve classification performance between data and negative example finally a coefficient is learned which determines the importance of this feature relative to one already in the pool negative example only need to be generated once to learn each new feature the validity of the approach is demonstrated on binary digit and continuous synthetic data 
considerable progress wa recently achieved on semi supervised learning which diers from the traditional supervised learning by additionally exploring the information of the unlabelled example however a disadvantage of many existing method is that it doe not generalize to unseen input this paper investigates learning method that eectively make use of both labelled and unlabelled data to build predictive function which are defined on not just the seen input but the whole space a a nice property the proposed method allows ecient training and can easily handle new test point we validate the method based on both toy data and real world data set 
we consider the problem of recovering an underwater image distorted by surface wave a large amount of video data of the distorted image is acquired the problem is posed in term of finding an undistor ted image patch at each spatial location this challenging reconstruction task can be formulated a a manifold learning problem such that the center of the manifold is the image of the undistorted patch to compute the center we present a new technique to estimate global distance on the manifold our technique achieves robustness through convex flow computation and solves the leakage problem inherent in recent manifold embedding technique 
we show two related thing given a classier which consists of a weighted sum of featureswith a large margin we can construct a stochastic classier withnegligibly larger training error rate the stochastic classier hasa future error rate bound that depends on the margin distributionand is independent of the size of the base hypothesis class 
in this paper we propose a novel method for learning a mahalanobis distance measure to be used in the knn classification algorit hm the algorithm directly maximizes a stochastic variant of the le ave one out knn score on the training set it can also learn a low dimensional linear embedding of labeled data that can be used for data visualization and fast classification unlike other method our classific ation model is non parametric making no assumption about the shape of the class distribution or the boundary between them the performance of the method is demonstrated on several data set both for metric learning and linear dimensionality reduction 
many problem in information processing involve some form of dimensionality reduction in this paper we introduce locality preserving projection lpp these are linear projective map that arise by solving a variational problem that optimally preserve the neighborhood structure of the data set lpp should be seen a an alternative to principal component analysis pca a classical linear technique that project the data along the direction of maximal variance when the high dimensional data lie on a low dimensional manifold embedded in the ambient space the locality preserving projection are obtained by finding the optimal linear approximation to the eigenfunctions of the laplace beltrami operator on the manifold a a result lpp share many of the data representation property of nonlinear technique such a laplacian eigenmaps or locally linear embedding yet lpp is linear and more crucially is defined everywhere in ambient space rather than just on the training data point this is borne out by illustrative example on some high dimensional data set 
spectral clustering refers to a class of technique which re ly on the eigenstructure of a similarity matrix to partition point into di sjoint cluster with point in the same cluster having high similarity and point in different cluster having low similarity in this paper we der ive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem minimizing this cost function with respect to the partition lead to a new spectral clustering algorithm minimizing with respect to the similarity matrix lead to an algorithm f or learning the similarity matrix we develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors 
we address in this paper the question of how the knowledge of the marginal distribution p x can be incorporated in a learning algorithm we suggest three theoretical method for taking into account this distribution for regularization and provide link to existing graph based semi supervised learning algorithm we also propose practical implementation 
the similarity between object is a fundamental element of many learning algorithm most non parametric method take this similarity to be fixed but much recent work ha shown the advantage of learni ng it in particular to exploit the local invariance in the data or to capture the possibly non linear manifold on which most of the data lie we propose a new non parametric kernel density estimation method which capture the local structure of an underlying manifold through the le ading eigenvectors of regularized local covariance matrix experim ents in density estimation show significant improvement with respect to pa rzen density estimator the density estimator can also be used within bayes classifier yielding classification rate similar to svms and much superior to the parzen classifier 
we describe a way of using multiple different type of similarity relationship to learn a low dimensional embedding of a dataset our method chooses different possibly overlapping representation of similarity by individually reweighting the dimension of a common underlying latent space when applied to a single similarity relation that is based on euclidean distance between the input data point the method reduces to simple dimensionality reduction if additional information is available about the dataset or about subset of it we can use this information to clean up or otherwise improve the embedding we demonstrate the potential usefulness of this form of semi supervised dimensionality reduction on some simple example 
monaural speech separation ha been studied in previous system that incorporate auditory scene analysis principle a major problem for these system is their inability to deal with speech in the highfrequency range psychoacoustic evidence suggests that different perceptual mechanism are involved in handling resolved and unresolved harmonic motivated by this we propose a model for monaural separation that deal with low frequency and highfrequency signal differently for resolved harmonic our model generates segment based on temporal continuity and cross channel correlation and group them according to periodicity for unresolved harmonic the model generates segment based on amplitude modulation am in addition to temporal continuity and group them according to am repetition rate derived from sinusoidal modeling underlying the separation process is a pitch contour obtained according to psychoacoustic constraint our model is systematically evaluated and it yield substantially better performance than previous system especially in the high frequency range 
learning from structured data is becoming increasingly important however most prior work on kernel method ha focused on learning from attribute value data only recently research started investigating kernel for structured data this paper considers kernel for multi instance problem a class of concept on individual represented by set the main result of this paper is a kernel on multi instance data that can be shown to separate positive and negative set under natural assumption this kernel compare favorably with state of the art multi instance learning algorithm in an empirical study finally we give some concluding remark and propose future work that might further improve the result 
we propose a new method for clustering based on finding maximum mar gin hyperplanes through data by reformulating the problem in term of the implied equivalence relation matrix we can pose the problem a a convex integer program although this still yield a difficult com putational problem the hard clustering constraint can be relaxed to a soft clustering formulation which can be feasibly solved with a semidef inite program since our clustering technique only depends on the data through the kernel matrix we can easily achieve nonlinear clustering in the same manner a spectral clustering experimental result show that our maximum margin clustering technique often obtains more accurate result than conventional clustering method the real benefit of our ap proach however is that it lead naturally to a semi supervised training method for support vector machine by maximizing the margin simul taneously on labeled and unlabeled training data we achieve state of the art performance by using a single integrated learning principle 
we generalise the gaussian process gp framework for regression by learning a nonlinear transformation of the gp output this allows for non gaussian process and non gaussian noise the learning algorithm chooses a nonlinear transformation such that transformed data is well modelled by a gp this can be seen a including a preprocessing transformation a an integral part of the probabilistic modelling problem rather than a an ad hoc step we demonstrate on several real regression problem that learning the transformation can lead to significantly better performance than using a regular gp or a gp with a fix ed transformation 
a general linear response method for deriving improved estimate of correlation in the variational bayes framework is presented three application are given and it is discussed how to use linear response a a general principle for improving mean field approximation 
we describe a visualization technique that us brushed parallel histogram to aid in understanding concept drift in multidimensional problem space this technique illustrates the relationship between change in distribution of multiple antecedent feature value and the outcome distribution we can also observe effect on the relative utilization of predictive rule our parallel histogram technique solves the over plotting difficulty of parallel coordinate graph and the difficulty of comparing distribution of brushed and original data we demonstrate our technique s usefulness in understanding concept drift in power demand and stock investment return 
in we introduced a linear statistical model of joint color change in image due to variation in lighting and certain non geometric camera parameter we did this by measuring the mapping of color in one image of a scene to color in another image of the same scene under different lighting condition here we increase the flexibility of this color flow model by allowing flow coefficient to vary according to a low order polynomial over the image this allows u to better fit smoothly varying lighting condition a well a curved surface without endowing our model with too much capacity we show result on image matching and shadow removal and detection addressing the variability of image due to these photic parameter ha been an important problem in machine vision we distinguish photic parameter from geometric parameter such a camera orientation or blurring that affect which part of the scene a particular pixel represents we also note that photic parameter are more general than lighting parameter and include anything which affect the final rgb value in an image given that the geometric parameter and the object in the scene have been fixed we present a statistical linear model of color change space that is learned by observing how the color in static image change jointly under common naturally occurring lighting change such a model can be used for a number of task including synthesis of image of new object under different lighting condition image matching and shadow detection result for each of these task will be reported several aspect of our model merit discussion first it is obtained from video data in a completely unsupervised fashion the model us no prior knowledge of lighting condition surface reflectance or other parameter during data collection and modeling it also ha no built in knowledge of the physic of image acquisition or typical image color 
although discriminatively trained classifier are usually more accurate when labeled training data is abundant previous work ha shown that when training data is limited generative classifier can ou t perform them this paper describes a hybrid model in which a high dimensional subset of the parameter are trained to maximize generative likelihood and another small subset of parameter are discriminatively trained to maximize conditional likelihood we give a sample complexity bound showing that in order to fit the discriminative parameter we ll the number of training example required depends only on the logarithm of the number of feature occurrence and feature set size experimental result show that hybrid model can provide lower test error and can produce better accuracy coverage curve than either their purely g enerative or purely discriminative counterpart we also discus several advantage of hybrid model and advocate further work in this area 
this work present an architecture based on perceptrons to recognize phrase structure and an online learning algorithm to train the perceptrons together and dependently the recognition strategy applies learning in two layer a filtering layer which reduces the search space by identifying plausible phrase candidate and a ranking layer which recursively build the optimal phrase structure we provide a recognition based feedback rule which reflects to each local function it committed error from a global point of view and allows to train them together online a perceptrons experimentation on a syntactic parsing problem the recognition of clause hierarchy improves state of the art result and evinces the advantage of our global training method over optimizing each function locally and independently 
abstract we consider a learning setting in which thereare well dened relation that exist amonginstances of certain class in particular we consider the domain of predicting varioustypes of gene regulation element in bacterialgenomes given instance of one class wecan often acquire weakly labeled quot trainingdata for another class by taking advantage ofknown relationship that exist between thetwo class the example are weakly labeledin that either the class label is 
we present an algorithm for unsupervised learning and semantic classification of name and term given a small number of seed example and an unlabeled training corpus the algorithm learns pattern that identify more example in a bootstrapping cycle multiple class are learned simultaneously including negative class that serve to provide negative example for the target class we apply the algorithm to text from several domain in english and chinese 
detection of near duplicate document is an important problem in many data mining and information filtering application when faced with massive quantity of data traditional duplicate detection technique relying on direct inter document similarity computation e g using the cosine measure are often not feasible given the time and memory performance constraint on the other hand fingerprint based method such a i match are very attractive computationally but may be brittle with respect to small change to document content we focus on approach to near replica detection that are based upon large collection statistic and present a general technique of increasing their robustness via multiple lexicon randomization in experiment with large web page and spam email datasets the proposed method is shown to consistently outperform traditional i match with the relative improvement in duplicate document recall reaching a high a the large gain in detection accuracy are offset by only small increase in computational requirement 
we consider an mdp setting in which the reward function is allowed to change during each time step of play possibly in an adversarial manner yet the dynamic remain fixed similar to the expert setting we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time we provide efficient algorithm which have regret bound with no dependenceon the size of state space instead these bound depend only on a certain horizon time of the process and logarithmically on the number of action we also show that in the case that the dynamic change over time the problem becomes computationally hard 
collaborative and content based filtering are two paradigm that have been applied in the context of recommender system and user preference prediction this paper proposes a novel unified approach that systematically integrates all available training information such a past user item rating a well a attribute of item or user to learn a prediction function the key ingredient of our method is the design of a suitable kernel or similarity function between user item pair that allows simultaneous generalization across the user and item dimension we propose an on line algorithm jrank that generalizes perceptron learning experimental result on the eachmovie data set show significant improvement over standard approach 
cross entropy and mean squared error are typical cost function used to optimize classifier performance the goal of the optimization is usually to achieve the best correct classification rate however for many two class real world problem the roc curve is a more meaningful performance measure we demonstrate that minimizing cross entropy or mean squared error doe not necessarily maximize the area under the roc curve auc we then consider alternative objective function for training a 
we e xplore dynamic shaping to integrate our prior belief of the final policy into a conventional reinforcement learning system shaping provides a positive or negative artificial increment t o the native task reward in order to encourage or discourage behavior previously shaping function have been static the additional reward do not vary with experience but some prior knowledge ca nnot be e xpressed a s tatic shaping we take an explanation based approach in which the specific shaping function emerges from initial experience with the world we compare no shaping static shaping and dynamic shaping in the task of learning bipedal walking on a simulator we e mpirically evaluate the convergence rate a nd final performance a mong these c onditions while varying the acc uracy of the prior knowledge we c onclude that i n the appropriate context dynamic shaping can greatly improve the learning of action policy 
hidden variable evolving over time appear in multiple setting where it is valuable to recover them typically from observed sum our driving application is network tomography where we need to estimate the origin destination od traffic flow to determine e g who is communicating with whom in a local area network this information allows network engineer and manager to solve problem in design routing configuration debugging monitoring and pricing unfortunately the direct measurement of the od traffic is usually difficult or even impossible instead we can easily measure the load on every link that is sum of desirable od flow in this paper we propose i filter a method to solve this problem which improves the state of the art by a introducing explicit time dependence and by b using realistic non gaussian marginals in the statistical model for the traffic flow a never attempted before we give experiment on real data where i filter scale linearly with new observation and out performs the best existing solution in a wide variety of setting specifically on real network traffic measured at cmu and at at t i filter reduced the estimation error between and in all case 
the banking industry regularly mount campaign to improve customer value by offering new product to existing customer in recent year this approach ha gained significant momentum because of the increasing availability of customer data and the improved analysis capability in data mining typically response model based on historical data are used to estimate the probability of a customer purchasing an additional product and the expected return from that additional purchase even with these computational improvement and accurate model of customer behavior the problem of efficiently using marketing resource to maximize the return on marketing investment is a challenge this problem is compounded because of the capability to launch multiple campaign through several distribution channel over multiple time period the combination of alternative creates a complicated array of possible action this paper present a solution that answer the question of what product if any to offer to each customer in a way that maximizes the marketing return on investment the solution is an improvement over the usual approach of picking the customer that have the largest expected value for a particular product because it is a global maximization from the viewpoint of the bank and allows for the effective implementation of business constraint across customer and business unit the approach account for limited resource multiple sequential campaign and other business constraint furthermore the solution provides insight into the cost of these constraint in term of decreased profit and thus is an effective tool for both tactical campaign execution and strategic planning 
in this paper we address the problem of statistical learnin g for multitopic text categorization mtc whose goal is to choose all relevant topic a label from a given set of topic the proposed algorithm maximal margin labeling mml treat all possible label a independent class and learns a multi class classifier on the induced mu lti class categorization problem to cope with the data sparseness caused by the huge number of possible label mml combine some prior knowledge about label prototype and a maximal margin criterion in a novel way experiment with multi topic web page show that mml outperforms existing learning algorithm including support vector machine 
is it feasible to train classifier to decode the cognitive state of a human subject based on single episode fmri data if so these trained classifier could be used a virtual sensor to detect hidden cognitive state of a subject providing a key tool for experimental research in cognitive science and in diagnosis of mental process in patient with brain injury whereas much work ha been done on fmri data analysis method that average together data collected from repeated stimulus over multiple episode little is known about the feasibility of training classifier to decode cognitive state from single episode this paper present several case study in which we have successfully trained such classifier we explore the technical issue involved in training such single episode classifier and discus area for future research these case study include training a classifier to determine which of twelve semantic category of word is being read by a human subject e g a word describing animal or one describing building whether or not a subject find a sentence ambiguous and whether the subject is looking at a picture or at a sentence describing a picture 
de novo sequencing of peptide is a challenging task in proteome research while there exist reliable dna sequencing method the highthroughput de novo sequencing of protein by mass spectrometry is still an open problem current approach suffer from a lack in precision to detect mass peak in the spectrogram in this paper we present a novel method for de novo peptide sequencing based on a hidden markov model experiment effectively demonstrate that this new method significantly outperforms standard approach in matching quality 
many automated learning procedure lack interpretability operating effectively a a black box providing a prediction tool but no explanation of the underlying dynamic that drive it a common approach to interpretation is to plot the dependence of a learned function on one or two predictor we present a method that seek not to display the behavior of a function but to evaluate the importance of non additive interaction within any set of variable should the function be close to a sum of low dimensional component these component can be viewed and even modeled parametrically alternatively the work here provides an indication of where intrinsically high dimensional behavior take place the calculation used in this paper correspond closely with the functional anova decomposition a well developed construction in statistic in particular the proposed score of interaction importance measure the loss associated with the projection of the prediction function onto a space of additive model the algorithm run in linear time and we present display of the output a a graphical model of the function for interpretation purpose 
segmentation and recognition have long been treated a two separate process we propose a mechanism based on spectral graph partitioning that readily combine the two process into one a part based recognition system detects object patch supply their partial segmentation and knowledge about the spatial configuration of the object the goal of patch grouping is to find a set of patch that conform best to the object configuration this process is integrated with the pixel grouping based on low level feature similarity through pixel patch interaction and patch competition that is encoded a constraint in the solution space the globally optimal partition is obtained by solving a constrained eigenvalue problem we demonstrate that the resulting object segmentation eliminates local false positive at the high level of part detection while overcoming occlusion and weak contour at the low level of edge detection 
many inference and optimization task in machine learning can be solved by sampling approach such a markov chain monte carlo mcmc and simulated annealing these method can be slow if a single target density query requires many run of a simulation or a complete sweep of a training data set we introduce a hierarchy of mcmc sampler that allow most step to be taken in the solution space using only a small sample of simulation run or training example this is shown to accelerate learning in a policy search optimization task 
the basic tool of machine learning appear in the inner loop of most reinforcement learning algorithm typically in the form of monte carlo method or function approximation technique to a large extent however current reinforcement learning algorithm draw upon machine learning technique that are at least ten year old and with a few exception very little ha been done to exploit recent advance in classification learning for the purpose of reinforcement learning we use a variant of approximate policy iteration based on rollouts that allows u to use a pure classification learner such a a support vector machine svm in the inner loop of the algorithm we argue that the use of svms particularly in combination with the kernel trick can make it easier to apply reinforcement learning a an outof the box technique without extensive feature engineering our approach open the door to modern classification method but doe not preclude the use of classical method we present experimental result in the pendulum balancing and bicycle riding domain using both svms and neural network for classifier 
is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in taking the case of space for example is there a way for this algorithm to realize that it body is in a three dimensional world is it possible for this algorithm to discover how to move in a straight line and more basically do these question make any sense at all given that the algorithm only ha access to the very high dimensional data consisting of it sensory input and motor output we demonstrate in this article how these question can be given a positive answer we show that it is possible to make an algorithm that by analyzing the law that link it motor output to it sensory input discovers information about the structure of the world regardless of the device constituting the body it is linked to we present result from simulation demonstrating a way to issue motor order resulting in fundamental movement of the body a regard the structure of the physical world 
thanks to it increasing availability electronic literature can now be a major source of information when developing complex statistical model where data is scarce or contains much noise this raise the question of how to integrate information from domain literature with statistical data because quantifying similarity or dependency between variable is a basic building block in knowledge discovery we consider here the following question which vector representation of text and which statistical score of similarity or dependency support best the use of literature in statistical model for the text source we assume to have annotation for the domain variable a short free text description and optionally to have a large literature repository from which we can further expand the annotation for evaluation we contrast the variable similarity or dependency obtained from text using different annotation source and vector representation with those obtained from measurement data or expert assessment specifically we consider two learning problem clustering and bayesian network learning firstly we report performance against an expert reference for clustering yeast gene from textual annotation secondly we ass the agreement between text based and data based score of variable dependency when learning bayesian network substructure for the task of modeling the joint distribution of clinical measurement of ovarian tumor 
in applying hidden markov model to the analysis of massive data stream it is often necessary to use an artiflcially reduced set of state this is due in large part to the fact that the basic hmm estimation algorithm have a quadratic dependence on the size of the state set we present algorithm that reduce this computational bottleneck to linear or near linear time when the state can be embedded in an underlying grid of parameter this type of state representation arises in many domain in particular we show an application to tra c analysis at a high volume web site 
this paper investigates the effect of kernel principal component analysis kpca within the classification framework essentially the regularization property of this dimensionality reduction method kpca ha been previously used a a pre processing step before applying an svm but we point out that this method is somewhat redundant from a regularization point of view and we propose a new algorithm called kernel projection machine to avoid this redundancy based on an analogy with the statistical framework of regression for a gaussian white noise model preliminary experimental result show that this algorithm reach the same performance a an svm 
motor control depends on sensory feedback in multiple modality with different latency in this paper we consider within the framework of reinforcement learning how different sensory modality can be combined and selected for real time optimal movement control we propose an actor critic architecture with multiple module whose output are combined using a softmax function we tested our architecture in a simulation of a sequential reaching task reaching wa initially guided by visual feedback with a long latency our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory in simulation with different latency for visual and somatosensory feedback we found that the agent depended more on feedback with shorter latency 
in many real world classification problem the input contains a large number of potentially irrelevant feature this paper proposes a new bayesian framework for determining the relevance of input feature this approach extends one of the most successful bayesian method for feature selection and sparse learning known a automatic relevance determination ard ard find the relevance of feature by optimizing the model marginal likelihood also known a the evidence we show that this can lead to overfitting to address this problem we propose predictive ard based on estimating the predictive performance of the classifier while the actual leave one out predictive performance is generally very costly to compute the expectation propagation ep algorithm proposed by minka provides an estimate of this predictive performance a a side effect of it iteration we exploit this in our algorithm to do feature selection and to select data point in a sparse bayesian kernel classifier moreover we provide two other improvement to previous algorithm by replacing laplace s approximation with the generally more accurate ep and by incorporating the fast optimization algorithm proposed by faul and tipping our experiment show that our method based on the ep estimate of predictive performance is more accurate on test data than relevance determination by optimizing the evidence 
procedure for collective inference make simultaneous statistical judgment about the same variable for a set of related data instance for example collective inference could be used to simultaneously classify a set of hyperlinked document or infer the legitimacy of a set of related financial transaction several recent study indicate that collective inference can significantly reduce classification error when compared with traditional inference technique we investigate the underlying mechanism for this error reduction by reviewing past work on collective inference and characterizing different type of statistical model used for making inference in relational data we show important difference among these model and we characterize the necessary and sufficient condition for reduced classification error based on experiment with real and simulated data 
the detection and pose estimation of people in image and video is made challenging by the variability of human appearance the complexity of natural scene and the high dimensionality of articulated body model to cope with these problem we represent the d human body a a graphical model in which the relationship between the body part are represented by conditional probability distribution we formulate the pose estimation problem a one of probabilistic inference over a graphical model where the random variable correspond to the individual limb parameter position and orientation because the limb are described by dimensional vector encoding pose in space discretization is impractical and the random variable in our model must be continuousvalued to approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter this framework facilitates the automatic initialization of the body model from low level cue and is robust to occlusion of body part and scene clutter 
this paper present a novel hidden markov model architecture to model the joint probability of pair of asynchronous sequence describing the same event it is based on two other markovian model namely asynchronous input output hidden markov model and pair hidden markov model an em algorithm to train the model is presented a well a a viterbi decoder that can be used to obtain the optimal state sequence a well a the alignment between the two sequence the model ha been tested on an audio visual speech recognition task using the m vt database and yielded robust performance under various noise condition 
certain simple image are known to trigger a percept of transparency the input image i is perceived a the sum of two image i x y i x y i x y this percept is puzzling first why dowechoosethe morecomplicated descriptionwithtwoimages ratherthanthe simpler explanation i x y i x y second giventheinflnitenumberofwaystoexpress i asasumoftwo image how do we compute the best decomposition herewesuggestthattransparencyistherationalperceptofasystemthatisadaptedtothestatisticsofnaturalscenes wepresent a probabilistic model of image based on the qualitative statistic of derivative fllters and corner detector in natural scene and usethismodeltoflndthemostprobabledecompositionofanovel image the optimization is performed using loopy belief propagation we show that our model computes perceptually correct decomposition on synthetic image and discus it application to real image 
classifier learning method commonly assume that the training data consist of randomly drawn example from the same distribution a the test example about which the learned model is expected to make prediction in many practical situation however this assumption is violated in a problem known in econometrics a sample selection bias in this paper we formalize the sample selection bias problem in machine learning term and study analytically and experimentally how a number of well known classifier learning method are affected by it we also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias 
statistical language model estimate the probability of a word occurring in a given context the most common language model rely on a discrete enumeration of predictive context e g n gram and consequently fail to capture and exploit statistical regularity across these context in this paper we show how to learn hierarchical distributed representation of word context that maximize the predictive value of a statistical language model the representation are initialized by unsupervised algorithm for linear and nonlinear dimensionality reduction then fed a input into a hierarchical mixture of expert where each expert is a multinomial distribution over predicted word while the distributed representation in our model are inspired by the neural probabilistic language model of bengio et al our particular architecture enables u to work with significantly larger vocabulary and training corpus for example on a large scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentence we demonstrate consistent improvement over class based bigram model we also discus extension of our approach to longer multiword context 
the standard model of supervised learning assumes that training and test data are drawn from the same underlying distribution this paper explores an application in which a second auxiliary source of data is available drawn from a different distribution this auxiliary data is more plentiful but of significantly lower quality than the training and test data in the svm framework a training example ha two role a a a data point to constrain the learning process and b a a candidate support vector that can form part of the definition of the classifier the paper considers using the auxiliary data in either or both of these role this auxiliary data framework is applied to a problem of classifying image of leaf of maple and oak tree using a kernel derived from the shape of the leaf experiment show that when the training data set is very small training with auxiliary data can produce large improvement in accuracy even when the auxiliary data is significantly different from the training and test data the paper also introduces technique for adjusting the kernel score of the auxiliary data point to make them more comparable to the training data point 
support vector machine and other kernel method have proven to be very effective for nonlinear inference practical issue are how to select the type of kernel including any parameter and how to deal with the computational issue caused by the fact that the kernel matrix grows quadratically with the data inspired by ensemble and boosting method like mart we propose the multiple additive regression kernel mark algorithm to address these issue mark considers a large potentially infinite library of kernel matrix formed by different kernel function and parameter using gradient boosting column generation mark construct column of the heterogeneous kernel matrix the base hypothesis on the fly and then add them into the kernel ensemble regularization method such a used in svm kernel ridge regression and mart are used to prevent overfitting we investigate how mark is applied to heterogeneous kernel ridge regression the resulting algorithm is simple to implement and efficient kernel parameter selection is handled within mark sampling and weak kernel are used to further enhance the computational efficiency of the resulting additive algorithm the user can incorporate and potentially extract domain knowledge by restricting the kernel library to interpretable kernel mark compare very favorably with svm and kernel ridge regression on several benchmark datasets 
we describe method for taking into account unlabeled data in the training of a kernel based classifier such a a support vector machine svm we propose two approach utilizing unlabeled point in the vicinity of labeled one both of the approach effectively modify the metric of the pattern space either by using nonspherical gaussian density estimate which are determined using em or by modifying the kernel function using displacement vector computed from pair of unlabeled and labeled point the latter is linked to technique for training invariant svms we present experimental result indicating that the proposed technique can lead to substantial improvement of classification accuracy 
while clustering is usually an unsupervised operation there are circumstance in which we believe with varying degree of certainty that item a and b should be assigned to the same cluster while item a and c should not we would like such pairwise relation to influence cluster assignment of out of sample data in a manner consistent with the prior knowledge expressed in the training set our starting point is probabilistic clustering based on gaussian mixture model gmm of the data distribution we express clustering preference in the prior distribution over assignment of data point to cluster this prior penalizes cluster assignment according to the degree with which they violate the preference we fit the model parameter with em experiment on a variety of data set show that ppc can consistently improve clustering result 
decision tree construction is a well studied problem in data mining recently there ha been much interest in mining streaming data domingo and hulten have presented a one pas algorithm for decision tree construction their work us hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed in this paper we revisit this problem we make the following two contribution we present a numerical interval pruning nip approach for efficiently processing numerical attribute our result show an average of reduction in execution time we exploit the property of the gain function entropy and gini to reduce the sample size required for obtaining a given bound on the accuracy our experimental result show a reduction in the number of data instance required 
data mining with bayesian network learning ha two important characteristic under condition learned edge between variable correspond to casual influence and second for every variable t in the network a special subset markov blanket identifiable by the network is the minimal variable set required to predict t however all known algorithm learning a complete bn do not scale up beyond a few hundred variable on the other hand all known sound algorithm learning a local region of the network require an exponential number of training instance to the size of the learned region the contribution of this paper is two fold we introduce a novel local algorithm that return all variable with direct edge to and from a target variable t a well a a local algorithm that return the markov blanket of t both algorithm i are sound ii can be run efficiently in datasets with thousand of variable and iii significantly outperform in term of approximating the true neighborhood previous state of the art algorithm using only a fraction of the training size required by the existing method a fundamental difference between our approach and existing one is that the required sample depends on the generating graph connectivity and not the size of the local region this yield up to exponential saving in sample relative to previously known algorithm the result presented here are promising not only for discovery of local causal structure and variable selection for classification but also for the induction of complete bns 
in this paper we propose a new information theoretic divisive algorithm for word clustering applied to text classification in previous work such distributional clustering of feature ha been found to achieve improvement over feature selection in term of classification accuracy especially at lower number of feature however the existing clustering technique are agglomerative in nature and result in i sub optimal word cluster and ii high computational cost in order to explicitly capture the optimality of word cluster in an information theoretic framework we first derive a global criterion for feature clustering we then present a fast divisive algorithm that monotonically decrease this objective function value thus converging to a local minimum we show that our algorithm minimizes the within cluster jensen shannon divergence while simultaneously maximizing the between cluster jensen shannon divergence in comparison to the previously proposed agglomerative strategy our divisive algorithm achieves higher classification accuracy especially at lower number of feature we further show that feature clustering is an effective technique for building smaller class model in hierarchical classification we present detailed experimental result using naive bayes and support vector machine on the newsgroups data set and a level hierarchy of html document collected from dmoz open directory 
we study the problem of modeling specie geographic distribution a critical problem in conservation biology we propose the use of maximum entropy technique for this problem specifically sequential update algorithm that can handle a very large number of feature we describe experiment comparing maxent with a standard distribution modeling tool called garp on a dataset containing observation data for north american breeding bird we also study how well maxent performs a a function of the number of training example and training time analyze the use of regularization to avoid overfitting when the number of example is small and explore the interpretability of model constructed using maxent 
neural network are successful in acquiring hidden knowledge in datasets their biggest weakness is that the knowledge they acquire is represented in a form not understandable to human researcher tried to address this problem by extracting rule from trained neural network most of the proposed rule extraction method required specialized type of neural network some required binary input and some were computationally expensive craven proposed extracting mofn type decision tree from neural network we believe mofn type decision tree are only good for mofn type problem and tree created for regular high dimensional real world problem may be very complex in this paper we introduced a new method for extracting regular c like decision tree from trained neural network we showed that the new method dectext is effective in extracting high fidelity tree from trained network we also introduced a new discretization technique to make dectext be able to handle continuous feature and a new pruning technique for finding simplest tree with the highest fidelity 
this paper address the problem of finding a small and coherent subset of point in a given data this problem sometimes referred to a one class or set covering requires to find a small radius ball that cover a many data point a possible it rise naturally in a wide range of application from finding gene module to extracting document topic where many data point are irrelevant to the task at hand or in application where only positive example are available most previous approach to this problem focus on identifying and discarding a possible set of outlier in this paper we adopt an opposite approach which directly aim to find a small set of coherently structured region by using a loss function that focus on local property of the data we formalize the learning task a an optimization problem using the information bottleneck principle an algorithm to solve this optimization problem is then derived and analyzed experiment on gene expression data and a text document corpus demonstrate the merit of our approach 
we describe a sparse bayesian regression method for recovering d human body motion directly from silhouette extracted from monocular video sequence no detailed body shape model is needed and realism is ensured by training on real human motion capture data the tracker estimate d body pose by using relevance vector machine regression to combine a learned autoregressive dynamical model with robust shape descriptor extracted automatically from image silhouette we studied several different combination method the most effective being to learn a nonlinear observation update correction based on joint regression with respect to the predicted state and the observation we demonstrate the method on a parameter full body pose model both quantitatively using motion capture based test sequence and qualitatively on a test video sequence 
surrogate maximization or minimization sm algorithm are a family of algorithm that can be regarded a a generalization of expectation maximization em algorithm there are three major approach to the construction of surrogate function all relying on the convexity of some function in this paper we solve the boosting problem by proposing sm algorithm for the corresponding optimization problem specifically for adaboost we derive an sm algorithm that can be shown to be identical to the algorithm proposed by collins et al based on bregman distance more importantly for logitboost or logistic boosting we use several method to construct different surrogate function which result in different sm algorithm by combining multiple method we are able to derive an sm algorithm that is also the same a an algorithm derived by collins et al our approach based on sm algorithm is much simpler and convergence result follow naturally 
the class transduction problem a formulated by vapnik involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test point in this form the problem ha exponential computational complexity in the size of the working set so far it ha been attacked by mean of integer programming technique that do not scale to reasonable problem size or by local search procedure in this paper we present a relaxation of this task based on semidefinite programming sdp resulting in a convex optimization problem that ha polynomial complexity in the size of the data set the result are very encouraging for mid sized data set however the cost is still too high for large scale problem due to the high dimensional search space to this end we restrict the feasible region by introducing an approximation based on solving an eigenproblem with this approximation the computational cost of the algorithm is such that problem with more than point can be treated 
recently interest is growing to develop an effective communication interface connecting the human brain to a computer the brain computer interface bci one motivation of bci research is to provide a new communication channel substituting normal motor output in patient with severe neuromuscular disability in the last decade various neurophysiological cortical process such a slow potential shift movement related potential mrps or event related desynchronization erd of spontaneous eeg rhythm were shown to be suitable for bci and consequently different independent approach of extracting bci relevant eeg feature for single trial analysis are under investigation here we present and systematically compare several concept for combining such eeg feature to improve the single trial classification feature combination are evaluated on movement imagination experiment with subject where eeg feature are based on either mrps or erd or both those combination method that incorporate the assumption that the single eeg feature are physiologically mutually independent outperform the plain method of adding evidence where the single feature vector are simply concatenated these result strengthen the hypothesis that mrp and erd reflect at least partially independent aspect of cortical process and open a new perspective to boost bci effectiveness 
time difference of arrival tdoa is commonly used to estimate the azimuth of a source in a microphone array the most common method to estimate tdoa are based on finding extremum in generalized crosscorrelation waveform in this paper we apply microphone array technique to a manikin head by considering the entire cross correlation waveform we achieve azimuth prediction accuracy that exceeds extremum locating method we do so by quantizing the azimuthal angle and treating the prediction problem a a multiclass categorization task we demonstrate the merit of our approach by evaluating the various approach on sony s aibo robot 
the volume of mass unsolicited electronic mail often known a spam ha recently increased enormously and ha become a serious threat to not only the internet but also to society this paper proposes a new spam detection method which us document space density information although it requires extensive e mail traffic to acquire the necessary information an unsupervised learning engine with a short white list can achieve a recall rate and precision a direct mapped cache method contributes handling of over e mail per second experimental result which were conducted using over million actual e mail of traffic are also reported in this paper 
we present and empirically test a novel approach for categorizing d free form object shape represented by range data in contrast to traditional surface signature based system that use alignment to match specific object we adapted the newly introduced symbolicsignature representation to classify deformable shape our approach construct an abstract description of shape class using an ensemble of classifier that learn object class part and their corresponding geometrical relationship from a set of numeric and symbolic descriptor we used our classification engine in a series of discrimination experiment on two well defined class that share many common distinctive feature the experimental result suggest that our method is to the best of our knowledge the first capable of classifying shape class that are difficult to discriminate by human standard 
this paper explores the problem of how to construct lazy decision tree ensemble we present and empirically evaluate a relevancebased boosting style algorithm that build a lazy decision tree ensemble customized for each test instance from the experimental result we conclude that our boosting style algorithm signicantly improves the performance of the base learner an empirical comparison to boosted regular decision tree show that ensemble of lazy decision tree achieve comparable accuracy and better comprehensibility we also introduce a novel distance based pruning strategy for the lazy decision tree algorithm to address the problem of over tting our experiment show that the pruning strategy improves the accuracy and comprehensibility of both single lazy decision tree and boosted ensemble 
what determines the caliber of axonal branch we pursue the hypothesis that the axonal caliber ha evolved to m inimize signal propagation delay while keeping arbor volume to a minimum we show that for a general cost function the optimal d iameters of mother d and daughter d d branch at a bifurcation obey 
successful application of reinforcement learning algorithm often involves considerable hand crafting of the necessary non linear feature to reduce the complexity of the value function and hence to promote convergence of the algorithm in contrast the human brain readily and autonomously find the complex feature when provided with sufficient training recent work in machine learning and neurophysiology ha demonstrated the role of the basal ganglion and the frontal cortex in mammalian reinforcement learning this paper develops and explores new reinforcement learning algorithm inspired by neurological evidence that provides potential new approach to the feature construction problem the algorithm are compared and evaluated on the acrobot task 
the extraction of a single high quality image from a set of low resolution image is an important problem which arises in flelds such a remote sensing surveillance medical imaging and the extraction of still image from video typical approach are based on the use of cross correlation to register the image followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution image using regularization to resolve the ill posed nature of the inversion process in this paper we develop a bayesian treatment of the super resolution problem in which the likelihood function for the image registration parameter is based on a marginalization over the unknown highresolution image this approach allows u to estimate the unknown point spread function and is rendered tractable through the introduction of a gaussian process prior over image result indicate a signiflcant improvement over technique based on map maximum a posteriori point optimization of the high resolution image and associated registration parameter 
we describe a new approximation algorithm for solving partially observable mdps our bounded policy iterationapproach search through the space of bounded size stochastic finite state controller combining several advantage of gradient ascent efficiency search through restricted controller space and policy iteration le vulnerability to local optimum 
we introduce a novel learning algorithm for binary classication with hyperplane discriminants based on pair of training point from opposite class dyadic hypercuts this algorithm is further extended to nonlinear discriminants using kernel function satisfying mercer s condition an ensemble of simple dyadic hypercuts is learned incrementally by mean of a condence rated version of adaboost which provides a sound strategy for searching through the nite set of hypercut hypothesis in experiment with real world datasets from the uci repository the generalization performance of the hypercut classiers wa found to be comparable to that of svms and k nn classiers furthermore the computational cost of classication at run time wa found to be similar to or better than that of svm similarly to svms boosted dyadic kernel discriminants tend to maximize the margin via adaboost in contrast to svms however we oer an on line and incremental learning machine for building kernel discriminants whose complexity number of kernel evaluation can be directly controlled traded o for accuracy 
large sparse binary matrix arise in numerous data mining application such a the analysis of market basket web graph social network co citation a well a information retrieval collaborative filtering sparse matrix reordering etc virtually all popular method for the analysis of such matrix e g k mean clustering metis graph partitioning svd pca and frequent itemset mining require the user to specify various parameter such a the number of cluster number of principal component number of partition and support choosing suitable value for such parameter is a challenging problem cross association is a joint decomposition of a binary matrix into disjoint row and column group such that the rectangular intersection of group are homogeneous starting from first principle we furnish a clear information theoretic criterion to choose a good cross association a well a it parameter namely the number of row and column group we provide scalable algorithm to approach the optimal our algorithm is parameter free and requires no user intervention in practice it scale linearly with the problem size and is thus applicable to very large matrix finally we present experiment on multiple synthetic and real life datasets where our method give high quality intuitive result 
the problem of super resolution involves generating feasible higher resolution image which are pleasing to the eye and realistic from a given low resolution image this might be attempted by u ing simple lters for smoothing out the high resolution block or through application where substantial prior information is used to imply the texture and shape which will occur in the image in this paper we describe an approach which lie between the two extreme it is a generic unsupervised method which is usable in all domain but go beyond simple smoothing method in what it achieves we use a dynamic tree like architecture to model the high resolution data approximate conditioning on the low resolution image is achieved through a mean eld approach 
adaptation is a ubiquitous neural and psychological phenomenon with a wealth of instantiation and implication although a basic form of plasticity it ha bar some notable exception attracted computational theory of only one main variety in this paper we study adaptation from the perspective of factor analysis a paradigmatic technique of unsupervised learning we use factor analysis to re interpret a standard view of adaptation and apply our new model to some recent data on adaptation in the domain of face discrimination 
we are constructing caching policy that have lower miss rate than the best of twelve baseline policy over a large variety of request stream this represents an improvement of over least recently used the most commonly implemented policy we achieve this not by designing a specific new policy but by using on line machine learning algorithm to dynamically shift between the standard policy based on their observed miss rate a thorough experimental evaluation of our technique is given a well a a discussion of what make caching an interesting on line learning problem 
we consider the question of how well a given distribution can be approximated with probabilistic graphical model we introduce a new parameter effective treewidth that capture the degree of approximability a a tradeoff between the accuracy and the complexity of approximation we present a simple approach to analyzing achievable tradeoff that exploit the threshold behavior of monotone graph property and provide experimental result that support the approach 
this paper describes a system for the unsupervised learning of morphological sufx e and stem from word list the system is composed of a generative probability model and hill climbing and directed search algorithm by extracting and examining morphologically rich subset of an input lexicon the directed search identies highly productive paradigm the hill climbing algorithm then further maximizes the probability of the hypothesis quantitative result are shown by measuring the accuracy of the morphological relation identied experiment in english and polish a well a comparison with another recent unsupervised morphology learning algorithm demonstrate the effectiveness of this technique 
we propose a model that can learn part based representation of highdimensional data our key assumption is that the dimension of the data can be separated into several disjoint subset or factor which take on value independently of each other we assume each factor ha a small number of discrete state and model it using a vector quantizer the selected state of each factor represent the multiple cause of the input given a set of training example our model learns the association of data dimension with factor a well a the state of each vq inference and learning are carried out efficiently via variational algorithm we present application of this model to problem in image decomposition collaborative filtering and text classification 
in typical classification task we seek a function which assigns a label to a single object kernel based approach such a support vector machine svms which maximize the margin of confidence of the classifier are the method of choice for many such task their popularity stem both from the ability to use high dimensional feature space and from their strong theoretical guarantee however many real world task involve sequential spatial or structured data where multiple label must be assigned existing kernel based method ignore structure in the problem assigning label independently to each object losing much useful information conversely probabilistic graphical model such a markov network can represent correlation between label by exploiting problem structure but cannot handle high dimensional feature space and lack strong theoretical generalization guarantee in this paper we present a new framework that combine the advantage of both approach maximum margin markov m network incorporate both kernel which efficiently deal with high dimensional feature and the ability to capture correlation in structured data we present an efficient algorithm for learning m network based on a compact quadratic program formulation we provide a new theoretical bound for generalization in structured domain experiment on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gain over previous approach 
margin maximizing property play an important role in the analysis of classification model such a boosting and support vector machine margin maximization is theoretically interesting because it facilitates generalization error analysis and practically interesting because it present a clear geometric interpretation of the model being built we formulate and prove a sufficient condition for the solution of regularized loss function to converge to margin maximizing separator a the regularization vanishes this condition cover the hinge loss of svm the exponential loss of adaboost and logistic regression loss we also generalize it to multi class classification problem and present margin maximizing multiclass version of logistic regression and support vector machine 
we present a novel approach to collaborative prediction using low norm instead of low rank factorization the approach is inspired by and ha strong connection to large margin linear discrimination we show how to learn low norm factorization by solving a semi definite program and discus generalization error bound for them 
we expand on the problem of learning a kernel via a rkhs on the space of kernel itself the resulting optimization problem is shown to have a semidefinite programming solution we demonstrate that it is possible to learn the kernel for various formulation of machine learning problem specifically we provide mathematical programming formulation and experimental result for the csvm svm and lagrangian svm for classification on uci data and novelty detection 
we propose and test an objective criterion for evaluation of clustering performance how well doe a clustering algorithm run on unlabeled data aid a classification algorithm the accuracy is quantified using the pac mdl bound in a semisupervised setting clustering algorithm which naturally separate the data according to hidden label with a small number of cluster perform well a simple extension of the argument lead to an objective model selection method experimental result on text analysis datasets demonstrate that this approach empirically result in very competitive bound on test set performance on natural datasets 
we describe technique for combining two type of knowledge system expert and machine learning both the expert system and the learning system represent information by logical decision rule or tree unlike the classical view of knowledge base evaluation or refinement our view accepts the content of the knowledge base a completely correct the knowledge base and the result of it stored case will provide direction for the discovery of new relationship in the form of newly induced decision rule an expert system called sea wa built to discover sale lead for computer product and solution the system interview executive by asking question and based on the response recommends product that may improve a business operation leveraging this expert system we record the result of the interview and the program s recommendation the very same data stored by the expert system is used to find new predictive rule among the potential advantage of this approach are a the capability to spot new sale trend and b the substitution of le expensive probabilistic rule that use database data instead of interview 
the focus of the paper is the problem of learning kernel operator from empirical data we cast the kernel design problem a the construction of an accurate kernel from simple and le accurate base kernel we use the boosting paradigm to perform the kernel construction process to do so we modify the booster so a to accommodate kernel operator we also devise an efficient weak learner for simple kernel that is based on generalized eigen vector decomposition we demonstrate the effectiveness of our approach on synthetic data and on the usps dataset on the usps dataset the performance of the perceptron algorithm with learned kernel is systematically better than a fixed rbf kernel 
we introduce the first algorithm for learning predictive state representation predictive state representation or psrs are a way of representing the state of a controlled dynamical system in term of prediction of test where test are sequence of action and observation said to be true if and only if all the observation occur given that all the action are taken the problem of finding a good psr one that is a sufficient statistic for the dynamical system can be divided into two part discovery of a good set of test and learning to make accurate prediction for those test in this paper we address the learning part of the problem for linear psrs and show that our algorithm make correct prediction in several sample system 
an important aspect of clustering algorithm is whether the partition constructed on finite sample converge to a useful clustering of the whole data space a the sample size increase this paper investigates this question for normalized and unnormalized version of the popular spectral clustering algorithm surprisingly the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case even though recently some first result on the convergence of normalized spectral clustering have been obtained for the unnormalized case we have to develop a completely new approach combining tool from numerical integration spectral and perturbation theory and probability it turn out that while in the normalized case spectral clustering usually converges to a nice partition of the data space in the unnormalized case the same only hold under strong additional assumption which are not always satisfied we conclude that our analysis give strong evidence for the superiority of normalized spectral clustering it also provides a basis for future exploration of other laplacian based method 
classification tree are widely used in the machine learning and data mining community for modeling propositional data recent work ha extended this basic paradigm to probability estimation tree traditional tree learning algorithm assume that instance in the training data are homogenous and independently distributed relational probability tree rpts extend standard probability estimation tree to a relational setting in which data instance are heterogeneous and interdependent our algorithm for learning the structure and parameter of an rpt search over a space of relational feature that use aggregation function e g average mode count to dynamically propositionalize relational data and create binary split within the rpt previous work ha identified a number of statistical bias due to characteristic of relational data such a autocorrelation and degree disparity the rpt algorithm us a novel form of randomization test to adjust for these bias on a variety of relational learning task rpts built using randomization test are significantly smaller than other model and achieve equivalent or better performance 
we consider situation where training data is abundant and computing resource are comparatively scarce we argue that suitably designed online learning algorithm asymptotically outperform any batch learning algorithm both theoretical and experimental evidence are presented 
the kernel function play a central role in kernel method existing method typically fix the functional form of the kernel in advance and then only adapt the associated kernel parameter based on empirical data in this paper we consider the problem of adapting the kernel so that it becomes more similar to the so called ideal kernel we formulate this a a distance metric learning problem that search for a suitable linear transform fcature weighting in the kernel induced feature space this formulation is applicable even when the training set can only provide example of similar and dissimilar pair but not explicit class label information computationally this lead to a local optimum free quadratic programming problem with the number of variable independent of the number of feature performance of this method is evaluated on classification and clustering task on both toy and real world data set 
in this paper we study a special kind of learning problem in which each training instance is given a set of or distribution over candidate class label and only one of the candidate label is the correct one such a problem can occur e g in an information retrieval setting where a set of word is associated with an image or if class label are organized hierarchically we propose a novel discriminative approach for handling the ambiguity of class label in the training example the experiment with the proposed approach over five different uci datasets show that our approach is able to find the correct label among the set of candidate label and actually achieve performance close to the case when each training instance is given a single correct label in contrast na ve method degrade rapidly a more ambiguity is introduced into the label 
we formulate linear dimensionality reduction a a semi parametric estimation problem enabling u to study it asymptotic behavior we generalize the problem beyond additive gaussian noise to unknown nongaussian additive noise and to unbiased non additive model 
markov network are extensively used to model complex sequential spatial and relational interaction in field a diverse a image processing natural language analysis and bioinformatics however inference and learning in general markov network is intractable in this paper we focus on learning a large subclass of such model called associative markov network that are tractable or closely approximable this subclass contains network of discrete variable with k label each and clique potential that favor the same label for all variable in the clique such network capture the guilt by association pattern of reasoning present in many domain in which connected associated variable tend to have the same label our approach exploit a linear programming relaxation for the task of finding the best joint assignment in such network which provides an approximate quadratic program qp for the problem of learning a margin maximizing markov network we show that for associative markov network over binary valued variable this approximate qp is guaranteed to return an optimal parameterization for markov network of arbitrary topology for the nonbinary case optimality is not guaranteed but the relaxation produce good solution in practice experimental result with hypertext and newswire classification show significant advantage over standard approach 
we present an algorithmic framework for supervised classification learning where the set of label is organized in a predefined hierarchical structure this structure is encoded by a rooted tree which induces a metric over the label set our approach combine idea from large margin kernel method and bayesian analysis following the large margin principle we associate a prototype with each label in the tree and formulate the learning task a an optimization problem with varying margin constraint in the spirit of bayesian method we impose similarity requirement between the prototype corresponding to adjacent label in the hierarchy we describe new online and batch algorithm for solving the constrained optimization problem we derive a worst case loss bound for the online algorithm and provide generalization analysis for it batch counterpart we demonstrate the merit of our approach with a series of experiment on synthetic text and speech data 
two dimensional contingency or co occurrence table arise frequently in important application such a text web log and market basket data analysis a basic problem in contingency table analysis is co clustering simultaneous clustering of the row and column a novel theoretical formulation view the contingency table a an empirical joint probability distribution of two discrete random variable and pose the co clustering problem a an optimization problem in information theory the optimal co clustering maximizes the mutual information between the clustered random variable subject to constraint on the number of row and column cluster we present an innovative co clustering algorithm that monotonically increase the preserved mutual information by intertwining both the row and column clustering at all stage using the practical example of simultaneous word document clustering we demonstrate that our algorithm work well in practice especially in the presence of sparsity and high dimensionality 
statistical approach to language learning typically foc u on either short range syntactic dependency or long range semantic dependency between word we present a generative model that us both kind of dependency and can be used to simultaneously find syntact ic class and semantic topic despite having no representation of syntax or semantics beyond statistical dependency this model is competitive on task like part of speech tagging and document classification wi th model that exclusively use shortand long range dependency respectively 
psychologist call behavior intrinsically motivated when it is engaged in for it own sake rather than a a step toward solving a specific problem of clear practical value but what we learn during intrinsically motivated behavior is essential for our development a competent autonomous entity able to efficiently solve a wide range of practical problem a they arise in this paper we present initial result from a computational study of intrinsically motivated reinforcement learningaimed at allowing artificial agent to construct and extend hierarchy of reusable skill that are needed for competent autonomy 
we consider a variant of q learning in continuous state space under the total expected discounted cost criterion combined with local function approximation method provided that the function approximator satisfies certain interpolation property the resulting algorithm is shown to converge with probability one the limit function is shown to satisfy a fixed point equation of the bellman type where the fixed point operator depends on the stationary distribution of the exploration policy and the function approximation method the basic algorithm is extended in several way in particular a variant of the algorithm is obtained that is shown to converge in probability to the optimal q function preliminary computer simulation are presented that confirm the validity of the approach 
committee of classifier with learning capability have good performance in a variety of domain we focus on committee of agent with learning capability where no agent is omniscient but ha a local limited individual view of data in this framework a major issue is how to integrate the individual result in an overall result usually a voting mechanism is used we propose a setting where agent can express a symbolic justification of their individual result justification can then be examined by other agent and accepted or found wanting we propose a specific interaction protocol that support revision of justification created by dierent agent finally the opinion of individual agent are aggregated into a global outcome using a weighted voting scheme 
anomaly detection is an area that ha received much attention in recent year it ha a wide variety of application including fraud detection and network intrusion detection a good deal of research ha been performed in this area often using string or attribute value data a the medium from which anomaly are to be extracted little work however ha focused on anomaly detection in graph based data in this paper we introduce two technique for graph based anomaly detection in addition we introduce a new method for calculating the regularity of a graph with application to anomaly detection we hypothesize that these method will prove useful both for finding anomaly and for determining the likelihood of successful anomaly detection within graph based data we provide experimental result using both real world network intrusion data and artificially created data 
adaboost minimizes an upper error bound which is an exponential function of the margin on the training set however the ultimate goal in application of pattern classification is always minimum error rate on the other hand adaboost need an effective procedure for learning weak classifier which by itself is difficult especially for high dimensional data in this paper we present a novel procedure called floatboost for learning a better boosted classifier floatboost us a backtrack mechanism after each iteration of adaboost to remove weak classifier which cause higher error rate the resulting float boosted classifier consists of fewer weak classifier yet achieves lower error rate than adaboost in both training and test we also propose a statistical model for learning weak classifier based on a stagewise approximation of the posterior using an overcomplete set of scalar feature experimental comparison of floatboost and adaboost are provided through a difficult classification problem face detection where the goal is to learn from training example a highly nonlinear classifier to differentiate between face and nonface pattern in a high dimensional space the result clearly demonstrate the promise made by floatboost over adaboost 
this paper introduces the field programmable learning array a new paradigm for rapid prototyping of learning primitive and machinelearning algorithm in silicon the fpla is a mixed signal counterpart to the all digital field programmable gate array in that it enables rapid prototyping of algorithm in hardware unlike the fpga the fpla is targeted directly for machine learning by providing local parallel online analog learning using floating gate mo synapse transistor we present a prototype fpla chip comprising an array of reconfigurable computational block and local interconnect we demonstrate the viability of this architecture by mapping several learning circuit onto the prototype chip 
hierarchical reinforcement learning is a general framework which attempt to accelerate policy learning in large domain on the other hand policy gradient reinforcement learning pgrl method have received recent attention a a mean to solve problem with continuous state space however they suer from slow convergence in this paper we combine these two approach and propose a family of hierarchical policy gradient algorithm for problem with continuous state and or action space we also introduce a class of hierarchical hybrid algorithm in which a group of subtasks usually at the higher level of the hierarchy are formulated a value function based rl vfrl problem and the others a pgrl problem we demonstrate the performance of our proposed algorithm using a simple taxi fuel problem and a complex continuous state and action ship steering domain 
this paper introduces fast a novel two phase sampling based algorithm for discovering association rule in large database in phase i a large initial sample of transaction is collected and used to quickly and accurately estimate the support of each individual item in the database in phase ii these estimated support are used to either trim outlier transaction or select representative transaction from the initial sample thereby forming a small final sample that more accurately reflects the statistical characteristic i e itemset support of the entire database the expensive operation of discovering association rule is then performed on the final sample in an empirical study fast wa able to achieve accuracy using a final sample having a size of only of that of a comparable random sample this efficiency gain resulted in a speedup by roughly a factor of over previous algorithm that require expensive processing of the entire database even efficient algorithm that exploit sampling our new sampling technique can be used in conjunction with almost any standard association rule algorithm and can potentially render scalable other algorithm that mine count data 
the context in which a name appears in a caption provides powerful cue a to who is depicted in the associated image we obtain face image using a face detector from approximately half a million captioned news image and automatically link name obtained using a named entity recognizer with these face a simple clustering method can produce fair result we improve these result significantly by combining the clustering process with a model of the probability that an individual is depicted given it context once the labeling procedure is over we have an accurately labeled set of face an appearance model for each individual depicted and a natural language model that can produce accurate result on caption in isolation 
we study the common problem of approximating a target matrix with a matrix of lower rank we provide a simple and ecient em algorithm for solving weighted low rank approximation problem which unlike their unweighted version do not admit a closedform solution in general we analyze in addition the nature of locally optimal solution that arise in this context demonstrate the utility of accommodating the weight in reconstructing the underlying low rank representation and extend the formulation to nongaussian noise model such a logistic model finally we apply the method developed to a collaborative filtering task 
in model that define probability via energy maximum likelihood learning typically involves using markov chain monte carlo to sample from the model s distribution if the markov chain is started at the data distribution learning often work well even if the chain is only run for a few time step but if the data distribution contains mode separated by region of very low density brief mcmc will not ensure that different mode have the correct relative energy because it cannot move particle from one mode to another we show how to improve brief mcmc by allowing long range move that are suggested by the data distribution if the model is approximately correct these long range move have a reasonable acceptance rate 
we study a number of open issue in spectral clustering i selecting the appropriate scale of analysis ii handling multi scale data iii clustering with irregular background clutter and iv finding automatically the number of group we first propose that a local scale should be used to compute the affinity between each pair of point this local scaling lead to better clustering especially when the data includes multiple scale and when the cluster are placed within a cluttered background we further suggest exploiting the structure of the eigenvectors to infer automatically the number of group this lead to a new algorithm in which the final randomly initialized k mean stage is eliminated 
automated detection of the first document reporting each new event in temporally sequenced stream of document is an open challenge in this paper we propose a new approach which address this problem in two stage using a supervised learning algorithm to classify the on line document stream into pre defined broad topic category and performing topic conditioned novelty detection for document in each topic we also focus on exploiting named entity for event level novelty detection and using feature based heuristic derived from the topic history evaluating these method using a set of broadcast news story our result show substantial performance gain over the traditional one level approach to the novelty detection problem 
we introduce a new learning algorithm for decision list to allow feature that are constructed from the data and to allow a tradeo between accuracy and complexity we bound it generalization error in term of the number of error and the size of the classifier it find on the training data we also compare it performance on some natural data set with the set covering machine and the support vector machine 
people routinely make sophisticated causal inference unconsciously effortlessly and from very little data often from just one or a few observation we argue that these inference can be explained a bayesian computation over a hypothesis space of causal graphical model shaped by strong top down prior knowledge in the form of intuitive theory we present two case study of our approach including quantitative model of human causal judgment and brief comparison with traditional bottom up model of inference 
abstract the majority of theoretical work in machine learning is done under the assumption of exchangeability essentially it is assumed that the example are generated from the same probability distribution independently this paper is concerned with the problem of testing the exchangeability assumption in the on line mode example are observed one by one and the goal is to monitor on line the strength of evidence against the hypothesis of exchangeability we introduce the notion of exchangeability martingale which are on line procedure for detecting deviation from exchangeability in essence they are betting scheme that never risk bankruptcy and are fair under the hypothesis of exchangeability some specific exchangeability martingale are constructed using transductive confi dence machine we report experimental re sults showing their performance on the usps benchmark data set of hand written digit known to be somewhat heterogeneous one of them multiplies the initial capital by more than this mean that the hypothesis of exchangeability is rejected at the significance level 
in several research domain concerned with classification task curve like roc are often used to ass the quality of a particular model or to compare two or more model with respect to various operating point researcher also often publish some statistic coming from the roc such a the socalled break even point or equal error rate the purpose of this paper is to first argue that these measure can be misleading in a machine learning context and should be used with care instead we propose to use the expected performance curve epc which provide unbiased estimate of performance at various operating point furthermore we show how to use adequately a non parametric statistical test in order to produce epcs with confidence interval or ass the statistical significant dierence between two model under various setting 
theory of access consciousness address how it is that some mental state but not others are available for evaluation choice behavior and verbal report farah o reilly and vecera argue that quality of representation is critical dehaene sergent and changeux argue that the ability to communicate representation is critical we present a probabilistic information transmissionor pit model that suggests both of these condition are essential for access consciousness having successfully modeled data from the repetition priming literature in the past we use the pit model to account for data from two experiment on subliminal priming showing that the model produce priming even in the absence of accessibility and reportability of internal state the model provides a mechanistic basis for understanding the dissociation of priming and awareness philosophy ha made many attempt to identify distinct aspect of consciousness perhaps the most famous effort is block s delineation of phenomenal and access consciousness phenomenal consciousness ha to do with what it is like to experience chocolate or a pin prick access consciousness refers to internal state whose content is inferentially promiscuous i e poised to be used a a premise in reasoning poised for control of action and poised for rational control of speech p the scientific study of consciousness ha exploded in the past six year and an important catalyst for this explosion ha been the decision to focus on the problem of access consciousness how is it that some mental state but not others become available for evaluation choice behavior verbal report and storage in working memory another reason for the recent explosion of consciousness research is the availability of functional imaging technique to explore difference in brain activation between conscious and unconscious state a well a the development of clever psychological experiment that show that a stimulus that is not consciously perceived can nonetheless influence cognition which we describe shortly 
two mode and co occurrence data have been frequently seen in the real world we address the issue of predicting unknown cooccurrence event from known event for this issue we propose a new method that naturally combine observable co occurrence event with their existing latent knowledge by using tlierarchical latent variable this method build a space efficient hierarchical latent variable model and estimate the probability parameter of the model with an em algorithm in this paper we focus on a data set of protein protein interaction a typical co occurrence data set and apply our method to the problem of predicting unknown protein protein interaction an important research issue in current computational biology using a real data set of protein protein interaction and latent knowledge we tested the performance of our method comparing it with other recent machine learning approach including support vector machine our experimental result show that our method clearly outperforms the predictive performance obtained by other approach the result indicate that both our idea of using existing knowledge a a latent variable and our inethodology impleinenting it are effective for drastically improving the predictive performance of existing unsupervised learning approach for cooccurrence data 
the google search engine ha enjoyed huge success with it web page ranking algorithm which exploit global rather than local hyperlink structure of the web using random walk here we propose a simple universal ranking algorithm for data lying in the euclidean space such a text or image data the core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data encouraging experimental result from synthetic image and text data illustrate the validity of our method 
past empirical work ha shown that learning multiple related task from data simultaneously can be advantageous in term of predictive performance relative to learning these task independently in this paper we present an approach to multi task learning based on the minimization of regularization functionals similar to existing one such a the one for support vector machine svms that have been successfully used in the past for single task learning our approach allows to model the relation between task in term of a novel kernel function that us a task coupling parameter we implement an instance of the proposed approach similar to svms and test it empirically using simulated a well a real data the experimental result show that the proposed method performs better than existing multi task learning method and largely outperforms single task learning using svms 
greedy importance sampling is an unbiased estimation technique that reduces the variance of standard importance sampling by explicitly searching for mode in the estimation objective previous work ha demonstrated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domain in this paper we present a reformulation of greedy importance sampling that eliminates the free parameter from the original estimator and introduces a new regularization strategy that further reduces variance without compromising unbiasedness the resulting estimator is shown to be effective for difficult estimation problem arising in markov random field inference in particular improvement are achieved over standard mcmc estimator when the distribution ha multiple peaked mode 
receiver operating characteristic roc curve provide a powerful tool for visualizing and comparing classification result regression error characteristic rec curve generalize roc curve to regression rec curve plot the error tolerance on the xaxis versus the percentage of point predicted within the tolerance on the y axis the resulting curve estimate the cumulative distribution function of the error the rec curve visually present commonly used statistic the area over the curve aoc is a biased estimate of the expected error the r value can be estimated using the ratio of the aoc for a given model to the aoc for the null model user can quickly ass the relative merit of many regression function by examining the relative position of their rec curve the shape of the curve reveals additional information that can be used to guide modeling 
a standard view of memory consolidation is that episode are stored temporarily in the hippocampus and are transferred to the neocortex through replay various recent experimental challenge to the idea of transfer particularly for human memory are forcing it re evaluation however although there is independent neurophysiological evidence for replay short of transfer there are few theoretical idea for what it might be doing we suggest and demonstrate two important computational role associated with neocortical index 
we propose a new particle filter that incorporates a model of cost when generating particle the approach is motivated by the observation that the cost of accidentally not tracking hypothesis might be significant in some area of state space and next to irrelevant in others by incorporating a cost model into particle filtering state that are more critical to the system performance are more likely to be tracked automatic calculation of the cost model is implemented using an mdp value function calculation that estimate the value of tracking a particular state experiment in two mobile robot domain illustrate the appropriateness of the approach this paper address a primary deficiency of particle filter particle filter are insensitive to cost that might arise from the approximate nature of the particle representation their only criterion for generating a particle is the posterior likelihood of a state to illustrate this point consider the example of a space shuttle failure of the engine system are extremely unlikely even in the presence of evidence to the contrary should we therefore not track the possibility of such failure just because they are unlikely if failure to track such lowlikelihood event may incur high cost such a a mission failure these variable should be tracked even when their posterior probability is low this observation suggests that cost should be taken into consideration when generating particle in the filtering process this paper proposes a particle filter that generates particle according to a distribution that combine the posterior probability with a risk function the risk function measure the importance of a state location on future cumulative cost we obtain this risk function via an mdp that calculates the approximate future risk of decision made in a particular state experimental result in two robotic domain illustrate that our approach yield significantly better result than a particle filter insensitive to cost 
in this paper we use the rollout method for policy improvement to analyze a version of klondike solitaire this version sometimes called thoughtful solitaire ha all card revealed to the player but then follows the usual klondike rule a strategy that we establish using iterated rollouts win about twice a many game on average a an expert human player doe 
model selection is linked to model assessment which is the problem of comparing different model or model parameter for a specific learning task for supervised learning the standard practical technique is crossvalidation which is not applicable for semi supervised and unsupervised setting in this paper a new model assessment scheme is introduced which is based on a notion of stability the stability measure yield an upper bound to cross validation in the supervised case but extends to semi supervised and unsupervised problem in the experimental part the performance of the stability measure is studied for model order selection in comparison to standard technique in this area 
we propose a method for sequential bayesian kernel regression a is the case for the popular relevance vector machine rvm the method automatically identifies the number and location of the kernel our algorithm overcomes some of the computational difficulty related to batch method for kernel regression it is non iterative and requires only a single pas over the data it is thus applicable to truly sequential data set and batch data set alike the algorithm is based on a generalisation of importance sampling which allows the design of intuitively simple and efficient proposal distribution for the model parameter comparative result on two standard data set show our algorithm to compare favourably with existing batch estimation strategy standard batch method for kernel regression suffer from a computational drawback in that they are iterative in nature with a computational complexity that is normally cubic in the number of data point at each iteration a large proportion of the research effort in this area is devoted to the development of estimation algorithm with reduced computational complexity for the rvm for example a strategy is proposed in that exploit the structure of the marginal likelihood function to significantly reduce the number of computation in this paper we propose a full bayesian formulation for kernel regression on sequential data our algorithm is non iterative and requires only a single pas over the data it is equally applicable to batch data set by presenting the data point one at a time with the order of presentation being unimportant the algorithm is especially effective for large data set a opposed to batch strategy that attempt to find the optimal solution conditional on all the data the sequential strategy includes the data one at a time so that the poste 
learning in multiagent system suffers from the fact that both the state and the action space scale exponentially with the number of agent in this paper we are interested in using q learning to learn the coordinated action of a group of cooperative agent using a sparse representation of the joint state action space of the agent we first examine a compact representation in which the agent need to explicitly coordinate their action only in a predefined set of state next we use a coordination graph approach in which we represent the q value by value rule that specify the coordination dependency of the agent at particular state we show how q learning can be efficiently applied to learn a coordinated policy for the agent in the above framework we demonstrate the proposed method on the predator prey domain and we compare it with other related multiagent q learning method 
in the problem of probability forecasting the learner s goal is to output given a training set and a new object a suitable probability measure on the possible value of the new object s label an on line algorithm for probability forecasting is said to be well calibrated if the probability it output agree with the observed frequency we give a natural nonasymptotic formalization of the notion of well calibratedness which we then study under the assumption of randomness the object label pair are independent and identically distributed it turn out that although no probability forecasting algorithm is automatically well calibrated in our sense there exists a wide class of algorithm for multiprobability forecasting such algorithm are allowed to output a set ideally very narrow of probability measure which satisfy this property we call the algorithm in this class venn probability machine our experimental result demonstrate that a nearest neighbor venn probability machine performs reasonably well on a standard benchmark data set and one of our theoretical result asserts that a simple venn probability machine asymptotically approach the true conditional probability regardless and without knowledge of the true probability measure generating the example 
we present a unified view for online classification regression and uniclass problem this view lead to a single algorithmic framework for the three problem we prove worst case loss bound for various algorithm for both the realizable case and the non realizable case the end result is new algorithm and accompanying loss bound for hinge loss regression and uniclass we also get refined loss bound for previously studied classification algorithm 
we investigate the problem of reducing the complexity of a graphical model g pg by finding a subgraph h of g chosen from a class of subgraphsh such that h is optimal with respect to kl divergence we do this by first defining a decomposition tree representation for g which is closely related to the junction tree representation for g we then give an algorithm which us this representation to compute the optimal h h gavril and tarjan have used graph separation property to solve several combinatorial optimization problem when the size of the minimal separator in the graph is bounded we present an extension of this technique which applies to some important choice ofh even when the size of the minimal separator of g are arbitrarily large in particular this applies to problem such a finding an optimal subgraphical model over a k tree of a graphical model over a k tree for arbitrary k and selecting an optimal subgraphical model with a constant d fewer edge with respect to kl divergence can be solved in time polynomial in jv g j using this formulation 
a graph based prior is proposed for parametric semi supervised classification the prior utilizes both labelled and unlabelled data it also integrates feature from multiple view of a given sample e g multiple sensor thus implementing a bayesian form of co training an em algorithm for training the classifier automatically adjusts the tradeoff between the contribution of a the labelled data b the unlabelled data and c the co training information active label query selection is performed using a mutual information based criterion that explicitly us the unlabelled data and the co training information encouraging result are presented on public benchmark and on measured data from single and multiple sensor 
there exist many different generalization error bound for classification each of these bound contains an improvement over the others for certain situation our goal is to combine these different improvement into a single bound in particular we combine the pac bayes approach introduced by mcallester which is interesting for averaging classifier with the optimal union bound provided by the generic chaining technique developed by fernique and talagrand this combination is quite natural since the generic chaining is based on the notion of majorizing measure which can be considered a prior on the set of classifier and such prior also arise in the pac bayesian setting 
abstract we explore the consequence of viewing semantic association asthe result of attempting to predict the concept likely to arise in aparticular context we argue that the success of existing accountsof semantic representation come a a result of indirectly addressingthis problem and show that a closer correspondence to human datacan be obtained by taking a probabilistic approach that explicitlymodels the generative structure of language 
we show that state of a dynamical system can be usefully represented by multi step action conditional prediction of future observation state representation that are grounded in data in this way may be easier to learn generalize better and be le dependent on accurate prior model than for example pomdp state representation building on prior work by jaeger and by rivest and schapire in this paper we compare and contrast a linear specialization of the predictive approach with the state representation used in pomdps and in k order markov model ours is the flrst speciflc formulation of the predictive idea that includes both stochasticity and action control we show that any system ha a linear predictive state representation with number of prediction le than or equal to the number of state in it minimal pomdp model in predicting or controlling a sequence of observation the concept of state and state estimation inevitably arise there have been two dominant approach the generative model approach typifled by research on partially observable markov decision process hypothesizes a structure for generating observation and estimate it state and state dynamic the history based approach typifled by k order markov method us simple function of past observation a state that is a the immediate basis for prediction or control the data ow in these two approach are diagrammed in figure of the two the generative model approach is more general the model s internal state give it temporally unlimited memory the ability to remember an event that happened arbitrarily long ago whereas a history based approach can only remember a far back a it history extends the bane of generative model approach is that they are often strongly dependent on a good model of the system s dynamic most us of partially observable markov decision process pomdps for example assume a perfect dynamic model and attempt only to estimate state there are algorithm for simultaneously estimating state and dynamic e g chrisman analogous to the baum welch algorithm for the uncontrolled case baum et al but these are only efiective at 
roc analysis is increasingly being recognised a an important tool for evaluation and comparison of classifier when the operating characteristic i e class distribution and cost parameter are not known at training time usually each classi fier is characterised by it estimated true and false positive rate and is represented by a single point in the roc diagram in this paper we show how a single decision tree can represent a set of classifier by choosing different labellings of it leaf or equivalently an ordering on the leaf in this setting rather than estimating the accuracy of a single tree it make more sense to use the area under the roc curve auc a a quality metric we also propose a novel splitting criterion which chooses the split with the highest local auc to the best of our knowledge this is the first probabilistic splitting criterion that is not based on weighted average impurity we present experiment suggesting that the auc splitting criterion lead to tree with equal or better auc value without sacrificing accuracy if a single labelling is chosen 
the area under an roc curve auc is a criterion used in many application to measure the quality of a classification algorithm however the objective function optimized in most of these algorithm is the error rate and not the auc value we give a detailed statistical analysis of the relationship between the auc and the error rate including the first exact expression of the expected value and the variance of the auc for a fixed error rate our result show that the average auc is monotonically increasing a a function of the classification accuracy but th at the standard deviation for uneven distribution and higher error rate i s noticeable thus algorithm designed to minimize the error rate may not lead to the best possible auc value we show that under certain condition the global function optimized by the rankboost algorithm is exactly the auc we report the result of our experiment with rankboost in several datasets demonstrating the benefit of an algorithm specific ally designed to globally optimize the auc over other existing algorithm optimizing an approximation of the auc or only locally optimizing the auc 
many criterion can be used to evaluate the performance of supervised learning different criterion are appropriate in different setting and it is not always clear which criterion to use a further complication is that learning method that perform well on one criterion may not perform well on other criterion for example svms and boosting are designed to optimize accuracy whereas neural net typically optimize squared error or cross entropy we conducted an empirical study using a variety of learning method svms neural net k nearest neighbor bagged and boosted tree and boosted stump to compare nine boolean classification performance metric accuracy lift f score area under the roc curve average precision precision recall break even point squared error cross entropy and probability calibration multidimensional scaling md show that these metric span a low dimensional manifold the three metric that are appropriate when prediction are interpreted a probability squared error cross entropy and calibration lay in one part of metric space far away from metric that depend on the relative order of the predicted value roc area average precision break even point and lift in between them fall two metric that depend on comparing prediction to a threshold accuracy and f score a expected maximum margin method such a svms and boosted tree have excellent performance on metric like accuracy but perform poorly on probability metric such a squared error what wa not expected wa that the margin method have excellent performance on ordering metric such a roc area and average precision we introduce a new metric sar that combine squared error accuracy and roc area into one metric md and correlation analysis show that sar is centrally located and correlate well with other metric suggesting that it is a good general purpose metric to use when more specific criterion are not known 
the growth of bioinformatics ha resulted in datasets with new characteristic these datasets typically contain a large number of column and a small number of row for example many gene expression datasets may contain column but only row such datasets pose a great challenge for existing closed frequent pattern discovery algorithm since they have an exponential dependence on the average row length in this paper we describe a new algorithm called carpenter that is specially designed to handle datasets having a large number of attribute and relatively small number of row several experiment on real bioinformatics datasets show that carpenter is order of magnitude better than previous closed pattern mining algorithm like closet and charm 
we present an examination of the state of the art for using value iteration to solve large scale discrete markov decision process we introduce an architecture which combine three independent performance enhancement the intelligent prioritization of computation state partitioning and massively parallel processing into a single algorithm we show that each idea improves performance in a different way meaning that algorithm designer do not have to trade one improvement for another we give special attention to parallelization issue discussing how to efficiently partition state distribute partition to processor minimize message passing and ensure high scalability we present experimental result which demonstrate that this approach solves large problem in reasonable time 
we attempt to understand visual classification in human using both psychophysical and machine learning technique frontal view of human face were used for a gender classification task human subject classified the face and their gender judgment reaction time and confidence rating were recorded several hyperplane learning algorithm were used on the same classification task using the principal component of the texture and shape representation of the face the classification performance of the learning algorithm wa estimated using the face database with the true gender of the face a label and also with the gender estimated by the subject we then correlated the human response to the distance of the stimulus to the separating hyperplane of the learning algorithm our result suggest that human classification can be modeled by some hyperplane algorithm in the feature space we used for classification the brain need more processing for stimulus close to that hyperplane than for those further away 
the online incremental gradient or backpropagation algorithm is widely considered to be the fastest method for solving large scale neural network nn learning problem in contrast we show that an appropriately implemented iterative batch mode or block mode learning method can be much faster for example it is three time faster in the uci letter classification problem output data item parameter with a two hidden layer multilayer perceptron and time faster in a nonlinear regression problem arising in color recipe prediction output data item parameter with a neuro fuzzy modular network the three principal innovative ingredient in our algorithm are the following first we use scaled trust region regularization with inner outer iteration to solve the associated overdetermined nonlinear least square problem where the inner iteration performs a truncated or inexact newton method second we employ pearlmutter s implicit sparse hessian matrix vector multiply algorithm to construct the krylov subspace used to solve for the truncated newton update third we exploit sparsity for preconditioning in the matrix resulting from the nns having many output 
abstract extends mcallester s pac bayesian theorem allowing for other tail bound than hoeding s and simplies the proof somewhat mcallester s pac bayesian theorem 
the paper extends the notion of linear programming boosting to handle uneven datasets extensive experiment with text classification problem compare the performance of a number of dierent boosting strategy concentrating on the problem posed by uneven datasets 
in real world planning problem time for deliberation is often limited anytime planner are well suited for these problem they find a feasible solution quickly and then continually work on improving it until time run out in this paper we propose an anytime heuristic search ara which tune it performance bound based on available search time it start by finding a suboptimal solution quickly using a loose bound then tightens the bound progressively a time allows given enough time it find a provably optimal solution while improving it bound ara reuses previous search effort and a a result is significantly more efficient than other anytime search method in addition to our theoretical analysis we demonstrate the practical utility of ara with experiment on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover 
to provide a compact generative representation of the sequential activity of a number of individual within a group there is a tradeoff between the defini tion of individual specific and global model this paper proposes a linear time distributed model for finite state symbolic sequence representing trace of individual user activity by making the assumption that heterogeneous user be havior may be explained by a relatively small number of structurally simple common behavioral pattern which may interleave randomly in a user specific proportion the result of an empirical study on three different source of user trace indicates that this modelling approach provides an efficient representation scheme reflected by improved prediction performance a well a providing low complexity and intuitively interpretable representation 
feature selection is the task of choosing a small set out of a given set of feature that capture the relevant property of the data in the context of supervised classification problem the relevance is determined by the given label on the training data a good choice of feature is a key for building compact and accurate classifier in this paper we introduce a margin based feature selection criterion and apply it to measure the quality of set of feature using margin we devise novel selection algorithm for multi class classification problem and provide theoretical generalization bound we also study the well known relief algorithm and show that it resembles a gradient ascent over our margin criterion we apply our new algorithm to various datasets and show that our new simba algorithm which directly optimizes the margin outperforms relief 
we devise a boosting approach to classification and regression based on column generation using a mixture of kernel traditional kernel method construct model based on a single positive semi definite kernel with the type of kernel predefined and kernel parameter chosen according to cross validation performance our approach creates model that are mixture of a library of kernel model and our algorithm automatically determines kernel to be used in the final model the norm and norm regularization method are employed to restrict the ensemble of kernel model the proposed method produce sparser solution and thus significantly reduces the testing time by extending the column generation cg optimization which existed for linear program with norm regularization to quadratic program with norm regularization we are able to solve many learning formulation by leveraging various algorithm for constructing single kernel model by giving different priority to column to be generated we are able to scale cg boosting to large datasets experimental result on benchmark data are included to demonstrate it effectiveness 
this paper develops a new representational model of similarity data that combine continuous dimension with discrete feature an algorithm capable of learning these representation is described and a bayesian model selection approach for choosing the appropriate number of dimension and feature is developed the approach is demonstrated on a classic data set that considers the similarity between the number through 
during the last ten year there ha been growing interest in the development of brain computer interface bcis the eld ha mainly been driven by the need of completely paralyzed patient to communicate with a few exception most human bcis are based on extracranial electroencephalography eeg however reported bit rate are still low one reason for this is the low signal to noise ratio of the eeg we are currently investigating if bcis based on electrocorticography ecog are a viable alternative in this paper we present the method and example of intracranial eeg recording of three epilepsy patient with electrode grid placed on the motor cortex the patient were asked to repeatedly imagine movement of two kind e g tongue or nger movement we analyze the classiability of the data using support vector machine svms and recursive channel elimination rce 
we consider learning in a markov decision process where we are not explicitly given a reward function but where instead we can observe an expert demonstrating the task that we want to learn to perform this setting is useful in application such a the task of driving where it may be difficult to write down an explicit reward function specifying exactly how different desideratum should be traded off we think of the expert a trying to maximize a reward function that is expressible a a linear combination of known feature and give an algorithm for learning the task demonstrated by the expert our algorithm is based on using inverse reinforcement learning to try to recover the unknown reward function we show that our algorithm terminates in a small number of iteration and that even though we may never recover the expert s reward function the policy output by the algorithm will attain performance close to that of the expert where here performance is measured with respect to the expert s unknown reward function 
in this paper we study the problem of constructing accurate decision tree model from data stream data stream are incremental task that require incremental online and any time learning algorithm one of the most successful algorithm for mining data stream is vfdt in this paper we extend the vfdt system in two direction the ability to deal with continuous data and the use of more powerful classification technique at tree leaf the proposed system vfdtc can incorporate and classify new information online with a single scan of the data in time constant per example the most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets this is relevant due to the any time property we study the behaviour of vfdtc in different problem and demonstrate it utility in large and medium data set under a bias variance analysis we observe that vfdtc in comparison to c is able to reduce the variance component 
pattern ordering is an important task in data mining because the number of pattern extracted by standard data mining algorithm often exceeds our capacity to manually analyze them in this paper we present an effective approach to address the pattern ordering problem by combining the rank information gathered from disparate source although rank aggregation technique have been developed for application such a meta search engine they are not directly applicable to pattern ordering for two reason first the technique are mostly supervised i e they require a sufficient amount of labeled data second the object to be ranked are assumed to be independent and identically distributed i i d an assumption that seldom hold in pattern ordering the method proposed in this paper is an adaptation of the original hedge algorithm modified to work in an unsupervised learning setting technique for addressing the i i d violation in pattern ordering are also presented experimental result demonstrate that our unsupervised hedge algorithm outperforms many alternative technique such a those based on weighted average ranking and singular value decomposition 
in this paper we propose to combine two powerful idea boosting and manifold learning on the one hand we improve adaboost by incorporating knowledge on the structure of the data into base classifier design and selection on the other hand we use adaboost s efficient learning mechanism to significantly improve supervised and semi supervised algorithm proposed in the context of manifold learning beside the specific manifold based penalization the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithm 
the effort necessary to construct labeled set of example in a supervised learning scenario is often disregarded though in many application it is a time consuming and expensive procedure while this already constitutes a major issue in classification learning it becomes an even more serious problem when dealing with the more complex target domain of total order over a set of alternative considering both the pairwise decomposition and the constraint classification technique to represent label ranking function we introduce a novel generalization of pool based active learning to address this problem 
clustering algorithm typically operate on a feature vector representation of the data and find cluster that are compact with respect to an assumed dis similarity measure between the data point in feature space this make the type of cluster identified highly dependent on the assumed similarity measure building on recent work in this area we formally define a class of spatially varying dissimilarity measure and propose algorithm to learn the dissimilarity measure automatically from the data the idea is to identify cluster that are compact with respect to the unknown spatially varying dissimilarity measure our experiment show that the proposed algorithm are more stable and achieve better accuracy on various textual data set when compared with similar algorithm proposed in the literature 
event management is a focal point in building and maintaining high quality information infrastructure we have witnessed the shift of the paradigm of event management in practice from root cause analysis rca to action oriented analysis aoa ibm ha developed a pioneer event management methodology emd based on the aoa paradigm and applied it to more than two hundred production site with success foreseeably more and more event management professional will apply aoa in different incarnation in building proactive management facility by that building correct and effective event relationship network ern becomes the dominating activity in aoa service design process currently the quality of ern and the cost of building them largely depend on the knowledge of domain expert we believe that we can utilize historical event log in shortening the ern design process and perfecting the quality of ern in this paper we describe in detail how to apply this data driven approach in ern validation completion and construction 
we present a hierarchical bayesian model for learning efficient code of higher order structure in natural image the model a non linear generalization of independent component analysis replaces the standard assumption of independence for the joint distribution of coefficient with a distribution that is adapted to the variance structure of the coefficient of an efficient image basis this offer a novel description of higherorder image structure and provides a way to learn coarse coded sparsedistributed representation of abstract image property such a object location scale and texture 
we describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low dimensional embedding thereby gaining the benefit of both approach in a single image one can view all the cluster examine the relation between them and study many of their property the method is based on an algorithm for low dimensional embedding of clustered data with the property that separation between all cluster is guaranteed regardless of their nature in particular the algorithm wa designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data so that every two disjoint cluster in the hierarchy are drawn separately 
this paper present a new approach for identifying and eliminating mislabeled instance in large or distributed datasets we first partition a dataset into subset each of which is small enough to be processed by an induction algorithm at one time we construct good rule from each subset and use the good rule to evaluate the whole dataset for a given instance ik two error count variable are used to count the number of time it ha been identified a noise by all subset the instance with higher error value will have a higher probability of being a mislabeled example two threshold scheme majority and non objection are used to identify the noise experimental result and comparative study from real world datasets are reported to evaluate the effectiveness and efficiency of the proposed approach 
alternative splicing a is an important and frequent step in mammalian gene expression that allows a single gene to specify multiple product and is crucial for the regulation of fundamental biological process the extent of a regulation and the mechanism involved are not well understood we have developed a custom dna microarray platform for surveying a level on a large scale we present here a generative model for the a array platform genasap and demonstrate it utility for quantifying a level in different mouse tissue learning is performed using a variational expectation maximization algorithm and the parameter are shown to correctly capture expected a trend a comparison of the result obtained with a well established but low through put experimental method demonstrate that a level obtained from genasap are highly predictive of a level in mammalian tissue biological diversity through alternative splicing current estimate place the number of gene in the human genome at approximately which is a surprisingly small number when one considers that the genome of yeast a singlecelled organism ha gene the number of gene alone cannot account for the complexity and cell specialization exhibited by higher eukaryote i e mammal plant etc some of that added complexity can be achieved through the use of alternative splicing whereby a single gene can be used to code for a multitude of product gene are segment of the double stranded dna that contain the information required by the cell for protein synthesis that information is coded using an alphabet of a c g and t corresponding to the four nucleotide that make up the dna in what is known a the central dogma of molecular biology dna is transcribed to rna which in turn is translated into protein messenger rna mrna is synthesized in the nucleus of the cell and carry the genomic information to the ribosome in eukaryote gene are generally comprised of both exon which contain the information needed by the cell to synthesize protein and intron sometimes referred to a spacer dna which are spliced out of the pre mrna to create mature mrna an estimated of human gene can be 
dopamine exerts two class of effect on the sustained neural activity in prefrontal cortex that underlies working memory direct release in the cortex increase the contrast of prefrontal neuron enhancing the robustness of storage release of dopamine in the striatum is associated with salient stimulus and make medium spiny neuron bistable this modulation of the output of spiny neuron affect prefrontal cortex so a to indirectly gate access to working memory and additionally damp sensitivity to noise existing model have treated dopamine in one or other structure or have addressed basal ganglion gating of working memory exclusive of dopamine effect in this paper we combine these mechanism and explore their joint effect we model a memory guided saccade task to illustrate how dopamine s action lead to working memory that is selective for salient input and ha increased robustness to distraction 
we investigate the problem of learning a classiflcation task for datasets which are described by matrix row and column of these matrix correspond to object where row and column object may belong to difierent set and the entry in the matrix express the relationship between them we interpret the matrix element a being produced by an unknown kernel which operates on object pair and we show that under mild assumption these kernel correspond to dot product in some unknown feature space minimizing a bound for the generalization error of a linear classifler which ha been obtained using covering number we derive an objective function for model selection according to the principle of structural risk minimization the new objective function ha the advantage that it allows the analysis of matrix which are not positive deflnite and not even symmetric or square we then consider the case that row object are interpreted a feature we suggest an additional constraint which imposes sparseness on the row object and show that the method can then be used for feature selection finally we apply this method to data obtained from dna microarrays where column object correspond to sample row object correspond to gene and matrix element correspond to expression level benchmark are conducted using standard one gene classiflcation and support vector machine and k nearest neighbor after standard feature selection our new method extract a sparse set of gene and provides superior classiflcation result 
a recent area of significant progress in speaker recognition is the use of high level feature idiolect phonetic relation prosody discourse structure etc a speaker not only ha a distinctive acoustic sound but us language in a characteristic manner large corpus of speech data available in recent year allow experimentation with long term statistic of phone pattern word pattern etc of an individual we propose the use of support vector machine and term frequency analysis of phone sequence to model a given speaker to this end we explore technique for text categorization applied to the problem we derive a new kernel based upon a linearization of likelihood ratio scoring we introduce a new phone based svm speaker recognition approach that half the error rate of conventional phone based approach 
given an image or video clip or audio song how do we automatically assign keywords to it the general problem is to find correlation across the medium in a collection of multimedia object like video clip with color and or motion and or audio and or text script we propose a novel graph based approach mmg to discover such cross modal correlation our mmg method requires no tuning no clustering no user determined constant it can be applied to any multimedia collection a long a we have a similarity function for each medium and it scale linearly with the database size we report auto captioning experiment on the standard corel image database of mb where it outperforms domain specific fine tuned method by up to percentage point in captioning accuracy relative improvement 
a number of medically important disease causing bacteria collectively called gram negative bacteria are noted for the extra outer membrane that surround their cell protein resident in this membrane outer membrane protein or omps are of primary research interest for antibiotic and vaccine drug design a they are on the surface of the bacteria and so are the most accessible target to develop new drug against with the development of genome sequencing technology and bioinformatics biologist can now deduce all the protein that are likely produced in a given bacteria and have attempted to classify where protein are located in a bacterial cell however such protein localization program are currently least accurate when predicting omps and so there is a current need for the development of a better omp classifier data mining research suggests that the use of frequent pattern ha good performance in aiding the development of accurate and efficient classification algorithm in this paper we present two method to identify omps based on frequent subsequence and test them on all gram negative bacterial protein whose localization have been determined by biological experiment one classifier follows an association rule approach while the other is based on support vector machine svms we compare the proposed method with the state of the art method in the biological domain the result demonstrate that our method are better both in term of accurately identifying omps and providing biological insight that increase our understanding of the structure and function of these important protein 
security of computer system is essential to their acceptance and utility computer security analyst use intrusion detection system to assist them in maintaining computer system security this paper deal with the problem of differentiating between masquerader and the true user of a computer terminal prior efficient solution are le suited to real time application often requiring all training data to be labeled and do not inherently provide an intuitive idea of what the data model mean our system called admit relaxes these constraint by creating user profile using semi incremental technique it is a real time intrusion detection system with host based data collection and processing our method also suggests idea for dealing with concept drift and affords a detection rate a high a and a false positive rate a low a 
this paper report the result of our experimental study on a new method of applying an association rule miner to discover useful information from customer inquiry database in a call center of a company it ha been claimed that association rule mining is not suited for text mining to overcome this problem we propose to generate sequential data set of word with dependency structure from the japanese text database and to employ a new method for extracting meaningful association rule by applying a new rule selection criterion each inquiry in the sequential data wa represented a a list of word pair each of which consists of a verb and it dependent noun the association rule were induced regarding each pair of word a an item the rule selection criterion come from our principle that we put heavier weight to co occurrence of multiple item more than single item occurrence we regarded a rule important if the existence of the item in the rule body significantly affect the occurrence of the item in the rule head the selected rule were then categorized to form meaningful information class with this method we succeeded in extracting useful information class from the text database which were not acquired by only simple keyword retrieval also inquiry with multiple aspect were properly classified into corresponding multiple category 
many supervised and unsupervised learning algorithm are very sensitive to the choice of an appropriate distance metric while classification task can make use of class label information for metric learning such information is generally unavailable in conventional clustering task some recent research sought to address a variant of the conventional clustering problem called semi supervised clustering which performs clustering in the presence of some background knowledge or supervisory information expressed a pairwise similarity or dissimilarity constraint however existing metric learning method for semi supervised clustering mostly perform global metric learning through a linear transformation in this paper we propose a new metric learning method which performs nonlinear transformation globally but linear transformation locally in particular we formulate the learning problem a an optimization problem and present two method for solving it through some toy data set we show empirically that our locally linear metric adaptation llma method can handle some difficult case that cannot be handled satisfactorily by previous method we also demonstrate the effectiveness of our method on some real data set 
in this paper we propose a new approach to discover informative content from a set of tabular document or web page of a web site our system infodiscoverer first partition a page into several content block according to html tag in a web page based on the occurrence of the feature term in the set of page it calculates entropy value of each feature according to the entropy value of each feature in a content block the entropy value of the block is defined by analyzing the information measure we propose a method to dynamically select the entropy threshold that partition block into either informative or redundant informative content block are distinguished part of the page whereas redundant content block are common part based on the answer set generated from manually tagged news web site with a total of web page experiment show that both recall and precision rate are greater than that is using the approach informative block news article of these site can be automatically separated from semantically redundant content such a advertisement banner navigation panel news category etc by adopting infodiscoverer a the preprocessor of information retrieval and extraction application the retrieval and extracting precision will be increased and the indexing size and extracting complexity will also be reduced 
classification tree are one of the most popular type of classifier with ease of implementation and interpretation being among their attractive feature despite the widespread use of classification tree theoretical analysis of their performance is scarce in this paper we show that a new family of classification tree called dyadic classification tree dcts are near optimal in a minimax sense for a very broad range of classification problem this demonstrates that other scheme e g neural network support vector machine cannot perform significantly better than dcts in many case we also show that this near optimal performance is attained with linear in the number of training data complexity growing and pruning algorithm moreover the performance of dcts on benchmark datasets compare favorably to that of standard cart which is generally more computationally intensive and which doe not posse similar near optimality property our analysis stem from theoretical result on structural risk minimization on which the pruning rule for dcts is based 
in gene expression microarray data analysis selecting a small number of discriminative gene from thousand of gene is an important problem for accurate classification of disease or phenotype the problem becomes particularly challenging due to the large number of feature gene and small sample size traditional gene selection method often select the top ranked gene according to their individual discriminative power without handling the high degree of redundancy among the gene latest research show that removing redundant gene among selected one can achieve a better representation of the characteristic of the targeted phenotype and lead to improved classification accuracy hence we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant gene the efficiency and effectiveness of our method in comparison with representative method ha been demonstrated through an empirical study using public microarray data set 
we propose a novel a framework for deriving approximation for intractable probabilistic model this framework is based on a free energy negative log marginal likelihood and can be seen a a generalization of adaptive tap and expectation propagation ep the free energy is constructed from two approximating distribution which encode different aspect of the intractable model such a single node constraint and coupling and are by construction consistent on a chosen set of moment we test the framework on a difficult benchmark problem with binary variable on fully connected graph and d grid graph we find good performance using set of moment which either specify factorized node or a spanning tree on the node structured approximation surprisingly the bethe approximation give very inferior result even on grid 
we formulate the problem of solving stochastic linear operator equation in a bayesian ganssian process gp framework the solution is obtained in the spirit of a collocation method based on noisy evaluation of the target function at randomly drawn or deliberately chosen point prior knowledge about the solution is encoded by the covariance kernel of the gp a in gp regression analytical expression for the mean and variance of the estimated target function are obtained from which the solution of the operator equation follows by a manipulation of the kernel linear initial and boundary value constraint can be enforced by embedding the non parametric model in a form that automatically satisfies the boundary condition the method is illustrated on a noisy linear first order ordinary differential equation with initial condition and on a noisy second order partial differential equation with dirichlet boundary condition 
visual action recognition is an important problem in computer vision in this paper we propose a new method to probabilistically model and recognize action of articulated object such a hand or bo dy gesture in image sequence our method consists of three level of representation at the low level we first extract a feature vector invar iant to scale and in plane rotation by using the fourier transform of a circular spatial histogram then spectral partitioning is utilized to obtain an initial clustering this clustering is then refined using a temporal smoothness constraint gaussian mixture model gmm based clustering and density estimation in the subspace of linear discriminant analysis lda are then applied to thousand of image feature vector to obtain an intermediate level representation finally at the high level we build a t emporal multiresolution histogram model for each action by aggregating the clustering weight of sampled image belonging to that action we discus how this high level representation can be extended to achieve temporal scaling invariance and to include bi gram or multi gram transition information both image clustering and action recognition segmentation result are given to show the validity of our three tiered representatio n 
tangent distance td is one classical method for invariant pattern classification however conventional td need pre obtain tangent vector which is difficult except for image object this paper extends td to more general pattern classification task the basic assumption is that tangent vector can be approximately represented by the pattern variation we propose three probabilistic subspace model to encode the variation the linear subspace nonlinear subspace and manifold subspace model these three model are addressed in a unified view namely probabilistic tangent subspace pt experiment show that pt can achieve promising classification performance in non image data set 
we devise and experiment with a dynamical kernel based system for tracking hand movement from neural activity the state of the system corresponds to the hand location velocity and accelerati on while the system s input are the instantaneous spike rate the syste m s state dynamic is defined a a combination of a linear mapping from the previous estimated state and a kernel based mapping tailored for modeling neural activity in contrast to generative model the activity to state mapping is learned using discriminative method by minimizing a noise robust loss function we use this approach to predict hand trajecto ries on the basis of neural activity in motor cortex of behaving monkey and find that the proposed approach is more accurate than both a stati c approach based on support vector regression and the kalman filter 
designing a hypothesis test to determine the best of two machine learning algorithm with only a small data set available is not a simple task many popular test suer from low power x cv or high type i error weka s x cross validation furthermore many test show a low level of replicability so that test performed by different scientist with the same pair of algorithm the same data set and the same hypothesis test still may present dierent result we show that x cv resampling and fold cv suer from low replicability the main complication is due to the need to use the data multiple time a a consequence independence assumption for most hypothesis test are violated in this paper we pose the case that reuse of the same data cause the eective degree of freedom to be much lower than theoretically expected we show how to calibrate the eective degree of freedom empirically for various test some test are not calibratable indicating another flaw in the design however the one that are calibratable all show very similar behavior moreover the type i error of those test is on the mark for a wide range of circumstance while they show a power and replicability that is a considerably higher than currently popular hypothesis test 
we show the existence of critical point a line for the likelihood function of mixture type model they are given by embedding of a critical point for model with le component a sufficient condition that the critical line give local maximum or saddle point is also derived based on this fact a component split method is proposed for a mixture of gaussian component and it effectiveness is verified through experiment this paper discus the critical point of the likelihood function for mixture type model by analyzing their hierarchical symmetric structure a generalization of we show that given a critical point of the likelihood for the model with h component duplication of any of the component give critical point a line for the model with h component we call them critical line of mixture model we derive also a sufficient condition that the critical line give maximum or saddle point of the larger model and show that given a maximum of the likelihood for a mixture of gaussian component an appropriate split of any component always give an ascending direction of the likelihood based on this theory we propose a stable method of splitting a component which work effectively with the em optimization for avoiding the dependency on the initial condition and improving the optimization the usefulness of the algorithm is verified through experiment 
machine learning is often used to automatically solve human task in this paper we look for task where machine learning algorithm are not a good a human with the hope of gaining insight into their current limitation we studied various human interactive proof hip on the market because they are system designed to tell computer and human apart by posing challenge presumably too hard for computer we found that most hip are pure recognition task which can easily be broken using machine learning the harder hip use a combination of segmentation and recognition task from this observation we found that building segmentation task is the most effective way to confuse machine learning algorithm this ha enabled u to build effective hip which we deployed in msn passport a well a design challenging segmentation task for machine learning algorithm 
we extend recent work on the connection between loopy belief propagation and the bethe free energy constrained minimization of the bethe free energy can be turned into an unconstrained saddle point problem both converging double loop algorithm and standard loopy belief propagation can be interpreted a attempt to solve this saddle point problem stability analysis then lead u to conclude that stable fixed point of loopy belief propagation must be local minimum of the bethe free energy 
we propose a new unsupervised learning technique for extracting information from large text collection we model document a if they were generated by a two stage stochastic process each author is represented by a probability distribution over topic and each topic is represented a a probability distribution over word for that topic the word in a multi author paper are assumed to be the result of a mixture of each author topic mixture the topic word and author topic distribution are learned from data in an unsupervised manner using a markov chain monte carlo algorithm we apply the methodology to a large corpus of abstract and author from the well known citeseer digital library and learn a model with topic we discus in detail the interpretation of the result discovered by the system including specific topic and author model ranking of author by topic and topic by author significant trend in the computer science literature between and parsing of abstract by topic and author and detection of unusual paper by specific author an online query interface to the model is also discussed that allows interactive exploration of author topic model for corpus such a citeseer 
an auditory scene composed of overlapping acoustic source can be viewed a a complex object whose constituent part are the individual source pitch is known to be an important cue for auditory scene analysis in this paper with the goal of building agent that oper ate in human environment we describe a real time system to identify the presence of one or more voice and compute their pitch the signal processing in the front end is based on instantaneous frequency estimation a method for tracking the partial of voiced speech while the pattern m atching in the back end is based on nonnegative matrix factorization an unsupervised algorithm for learning the part of complex object while supporting a framework to analyze complicated auditory scene our system maintains real time operability and state of the art performance i n clean speech 
many enterprise incorporate information gathered from a variety of data source into an integrated input for some learning task for example aiming towards the design of an automated diagnostic tool for some disease one may wish to integrate data gathered in many different hospital a major obstacle to such endeavor is that different data source may vary considerably in the way they choose to represent related data in practice the problem is usually solved by a manual construction of semantic mapping and translation between the different source recently there have been attempt to introduce automated algorithm based on machine learning tool for the construction of such translation in this work we propose a theoretical framework for making classification prediction from a collection of different data source without creating explicit translation between them our framework allows a precise mathematical analysis of the complexity of such task and it provides a tool for the development and comparison of different learning algorithm our main objective at this stage is to demonstrate the usefulness of computational learning theory to this practically important area and to stimulate further theoretical and experimental research of question related to this framework 
we present an algorithm to perform blind one microphone speech separation our algorithm separate mixture of speech without modeling individual speaker instead we formulate the problem of speech separation a a problem in segmenting the spectrogram of the signal into two or more disjoint set we build feature set for our segmenter using classical cue from speech psychophysics we then combine these feature into parameterized affinity matrix we also take adv antage of the fact that we can generate training example for segmentation by artificially superposing separately recorded signal thus the parameter of the affinity matrix can be tuned using recent work on learni ng spectral clustering this yield an adaptive speech specific se gmentation algorithm that can successfully separate one microphone speech mixture 
we investigate data based procedure for selecting the kernel when learning with support vector machine we provide generalization error bound by estimating the rademacher complexity of the corresponding function class in particular we obtain a complexity bound for function class induced by kernel with given eigenvectors i e we allow to vary the spectrum and keep the eigenvectors fix this bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel however optimizing the margin over such class lead to overfitting we thus propose a suitable way of constraining the class we use an efficient algorithm to solve the resulting optimization problem present preliminary experimental result and compare them to an alignment based approach 
abstract when clustering a dataset the right number k of cluster to use is often not obvious and choosing k automatically is a hard algorithmic problem in this paper we present an improved algorithm for learning k while clustering the g mean algorithm is based on a statistical test for the hypothesis that a subset of data follows a gaussian distribution g mean run k mean with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each 
we developed a robust control policy design method in high dimensional state space by using differential dynamic programming with a minimax criterion a an example we applied our method to a simulated five link biped robot the result show lower joint torque from the optimal control policy compared to a hand tuned pd servo controller result also show that the simulated biped robot can successfully walk with unknown disturbance that cause controller generated by standard differential dynamic programming and the hand tuned pd servo to fail learning to compensate for modeling error and previously unknown disturbance in conjunction with robust control design is also demonstrated 
how to determine a priori whether a learning algorithm is suited to a learning problem instance is a major scientific and technological challenge a first step toward this goal inspired by the phase transition pt paradigm developed in the constraint satisfaction domain is presented in this paper based on the pt paradigm extensive and principled experiment allow for constructing the competence map associated to a learning algorithm describing the region where this algorithm on average fails or succeeds the approach is illustrated on the long and widely used c algorithm a non trivial failure region in the landscape of k term dnf language is observed and some interpretation are offered for the experimental result 
in this paper we present a general machine learning approach to the problem of deciding when to share probabilistic belief between agent for distributed monitoring our approach can generally be applied to domain that use a probabilistic model for evaluating hypothesis and have a method for combining belief from multiple agent we demonstrate the effectiveness of our approach in a concrete application in network intrusion detection a an example of a multi agent monitoring problem based on an evaluation using packet trace data from a real network we demonstrate that our learning approach can reduce both the delay and communication overhead required to detect network intrusion 
in various application domain including image recognition it is natural to represent each example a a set of vector with a base kernel we can implicitly map these vector to a hilbert space and flt a gaussian distribution to the whole set using kernel pca we deflne our kernel between example a bhattacharyya s measure of a nity between such gaussians the resulting kernel is computable in closed form and enjoys many favorable property including graceful behavior under transformation potentially justifying the vector set representation even in case when more conventional representation also exist 
we describe the tivo television show collaborative recommendation system which ha been fielded in over one million tivo client for four year over this install base tivo currently ha approximately million rating by user over approximately distinct tv show and movie tivo us an item item show to show form of collaborative filtering which obviates the need to keep any persistent memory of each user s viewing preference at the tivo server taking advantage of tivo s client server architecture ha produced a novel collaborative filtering system in which the server doe a minimum of work and most work is delegated to the numerous client nevertheless the server side processing is also highly scalable and parallelizable although we have not performed formal empirical evaluation of it accuracy internal study have shown it recommendation to be useful even for multiple user household tivo s architecture also allows for throttling of the server so if more server side resource become available more correlation can be computed on the server allowing tivo to make recommendation for niche audience 
given a set of hidden variable with an a priori markov structure we derive an online algorithm which approximately update the posterior a pairwise measurement between the hidden variable become available the update is performed using assumed density filtering to incorporate each pairwise measurement we compute the optimal markov structure which represents the true posterior and use it a a prior for incorporating the next measurement we demonstrate the resulting algorithm by calculating globally consistent trajectory of a robot a it navigates along a d trajectory to update a trajectory of length t the update take o t when all conditional distribution are linear gaussian the algorithm can be thought of a a kalman filter which simplifies the state covariance matrix after incorporating each measurement 
we investigate the following problem given a set of document of a particular topic or class and a large set of mixed document that contains document from class and other type of document identify the document from class in the key feature of this problem is that there is no labeled nondocument which make traditional machine learning technique inapplicable a they all need labeled document of both class we call this problem partially supervised classification in this paper we show that this problem can be posed a a constrained optimization problem and that under appropriate condition solution to the constrained optimization problem will give good solution to the partially supervised classification problem we present a novel technique to solve the problem and demonstrate the effectiveness of the technique through extensive experimentation 
many technique for association rule mining and feature selection require a suitable metric to capture the dependency among variable in a data set for example metric such a support confidence lift correlation and collective strength are often used to determine the interestingness of association pattern however many such measure provide conflicting information about the interestingness of a pattern and the best metric to use for a given application domain is rarely known in this paper we present an overview of various measure proposed in the statistic machine learning and data mining literature we describe several key property one should examine in order to select the right measure for a given application domain a comparative study of these property is made using twenty one of the existing measure we show that each measure ha different property which make them useful for some application domain but not for others we also present two scenario in which most of the existing measure agree with each other namely support based pruning and table standardization finally we present an algorithm to select a small set of table such that an expert can select a desirable measure by looking at just this small set of table 
we describe an algorithm for support vector machine svm that can be parallelized efficiently and scale to very large problem with hundred of thousand of training vector instead of analyzing the whole training set in one optimization step the data are split into subset and optimized separately with multiple svms the partial result are combined and filtered again in a cascade of svms until the global optimum is reached the cascade svm can be spread over multiple processor with minimal communication overhead and requires far le memory since the kernel matrix are much smaller than for a regular svm convergence to the global optimum is guaranteed with multiple pass through the cascade but already a single pas provides good generalization a single pas is x x faster than a regular svm for problem of vector when implemented on a single processor parallel implementation on a cluster of processor were tested with over million vector class problem converging in a day or two while a regular svm never converged in over a week 
co training is a method for combining labeled and unlabeled data when example can be thought of a containing two distinct set of feature it ha had a number of practical success yet previous theoretical analysis have needed very strong assumption on the data that are unlikely to be satisfied in practice in this paper we propose a much weaker expansion assumption on the underlying data distribution that we prove is sufficient fo r iterative cotraining to succeed given appropriately strong pac learning algorithm on each feature set and that to some extent is necessary a well this expansion assumption in fact motivates the iterative nature of the original co training algorithm unlike stronger assumption s uch a independence given the label that allow a simpler one shot co trai ning to succeed we also heuristically analyze the effect on performance of noise in the data predicted behavior is qualitatively matched in sy nthetic experiment on expander graph 
we consider incorporating action elimination procedure in reinforcement learning algorithm we suggest a framework that is based on learning an upper and a lower estimate of the value function or the q function and eliminating action that are not optimal we provide a model based and a model free variant of the elimination method we further derive stopping condition that guarantee that the learned policy is approximately optimal with high probability simulation demonstrate a considerable speedup and added robustness 
abstract we derive an optimal learning rule in the sense of mutual information maximization for a spiking neuron model under the assumption of small fluctuation of the input we find a spike timing dependent plasticity stdp function which depends on the time course of excitatory postsynaptic potential epsps and the autocorrelation function of the postsynaptic neuron we show that the stdp function ha both positive and negative phase the positive phase is related to the shape of the epsp while the negative phase is controlled by neuronal refractoriness 
a distance based conditional model on the ranking poset is presented for use in classification and ranking the model is an extension of the mallow model and generalizes the classifier combination method used by several ensemble learning algorithm including error correcting output code discrete adaboost logistic regression and cranking the algebraic structure of the ranking poset lead to a simple bayesian interpretation of the conditional model and it special case in addition to a unifying view the framework suggests a probabilistic interpretation for error correcting output code and an extension beyond the binary coding scheme 
autonomous helicopter flight represents a challenging control problem with complex noisy dynamic in this paper we describe a successful application of reinforcement learning to autonomous helicopter flight we first fit a stochastic nonlinear model of the helicopter dynamic we then use the model to learn to hover in place and to fly a number of maneuver taken from an rc helicopter competition 
we are concerned with the issue of outlier detection and change point detection from a data stream in the area of data mining there have been increased interest in these issue since the former is related to fraud detection rare event discovery etc while the latter is related to event trend by change detection activity monitoring etc specifically it is important to consider the situation where the data source is non stationary since the nature of data source may change over time in real application although in most previous work outlier detection and change point detection have not been related explicitly this paper present a unifying framework for dealing with both of them on the basis of the theory of on line learning of non stationary time series in this framework a probabilistic model of the data source is incrementally learned using an on line discounting learning algorithm which can track the changing data source adaptively by forgetting the effect of past data gradually then the score for any given data is calculated to measure it deviation from the learned model with a higher score indicating a high possibility of being an outlier further change point in a data stream are detected by applying this scoring method into a time series of moving averaged loss for prediction using the learned model specifically we develop an efficient algorithm for on line discounting learning of auto regression model from time series data and demonstrate the validity of our framework through simulation and experimental application to stock market data analysis 
unsupervised clustering can be significantly improved using supervision in the form of pairwise constraint i e pair of instance labeled a belonging to same or different cluster in recent year a number of algorithm have been proposed for enhancing clustering quality by employing such supervision such method use the constraint to either modify the objective function or to learn the distance measure we propose a probabilistic model for semi supervised clustering based on hidden markov random field hmrfs that provides a principled framework for incorporating supervision into prototype based clustering the model generalizes a previous approach that combine constraint and euclidean distance learning and allows the use of a broad range of clustering distortion measure including bregman divergence e g euclidean distance and i divergence and directional similarity measure e g cosine similarity we present an algorithm that performs partitional semi supervised clustering of data by minimizing an objective function derived from the posterior energy of the hmrf model experimental result on several text data set demonstrate the advantage of the proposed framework 
an important task in unsupervised learning is maximum likelihood mixture estimation mlme for exponential family in this paper we prove a mathematical equivalence between this mlme problem and the rate distortion problem for bregman divergence we also present new theoretical result in rate distortion theory for bregman divergence further an analysis of the problem a a trade off between compression and preservation of information is presented that yield the information bottleneck method a an interesting special case 
in most computer system page fault rate is currently minimized by generic page replacement algorithm which try to model the temporal locality inherent in program in this paper we propose two algorithm one greedy and the other stochastic designed for program specific code restructuring a a mean of increasing spatial locality within a program both algorithm effectively decrease average working set size and hence the page fault rate our method are more effective than traditional approach due to use of domain information we illustrate the efficacy of our algorithm on actual data mining algorithm 
in this paper we present a new algorithm suitable for matching discrete object such a string and tree in linear time thus obviat ing dynamic programming with quadratic time complexity furthermore prediction cost in many case can be reduced to linear cost in the length of the sequence to be classified regardless of the number of support v ectors this improvement on the currently available algorithm make string kernel a viable alternative for the practitioner 
we consider a graph theoretic approach for automatic construction of option in a dynamic environment a map of the environment is generated on line by the learning agent representing the topological structure of the state transition a clustering algorithm is then used to partition the state space to different region policy for reaching the different part of the space are separately learned and added to the model in a form of option macro action the option are used for accelerating the q learning algorithm we extend the basic algorithm and consider building a map that includes preliminary indication of the location of interesting region of the state space where the value gradient is significant and additional exploration might be beneficial experiment indicate significant speedup especially in the initial learning phase 
gaussian process provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observation in an empirical model this is of particular importance in identification of nonlinear dynamic system from experimental data it allows u to combine derivative information and associated uncertainty with normal function observation into the learning and inference process this derivative information can be in the form of prior specified by an expert or identified from perturbation data close to equilibrium it allows a seamless fusion of multiple local linear model in a consistent manner inferring consistent model and ensuring that integrability constraint are met it improves dramatically the computational efficiency of gaussian process model for dynamic system identification by summarising large quantity of near equilibrium data by a handful of linearisations reducing the training set size traditionally a problem for gaussian process model 
this paper analyzes the performance of semisupervised learning of mixture model we show that unlabeled data can lead to an increase in classification error even in situation where additional labeled data would decrease classification error this behavior contradicts several empirical result reported in the literature we present a mathematical analysis of this degradation phenomenon and show that it is due to the fact that bias may be adversely affected by unlabeled data we study the impact of these theoretical result to classifier based on bayesian network some situation call for structural learning while others are best handled by relatively simple classifier 
psychophysical study suggest the existence of specialized detector for component motion pat tern radial circular and spiral that are consistent with the visual motion property of cell in the dorsal medial superior t emporal area mstd of non human primate here we use a bi ologically constrained model of visual motion proce ssing i n mstd i n conjunction wi th psychophysical performance on two motion pattern task to elucidate the computational mechanism associated with the processing of widefield motion pattern encountered during self motion in both task discrimination t hresholds varied si gnificantly with the type of motion pat tern presented suggesting perceptual correlate t o the preferred motion bi a repo rted i n mstd through t he model we demonstrate that while independently responding motion pat tern unit are capable of encoding information relevant to the vi sual motion t asks equivalent psychophysical performance can only be achieved using interconnected neural population that systematically i nhibit non responsive unit these result suggest the cyclic trend in psychophysical performance may be mediated in part by recurrent connection within motion pattern responsive area whose structure is a funct ion of the sim ilarity in preferred motion pattern and receptive field location between unit 
replicability of machine learning experiment measure how likely it is that the outcome of one experiment is repeated when performed with a different randomization of the data in this paper we present an estimator of replicability of an experiment that is efficient more precisely the estimator is unbiased and ha lowest variance in the class of estimator formed by a linear combination of outcome of experiment on a given data set we gathered empirical data for comparing experiment consisting of different sampling scheme and hypothesis test both factor are shown to have an impact on replicability of experiment the data suggests that sign test should not be used due to low replicability ranked sum test show better performance but the combination of a sorted run sampling scheme with a t test give the most desirable performance judged on type i and ii error and replicability 
many work have shown that strong connection relate learning from example to regularization technique for ill posed inverse problem nevertheless by now there wa no formal evidence neither that learning from example could be seen a an inverse problem nor that theoretical result in learning theory could be independently derived using tool from regularization theory in this paper we provide a positive answer to both question indeed considering the square loss we translate the learning problem in the language of regularization theory and show that consistency result and optimal regularization parameter choice can be derived by the discretization of the corresponding inverse problem 
in front of modern database noise tolerance ha become today one of the most studied topic in machine learning many algorithm have been suggested for dealing with noisy data in the case of numerical instance either by filtering them during a preprocess or by treating them during the induction however this research subject remains widely open when one learns from unbounded symbolic sequence which is the aim in grammatical inference in this paper we propose a statistical approach for dealing with noisy data during the inference of automaton by the state merging algorithm rpni our approach is based on a proportion comparison test which relaxes the merging rule of rpni without endangering the generalization error beyond this relevant framework we provide some useful theoretical property about the behavior of our new version of rpni called rpni finally we describe a large comparative study on several datasets 
the purpose of this paper is to investigate infinity sample property of risk minimization based multi category classification method these method can be considered a natural extension to binary large margin classification we establish condition that guarantee the infinity sample consistency of classifier obtained in the risk minimization framework example are provided for two specific form of the general formulation which extend a number of known method using these example we show that some risk minimization formulation can also be used to obtain conditional probability estimate for the underlying problem such conditional probability information will be useful for statistical inferencing task beyond classification 
this paper address cost sensitive classication in the setting where there are cost for measuring each attribute a well a cost for misclassication error we show how to formulate this a a markov decision process in which the transition model is learned from the training data specically we assume a set of training example in which all attribute and the true class have been measured we describe a learning algorithm based on the ao heuristic search procedure that search for the classication policy with minimum expected cost we provide an admissible heuristic for ao that substantially reduces the number of node that need to be expanded particularly when attribute measurement cost are high to further prune the search space we introduce a statistical pruning heuristic based on the principle that if the value of two policy are statistically indistinguishable on the training data then we can prune one of the policy from the ao search space experiment with realistic and synthetic data demonstrate that these heuristic can substantially reduce the memory needed for ao search without signicantly aecting the quality of the learned policy hence these heuristic expand the range of cost sensitive learning problem for which ao is feasible 
missing data is common in real world datasets and is a problem for many estimation technique we have developed a variational bayesian method to perform independent component analysis ica on high dimensional data containing missing entry missing data are handled naturally in the bayesian framework by integrating the generative density model modeling the distribution of the independent source with mixture of gaussians allows source to be estimated with different kurtosis and skewness the variational bayesian method automatically determines the dimensionality of the data and yield an accurate density model for the observed data without overfitting problem this allows direct probability estimation of missing value in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data 
the problem of extracting the relevant aspect of data in face of multiple conflicting structure is inherent to modeling of complex data extracting structure in one random variable that is relevant for another variable ha been principally addressed recently via the information bottleneck method however such auxiliary variable often contain more information than is actually required due to structure that are irrelevant for the task in many other case it is in fact easier to specify what is irrelevant than what is for the task at hand identifying the relevant structure however can thus be considerably improved by also minimizing the information about another irrelevant variable in this paper we give a general formulation of this problem and derive it formal a well a algorithmic solution it operation is demonstrated in a synthetic example and in the context of text categorization while the original information bottleneck problem is related to rate distortion theory with the distortion measure replaced by the relevant information extracting discriminative relevant feature is formally related to rate distortion with side information 
bayesian network are a powerful probabilistic representation and their use for classification ha received considerable attention however they tend to perform poorly when learned in the standard way this is attributable to a mismatch between the objective function used likelihood or a function thereof and the goal of classification maximizing accuracy or conditional likelihood unfortunately the computational cost of optimizing structure and parameter for conditional likelihood is prohibitive in this paper we show that a simple approximation choosing structure by maximizing conditional likelihood while setting parameter by maximum likelihood yield good result on a large suite of benchmark datasets this approach produce better class probability estimate than naive bayes tan and generatively trained bayesian network 
we present a novel generative model for natural language tree structure in which semantic lexical dependency and syntactic pcfg structure are scored with separate model this factorization provides conceptual simplicity straightforward opportunity for separately improving the component model and a level of performance comparable to similar non factored model most importantly unlike other modern parsing model the factored model admits an extremely effective a parsing algorithm which enables efficient exact inference 
we present a method to distinguish direct connection between two neuron from common input originating from other unmeasured neuron the distinction is computed from the spike time of the two neuron in response to a white noise stimulus although the method is based on a highly idealized linear nonlinear approximation of neural response we demonstrate via simulation that the approach can work with a more realistic integrate and fire neuron model we propose that the approach exemplified by this analysis may yield viable tool for reconstructing stimulus driven neural network from data gathered in neurophysiology experiment 
we give a statistical interpretation of proximal support vector machine psvm proposed at kdd a linear approximaters to nonlinear support vector machine svm we prove that psvm using a linear kernel is identical to ridge regression a biased regression method known in the statistical community for more than thirty year technique from the statistical literature to estimate the tuning constant that appears in the svm and psvm framework are discussed better shrinkage strategy that incorporate more than one tuning constant are suggested for nonlinear kernel the minimization problem posed in the psvm framework is equivalent to finding the posterior mode of a bayesian model defined through a gaussian process on the predictor space apart from providing new insight these interpretation help u attach an estimate of uncertainty to our prediction and enable u to build richer class of model in particular we propose a new algorithm called psvmmix which is a combination of ridge regression and a gaussian process model extension to the case of continuous response is straightforward and illustrated with example datasets 
many algorithm rely critically on being given a good metric over their input for instance data can often be clustered in many plausible way and if a clustering algorithm such a k mean initially fails to find one that is meaningful to a user the only recourse may be for the user to manually tweak the metric until sufficiently good cluster are found for these and other application requiring good metric it is desirable that we provide a more systematic way for user to indicate what they consider similar for instance we may ask them to provide example in this paper we present an algorithm that given example of similar and if desired dissimilar pair of point in learns a distance metric over that respect these relationship our method is based on posing metric learning a a convex optimization problem which allows u to give efficient local optimum free algorithm we also demonstrate empirically that the learned metric can be used to significantly improve clustering performance 
we present a method for constructing ensemble from library of thousand of model model library are generated using different learning algorithm and parameter setting forward stepwise selection is used to add to the ensemble the model that maximize it performance ensemble selection allows ensemble to be optimized to performance metric such a accuracy cross entropy mean precision or roc area experiment with seven test problem and ten metric demonstrate the benefit of ensemble selection 
simplicity of linear representation make them a popular tool in image analysis the two widely used linear representation are i linear projection of image to low dimensional euclidean subspace and ii linear spectral filtering of image in view of the orthogonality and other constraint imposed on these representation the subspace or the filter they take value on nonlinear manifold grassmann stiefel or rotation group we use a family of stochastic algorithm that exploit the geometry of the underlying manifold to find optimal linear representation for specified task a application we demonstrate the effectiveness of algorithm by finding subspace with optimal generalization through separation maximization and filter that are both sparse and effective for recognition we also show empirically how the learned representation can improve the performance of support vector machine 
the rapid growth of the world wide web ha created many challenge for both general purpose crawling search engine and web directory making it difficult to find index and classify web page based on a topic topic driven crawler can complement search engine because they pre classify the page retrieved by the crawl to implement such a focused crawler a strategy for ordering the crawl frontier is required such a strategy can only use information gleaned from previously crawled page to estimate the relevance of a newly observed url because the best strategy for ranking url in the crawl frontier is not immediately apparent we discover strategy by evolving them using a genetic algorithm strategy are learned by evaluating the result of crawl simulated using a database generated by a previous more general crawl we conclude that a rank function that combine analysis of text and link structure yield effective strategy the evolved strategy perform better than the commonly used best first strategy 
abstract standard value function approach to finding policy for partially observable markov decision process pomdps are intractable for large model the intractability of these algorithm is due to a great extent to their generating an optimal policy over the entire belief space however in real pomdp problem most belief state are unlikely and there is a structured low dimensional manifold of plausible belief embedded in the high dimensional belief space we introduce a new method for solving large scale pomdps by taking advantage of belief space sparsity we reduce the dimensionality of the belief space by exponential family principal component analysis which allows u to turn the sparse highdimensional belief space into a compact low dimensional representation in term of learned feature of the belief state we then plan directly on the low dimensional belief feature by planning in a low dimensional space we can find policy for pomdps that are order of magnitude larger than can be handled by conventional technique we demonstrate the use of this algorithm on a synthetic problem and also on a mobile robot navigation task 
a neurotrophic model for the co development of topography and ocular dominance column in the primary visual cortex ha recently been proposed in the present work we test this model by driving it with the output of a pair of neuronal vision sensor stimulated by disparate moving pattern we show that the temporal correlation in the spike train generated by the two sensor elicit the development of refined topography and ocular dominance column even in the presence of significant amount of spontaneous activity and fixed pattern noise in the sensor 
we present a new method for automatically creating useful temporal abstraction in reinforcement learning we argue that state that allow the agent to transition to a different region of the state space are useful subgoals and propose a method for identifying them using the concept of relative novelty when such a state is identified a temporally extended activity e g an option is generated that take the agent efficiently to this state we illustrate the utility of the method in a number of task 
we address the problem of identifying specific instance of a class car from a set of image all belonging to that class although we cannot build a model for any particular instance a we may be provided with only one training example of it we can use information extracted from observing other member of the class we pose this task a a learning problem in which the learner is given image pair labeled a matching or not and must discover which image feature are most consistent for matching instance and discriminative for mismatch we explore a patch based representation where we model the distribution of similarity measurement defined on the patch finally we describe an algorithm that selects the most salient patch based on a mutual information criterion this algorithm performs identification well for our challenging dataset of car image after matching only a few well chosen patch 
naive bayes is often used a a baseline in text classification because it is fast and easy to implement it severe assumption make such efficiency possible but also adversely affect the quality of it result in this paper we propose simple heuristic solution to some of the problem with naive bayes classifier addressing both systemic issue a well a problem that arise because text is not actually generated according to a multinomial model we find that our simple correction result in a fast algorithm that is competitive with state of the art text classification algorithm such a the support vector machine 
a longstanding goal of reinforcement learning is to develop nonparametric representation of policy and value function that support rapid learning without suffering from interference or the curse of dimensionality we have developed a trajectory based approach in which policy and value function are represented nonparametrically along trajectory these trajectory policy and value function are updated a the value function becomes more accurate or a a model of the task is updated we have applied this approach to periodic task such a hopping and walking which required handling discount factor and discontinuity in the task dynamic and using function approximation to represent value function at discontinuity we also describe extension of the approach to make the policy more robust to modeling error and sensor noise 
recent year have witnessed a dramatic increase in the quantity of image data collected due to advance in field such a medical imaging reconnaissance surveillance astronomy multimedia etc with this increase ha come the need to be able to store transmit and query large volume of image data efficiently a common operation on image database is the retrieval of all image that are similar to a query image for this the image in the database are often represented a vector in a high dimensional space and a query is answered by retrieving all image vector that are proximal to the query image in this space under a suitable similarity metric to overcome problem associated with high dimensionality such a high storage and retrieval time a dimension reduction step is usually applied to the vector to concentrate relevant information in a small number of dimension principal component analysis pca is a well known dimension reduction scheme however since it work with vectorized representation of image pca doe not take into account the spatial locality of pixel in image in this paper a new dimension reduction scheme called generalized principal component analysis gpca is presented this scheme work directly with image in their native state a two dimensional matrix by projecting the image to a vector space that is the tensor product of two lower dimensional vector space experiment on database of face image show that for the same amount of storage gpca is superior to pca in term of quality of the compressed image query precision and computational cost 
high dimensional directional data is becoming increasingly important in contemporary application such a analysis of text and gene expression data a natural model for multi variate directional data is provided by the von mi fisher vmf distribution on the unit hypersphere that is analogous to the multi variate gaussian distribution in rd in this paper we propose modeling complex directional data a a mixture of vmf distribution we derive and analyze two variant of the expectation maximization em framework for estimating the parameter of this mixture we also propose two clustering algorithm corresponding to these variant an interesting aspect of our methodology is that the spherical kmeans algorithm kmeans with cosine similarity can be shown to be a special case of both our algorithm thus modeling text data by vmf distribution lends theoretical validity to the use of cosine similarity which ha been widely used by the information retrieval community a part of experimental validation we present result on modeling high dimensional text and gene expression data a a mixture of vmf distribution the result indicate that our approach yield superior clustering especially for difficult clustering task in high dimensional space 
abstract we propose the framework of mutual information kernel for learning covariance kernel a used in support vector machine and gaussian process classiers from unlabeled task data using bayesian technique we describe a powerful implementation of this framework which us variational bayesian mixture of factor analyzer in order to attack classication problem in high dimensional space where labeled data is sparse but unlabeled data is abundant content 
we propose a framework to incorporate unlabeled data in kernel classifier based on the idea that two point in the same cluster are more likely to have the same label this is achieved by modifying the eigenspectrum of the kernel matrix experimental result ass the validity of this approach 
a method is described for real time market intelligence and competitive analysis news story are collected online for a designated group of company the goal is to detect critical difference in the text written about a company versus the text for it competitor a solution is found by mapping the task into a non stationary text categorization model the overall design consists of the following component a a real time crawler that monitor newswires for story about the competitor b a conditional document retriever that selects only those document that meet the indicated condition c text analysis technique that convert the document to a numerical format d rule induction method for finding pattern in data e presentation technique for displaying result the method is extended to combine text with numerical measure such a those based on stock price and market capitalization that allow for more objective evaluation and projection 
amino acid profile which capture position specific mutation probability are a richer encoding of biological sequence than the individual sequence themselves however profile comparison are much more computationally expensive than discrete symbol comparison making profile impractical for many large datasets furthermore because they are such a rich representation profile can be dicult to visualize to overcome these problem we propose a discretization for profile using an expanded alphabet representing not just individual amino acid but common profile by using an extension of information bottleneck ib incorporating constraint and prior on the class distribution we find an informationally optimal alphabet this discretization yield a concise informative textual representation for profile sequence also alignment between these sequence while nearly a accurate a the full profileprofile alignment can be computed almost a quickly a those between individual or consensus sequence a full pairwise alignment of swissprot would take year using profile but le than day using a discrete ib encoding illustrating how discrete encoding can expand the range of sequence problem to which profile information can be applied 
we propose to use the community structure of usenet for organizing and retrieving the information stored in newsgroups in particular we study the network formed by cross post message that are posted to two or more newsgroups simultaneously we present what is to our knowledge by far the most detailed data that ha been collected on usenet cross posting we analyze this network to show that it is a small world network with significant clustering we also present a spectral algorithm which cluster newsgroups based on the cross post matrix the result of our clustering provides a topical classification of newsgroups our clustering give many example of significant relationship that would be missed by semantic clustering method 
in this paper we propose a probabilistic model for online document clustering we use non parametric dirichlet process prior to model the growing number of cluster and use a prior of general english language model a the base distribution to handle the generation of novel cluster furthermore cluster uncertainty is modeled with a bayesian dirichletmultinomial distribution we use empirical bayes method to estimate hyperparameters based on a historical dataset our probabilistic model is applied to the novelty detection task in topic detection and tracking tdt and compared with existing approach in the literature 
we establish learning rate to the bayes risk for support vector machine svms with hinge loss in particular for svms with gaussian rbf kernel we propose a geometric condition for distribution which can be used to determine approximation property of these kernel finally we compare our method with a recent paper of g blanchard et al 
we present a framework for mining association rule from transaction consisting of categorical item where the data ha been randomized to preserve privacy of individual transaction while it is feasible to recover association rule and preserve privacy using a straightforward uniform randomization the discovered rule can unfortunately be exploited to find privacy breach we analyze the nature of privacy breach and propose a class of randomization operator that are much more effective than uniform randomization in limiting the breach we derive formula for an unbiased support estimator and it variance which allow u to recover itemset support from randomized datasets and show how to incorporate these formula into mining algorithm finally we present experimental result that validate the algorithm by applying it on real datasets 
a novel native stochastic local search algorithm for solving k term dnf problem is presented it is evaluated on hard k term dnf problem that lie on the phase transition and compared to the performance of gsat and walksat type algorithm on sat encoding of k term dnf problem we also evaluate state of the art separate and conquer algorithm on these problem finally we demonstrate the practical relevance of our algorithm on a chess endgame database 
introductioninformation retrieval is in large part the study of method for assessing the similarity of pair ofdocuments document similarity metric have been used for many task including ad hocdocument retrieval text classification yc and summarization gc ssmb another problem area in which similarity metric are central is record linkage e g ka where one wish to determine if two database record taken from different source database referto the same 
the problem of detecting atypical object or outlier is one of the classical topic in robust statistic recently it ha been proposed to address this problem by mean of one class svm classifier the main conceptual shortcoming of most one class approach however is that in a strict sense they are unable to detect outlier since the expected fraction of outlier ha to be specified in advance the method presented in this paper overcomes this problem by relating kernelized one class classification to gaussian density estimation in the induced feature space having established this relation it is possible to identify atypical object by quantifying their deviation from the gaussian model for rbf kernel it is shown that the gaussian model is rich enough in the sense that it asymptotically provides an unbiased estimator for the true density in order to overcome the inherent model selection problem a cross validated likelihood criterion for selecting all free model parameter is applied 
we present a new method for calculating approximate marginals for probability distribution defined by graph with cycle based on a gaussian entropy bound combined with a semidefinite outer bound on the marginal polytope this combination lead to a log determinant maximization problem that can be solved by efficient interior point method a with the bethe approximation and it generalization the optimizing argument of this problem can be taken a approximation to the exact marginals in contrast to bethe kikuchi approach our variational problem is strictly convex and so ha a unique global optimum an additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function in experimental trial the performance of the log determinant relaxation is comparable to or better than the sum product algorithm and by a substantial margin for certain problem class finally the zero temperature limit of our log determinant relaxation recovers a class of well known semidefinite relaxation for integer programming e g 
many application in text and speech processing require the analysis of distribution of variable length sequence we recently introduced a general kernel framework rational kernel to extend kernel method to the analysis of such variable length sequence or more generally weighted automaton these kernel are efficient to compute and have been successfully used in application such a spoken dialog classification using support vector machine however the rational kernel previously introduced do not fully encompass distribution over alternate sequence prior similarity measure between two weighted automaton are based only on the expected count of co occurring subsequence and ignore similarity or dissimilarity in higher order moment of the distribution of these count in this paper we introduce a new family of rational kernel moment kernel that precisely exploit this additional information these kernel are distribution kernel based on moment of count of string we describe efficient algorithm to compute moment kernel and apply them to several difficult spoken dialog classification task our experiment show that using the second moment of the count of n gram sequence consistently improves the classification accuracy in these task 
we present and discus the important business problem of estimating the effect of retention effort on the lifetime value of a customer in the telecommunication industry we discus the component of this problem in particular customer value and length of service or tenure modeling and present a novel segment based approach motivated by the segment level view marketing analyst usually employ we then describe how we build on this approach to estimate the effect of retention on lifetime value our solution ha been successfully implemented in amdocs business insight bi platform and we illustrate it usefulness in real world scenario 
an adaptive semi supervised ensemble method assemble is proposed that construct classification ensemble based on both labeled and unlabeled data assemble alternate between assigning pseudo class to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data mathematically this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space a measured on both the labeled and unlabeled of data unlike alternative approach assemble doe not require a semi supervised learning method for the base classifier assemble can be used in conjunction with any cost sensitive classification algorithm for both two class and multi class problem assemble using decision tree won the nip unlabeled data competition in addition strong result on several benchmark datasets using both decision tree and neural network support the proposed method 
in the context of binary classification we define disagreement a a measure of how often two independently trained model differ in their classification of unlabeled data we explore the use of disagreement for error estimation and model selection we call the procedure co validation since the two model effectively in validate one another by comparing result on unlabeled data which we assume is relatively cheap and plentiful compared to labeled data we show that per instance disagreement is an unbiased estimate of the variance of error for that instance we also show that disagreement provides a lower bound on the prediction generalization error and a tight upper bound on the variance of prediction error or the variance of the average error across instance where variance is measured across training set we present experimental result on several data set exploring co validation for error estimation and model selection the procedure is especially effective in active learning setting where training set are not drawn at random and cross validation overestimate error 
this paper report on a family of computationally practical classifier that converge to the bayes error at near minimax optimal rate for a variety of distribution the classifier are based on dyadic classification tree dcts which involve adaptively pruned partition of the feature space a key aspect of dcts is their spatial adaptivity which enables local rather than global fitting of the decision boundary ou r risk analysis involves a spatial decomposition of the usual concentration inequality leading to a spatially adaptive data dependent pruning cr iterion for any distribution on x y whose bayes decision boundary behaves locally like a lipschitz smooth function we show that the dct error converges to the bayes error at a rate within a logarithmic factor of the minimax optimal rate we also study dcts equipped with polynomial classification rule at each leaf and show that a the smoothness of the boundary increase their error converge to the bayes error at a rate a pproaching n the parametric rate we are not aware of any other practical classifier that provide similar rate of convergence guarantee f ast algorithm for tree pruning are discussed 
recent research on pattern discovery ha progressed form mining frequent itemsets and sequence to mining structured pattern including tree lattice and graph a a general data structure graph can model complicated relation among data with wide application in bioinformatics web exploration and etc however mining large graph pattern in challenging due to the presence of an exponential number of frequent subgraphs instead of mining all the subgraphs we propose to mine closed frequent graph pattern a graph g is closed in a database if there exists no proper supergraph of g that ha the same support a g a closed graph pattern mining algorithm closegraph is developed by exploring several interesting pruning method our performance study show that closegraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increase the efficiency of mining especially in the presence of large graph pattern 
decoding is a strategy that allows u to ass the amount of information neuron can provide about certain aspect of the visual scene in this study we develop a method based on bayesian sequential updating and the particle filtering algorithm to decode the activity of v neuron in awake monkey a distinction in our method is the use of volterra kernel to filter the particle which live in a high dimensional space this parametric bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder interestingly our result suggest that for decoding in real time spike train of a few a independent but similar neuron would be sufficient for decoding a critical scene variable in a particular class of visual stimulus the reconstructed variable can predict the neural activity about a well a the actual signal with respect to the volterra kernel bayesian decoding scheme which are nonlinear might be useful in this context bayesian sequential updating or belief propagation implemented in the form of particle filtering ha recently been used in estimating the hand trajectory of monkey based on m neuron s response and the location of a rat based on the response of the place cell in the hippocampus however linear method have been shown to be quite adequate for decoding lgn motor cortical or hippocampal place cell signal using population vector or the optimal linear decoder bayesian method with proper probability model assumption could work better than the linear method but they apparently are not critical to solving those problem these method may be more useful or important in the decoding of nonlinear visual neuronal response here we implement an algorithm based on bayesian sequential updating in the form particle filtering to decode nonlinear visual neuron in awake behaving monkey the strategy is similar to the one used by brown et 
we introduce a new algorithm based on linear programming that approximates the dieren tial value function of an average cost markov decision process via a linear combination of pre selected basis function the algorithm carry out a form of cost shaping and minimizes a version of bellman error we establish an error bound that scale gracefully with the number of state without imposing the strong lyapunov condition required by it counterpart in we propose a path following method that automates selection of important algorithm parameter which represent counterpart to the state relevance weight studied in 
consider a number of moving point where each point is attached to a joint of the human body and projected onto an image plane johannson showed that human can effortlessly detect and recognize the presence of other human from such display this is true even when some of the body point are missing e g because of occlusion and unrelated clutter point are added to the display we are interested in replicating this ability in a machine to this end we present a labelling and detection scheme in a probabilistic framework our method is based on representing the joint probability density of position and velocity of body point with a graphical model and using loopy belief propagation to calculate a likely interpretation of the scene furthermore we introduce a global variable representing the body s centroid experiment on one motion captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical model especially when very few part are visible the improvement is due both to the more general graph structure we use and more significantly to the introduction of the centroid variable 
we present a class of algorithm for learning the structure of graphical model from data the algorithm are based on a measure known a the kernel generalized variance kgv which essentially allows u to treat all variable on an equal footing a gaussians in a feature space obtained from mercer kernel thus we are able to learn hybrid graph involving discrete and continuous variable of arbitrary type we explore the computational property of our approach showing how to use the kernel trick to compute the relevant statistic in linear time we illustrate our framework with experiment involving discrete and continuous data 
when we learn a new motor skill we have to contend with both the variability inherent in our sensor and the task the sensory uncertainty can be reduced by using information about the distribution of previously experienced task here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback we show that subject internally represent both the distribution of the task a well a their sensory uncertainty moreover they combine these two source of information in a way that is qualitatively predicted by optimal bayesian processing we further analyze if the subject can represent multimodal distribution such a mixture of gaussians the result show that the cns employ probabilistic model during sensorimotor learning even when the prior are multimodal 
learning algorithm have enjoyed numerous success in robotic control task in problem with time varying dynamic online learning method have also proved to be a powerful tool for automatically tracking and or adapting to the changing circumstance however for safety critical application such a airplane flight the adoption of these algorithm ha been significantly hampered by their lack of safety such a stability guarantee rather than trying to show difficult a priori stability guarantee for specific learning method in this paper we propose a method for monitoring the controller suggested by the learning algorithm online and rejecting controller leading to instability we prove that even if an arbitrary online learning method is used with our algorithm to control a linear dynamical system the resulting system is stable 
we are interested in finding natural community in large scale linked network our ultimate goal is to track change over time in such community for such temporal tracking we require a clustering algorithm that is relatively stable under small perturbation of the input data we have developed an efficient scalable agglomerative strategy and applied it to the citation graph of the nec citeseer database paper million citation agglomerative clustering technique are known to be unstable on data in which the community structure is not strong we find that some community are essentially random and thus unstable while others are natural and will appear in most clustering these natural community will enable u to track the evolution of community over time 
given a database structure mining algorithm search for substructure that satisfy constraint such a minimum frequency minimum confidence minimum interest and maximum frequency example of substructure include graph tree and path for these substructure many mining algorithm have been proposed in order to make graph mining more efficient we investigate the use of the quickstart principle which is based on the fact that these class of structure are contained in each other thus allowing for the development of structure mining algorithm that split the search into step of increasing complexity we introduce the graph sequence tree extraction gaston algorithm that implement this idea by searching first for frequent path then frequent free tree and finally cyclic graph we investigate two alternative for computing the frequency of structure and present experimental result to relate these alternative 
many different high dimensional data set are characterized by the same underlying mode of variability when these mode of variability are continuous and few in number they can be viewed a parameterizing a low dimensional manifold the manifold provides a compact shared representation of the data suggesting correspondence between the high dimensional example from different data set these correspondence though naturally induced by the underlying manifold are difficult to learn using traditional method in supervised learning in this paper we generalize three method in unsupervised learning principal component analysis factor analysis and locally linear embedding to discover subspace and manifold that provide common low dimensional representation of different high dimensional data set we use the shared representation discovered by these algorithm to put high dimensional example from different data set into correspondence finally we show that a notion of self correspondence between example in the same data set can be used to improve the performance of these algorithm on small data set the algorithm are demonstrated on image and text 
in this paper we address the problem of detecting multiple topic or category of text where each text is not assumed to belong to one of a number of mutually exclusive category conventionally the binary classification approach ha been employed in which whether or not text belongs to a category is judged by the binary classifier for every category in this paper we propose a more sophisticated approach to simultaneously detect multiple category of text using parametric mixture model pmms newly presented in this paper pmms are probabilistic generative model for text that ha multiple category our pmms are essentially different from the conventional mixture of multinomial distribution in the sense that in the former several basis multinomial parameter are mixed in the parameter space while in the latter several multinomial component are mixed we derive efficient learning algorithm for pmms within the framework of the maximum a posteriori estimate we also empirically show that our method can outperform the conventional binary approach when applied to multitopic detection of world wide web page focusing on those from the yahoo com domain 
a new class of nonparametric algorithm for high dimensional binary classification is proposed using cascade of low dimensional polynomial structure construction of polynomial cascade is based on minimax probability machine classification mpmc which result in direct estimate of classification accuracy and provides a simple stopping criterion that doe not require expensive cross validation measure this polynomial mpmc cascade pmc algorithm is constructed in linear time with respect to the input space dimensionality and linear time in the number of example making it a potentially attractive alternative to algorithm like support vector machine and standard mpmc experimental evidence is given showing that compared to state of the art classifier pmcs are competitive inherently fast to compute not prone to overfitting and generally yield accurate estimate of the maximum error rate on unseen data 
we present a discriminative part based approach for the recognition of object class from unsegmented cluttered scene object are modeled a flexible constellation of part conditioned on local observation found by an interest operator for each object class the probability of a given assignment of part to local feature is modeled by a conditional random field crf we propose an extension of the crf framework that incorporates hidden variable and combine class conditional crfs into a unified framework for part based object recognition the parameter of the crf are estimated in a maximum likelihood framework and recognition proceeds by finding the most likely class under our model the main advantage of the proposed crf framework is that it allows u to relax the assumption of conditional independence of the observed data i e local feature often used in generative approach an assumption that might be too restrictive for a considerable number of object class 
abstract bayesian classi er such a naive bayes or tree augmented naive bayes tan have shown excellent performance given their sim plicity and heavy underlying independence assumption in this paper we introduce a classi er taking a basis the tan model and taking into account uncertainty in model se lection to do this we introduce decompos able distribution over tan and show that they allow the expression resulting from the bayesian model averaging of tan model to be integrated into closed form with this re sult we construct a classi er with a shorter learning time and a longer classi cation time than tan empirical result show that the classi er is most of the case more accurate than tan and approximates better the class probability 
in this article we present a novel approach to solving the localization problem in cellular network the goal is to estimate a mobile user s position based on measurement of the signal strength received from network base station our solution work by building gaussian process model for the distribution of signal strength a obtained in a series of calibration measurement in the localization stage the user s position can be estimated by maximizing the likelihood of received signal strength with respect to the position we investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network 
surveillance system have long been used to monitor industrial process and are becoming increasingly popular in public health and anti terrorism application most early detection system produce a time series of p value or some other statistic a their output typically the decision to signal an alarm is based on a threshold or other simple algorithm such a cusum that accumulates detection information temporally we formulate a pomdp model of underlying event and observation from a detector we solve the model and show how it is used for single output detector when dealing with spatio temporal data scan statistic are a popular method of building detector we describe the use of scan statistic in surveillance and how our pomdp model can be used to perform alarm signaling with them we compare the result obtained by our method with simple thresholding and cusum on synthetic and semi synthetic health data 
synapsis are a critical element of biologically realistic spike based neural computation serving the role of communication computation and modification many different circuit implementation of sy napse function exist with different computational goal in mind in th is paper we describe a new cmos synapse design that separately control quiescent leak current synaptic gain and time constant of decay this circuit implement part of a commonly used kinetic model of synaptic conductance we show a theoretical analysis and experimental data for prototype fabricated in a commercially available m cmos process 
abstract in this paper we consider formulation of multi class probl em based on a generalized notion of a margin and using output coding this includes but is not restricted to standard multi class svm formulat ion differently from many previous approach we learn the code a well a the embedding function we illustrate how this can lead to a formulation that allows for solving a wider range of problem with e g many class or even missing class to keep our optimization problem tractable we propose an algorithm capable of solving them using two class classifier similar in spirit to boosting 
abstract at a cocktail party a listener can selectively attend to a single voice and filter out other acoustical interference how to simulate this perceptual ability remains a great challenge this paper describes a novel supervised learning approach to speech segregation in which a target speech signal is separated from interfering sound using spatial location cue interaural time difference itd and interaural intensity difference iid motivated by the auditory masking effect we employ the notion of an ideal time frequency binary mask which selects the target if it is stronger than the interference in a local time frequency unit within a narrow frequency band modification to the relative strength of the target source with respect to the interference trigger systematic change for estimated itd and iid for a given spatial configuration this interaction produce characteristic clustering in the binaural feature space consequently we perform pattern classification in order to estimate ideal binary mask a systematic evaluation in term of signal to noise ratio a well a automatic speech recognition performance show that the resulting system produce mask very close to ideal binary one a quantitative comparison show that our model yield significant improvement in performance over an existing approach furthermore under certain condition the model produce large speech intelligibility improvement with normal listener 
we propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classification that combine the computational advantage of a parametric solution with the flexibility of sequential sampling technique we regard the parameter of the classifier a latent state in a first order markov process and propose an algorithm which can be regarded a variational generalization of standard kalman filtering the variational kalman filter is based on two novel lower bound that enable u to use a non degenerate distribution over the adaptation rate an extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classifier both in stationary and non stationary environment although we focus on classification the algorithm is easily extended to other generalized nonlinear model 
we study the problem of computing classification rule set from relational database so that accurate prediction can be made on test data with missing attribute value traditional classifier perform badly when test data are not a complete a the training data because they tailor a training database too much we introduce the concept of one rule set being more robust than another that is able to make more accurate prediction on test data with missing attribute value we show that the optimal class association rule set is a robust a the complete class association rule set we then introduce the k optimal rule set which provides prediction exactly the same a the optimal class association rule set on test data with up to k missing attribute value this lead to a hierarchy of k optimal rule set in which decreasing size corresponds to decreasing robustness and they all more robust than a traditional classification rule set we introduce two method to find k optimal rule set i e an optimal association rule mining approach and a heuristic approximate approach we show experimentally that a k optimal rule set generated by the optimal association rule mining approach performs better than that by the heuristic approximate approach and both rule set perform significantly better than a typical classification rule set c rule on incomplete test data 
in today s industry the design of software test is mostly based on the tester expertise while test automation tool are limited to execution of pre planned test only evaluation of test output is also associated with a considerable effort by human tester who often have imperfect knowledge of the requirement specification not surprisingly this manual approach to software testing result in heavy loss to the world s economy the cost of the so called catastrophic software failure such a mar polar lander shutdown in are even hard to measure in this paper we demonstrate the potential use of data mining algorithm for automated induction of functional requirement from execution data the induced data mining model of tested software can be utilized for recovering missing and incomplete specification designing a minimal set of regression test and evaluating the correctness of software output when testing new potentially flawed release of the system to study the feasibility of the proposed approach we have applied a novel data mining algorithm called info fuzzy network ifn to execution data of a general purpose code for solving partial differential equation after being trained on a relatively small number of randomly generated input output example the model constructed by the ifn algorithm ha shown a clear capability to discriminate between correct and faulty version of the program 
this paper describes a novel algorithm and deployed system golden path analyzer gpa that analyzes clickstreams of people trying to complete the same task on a website it find the shortest successful path taken by user golden path and us these a seed for clickstream cluster other user are assigned to a cluster if their clickstream is a supersequence of the golden path the advantage of this approach are that the resulting cluster are easily comprehended they are few in number correspond to semantically different strategy used by the user and jointly partition all the clickstreams gpa s key contribution over prior work in process funnel is that by not excluding user that make diversion from the golden path gpa is able to assign more user to fewer cluster another key contribution is to use actual full clickstreams a cluster seed to which supersequences of other user are added golden path correspond to complete clickstreams that are based on actual user page transition gpa is particularly useful for site designer to improve process such a shopping return and registration it analysis identify which web page cause many user to deviate from a golden path which link distract user and the percentage of user taking each golden path gpa ha demonstrated value on more than twenty client project in diverse industry 
a new method for classiflcation is proposed this is based on kernel orthonormalized partial least square pls dimensionality reduction of the original data space followed by a support vector classifler unlike principal component analysis pca which ha previously served a a dimension reduction step for discrimination problem orthonormalized pls is closely related to fisher s approach to linear discrimination or equivalently to canonical correlation analysis for this reason orthonormalized pls is preferable to pca for discrimination good behavior of the proposed method is demonstrated on difierent benchmark data set and on the real world problem of classifying flnger movement period from non movement period based on electroencephalogram 
abstract in the last decade there ha been an explosion of interest in mining time series data literally hundred of paper have introduced new algorithm to index clas sify cluster and segment time series in this work we make the following claim much of this work ha very little utility because the contribution made speed in the case of indexing accuracy in the case of classification and clustering model accuracy in the case of segmentation offer an amount of improvement that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets or the variance that would have been observed by changing minor unstated implementation detail to illustrate our point we have undertaken the most exhaustive 
we present an improved method for clustering in the presence of very limited supervisory information given a pairwise instance constraint by allowing instance level constraint to have spacelevel inductive implication we are able to successfully incorporate constraint for a wide range of data set type our method greatly improves on the previously studied constrained mean algorithm generally requiring le than half a many constraint to achieve a given accuracy on a range of real world data while also being more robust when over constrained we additionally discus an active learning algorithm which increase the value of constraint even further 
an object tracking algorithm using a novel simple symmetric similarity function between spatially smoothed kernel density estimate of the model and target distribution is proposed and tested the similarity measure is based on the expectation of the density estimate over the model or target image the density is estimated using radial basis kernel function that measure the affinity between point and pr ovide a better outlier rejection property the mean shift algorithm i s used to track object by iteratively maximizing this similarity functio n to alleviate the quadratic complexity of the density estimation we employ gaussian kernel and the fast gauss transform to reduce the computation to linear order this lead to a very efficient and robust nonparametri c tracking algorithm the proposed algorithm is tested with several image sequence and shown to achieve robust and reliable real time tracking several sequence are placed at http www c umd edu user yangcj node html 
a lot of learning machine with hidden variable used in information science have singularity in their parameter space at singularity the fisher information matrix becomes degenerate resulting that the learning theory of regular statistical model doe not hold recently it wa proven that if the true parameter is contained in singularity then the coecien t of the bayes generalization error is equal to the pole of the zeta function of the kullback information in this paper under the condition that the true parameter is almost but not contained in singularity we show two result if the dimension of the parameter from input to hidden unit is not larger than three then there exit a region of true parameter where the generalization error is larger than those of regular model however if otherwise then for any true parameter the generalization error is smaller than those of regular model the symmetry of the generalization error and the training error doe not hold in singular model in general 
text classification is the process of classifying document into predefined category based on their content existing supervised learning algorithm to automatically classify text need sufficient labeled document to learn accurately applying the expectation maximization em algorithm to this problem is an alternative approach that utilizes a large pool of unlabeled document to augment the available labeled document unfortunately the time needed to learn with these large unlabeled document is too high this paper introduces a novel parallel learning algorithm for text classification task the parallel algorithm is based on the combination of the em algorithm and the naive bayes classifier our goal is to improve the computational time in learning and classifying process we studied the performance of our parallel algorithm on a large linux pc cluster called pirun cluster we report both timing and accuracy result these result indicate that the proposed parallel algorithm is capable of handling large document collection 
optimal solution to markov decision problem mdps are very sensitive with respect to the state transition probability in many practical problem the estimation of those probability is far from accurate hence estimation error are limiting factor in applying mdps to realworld problem we propose an algorithm for solving finite state and finite action mdps where the solution is guaranteed to be robust with respect to estimation error on the state transition probability our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty via kullback leibler divergence bound the worst case complexity of the robust algorithm is the same a the original bellman recursion hence robustness can be added at practically no extra computing cost 
we present a method for very high dimensional correlation analysis the method relies equally on rigorous search strategy and on human interaction at each step the method conservatively shave off a fraction of the database tuples and attribute so that most of the correlation present in the data are not affected by the decomposition instead the correlation become more obvious to the user because they are hidden in a much smaller portion of the database this process can be repeated iteratively and interactively until only the most important correlation remain the main technical difficulty of the approach is figuring out how to shave off part of the database so a to preserve most correlation we develop an algorithm for this problem that ha a polynomial running time and guarantee result quality 
we study generalization property of linear learning algorithm and develop a data dependent approach that is used to derive generalization bound that depend on the margin distribution our method make use of random projection technique to allow the use of existing vc dimension bound in the eective lower dimension of the data comparison with existing generalization bound show that our bound are tighter and meaningful in case existing bound are not 
the concept of data squashing wa introduced by dumouchel et al a a method of summarizing massive data set that preserve statistical relationship among variable the idea is to create a smaller data set that allows statistical modeling to take place using in memory algorithm and to preserve the modeling result more accurately than would a same size random sample from the massive data set this research attempt to avoid several limitation of previous approach to data squashing our method avoids the curse of dimensionality by a double use of principal component transformation that make computing time linear in the number of case and quadratic in the number of variable categorical and continuous variable are smoothly integrated because the binning is based on principal component which are uncorrelated we can use fractional factorial design that sample le than one point per bin we also investigate various weighting scheme for the squashed sample to see whether matching moment or matching subregion data count is more effective finally previous work required the specification of a statistical model either to perform the squashing algorithm or to compare the worth of different squashing method our approach to evaluation is model free and doe not even require the specification of variable a response or predictor instead we develop a chi squared like measure of accuracy to compare the closeness of various discrete density the squashed data set to the discrete massive data set 
in a variety of modern mining application data are commonly viewed a infinite time ordered data stream rather a finite data set stored on disk this view challenge fundamental assumption commonly made in the context of several data mining algorithm in this paper we study the problem of identifying correlation between multiple data stream in particular we propose algorithm capable of capturing correlation between multiple continuous data stream in a highly efficient and accurate manner our algorithm and technique are applicable in the case of both synchronous and asynchronous data streaming environment we capture correlation between multiple stream using the well known technique of singular value decomposition svd correlation between data item and the svd technique in particular have been repeatedly utilized in an off line non stream data mining problem for example forecasting approximate query answering and data reduction we propose a methodology based on a combination of dimensionality reduction and sampling to make the svd technique suitable for a data stream context our technique are approximate trading accuracy with performance and we analytically quantify this tradeoff we present a through experimental evaluation using both real and synthetic data set from a prototype implementation of our technique investigating the impact of various parameter in the accuracy of the overall computation our result indicate that correlation between multiple data stream can be identified very efficiently and accurately the algorithm proposed herein are presented a generic tool with a multitude of application on data stream mining problem 
this paper present a novel graph theoretic approach named ratio contour to extract perceptually salient boundary from a set of noisy boundary fragment detected in real image the boundary saliency is defined using the gestalt law of closure proximity and continuity this paper first construct an undirected graph with two different set of edge solid edge and dashed edge the weight of solid and dashed edge measure the local saliency in and between boundary fragment respectively then the most salient boundary is detected by searching for an optimal cycle in this graph with minimum average weight the proposed approach guarantee the global optimality without introducing any bias related to region area or boundary length we collect a variety of image for testing the proposed approach with encouraging result 
one major problem of existing method to mine data stream is that it make ad hoc choice to combine most recent data with some amount of old data to search the new hypothesis the assumption is that the additional old data always help produce a more accurate hypothesis than using the most recent data only we first criticize this notion and point out that using old data blindly is not better than gambling in other word it help increase the accuracy only if we are lucky we discus and analyze the situation where old data will help and what kind of old data will help the practical problem on choosing the right example from old data is due to the formidable cost to compare different possibility and model this problem will go away if we have an algorithm that is extremely efficient to compare all sensible choice with little extra cost based on this observation we propose a simple efficient and accurate cross validation decision tree ensemble method 
we show that temporal logic and combination of temporal logic and modal logic of knowledge can be efiectively represented in artiflcial neural network we present a translation algorithm from temporal rule to neural network and show that the network compute a flxed point semantics of the rule we also apply the translation to the muddy child puzzle which ha been used a a testbed for distributed multi agent system we provide a complete solution to the puzzle with the use of simple neural network capable of reasoning about time and of knowledge acquisition through inductive learning 
a reactive environment is one that responds to the action of an agent rather than evolving obliviously in reactive environment expert algorithm must balance exploration and exploitation of expert more carefully than in oblivious one in addition a more subtle denition of a learnable value of an expert is required a general exploration exploitation expert method is presented along with a proper denition of value the method is shown to asymptotically perform a well a the best available expert several variant are analyzed from the viewpoint of the exploration exploitation tradeoff including explore then exploit polynomially vanishing exploration constant frequency exploration and constant size exploration phase complexity and performance bound are proven 
there is a notable interest in extending probabilistic generative modeling principle to accommodate for more complex structured data type in this paper we develop a generative probabilistic model for visualizing set of discrete symbolic sequence the model a constrained mixture of discrete hidden markov model is a generalization of density based visualization method previously developed for static data set we illustrate our approach on sequence representing web log data and choral by j s bach 
when the training instance of the target class are heavily outnumbered by non target training instance svms can be ineffective in determining the class boundary to remedy this problem we propose an adaptive conformal transformation act algorithm act considers feature space distance and the class imbalance ratio when it performs conformal transformation on a kernel function experimental result on uci and real world datasets show act to be effective in improving class prediction accuracy 
the barn owl is a nocturnal hunter capable of capturing prey using auditory information alone the neural basis for this localization behavior is the existence of auditory neuron with spatial receptive field we provide a mathematical description of the operation performed on auditory input signal by the barn owl that facilitate the creation of a representation of auditory space to develop our model we first formulate the sound localization problem solved by the barn owl a a statistical estimation problem the implementation of the solution is constrained by the known neurobiology 
learning from ambiguous training data is highly relevant in many application we present a new learning algorithm for classification problem where label are associated with set of pattern instead of individual pattern this encompasses multiple instance learning a a special case our approach is based on a generalization of linear programming boosting and us result from disjunctive programming to generate successively stronger linear relaxation of a discrete non convex problem 
one current explanation of the view independent representation of space by the place cell of the hippocampus is that they arise out of the summation of view dependent gaussians this proposal assumes that visual representation show bounded invariance here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representation our analysis is based on the behavior of a simulated robot in a virtual environment containing specic visual cue our result show that the temporal population code provides a representational substrate that can naturally account for the formation of place eld 
an analog system on chip for kernel based pattern classification and sequence estimation is presented state transition probability conditioned on input data are generated by an integrated support vector machine dot product based kernel and support vector coefficient are implemented in analog programmable floating gate translinear circuit and probability are propagated and normalized using sub threshold current mode circuit a input state and support vector forward decoding kernel machine is integrated on a mm mm chip in m cmos technology experiment with the processor trained for speaker verification and phoneme sequence estimation demonstrate real time recognition accuracy at par with floating point software at sub microwatt power 
the maximisation of information transmission over noisy channel is a common albeit generally computationally difficult problem we approach the difficulty of computing the mutual information for noisy channel by using a variational approximation the resulting im algorithm is analagous to the em algorithm yet maximises mutual information a opposed to likelihood we apply the method to several practical example including linear compression population encoding and cdma 
this paper proposes a novel data envelopment analysis dea based approach for model combination we first prove that for the class classification problem dea model identify the same convex hull a the popular roc analysis used for model combination for general k class classifier we then develop a dea based method to combine multiple classifier experiment show that the method outperforms other benchmark method and suggest that dea can be a promising tool for model combination 
the problem of learning a semantic representation of a text document from data is addressed in the situation where a corpus of unlabeled paired document is available each pair being formed by a short english document and it french translation this representation can then be used for any retrieval categorization or clustering task both in a standard and in a cross lingual setting by using kernel function in this case simple bag of word inner product each part of the corpus is mapped to a high dimensional space the correlation between the two space are then learnt by using kernel canonical correlation analysis a set of direction is found in the first and in the second space that are maximally correlated since we assume the two representation are completely independent apart from the semantic content any correlation between them should reflect some semantic similarity certain pattern of english word that relate to a specific meaning should correlate with certain pattern of french word corresponding to the same meaning across the corpus using the semantic representation obtained in this way we first demonstrate that the correlation detected between the two version of the corpus are significantly higher than random and hence that a representation based on such feature doe capture statistical pattern that should reflect semantic information then we use such representation both in cross language and in single language retrieval task observing performance that is consistently and significantly superior to lsi on the same data 
graph are an increasingly important data source with such important graph a the internet and the web other familiar graph include cad circuit phone record gene sequence city street social network and academic citation any kind of relationship such a actor appearing in movie can be represented a a graph this work present a data mining tool called anf that can quickly answer a number of interesting question on graph represented data such a the following how robust is the internet to failure what are the most influential database paper are there gender difference in movie appearance pattern at it core anf is based on a fast and memory efficient approach for approximating the complete neighbourhood function for a graph for the internet graph k node anf s highly accurate approximation is more than time faster than the exact computation this reduces the running time from nearly a day to a matter of a minute or two allowing user to perform ad hoc drill down task and to repeatedly answer question about changing data source to enable this drill down anf employ new technique for approximating neighbourhood type function for graph with distinguished node and or edge when compared to the best existing approximation anf s approach is both faster and more accurate given the same resource additionally unlike previous approach anf scale gracefully to handle disk resident graph finally we present some of our result from mining large graph using anf 
a behavior based architecture that enables a simulated agent to exist and navigate in an artificial environment without any kind of spatial representation is presented hebbian learning is used to combine reactive behavior that enable the agent to exploit spatial and temporal regularity in the environment the agent is then able to apply it innate behavior in situation that were not initially designed to trigger these reactive behavior the system can also accommodate change in the environment simulation result that measure the performance of the system are also presented is easier to recognize location of landmark than location of food in this case no representation of the environment need be used but based on it past experience the agent can generate a new method for finding food the architecture of the system presented in this report is connectionist and it us hebbian learning to build new combination of innate behavior thus enabling the agent to exploit regularity in it environment the connectionist nature of the system also enables the agent to adapt it internal representation to reflect change in it surroundings 
in this paper we present discriminative random field drf a discriminative framework for the classification of natural image region by incorporating neighborhood spatial dependency in the label a well a the observed data the proposed model exploit local discriminative model and allows to relax the assumption of conditional independence of the observed data given the label commonly used in the markov random field mrf framework the parameter of the drf model are learned using penalized maximum pseudo likelihood method furthermore the form of the drf model allows the map inference for binary classification problem using the graph min cut algorithm the performance of the model wa verified on the synthetic a well a the real world image the drf model outperforms the mrf model in the experiment 
label ranking is the task of inferring a total order over a predefined set of label for each given instance we present a general framework for batch learning of label ranking function from supervised data we assume that each instance in the training data is associated with a list of preference over the label set however we do not assume that this list is either complete or consistent this enables u to accommodate a variety of ranking problem in contrast to the general form of the supervision our goal is to learn a ranking function that induces a total order over the entire set of label special case of our setting are multilabel categorization and hierarchical classification we present a general boosting based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration the applicability of our approach is demonstrated with a set of experiment on a large scale text corpus 
it ha been suggested that the primary goal of the sensory system is to represent input in such a way a to reduce the high degree of redundancy given a noisy neural representation however solely reducing redundancy is not desirable since redundancy is the only clue to reduce the effect of noise here we propose a model that best balance redundancy reduction and redundant representation like previous model our model account for the localized and oriented structure of simple cell but it also predicts a different organization for the population with noisy limited capacity unit the optimal representation becomes an overcomplete multi scale representation which compared to previous model is in closer agreement with physiological data these result offer a new perspective on the expansion of the number of neuron from retina to v and provide a theoretical model of incorporating useful redundancy into efficient neural representation 
a novel algorithm for actively trading stock is presented while tradi tional universal algorithm and technical trading heuristic attempt to predict winner or trend our approach relies on predictable statistical relation between all pair of stock in the market our empirical result on historical market provide strong evidence that this type of techni cal trading can beat the market and moreover can beat the best stock in the market in doing so we utilize a new idea for smoothing critical parameter in the context of expert learning 
we consider the problem of structured classification where the task is to predict a label y from an input x and y ha meaningful internal structure our framework includes supervised training of markov random field and weighted context free grammar a special case we describe an algorithm that solves the large margin optimization problem defined in using an exponential family gibbs distribution representation of structured object the algorithm is efficient even in case where the number of label y is exponential in size provided that certain expectation under gibbs distribution can be calculated efficiently the method for structured label relies on a more general result specifically the application of exponentiated gradient update to quadratic program 
burst detection is the activity of finding abnormal aggregate in data stream such aggregate are based on sliding window over data stream in some application we want to monitor many sliding window size simultaneously and to report those window with aggregate significantly different from other period we will present a general data structure for detecting interesting aggregate over such elastic window in near linear time we present application of the algorithm for detecting gamma ray burst in large scale astrophysical data detection of period with high volume of trading activity and high stock price volatility is also demonstrated using real time trade and quote taq data from the new york stock exchange nyse our algorithm beat the direct computation approach by several order of magnitude 
one of the most well studied problem in data mining is computing the collection of frequent item set in large transactional database one obstacle for the applicability of frequent set mining is that the size of the output collection can be far too large to be carefully examined and understood by the user even restricting the output to the border of the frequent item set collection doe not help much in alleviating the problem in this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem what are the k set that best approximate a collection of frequent item set our measure of approximating a collection of set by k set is defined to be the size of the collection covered by the the k set i e the part of the collection that is included in one of the k set we also specify a bound on the number of extra set that are allowed to be covered we examine different problem variant for which we demonstrate the hardness of the corresponding problem and we provide simple polynomial time approximation algorithm we give empirical evidence showing that the approximation method work well in practice 
a mixed signal image filtering vlsi ha been developed aiming at real time generation of edge based image vector for robust image recognition a four stage asynchronous median detection architecture based on analog digital mixed signal circuit ha been introduced to determine the threshold value of edge detection the key processing parameter in vector generation a a result a fully seamless pipeline processing from threshold detection to edge feature map generation ha been established a prototype chip wa designed in a m double polysilicon three metal layer cmos technology and the concept wa verified by the fabricated chip the chip generates a dimension feature vector from a x pixel gray scale image every sec this is about time faster than the software computation making a real time image recognition system feasible 
we propose a novel projection based visualization method for high dimensional datasets by combining concept from md and the geometry of the hyperbolic space our approach hyperbolic multi dimensional scaling h md extends earlier work using hyperbolic space for visualization of tree structure data hyperbolic tree browser by borrowing concept from multi dimensional scaling we map proximity data directly into the dimensional hyperbolic space h this remove the restriction to quasihierarchical graph based data limiting previous work since a suitable distance function can convert all kind of data to proximity or distance based data this type of data can be considered the most general we used the circular poincar model of the h which allows effective human computer interaction by moving the focus via mouse the user can navigate in the data without loosing the context in h the fish eye behavior originates not simply by a non linear view transformation but rather by extraordinary non euclidean property of the h especially the exponential growth of length and area of the underlying space make the h a prime target for mapping hierarchical and now also high dimensional data we present several high dimensional mapping example including synthetic and real world data and a successful application for unstructured text by analyzing and integrating multiple film critique from news rec art movie review and the internet movie database each movie becomes placed within the h here the idea is that related film share more word in their review than unrelated their semantic proximity lead to a closer arrangement the result is a kind of high level content structured display allowing the user to explore the space of movie 
abstract the application of kernel based learning algorithm ha so far largely been confined to realvalued data and a few special data type such a string in this paper we propose a general method of constructing natural family of kernel over discrete structure based on the matrix exponentiation idea in particular we focus on generating kernel on graph for which we propose a special class of exponential kernel based on the heat equation called diffusion kernel and show that these can be regarded a the discretization of the familiar gaussian kernel of euclidean space 
the structural similarity of neural network and genetic regulatory network to digital circuit and hence to each other wa noted from the very beginning of their study in this work we propose a simple biochemical system whose architecture mimic that of genetic regulation and whose component allow for in vitro implementation of arbitrary circuit we use only two enzyme in addition to dna and rna molecule rna polymerase rnap and ribonuclease rnase we develop a rate equation for in vitro transcriptional network and derive a correspondence with general neural network rate equation a proof of principle demonstration an associative memory task and a feedforward network computation are shown by simulation a difference between the neural network and biochemical model is also highlighted global coupling of rate equation through enzyme saturation can lead to global feedback regulation thus allowing a simple network without explicit mutual inhibition to perform the winner take all computation thus the full complexity of the cell is not necessary for biochemical computation a wide range of functional behavior can be achieved with a small set of biochemical component 
class membership probability estimate are important for many application of data mining in which classification output are combined with other source of information for decision making such a example dependent misclassification cost the output of other classifier or domain knowledge previous calibration method apply only to two class problem here we show how to obtain accurate probability estimate for multiclass problem by combining calibrated binary probability estimate we also propose a new method for obtaining calibrated two class probability estimate that can be applied to any classifier that produce a ranking of example using naive bayes and support vector machine classifier we give experimental result from a variety of two class and multiclass domain including direct marketing text categorization and digit recognition 
in this paper we propose to combine an ecien t image representation based on local descriptor with a support vector machine classier in order to perform object categorization for this purpose we apply kernel dened on set of vector after testing dieren t combination of kernel local descriptor we have been able to identify a very performant one 
abstract we present a modified version of the perceptron learning algorithm pla which solves semidefinite program sdps in polynomial time the algorithm is based on the following three observation i semidefinite program are linear program with infinitely many linear constraint ii every linear program can be solved by a sequence of constraint satisfaction problem with linear constraint iii in general the perceptron learning algorithm solves a constraint satisfaction problem with linear constraint in finitely many update combining the pla with a probabilistic rescaling algorithm which on average increase the size of the feasable region result in a probabilistic algorithm for solving sdps that run in polynomial time we present preliminary result which demonstrate that the algorithm work but is not competitive with state of the art interior point method 
abstract recent algorithm for sparse coding and independent component analysis ica have demonstrated how localized feature can be learned from natural image however these approach do not take image transformation into account a a result they produce image code that are redundant because the same feature is learned at multiple location we describe an algorithm for sparse coding based on a bilinear generative model of image by explicitly modeling the interaction between image feature and their transformation the bilinear approach help reduce redundancy in the image code and provides a basis for transformationinvariant vision we present result demonstrating bilinear sparse coding of natural image we also explore an extension of the model that can capture spatial relationship between the independent feature of an object thereby providing a new framework for part based object recognition 
feature extraction is important in many application such a text and image retrieval because of high dimensionality uncorrelated linear discriminant analysis ulda wa recently proposed for feature extraction the extracted feature via ulda were shown to be statistically uncorrelated which is desirable for many application in this paper we will first propose the ulda qr algorithm to simplify the previous implementation of ulda then we propose the ulda gsvd algorithm based on a novel optimization criterion to address the singularity problem it is applicable for undersampled problem where the data dimension is much larger than the data size such a text and image retrieval the novel criterion used in ulda gsvd is the perturbed version of the one from ulda qr while surprisingly the solution to ulda gsvd is shown to be independent of the amount of perturbation applied we did extensive experiment on text and face image data to show the effectiveness of ulda gsvd and compare with other popular feature extraction algorithm 
we propose a functional mixture model for simultaneous clustering and alignment of set of curve measured on a discrete time grid the model is specifically tailored to gene expression time course data each functional cluster center is a nonlinear combination of solution of a simple linear differential equation that describes the change of individual mrna level when the synthesis and decay rate are constant the mixture of continuoustimeparametricfunctionalformsallowsoneto a accountfor the heterogeneity in the observed profile b align the profile in time by estimating real valued time shift c capture the synthesis and decay of mrna in the course of an experiment and d regularize noisy profile by enforcing smoothness in the mean curve we derive an em algorithm for estimating the parameter of the model and apply the proposed approach to the set of cycling gene in yeast the experiment show consistent improvement in predictive power and within cluster variance compared to regular gaussian mixture 
a bal anced network lead to contradictory constraint on memory model a e xemplified in previous work on accomm odation of synfire chai n here we show that these constraint can be overcome by introducing a shadow inhibitory pattern for each excitatory p attern of the mo del this is interpreted a a d oublebalance principle whereby there exists both global balance between average excitatory and inhibitory current and local balance between the current carrying coherent activity at any given time frame thi s principle can be appl ied t o network with hebbian cell assembly leading to a high capacity of the associative memory the number of possible pattern is limited by a com binatorial constraint that turn out to be p n within the specific model that we employ th is limit is reac hed by the hebbian cell assembly network to the best of our knowledge this is the first time that such high memory capacity are demonstrated in the asynchronous state of model of spiking neuron 
in this paper we analyze the most popular evaluation metric for separate and conquer rule learning algorithm our result show that all commonly used heuristic including accuracy weighted relative accuracy entropy gini index and information gain are equivalent to one of two fundamental prototype precision which try to optimize the area under the roc curve for unknown cost and a cost weighted difference between covered positive and negative example which try to find the optimal point under known or assumed cost we also show that a straightforward generalization of the m estimate trade off these two prototype 
p nbsp p div a key challenge for neural modeling is to explain how a continuous stream of multi modal input from a rapidly changing environment can be processed by stereotypical recurrent circuit of integrate and fire neuron in real time we propose a new computational model that doe not require a task dependent construction of neural circuit instead it is based on principle of high dimensional dynamical system in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry div p nbsp p 
saliency mechanism play an important role when visual recognition must be performed in cluttered scene we propose a computational definition of saliency that deviate from existing model by equating saliency to discrimination in particular the salient attribute of a given visual class are defined a the feature that enable best discrimination between that class and all other class of recognition interest it is shown that this definition lead to saliency algorithm of low complexity that are scalable to large recognition problem and is compatible with existing model of early biological vision experimental result demonstrating success in the context of challenging recognition problem are also presented 
in this paper we introduce methodology to determine the bifurcation structure of optimum for a class of similar cost function from rate distortion theory deterministic annealing information distortion and the information bottleneck method we also introduce a numerical algorithm which us the explicit form of the bifurcating branch to find optimum at a bifurcation point 
we address the problem of learning topic hierarchy from data the model selection problem in this domain is daunting which of the large collection of possible tree to use we take a bayesian approach generating an appropriate prior via a distribution on partition that we refer to a the nested chinese restaurant process this nonparametric prior allows arbitrarily large branching factor and readily accommodates growing data collection we build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent dirichlet allocation we illustrate our approach on simulated data and with an application to the modeling of nip abstract 
recent research ha demonstrated that useful pomdp solution do not require consideration of the entire belief space we extend this idea with the notion of temporal abstraction we present and explore a new reinforcement learning algorithm over grid point in belief space which us macro action and monte carlo update of the q value we apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space we can learn pomdp policy faster and we can do information gathering more efficiently 
many sequential prediction task involve locating instance of pattern in sequence generative probabilistic language model such a hidden markov model hmms have been successfully applied to many of these task a limitation of these model however is that they cannot naturally handle case in which pattern instance overlap in arbitrary way we present an alternative approach based on conditional markov network that can naturally represent arbitrarily overlapping element we show how to ecien tly train and perform inference with these model experimental result from a genomics domain show that our model are more accurate at locating instance of overlapping pattern than are baseline model based on hmms 
we examine the use of hidden markov and hidden semi markov model for automatically segmenting an electrocardiogram waveform into it constituent waveform feature an undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling we show that the state duration implicit in a standard hidden markov model are ill suited to those of real ecg feature and we investigate the use of hidden semi markov model for improved state duration modelling 
the rapid growth of the world wide web had made the problem of topic specific resource discovery an important one in recent year in this problem it is desired to find web page which satisfy a predicate specified by the user such a predicate could be a keyword query a topical query or some arbitrary contraint several technique such a focussed crawling and intelligent crawling have recently been proposed for topic specific resource discovery all these crawler are linkage based since they use the hyperlink behavior in order to perform resource discovery recent study have shown that the topical correlation in hyperlink are quite noisy and may not always show the consistency necessary for a reliable resource discovery process in this paper we will approach the problem of resource discovery from an entirely different perspective we will mine the significant browsing pattern of world wide web user in order to model the likelihood of web page belonging to a specified predicate this user behavior can be mined from the freely available trace of large public domain proxy on the world wide web we refer to this technique a collaborative crawling because it mine the collective user experience in order to find topical resource such a strategy is extremely effective because the topical consistency in world wide web browsing pattern turn out to very reliable in addition the user centered crawling system can be combined with linkage based system to create an overall system which work more effectively than a system based purely on either user behavior or hyperlink 
spike timing plasticity stdp is a special form of synaptic plasticity where the relative timing of postand presynaptic activity determines the change of the synaptic weight on the postsynaptic side active backpropagating spike in dendrite seem to play a crucial role in the induction of spike timing dependent plasticity we argue that postsynaptically the temporal change of the membrane potential determines the weight change coming from the presynaptic side induction of stdp is closely related to the activation of nmda channel therefore we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the nmda channel thus for this calculation we utilise biophysical variable of the physiological cell the final result show a weight change curve which conforms with measurement from biology the positive part of the weight change curve is determined by the nmda activation the negative part of the weight change curve is determined by the membrane potential change therefore the weight change curve should change it shape depending on the distance from the soma of the postsynaptic cell we find temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite 
if the cortex us spike timing to compute the timing of the spike must be robust to perturbation based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial condition and numerical simulation of a variety of network architecture we argue within the limit set by our model of the neuron that it is unlikely that precise sequence of spike timing are used for computation under condition typically found in the cortex 
this paper present an efficient framework for error bounded compression of high dimensional discrete attributed datasets such datasets which frequently arise in a wide variety of application pose some of the most significant challenge in data analysis subsampling and compression are two key technology for analyzing these datasets proximus provides a technique for reducing large datasets into a much smaller set of representative pattern on which traditional expensive analysis algorithm can be applied with minimal loss of accuracy we show desirable property of proximus in term of runtime scalability to large datasets and performance in term of capability to represent data in a compact form we also demonstrate application of proximus in association rule mining in doing so we establish proximus a a tool for preprocessing data before applying computationally expensive algorithm or a a tool for directly extracting correlated pattern our experimental result show that use of the compressed data for association rule mining provides excellent precision and recall value near across a range of support threshold while reducing the time required for association rule mining drastically 
we introduce a computationally efficient method to estimate the validity of the bp method a a function of graph topology the connectivity strength frustration and network size we present numerical result that demonstrate the correctness of our estimate for the uniform random model and for a real world network c elegans although the method is restricted to pair wise interaction no local evidence zero bias and binary variable we believe that it prediction correctly capture the limitation of bp for inference and map estimation on arbitrary graphical model using this approach we find that bp always performs better than mf especially for large network with broad degree distribution such a scale free network bp turn out to significantly outperform mf 
we propose a convex optimization based strategy to deal with uncertainty in the observation of a classification problem we assume that instead of a sample xi yi a distribution over xi yi is specified in particular we derive a robust formulation when the distribution is given by a normal distribution it lead to second order cone programming formulation our method is applied to the problem of missing data where it outperforms direct imputation 
we use unsupervised probabilistic machine learning idea to try to explain the kind of learning observed in real neuron the goal being to connect abstract principle of self organisation to known biophysical process for example we would like to explain spike timingdependent plasticity see and figure a in term of information theory starting out we explore the optimisation of a network sensitivity measure related to maximising the mutual information between input spike timing and output spike timing our derivation are analogous to those in ica except that the sensitivity of output timing to input timing is maximised rather than the sensitivity of output firing rate to input ica and related approach have been successful in explaining the learning of many property of early visual receptive field in rate coding model and we are hoping for similar gain in understanding of spike coding in network and how this is supported in principled probabilistic way by cellular biophysical process for now in our initial simulation we show that our derived rule can learn synaptic weight which can unmix or demultiplex mixed spike train that is it can recover independent point process embedded in distributed correlated input spike train using an adaptive single layer feedforward spiking network 
abstract the basic tool of machine learning appear inthe inner loop of most reinforcement learning algorithm typically in the form of monte carlomethods or function approximation technique 
abstract using an wireless client a a location sensor is an increasingly popular way of enabling location based service triangulation on signal strength from multiple access point can be used to pinpoint location down to a few meter however this level of accuracy come at the price of a manual tedious spatially highdensity calibration of signal strength a a function of location this paper present a new location algorithm based on a relatively coarse calibration this help answer the question of how accurate location can 
existing patient record are a valuable resource for automated outcome analysis and knowledge discovery however key clinical data in these record is typically recorded in unstructured form a free text and image and most structured clinical information is poorly organized time consuming interpretation and analysis is required to convert these record into structured clinical data thus only a tiny fraction of this resource is utilized we present remind a bayesian framework for reliable extraction and meaningful inference from nonstructured data remind integrates and blend the structured and unstructured clinical data in patient record to automatically created high quality structured clinical data this structuring allows existing patient record to be mined for quality assurance regulatory compliance and to relate financial and clinical factor we demonstrate remind on two medical application a extract recurrence the key outcome for measuring treatment effectiveness for colon cancer patient ii extract key diagnosis and complication for acute myocardial infarction heart attack patient and demonstrate the impact of these clinical factor on financial outcome 
prototype based algorithm are commonly used to reduce the computational complexity of nearest neighbour nn classifier in this paper we discus theoretical and algorithmical aspect of such algorithm on the theory side we present margin based generalization bound that suggest that these kind of classifier can be more accurate then the nn rule furthermore we derived a training algorithm that selects a good set of prototype using large margin principle we also show that the year old learning vector quantization lvq algorithm emerges naturally from our framework 
multiple realization of continuous valued time series fr om a stochastic process often contain systematic variation in rate and amp litude to leverage the information contained in such noisy replicate set we need to align them in an appropriate way for example to allow the data to be properly combined by adaptive averaging we present the continuous profile model cpm a generative model in which each observe d time series is a non uniformly subsampled version of a single lat ent trace to which local rescaling and additive noise are applied after unsupervised training the learned trace represents a canonical high re solution fusion of all the replicates a well an alignment in time and scale of each observation to this trace can be found by inference in the model we apply cpm to successfully align speech signal from multiple speaker and set of liquid chromatography mass spectrometry proteomic data 
motivated by the particular problem involved in communicating with locked in paralysed patient we aim to develop a braincomputer interface that us auditory stimulus we describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequence using support vector machine classification and recursive channel elimination on the independent component of averaged eventrelated potential we show that an untrained user s eeg data can be classified with an encouragingly high level of accuracy this suggests that it is possible for user to modulate eeg signal in a single trial by the conscious direction of attention well enough to be useful in bci 
an optoelectronic implementation of a spiking neuron model based on the fitzhugh nagumo equation is presented a tunable semiconductor laser source and a spectral filter provide a nonlinear mapping from driver voltage to detected signal linear electronic feedback completes the implementation which allows either electronic or optical input signal experimental result for a single system and numeric result of model interaction confirm that important feature of spiking neural model can be implemented through this approach 
we consider the problem of multi step ahead prediction in time series analysis using the non parametric gaussian process model step ahead forecasting of a discrete time non linear dynamic system can be performed by doing repeated one step ahead prediction for a state space model of the form the prediction of at time is based on the estimate of the previous output in this paper we show how using an analytical gaussian approximation we can formally incorporate the uncertainty about future regressor value thus updating the uncertainty on the current pre diction 
in this paper we propose an efficient algorithm for reducing a large mixture of gaussians into a smaller mixture while still preserving the component structure of the original model this is achieved by clustering grouping the component the method minimizes a new easily computed distance measure between two gaussian mixture that can be motivated from a suitable stochastic model and the iteration of the algorithm use only the model parameter avoiding the need for explicit resampling of datapoints we demonstrate the method by performing hierarchical clustering of scenery image and handwritten digit 
the problem of measuring similarity of object arises in many application and many domain specific measure have been developed e g matching text across document or computing overlap among item set we propose a complementary approach applicable in any domain with object to object relationship that measure similarity of the structural context in which object occur based on their relationship with other object effectively we compute a measure that say two object are similar if they are related to similar object this general similarity measure called simrank is based on a simple and intuitive graph theoretic model for a given domain simrank can be combined with other domain specific similarity measure we suggest technique for efficient computation of simrank score and provide experimental result on two application domain showing the computational feasibility and effectiveness of our approach 
relational reinforcement learning rrl is a q learning technique which us first order regression technique to generalize the qfunction both the relational setting and the q learning context introduce a number of difficulty which must be dealt with in this paper we investigate a few dierent method that do incremental relational instance based regression and can be used for rrl this lead u to dierent approach which limit both memory consumption and processing time we implemented a number of these approach and experimentally evaluated and compared them to each other and an existing rrl algorithm these experiment show relational instance based regression to work well and to add robustness to rrl 
area of the brain involved in various form of memory exhibit pattern of neural activity quite unlike those in canonical computational model we show how to use well founded bayesian probabilistic autoassociative recall to derive biologically reasonable neuronal dynamic in recurrently coupled model together with appropriate value for parameter such a the membrane time constant and inhibition we explicitly treat two case one arises from a standard hebbian learning rule and involves activity pattern that are coded by graded firing rate the other arises from a spike timing dependent learning rule and involves pattern coded by the phase of spike time relative to a coherent local field potential oscillation our model offer a new and more complete understanding of how neural dynamic may support autoassociation 
human are able to detect blurring of visual image but the mechanism by which they do so is not clear a traditional view is that a blurred image look unnatural because of the reduction in energy either glob ally or locally at high frequency in this paper we propose that the disruption of local phase can provide an alternative explanation for blur perception we show that precisely localized feature such a step edge result in strong local phase coherence structure across scale and space in the complex wavelet transform domain and blurring cause loss of such phase coherence we propose a technique for coarse to fine phase pre diction of wavelet coefficient and observe that such prediction are highly effective in natural image phase coherence increase with the strength of image feature and blurring disrupts the phase coherence relationship in image we thus lay the groundwork for a new theory of perceptual blur estimation a well a a variety of algorithm for restora tion and manipulation of photographic image 
policy gradient method have received increased attention recently a a mechanism for learning to act in partially observable environment they have shown promise for problem admitting memoryless policy but have been le successful when memory is required in this paper we develop several improved algorithm for learning policy with memory in an innite horizon setting directly when a known model of the environment is available and via simulation otherwise we compare these algorithm on some large pomdps including noisy robot navigation and multi agent problem 
policy gradient method based on reinforce are model free in the sense that they estimate the gradient using only online experience executing the current stochastic policy this is extremely wasteful of training data a well a being computationally inefficient this paper present a new modelbased policy gradient algorithm that us training experience much more efficiently our approach construct a series of incomplete model of the mdp and then applies these model to compute the policy gradient in closed form the paper describes an algorithm that alternate between pruning to remove irrelevant part of the incomplete mdp model exploration to gather training data in the relevant part of the state space and gradient ascent search we show experimental result on several benchmark problem including resource constrained scheduling the overall feasibility of this approach depends on whether a sufficiently informative partial model can fit into available memory 
we describe a markov chain method for sampling from the distribution of the hidden state sequence in a non linear dynamical system given a sequence of observation this method update all state in the sequence simultaneously using an embedded hidden markov model hmm an update begin with the creation of pool of candidate state at each time we then define an embedded hmm whose state are index within these pool using a forward backward dynamic programming algorithm we can efficiently choose a state sequence with the appropriate probability from the exponentially large number of state sequence that pas through state in these pool we illustrate the method in a simple one dimensional example and in an example showing how an embedded hmm can be used to in effect discretize the state space without any discretization error we also compare the embedded hmm to a particle smoother on a more substantial problem of inferring human motion from d trace of marker 
here we derive optimal gain function for minimum mean square reconstruction from neural rate response subjected to poisson noise the shape of these function strongly depends on the length of the time window within which spike are counted in order to estimate the underlying firing rate a phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero for a particular function class we were able to prove the existence of a second order phase transition analytically the critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical result we conclude that under most circumstance relevant to information processing in the brain rate coding can be better ascribed to a binary low entropy code than to the other extreme of rich analog coding 
we present an unsupervised algorithm for registering d surface scan of an object undergoing signicant deformation our algorithm doe not use marker nor doe it assume prior knowledge about object shape the dynamic of it deformation or scan alignment the algorithm register two mesh by optimizing a joint probabilistic model over all point topoint correspondence between them this model enforces preservation of local mesh geometry a well a more global constraint that capture the preservation of geodesic distance between corresponding point pair the algorithm applies even when one of the mesh is an incomplete range scan thus it can be used to automatically ll in the remaining surface for this partial scan even if those surface were previously only seen in a different conguration we evaluate the algorithm on several real world datasets where we demonstrate good result in the presence of signicant movement of articulated part and non rigid surface deformation finally we show that the output of the algorithm can be used for compelling computer graphic task such a interpolation between two scan of a non rigid object and automatic recovery of articulated object model 
this paper present an application of boosting for classifying labeled graph general structure for modeling a number of real world data such a chemical compound natural language text and bio sequence the proposal consists of i decision stump that use subgraph a feature and ii a boosting algorithm in which subgraph based decision stump are used a weak learner we also discus the relation between our algorithm and svms with convolution kernel two experiment using natural language data and chemical compound show that our method achieves comparable or even better performance than svms with convolution kernel a well a improves the testing efficiency 
we establish a mistake bound for an ensemble method for classification based on maximizing the entropy of voting weight subject to margin constraint the bound is the same a a general bound proved for the weighted majority algorithm and similar to bound for other variant of winnow we prove a more refined bound that lead to a nearly optimal algorithm for learning disjunction again based on the maximum entropy principle we describe a simplification of the on line maximum entropy method in which after each iteration the margin constraint are replaced with a single linear inequality the simplified algorithm which take a similar form to winnow achieves the same mistake bound 
web page classification is one of the essential technique for web mining specifically classifying web page of a user interesting class is the first step of mining interesting information from the web however constructing a classifier for an interesting class requires laborious pre processing such a collecting positive and negative training example for instance in order to construct a homepage classifier one need to collect a sample of homepage positive example and a sample of non homepage negative example in particular collecting negative training example requires arduous work and special caution to avoid biasing them we introduce in this paper the positive example based learning pebl framework for web page classification which eliminates the need for manually collecting negative training example in pre processing we present an algorithm called mapping convergence m c that achieves classification accuracy with positive and unlabeled data a high a that of traditional svm with positive and negative data our experiment show that when the m c algorithm us the same amount of positive example a that of traditional svm the m c algorithm performs a well a traditional svm 
locally weighted projection regression is a new algorithm that achieves nonlinear function approximation in high dimensional space with redundant and irrelevant input dimension at it core it us locally linear model spanned by a small number of univariate regression in selected direction in input space this paper evaluates different method of projection regression and derives a nonlinear function approximator based on them this nonparametric local learning system i learns rapidly with second order learning method based on incremental training ii us statistically sound stochastic cross validation to learn iii adjusts it weighting kernel based on local information only iv ha a computational complexity that is linear in the number of input and v can deal with a large number of possibly redundant input a shown in evaluation with up to dimensional data set to our knowledge this is the first truly incremental spatially localized learning method to combine all these property 
we consider data which are image containing view of multiple object our task is to learn about each of the object present in the image this task can be approached a a factorial learning problem where each image must be explained by instantiating a model for each of the object present with the correct instantiation parameter a major problem with learning a factorial model is that a the number of object increase there is a combinatorial explosion of the number of configuration that need to be considered we develop a method to extract object model sequentially from the data by making use of a robust statistical method thus avoiding the combinatorial explosion and present result showing successful extraction of object from real image 
in labelling or prediction task a trained model s test performance is often based on the quality of it single time marginal distribution over label rather than it joint distribution over label sequence we propose using a new cost function for discriminative learning that more accurately reflects such test time condition we present an efficient method to compute the gradient of this cost for maximum entropy markov model conditional random field and for an extension of these model 
several important time series data mining problem reduce to the core task of finding approximately repeated subsequence in a longer time series in an earlier work we formalized the idea of approximately repeated subsequence by introducing the notion of time series motif two limitation of this work were the poor scalability of the motif discovery algorithm and the inability to discover motif in the presence of noise here we address these limitation by introducing a novel algorithm inspired by recent advance in the problem of pattern discovery in biosequences our algorithm is probabilistic in nature but a we show empirically and theoretically it can find time series motif with very high probability even in the presence of noise or don t care symbol not only is the algorithm fast but it is an anytime algorithm producing likely candidate motif almost immediately and gradually improving the quality of result over time 
existing data mining algorithm on graph look for node satisfying specific property such a specific notion of structural similarity or specific measure of link based importance while such analysis for predetermined property can be effective in well understood domain sometimes identifying an appropriate property for analysis can be a challenge and focusing on a single property may neglect other important aspect of the data in this paper we develop a foundation for mining the property themselves we present a theoretical framework defining the space of graph property a variety of mining query enabled by the framework technique to handle the enormous size of the query space and an experimental system called f miner that demonstrates the utility and feasibility of property mining 
the temporal coding hypothesis of miller and colleague suggests that animal integrate related temporal pattern of stimulus into single memory representation we formalize this concept using quasi bayes estimation to update the parameter of a constrained hidden markov model this approach allows u to account for some surprising temporal eects in the second order conditioning experiment of miller et al which other model are unable to explain 
a recommender system suggests the item expected to be preferred by the user recommender system use collaborative filtering to recommend item by summarizing the preference of people who have tendency similar to the user preference traditionally the degree of preference is represented by a scale for example one that range from one to five this type of measuring technique is called the semantic differential sd method web adopted the ranking method however rather than the sd method since the sd method is intrinsically not suited for representing individual preference in the ranking method the preference are represented by order which are sorted item sequence according to the user preference we here propose some method to recommed item based on these order response and carry out the comparison experiment of these method 
conditional exponential model ha been one of the widely used conditional model in machine learning field and improved iterative scaling ii ha been one of the major algorithm for finding the optimal parameter for the conditional exponential model in this paper we proposed a faster iterative algorithm named fis that is able to find the optimal parameter faster than the ii algorithm the theoretical analysis show that the proposed algorithm yield a tighter bound than the traditional ii algorithm empirical study on the text classification over three different datasets showed that the new iterative scaling algorithm converges substantially faster than both the ii algorithm and the conjugate gradient algorithm cg furthermore we examine the quality of the optimal parameter found by each learning algorithm in the case of incomplete training experiment have shown that when only a limited amount of computation is allowed e g no convergence is achieved the new algorithm fis is able to obtain lower testing error than both the ii method and the cg method 
in this work we study an information filtering model where the relevance label associated to a sequence of feature vector are realization of an unknown probabilistic linear function building on the analysis of a restricted version of our model we derive a general filtering rule based on the margin of a ridge regression estimator while our rule may observe the label of a vector only by classfying the vector a relevant experiment on a real world document filtering problem show that the performance of our rule is close to that of the on line classifier which is allowed to observe all label these empirical result are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that it expected number of mistake is never much larger than that of the optimal filtering rule which know the hidden linear model 
mining maximal frequent itemsets is one of the most fundamental problem in data mining in this paper we study the complexity theoretic aspect of maximal frequent itemset mining from the perspective of counting the number of solution we present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transaction given an arbitrary support threshold is p complete thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is np hard this result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in p we also extend our complexity analysis to other similar data mining problem dealing with complex data structure such a sequence tree and graph which have attracted intensive research interest in recent year normally in these problem a partial order among frequent pattern can be defined in such a way a to preserve the downward closure property with maximal frequent pattern being those without any successor with respect to this partial order we investigate several variant of these mining problem in which the pattern of interest are subsequence subtrees or subgraphs and show that the associated problem of counting the number of maximal frequent pattern are all either p complete or p hard 
given an n x n grid of square where each square ha a count cij and an underlying population pij our goal is to find the rectangular region with the highest density and to calculate it significance by randomization an arbitrary density function d dependent on a region s total count c and total population p can be used for example if each count represents the number of disease case occurring in that square we can use kulldorff s spatial scan statistic dk to find the most significant spatial disease cluster a naive approach to finding the maximum density region requires o n time and is generally computationally infeasible we present a multiresolution algorithm which partition the grid into overlapping region using a novel overlap kd tree data structure bound the maximum score of subregions contained in each region and prune region which cannot contain the maximum density region for sufficiently dense region this method find the maximum density region in o n log n time in practice resulting in significant x speedup on both real and simulated datasets 
we present the bump mixture model a statistical model for analog data where the probabilistic semantics inference and learning rule derive from low level transistor behavior the bump mixture model relies on translinear circuit to perform probabilistic inference and floating gate device to perform adaptation this system is low power asynchronous and fully parallel and support various on chip learning algorithm in addition the mixture model can perform several task such a probability estimation vector quantization classification and clustering we tested a fabricated system on clustering quantization and classification of handwritten digit and show performance comparable to the e m algorithm on mixture of gaussians 
first order markov model have been successfully applied to many problem for example in modeling sequential data using markov chain and modeling control problem using the markov decision process mdp formalism if a first order markov model s parameter are e timated from data the standard maximum likelihood estimator considers only the first order single step transition but for many pro blems the firstorder conditional independence assumption are not satisfi ed and a a result the higher order transition probability may be poorl y approximated motivated by the problem of learning an mdp s parameter for control we propose an algorithm for learning a first order markov mod el that explicitly take into account higher order interaction duri ng training our algorithm us an optimization criterion different from maximum likelihood and allows u to learn model that capture longer range effect but without giving up the benefit of using first order markov mod el our experimental result also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problem where the mdp s parameter are estimated from data 
planning algorithm designed for deterministic world such a a search usually run much faster than algorithm designed for world with uncertain action outcome such a value iteration real world planning problem often exhibit uncertainty which force u to use the slower algorithm to solve them many real world planning problem exhibit sparse uncertainty there are long sequence of deterministic action which accomplish task like moving sensor platform into place interspersed with a small number of sensing action which have uncertain outcome in this paper we describe a new planning algorithm called mcp short for mdp compression planning which combine a search with value iteration for solving stochastic shortest path problem in mdps with sparse stochasticity we present experiment which show that mcp can run substantially faster than competing planner in domain with sparse uncertainty these experiment are based on a simulation of a ground robot cooperating with a helicopter to fill in a partial map and move to a goal location in deterministic planning problem optimal path are acyclic no state is visited more than once because of this property algorithm like a search can guarantee that they visit each state in the state space no more than once by visiting the state in an appropriate order it is possible to ensure that we know the exact value of all of a state s possible successor before we visit that state so the first time we visit a state we can compute it correct value by contrast if action have uncertain outcome optimal path may contain cycle some state will be visited two or more time with positive probability because of these cycle there is no way to order state so that we determine the value of a state s successor before we visit the state itself instead the only way to compute state value is to solve a set of simultaneous equation in problem with sparse stochasticity only a small fraction of all state have uncertain outcome it is these few state that cause all of the cycle while a deterministic state s may participate in a cycle the only way it can do so is if one of it successor ha an action with a stochastic outcome and only if this stochastic action can lead to a predecessor of s in such problem we would like to build a smaller mdp which contains only state which are related to stochastic action we will call such an mdp a compressed mdp and we will call it state distinguished state we could then run fast algorithm like a search to plan path between distinguished state and reserve slower algorithm like value iteration for deciding how to deal with stochastic outcome 
we show a close relationship between the expectation maximization em algorithm and direct optimization algorithm such a gradientbased method for parameter learning we identify analytic condition under which em exhibit newton like behavior and condition under which it posse poor first order convergence based on this analysis we propose two novel algorithm for maximum likelihood estimation of latent variable model and report empirical result showing that a predicted by theory the proposed new algorithm can substantially outperform standard em in term of speed of convergence in certain case 
previous discretization technique have discretized numeric attribute into disjoint interval we argue that this is neither necessary nor appropriate for naive bayes classifier the analysis lead to a new discretization method non disjoint discretization ndd ndd form overlapping interval for a numeric attribute always locating a value toward the middle of an interval to obtain more reliable probability estimation it also adjusts the number and size of discretized interval to the number of training instance seeking an appropriate trade o between bias and variance of probability estimation we justify ndd in theory and test it on a wide cross section of datasets our experimental result suggest that for naivebayes classifier ndd work better than alternative discretization approach 
approximation structure play an important role in inference on loopy graph a a tractable structure tree approximation have been utilized in the variational method of ghahramani jordan and the sequential projection method of frey et al however belief propagation represents each factor of the graph with a product of single node message in this paper belief propagation is extended to r epresent factor with tree approximation by way of the expectation propagation framework that is each factor sends a message to all pair of node in a tree structure the result is more accurate inference a nd more frequent convergence than ordinary belief propagation at a lower cost than variational tree or double loop algorithm 
the problem of learning with positive and unlabeled example arises frequently in retrieval application we transform the problem into a problem of learning with noise by labeling all unlabeled example a negative and use a linear function to learn from the noisy example to learn a linear function with noise we perform logistic regression after weighting the example to handle noise rate of greater than a half with appropriate regularization the cost function of the logistic regression problem is convex allowing the problem to be solved efficiently we also propose a performance measure that can be estimated from positive and unlabeled example for evaluating retrieval performance the measure which is proportional to the product of precision and recall can be used with a validation set to select regularization parameter for logistic regression experiment on a text classification corpus show that the method proposed are effective 
we explore approximate policy iteration api replacing the usual costfunction learning step with a learning step in policy space we give policy language bias that enable solution of very large relational markov decision process mdps that no previous technique can solve in particular we induce high quality domain specific plan ners for classical planning domain both deterministic and stochastic variant by solving such domain a extremely large mdps 
in many application that track and analyze spatiotemporal data movement obey periodic pattern the object follow the same route approximately over regular time interval for example people wake up at the same time and follow more or le the same route to their work everyday the discovery of hidden periodic pattern in spatiotemporal data apart from unveiling important information to the data analyst can facilitate data management substantially based on this observation we propose a framework that analyzes manages and query object movement that follow such pattern we define the spatiotemporal periodic pattern mining problem and propose an effective and fast mining algorithm for retrieving maximal periodic pattern we also devise a novel specialized index structure that can benefit from the discovered pattern to support more efficient execution of spatiotemporal query we evaluate our method experimentally using datasets with object trajectory that exhibit periodicity 
reluctance of data owner to share their possibly confidential or proprietary data with others who own related database is a serious impediment to conducting a mutually beneficial data mining analysis we address the case of vertically partitioned data multiple data owner agency each posse a few attribute of every data record we focus on the case of the agency wanting to conduct a linear regression analysis with complete record without disclosing value of their own attribute this paper describes an algorithm that enables such agency to compute the exact regression coefficient of the global regression equation and also perform some basic goodness of fit diagnostics while protecting the confidentiality of their data in more general setting beyond the privacy scenario this algorithm can also be viewed a method for the distributed computation for regression analysis 
we present a novel approach to embedding data represented by a network into a lowdimensional euclidean space unlike existing method the proposed method attempt to minimize an energy function based on the cross entropy between desirable and embedded node congurations without directly utilizing pairwise distance between node we also propose a natural criterion to effectively evaluate an embedded network layout in term of how well node connectivity are preserved experimental result show that the proposed method provides better layout than those produced by some of the well known embedding method in term of the proposed criterion we believe that our method produce a natural embedding of a large scale network suitable for analyzing by manual browsing in a twoor threedimensional euclidean space 
the successful implementation of machine learning in autonomous rover traverse science requires addressing challenge that range from the analytical technical realm to the fuzzy philosophical domain of entrenched belief system within scientist and mission manager these challenge are many they include helping scientist understand the benefit and risk of using machine leaming onboard and guiding them to distill and translate science goal into clear task that can be accomplished by algorithm the technical challenge include among other thing developing robust verification and validation plan the ultimate test of onboard machine learning acceptance is if it fly and is executed onboard and delivers successful result during the mission this last hurdle can be overcome by developing an implementation plan that pose an acceptable risk v reward scenario for mission manager we are working on rover traverse science application that address each of these issue in this paper we will describe what we are doing and how we approach these challenge 
survey propagation is a powerful technique from statistical physic that ha been applied to solve the sat problem both in principle and in practice we give using only probability argument a common derivation of survey propagation belief propagation and several interesting hybrid method we then present numerical experiment which use wsat a widely used random walk based sat solver to quantify the complexity of the sat formula a a function of their parameter both a randomly generated and after simpli cation guided by survey propagation some property of wsat which have not previously been reported make it an ideal tool for this purpose it mean cost is proportional to the number of variable in the formula at a xed ratio of clause to variable in the easy sat regime and slightly beyond and it behavior in the hardsat regime appears to reect the underlying structure of the solution space that ha been predicted by replica symmetry breaking argument an analysis of the tradeoff between the various method of search for satisfying assignment show wsat to be far more powerful than ha been appreciated and suggests some interesting new direction for practical algorithm development 
asymmetric lateral connection are one possible mechanism that can account for the direction selectivity of cortical neuron we present a mathematical analysis for a class of these model contrasting with earlier theoretical work that ha relied on method from linear system theory we study the network s nonlinear dynamic property that arise when the threshold nonlinearity of the neuron is taken into account we show that such network have stimulus locked traveling pulse solution that are appropriate for modeling the response of direction selective cortical neuron in addition our analysis show that outside a certain regime of stimulus speed the stability of this solution break down giving rise to another class of solution that are characterized by specific spatiotemporal periodicity this predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connection lurching activity wave might be observable in ensemble of direction selective cortical neuron within appropriate regime of the stimulus speed 
new feature selection algorithm for linear threshold function are described which combine backward elimination with an adaptive regularization method this make them particularly suitable to the classification of microarray expression data where the goal is to obtain accurate rule depending on few gene only our algorithm are fast and easy to implement since they center on an incremental large margin algorithm which allows u to avoid linear quadratic or higher order programming method we report on preliminary experiment with five known dna microarray datasets these experiment suggest that multiplicative large margin algorithm tend to outperform additive algorithm such a svm on feature selection task 
a large amount of information on the web is contained in regularly structured object which we call data record such data record are important because they often present the essential information of their host page e g list of product or service it is useful to mine such data record in order to extract information from them to provide value added service existing automatic technique are not satisfactory because of their poor accuracy in this paper we propose a more effective technique to perform the task the technique is based on two observation about data record on the web and a string matching algorithm the proposed technique is able to mine both contiguous and non contiguous data record our experimental result show that the proposed technique outperforms existing technique substantially 
we compute approximate analytical bootstrap average for support vector classification using a combination of the replica method of statistical physic and the tap approach for approximate inference we test our method on a few datasets and compare it with exact average obtained by extensive monte carlo sampling 
selective sampling a part of the active learning method reduces the cost of labeling supplementary training data by asking for the label only of the most informative unlabeled example this additional information added to an initial randomly chosen training set is expected to improve the generalization performance of a learning machine we investigate some method for a selection of the most informative example in the context of one class classification problem occ i e problem where only or nearly only the example of the so called target class are available we applied selective sampling algorithm to a variety of domain including realworld problem mine detection and texture segmentation the goal of this paper is to show why the best or most often used selective sampling method for twoor multi class problem are not necessarily the best one for the one class classification problem by modifying the sampling method we present a way of selecting a small subset from the unlabeled data to be presented to an expert for labeling such that the performance of the retrained one class classifier is significantly improved 
we present a novel connectionist model for acquiring the semantics of language through the behavioral experience of a real robot we focus on the compositionality of semantics which is a fundamental characteristic of human language namely the fact that we can understand the meaning of a sentence a a combination of the meaning of word the essential claim is that a compositional semantic representation can be self organized by generalizing correspondence between sentence and behavioral pattern this claim is examined and confirmed through simple experiment in which a robot generates corresponding behavior from unlearned sentence by analogy with the correspondence between learned sentence and behavior 
relational markov model rmms are a generalization of markov model where state can be of different type with each type described by a different set of variable the domain of each variable can be hierarchically structured and shrinkage is carried out over the cross product of these hierarchy rmms make effective learning possible in domain with very large and heterogeneous state space given only sparse data we apply them to modeling the behavior of web site user improving prediction in our proteus architecture for personalizing web site we present experiment on an e commerce and an academic web site showing that rmms are substantially more accurate than alternative method and make good prediction even when applied to previously unvisited part of the site 
in this paper we address the issue of using local embeddings for data visualization in two and three dimension and for classification we advocate their use on the basis that they provide an efficient mapping procedure from the original dimension of the data to a lower intrinsic dimension we depict how they can accurately capture the user s perception of similarity in high dimensional data for visualization purpose moreover we exploit the low dimensional mapping provided by these embeddings to develop new classification technique and we show experimentally that the classification accuracy is comparable albeit using fewer dimension to a number of other classification procedure 
an interesting alternative to domain independent planning is to provide example plan to demonstrate how to solve problem in a particular domain and to use that information to learn domainspecific planner others have used example plan for case based planning but the retrieval and adaptation mechanism for the inevitably large case library raise efficiency issue of concern in this paper we introduce dsplanners or automatically generated domain specific planner we present the distill algorithm for learning dsplanners automatically from example plan distill convert a plan into a dsplanner and then merges it with previously learned dsplanners our result show that the dsplanners automatically learned by distill compactly represent it domain specific planning experience furthermore the dsplanners situationally generalize the given example plan thus allowing them to efficiently solve problem that have not previously been encountered finally we present the distill procedure to automatically acquire one step loop from example plan which permit experience acquired from small problem to be applied to solving arbitrarily large one 
we establish a new hardness result that show that the difficulty of planning in factored markov decision process is representational rather than just computational more precisely we give a fixed family of factored mdps with linear reward whose optimal policy and value function simply cannot be represented succinctly in any standard parametric form previous hardness result indicated that computing good policy from the mdp parameter wa difficult but left open the possibility of succinct function approximation for any fixed factored mdp our result applies even to policy which yield a polynomially poor approximation to the optimal value and highlight interesting connection with the complexity class of arthur merlin game 
x ray crystallography is currently the most common way protein structure are elucidated one of the most time consuming step in the crystallographic process is interpretation of the electron density map a task that involves finding pattern in a three dimensional picture of a protein this paper describes deft deformable template an algorithm using pictorial structure to build a flexible protein model from the protein s amino acid sequence matching this pictorial structure into the density map is a way of automating density map interpretation also described are several extension to the pictorial structure matching algorithm necessary for this automated interpretation deft is tested on a set of density map ranging from to resolution producing rootmean squared error ranging from to 
abstract we present a novel flexible statistical approach for modelling music and text jointly the approach is based on multi modal mixture model and maximum a posteriori estimation the learned model can be used to browse database with document containing music and text to search for music using query consisting of music and text lyric and other contextual information to annotate text document with music and to automatically recommend or identify similar song 
distance function computation is a key subtask in many data mining algorithm and application the most effective form of the distance function can only be expressed in the context of a particular data domain it is also often a challenging and non trivial task to find the most effective form of the distance function for example in the text domain distance function design ha been considered such an important and complex issue that it ha been the focus of intensive research over three decade the final design of distance function in this domain ha been reached only by detailed empirical testing and consensus over the quality of result provided by the different variation with the increasing ability to collect data in an automated way the number of new kind of data continues to increase rapidly this make it increasingly difficult to undertake such effort for each and every new data type the most important aspect of distance function design is that since a human is the end user for any application the design must satisfy the user requirement with regard to effectiveness this creates the need for a systematic framework to design distance function which are sensitive to the particular characteristic of the data domain in this paper we discus such a framework the goal is to create distance function in an automated waywhile minimizing the work required from the user we will show that this framework creates distance function which are significantly more effective than popularly used function such a the euclidean metric 
this paper introduces an approach for clustering classification which is based on the use of local high order structure present in the data for some problem this local structure might be more relevant for classification than other measure of point similarity used by popular unsupervised and semi supervised clustering method under this approach change in the class label are associated to change in the local property of the data using this idea we also pursue to learn how to cluster given example of clustered data including from different datasets we make these concept formal by presenting a probability model that capture their fundamental and show that in this setting learning to cluster is a well defined and tractable task based on probabilistic inference method we then present an algorithm for computing the posterior probability distribution of class label for each data point experiment in the domain of spatial grouping and functional gene classification are used to illustrate and test these concept 
we present a new method for transductive learning which can be seen a a transductive version of the k nearest neighbor classifier unlike for many other transductive learning method the training problem ha a meaningful relaxation that can be solved globally optimally using spectral method we propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently a key advantage of the algorithm is that it doe not require additional heuristic to avoid unbalanced split furthermore we show a connection to transductive support vector machine and that an effective co training algorithm arises a a special case 
wepresentan optimalgridbasedalgorithm for pomdps that is tractable in the discountfactorandthemaximumabsolutevalue of the cost function but exponential in the dimension of the state space to the best of our knowledge this is the flrst optimal grid based algorithm for pomdps all other optimal algorithm that we know are based onsondik srepresentationofthevaluefunction we also propose a robustness criterion for grid based algorithm and show that the new algorithm is robust in such sense 
the support vector machine svm is a widely used tool for classification many efficient implementation exist for fitting a two class svm model the user ha to supply value for the tuning parameter the regularization cost parameter and the kernel parameter it seems a common practice is to use a default value for the cost parameter often leading to the least restrictive model in this paper we argue that the choice of the cost parameter can be critical we then derive an algorithm that can fit the entire path of svm solution for every value of the cost parameter with essentially the same computational cost a fitting one svm model we illustrate our algorithm on some example and use our representation to give further insight into the range of svm solution 
we propose a dynamic bayesian model for motif in biopolymer sequence which capture rich biological prior knowledge and positional dependency in motif structure in a principled way our model posit that the position specific multinomial parameter for mono mer distribution are distributed a a latent dirichlet mixture random variable and the position specific dirichlet component is determined by a hi dden markov process model parameter can be fit on training motif using a variational em algorithm within an empirical bayesian framework variational inference is also used for detecting hidden motif o ur model improves over previous model that ignore biological prior and positional dependence it ha much higher sensitivity to motif during detection and a notable ability to distinguish genuine motif from false recurring pattern 
we propose a sequential information maximization model a a general strategy for programming eye movement the model reconstructs high resolution visual information from a sequence of fixation taking into account the fall off in resolution from the fovea to the periphery from this framework we get a simple rule for predicting fixation sequence after each fixation fixate next at the location that minimizes uncertainty maximizes information about the stimulus by comparing our model performance to human eye movement data and to prediction from a saliency and random model we demonstrate that our model is best at predicting fixation location modeling additional biological constraint will improve the prediction of fixation sequence our result suggest that information maximization is a useful principle for programming eye movement 
according to a series of influential model dopamine da neuron signal reward prediction error using a temporal difference td algorithm we address a problem not convincingly solved in these account how to maintain a representation of cue that predict delayed consequence our new model us a td rule grounded in partially observable semi markov process a formalism that capture two largely neglected feature of da experiment hidden state and temporal variability previous model predicted reward using a tapped delay line representation of sensory input we replace this with a more active process of inference about the underlying state of the world the da system can then learn to map these inferred state to reward prediction using td the new model can explain previously vexing data on the response of da neuron in the face of temporal variability by combining statistical model based learning with a physiologically grounded td theory it also brings into contact with physiology some insight about behavior that had previously been confined to more abstract psychological model 
given a user specified minimum correlation threshold and a market basket database with n item and t transaction an all strong pair correlation query find all item pair with correlation above the threshold however when the number of item and transaction are large the computation cost of this query can be very high in this paper we identify an upper bound of pearson s correlation coefficient for binary variable this upper bound is not only much cheaper to compute than pearson s correlation coefficient but also exhibit a special monotone property which allows pruning of many item pair even without computing their upper bound a two step all strong pair correlation que ry taper algorithm is proposed to exploit these property in a filter and refine manner furthermore we provide an algebraic cost model which show that the computation saving from pruning is independent or improves when the number of item is increased in data set with common zipf or linear rank support distribution experimental result from synthetic and real data set exhibit similar trend and show that the taper algorithm can be an order of magnitude faster than brute force alternative 
a critical problem in cluster ensemble research is how to combine multiple clustering to yield a final superior clustering result leveraging advanced graph partitioning technique we solve this problem by reducing it to a graph partitioning problem we introduce a new reduction method that construct a bipartite graph from a given cluster ensemble the resulting graph model both instance and cluster of the ensemble simultaneously a vertex in the graph our approach retains all of the information provided by a given ensemble allowing the similarity among instance and the similarity among cluster to be considered collectively in forming the final clustering further the resulting graph partitioning problem can be solved efficiently we empirically evaluate the proposed approach against two commonly used graph formulation and show that it is more robust and achieves comparable or better performance in comparison to it competitor 
the problem of analyzing microarray data became one of important topic in bioinformatics over the past several year and different data mining technique have been proposed for the analysis of such data in this paper we propose to use association rule discovery method for determining association among expression level of different gene one of the main problem related to the discovery of these association is the scalability issue microarrays usually contain very large number of gene that are sometimes measured in s therefore analysis of such data can generate a very large number of association that can often be measured in million the paper address this problem by presenting a method that enables biologist to evaluate these very large number of discovered association rule during the post analysis stage of the data mining process this is achieved by providing several rule evaluation operator including rule grouping filtering browsing and data inspection operator that allow biologist to validate multiple individual gane regulation pattern at a time by iteratively applying these operator biologist can explore a significant part of all the initially generated rule in an acceptable period of time and thus answer biological question that are of a particular interest to him or her to validate our method we tested our system on the microarray data pertaining to the study of environmental hazard and their influence of gane expression process a a result we managed to answer several question that were of interest to the biologist that had collected this data 
we consider the problem of multi step ahead prediction in time series analysis using the non parametric gaussian process model step ahead forecasting of a discrete time non linear dynamic system can be performed by doing repeated one step ahead prediction for a state space model of the form the prediction of at time is based on the point estimate of the previous output in this paper we show how using an analytical gaussian approximation we can formally incorporate the uncertainty about intermediate regressor value thus updating the uncertainty on the current prediction 
developing regression model for large datasets that are both accurate and easy to interpret is a very important data mining problem regression tree with linear model in the leaf satisfy both these requirement but thus far no truly scalable regression tree algorithm is known this paper proposes a novel regression tree construction algorithm secret that produce tree of high quality and scale to very large datasets at every node secret us the em algorithm for gaussian mixture to find two cluster in the data and to locally transform the regression problem into a classification problem based on closeness to these cluster goodness of split measure like the gini gain can then be used to determine the split variable and the split point much like in classification tree construction scalability of the algorithm can be achieved by employing scalable version of the em and classification tree construction algorithm an experimental evaluation on real and artificial data show that secret ha accuracy comparable to other linear regression tree algorithm but take order of magnitude le computation time for large datasets 
we present a new method for learning good strategy in zero sum markov game in which each side is composed of multiple agent collaborating against an opposing team of agent our method requires full observability and communication during learning but the learned policy can be executed in a distributed manner the value function is represented a a factored linear architecture and it structure determines the necessary computational resource and communication bandwidth this approach permit a tradeoff between simple representation with little or no communication between agent and complex computationally intensive representation with extensive coordination between agent thus we provide a principled mean of using approximation to combat the exponential blowup in the joint action space of the participant the approach is demonstrated with an example that show the efficiency gain over naive enumeration 
we show how a conceptually simple search operator called optimal reinsertion can be applied to learning bayesian network structure from data on each step we pick a node called the target we delete all arc entering or exiting the target we then find subject to some constraint the globally optimal combination of in arc and out arc with which to reinsert it the heart of the paper is a new algorithm called orsearch which allows each optimal reinsertion step to be computed efficiently on large datasets our empirical result compare optimal reinsertion against a highly tuned implementation of multi restart hill climbing the result typically show one to two order of magnitude speed up on a variety of datasets they usually show better final result both in term of bdeu score and in modeling of future data drawn from the same distribution 
we introduce a new algorithm for mining sequential pattern our algorithm is especially efficient when the sequential pattern in the database are very long we introduce a novel depth first search strategy that integrates a depth first traversal of the search space with effective pruning mechanism our implementation of the search strategy combine a vertical bitmap representation of the database with efficient support counting a salient feature of our algorithm is that it incrementally output new frequent itemsets in an online fashion in a thorough experimental evaluation of our algorithm on standard benchmark data from the literature our algorithm outperforms previous work up to an order of magnitude 
color detection can be seriously affected by lighting condition and other variation in the environment the robot vision system need to be recalibrated a lighting condition change otherwise they fail to recognize object or classify them incorrectly this paper describes experiment toward object recognition under different lightning condition we propose to train the vision system to recognize object under different lightning condition using machine learning learned knowledge is then used for object recognition having attached leaning module to the vision system facilitates the object recognition and provides condition for automatic adaptation of the vision system to new environment 
microarray data provides a powerful basis for analysis of gene expression data mining method such a clustering have been widely applied to microarray data to link gene that show similar expression pattern however this approach usually fails to unveil multiple interaction by the same gene association rule mining ha been used for this purpose but the inherent limitation of association rule limit the applicability of the result in this paper we use a combination of association rule mining and loglinear modeling to discover k gene interaction using this technique we can discover interaction among k gene that cannot be explained by the combined eects of any of the subset of those gene we test our technique experimentally using yeast microarray data our result reveal some previously unknown association that have solid biological explanation 
in this work we quantitatively investigate the way in which a given person influence the joint turn taking behavior in a conversation after collecting an auditory database of social interaction among a group of twenty three people via wearable sensor hour of data each over two week we apply speech and conversation detection method to the auditory stream these method automatically locate the conversation determine their participant and mark which participant wa speaking when we then model the joint turn taking behavior a a mixed memory markov model that combine the statistic of the individual subject self transition and the partner cross transition the mixture parameter in this model describe how much each person s individual behavior contributes to the joint turn taking behavior of the pair by estimating these parameter we thus estimate how much influence each participant ha in determining the joint turntaking behavior we show how this measure correlate significantly with betweenness centrality an independent measure of an individual s importance in a social network this result suggests that our estimate of conversational influence is predictive of social influence 
we consider the problem of learning to classify partially specified instance i e instance that are described in term of attribute value at different level of precision using user supplied attribute value taxonomy avt we formalize the problem of learning from avt and data and present an avt guided decision tree learning algorithm avt dtl to learn classification rule at multiple level of abstraction the proposed approach generalizes existing technique for dealing with missing value to handle instance with partially missing value we present experimental result that demonstrate that avt dtl is able to effectively learn robust high accuracy classifier from partially specified example our experiment also demonstrate that the use of avt dtl outperforms standard decision tree algorithm c and it variant when applied to data with missing attribute value and produce substantially more compact decision tree than those obtained by standard approach 
in the last several year large olap database have become common in a variety of application such a corporate data warehouse and scientific computing to support interactive analysis many of these database are augmented with hierarchical structure that provide meaningful level of abstraction that can be leveraged by both the computer and analyst this hierarchical structure generates many challenge and opportunity in the design of system for the query analysis and visualization of these database in this paper we present an interactive visual exploration tool that facilitates exploratory analysis of data warehouse with rich hierarchical structure such a might be stored in data cube we base this tool on polaris a system for rapidly constructing table based graphical display of multidimensional database polaris build visualization using an algebraic formalism derived from the interface and interpreted a a set of query to a database we extend the user interface algebraic formalism and generation of data query in polaris to expose and take advantage of hierarchical structure in the resulting system analyst can navigate through the hierarchical projection of a database rapidly and incrementally generating visualization for each projection 
we propose a non linear canonical correlation analysis cca method which work by coordinating or aligning mixture of linear model in the same way that cca extends the idea of pca our work extends recent method for non linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinate are observed each lying on a different high dimensional manifold we also show that a special case of our method when applied to only a single manifold reduces to the laplacian eigenmaps algorithm a with previous alignment scheme once the mixture model have been estimated all of the parameter of our model can be estimated in closed form without local optimum in the learning experimental result illustrate the viability of the approach a a non linear extension of cca 
abstract loopy belief propagation bp ha been successfully used in a numberof di cult graphical model to find the most probable configurationof the hidden variable in application ranging from proteinfolding to image analysis one would like to find not just the bestconfiguration but rather the top m while this problem ha beensolved using the junction tree formalism in many real world problemsthe clique size in the junction tree is prohibitively large inthis work we address the 
the hepatitis temporal database collected at chiba university hospital between wa recently given to challenge the kdd research the database is large where each patient corresponds to test represented a sequence of irregular timestamp point with different length this paper present a temporal abstraction approach to mining knowledge from this hepatitis database exploiting hepatitis background knowledge and data analysis we introduce new notion and method for abstracting short term changed and long term changed test the abstracted data allow u to apply different machine learning method for finding knowledge part of which is considered a new and interesting by medical doctor 
for the discovery of similar pattern in d time series it is very typical to perform a normalization of the data for example a transformation so that the data follow a zero mean and unit standard deviation such transformation can reveal latent pattern and are very commonly used in datamining application however when dealing with multidimensional time series which appear naturally in application such a video tracking motion capture etc similar motion pattern can also be expressed at different orientation it is therefore imperative to provide support for additional transformation such a rotation in this work we transform the positional information of moving data into a space that is translation scale and rotation invariant our distance measure in the new space is able to detect elastic match and can be efficiently lower bounded thus being computationally tractable the proposed method are easy to implement fast to compute and can have many application for real world problem in area such a handwriting recognition and posture estimation in motion capture data finally we empirically demonstrate the accuracy and the efficiency of the technique using real and synthetic handwriting data 
we compute a common feature selection or kernel selection configuration for multiple support vector machine svms trained on different yet inter related datasets the method is advantageous when multiple classification task and differently labeled datasets exist over a common input space different datasets can mutually reinforce a common choice of representation or relevant feature for their various classifier we derive a multi task representation learning approach using the maximum entropy discrimination formalism the resulting convex algorithm maintain the global solution property of support vector machine however in addition to multiple svm classification regression parameter they also jointly estimate an optimal subset of feature or optimal combination of kernel experiment are shown on standardized datasets 
it ha been demonstrated that basic aspect of human visual motion perception are qualitatively consistent with a bayesian estimation framework where the prior probability distribution on velocity favor slow speed here we present a refined probabilistic model that can account for the typical trial to trialvariabilities observedin psychophysicalspeed perception experiment we also show that data from such experiment can be used to constrain both the likelihood and prior function of the model specifically we measured matching speed and threshold in a two alternative forced choice speed discrimination task parametric fit to the data reveal that the likelihood function is well approximated by a lognormal distribution with a characteristic contrast dependent variance and that the prior distribution on velocity exhibit significantly heavier tail than a gaussian and approximately follows a power law function 
learning general functional dependency is one of the main goal in machine learning recent progress in kernel based method ha focused on designing flexible and powerful input representation this paper address the complementary issue of problem involving complex output such a multiple dependent output variable and structured output space we propose to generalize multiclass support vector machine learning in a formulation that involves feature extracted jointly from input and output the resulting optimization problem is solved efficiently by a cutting plane algorithm that exploit the sparseness and structural decomposition of the problem we demonstrate the versatility and effectiveness of our method on problem ranging from supervised grammar learning and named entity recognition to taxonomic text classification and sequence alignment 
multi view algorithm reduce the amount of required training data by partitioning the domain feature into separate subset or view that are sufficient to learn the target concept such algorithm rely on the assumption that the view are sufficiently compatible for multi view learning i e most example are labeled identically in all view in practice it is unclear whether or not two view are sufficiently compatible for solving a new unseen learning task in order to cope with this problem we introduce a view validation algorithm given a learning task the algorithm predicts whether or not the view are sufficiently compatible for solving that particular task we use information acquired while solving several exemplar learning task to train a classifier that discriminates between the task for which the view are sufficiently and insufficiently compatible for multi view learning our experiment on wrapper induction and text classification show that view validation requires only a modest amount of training data to make high accuracy prediction 
we focus on the problem of efficient learning of dependency tree it is well known that given the pairwise mutual information coefficient a minimum weight spanning tree algorithm solves this problem exactly and in polynomial time however for large data set it is the construction of the correlation matrix that dominates the running time we have developed a new spanning tree algorithm which is capable of exploiting partial knowledge about edge weight the partial knowledge we maintain is a probabilistic confidence interval on the coefficient which we derive by examining just a small sample of the data the algorithm is able to flag the need to shrink an interval which translates to inspection of more data for the particular attribute pair experimental result show running time that is near constant in the number of record without significant loss in accuracy of the generated tree interestingly our spanning tree algorithm is based solely on tarjan s red edge rule which is generally considered a guaranteed recipe for bad performance 
mining frequent structural pattern from graph database is an interesting problem with broad application most of the previous study focus on pruning unfruitful search subspace effectively but few of them address the mining on large disk based database a many graph database in application cannot be held into main memory scalable mining of large disk based graph database remains a challenging problem in this paper we develop an effective index structure adi for adjacency index to support mining various graph pattern over large database that cannot be held into main memory the index is simple and efficient to build moreover the new index structure can be easily adopted in various existing graph pattern mining algorithm a an example we adapt the well known gspan algorithm by using the adi structure the experimental result show that the new index structure enables the scalable graph pattern mining over large database in one set of the experiment the new disk based method can mine graph database with one million graph while the original gspan algorithm can only handle database of up to thousand graph moreover our new method is faster than gspan when both can run in main memory 
we derive the limiting form of the eigenvalue spectrum for sample covariance matrix produced from non isotropic data for the analysis of standard pca we study the case where the data ha increased variance along a small number of symmetry breaking direction the limiting form of the eigenvalue spectrum depends on the strength of the symmetry breaking signal and on a parameter which is the ratio of sample size to data dimension result are derived in the limit of large data dimension while keeping fixed a increase there are transition in which delta function emerge from the upper end of the bulk spectrum corresponding to the symmetry breaking direction in the data and we calculate the bias in the corresponding eigenvalue for kernel pca the covariance matrix in feature space may contain symmetrybreaking structure even when the data component are independently distributed with equal variance we show example of phase transition behaviour analogous to the pca result in this case 
many different metric are used in machine learning and data mining to build and evaluate model however there is no general theory of machine learning metric that could answer question such a when we simultaneously want to optimise two criterion how can or should they be traded off some metric are inherently independent of class and misclassification cost distribution while other are not can this be made more precise this paper provides a derivation of roc space from first principle through d roc space and the skew ratio and redefines metric in these dimension the paper demonstrates that the graphical depiction of machine learning metric by mean of roc isometric give many useful insight into the characteristic of these metric and provides a foundation on which a theory of machine learning metric can be built 
the problem of inferring haplotype from genotype of single nucleotide polymorphism snp is essential for the understanding of genetic variation within and among population with important application to the genetic analysis of disease propensity and other complex trait the problem can be formulated a a mixture model where the mixture component correspond to the pool of haplotype in the population the size of this pool is unknown indeed knowing the size of the pool would correspond to knowing something significant about the genome and it history thus method for fitting the genotype mixture must crucially address the problem of estimating a mixture with an unknown number of mixture component in this paper we present a bayesian approach to this problem based on a nonparametric prior known a the dirichlet process the model also incorporates a likelihood that capture statistical error in the haplotype genotype relationship we apply our approach to the analysis of both simulated and real genotype data and compare to extant method 
analyzing gene expression data from microarray device ha many important application in medicine and biology but present significant challenge to data mining microarray data typically ha many attribute gene and few example sample making the process of correctly analyzing such data difficult to formulate and prone to common mistake for this reason it is unusually important to capture and record good practice for this form of data mining this paper present a process for analyzing microarray data including pre processing gene selection randomization testing classification and clustering this process is captured with clementine application template the paper describes the process in detail and includes three case study showing how the process is applied to class classification multi class classification and clustering analysis for publicly available microarray datasets 
an approach to semi supervised learning is proposed that is based on a gaussian random field model labeled and unlabeled data are represented a vertex in a weighted graph with edge weight encoding the similarity between instance the learning problem is then formulated in term of a gaussian random field on this graph where the mean of the field is characterized in term of harmonic function and is efficiently obtained using matrix method or belief propagation the resulting learning algorithm have intimate connection with random walk electric network and spectral graph theory we discus method to incorporate class prior and the prediction of classifier obtained by supervised learning we also propose a method of parameter learning by entropy minimization and show the algorithm s ability to perform feature selection promising experimental result are presented for synthetic data digit classification and text classification task 
in response to attack against enterprise network administrator increasingly deploy intrusion detection system these system monitor host network and other resource for sign of security violation the use of intrusion detection ha given rise to another difficult problem namely the handling of a generally large number of alarm in this paper we mine historical alarm to learn how future alarm can be handled more efficiently first we investigate episode rule with respect to their suitability in this approach we report the difficulty encountered and the unexpected insight gained in addition we introduce a new conceptual clustering technique and use it in extensive experiment with real world data to show that intrusion detection alarm can be handled efficiently by using previously mined knowledge 
we applied statistical mechanic to an inverse problem of linear mapping to investigate the physic of optimal lossy compression we used the replica symmetry breaking technique with a toy model to demonstrate shannon s result the rate distortion function which is widely known a the theoretical limit of the compression with a fidelity criterion is derived numerical study show that sparse construction of the model provide suboptimal compression 
in the task of adaptive information filtering a system receives a stream of document but delivers only those that match a person s information need a the system filter it also refines it knowledge about the user s information need based on relevance feedback from the user delivering a document thus ha two effect i it satisfies the user s information need immediately and ii it help the system better satisfy the user in the future by improving it model of the user s information need the traditional approach to adaptive information filtering fails to recognize and model this second effect this paper proposes utility divergence a the measure of model quality unlike the model quality measure used in most active learning method utility divergence is represented on the same scale a the filtering system s target utility function thus it is meaningful to combine the expected immediate utility with the model quality and to quantitatively manage the trade off between exploitation and exploration the proposed algorithm is implemented for setting the filtering system s dissemination threshold a major problem for adaptive filtering system experiment with trec and trec filtering data demonstrate that the proposed method is effective 
the paper study machine learning problem where each example is described using a set of boolean feature and where hypothesis are represented by linear threshold element one method of increasing the expressiveness of learned hypothesis in this context is to expand the feature set to include conjunction of basic feature this can be done explicitly or where possible by using a kernel function focusing on the well known perceptron and winnow algorithm the paper demonstrates a tradeofi between the computational e ciency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm we flrst describe several kernel function which capture either limited form of conjunction or all conjunction we show that these kernel can be used to e ciently run the perceptron algorithm over a feature space of exponentially many conjunction however we also show that using such kernel the perceptron algorithm can provably make an exponential number of mistake even when learning simple function we then consider the question of whether kernel function can analogously be used to run the multiplicative update winnow algorithm over an expanded feature space of exponentially many conjunction known upper bound imply that the winnow algorithm can learn disjunctive normal form dnf formula with a polynomial mistake bound in this setting however we prove that it is computationally hard to simulate winnow s behavior for learning dnf over such a feature set this implies that the kernel function which correspond to running winnow for this problem are not e ciently computable and that there is no general construction that can run winnow with kernel 
data cube pre computation is an important concept for supporting olap online analytical processing and ha been studied extensively it is often not feasible to compute a complete data cube due to the huge storage requirement recently proposed quotient cube addressed this issue through a partitioning method that group cube cell into equivalence partition such an approach is not only useful for distributive aggregate function such a sum but can also be applied to the holistic aggregate function like median maintaining a data cube for holistic aggregation is a hard problem since it difficulty lie in the fact that history tuple value must be kept in order to compute the new aggregate when tuples are inserted or deleted the quotient cube make the problem harder since we also need to maintain the equivalence class in this paper we introduce two technique called addset data structure and sliding window to deal with this problem we develop efficient algorithm for maintaining a quotient cube with holistic aggregation function that take up reasonably small storage space performance study show that our algorithm are effective efficient and scalable over large database 
we analyze the formal grounding behind negative correlation nc learning an ensemble learning technique developed in the evolutionary computation literature we show that by removing an assumption made in the original work nc can be seen to be exploiting the well known ambiguity decomposition of the ensemble error grounding it in a statistic framework around the biasvariance decomposition we use this grounding to find bound for the parameter and provide insight into the behaviour of the optimal parameter value these observation allow u understand how nc relates to other algorithm identifying a group of paper spread over the last decade that have all exploited the ambiguity decomposition for machine learning problem when taking into account our new understanding of the algorithm significant reduction in error rate were observed in empirical test 
we give a fast rejection scheme that is based on image segment and demonstrate it on the canonical example of face detection however instead of focusing on the detection step we focus on the rejection step and show that our method is simple and fast to be learned thus making it an excellent pre processing step to accelerate standard machine learning classifier such a neural network bayes classifier or svm we decompose a collection of face image into region of pixel with similar behavior over the image set the relationship between the mean and variance of image segment are used to form a cascade of rejectors that can reject over of image patch thus only a small fraction of the image patch must be passed to a full scale classifier moreover the training time for our method is much le than an hour on a standard pc the shape of the feature i e image segment we use is data driven they are very cheap to compute and they form a very low dimensional feature space in which exhaustive search for the best feature is tractable 
this paperpresentsan adaptivediscriminativegenerativemodelthat generalizes the conventional fisher linear discriminant algorithm and render a proper probabilistic interpretation within the context of object tracking we aim to find a discriminative generative model that best separate the target from the background we present a computationally efficient algorithm to constantly update this discriminative model a time progress while most tracking algorithm operate on the premise that the objectappearanceorambientlightingconditiondoesnot significantly change a time progress our method adapts a discriminative generative model to reflect appearance variation of the target and background thereby facilitating the tracking task in ever changingenvironments numerous experiment show that our method is able to learn a discriminative generative model for tracking target object undergoing large pose and lighting change 
secure multiparty computation allows party to jointly compute a function of their private input without revealing anything but the output theoretical result provide a general construction of such protocol for any function protocol obtained in this way are however inefficient and thus practically speaking useless when a large number of participant are involved the contribution of this paper is to define a new privacy model k privacy by mean of an innovative yet natural generalization of the accepted trusted third party model this allows implementing cryptographically secure efficient primitive for real world large scale distributed system a an example for the usefulness of the proposed model we employ k privacy to introduce a technique for obtaining knowledge by way of an association rule mining algorithm from large scale data grid while ensuring that the privacy is cryptographically secure 
loopybeliefpropagation bp hasbeensuccessfullyusedinanumberofdi cultgraphicalmodelstoflndthemostprobableconflgurationofthehiddenvariables inapplicationsrangingfromprotein folding to image analysis one would like to flnd not just the best conflguration but rather the top m while this problem ha been solvedusingthejunctiontreeformalism inmanyrealworldproblems the clique size in the junction tree is prohibitively large in thisworkweaddresstheproblemofflndingthe m bestconflgurations when exact inference is impossible westartbydevelopinganewexactinferencealgorithmforcalculating the best conflgurations that us only max marginals for approximateinference wereplacethemax marginalswiththebeliefs calculatedusingmax productbpandgeneralizedbp weshowempiricallythatthealgorithmcanaccuratelyandrapidlyapproximate the m best conflgurations in graph with hundredsof variable 
privacy and security concern can prevent sharing of data derailing data mining project distributed knowledge discovery if done correctly can alleviate this problem the key is to obtain valid result while providing guarantee on the non disclosure of data we present a method for k mean clustering when different site contain different attribute for a common set of entity each site learns the cluster of each entity but learns nothing about the attribute at other site 
an essential step in understanding the function of sensory nervous system is to characterize a accurately a possible the stimulus response function srf of the neuron that relay and process sensory information one increasingly common experimental approach is to present a rapidly varying complex stimulus to the animal while recording the response of one or more neuron and then to directly estimate a functional transformation of the input that account for the neuronal firing the estimation technique usually employed such a wiener filtering or other correlation based estimation of the wiener or volterra kernel are equivalent to maximum likelihood estimation in a gaussian output noise regression model we explore the use of bayesian evidence optimization technique to condition these estimate we show that by learning hyperparameters that control the smoothness and sparsity of the transfer function it is possible to improve dramatically the quality of srf estimate a measured by their success in predicting response to novel input 
a generative probabilistic model for object in image is presented an object consists of a constellation of feature feature appearance and pose are modeled probabilistically scene image are generated by drawing a set of object from a given database with random clutter sprinkled on the remaining image surface occlusion is allowed we study the case where feature from the same object share a common reference frame moreover parameter for shape and appearance density are shared across feature this is to be contrasted with previous work on probabilistic constellation model where feature depend on each other and each feature and model have different pose and appearance statistic these two difference allow u to build model containing hundred of feature a well a to train each model from a single example our model may also be thought of a a probabilistic revisitation of lowe s model we propose an efficient entropy minimization inference algorithm that construct the best interpretation of a scene a a collection of object and clutter we test our idea with experiment on two image database we compare with lowe s algorithm and demonstrate better performance in particular in presence of large amount of background clutter 
the goal of this work is to accurately detect and localize boundary in natural scene using local image measurement we formulate feature that respond to characteristic change in brightness and texture associated with natural boundary in order to combine the information from these feature in an optimal way a classifier is trained using human labeled image a ground truth we present precision recall curve showing that the resulting detector outperforms existing approach 
this paper is concerned with transductive learning although transduction appears to be an easier task than induction there have not been many provably useful algorithm and bound for transduction we present explicit error bound for transduction and derive a general technique for devising bound within this setting the technique is applied to derive error bound for compression scheme such a transductive svms and for transduction algorithm based on clustering 
reinforcement learning deal with learning optimal or near optimal policy while interacting with the environment application domain with many continuous variable are difficult to solve with existing reinforcement learning method due to the large search space in this paper we use a relational representation to define powerful abstraction that allow u to incorporate domain knowledge and re use previously learned policy in other similar problem we also describe how to learn useful action from human trace using a behavioural cloning approach combined with an exploration phase since several conflicting action may be induced for the same abstract state reinforcement learning is used to learn an optimal policy over this reduced space it is shown experimentally how a combination of behavioural cloning and reinforcement learning using a relational representation is powerful enough to learn how to fly an aircraft through different point in space and different turbulence condition 
a good distance function is an essential tool in application which involve querying large database such a image retrieval and bioinformatics we describe a non parametric algorithm for distance function learning which is based on the boosting of low grade weak learner in a product space the algorithm learns a function defined over pair of point using supervision in the form of equivalence constraint the weak learner are based on partitioning the original feature space using a generic density estimation generative model gmm augmented by equivalence constraint on pair of datapoints using a number of database from the uci repository we show significantly improved result over method which learn the parametric mahalanobis distance we also show initial result of image retrieval using a large database of facial image yaleb 
abstract a part of an environmental observation and forecasting system sensor deployed in the columbia river estuary corie gatherinformation on physical dynamic and change in estuary habitat 
this paper present a general family of algebraic positive definite similarity function over space of matrix with varying column rank the column can represent local region in an image whereby image have varying number of local part image of an image sequence motion trajectory in a multibody motion and so forth the family of set kernel we derive is based on a group invariant tensor product lifting with parameter that can be naturally tuned to provide a cook book of sort covering the possible wish list from similarity measure over set of varying cardinality we highlight the strength of our approach by demonstrating the set kernel for visual recognition of pedestrian using local part representation 
protein interaction typically arise from a physical inter action of one or more small site on the surface of the two protein identify ing these site is very important for drug and protein design in this paper we propose a computational method based on probabilistic relational model that attempt to address this task using high throughput protein i nteraction data and a set of short sequence motif we learn the model using the em algorithm with a branch and bound algorithm a an approximate inference for the e step our method search for motif whose presence in a pair of interacting protein can explain their observed int eraction it also try to determine which motif pair have high affinity and c an therefore lead to an interaction we show that our method is more accurate than others at predicting new protein protein interaction mo re importantly by examining solved structure of protein complex we find that of the predicted active motif correspond to actual interacti on site 
most text categorization system use simple model of document and document collection in this paper we describe a technique that improves a simple web page classifier s performance on page from a new unseen web site by exploiting link structure within a site a well a page structure within hub page on real world test case this technique significantly and substantially improves the accuracy of a bag of word classifier reducing error rate by about half on average the system us a variant of co training to exploit unlabeled data from a new site page are labeled using the base classifier the result are used by a restricted wrapper learner to propose potential main category anchor wrapper and finally these wrapper are used a feature by a third learner to find a categorization of the site that implies a simple hub structure but which also largely agrees with the original bag of word classifier 
a number of vertical mining algorithm have been proposed recently for association mining which have shown to be very effective and usually outperform horizontal approach the main advantage of the vertical format is support for fast frequency counting via intersection operation on transaction id tids and automatic pruning of irrelevant data the main problem with these approach is when intermediate result of vertical tid list become too large for memory thus affecting the algorithm scalability in this paper we present a novel vertical data representation called diffset that only keep track of difference in the tids of a candidate pattern from it generating frequent pattern we show that diffsets drastically cut down the size of memory required to store intermediate result we show how diffsets when incorporated into previous vertical mining method increase the performance significantly 
analyzing data to find trend correlation and stable pattern is an important task in many industrial application this paper proposes a new technique based on parallel coordinate visualization previous work on parallel coordinate method ha shown that they are effective only when variable that are correlated and or show similar pattern are displayed adjacently although current parallel coordinate tool allow the user to manually rearrange the order of variable this process is very time consuming when the number of variable is large automated assistance is required this paper introduces an edit distance based technique to rearrange variable so that interesting change pattern can be easily detected visually the visual miner v miner software includes both automated method for visualizing common pattern and a query tool that enables the user to describe specific target pattern to be mined or displayed by the system in addition the system can filter data according to rule set imported from other data mining tool this feature wa found very helpful in practice because it enables decision maker to visually identify interesting rule and data segment for further analysis or data mining this paper begin with an introduction to the proposed technique and the v miner system next a case study illustrates how v miner ha been used at motorola to guide product design and test decision 
text clustering method can be used to structure large set of text or hypertext document the well known method of text clustering however do not really address the special problem of text clustering very high dimensionality of the data very large size of the database and understandability of the cluster description in this paper we introduce a novel approach which us frequent item term set for text clustering such frequent set can be efficiently discovered using algorithm for association rule mining to cluster based on frequent term set we measure the mutual overlap of frequent set with respect to the set of supporting document we present two algorithm for frequent term based text clustering ftc which creates flat clustering and hftc for hierarchical clustering an experimental evaluation on classical text document a well a on web document demonstrates that the proposed algorithm obtain clustering of comparable quality significantly more efficiently than state of theart text clustering algorithm furthermore our method provide an understandable description of the discovered cluster by their frequent term set 
dimensionality reduction technique such a principal component analysis and factor analysis are used to discover a linear mapping between high dimensional data sample and point in a lower dimensional subspace in jojic and frey introduced mixture of transformation invariant component analyzer mtca that can account for global transformation such a translation and rotation perform clustering and learn local appearance deformation by dimensionality reduction however due to enormous computational requirement of the em algorithm for learning the model o where is the dimensionality of a data sample mtca wa not practical for most application in this paper we demonstrate how fast fourier transforms can reduce the computation to the order of log with this speedup we show the effectiveness of mtca in various application tracking video texture clustering video sequence object recognition and object detection in image 
xml document have recently become ubiquitous because of their varied applicability in a number of application classification is an important problem in the data mining domain but current classification method for xml document use ir based method in which each document is treated a a bag of word such technique ignore a significant amount of information hidden inside the document in this paper we discus the problem of rule based classification of xml data by using frequent discriminatory substructure within xml document such a technique is more capable of finding the classification characteristic of document in addition the technique can also be extended to cost sensitive classification we show the effectiveness of the method with respect to other classifier we note that the methodology discussed in this paper is applicable to any kind of semi structured data 
abstract feature selection a a preprocessing step tomachine learning ha been shown very effectivein reducing dimensionality removingirrelevant data increasing learning accuracy and improving comprehensibility in this paper we consider the problem of active featureselection in alter model setting wedescribe a formalism of active feature selectioncalled selective sampling demonstrate itby applying it to a widely used feature selectionalgorithm relief and show how it 
numerous application of data mining to scientific data involve the induction of a classification model in many case the collection of data is not performed with this task in mind and therefore the data might contain irrelevant or redundant feature that affect negatively the accuracy of the induction algorithm the size and dimensionality of typical scientific data make it difficult to use any available domain information to identify feature that discriminate between the class of interest similarly exploratory data analysis technique have limitation on the amount and dimensionality of the data they can process effectively in this paper we describe application of efficient feature selection method to data set from astronomy plasma physic and remote sensing we use variation of recently proposed filter method a well a traditional wrapper approach where practical we discus the general challenge of feature selection in scientific datasets the strategy for success that were common among our diverse application and the lesson learned in solving these problem 
we present a bayesian approach to color constancy which utilizes a nongaussian probabilistic model of the image formation process the parameter of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameter are chosen using cross validation the algorithm is empirically shown to exhibit rms error lower than other color constancy algorithm based on the lambertian surface reflectance model when estimating the illuminant of a set of test image this is demonstrated via a direct performance comparison utilizing a publicly available set of real world test image and code base 
a novel approach to combining clustering and feature selection is presented it implement a wrapper strategy for feature selection in the sense that the feature are directly selected by optimizing the discriminative power of the used partitioning algorithm on the technical side we present an efficient optimization algorithm with guaranteed local convergence property the only free parameter of this method is selected by a resampling based stability analysis experiment with real world datasets demonstrate that our method is able to infer both meaningful partition and meaningful subset of feature 
abstract the need to discretize a numerical rangeinto class coherent interval is a problemfrequently encountered training set error tse is one of the commonly used impurityfunctions in this task 
spatial collocation pattern associate the co existence of non spatial feature in a spatial neighborhood an example of such a pattern can associate contaminated water reservoir with certain decease in their spatial neighborhood previous work on discovering collocation pattern convert neighborhood of feature instance to itemsets and applies mining technique for transactional data to discover the pattern we propose a method that combine the discovery of spatial neighborhood with the mining process our technique is an extension of a spatial join algorithm that operates on multiple input and count long pattern instance a demonstrated by experimentation it yield significant performance improvement compared to previous approach 
computer experiment often require dense sweep over input parameter to obtain a qualitative understanding of their response such sweep can be prohibitively expensive and are unnecessary in region where the response is easy predicted well chosen design could allow a mapping of the response with far fewer simulation run thus there is a need for computationally inexpensive surrogate model and an accompanying method for selecting small design we explore a general methodology for addressing this need that us non stationary gaussian process binary tree partition the input space to facilitate non stationarity and a bayesian interpretation provides an explicit measure of predictive uncertainty that can be used to guide sampling our method are illustrated on several example including a motivating example involving computational fluid dynamic simulation of a nasa reentry vehicle 
well calibrated probability are necessary in many application like probabilistic framework or cost sensitive task based on previous success of asymmetric laplace method in calibrating text classifier score we propose to use piecewise logistic regression which is a simple extension of standard logistic regression a an alternative method in the discriminative family we show that both method have the flexibility to be piecewise linear function in log odds but they are based on quite different assumption we evaluated asymmetric laplace method piecewise logistic regression and standard logistic regression over standard text categorization collection reuters and trecap with three classifier svm naive bayes and logistic regression classifier and observed that piecewise logistic regression performs significantly better than the other two method in the log loss metric 
lasso least absolute shrinkage and selection operator is a useful tool to achieve the shrinkage and variable selection simultaneously since lasso us the l penalty the optimization should rely on the quadratic program qp or general non linear program which is known to be computational intensive in this paper we propose a gradient descent algorithm for lasso even though the final result is slightly le accurate the proposed algorithm is computationally simpler than qp or non linear program and so can be applied to large size problem we provide the convergence rate of the algorithm and illustrate it with simulated model a well a real data set 
i will discus the use of graphical model for data mining i will review key research area including structure learning variational method a relational modeling and describe application ranging from web traffic analysis to aid vaccine design 
abstract pairwise data in empirical science typically violate metricity eitherdue to noise or due to fallible estimate and therefore arehard to analyze by conventional machine learning technology inthis paper we therefore study way to work around this problem 
we exploit some useful property of gaussian process gp regression model for reinforcement learning in continuous state space and discrete time we demonstrate how the gp model allows evaluation of the value function in closed form the resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space further we speculate that the intrinsic ability of gp model to characterise distribution of function would allow the method to capture entire distribution over future value instead of merely their expectation which ha traditionally been the focus of much of reinforcement learning 
large and complex graph representing relationship among set of entity are an increasingly common focus of interest in data analysis example include social network web graph telecommunication network and biological network in interactive analysis of such data a natural query is which entity are most important in the network relative to a particular individual or set of individual we investigate the problem of answering such query in this paper focusing in particular on defining and computing the importance of node in a graph relative to one or more root node we define a general framework and a number of different algorithm building on idea from social network graph theory markov model and web graph analysis we experimentally evaluate the different property of these algorithm on toy graph and demonstrate how our approach can be used to study relative importance in real world network including a network of interaction among september th terrorist a network of collaborative research in biotechnology among company and university and a network of co authorship relationship among computer science researcher 
by comparison to some other sensory cortex the functional property of cell in the primary auditory cortex are not yet well understood recent attempt to obtain a generalized description of auditory cortical response have often relied upon characterization of the spectrotemporal receptive field strf which amount to a model of the stimulusresponse function srf that is linear in the spectrogram of the stimulus how well can such a model account for neural response at the very first stage of auditory cortical processing to answer this question we develop a novel methodology for evaluating the fraction of stimulus related response power in a population that can be captured by a given type of srf model we use this technique to show that in the thalamo recipient layer of primary auditory cortex strf model account for no more than of the stimulus related power in neural response 
we report a system that classifies and can learn to classify pattern of visual motion on line the complete system is described by the dynamic of it physical network architecture the combination of the following property make the system novel firstly the front end of the system consists of an avlsi optical flow chip that collectively computes d global visual motion in real time secondly the complexity of the classification task is significantly reduced by mapping the continuous motion trajectory to sequence of motion event and thirdly all the network structure are simple and with the exception of the optical flow chip based on a winner take all wta architecture we demonstrate the application of the proposed generic system for a contactless man machine interface that allows to write letter by visual motion regarding the low complexity of the system it robustness and the already existing front end a complete avlsi system on chip implementation is realistic allowing various application in mobile electronic device 
learning to fly an aircraft is a complex task that requires the development of control skill and goal achievement strategy this paper present a behavioural cloning system that learns to successfully fly manoeuvre in turbulence of a realistic aircraft simulation a hierarchical decomposition of the problem is employed where goal setting and the control skill to achieve them are learnt the benefit of this goal directed approach is demonstrated in the reuse of the learnt manoeuvre to a flight path that includes novel manoeuvre the system is based on an error minimisation technique that benefit from the use of model tree learner the model tree provide a compact and comprehensible representation of the underlying control skill and goal structure the performance of the system wa improved by compensating for human reaction time and goal anticipation we conclude that our system address current limitation of behavioural cloning by producing robust reusable and readable clone 
we consider the general problem of learning from labeled and unlabeled data which is often called semi supervised learning or transductive inference a principled approach to semi supervised learning is to design a classifying function which is sufciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled point we present a simple algorithm to obtain such a smooth solution our method yield encouraging experimental result on a number of classication problem and demonstrates effective use of unlabeled data 
we present an analysis of concentration of expectation phenomenon in layered bayesian network that use generalized linear model a the local conditional probability this framework encompasses a wide variety of probability distribution including both discrete and continuous random variable we utilize idea from large deviation analysis and the delta method to devise and evaluate a class of approximate inference algorithm for layered bayesian network that have superior asymptotic error bound and very fast computation time 
we present an extension to the jojic and frey layered sprite model which allows for layer to undergo afne transformation this extension allows for afne object pose to be inferred whilst simultaneously learning the object shape and appearance learning is carried out by applying an augmented variational inference algorithm which includes a global search over a discretised transform space followed by a local optimisation to aid correct convergence we use bottom up cue to restrict the space of possible afne transformation we present result on a number of video sequence and show how the model can be extended to track an object whose appearance change throughout the sequence 
the use of domain knowledge in a learner can greatly improve the model it produce however high quality expert knowledge is very difficult to obtain traditionally researcher have assumed that knowledge come from a single self consistent source a little explored but often more feasible alternative is to use multiple weaker source in this paper we take a step in this direction by developing a method for learning the structure of a bayesian network from multiple expert data is then used to refine the structure and estimate parameter a simple analysis show that even relatively few noisy expert can produce high quality knowledge when combined experiment with real and simulated expert in a variety of domain show the benefit of this approach 
the information bottleneck ib method is an information theoretic formulation for clustering problem given a joint distribution this method construct a new variable that defines partition over the value of that are informative about maximum likelihood ml of mixture model is a standard statistical approach to clustering problem in this paper we ask how are the two method related we define a simple mapping between the ib problem and the ml problem for the multinomial mixture model we show that under this mapping the problem are strongly related in fact for uniform input distribution over or for large sample size the problem are mathematically equivalent specifically in these case every fixed point of the ib functional defines a fixed point of the log likelihood and vice versa moreover the value of the functionals at the fixed point are equal under simple transformation a a result in these case every algorithm that solves one of the problem induces a solution for the other 
we examine the marriage of recent probabilistic generative model for social network with classical framework from mathematical economics we are particularly interested in how the statistical structure of such network influence global economic quantity such a price variation our finding are a mixture of formal analysis simulation and experiment on an international trade data set from the united nation 
nonlinear ltering can solve very complex problem but typically involve very time consuming calculation here we show that for lters that are constructed a a rbf network with gaussian basis function a decomposition into linear lters exists which can be computed ecien tly in the frequency domain yielding dramatic improvement in speed we present an application of this idea to image processing in electron micrograph image of photoreceptor terminal of the fruit y drosophila synaptic vesicle containing neurotransmitter should be detected and labeled automatically we use hand label provided by human expert to learn a rbf lter using support vector regression with gaussian kernel we will show that the resulting nonlinear lter solves the task to a degree of accuracy which is close to what can be achieved by human expert this allows the very time consuming task of data evaluation to be done ecien tly 
positive definite kernel between labeled graph have recently been proposed they enable the application of kernel method such a support vector machine to the analysis and classification of graph for example chemical compound these graph kernel are obtained by marginalizing a kernel between path with respect to a random walk model on the graph vertex along the edge we propose two extension of these graph kernel with the double goal to reduce their computation time and increase their relevance a measure of similarity between graph first we propose to modify the label of each vertex by automatically adding information about it environment with the use of the morgan algorithm second we suggest a modification of the random walk model to prevent the walk from coming back to a vertex that wa just visited these extension are then tested on benchmark experiment of chemical compound classification with promising result 
the problem of identifying approximately duplicate record in database is an essential step for data cleaning and data integration process most existing approach have relied on generic or manually tuned distance metric for estimating the similarity of potential duplicate in this paper we present a framework for improving duplicate detection using trainable measure of textual similarity we propose to employ learnable text distance function for each database field and show that such measure are capable of adapting to the specific notion of similarity that is appropriate for the field s domain we present two learnable text similarity measure suitable for this task an extended variant of learnable string edit distance and a novel vector space based measure that employ a support vector machine svm for training experimental result on a range of datasets show that our framework can improve duplicate detection accuracy over traditional technique 
the task of object identification occurs when integrating information from multiple website the same data object can exist in inconsistent text format across site making it difficult to identify matching object using exact text match previous method of object identification have required manual construction of domain specific string transformation or manual setting of general transformation parameter weight for recognizing format inconsistency this manual process can be time consuming and error prone we have developed an object identification system called active atlas which applies a set of domain independent string transformation to compare the object shared attribute in order to identify matching object in this paper we discus extension to the active atlas system which allow it to learn to tailor the weight of a set of general transformation to a specific application domain through limited user input the experimental result demonstrate that this approach achieves higher accuracy and requires le user involvement than previous method across various application domain 
log concavity is an important property in the context of optimization laplace approximation and sampling bayesian method based on gaussian process prior have become quite popular recently for classification regression density estimation and point process intensi ty estimation here we prove that the predictive density corresponding to each of these application are log concave given any observed data we also prove that the likelihood is log concave in the hyperparameters controlling the mean function of the gaussian prior in the density and point process intensity estimation case and the mean covariance and observation noise parameter in the classification and regression case this result lead to a useful parameterization of these hyperparameters indicating a suitably large class of prior for which the corresponding maximum a posteriori problem is log concave 
recent eye tracking study in natural task suggest that there is a tight link between eye movement and goal directed motor action however most existing model of human eye movement provide a bottom up account that relates visual attention to attribute of the visual scene the purpose of this paper is to introduce a new model of human eye movement that directly tie eye movement to the ongoing demand of behavior the basic idea is that eye movement serve to reduce uncertainty about environmental variable that are task relevant a value is assigned to an eye movement by estimating the expected cost of the uncertainty that will result if the movement is not made if there are several candidate eye movement the one with the highest expected value is chosen the model is illustrated using a humanoid graphic figure that navigates on a sidewalk in a virtual urban environment simulation show our protocol is superior to a simple round robin scheduling mechanism this paper introduces a new framework for understanding the scheduling of human eye movement the human eye is characterized by a small high resolution fovea the importance of foveal vision mean that fast ballistic eye movement called saccade are made at a rate of approximately three per second to direct gaze to relevant area of the visual field since the location of the fovea provides a powerful clue to what information the visual system is processing understanding the scheduling and targeting of eye movement is key to understanding the organization of human vision the recent advent of portable eye tracker ha made it possible to study eye movement in everyday behavior these study show that behavior such a driving or navigating a city sidewalk show rapid alternating saccade to different target indicative of competing perceptual demand this paper introduces a model of how human select visual target in term of the value of the information obtained previous work ha modeled the direction of the eye to target primarily in term of visual saliency such model fail to incorporate the role of task demand and do not address the problem of resource contention in contrast our underlying premise is that much of routine human behavior can be understood in the framework of reward maximization in other word human choose action by trading off the cost of the 
we present analog neuromorphic circuit for implementing bistable synapsis with spike timing dependent plasticity stdp property in these type of synapsis the short term dynamic of the synaptic efficacy are governed by the relative timing of the preand post synaptic spike while on long time scale the efficacy tend asymptotically to either a potentiated state or to a depressed one we fabricated a prototype vlsi chip containing a network of integrate and fire neuron interconnected via bistable stdp synapsis test result from this chip demonstrate the synapse s stdp learning property and it long term bistable characteristic 
information gain is a well known and empirically proven method for high dimensional feature selection we found that it and other existing method failed to produce good result on an industrial text classification problem on investigating the root cause we find that a large class of feature scoring method suffers a pitfall they can be blinded by a surplus of strongly predictive feature for some class while largely ignoring feature needed to discriminate difficult class in this paper we demonstrate this pitfall hurt performance even for a relatively uniform text classification task based on this understanding we present solution inspired by round robin scheduling that avoid this pitfall without resorting to costly wrapper method empirical evaluation on datasets show substantial improvement 
we consider a dynamic market place of self interested agent with differing capability a task to be completed is proposed to the agent population an agent attempt to form a coalition of agent to perform the task before proposing a coalition the agent must determine the optimal set of agent with whom to enter into a coalition for this task we refer to this activity a coalition calculation to determine the optimal coalition the agent must have a mean of calculating the value of any given coalition multiple metric cost time quality etc determine the true value of a coalition however because of conflicting metric differing metric importance and the tendency of metric importance to vary over time it is difficult to obtain a true valuation of a given coalition previous work ha not addressed these issue we present a solution based on the adaptation of a multi objective optimization evolutionary algorithm in order to obtain a true valuation of any coalition we use the concept of pareto dominance coupled with a distance weighting algorithm we determine the pareto optimal set of coalition and then use an instance based learning algorithm to select the optimal coalition we show through empirical evaluation that the proposed technique is capable of eliciting metric importance and adapting to metric variation over time 
we present metric a provably near optimal algorithm for reinforcement learning in markov decision process in which there is a natural metric on the state space that allows the construction of accurate local model the algorithm is a generalization of the algorithm of kearns and singh and assumes a black box for approximate planning unlike the original metricfinds a near optimal policy in an amount of time that doe not directly depend on the size of the state space but instead depends on the covering number of the state space informally the covering number is the number of neighborhood required for accurate local modeling 
traditional clustering is a descriptive task that seek to identify homogeneous group of object based on the value of their attribute while domain knowledge is always the best way to justify clustering few clustering algorithm have ever take domain knowledge into consideration in this paper the domain knowledge is represented by hierarchical ontology we develop a framework by directly incorporating domain knowledge into clustering process yielding a set of cluster with strong ontology implication during the clustering process ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithm meanwhile the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace cluster with significant categorical enrichment onto the ontology hierarchy our experiment on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality meanwhile many hierarchical organization of gene cluster corresponding to a sub hierarchy in gene ontology were also successfully captured 
decision tree are surprisingly adaptive in three importan t respect they automatically adapt to favorable condition near the bayes decision boundary focus on data distributed on lower dimensional manifold reject irrelevant feature in this paper we examine a decision tre e based on dyadic split that adapts to each of these condition to achieve minimax optimal rate of convergence the proposed classifi er is the first known to achieve these optimal rate while being practi cal and implementable each of the above property can be formalized and translated into a class of distribution with known minimax rate of convergence adaptivity is a highly desirable quality of classifier since in practice the precise characteristic o f the distribution are unknown we show that dyadic decision tree achieve the minimax optimal rate to within a log factor without needing to know the specific parameter defin ing the class such tree are constructed by minimizing a complexity penalized empirical risk over an appropriate family of dyadic partition the complexity term is derived from a new generalization error bound for tree inspired by this bound in turn lead to an oracle inequality from which the optimal rate are derived full proof of all result are given in 
approximate linear programming alp ha emerged recently a one of the most promising method for solving complex factored mdps with nite state space in this work we show that alp solution are not limited only to mdps with nite state space but that they can also be applied successfully to factored continuous state mdps cmdps we show how one can build an alp based approximation for such a model and contrast it to existing solution method we argue that this approach offer a robust alternative for solving high dimensional continuous state space problem the point is supported by experiment on three cmdp problem with continuous state factor 
this paper extends our earlier analysis on approximate linear programming a an approach to approximating the cost to go function in a discounted cost dynamic program in this paper we consider the average cost criterion and a version of approximate linear programming that generates approximation to the optimal average cost and differential cost function we demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well aligned with the objective of deriving a policy with low average cost for that the algorithm should aim at producing a good approximation of the differential cost function we propose a twophase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portion of the state space via state relevance weight performance bound suggest that the new algorithm is compatible with the objective of optimizing performance and provide guidance on appropriate choice for state relevance weight 
we define a connection subgraph a a small subgraph of a large graph that best capture the relationship between two node the primary motivation for this work is to provide a paradigm for exploration and knowledge discovery in large social network graph we present a formal definition of this problem and an ideal solution based on electricity analogue we then show how to accelerate the computation to produce approximate but high quality connection subgraphs in real time on very large disk resident graph we describe our operational prototype and we demonstrate result on a social network graph derived from the world wide web our graph contains million node and million edge and our system still produce quality response within second 
we present a connectionist architecture that can learn a model of the relation between perception and action and use this model for behavior planning state representation are learned with a growing selforganizing layer which is directly coupled to a perception and a motor layer knowledge about possible state transition is encoded in the lateral connectivity motor signal modulate this lateral connectivity and a dynamic field on the layer organizes a planning process all mechanism are local and adaptation is based on hebbian idea the model is continuous in the action perception and time domain 
we explore the phenomenon of subjective randomness a a case study in understanding how people discover structure embedded in noise we present a rational account of randomness perception based on the statistical problem of model selection given a stimulus inferring whether the process that generated it wa random or regular inspired by the mathematical definition of randomness given by kolmogorov complexity we characterize regularity in term of a hierarchy of automaton that augment a finite controller with different form of memory we find that the regularity detected in binary sequence depend upon presentation format and that the kind of automaton that can identify these regularity are informative about the cognitive process engaged by different format 
data quality is a central issue for many information oriented organization recent advance in the data quality field reflect the view that a database is the product of a manufacturing process while routine error such a non existent zip code can be detected and corrected using traditional data cleansing tool many error systemic to the manufacturing process cannot be addressed therefore the product of the data manufacturing process is an imprecise recording of information about the entity of interest i e customer transaction or asset in this way the database is only one flawed version of the entity it is supposed to represent quality assurance system such a motorola s six sigma and other continuous improvement method document the data manufacturing process s shortcoming a widespread method of documentation is quality matrix in this paper we explore the use of the readily available data quality matrix for the data mining classification task we first illustrate that if we do not factor in these quality matrix then our result for prediction are sub optimal we then suggest a general purpose ensemble approach that perturbs the data according to these quality matrix to improve the predictive accuracy and show the improvement is due to a reduction in variance 
the focus of this paper is to present a methodology for validating the relevance of autonomy technology to current and future space mission in this paper we will discus the objective of nasa space exploration mission and explain the requirement needed for autonomy technology to achieve mission goal by focusing on the underlying purpose of the mission that of maximizing scientific yield we will analyze how autonomy technology address achievement of mission objective we will discus how technology such a reasoning planning and autonomous control have a direct influence on mission success the methodology proposed break down mission component into operational function and discus how technology based on performance metric enable achievement of these function and increase in science return specific example of validating autonomy technology applied to surface exploration mission will be provided 
we propose a multiclass mc classification approach to text categorization tc to fully take advantage of both positive and negative training example a maximal figure of merit mfom learning algorithm is introduced to train high performance mc classifier in contrast to conventional binary classification the proposed mc scheme assigns a uniform score function to each category for each given test sample and thus the classical bayes decision rule can now be applied since all the mc mfom classifier are simultaneously trained we expect them to be more robust and work better than the binary mfom classifier which are trained separately and are known to give the best tc performance experimental result on the reuters tc task indicate that the mc mfom classifier achieve a micro averaging f value of which is significantly better than obtained with the binary mfom classifier for the category with le than training sample furthermore for all category most with large training size the mc mfom classifier give a micro averaging f value of better than obtained with the binary mfom classifier 
we formulate the regression problem a one of maximizing the minimum probability symbolized by that future predicted output of the regression model will be within some bound of the true regression function our formulation is unique in that we obtain a direct estimate of this lower probability bound the proposed framework minimax probability machine regression mpmr is based on the recently described minimax probability machine classification algorithm lanckriet et al and us mercer kernel to obtain nonlinear regression model mpmr is tested on both toy and real world data verifying the accuracy of the bound and the efficacy of the regression model 
we describe a new algorithm for computing a nash equilibrium ingraphical game a compact representation for multi agent systemsthat we introduced in previous work the algorithm is the rstto compute equilibrium both eciently and exactly for a non trivialclass of graphical game 
similarity measure of time series is an important subroutine in many kdd application previous similarity model mainly focus on the prominent series behavior by considering the whole information of time series in this paper we address the problem which portion of information is more suitable for similarity measure for the data collected from a certain field we propose a model for the retrieval and representation of the partial information in time series data and a methodology for evaluating the similarity measurement based on partial information the methodology is to retrieve various portion of information from the raw data and represent it in a concise form then cluster the time series using the partial information and evaluate the similarity measurement through comparing the result with a standard classification experiment on data set from stock market give some interesting observation and justify the usefulness of our approach 
how do cortical neuron represent the acoustic environment this question is often addressed by probing with simple stimulus such a click or tone pip such stimulus have the advantage of yielding easily interpreted answer but have the disadvantage that they may fail to uncover complex or higher order neuronal response property here we adopt an alternative approach probing neuronal response with complex acoustic stimulus including animal vocalization and music we have used in vivo whole cell method in the rat auditory cortex to record subthreshold membrane potential fluctuation elicited by these stimulus whole cell recording reveals the total synaptic input to a neuron from all the other neuron in the circuit instead of just it output a sparse binary spike train a in conventional single unit physiological recording whole cell recording thus provides a much richer source of information about the neuron s response many neuron responded robustly and reliably to the complex stimulus in our ensemble here we analyze the linear component the spectrotemporal receptive field strf of the transformation from the sound a represented by it time varying spectrogram to the neuron s membrane potential we find that the strf ha a rich dynamical structure including excitatory region positioned in general accord with the prediction of the simple tuning curve we also find that in many case much of the neuron s response although deterministically related to the stimulus cannot be predicted by the linear component indicating the presence of a yet uncharacterized nonlinear response property 
we interpret several well known algorithm for dimensionality reduction of manifold a kernel method isomap graph laplacian eigenmap and locally linear embedding lle all utilize local neighborhood information to construct a global embedding of the manifold we show how all three algorithm can be described a kernel pca on specially constructed gram matrix and illustrate the similarity and difference between the algorithm with representative example 
deduplication is a key operation in integrating data from multiple source the main challenge in this task is designing a function that can resolve when a pair of record refer to the same entity in spite of various data inconsistency most existing system use hand coded function one way to overcome the tedium of hand coding is to train a classifier to distinguish between duplicate and non duplicate the success of this method critically hinge on being able to provide a covering and challenging set of training pair that bring out the subtlety of deduplication function this is non trivial because it requires manually searching for various data inconsistency between any two record spread apart in large list we present our design of a learning based deduplication system that us a novel method of interactively discovering challenging training pair using active learning our experiment on real life datasets show that active learning significantly reduces the number of instance needed to achieve high accuracy we investigate various design issue that arise in building a system to provide interactive response fast convergence and interpretable output 
abstract knowledge about local invariance with respect to given pattern transformation can greatly improve the accuracy of classification previous approach are either based on regularisation or on the generation of virtual transformed example we develop a new framework for learning linear classifier under known transformation based on semidefinite programming we present a new learning algorithm the semidefinite programming machine sdpm which is able to find a maximum margin hyperplane when the training example are polynomial trajectory instead of single point the solution is found to be sparse in dual variable and allows to identify those point on the trajectory with minimal real valued output a virtual support vector extension to segment of trajectory to more than one transformation parameter and to learning with kernel are discussed in experiment we use a taylor expansion to locally approximate rotational invariance in pixel image from usps and find improvement over known method 
a population of neuron typically exhibit a broad diversity of response to sensory input the intuitive notion of functional classification is that cell can be clustered so that most of the diversity is captured by the identity of the cluster rather than by individual within cluster we show how this intuition can be made precise using information theory without any need to introduce a metric on the space of stimulus or response applied to the retinal ganglion cell of the salamander this approach recovers classical result but also provides clear evidence for subclass beyond those identified previously further we find that each of the ganglion cell is functionally unique and that even within the same subclass only a few spike are needed to reliably distinguish between cell 
weprovideamethodformassmeta analysisinaneuroinformatics database containing stereotaxic talairach coordinate from neuroimaging experiment database label are used to group the individualexperiments e g accordingtocognitivefunction andthe consistent pattern of the experiment within the group are determined themethodvoxelizeseachgroupofexperimentviaakernel densityestimationformingprobabilitydensityvolumes thevalues intheprobabilitydensityvolumesarecomparedtonull hypothesis distributionsgeneratedfromresamplingintheentireunlabeledset of experiment and the distance to the null hypothesis are used to sort the voxel across group of experiment this allows for mass meta analysis with the construction of a list with the most prominent association between brain area and group label furthermore themethodcanbeusedforfunctionallabelingofvoxels 
recently the isomap algorithm ha been proposed for learning a nonlinear manifold from a set of unorganized high dimensional data point it is based on extending the classical multidimensional scaling method for dimension reduction in this paper we present a continuous version of isomap which we call continuum isomap and show that manifold learning in the continuous framework is reduced to an eigenvalue problem of an integral operator we also show that the continuum isomap can perfectly recover the underlying natural parametrization if the nonlinear manifold can be isometrically embedded onto an euclidean space several numerical example are given to illustrate the algorithm 
various problem in machine learning database and statistic involve pairwise distance among a set of object it is often desirable for these distance to satisfy the property of a metric especially the triangle inequality application where metric data is useful include clustering classification metric based indexing and approximation algorithm for various graph problem this paper present the metric nearness problem given a dissimilarity matrix find the nearest matrix of distance that satisfy the triangle inequality for p nearness measure this paper develops efficient triangle fixing algorithm that compute globally optimal solution by exploiting the inherent structure of the problem empirically the algorithm have time and storage cost that are linear in the number of triangle constraint the method can also be easily parallelized for additional speed 
active and semi supervised learning are important technique when labeled data are scarce we combine the two under a gaussian random field model labeled and unlabeled data are represented a vertex in a weighted graph with edge weight encoding the similarity between instance the semi supervised learning problem is then formulated in term of a gaussian random field on this graph the mean of which is characterized in term of harmonic function active learning is performed on top of the semisupervised learning scheme by greedily selecting query from the unlabeled data to minimize the estimated expected classification error risk in the case of gaussian field the risk is efficiently computed using matrix method we present experimental result on synthetic data handwritten digit recognition and text classification task the active learning scheme requires a much smaller number of query to achieve high accuracy compared with random query selection 
existing algorithm for discrete partially observable markov decision process can at best solve problem of a few thousand state due to two important source of intractability the curse of dimensionality and the policy space complexity this paper describes a new algorithm vdcbpi that mitigates both source of intractability by combining the value directed compression vdc technique with bounded policy iteration bpi the scalability of vdcbpi is demonstrated on synthetic network management problem with up to million state 
bayesian regularization and nonnegative deconvolution brand is proposed for estimating time delay of acoustic signal in reverberant environment sparsity of the nonnegative filter coefficient is enforced using an l norm regularization a probabilistic generative model is used to simultaneously estimate the regularization parameter and filter coefficient from the signal data iterative update rule are derived under a bayesian framework using the expectation maximization procedure the resulting time delay estimation algorithm is demonstrated on noisy acoustic data 
we extend recent work on the connection between loopy belief propagation and the bethe free energy constrained minimization of the bethe free energy can be turned into an unconstrained saddle point problem both converging double loop algorithm and standard loopy belief propagation can be interpreted a attempt to solve this saddle point problem stability analysis then lead u to conclude that stable xed point of loopy belief propagation must be local minimum of the bethe free energy perhaps surprisingly the converse need not be the case minimum can be unstable xed point we illustrate this with an example and discus implication 
we analyze the convergence property of three spike triggered data analysis technique all of our result are obtained in the setting of a possibly multidimensional linear nonlinear ln cascade model for stimulus driven neural activity we start by giving exact rate of convergence result for the common spike triggered average sta technique next we analyze a spike triggered covariance method variant of which have been recently exploited successfully by bialek simoncelli and colleague these rst two method suffer from extraneous condition on their convergence therefore we introduce an estimator for the ln model parameter which is designed to be consistent under general condition we provide an algorithm for the computation of this estimator and derive it rate of convergence we close with a brief discussion of the eciency of these estimator and an application to data recorded from the primary motor cortex of awake behaving primate 
learning kernel parameter is important for kernel based method because these parameter have significant impact on the generalization ability of these method besides the method of cross validation and leave one out minimizing some upper bound on the generalization error such a the radius margin bound wa also proposed to more efficiently learn the optimal kernel parameter in this paper a class separability criterion is proposed for learning kernel parameter the optimal kernel parameter are regarded a those that can maximize the class separability in the induced feature space with this criterion learning the kernel parameter in svm can avoid solving the quadratic programming problem the relationship between this criterion and the radius margin bound is also explored both theoretical analysis and experimental result show that the class separability criterion is effective in learning kernel parameter for svm 
we describe a procedure which find a hierarchical clustering by hillclimbing the cost function we use is a hierarchical extension of the mean cost our local move are tree restructurings and node reordering we show these can be accomplished efficiently by exploiting special property of squared euclidean distance and by using technique from scheduling and vlsi algorithm 
we describe a modification to the adaboostalgorithm that permit the incorporation ofprior human knowledge a a mean of compensatingfor a shortage of training data wegive a convergence result for the algorithm 
existing source location and recovery algorithm used in magnetoencephalographic imaging generally assume that the source activity at different brain location is independent or that the correlation structure is known however electrophysiological recording of local field potential show strong correlation in aggregate activity over significant distance indeed it seems very likely that stimulus evoked activity would follow strongly correlated time course in different brain area here we present and validate through simulation a new approach to source reconstruction in which the correlation between source is modelled and estimated explicitly by variational bayesian method facilitating accurate recovery of source location and the time course of their activation 
identity uncertainty is a pervasive problem in real world data analysis it arises whenever object are not labeled with unique identifier or when those identifier may not be perceived perfectly in such case two observation may or may not correspond to the same object in this paper we consider the problem in the context of citation matching the problem of deciding which citation correspond to the same publication our approach is based on the use of a relational probability model to define a generative model for the domain including model of author and title corruption and a probabilistic citation grammar identity uncertainty is handled by extending standard model to incorporate probability over the possible mapping between term in the language and object in the domain inference is based on markov chain monte carlo augmented with specific method for generating efficient proposal when the domain contains many object result on several citation data set show that the method outperforms current algorithm for citation matching the declarative relational nature of the model also mean that our algorithm can determine object characteristic such a author name by combining multiple citation of multiple paper 
we describe a method that can make a scanned handwritten mediaeval latin manuscript accessible to full text search a generalized hmm is tted using transcribed latin to obtain a transition model and one example each of letter to obtain an emission model we show result for unigram bigram and trigram model our method transcribes page of a manuscript of terence with fair accuracy of letter correctly transcribed search result are very strong we use example of variant spelling to demonstrate that the search respect the ink of the document furthermore our model produce fair search on a document from which we obtained no training data intoduction there are many large corpus of handwritten scanned document and their number is growing rapidly collection range from the complete work of mark twain to thousand of page of zoological note spanning two century large scale analysis of such corpus is currently very dif cult because handwriting recognition work poorly recently rath and manmatha have demonstrated that one can use small body of aligned material a supervised data to train a word spotting mechanism the result can make scanned handwritten document searchable current technique assume a closed vocabulary one can search only for word in the training set and search for instance of whole word this approach is particularly unattractive for an in ected language because individual word can take so many form that one is unlikely to see all in the training set furthermore one would like the method used to require very little aligned training data so that it is possible to process document written by different scribe with little overhead mediaeval latin manuscript are a natural rst corpus for studying this problem because there are many scanned manuscript and because the handwriting is relatively regular we expect the primary user need to be search over a large body of document to allow comparison between document rather than transcription of a particular document which is usually relatively easy to do by hand desirable feature for a system are first that it use little or no aligned training data an 
in this paper we provide a fast data driven solution to the failing query problem given a query that return an empty answer how can one relax the query s constraint so that it return a non empty set of tuples we introduce a novel algorithm loqr which is designed to relax query that are in the disjunctive normal form and contain a mixture of discrete and continuous attribute loqr discovers the implicit relationship that exist among the various domain attribute and then us this knowledge to relax the constraint from the failing query in a first step loqr us a small randomly chosen subset of the target database to learn a set of decision rule that predict whether an attribute s value satisfies the constraint in the failing query this query driven operation is performed online for each failing query in the second step loqr us nearest neighbor technique to find the learned rule that is the most similar to the failing query then it us the attribute value from this rule to relax the failing query s constraint our experiment on six application domain show that loqr is both robust and fast it successfully relaxes more than of the failing query and it take under a second for processing query that consist of up to attribute larger query of up to attribute are processed in several second 
subspace learning approach have attracted much attention in academia recently however the classical batch algorithm no longer satisfy the application on streaming data or large scale data to meet this desirability incremental principal component analysis ipca algorithm ha been well established but it is an unsupervised subspace learning approach and is not optimal for general classification task such a face recognition and web document categorization in this paper we propose an incremental supervised subspace learning algorithm called incremental maximum margin criterion immc to infer an adaptive subspace by optimizing the maximum margin criterion we also present the proof for convergence of the proposed algorithm experimental result on both synthetic dataset and real world datasets show that immc converges to the similar subspace a that of batch approach 
a fast and accurate unsupervised clustering algorithm ha been developed for clustering very large datasets though designed for very large volume of geospatial data the algorithm is general enough to be used in a wide variety of domain application the number of computation the algorithm requires is o n and thus faster than hierarchical algorithm unlike the popular k mean heuristic this algorithm doe not require a series of iteration to converge to a solution in addition this method doe not depend on initialization of a given number of cluster representative and so is insensitive to initial condition being unsupervised the algorithm can also rank each cluster based on density the method relies on weighting a dataset to grid point on a mesh and using a small number of rule based agent to find the high density cluster this method effectively reduces large datasets to the size of the grid which is usually many order of magnitude smaller numerical experiment are shown that demonstrate the advantage of this algorithm over other technique 
most existing tracking algorithm construct a representation of a target object prior to the tracking task start and utilize invariant feature to handle appearance variation of the target caused by lighting pose and view angle change in this paper we present an efficient and effective online algorithm that incrementally learns and adapts a low dimensional eigenspace representation to reflect appearance change of the target thereby facilitating the tracking task furthermore our incremental method correctly update the sample mean and the eigenbasis whereas existing incremental subspace update method ignore the fact the sample mean varies over time the tracking problem is formulated a a state inference problem within a markov chain monte carlo framework and a particle filter is incorporated for propagating sample distribution over time numerous experiment demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environment where the target object undergo large pose and lighting change 
we propose a new method for estimating intrinsic dimension of a dataset derived by applying the principle of maximum likelihood to the distance between close neighbor we derive the estimator by a poisson process approximation ass it bias and variance theoretically and by simulation and apply it to a number of simulated and real datasets we also show it ha the best overall performance compared with two other intrinsic dimension estimator 
using sql ha not been considered an efficient and feasible way to implement data mining algorithm although this is true for many data mining machine learning and statistical algorithm this work show it is feasible to get an efficient sql implementation of the well known k mean clustering algorithm that can work on top of a relational dbms the article emphasizes both correctness and performance from a correctness point of view the article explains how to compute euclidean distance nearest cluster query and updating clustering result in sql from a performance point of view it is explained how to cluster large data set defining and indexing table to store and retrieve intermediate and final result optimizing and avoiding join optimizing and simplifying clustering aggregation and taking advantage of sufficient statistic experiment evaluate scalability with synthetic data set varying size and dimensionality the proposed k mean implementation can cluster large data set and exhibit linear scalability 
the constraint classification framework capture many flavor of multiclass classification including winner take all multiclass classification multilabel classification and ranking we present a meta algorithm for learning in this framework that learns via a single linear classifier in high dimension we discus distribution independent a well a margin based generalization bound and present empirical and theoretical evidence showing that constraint classification benefit over existing method of multiclass classification 
using neural network to represent value function in reinforcement learning algorithm often involves a lot of work in hand crafting the network structure and tuning the learning parameter in this paper we explore the potential of using constructive neural network in reinforcement learning constructive neural network method are appealing because they can build the network structure based on the data that need to be represented to our knowledge such algorithm have not been used in reinforcement learning a major issue is that constructive algorithm often work in batch mode while many reinforcement learning algorithm work on line we use a cache to accumulate data then use a variant of cascade correlation to update the value function preliminary result on the game of tic tac toe show the potential of this new algorithm compared to using static feed forward neural network trained with backpropagation 
the mean algorithm is by far the most widely used method for discovering cluster in data we show how to accelerate it dramatically while still always computing exactly the same result a the standard algorithm the accelerated algorithm avoids unnecessary distance calculation by applying the triangle inequality in two different way and by keeping track of lower and upper bound for distance between point and center experiment show that the new algorithm is effective for datasets with up to dimension and becomes more and more effective a the number of cluster increase for it is many time faster than the best previously known accelerated mean method 
many real world graph have been shown to be scale free vertex degree follow power law distribution vertex tend to cluster and the average length of all shortest path is small we present a new model for understanding scale free network based on multilevel geodesic approximation using a new data structure called a multilevel mesh using this multilevel framework we propose a new kind of graph clustering for data reduction of very large graph system such a social biological or electronic network finally we apply our algorithm to real world social network and protein interaction graph to show that they can reveal knowledge embedded in underlying graph structure we also demonstrate how our data structure can be used to quickly answer approximate distance and shortest path query on scale free network 
in many application good ranking is a highly desirable performance for a classifier the criterion commonly used to measure the rank ing quality of a classification algorithm is the area under the roc curve auc to report it properly it is crucial to determine an interval of confidence for it value this paper provides confidence interval for the a uc based on a statistical and combinatorial analysis using only simple parameter such a the error rate and the number of positive and negative example the analysis is distribution independent it make no assumption about the distribution of the score of negative or positive example the result are of practical use and can be viewed a the equivalent for auc of the standard confidence interval given in the case of the error r ate they are compared with previous approach in several standard classification task demonstrating the benefit of our analysis 
abstract this paper introduces an algorithm for the automatic relevance determinationof input variable in kernelized support vector machine relevanceis measured by scale factor defining the input space metric andfeature selection is performed by assigning zero weight to irrelevantvariables the metric is automatically tuned by the minimization of thestandard svm empirical risk where scale factor are added to the usualset of parameter defining the classifier feature selection is 
the goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis to non traditional type of pattern and non binary data to that end we describe a framework for generalizing support that is based on the simple but useful observation that support can be viewed a the composition of two function a function that evaluates the strength or presence of a pattern in each object transaction and a function that summarizes these evaluation with a single number a key goal of any framework is to allow people to more easily express explore and communicate idea and hence we illustrate how our support framework can be used to describe support for a variety of commonly used association pattern such a frequent itemsets general boolean pattern and error tolerant itemsets we also present two example of the practical usefulness of generalized support one example show the usefulness of support function for continuous data another example show how the hyperclique pattern an association pattern originally defined for binary data can be extended to continuous data by generalizing a support function 
classification is a well established operation in text mining given a set of label a and a set da of training document tagged with these label a classifier learns to assign label to unlabeled test document suppose we also had available a different set of label b together with a set of document db marked with label from b if a and b have some semantic overlap can the availability of db help u build a better classifier for a and vice versa we answer this question in the affirmative by proposing cross training a new approach to semi supervised learning in presence of multiple label set we give distributional and discriminative algorithm for cross training and show through extensive experiment that cross training can discover and exploit probabilistic relation between two taxonomy for more accurate classification 
the goal of clustering is to identify distinct group in a dataset compared to non parametric clustering method like complete linkage hierarchical model based clustering ha the advantage of offering a way to estimate the number of group present in the data however it computational cost is quadratic in the number of item to be clustered and it is therefore not applicable to large problem we review an idea called fractionation originally conceived by cutting karger pedersen and tukey for non parametric hierarchical clustering of large datasets and describe an adaptation of fractionation to model based clustering a further extension called refractionation lead to a procedure that can be successful even in the difficult situation where there are large number of small group 
this paper present a novel discriminative learning technique for label sequence based on a combination of the two most successful learning algorithm support vector machine and hidden markov model which we call hidden markov support vector machine the proposed architecture handle dependency between neighboring label using viterbi decoding in contrast to standard hmm training the learning procedure is discriminative and is based on a maximum soft margin criterion compared to previous method like conditional random field maximum entropy markov model and label sequence boosting hm svms have a number of advantage most notably it is possible to learn non linear discriminant function via kernel function at the same time hm svms share the key advantage with other discriminative method in particular the capability to deal with overlapping feature we report experimental evaluation on two task named entity recognition and part of speech tagging that demonstrate the competitiveness of the proposed approach 
we consider the problem of illusory or artefactual structure from the visualisation of high dimensional structureless data in particular we examine the role of the distance metric in the use of topographic mapping based on the statistical field of multidimensional scaling we show that the use of a squared euclidean metric i e the sstress measure give rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution and we provide a theoretical justification for this observation 
kernel based learning algorithm work by embedding the data into a euclidean space and then searching for linear relation among the embedded data point the embedding is performed implicitly by specifying the inner product between each pair of point in the embedding space this information is contained in the so called kernel matrix a symmetric and positive definite matrix that encodes the relative position of all point specifying this matrix amount to specifying the geometry of the embedding space and inducing a notion of similarity in the input space classical model selection problem in machine learning in this paper we show how the kernel matrix can be learned from data via semi definite programming sdp technique when applied to a kernel matrix associated with both training and test data this give a powerful transductive algorithm using the labelled part of the data one can learn an embedding also for the unlabelled part the similarity between test point is inferred from training point and their label importantly these learning problem are convex so we obtain a method for learning both the model class and the function without local minimum furthermore this approach lead directly to a convex method to learn the norm soft margin parameter in support vector machine solving another important open problem finally the novel approach presented in the paper is supported by positive empirical result 
we discus an identification framework for noisy speech mixture a block based generative model is formulated that explicitly incorporates the time varying harmonic plus noise h n model for a number of latent source observed through noisy convolutive mixture all parameter including the pitch of the source signal the amplitude and phase of the source the mixing filter and the noise statistic are estimated by maximum likelihood using an em algorithm exact averaging over the hidden source is obtained using the kalman smoother we show that pitch estimation and source separation can be performed simultaneously the pitch estimate are compared to laryngograph egg measurement artificial and real room mixture are used to demonstrate the viability of the approach intelligible speech signal are re synthesized from the estimated h n model 
we describe how we used a data set of chorale harmonisation composed by johann sebastian bach to train hidden markov model using a probabilistic framework allows u to create a harmonisation system which learns from example and which can compose new harmonisation we make a quantitative comparison of our system s harmonisati on performance against simpler model and provide example harmonisation 
the performance of graph based clustering method critically depends on the quality of the distance function used to compute similarity between pair of neighboring node in this paper we learn distance function by training binary classifier with margin the classifier are defined over the product space of pair of point and are trained to distinguish whether two point come from the same class or not the signed margin is used a the distance value our main contribution is a distance learning method distboost which combine boosting hypothesis over the product space with a weak learner based on partitioning the original feature space each weak hypothesis is a gaussian mixture model computed using a semi supervised constrained em algorithm which is trained using both unlabeled and labeled data we also consider svm and decision tree boosting a margin based classifier in the product space we experimentally compare the margin based distance function with other existing metric learning method and with existing technique for the direct incorporation of constraint into various clustering algorithm clustering performance is measured on some benchmark database from the uci repository a sample from the mnist database and a data set of color image of animal in most case the distboost algorithm significantly and robustly outperformed it competitor 
in this paper we study market share rule rule that have a certain market share statistic associated with them such rule are particularly relevant for decision making from a business perspective motivated by market share rule in this paper we consider statistical quantitative rule sq rule that are quantitative rule in which the rh can be any statistic that is computed for the segment satisfying the lh of the rule building on prior work we present a statistical approach for learning all significant sq rule i e sq rule for which a desired statistic lie outside a confidence interval computed for this rule in particular we show how resampling technique can be effectively used to learn significant rule since our method considers the significance of a large number of rule in parallel it is susceptible to learning a certain number of false rule to address this we present a technique that can determine the number of significant sq rule that can be expected by chance alone and suggest that this number can be used to determine a false discovery rate for the learning procedure we apply our method to online consumer purchase data and report the result 
in this paper we propose a novel data mining technique for the efficient damage detection within the large scale complex mechanical structure every mechanical structure is defined by the set of finite element that are called structure element large scale complex structure may have extremely large number of structure element and predicting the failure in every single element using the original set of natural frequency a feature is exceptionally time consuming task traditional data mining technique simply predict failure in each structure element individually using global prediction model that are built considering all data record in order to reduce the time complexity of these model we propose a localized clustering regression based approach that consists of two phase building a local cluster around a data record of interest and predicting an intensity of damage only in those structure element that correspond to data record from the built cluster for each test data record we first build a cluster of data record from training data around it then for each data record that belongs to discovered cluster we identify corresponding structure element and we build a localized regression model for each of these structure element these regression model for specific structure element are constructed using only a specific set of relevant natural frequency and merely those data record that correspond to the failure of that structure element experiment performed on the problem of damage prediction in a large electric transmission tower frame indicate that the proposed localized clustering regression based approach is significantly more accurate and more computationally efficient than our previous hierarchical clustering approach a well a global prediction model 
we develop and test new machine learning method for the prediction of topological representation of protein structure in the form of coarseor ne grained contact or distance map that are translation and rotation invariant the method are based on generalized input output hidden markov model giohmms and generalized recursive neural network grnns the method are used to predict topology directly in the ne grained case and in the coarsegrained case indirectly by rst learning how to score candidate graph and then using the scoring function to search the space of possible congurations computer simulation show that the predictor achieve state of the art performance 
neuron are often assumed to operate in a highly unreliable manner a neuron can signal the same stimulus with a variable number of action potential however much of the experimental evidence supporting this view wa obtained in the visual cortex we have therefore assessed trial to trial variability in the auditory cortex of the rat to ensure single unit isolation we used cell attached recording tone evoked response were usually transient often consisting of on average only a single spike per stimulus surprisingly the majority of response were not just transient but were also binary consisting of or action potential but not more in response to each stimulus several dramatic example consisted of exactly one spike on of trial with no trial to trial variability in spike count the variability of such binary response differs from comparably transient response recorded in visual cortical area such a area mt and represent the lowest trial to trial variability mathematically possible for response of a given firing rate our study thus establishes for the first time that transient response in auditory cortex can be described a a binary process rather than a a highly variable poisson process these result demonstrate that cortical architecture can support a more precise control of spike number than wa previously recognized and they suggest a re evaluation of model of cortical processing that assume noisiness to be an inevitable feature of cortical code 
we provide a worst case analysis of selective sampling algorithm for learning linear threshold function the algorithm considered in this paper are perceptron like algorithm i e algorithm which can be efficiently run in any reproducing kernel hilbert space our algorithm exploit a simple margin based randomized rule to decide whether to query the current label we obtain selective sampling algorithm achieving on average the same bound a those proven for their deterministic counterpart but using much fewer label we complement our theoretical finding with an empirical comparison on two text categorization task the outcome of these experiment is largely predicted by our theoretical result our selective sampling algorithm tend to perform a good a the algorithm receiving the true label after each classification while observing in practice substantially fewer label 
relativized option combine model minimization method and a hierarchical reinforcement learning framework to derive compact reduced representation of a related family of task relativized option are dened without an absolute frame of reference and an option s policy is transformed suitably based on the circumstance under which the option is invoked in earlier work we addressed the issue of learning the option policy online in this article we develop an algorithm for choosing from among a set of candidate transformation the right transformation for each member of the family of task 
we propose a new interpretation of spiking neuron a bayesian integrator accumulating evidence over time about event in the external world or the body and communicating to other neuron their certainty about these event in this model spike signal the occurrence of new information i e what cannot be predicted from the past activity a a result firing statistic are close to poisson albeit providing a deterministic representation of probability we proceed to develop a theory of bayesian inference in spiking neural network recurrent interaction implementing a variant of belief propagation 
in this paper we present a novel algorithm opportune project for mining complete set of frequent item set by projecting database to grow a frequent item set tree our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structure array based or tree based to represent projected transaction subset and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to feature of the subset more importantly we propose novel method to build tree based pseudo projection and array based unfiltered projection for projected transaction subset which make our algorithm both cpu time efficient and memory saving basically the algorithm grows the frequent item set tree by depth first search whereas breadth first search is used to build the upper portion of the tree if necessary we test our algorithm versus several other algorithm on real world datasets such a bm po and on ibm artificial datasets the empirical result show that our algorithm is not only the most efficient on both sparse and dense database at all level of support threshold but also highly scalable to very large database 
the computation and memory required for kernel machine with n training sample is at least o n such a complexity is significant even for moderate size problem and is prohibitive for large datasets we present an approximation technique based on the improved fast gauss transform to reduce the computation to o n we also give an error bound for the approximation and provide experimental result on the uci datasets 
in this paper we present a generative latent variable model for rating based collaborative ltering called the user rating prole model urp the generative process which underlies urp is designed to produce complete user rating prole an assignment of one rating to each item for each user our model represents each user a a mixture of user attitude and the mixing proportion are distributed according to a dirichlet random variable the rating for each item is generated by selecting a user attitude for the item and then selecting a rating according to the preference pattern associated with that attitude urp is related to several model including a multinomial mixture model the aspect model and lda but ha clear advantage over each 
we argue that human inductive generalization is best explained in a bayesian framework rather than by traditional model based on similarity computation we go beyond previous work on bayesian concept learning by introducing an unsupervised method for constructing flexible hypothesis space and we propose a version of the bayesian occam s razor that trade off prior and likelihood to prevent underor over generalization in these flexible space we analyze two published data set on inductive reasoning a well a the result of a new behavioral study that we have carried out 
in this paper we will show that a restricted class of constrained minimum divergence problem named generalized inference problem can be solved by approximating the kl divergence with a bethe free energy the algorithm we derive is closely related to both loopy belief propagation and iterative scaling this unified propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraint are present experiment show the viability of our algorithm 
given is a problem sequence and a probability distribution the bias onprograms computing solution candidate we present an optimally fastway of incrementally solving each task in the sequence bias shift arecomputed by program prefix that modify the distribution on their suffixesby reusing successful code for previous task stored in non modifiablememory no tested program get more runtime than it probabilitytimes the total search time in illustrative experiment ours 
adaboost ha proved to be an eective method to improve the performance of base classifier both theoretically and empirically however previous study have shown that adaboost might suer from the overfitting problem especially for noisy data in addition most current work on boosting assumes that the combination weight are fixed constant and therefore doe not take particular input pattern into consideration in this paper we present a new boosting algorithm weightboost which try to solve these two problem by introducing an inputdependent regularization factor to the combination weight similarly to adaboost we derive a learning procedure for weightboost which is guaranteed to minimize training error empirical study on eight dierent uci data set and one text categorization data set show that weightboost almost always achieves a considerably better classification accuracy than adaboost furthermore experiment on data with artificially controlled noise indicate that the weightboost is more robust to noise than adaboost 
what happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data in a bayesian model learning framework the answer depends on the prior ex pectations of the dynamic of the model parameter that is to be inferred from the data local time constraint on the prior are insufficient to pick one interpretation over another on the other hand nonlocal time constraint induced by a f noise spectrum of the prior is shown to permit learning of a specific model parameter even when there are in finitely many equally plausible interpretation of the data this transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system allowing the use of powerful statisti cal mechanical method to uncover the transition from indeterminate to determinate model learning 
unexpected rule are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest in this paper we study three important issue that have been previously ignored in mining unexpected rule first the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario in addition to the knowledge itself second the prior knowledge should be considered right from the start to focus the search on unexpected rule third the unexpectedness of a rule depends on what other rule the user ha seen so far thus only rule that remain unexpected given what the user ha seen should be considered interesting we develop an approach that address all three problem above and evaluate it by mean of experiment focusing on finding interesting rule 
finding informative gene from microarray data is an important research problem in bioinformatics research and application most of the existing method rank feature according to their discriminative capability and then find a subset of discriminative gene usually top k gene in particular t statistic criterion and it variant have been adopted extensively this kind of method rely on the statistic principle of t test which requires that the data follows a normal distribution however according to our investigation the normality condition often cannot be met in real data set to avoid the assumption of the normality condition in this paper we propose a rank sum test method for informative gene discovery the method us a rank sum statistic a the ranking criterion moreover we propose using the significance level threshold instead of the number of informative gene a the parameter the significance level threshold a a parameter carry the quality specification in statistic we follow the pitman efficiency theory to show that the rank sum method is more accurate and more robust than the t statistic method in theory to verify the effectiveness of the rank sum method we use support vector machine svm to construct classifier based on the identified informative gene on two well known data set namely colon data and leukemia data the prediction accuracy reach on the colon data and on the leukemia data the result are clearly better than those from the previous feature ranking method by experiment we also verify that using significance level threshold is more effective than directly specifying an arbitrary k 
abstract one of the central problem in machine learning and pattern recognitionis to develop appropriate representation for complex data weconsider the problem of constructing a representation for data lyingon a low dimensional manifold embedded in a high dimensional space 
we propose an unsupervised methodology using independent component analysis ica to cluster gene from dna microarray data based on an ica mixture model of genomic expression pattern linear and nonlinear ica find component that are specific to certain biological process gene that exhibit significant up regulation or down regulation within each component are grouped into cluster we test the statistical significance of enrichment of gene annotation within each cluster ica based clustering outperformed other leading method in constructing functionally coherent cluster on various datasets this result support our model of genomic expression data a composite effect of independent biological process comparison of clustering performance among various ica algorithm including a kernel based nonlinear ica algorithm show that nonlinear ica performed the best for small datasets and natural gradient maximization likelihood worked well for all the datasets 
learning easily un derstandable decision rule from example is one of the c lassic problem in machine learning most l earning system for this problem employ some variation of a greedy separate and conquer algorithm which make the rule order dependent and hence difficult t o understand in this paper we describe a system called lerils that l earns highly accurate and comprehensible rule from example using a randomized iterative local search we c ompare it performance to c ripper cn g net smog and brutedl and show that it compare favorably in accuracy and simplicity of hypothesis in a number of domain 
while classical kernel based classifier are based on a single kernel in practice it is often desirable to base classifier on combination of multiple kernel lanckriet et al considered conic combination of kernel matrix for the support vector machine svm and showed that the optimization of the coefficient of such a combination reduces to a convex optimization problem known a a quadratically constrained quadratic program qcqp unfortunately current convex optimization toolbox can solve this problem only for a small number of kernel and a small number of data point moreover the sequential minimal optimization smo technique that are essential in large scale implementation of the svm cannot be applied because the cost function is non differentiable we propose a novel dual formulation of the qcqp a a second order cone programming problem and show how to exploit the technique of moreau yosida regularization to yield a formulation to which smo technique can be applied we present experimental result that show that our smo based algorithm is significantly more efficient than the general purpose interior point method available in current optimization toolbox 
support vector machine svms excel at two class discriminative learning problem they often outperform generative classifier especially those that use inaccurate generative model such a the na ve bayes nb classifier on the other hand generative classifier have no trouble in handling an arbitrary number of class efficiently and nb classifier train much faster than svms owing to their extreme simplicity in contrast svms handle multi class problem by learning redundant yes no one v others classifier for each class further worsening the performance gap we propose a new technique for multi way classification which exploit the accuracy of svms and the speed of nb classifier we first use a nb classifier to quickly compute a confusion matrix which is used to reduce the number and complexity of the two class svms that are built in the second stage during testing we first get the prediction of a nb classifier and use that to selectively apply only a subset of the two class svms on standard benchmark our algorithm is to time faster than svms and yet match or even exceeds their accuracy 
a major issue in evaluating speech enhancement and hearing compensation algorithm is to come up with a suitable metric that predicts intelligibility a judged by a human listener previous method such a the widely used speech transmission index sti fail to account for masking effect that arise from the highly nonlinear cochlear transfer function we therefore propose a neural articulation index nai that estimate speech intelligibility from the instantaneous neural spike rate over time produced when a signal is processed by an auditory neural model by using a well developed model of the auditory periphery and detection theory we show that human perceptual discrimination closely match the modeled distortion in the instantaneous spike rate of the auditory nerve in highly rippled frequency transfer condition the nai s prediction error is versus the sti s prediction error of 
online mechanism design omd address the problem of sequential decision making in a stochastic environment with multiple self interested agent the goal in omd is to make value maximizing decision despite this self interest in previous work we presented a markov decision process mdp based approach to omd in large scale problem domain in practice the underlying mdp needed to solve omd is too large and hence the mechanism must consider approximation this raise the possibility that agent may be able to exploit the approximation for selfish gain we adopt sparse sampling based mdp algorithm to implement efficient policy and retain truth revelation a an approximate bayesiannash equilibrium our approach is empirically illustrated in the context of the dynamic allocation of wifi connectivity to user in a coffeehouse 
inspired by event ranging from to the collapse of the accounting firm arthur ander sen economist kunreuther and heal recently introduced an interesting game theoretic model for problem of interdependent security id in which a large number of player must make individual investment decision related to security whether physical finan cial medical or some other type but in which the ultimate safety of each participant may depend in a complex way on the action of the entire population a simple example is the choice of whether to install a fire sprinkler system in an individual condominium in a large building while such a system might greatly reduce the chance of the owner s prop erty being destroyed by a fire originating within their own unit it might do little or nothing to reduce the chance of damage caused by fire originating in other unit since sprinkler can usually only douse small fire early if enough other unit owner have not made the investment in sprinkler it may be not cost effective for any individual to do so 
learning algorithm from the field of artificial neural network and machine learning typically do not take any cost into account or allow only cost depending on the class of the example that are used for learning a an extension of class dependent cost we consider cost that are example i e feature and class dependent we derive a costsensitive perceptron learning rule for nonseparable class that can be extended to multi modal class dipol we also derive aa approach for including example dependent cost into an arbitrary cost insensitive learning algorithm by sampling according to roodified probability distribution 
cortical neuron in vivo show fluctuation in their membrane potential of the order of several milli volt using simple and biophysically realistic model of a single neuron we demonstrate that noise induced fluctuation can be used to adaptively optimize the sensitivity of the neuron s output to ensemble of subthreshold input of different average strength optimal information transfer is achieved by changing the strength of the noise such that the neuron s average firing rate remains constant adaptation is fast because only crude estimate of the output rate are required at any time 
attribute interaction are the irreducible dependency between attribute interaction underlie feature relevance and selection the structure of joint probability and classification model if and only if the attribute interact they should be connected while the issue of way interaction especially of those between an attribute and the label ha already been addressed we introduce an operational definition of a generalized n way interaction by highlighting two model the reductionistic part to whole approximation where the model of the whole is reconstructed from model of the part and the holistic reference model where the whole is modelled directly an interaction is deemed significant if these two model are significantly different in this paper we propose the kirkwood superposition approximation for constructing part to whole approximation to model data we do not assume a particular structure of interaction but instead construct the model by testing for the presence of interaction the resulting map of significant interaction is a graphical model learned from the data we confirm that the p value computed with the assumption of the asymptotic x distribution closely match those obtained with the boot strap 
we present an automatic alignment procedure which map the disparate internal representation learned by several local dimensionality reduction expert into a single coherent global coordinate system for the original data space our algorithm can be applied to any set of expert each of which produce a low dimensional local representation of a high dimensional input unlike recent effort to coordinate such model by modifying their objective function our algorithm is invoked after training and applies an efficient eigensolver to post process the trained model the post processing ha no local optimum and the size of the sys tem it must solve scale with the number of local model rather than the number of original data point making it more efficient than model free algorithm such a isomap or lle 
this paper describes a system that can annotate a video sequence with a description of the appearance of each actor when the actor is in view and a representation of the actor s activity while in view the system doe not require a fixed background and is automatic the system work by tracking people in d and then using an annotated motion capture dataset synthesizing an annotated d motion sequence matching the d track the d motion capture data is manually annotated off line using a class structure that describes everyday motion and allows motion annotation to be composed one may jump while running for example description computed from video of real motion show that the method is accurate 
we study a class of overrelaxed bound optimization algorithm and their relationship to standard bound optimizers such a expectationmaximization iterative scaling cccp and non negative matrix factorization we provide a theoretical analysis of the convergence property of these optimizers and identify analytic condition under which they are expected to outperform the standard version based on this analysis we propose a novel simple adaptive overrelaxed scheme for practical optimization and report empirical result on several synthetic and real world data set showing that these new adaptive method exhibit superior performance in certain case by several time speedup compared to their traditional counterpart our extension are simple to implement apply to a wide variety of algorithm almost always give a substantial speedup and do not require any theoretical analysis of the underlying algorithm 
predictive state representation psrs use prediction of a set of test to represent the state of controlled dynamical system one reason why this representation is exciting a an alternative to partially observable markov decision process pomdps is that psr model of dynamical system may be much more compact than pomdp model empirical work on psrs to date ha focused on linear psrs which have not allowed for compression relative to pomdps we introduce a new notion of test which allows u to define a new type of psr that is nonlinear in general and allows for exponential compression in some deterministic dynamical system these new test called e test are related to the test used by rivest and schapire in their work with the diversity representation but our psr avoids some of the pitfall of their representation in particular it potential to be exponentially larger than the equivalent pomdp 
we present a generalization of the nonnegative matrix factorization nmf where a multilayer generative network with nonnegative weight is used to approximate the observed nonnegative data the multilayer generative network with nonnegativity constraint is learned by a multiplicative uppropagation algorithm where the weight in each layer are updated in a multiplicative fashion while the mismatch ratio is propagated from the bottom to the top layer the monotonic convergence of the multiplicative up propagation algorithm is shown in contrast to nmf the multiplicative uppropagation is an algorithm that can learn hierarchical representation where complex higher level representation are defined in term of le complex lower level representation the interesting behavior of our algorithm is demonstrated with face image data 
the paper is concerned with two class active learning while the common approach for collecting data in active learning is to select sample close to the classification boundary better performance can be achieved by taking into account the prior data distribution the main contribution of the paper is a formal framework that incorporates clustering into active learning the algorithm first construct a classifier on the set of the cluster representative and then propagates the classification decision to the other sample via a local noise model the proposed model allows to select the most representative sample a well a to avoid repeatedly labeling sample in the same cluster during the active learning process the clustering is adjusted using the coarse to fine strategy in order to balance between the advantage of large cluster and the accuracy of the data representation the result of experiment in image database show a better performance of our algorithm compared to the current method 
model induction from relational data requires aggregation of the value of attribute of related entity this paper make three contribution to the study of relational learning it present a hierarchy of relational concept of increasing complexity using relational schema characteristic such a cardinality and derives class of aggregation operator that are needed to learn these concept expanding one level of the hierarchy it introduces new aggregation operator that model the distribution of the value to be aggregated and for classification problem the difference in these distribution by class it demonstrates empirically on a noisy business domain that more complex aggregation method can increase generalization performance constructing feature using target dependent aggregation can transform relational prediction task so that well understood feature vector based modeling algorithm can be applied successfully 
two notion of optimality have been explored in previous work on hierarchical reinforcement learning hrl hierarchical optimality or the optimal policy in the space dened by a task hierarchy and a weaker local model called recursive optimality in this paper we introduce two new average reward hrl algorithm for nding hierarchically optimal policy we compare them to our previously reported algorithm for computing recursively optimal policy using a grid world taxi problem and a more real world agv scheduling problem the new algorithm are based on a three part value function decomposition proposed recently by andre and russell which generalizes dietterich s maxq value function decomposition a key dierence between the algorithm proposed in this paper and our previous work is that there is only a single global gain average reward instead of a gain for each subtask our result show the new average reward algorithm have better performance than both the previous recursively optimal counterpart a well a the corresponding discounted hierarchical optimal algorithm 
we apply the message from monte carlo mmc algorithm to inference of univariate polynomial mmc is an algorithm for point estimation from a bayesian posterior sample it partition the posterior sample into set of region that contain similar model each region ha an associated message length given by dowe s mmld approximation and a point estimate that is representative of model in the region the region and point estimate are chosen so that the kullbackleibler distance between model in the region and the associated point estimate is small using wallace s fsmml boundary rule we compare the mmc algorithm s point estimation performance with minimum message length and structural risk minimisation on a set of ten polynomial and nonpolynomial function with gaussian noise the orthonormal polynomial parameter are sampled using reversible jump markov chain monte carlo method 
in this paper we show how the generation of document can be thought of a a k stage markov process which lead to a fisher kernel from which the n gram and string kernel can be re constructed the fisher kernel view give a more flexible insight into the string kernel and suggests how it can be parametrised in a way that reflects the statistic of the training corpus furthermore the probabilistic modelling approach suggests extending the markov process to consider sub sequence of varying length rather than the standard fixed length approach used in the string kernel we give a procedure for determining which sub sequence are informative feature and hence generate a finite state machine model which can again be used to obtain a fisher kernel by adjusting the parametrisation we can also influence the weighting received by the feature in this way we are able to obtain a logarithmic weighting in a fisher kernel finally experiment are reported comparing the dierent kernel using the standard bag of word kernel a a baseline 
although the study of clustering is centered around an intuitively compelling goal it ha been very dicult to develop a unied framework for reasoning about it at a technical level and profoundly diverse approach to clustering abound in the research community here we suggest a formal perspective on the dicult y in nding such a unication in the form of an impossibility theorem for a set of three simple property we show that there is no clustering function satisfying all three relaxation of these property expose some of the interesting and unavoidable trade o at work in well studied clustering technique such a single linkage sum of pair k mean and k median 
side chainpredictionisanimportantsubtaskintheprotein folding problem we show that flnding a minimal energy side chain conflguration is equivalent to performing inference in an undirected graphical model the graphical model is relatively sparse yet ha manycycles weusedthisequivalencetoassesstheperformanceof approximate inference algorithm in a real world setting speciflcallywecomparedbeliefpropagation bp generalizedbp gbp and naive mean fleld mf in case where exact inference wa possible max product bp always found the global minimum of the energy except in few case where it failed to converge while other approximation algorithm of similar complexity did not in the full protein data set maxproduct bp always found a lower energy conflguration than the other algorithm including a widely used protein folding software scwrl 
abstract repeated spike pattern have often been taken a evidence for the synfire chain a phenomenon that a stable spike synchrony propagates through a feedforward network inter spike interval which represent a repeated spike pattern are influenced by the propagation speed of a spike packet however the relation between the propagation speed and network structure is not well understood while it is apparent that the propagation speed depends on the excitatory synapse strength it might also be related to spike pattern we analyze a feedforward network with mexican hattype connectivity fmh using the fokker planck equation we show that both a uniform and a localized spike packet are stable in the fmh in a certain parameter region we also demonstrate that the propagation speed depends on the distinct firing pattern in the same network 
this paper describes a logical extension to microsoft business framework mbf called analytical view av av consists of three component model service for design time business intelligence entity bie for programming model and intelldrill for runtime navigation between oltp and olap data source av feature set fulfills enterprise application requirement for analysis and decision support complementing the transactional feature set currently provided by mbf model service automatically transforms an object oriented model transactional view to a multi dimensional model analytical view without the traditional extraction transformation loading etl overhead and complexity it infers dimensionality from the object layer where richer metadata is stored eliminating the guesswork that a traditional data warehousing process requires when going through physical database schema bi entity are class code generated by model service a an intrinsic part of the framework bi entity enable a consistent object oriented way of programming model with strong type and rich semantics for olap similar to what mbf object persistence technology doe for oltp data more importantly data contained in bi entity have a higher degree of application awareness such a the integrated application level security and customizability intellidrill link together all the information island in mbf using metadata because of the automatic transformation from transactional view to analytical view enabled by model service we have the ability to understand natively what kind of drill ability an object would have thus making information navigation in mbf fully discover able with built in ontology 
recent multi agent extension of q learning require knowledge of other agent payoff and q function and assume game theoretic play at all time by all other agent this paper proposes a fundamentally different approach dubbed hyper q learning in which value of mixed strategy rather than base action are learned and in which other agent strategy are estimated from observed action via bayesian inference hyper q may be effective against many different type of adaptive agent even if they are persistently dynamic against certain broad category of adaptation it is argued that hyper q may converge to exact optimal time varying policy in test using rock paper scissors hyper q learns to significantly exploit an infinitesimal gradient ascent iga player a well a a policy hill climber phc player preliminary analysis of hyper q against itself is also presented 
shaping can be an effective method for improving the learning rate in reinforcement system previously shaping ha been heuristically motivated and implemented we provide a formal structure with which to interpret the improvement afforded by shaping reward central to our model is the idea of a reward horizon which focus exploration on an mdp s critical region a subset of state with the property that any policy that performs well on the critical region also performs well on the mdp we provide a simple algorithm and prove that it learning time is polynomial in the size of the critical region and crucially independent of the size of the mdp this identifies low reward horizon with easy to learn mdps shaping reward which encode our prior knowledge about the relative merit of decision can be seen a artificially reducing the mdp s natural reward horizon we demonstrate empirically the effect of using shaping to reduce the reward horizon 
while there is a lot of empirical evidence showing that traditional rule learning approach work well in practice it is nearly impossible to derive analytical result about their predictive accuracy in this paper we investigate rule learning from a theoretical perspective we show that the application of mcallester s pac bayesian bound to rule learning yield a practical learning algorithm which is based on ensemble of weighted rule set experiment with the resulting learning algorithm show not only that it is competitive with state of the art rule learner but also that it error rate can often be bounded tightly in fact the bound turn out to be tighter than one of the best bound for a practical learning scheme known so far the set covering machine finally we prove that the bound can be further improved by allowing the learner to abstain from uncertain prediction 
predicting the next request of a user a she visit web page ha gained importance a web based activity increase markov model and their variation or model based on sequence mining have been found well suited for this problem however higher order markov model are extremely complicated due to their large number of state whereas lower order markov model do not capture the entire behavior of a user in a session the model that are based on sequential pattern mining only consider the frequent sequence in the data set making it difficult to predict the next request following a page that is not in the sequential pattern furthermore it is hard to find model for mining two different kind of information of a user session we propose a new model that considers both the order information of page in a session and the time spent on them we cluster user session based on their pair wise similarity and represent the resulting cluster by a click stream tree the new user session is then assigned to a cluster based on a similarity measure the click stream tree of that cluster is used to generate the recommendation set the model can be used a part of a cache prefetching system a well a a recommendation model 
query by committee is an effective approach to selective sampling in which disagreement amongst an ensemble of hypothesis is used to select data for labeling query by bagging and query by boosting are two practical implementation of this approach that use bagging and boosting respectively to build the committee for effective active learning it is critical that the committee be made up of consistent hypothesis that are very different from each other decorate is a recently developed method that directly construct such diverse committee using artificial training data this paper introduces active decorate which us decorate committee to select good training example extensive experimental result demonstrate that in general active decorate outperforms both query by bagging and query by boosting 
we have designed and tested a single chip analog vlsi sensor that detects imminent collision by measuring radially expansive optic flow the design of the chip is based on a model proposed to explain leg extension behavior in fly during landing approach a new elementary motion detector emd circuit wa developed to measure optic flow this emd circuit model the bandpass nature of large monopolar cell lmcs immediately postsynaptic to photoreceptors in the fly visual system a array of d motion detector wa fabricated on a mm mm die in a standard m cmos process the chip consumes w of power from a v supply with the addition of wide angle optic the sensor is able to detect collision around m before impact in complex real world scene 
in a wide range of business area dealing with text data stream including crm knowledge management and web monitoring service it is an important issue to discover topic trend and analyze their dynamic in real time specifically we consider the following three task in topic trend analysis topic structure identification identifying what kind of main topic exist and how important they are topic emergence detection detecting the emergence of a new topic and recognizing how it grows topic characterization identifying the characteristic for each of main topic for real topic analysis system we may require that these three task be performed in an on line fashion rather than in a retrospective way and be dealt with in a single framework this paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically in this framework we propose the usage of a time stamp based discounting learning algorithm in order to realize real time topic structure identification this enables tracking the topic structure adaptively by forgetting out of date statistic further we apply the theory of dynamic model selection to detecting change of main component in the finite mixture model in order to realize topic emergence detection we demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamic of topic trend in a timely fashion 
regularization play a central role in the analysis of modern data where non regularized fitting is likely to lead to over fitted model useless for both prediction and interpretation we consider the design of incremental algorithm which follow path of regularized solution a the regularization varies these approach often result in method which are both efficient and highly flexible we suggest a general path following algorithm based on second order approximation prove that under mild condition it remains very close to the path of optimal solution and illustrate it with example 
we provide a principle for semi supervised learning based on optimizing the rate of communicating label for unlabeled point with side information the side information is expressed in term of identity of set of point or region with the purpose of biasing the label in each region to be the same the resulting regularization objective is convex ha a unique solution and the solution can be found with a pair of local propagation operation on graph induced by the region we analyze the property of the algorithm and demonstrate it performance on document classification task 
the primary goal of web usage mining is the discovery of pattern in the navigational behavior of web user standard approach such a clustering of user session and discovering association rule or frequent navigational path do not generally provide the ability to automatically characterize or quantify the unobservable factor that lead to common navigational pattern it is therefore necessary to develop technique that can automatically discover hidden semantic relationship among user a well a between user and web object probabilistic latent semantic analysis plsa is particularly useful in this context since it can uncover latent semantic association among user and page based on the co occurrence pattern of these page in user session in this paper we develop a unified framework for the discovery and analysis of web navigational pattern based on plsa we show the flexibility of this framework in characterizing various relationship among user and web object since these relationship are measured in term of probability we are able to use probabilistic inference to perform a variety of analysis task such a user segmentation page classification a well a predictive task such a collaborative recommendation we demonstrate the effectiveness of our approach through experiment performed on real world data set 
we show that anomaly detection can be interpreted a a binary classification problem using this interpretation we propose a support vector machine svm for anomaly detection we then present some theoretical result which include consistency and learning rate finally we experimentally compare our svm with the standard one class svm 
early disease outbreak detection system typically monitor health care data for irregularity by comparing the distribution of recent data against a baseline distribution determining the baseline is difficult due to the presence of different trend in health care data such a trend caused by the day of week and by seasonal variation in temperature and weather creating the baseline distribution without taking these trend into account can lead to unacceptably high false positive count and slow detection time this paper replaces the baseline method of wong et al with a bayesian network which produce the baseline distribution by taking the joint distribution of the data and conditioning on attribute that are responsible for the trend we show that our algorithm called wsare is able to detect outbreak in simulated data with almost the earliest possible detection time while keeping a low false positive count we also include the result of running wsare on real emergency department data 
we develop a protocol for optimizing dynamic behavior of a network of simple electronic component such a a sensor network an ad hoc network of mobile device or a network of communication switch this protocol requires only local communication and simple computation which are distributed among device the protocol is scalable to large network a a motivating example we discus a problem involving optimization of power consumption delay and buffer overflow in a sensor network our approach build on policy gradient method for optimization of markov decision process the protocol can be viewed a an extension of policy gradient method to a context involving a team of agent optimizing aggregate performance through asynchronous distributed communication and computation we establish that the dynamic of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective this paper is motivated by the potential of policy gradient method a a general approach to designing simple scalable distributed optimization protocol for network of electronic device we offer a general framework for such protocol that build on idea from the policy gradient literature we also explore a specific example involving a network of sensor that aggregate data in this context we propose a distributed optimization protocol that minimizes power consumption delay and buffer overflow the proposed approach for designing protocol based on policy gradient method comprises one contribution of this paper in addition this paper offer fundamental contribution to the policy gradient literature in particular the kind of protocol we propose can be viewed a extending policy gradient method to a context involving a team of agent optimizing system behavior through asynchronous distributed computation and parsimonious local communication our main theoretical contribution is to show that the dynamic of our protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective 
traditionally data quality program have acted a a preprocessing stage to make data suitable for a data mining or analysis operation recently data quality concept have been applied to database that support business operation such a provisioning and billing incorporating business rule that drive operation and their associated data process is critically important to the success of such project however there are many practical complication for example documentation on business rule is often meager rule change frequently domain knowledge is often fragmented across expert and those expert do not always agree typically rule have to be gathered from subject matter expert iteratively and are discovered out of logical or procedural sequence like a jigsaw puzzle our approach is to impement business rule a constraint on data in a classical expert system formalism sometimes called production rule our system work by allowing good data to pas through a system of constraint unchecked bad data violate constraint and are flagged and then fed back after correction constraint are added incrementally a better understanding of the business rule is gained we include a real life case study 
assume a uniform multidimensional grid of bivariate data where each cell of the grid ha a count ci and a baseline bi our goal is to find spatial region d dimensional rectangle where the ci are significantly higher than expected given bi we focus on two application detection of cluster of disease case from epidemiological data emergency department visit over the counter drug sale and discovery of region of increased brain activity corresponding to given cognitive task from fmri data each of these problem can be solved using a spatial scan statistic kulldorff where we compute the maximum of a likelihood ratio statistic over all spatial region and find the significance of this region by randomization however computing the scan statistic for all spatial region is generally computationally infeasible so we introduce a novel fast spatial scan algorithm generalizing the d scan algorithm of neill and moore to arbitrary dimension our new multidimensional multiresolution algorithm allows u to find spatial cluster up to x faster than the naive spatial scan without any loss of accuracy 
abstract auc area under the curve of roc receiveroperating characteristic ha beenrecently used a a measure for ranking performanceof learning algorithm in this paper we present a novel probability estimationalgorithm that improves the auc valueof decision tree instead of estimating theprobability at the single leaf where the examplefalls into our method average probabilityestimates from all leaf of the tree 
we propose a simple novel and yet effective method for building and testing decision tree that minimizes the sum of the misclassification and test cost more specifically we first put forward an original and simple splitting criterion for attribute selection in tree building our tree building algorithm ha many desirable property for a cost sensitive learning system that must account for both type of cost then assuming that the test case may have a large number of missing value we design several intelligent test strategy that can suggest way of obtaining the missing value at a cost in order to minimize the total cost we experimentally compare these strategy and c and demonstrate that our new algorithm significantly outperform c and it variation in addition our algorithm s complexity is similar to that of c and is much lower than that of previous work our work is useful for many diagnostic task which must factor in the misclassification and test cost for obtaining missing information 
inventory of manually compiled dictionary usually serve a a source for word sens however they often include many rare sens while missing corpus domain specific sens we present a clustering algorithm called cbc clustering by committee that automatically discovers word sens from text it initially discovers a set of tight cluster called committee that are well scattered in the similarity space the centroid of the member of a committee is used a the feature vector of the cluster we proceed by assigning word to their most similar cluster after assigning an element to a cluster we remove their overlapping feature from the element this allows cbc to discover the le frequent sens of a word and to avoid discovering duplicate sens each cluster that a word belongs to represents one of it sens we also present an evaluation methodology for automatically measuring the precision and recall of discovered sens 
this paper proposes a method for computing fast approximation to support vector decision function in the field of object detection in the present approach we are building on an existing algorithm where the set of support vector is replaced by a smaller so called reduced set of synthesized input space point in contrast to the existing method that find the reduced set via unconstrained optimization we impose a structural constraint on the synthetic point such that the resulting approximation can be evaluated via separable filter for application that require scanning large image this decrease the computational complexity by a significant amount experimental result show that in face detection rank deficient approximation are to time faster than unconstrained reduced set system 
knowing the reputation of your own and or competitor product is important for marketing and customer relationship management it is however very costly to collect and analyze survey data manually this paper present a new framework for mining product reputation on the internet it automatically collect people s opinion about target product from web page and it us text mining technique to obtain the reputation of those product on the basis of human test sample we generate in advance syntactic and linguistic rule to determine whether any given statement is an opinion or not a well a whether such any opinion is positive or negative in nature we first collect statement regarding target product using a general search engine and then using the rule extract opinion from among them and attach three label to each opinion label indicating the positive negative determination the product name itself and an numerical value expressing the degree of system confidence that the statement is in fact an opinion the labeled opinion are then input into an opinion database the mining of reputation i e the finding of statistically meaningful information included in the database is then conducted we specify target category using label value such a positive opinion of product a and perform four type of text mining extraction of characteristic word co occurrence word typical sentence for individual target category and correspondence analysis among multiple target category actual marketing data is used to demonstrate the validity and effectiveness of the framework which offer a drastic reduction in the overall cost of reputation analysis over that of conventional survey approach and support the discovery of knowledge from the pool of opinion on the web 
naive bayes classifier ha long been used for text categorization task it sibling from the unsupervised world the probabilistic mixture of multinomial model ha likewise been successfully applied to text clustering problem despite the strong independence assumption that these model make their attractiveness come from low computational cost relatively low memory consumption ability to handle heterogeneous feature and multiple class and often competitiveness with the top of the line model recently there ha been several attempt to alleviate the problem of naive bayes by performing heuristic feature transformation such a idf normalization by the length of the document and taking the logarithm of the count we justify the use of these technique and apply them to two problem classification of product in yahoo shopping and clustering the vector of collocated term in user query to yahoo search the experimental evaluation allows u to draw conclusion about the promise that these transformation carry with regard to alleviating the strong assumption of the multinomial model 
viewing knowledge discovery a a user centered process that requires an effective collaboration between the user and the discovery system our work aim to support an active role of the user in that process by developing synergistic visualization tool integrated in our discovery system d m these tool provide an ability of visualizing the entire process of knowledge discovery in order to help the user with data preprocessing selecting mining algorithm and parameter evaluating and comparing discovered model and taking control of the whole discover process our case study with two medical datasets on meningitis and stomach cancer show that with visualization tool in d m the user gain better insight in each step of the knowledge discovery process a well the relationship between data and discovered knowledge 
this paper describes a prototype that predicts the shopping list for customer in a retail store the shopping list prediction is one aspect of a larger system we have developed for retailer to provide individual and personalized interaction with customer a they navigate through the retail store instead of using traditional personalization approach such a clustering or segmentation we learn separate classifier for each customer from historical transactional data this allows u to make very fine grained and accurate prediction about what item a particular individual customer will buy on a given shopping trip we formally frame the shopping list prediction a a classification problem describe the algorithm and methodology behind our system it impact on the business case in which we frame it and explore some of the property of the data source that make it an interesting testbed for kdd algorithm our result show that we can predict a shopper s shopping list with high level of accuracy precision and recall we believe that this work impact both the data mining and the retail business community the formulation of shopping list prediction a a machine learning problem result in algorithm that should be useful beyond retail shopping list prediction for retailer the result is not only a practical system that increase revenue by up to but also enhances customer experience and loyalty by giving them the tool to individually interact with customer and anticipate their need 
mining frequent closed itemsets provides complete and non redundant result for frequent pattern analysis extensive study have proposed various strategy for efficient frequent closed itemset mining such a depth first search v breadthfirst search vertical format v horizontal format tree structure v other data structure top down v bottom up traversal pseudo projection v physical projection of conditional database etc it is the right time to ask what are the pro and con of the strategy and what and how can we pick and integrate the best strategy to achieve higher performance in general case in this study we answer the above question by a systematic study of the search strategy and develop a winning algorithm closet closet integrates the advantage of the previously proposed effective strategy a well a some one newly developed here a thorough performance study on synthetic and real data set ha shown the advantage of the strategy and the improvement of closet over existing mining algorithm including closet charm and op in term of runtime memory usage and scalability 
abstract this paper is about bound on futureerror rate we present a theorem forcombining an arbitrary test set basedbound with an arbitrary training setbased bound appropriate use of thistheorem result in a combined boundwith two property the combinedbound is never much worse than eitherthe training set based bound or thetest set based bound and the combinedbound is sometimes better thaneither bound individually empiricalvalidation is presented showing the 
this paper discus the application of particle filtering algorithm to fault diagnosis in complex industrial process we consider two ubiquitous process an industrial dryer and a level tank for these application we compared three particle filtering variant standard particle filtering rao blackwellised particle filtering and a version of raoblackwellised particle filtering that doe one step look ahead to select good sampling region we show that the overhead of the extra processing per particle of the more sophisticated method is more than compensated by the decrease in error and variance 
despite the popularity of connectionist model in cognitive science their performance can often be difficult to evaluate inspired by the geometric approach to statistical model selection we introduce a conceptually similar method to examine the global behavior of a connectionist model by counting the number and type of response pattern it can simulate the markov chain monte carlo based algorithm that we constructed nd these pattern efficiently we demonstrate the approach using two localist network model of speech perception 
data mining technique are routinely used by fundraiser to select those prospect from a large pool of candidate who are most likely to make a financial contribution these technique often rely on statistical model based on trial performance data this trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool collecting this trial data involves a cost therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospect we describe an experimental design approach to optimally choose the trial prospect from an existing large pool of prospect prospect are clustered to render the problem practically tractable we modify the standard d optimality algorithm to prevent repeated selection of the same prospect cluster since each prospect can only be solicited at most once we ass the benefit of this approach on the kdd data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size 
a variety of mining and analysis problem ranging from association rule discovery to contingency table analysis to materialization of certain approximate datacubes involve the extraction of knowledge from a set of categorical count data such data can be viewed a a collection of transaction where a transaction is a fixed length vector of count classical algorithm for solving count data problem require one or more computationally intensive pass over the entire database and can be prohibitively slow one effective method for dealing with this ever worsening scalability problem is to run the algorithm on a small sample of the data we present a new data reduction algorithm called ease for producing such a sample like the fast algorithm introduced by chen et al ease is especially designed for count data application both ease and fast take a relatively large initial random sample and then deterministically produce a subsample whose distance appropriately defined from the complete database is minimal unlike fast which obtains the final subsample by quasi greedy descent ease us epsilon approximation method to obtain the final subsample by a process of repeated halving experiment both in the context of association rule mining and classical contingency table analysis show that ease outperforms both fast and simple random sampling sometimes dramatically 
we consider learning to classify cognitive state of human subject based on their brain activity observed via functional magnetic resonance imaging fmri this problem is important because such classifier constitute virtual sensor of hidden cognitive state which may be useful in cognitive science research and clinical application in recent work mitchell et al have demonstrated the feasibility of training such classifier for individual human subject e g to distinguish whether the subject is reading an ambiguous or unambiguous sentence or whether they are reading a noun or a verb here we extend that line of research exploring how to train classifier that can be applied across multiple human subject including subject who were not involved in training the classifier we describe the design of several machine learning approach to training multiple subject classifier and report experimental result demonstrating the success of these method in learning cross subject classifier for two different fmri data set 
with application in biology the world wide web and several other area mining of graph structured object ha received significant interest recently one of the major research direction in this field is concerned with predictive data mining in graph database where each instance is represented by a graph some of the proposed approach for this task rely on the excellent classification performance of support vector machine to control the computational cost of these approach the underlying kernel function are based on frequent pattern in contrast to these approach we propose a kernel function based on a natural set of cyclic and tree pattern independent of their frequency and discus it computational aspect to practically demonstrate the effectiveness of our approach we use the popular nci hiv molecule dataset our experimental result show that cyclic pattern kernel can be computed quickly and offer predictive performance superior to recent graph kernel based on frequent pattern 
recognizing the e xpressive power of graph representation and the a bility of certain graph grammar to generalize we attempt to use graph grammar learning for concept formation in this paper we describe our initial progress toward that goal and focus on ho w certain graph grammar can be learned from example we also establish ground for using graph grammar in machine learning task several example are presented to highlight the validity of the approach 
although most time series data mining research ha concentrated on providing solution for a single distance function in this work we motivate the need for a single index structure that can support multiple distance measure our specific area of interest is the efficient retrieval and analysis of trajectory similarity trajectory datasets are very common in environmental application mobility experiment video surveillance and are especially important for the discovery of certain biological pattern our primary similarity measure is based on the longest common subsequence lcss model that offer enhanced robustness particularly for noisy data which are encountered very often in real world application however our index is able to accommodate other distance measure a well including the ubiquitous euclidean distance and the increasingly popular dynamic time warping dtw while other researcher have advocated one or other of these similarity measure a major contribution of our work is the ability to support all these measure without the need to restructure the index our framework guarantee no false dismissal and can also be tailored to provide much faster response time at the expense of slightly reduced precision recall the experimental result demonstrate that our index can help speed up the computation of expensive similarity measure such a the lcss and the dtw 
multi view algorithm such a co training and co em utilize unlabeled data when the available attribute can be split into independent and compatible subset co em outperforms co training for many problem but it requires the underlying learner to estimate class probability and to learn from probabilistically labeled data therefore co em ha so far only been studied with naive bayesian learner we cast linear classifier into a probabilistic framework and develop a co em version of the support vector machine we conduct experiment on text classification problem and compare the family of semi supervised support vector algorithm under different condition including violation of the assumption underlying multi view learning for some problem such a course web page classification we observe the most accurate result reported so far 
abstract we describe the application of kernel method to natural language processing nlp problem in many nlp task the object being modeled are string tree graph or other discrete structure which require some mechanism to convert them into feature vector we describe kernel for various natural language structure allowing rich high dimensional representation of these structure we show how a kernel over tree can be applied to parsing using the voted perceptron algorithm and we give experimental result on the atis corpus of parse tree 
we develop a maximum entropy maxent approach to generating recommendation in the context of a user s current navigation stream suitable for environment where data is sparse highdimensional and dynamic condition typical of many recommendation application we address sparsity and dimensionality reduction by rst clustering item based on user access pattern so a to attempt to minimize the apriori probability that recommendation will cross cluster boundary and then recommending only within cluster we address the inherent dynamic nature of the problem by explicitly modeling the data a a time series we show how this representational expressivity t naturally into a maxent framework we conduct experiment on data from researchindex a popular online repository of over computer science document we show that our maxent formulation outperforms several competing algorithm in oine test simulating the recommendation of document to researchindex user 
clustering and prediction of set of curve is an important problem in many area of science and engineering it is often the case that curve tend to be misaligned from each other in a continuous manner either in space across the measurement or in time we develop a probabilistic framework that allows for joint clustering and continuous alignment of set of curve in curve space a opposed to a fixed dimensional featurevector space the proposed methodology integrates new probabilistic alignment model with model based curve clustering algorithm the probabilistic approach allows for the derivation of consistent em learning algorithm for the joint clustering alignment problem experimental result are shown for alignment of human growth data and joint clustering and alignment of gene expression time course data 
we investigate an approach for simultaneously committing to multiple activity each modeled a a temporally extended action in a semi markov decision process smdp for each activity we dene a set of admissible solution consisting of the redundant set of optimal policy and those policy that ascend the optimal statevalue function associated with them a plan is then generated by merging them in such a way that the solution to the subordinate activity are realized in the set of admissible solution satisfying the superior activity we present our theoretical result and empirically evaluate our approach in a simulated domain 
automated image interpretation is an important task in numerous application ranging from security system to natural resource inventorization based on remote sensing recently a second generation of adaptive machine learned image interpretation system have shown expertlevel performance in several challenging domain while demonstrating an unprecedented improvement over hand engineered or first generation machine learned system in term of cross domain portability design cycle time and robustness such system are still severely limited in this paper we inspect the anatomy of a state of the art adaptive image interpretation system and discus the range of the corresponding machine learning problem we then report on the novel machine learning approach engaged and the resulting improvement 
traditional information retrieval typically represents data using a bag of word data mining typically us a highly structured database representation this paper explores the middle ground using a representation which we term entity model in which question about structured data may be posed and answered but the complexity and task specific restriction of ontology are avoided an entity model is a language model or word distribution associated with an entity such a a person place or organization using these perentity language model entity may be clustered link may be detected or described with a short summary entity may be collectively classified and question answering may be performed on a corpus of entity extracted from newswire and the web we group entity by profession with accuracy improve accuracy further on the task of classifying politician a liberal or conservative using collective classification and conditional random field and answer question about who a person is with mean reciprocal rank mrr of 
we introduce a class of string kernel called mismatch kernel for use with support vector machine svms in a discriminative approach to the protein classification problem these kernel measure sequence similarity based on shared occurrence of length subsequence counted with up to mismatch and do not rely on any generative model for the positive training sequence we compute the kernel efficiently using a mismatch tree data structure and report experiment on a benchmark scop dataset where we show that the mismatch kernel used with an svm classifier performs a well a the fisher kernel the most successful method for remote homology detection while achieving considerable computational saving 
abstract we introduce a class of string kernel called mismatch kernel for usewith support vector machine svms in a discriminative approach tothe protein classification problem these kernel measure sequence similaritybased on shared occurrence of k length subsequence countedwith up to m mismatch and do not rely on any generative model forthe positive training sequence we compute the kernel efficiently usinga mismatch tree data structure and report experiment on a benchmark 
metabolomics is the omics science of biochemistry the associated data include the quantitative measurement of all small molecule metabolite in a biological sample these datasets provide a window into dynamic biochemical network and conjointly with other omic data gene and protein have great potential to unravel complex human disease the dataset used in this study ha individual normal and diseased and the diseased are drug treated or not so there are three class the goal is to classify these individual using the observed metabolite level for measured metabolite there are a number of statistical challenge non normal data the number of sample is le than the number of metabolite there are missing data and the fact that data are missing is informative assay value below detection limit can point to a specific class also there are high correlation among the metabolite we investigate support vector machine svm and random forest rf for outlier detection variable selection and classification we use the variable selected with rf in svm and visa versa the benefit of this study is insight into interplay of variable selection and classification method we link our selected predictor to the biochemistry of the disease 
we present policyblocks an algorithm by which a reinforcement learning agent can extract useful macro action from a set of related task the agent creates macroactions by finding commonality in solution to previous task using these macro action learning to do future related task is accelerated 
text categorization or classification is the automated assigning of text document to pre defined class based on their content this problem ha been studied in information retrieval machine learning and data mining so far many effective technique have been proposed however most technique are based on some underlying model and or assumption when the data fit the model well the classification accuracy will be high however when the data doe not fit the model well the classification accuracy can be very low in this paper we propose a refinement approach to dealing with this problem of model misfit we show that we do not need to change the classification technique itself or it underlying model to make it more flexible instead we propose to use successive refinement of classification on the training data to correct the model misfit we apply the proposed technique to improve the classification performance of two simple and efficient text classifier the rocchio classifier and the na ve bayesian classifier these technique are suitable for very large text collection because they allow the data to reside on disk and need only one scan of the data to build a text classifier extensive experiment on two benchmark document corpus show that the proposed technique is able to improve text categorization accuracy of the two technique dramatically in particular our refined model is able to improve the na ve bayesian or rocchio classifier s prediction performance by on average 
clustering aim at extracting hidden structure in dataset while the problem of finding compact cluster ha been widely studied in the literature extracting arbitrarily formed elongated structure is considered a much harder problem in this paper we present a novel clustering algorithm which tackle the problem by a two step procedure first the data are transformed in such a way that elongated structure become compact one in a second step these new object are clustered by optimizing a compactness based criterion the advantage of the method over related approach are threefold i robustness property of compactness based criterion naturally transfer to the problem of extracting elongated structure leading to a model which is highly robust against outlier object ii the transformed distance induce a mercer kernel which allows u to formulate a polynomial approximation scheme to the generally nphard clustering problem iii the new method doe not contain free kernel parameter in contrast to method like spectral clustering or mean shift clustering 
abstract recently the fisher score or the fisher kernel is increasingly used a a feature extractor for classification problem the fisher score is a vector of parameter derivative of loglikelihood of a probabilistic model this paper give a theoretical analysis about how class information is pre served in the space of the fisher score which turn out that the fisher score consists of a few important dimension with class information and many nuisance dimension when we perform clustering with the fisher score k mean type method are obviously inappropriate because they make use of all dimension so we will develop a novel but simple clus tering algorithm specialized for the fisher score which can exploit im portant dimension this algorithm is successfully tested in experiment with artificial data and real data amino acid sequence 
this paper introduces a scalable method for feature extraction and navigation of large data set by mean of local clustering where cluster are modeled a overlapping neighborhood under the model intra cluster association and external differentiation are both assessed in term of a natural confidence measure minor cluster can be identified even when they appear in the intersection of larger cluster scalability of local clustering derives from recent generic technique for efficient approximate similarity search the cluster overlap structure give rise to a hierarchy that can be navigated and queried by user experimental result are provided for two large text database 
linear prediction method such a least square for regression logistic regression and support vector machine for classification have been extensively used in statistic and machine learning in this paper we study stochastic gradient descent sgd algorithm on regularized form of linear prediction method this class of method related to online algorithm such a perceptron are both efficient and very simple to implement we obtain numerical rate of convergence for such algorithm and discus it implication experiment on text data will be provided to demonstrate numerical and statistical consequence of our theoretical finding 
in this paper we broaden the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database specifically the causality rule explored in this paper consists of a sequence of triggering event and a set of consequential event and is designed with the capability of mining non sequential inter transaction information hence the causality rule mining provides a very general framework for rule derivation note however that the procedure of causality rule mining is very costly particularly in the presence of a huge number of candidate set and a distributed database and in our opinion cannot be dealt with by direct extension from existing rule mining method consequently we devise in this paper a series of level matching algorithm including level matching abbreviatedly a lm level matching with selective scan abbreviatedly a lm and distributed level matching abbreviatedly a distibuted lm to minimize the computing cost needed for the distributed data mining of causality rule in addition the phenomenon of time window constraint are also taken into consideration for the development of our algorithm a a result of properly employing the technology of level matching and selective scan the proposed algorithm present good efficiency and scalability in the mining of local and global causality rule scale up experiment show that the proposed algorithm scale well with the number of site and the number of customer transaction index term knowledge discovery distributed data mining causality rule triggering event consequential event 
part of the process of data integration is determining which set of identifier refer to the same real world entity in integrating database found on the web or obtained by using information extraction method it is often possible to solve this problem by exploiting similarity in the textual name used for object in different database in this paper we describe technique for clustering and matching identifier name that are both scalable and adaptive in the sense that they can be trained to obtain better performance in a particular domain an experimental evaluation on a number of sample datasets show that the adaptive method sometimes performs much better than either of two non adaptive baseline system and is nearly always competitive with the best baseline system 
this paper concern approximate nearest neighbor searching algorithm which have become increasingly important especially in high dimensional perception area such a computer vision with dozen of publication in recent year much of this enthusiasm is due to a successful new approximate nearest neighbor approach called locality sensitive hashing lsh in this paper we ask the question can earlier spatial data structure approach to exact nearest neighbor such a metric tree be altered to provide approximate answer to proximity query and if so how we introduce a new kind of metric tree that allows overlap certain datapoints may appear in both the child of a parent we also introduce new approximate k nn search algorithm on this structure we show why these structure should be able to exploit the same randomprojection based approximation that lsh enjoys but with a simpler algorithm and perhaps with greater efficiency we then provide a detailed empirical evaluation on five large high dimensional datasets which show up to fold acceleration over lsh this result hold true throughout the spectrum of approximation level 
we investigate method for planning in a markov decision process where the cost function is chosen by an adversary after we x our policy a a running example we consider a robot path planning problem where cost are inuenced by sensor that an adversary place in the environment we formulate the problem a a zero sum matrix game where row correspond to deterministic policy for the planning player and column correspond to cost vector the adversary can select for a xed cost vector fast algorithm such a value iteration are available for solving mdps we develop ecien t algorithm for matrix game where such best response oracle exist we show that for our path planning problem these algorithm are at least an order of magnitude faster than direct solution of the linear programming formulation 
this paper quantitatively analyzes indicator of agent policy seller adjuster indemnity claim adjuster producer policy purchaser holder indemnity behavior suggestive of collusion in the united state department of agriculture usda risk management agency rma national crop insurance program according to guidance from the federal law and using six indicator variable of indemnity behavior those entity equal to or exceeding of the county mean computed using a simple jackknife procedure on all entity relevant indicator were flagged a anomalous log linear analysis wa used to test i hierarchical node node arrangement and a non recursive model of node information sharing chi square distributed deviance statistic identified the optimal log linear model the result of the applied data mining technique used here suggest that the non recursive triplet and agent producer doublet collusion probabilistically account for the greatest proportion of waste fraud and abuse in the federal crop insurance program triplet and agent producer doublet need detailed investigation for possible collusion hence this data mining technique provided a high level of confidence when million record were quantitatively analyzed for possible fraud waste or other abuse of the crop insurance program administered by the usda rma and suspect entity reported to usda this data mining technique can be applied where vast amount of data are available to detect pattern of collusion or conspiracy a may be of interest to the criminal justice or intelligence agency 
we propose the hierarchical dirichlet process hdp a nonparametric bayesian model for clustering problem involving multiple group of data each group of data is modeled with a mixture with the number of component being open ended and inferred automatically by the model further component can be shared across group allowing dependency across group to be modeled effectively a well a conferring generalization to new group such grouped clustering problem occur often in practice e g in the problem of topic discovery in document corpus we report experimental result on three text corpus showing the effective and superior performance of the hdp over previous model 
in this paper we present a new algorithm suitable for matching discrete object such a string and tree in linear time thus obviat ing dynamic programming with quadratic time complexity furthermore prediction cost in many case can be reduced to linear cost in the length of the sequence to be classified regardless of the number of support v ectors this improvement on the currently available algorithm make string kernel a viable alternative for the practitioner 
there ha historically been very little concern with extrapolation in machine learning yet extrapolation can be critical to diagnose predictor function are almost always learned on a set of highly correlated data comprising a very small segment of predictor space moreover flexible predictor by their very nature are not controlled at point of extrapolation this becomes a problem for diagnostic tool that require evaluation on a product distribution it is also an issue when we are trying to optimize a response over some variable in the input space finally it can be a problem in non static system in which the underlying predictor distribution gradually drift with time or when typographical error misrecord the value of some predictor we present a diagnosis for extrapolation a a statistical test for a point originating from the data distribution a opposed to a null hypothesis uniform distribution this allows u to employ general classification method for estimating such a test statistic further we observe that cart can be modified to accept an exact distribution a an argument providing a better classification tool which becomes our extrapolation detection procedure we explore some of the advantage of this approach and present example of it practical application 
to understand the brain mechanism involved in reward prediction on different time scale we developed a markov decision task that requires prediction of both immediate and future reward and analyzed subject brain activity using functional mri we estimated the time course of reward prediction and reward prediction error on different time scale from subject performance data and used them a the explanatory variable for spm analysis we found topographic map of different time scale in medial frontal cortex and striatum the result suggests that different cortico basal ganglion loop are specialized for reward prediction on different time scale 
we consider the policy search approach to reinforcement learning we show that if a baseline distribution is given indicating roughly how often we expect a good policy to visit each state then we can derive a policy search algorithm that terminates in a finite number of step and for which we can provide non trivial performance guarantee we also demonstrate this algorithm on several grid world pomdps a planar biped walking robot and a double pole balancing problem 
dimensionality reduction via random projection ha attracted considerable attention in recent year the approach ha interesting theoretical underpinnings and offer computational advantage in this paper we report a number of experiment to evaluate random projection in the context of inductive supervised learning in particular we compare random projection and pca on a number of different datasets and using different machine learning method while we find that the random projection approach predictively underperforms pca it computational advantage may make it attractive for certain application 
even under perfect fixation the human eye is under steady motion tremor microsaccades slow drift the dynamic theory of vision state that eye movement can improve hyperacuity according to this theory eye movement are thought to create variable spatial excitation pattern on the photoreceptor grid which will allow for better spatiotemporal summation at later stage we reexamine this theory using a realistic model of the vertebrate retina by comparing response of a resting and a moving eye the performance of simulated ganglion cell in a hyperacuity task is evaluated by ideal observer analysis we find that in the central retina eye micromovements have no effect on the performance here optical blurring limit vernier acuity in the retinal periphery however eye micromovements clearly improve performance based on roc analysis our prediction are quantitatively testable in electrophysiological and psychophysical experiment 
two neural network that are trained on their mutual output synchronize to an identical time dependant weight vector this novel phenomenon can be used for creation of a secure cryptographic secret key using a public channel several model for this cryptographic system have been suggested and have been tested for their security under different sophisticated attack strategy the most promising model are network that involve chaos synchronization the synchronization process of mutual learning is described analytically using statistical physic method 
we consider the problem of improving named entity recognition ner system by using external dictionary more specifically the problem of extending state of the art ner system by incorporating information about the similarity of extracted entity to entity in an external dictionary this is difficult because most high performance named entity recognition system operate by sequentially classifying word a to whether or not they participate in an entity name however the most useful similarity measure score entire candidate name to correct this mismatch we formalize a semi markov extraction process which is based on sequentially classifying segment of several adjacent word rather than single word in addition to allowing a natural way of coupling high performance ner method and high performance similarity function this formalism also allows the direct use of other useful entity level feature and provides a more natural formulation of the ner problem than sequential word classification experiment in multiple domain show that the new model can substantially improve extraction performance over previous method for using external dictionary in ner 
behavioral goal are achieved reliably and repeatedly with movement rarely reproducible in their detail here we offer an explanation we show that not only are variability and goal achievement compatible but indeed that allowing variability in redundant dimension is the optimal control strategy in the face of uncertainty the optimal feedback control law for typical motor task obey a minimal intervention principle deviation from the average trajectory are only corrected when they interfere with the task goal the resulting behavior exhibit task constrained variability a well a synergetic coupling among actuator which is another unexplained empirical phenomenon 
existing association rule mining algorithm suffer from many problem when mining massive transactional datasets one major problem is the high memory dependency either the gigantic data structure built is assumed to fit in main memory or the recursive mining process is too voracious in memory resource another major impediment is the repetitive and interactive nature of any knowledge discovery process to tune parameter many run of the same algorithm are necessary leading to the building of these huge data structure time and again this paper proposes a new disk based association rule mining algorithm called inverted matrix which achieves it efficiency by applying three new idea first transactional data is converted into a new database layout called inverted matrix that prevents multiple scanning of the database during the mining phase in which finding frequent pattern could be achieved in le than a full scan with random access second for each frequent item a relatively small independent tree is built summarizing co occurrence finally a simple and non recursive mining process reduces the memory requirement a minimum candidacy generation and counting is needed experimental study reveal that our inverted matrix approach outperform fp tree especially in mining very large transactional database with a very large number of unique item our random access disk based approach is particularly advantageous in a repetitive and interactive setting 
capturing dependency in image in an unsupervised manner is important for many image processing application we propose a new method for capturing nonlinear dependency in image of natural scene this method is an extension of the linear independent component analysis ica method by building a hierarchical model based on ica and mixture of laplacian distribution the model parameter are learned via an em algorithm and it can accurately capture variance correlation and other high order structure in a simple manner we visualize the learned variance structure and demonstrate application to image segmentation and denoising 
in this paper we present a new framework for online novelty detection on temporal sequence this framework include a mechanism for associating each detection result with a confidence value based on this framework we develop a concrete online detection algorithm by modeling the temporal sequence using an online support vector regression algorithm experiment on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm 
in this paper we present a framework for using multi layer perceptron mlp network in nonlinear generative model trained by variational bayesian learning the nonlinearity is handled by linearizing it using a gauss hermite quadrature at the hidden neuron this yield an accurate approximation for case of large posterior variance the method can be used to derive nonlinear counterpart for linear algorithm such a factor analysis independent component factor analysis and state space model this is demonstrated with a nonlinear factor analysis experiment in which even source can be estimated from a real world speech data set 
a common characteristic of relational data set lead many relational learning algorithm to discover misleading correlation this characteristic degree disparity occurs when entity of one class participate in systematically higher number of relation than entity of another class in such case the aggregation function that are used in many relational learning algorithm e g avg mode sum exists count max min will result in misleading correlation and added complexity in model we examine this problem through a combination of simulation and experiment we show how two novel significance testing procedure can adjust for the effect of using aggregation function in the presence of degree disparity 
in this paper we propose a lattice based approach intended for extracting semantics from datacubes border of version space for supervised classification closed cube lattice to summarize the semantics of datacubes w r t count sum and covering graph of the quotient cube a a visualization tool of minimal multidimensional association with this intention we introduce two novel concept the cube transversals and the cube closure over the cube lattice of a categorical database relation we propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan we introduce the cube connection show that it is a galois connection and derive a closure operator over the cube lattice using cube transversals and closure we define a new characterization of boundary set which provide a condensed representation of version space used to enhance supervised classification the algorithm designed for computing such border improves the complexity of previous proposal we also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the galois lattice and on the other hand the quotient cube w r t count sum proposed in the quotient cube is a succinct summary of a datacube preserving the rollup drilldown semantics we show that the quotient cube w r t count sum and the closed cube lattice have a similar expression power but the latter ha the smallest possible size finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional association 
boosting is a strong ensemble based learning algorithm with the promise of iteratively improving the classification accuracy using any base learner a long a it satisfies the condition of yielding weighted accuracy in this paper we analyze boosting with respect to this basic condition on the base learner to see if boosting ensures prediction of rarely occurring event with high recall and precision first we show that a base learner can satisfy the required condition even for poor recall or precision level especially for very rare class furthermore we show that the intelligent weight updating mechanism in boosting even in it strong cost sensitive form doe not prevent case where the base learner always achieves high precision but poor recall or high recall but poor precision when mapped to the original distribution in either of these case we show that the voting mechanism of boosting fall to achieve good overall recall and precision for the ensemble in effect our analysis indicates that one cannot be blind to the base learner performance and just rely on the boosting mechanism to take care of it weakness we validate our argument empirically on variety of real and synthetic rare class problem in particular using adacost a the boosting algorithm and variation of pnrule and ripper a the base learner we show that if algorithm a achieves better recall precision balance than algorithm b then using a a the base learner in adacost yield significantly better performance than using b a the base learner 
in large multiagent game partial observability coordination and credit assignment persistently plague attempt to design good learning algorithm we provide a simple and efficient algorithm that in part us a linear system to model the world from a single agent s limited perspective and take advantage of kalman filtering to allow an agent to construct a good training signal and learn an effective policy 
machine learning ha reached a point where most probabilistic meth od can be understood a variation extension and combination of a much smaller set of abstract theme e g a different instance of the em algorithm this enables the systematic derivation of algorithm cu tomized for different model here we demonstrate the autobayes system which take a high level statistical model specification us pow erful symbolic technique based on schema based program synthesis and computer algebra to derive an efficient specialized algorithm for learning that model and generates executable code implementing that algorithm this capability is far beyond that of code collection such a matlab tool box or even tool for model independent optimization such a bug for gibbs sampling complex new algorithm can be generated with out new programming algorithm can be highly specialized and tightly crafted for the exact structure of the model and data and efficient and commented code can be generated for different language or system we present automatically derived algorithm ranging from closed form solution of bayesian textbook problem to recently proposed em algo rithms for clustering regression and a multinomial form of pca 
we present a biophysically constrained cerebellar model of classical conditioning implemented using a neuromorphic analog vlsi avlsi chip like it biological counterpart our cerebellar model is able to control adaptive behavior by predicting the precise timing of event here we describe the functionality of the chip and present it learning performance a evaluated in simulated conditioning experiment at the circuit level and in behavioral experiment using a mobile robot we show that this avlsi model support the acquisition and extinction of adaptively timed conditioned response under real world condition with ultra low power consumption 
abstract this paper give distribution free concentration inequality for the missing mass and the error rate of histogram rule negative association method can be used to reduce these concentration problem to concentration question about independent sum although the sum are independent they are highly heterogeneous such highly heterogeneous independent sum cannot be analyzed using standard concentration inequality such a hoeffding s inequality the angluin valiant bound bernstein s inequality bennett s inequality or mcdiarmid s theorem the concentration inequality for histogram rule error is motivated by the desire to construct a new class of bound on the generalization error of decision tree 
to address the problem of algorithm selection for the classication task we equip a case based system with new similarity measure that are able to cope with multirelational representation the proposed approach build on notion from clustering and is closely related to idea developed in distance based relational learning the idea presented are pertinent not only for metalearning representational issue but for all domain with similar representation requirement 
the relative depth of object cause small shift in the left and right retinal position of these object called binocular disparity here we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model which ha been proposed to model the response of disparity selective cell in the visual cortex our system consists of two silicon chip containing spiking neuron with monocular gabor type spatial receptive field rf and circuit that combine the spike output to compute a disparity selective complex cell response the disparity selectivity of the cell can be adjusted by both position and phase shift between the monocular rf profile which are both used in biology our neuromorphic system performs better with phase encoding because the relative response of neuron tuned to different disparity by phase shift are better matched than the response of neuron tuned by position shift the accurate perception of the relative depth of object enables both biological organism and artificial autonomous system to interact successfully with their environment binocular disparity the positional shift between corresponding point in two eye or camera caused by the difference in their vantage point is one important cue that can be used to infer depth in the mammalian visual system neuron in the visual cortex combine signal from the left and right eye to generate response selective for a particular disparity ohzawa et al proposed the binocular energy model to explain the response of binocular complex cell in the cat visual cortex and found that the prediction of this model are in good agreement with measured data this model also match data from the macaque in the energy model a neuron achieves it particular disparity tuning by either a position or a phase shift between it monocular receptive field rf profile for the left and right eye based on an analysis of a population of binocular cell anzai et al suggest that the cat primarily encodes disparity via a phase shift although position shift may play a larger role at higher spatial frequency computational study show that it is possible to estimate disparity from the relative response of model complex cell tuned to different disparity 
we examine the set covering machine when it us data dependent half spacesfor it set of feature and bound it generalization error in term of the number of training error and the number of half space it achieves on the training data we show that it provides a favorable alternative to data dependent ball on some natural data set compared to the support vector machine the set covering machine with data dependent halfspaces produce substantially sparser classifier with comparable and sometimes better generalization furthermore we show that our bound on the generalization error provides an effective guide for model selection 
market research ha shown that consumer exhibit a variety of different purchasing behavior specifically some tend to purchase product earlier than other consumer identifying such early buyer can help personalize marketing strategy potentially improving their effectiveness in this paper we present a non parametric approach to the problem of identifying early buyer from purchase data our formulation take a input the detailed purchase information of each consumer with which we construct a weighted directed graph whose node correspond to consumer and whose edge correspond to purchase consumer have in common the edge weight indicate how frequently consumer purchase product earlier than other consumer identifying early buyer corresponds to the problem of finding a subset of node in the graph with maximum difference between the weight of the outgoing and incoming edge this problem is a variation of the maximum cut problem in a directed graph we provide an approximation algorithm based on semidefinite programming sdp relaxation pioneered by goemans and williamson and analyze it performance we apply the algorithm to real purchase data from amazon com providing new insight into consumer behavior 
in this paper we present a novel method for learning complex concept hypothesis directly from raw training data the task addressed here concern data driven synthesis of recognition procedure for real world object recognition task the method us linear genetic programming to encode potential solution expressed in term of elementary operation and handle the complexity of the learning task by applying cooperative coevolution to decompose the problem automatically the training consists in coevolving feature extraction procedure each being a sequence of elementary image processing and feature extraction operation extensive experimental result show that the approach attains competitive performance for d object recognition in real synthetic aperture radar sar imagery 
we present a graph theoretic approach to discover storyline from search result storyline are window that offer glimpse into interesting theme latent among the top search result for a query they are different from and complementary to cluster obtained through traditional approach our framework is axiomatically developed and combinatorial in nature based on generalization of the maximum induced matching problem on bipartite graph the core algorithmic task involved is to mine for signature structure in a robust graph representation of the search result we present a very fast algorithm for this task based on local search experiment show that the collection of storyline extracted through our algorithm offer a concise organization of the wealth of information hidden beyond the first page of search result 
there are various source code archive on the world wide web these archive are usually organized by application category and programming language however manually organizing source code repository is not a trivial task since they grow rapidly and are very large on the order of terabyte we demonstrate machine learning method for automatic classification of archived source code into eleven application topic and ten programming language for topical classification we concentrate on c and c program from the ibiblio and the sourceforge archive support vector machine svm classifier are trained on example of a given programming language or program in a specified category we show that source code can be accurately and automatically classified into topical category and can be identified to be in a specific programming language class 
we present a competitive analysis of bayesian learning algorithm in the online learning setting and show that many simple bayesian algorithm such a gaussian linear regression and bayesian logistic regression perform favorably when compared in retrospect to the single best model in the model class the analysis doe not assume that the bayesian algorithm modeling assumption are correct and our bound hold even if the data is adversarially chosen for gaussian linear regression using logloss our error bound are comparable to the best bound in the online learning literature and we also provide a lower bound showing that gaussian linear regression is optimal in a certain worst case sense we also give bound for some widely used maximum a posteriori map estimation algorithm including regularized logistic regression 
online algorithm for classification often require vast amount of memory and computation time when employed in conjunction with kernel function in this paper we describe and analyze a simple approach for an on the fly reduction of the number of past example used for prediction experiment performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine svm although the latter being a batch algorithm access each training example multiple time 
detection of space time cluster is an important function in various domain e g epidemiology and public health the pioneering work on the spatial scan statistic is often used a the basis to detect and evaluate such cluster state of the art system based on this approach detect cluster with restrictive shape that cannot model growth and shift in location over time we extend these method significantly by using the flexible square pyramid shape to model such effect a heuristic search method is developed to detect the most likely cluster using a randomized algorithm in combination with geometric shape processing the use of monte carlo method in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issue our method is applied to a real data set on brain cancer occurrence over a year period the cluster detected by our method show both growth and movement which could not have been modeled with the simpler cylindrical shape used earlier our general framework can be extended quite easily to handle other flexible shape for the space time cluster 
understanding the difference between contrasting group is a fundamental task in data analysis this realization ha led to the development of a new special purpose data mining technique contrast set mining we undertook a study with a retail collaborator to compare contrast set mining with existing rule discovery technique to our surprise we observed that straightforward application of an existing commercial rule discovery system magnum opus could successfully perform the contrast set mining task this led to the realization that contrast set mining is a special case of the more general rule discovery task we present the result of our study together with a proof of this conclusion 
p an efficient method using bayesian and linear classifier is presented for analyzing the dynamic of information in high dimensional circuit state and applied to investigate emergent computation in generic cortical microcircuit model it is shown that such recurrent circuit of spiking neuron have an inherent capability to carry out rapid computation on complex spike pattern merging information contained in the order of spike arrival with previously acquired context information p 
we study a method of optimal data driven aggregation of classifier in a convex combination and establish tight upper bound on it excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse we use a boosting type algorithm of optimal aggregation to develop aggregate classifier of activation pattern in fmri based on locally trained svm classifier the aggregation coefficient are then used to design a boosting map of the brain needed to identify the region with most significant im pact on classification 
in the standard feature selection problem we are given a fixed set of candidate feature for use in a learning problem and must select a subset that will be used to train a model that is a good a possible according to some criterion in this paper we present an interesting and useful variant the online feature selection problem in which instead of all feature being available from the start feature arrive one at a time the learner s task is to select a subset of feature and return a corresponding model at each time step which is a good a possible given the feature seen so far we argue that existing feature selection method do not perform well in this scenario and describe a promising alternative method based on a stagewise gradient descent technique which we call grafting 
we consider the situation where a number of agent are distributed and each of them collect a data sequence generated according to an unknown probability distribution here each of the distribution is specified by common parameter and individual parameter e g a normal distribution with an identical mean and a different variance here we introduce a notion of an information consortium which is a framework where the agent cannot show raw data to one another but they like to enjoy significant information gain for estimating the respective distribution such an information consortium ha recently received much interest in a broad range of area including financial risk management ubiquitous network mining etc in this paper we are concerned with the following three issue how to design a collaborative strategy for agent to estimate the respective distribution in the information consortium characterizing when each agent ha a benefit in term of information gain for estimating it distribution or information loss for predicting future data and charracterizing how much benefit each agent obtains in this paper we yield a statistical formulation of information consortium and solve all of the above three problem for a general form of probability distribution specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit 
given an n n grid of square where each square ha a count and an underlying population our goal is to find the square region with the highest density and to calculate it significance by randomization any density measure d dependent on the total count and total population of a region can be used for example if each count represents the number of disease case occurring in that square we can use kulldorff s spatial scan statistic dk to find the most significant spatial disease cluster a naive approach to finding the maximum density region requires o n time and is generally computationally infeasible we present a novel algorithm which partition the grid into overlapping region bound the maximum score of subregions contained in each region and prune region which cannot contain the maximum density region for sufficiently dense region this method find the maximum density region in optimal o n time in practice resulting in significant x speedup 
the goal of feature induction is to automatically create nonlinear combination of existing feature a additional input feature to improve classification accuracy typically nonlinear feature are introduced into a support vector machine svm through a nonlinear kernel function one disadvantage of such an approach is that the feature space induced by a kernel function is usually of high dimension and therefore will substantially increase the chance of over fitting the training data another disadvantage is that nonlinear feature are induced implicitly and therefore are difficult for people to understand which induced feature are critical to the classification performance in this paper we propose a boosting style algorithm that can explicitly induces important nonlinear feature for svms we present empirical study with discussion to show that this approach is effective in improving classification accuracy for svms the comparison with an svm model using nonlinear kernel also indicates that this approach is effective and robust particularly when the number of training data is small 
we claim and present argument to the effect that a large class of manifold learning algorithm that are essentially local and can be framed a kernel learning algorithm will suffer from the curse of dimensionality at the dimension of the true underlying manifold this observation suggests to explore non local manifold learning algorithm which attempt to discover shared structure in the tangent plane at different position a criterion for such an algorithm is proposed and experiment estimating a tangent plane prediction function are presented showing it advantage with respect to local manifold learning algorithm it is able to generalize very far from training data on learning handwritten character image rotation where a local non parametric method fails 
according to a widely held view neuron in lateral geniculate nucleus lgn operate on visual stimulus in a linear fashion there is ample evidence however that lgn response are not entirely linear to account for nonlinearities we propose a model that synthesizes more than year of research in the field model neuron have a linear receptive field and a nonlinear divisive suppressive field the suppressive field computes local root meansquare contrast to test this model we recorded response from lgn of anesthetized paralyzed cat we estimate model parameter from a basic set of measurement and show that the model can accurately predict response to novel stimulus the model might serve a the new standard model of lgn response it specifies how visual processing in lgn involves both linear filtering and divisive gain control 
abstract applicability of ilp to real world problemsis constrained by the high dimensionality ofilp task this paper proposes to reducethe dimensionality of the ilp example spaceby bringing feature subset selection to therealm of ilp seen a a black box the methodreduces ilp example in the form of nonrecursivedatalog clause by removing literalsjudged irrelevant for the ilp task athand the approach exploit existing avlfeature selection f algorithm by changingthe 
in this paper we explore effect of various feature selection algorithm on document classification performance we propose to use two possibly distinct linear classifier one used exclusively for feature selection in order to obtain the feature space for training the second classifier using possibly a different training set the resulting classifier is used to classify new document experiment show that feature selection based on the linear svm algorithm combine well with different type of classifier based on the experimental result we make a conjecture that it is the level of sophis tication at which the scoring method take into account information about feature rather than it compatibility with the classifier in term of it design that make the feature selection method more or le successful 
to enable information integration schema matching is a critical step for discovering semantic correspondence of attribute across heterogeneous source while complex matchings are common because of their far more complex search space most existing technique focus on simple matchings to tackle this challenge this paper take a conceptually novel approach by viewing schema matching a correlation mining for our task of matching web query interface to integrate the myriad database on the internet on this deep web query interface generally form complex matchings between attribute group e g author corresponds to first name last name in the book domain we observe that the co occurrence pattern across query interface often reveal such complex semantic relationship grouping attribute e g first name last name tend to be co present in query interface and thus positively correlated in contrast synonym attribute are negatively correlated because they rarely co occur this insight enables u to discover complex matchings by a correlation mining approach in particular we develop the dcm framework which consists of data preparation dual mining of positive and negative correlation and finally matching selection unlike previous correlation mining algorithm which mainly focus on finding strong positive correlation our algorithm care both positive and negative correlation especially the subtlety of negative correlation due to it special importance in schema matching this lead to the introduction of a new correlation measure h measure distinct from those proposed in previous work we evaluate our approach extensively and the result show good accuracy for discovering complex matchings 
supervised learning technique for text classification often require a large number of labeled example to learn accurately one way to reduce the amount of labeled data required is to develop algorithm that can learn effectively from a small number of labeled example augmented with a large number of unlabeled example current text learning technique for combining labeled and unlabeled such a em and co training are mostly applicable for classification task with a small number of class and do not scale up well for large multiclass problem in this paper we develop a framework to incorporate unlabeled data in the error correcting output coding ecoc setup by first decomposing multiclass problem into multiple binary problem and then using co training to learn the individual binary classification problem we show that our method is especially useful for text classification task involving a large number of category and outperforms other semi supervised learning technique such a em and co training in addition to being highly accurate this method utilizes the hamming distance from ecoc to provide high precision result we also present result with algorithm other than co training in this framework and show that co training is uniquely suited to work well within ecoc 
we present test result from spike timing correlation learning experiment carried out with silicon neuron with stdp spike timing dependent plasticity synapsis the weight change scheme of the stdp synapsis can be set to either weight independent or weight dependent mode we present result that characterise the learning window implemented for both mode of operation when presented with spike train with dieren t type of synchronisation the neuron develop bimodal weight distribution we also show that a layered network of silicon spiking neuron with stdp synapsis can perform hierarchical synchrony detection 
we report on an automated runtime anomaly detection method at the application layer of multi node computer system although several network management system are available in the market none of them have sufficient capability to detect fault in multi tier web based system with redundancy we model a web based system a a weighted graph where each node represents a service and each edge represents a dependency between service since the edge weight vary greatly over time the problem we address is that of anomaly detection from a time sequence of graph in our method we first extract a feature vector from the adjacency matrix that represents the activity of all of the service the heart of our method is to use the principal eigenvector of the eigenclusters of the graph then we derive a probability distribution for an anomaly measure defined for a time series of directional data derived from the graph sequence given a critical probability the threshold value is adaptively updated using a novel online algorithm we demonstrate that a fault in a web application can be automatically detected and the faulty service are identified without using detailed knowledge of the behavior of the system 
extensive study have shown that mining microarray data set is important in bioinformatics research and biomedical application in this paper we explore a novel type of gene sample time microarray data set which record the expression level of various gene under a set of sample during a series of time point in particular we propose the mining of coherent gene cluster from such data set each cluster contains a subset of gene and a subset of sample such that the gene are coherent on the sample along the time series the coherent gene cluster may identify the sample corresponding to some phenotype e g disease and suggest the candidate gene correlated to the phenotype we present two efficient algorithm namely the sample gene search and the gene sample search to mine the complete set of coherent gene cluster we empirically evaluate the performance of our approach on both a real microarray data set and synthetic data set the test result have shown that our approach are both efficient and effective to find meaningful coherent gene cluster 
the formation of disulphide bridge among cysteine is an important feature of protein structure here we develop new method for the prediction of disulphide bond connectivity we first build a large curated data set of protein containing disulphide bridge and then use dimensional recursive neural network to predict bonding probability between cysteine pair these probability in turn lead to a weighted graph matching problem that can be addressed efficiently we show how the method consistently achieves better result than previous approach on the same validation data in addition the method can easily cope with chain with arbitrary number of bonded cysteine therefore it overcomes one of the major limitation of previous approach restricting prediction to chain containing no more than oxidized cysteine the method can be applied both to situation where the bonded state of each cysteine is known or unknown in which case bonded state can be predicted with precision and recall the method also yield an estimate for the total number of disulphide bridge in each chain 
significant progress in clustering ha been achieved by algorithm that are based on pairwise affinity between the datapoints in particular spectral clustering method have the advantage of being able to divide arbitrarily shaped cluster and are based on efficient eigenvector calculation however spectral method lack a straightforward probabilistic interpretation which make it difficult to automatically set parameter using training data in this paper we use the previously proposed typical cut framework for pairwise clustering we show an equivalence between calculating the typical cut and inference in an undirected graphical model we show that for clustering problem with hundred of datapoints exact inference may still be possible for more complicated datasets we show that loopy belief propagation bp and generalized belief propagation gbp can give excellent result on challenging clustering problem we also use graphical model to derive a learning algorithm for affinity matrix based on labeled data 
a animal interact with their environment they must constantly update estimate about their state bayesian model combine prior probability a dynamical model and sensory evidence to update estimate optimally these model are consistent with the result of many diverse psychophysical study however little is known about the neural representation and manipulation of such bayesian information particularly in population of spiking neuron we consider this issue suggesting a model based on standard neural architecture and activation we illustrate the approach on a simple random walk example and apply it to a sensorimotor integration task that provides a particularly compelling example of dynamic probabilistic computation bayesian model have been used to explain a gamut of experimental result in task which require estimate to be derived from multiple sensory cue these include a wide range of psychophysical study of perception motor action and decision making central to bayesian inference is that computation are sensitive to uncertainty about afferent and efferent quantity arising from ignorance noise or inherent ambiguity e g the aperture problem and that these uncertainty change over time a information accumulates and dissipates understanding how neuron represent and manipulate uncertain quantity is therefore key to understanding the neural instantiation of these bayesian inference 
kernel method have gained a great deal of popularity in the machine learning community a a method to learn indirectly in highdimensional feature space those interested in relational learning have recently begun to cast learning from structured and relational data in term of kernel operation we describe a general family of kernel function built up from a description language of limited expressivity and use it to study the benet and drawback of kernel learning in relational domain learning with kernel in this family directly model learning over an expanded feature space constructed using the same description language this allows u to examine issue of time complexity in term of learning with these and other relational kernel and how these relate to generalization ability the tradeos between using kernel in a very high dimensional implicit space versus a restricted feature space is highlighted through two experiment in bioinformatics and in natural language processing 
the marine corp project albert seek to model complex phenomenon by observing the behavior of relatively simple simulation over thousand of run a rich data base is developed by running the simulation thousand of time varying the agent and scenario input parameter a well a the random seed exploring this result space may provide significant insight into nonlinear surprising and emergent behavior capturing these result can provide a path for making the result usable for decision support to a military commander this paper present two data mining approach rule discovery and bayesian network for analyzing the albert simulation data the first approach generates rule from the data and then us them to create descriptive model the second generates bayesian network which provide a quantitative belief model for decision support both of these approach a well a the project albert simulation are framed in the context of a system architecture for decision support 
we describe a class of causal discrete latent variable model called multiple multiplicative factor model mmfs a data vector is represented in the latent space a a vector of factor that have discrete non negative expression level each factor proposes a distribution over the data vector the distinguishing feature of mmfs is that they combine the factor proposed distribution multiplicatively taking into account factor expression level the product formulation of mmfs allow factor to specialize to a subset of the item while the causal generative semantics mean mmfs can readily accommodate missing data this make mmfs distinct from both directed model with mixture semantics and undirected product model in this paper we present empirical result from the collaborative filtering domain showing that a binary multinomial mmf model match the performance of the best existing model while learning an interesting latent space description of the user 
we propose to selectively remove example from the training set using probabilistic estimate related to editing algorithm devijver and kittler this heuristic procedure aim at creating a separable distribution of training example with minimal impact on the position of the decision boundary it break the linear dependency between the number of svs and the number of training example and sharply reduces the complexity of svms during both the training and prediction stage 
go is an ancient oriental game whose complexity ha defeated attempt to automate it we suggest using probability in a bayesian sense to model the uncertainty arising from the vast complexity of the game tree we present a simple conditional markov random eld model for predicting the pointwise territory outcome of a game the topology of the model reects the spatial structure of the go board we describe a version of the swendsen wang process for sampling from the model during learning and apply loopy belief propagation for rapid inference and prediction the model is trained on several hundred record of professional game our experimental result indicate that the model successfully learns to predict territory despite it simplicity 
reminder system support people with impaired prospective memory and or executive function by providing them with reminder of their functional daily activity we integrate temporal constraint reasoning with reinforcement learning rl to build an adaptive reminder system and in a simulated environment demonstrate that it can personalize to a user and adapt to both shortand long term change in addition to advancing the application domain our integrated algorithm contributes to research on temporal constraint reasoning by showing how rl can select an optimal policy from amongst a set of temporally consistent one and it contributes to the work on rl by showing how temporal constraint reasoning can be used to dramatically reduce the space of action from which an rl agent need to learn 
in this paper we tackle a real world problem the search of a function to evaluate the merit of beef cattle a meat producer the independent variable represent a set of live animal measurement while the output cannot be captured with a single number since the available expert tend to ass each animal in a relative way comparing animal with the other partner in the same batch therefore this problem can not be solved by mean of regression method our approach is to learn the preference of the expert when they order small group of animal thus the problem can be reduced to a binary classification and can be dealt with a support vector machine svm improved with the use of a feature subset selection f method we develop a method based on recursive feature elimination rfe that employ an adaptation of a metric based method devised for model selection adj finally we discus the extension of the resulting method to more general setting and provide a comparison with other possible alternative 
semantic taxonomy such a wordnet provide a rich source of knowledge for natural language processing application but are expensive to build maintain and extend motivated by the problem of automatically constructing and extending such taxonomy in this paper we present a new algorithm for automatically learning hypernym is a relation from text our method generalizes earlier work that had relied on using small number of hand crafted regular expression pattern to identify hypernym pair using dependency path feature extracted from parse tree we introduce a general purpose formalization and generalization of these pattern given a training set of text containing known hypernym pair our algorithm automatically extract useful dependency path and applies them to new corpus to identify novel pair on our evaluation task determining whether two noun in a news article participate in a hypernym relationship our automatically extracted database of hypernym attains both higher precision and higher recall than wordnet 
recently relevance vector machine rvm have been fashioned from a sparse bayesian learning sbl framework to perform supervised learning using a weight prior that encourages sparsity of representation the methodology incorporates an additional set of hyperparameters governing the prior one for each weight and then adopts a specific a pproximation to the full marginalization over all weight and hyperparameters despite it empirical success however no rigorous motivation for this particular approximation is currently available to addre s this issue we demonstrate that sbl can be recast a the application of a rigorous variational approximation to the full model by expressing the prior in a dual form this formulation obviates the necessity of assuming any hyperpriors and lead to natural intuitive explanation of why spar sity is achieved in practice 
a new approach to ensemble learning is introduced that take ranking rather than classification a fundamental leading to model on the symmetric group and it cosets the approach us a generalization of the mallow model on permutation to combine multiple input ranking application include the task of combining the output of multiple search engine and multiclass or multilabel classification where a set of input classifier is viewed a generating a ranking of class label experiment for both type of application are presented 
problem in which abnormal or novel situation should be detected can be approached by describing the domain of the class of typical example these application come from the area of machine diagnostics fault detection illness identification or in principle refer to any problem where little knowledge is available outside the typical class in this paper we explain why proximity are natural representation for domain descriptor and we propose a simple one class classifier for dissimilarity representation by the use of linear programming an efficient one class description can be found based on a small number of prototype object this classifier can be made more robust by transforming the dissimilarity and cheaper to compute by using a reduced representation set finally a comparison to a comparable one class classifier by campbell and bennett is given 
we propose a model for natural image in which the probability of an image is proportional to the product of the probability of some filter output we encourage the system to find sparse feature by using a studentt distribution to model each filter output if the t distribution is used to model the combined output of set of neurally adjacent filter the system learns a topographic map in which the orientation spatial frequency and location of the filter change smoothly across the map even though maximum likelihood learning is intractable in our model the product form allows a relatively efficient learning procedure that work well even for highly overcomplete set of filter once the model ha been learned it can be used a a prior to derive the iterated wiener filter for the purpose of denoising image 
we give an unified convergence analysis of ensemble learning method including e g adaboost logistic regression and the least squareboost algorithm for regression these method have in common that they iteratively call a base learning algorithm which return hypothesis that are then linearly combined we show that these method are related to the gauss southwell method known from numerical optimization and state non asymptotical convergence result for all these method our analysis includes norm regularized cost function leading to a clean and general way to regularize ensemble learning 
we propose a framework for classifier design based on discriminative density for representation of the difference of the class conditional distribution in a way that is optimal for classification the density are selected from a parametrized set by constrained maximization of some objective function which measure the average bounded difference i e the contrast between discriminative density we show that maximization of the contrast is equivalent to minimization of an approximation of the bayes risk therefore using suitable class of probability density function the resulting maximum contrast classifier mccs can approximate the bayes rule for the general multiclass case in particular for a certain parametrization of the density function we obtain mccs which have the same functional form a the well known support vector machine svms we show that mcc training in general requires some nonlinear optimization but under certain condition the problem is concave and can be tackled by a single linear program we indicate the close relation between svmand mcc training and in particular we show that linear programming machine can be viewed a an approximate realization of mccs in the experiment on benchmark data set the mcc show a competitive classification performance 
in order to understand adaboost s dynamic especially it ability to maximize margin we derive an associated simplified nonlinear iterated map and analyze it behavior in low dimensional case we find stable cycle for these case which can explicitly be used to solve for adaboost s output by considering adaboost a a dynamical system we are able to prove r atsch and warmuth s conjecture that adaboost may fail to converge to a maximal margin combined classifier when given a nonoptimal weak learning algorithm adaboost is known to be a coordinate descent method but other known algorithm that explicitly aim to maximize the margin such a adaboost and arc gv are not we consider a differentiable function for which coordinate ascent will yield a maximum margin solution we then make a simple approximation to derive a new boosting algorithm whose update are slightly more aggressive than those of arc gv 
we consider an online learning scenario in which the learner can make prediction on the basis of a fixed set of expert we derive up per and lower relative loss bound for a class of universal learning algorithm involving a switching dynamic over the choice of the expert on the basis of the performance bound we provide the optimal a priori discretization of the switching rate parameter that governs the switching dynamic we demonstrate the algorithm in the context of wireless network 
synchronous reinforcement learning rl algorithm with linear function approximation are representable a inhomogeneous matrix iteration of a special form schoknecht merke in this paper we state condition of convergence for general inhomogeneous matrix iteration and prove that they are both necessary and sufficient this result extends the work presented in schoknecht merke where only a sufficient condition of convergence wa proved a the condition of convergence is necessary and sufficient the new result is suitable to prove convergence and divergence of rl algorithm with function approximation we use the theorem to deduce a new concise proof of convergence for the synchronous residual gradient algorithm baird moreover we derive a counterexample for which the uniform rl algorithm merke schoknecht diverges this yield a negative answer to the open question if the uniform rl algorithm converges for arbitrary multiple transition 
mining microarray gene expression data is an important research topic in bioinformatics with broad application while most of the previous study focus on clustering either gene or sample it is interesting to ask whether we can partition the complete set of sample into exclusive group called phenotype and find a set of informative gene that can manifest the phenotype structure in this paper we propose a new problem of simultaneously mining phenotype and informative gene from gene expression data some statistic based metric are proposed to measure the quality of the mining result two interesting algorithm are developed the heuristic search and the mutual reinforcing adjustment method we present an extensive performance study on both real world data set and synthetic data set the mining result from the two proposed method are clearly better than those from the previous method they are ready for the real world application between the two method the mutual reinforcing adjustment method is in general more scalable more effective and with better quality of the mining result 
the response of cortical sensory neuron are notoriously variable with the number of spike evoked by identical stimulus varying significantly from trial to trial this variability is most often interpreted a noise purely detrimental to the sensory system in this paper we propose an alternative view in which the variability is related to the uncertainty about world parameter which is inherent in the sensory stimulus specifically the response of a population of neuron are interpreted a stochastic sample from the posterior distribution in a latent variable model in addition to giving theoretical argument supporting such a representational scheme we provide simulation suggesting how some aspect of response variability might be understood in this framework 
recent theoretical result have shown that improved bound on generalization error of classiers can be obtained by explicitly taking the observed margin distribution of the training data into account currently algorithm used in practice do not make use of the margin distribution and are driven by optimization with respect to the point that are closest to the hyperplane this paper enhances earlier theoretical result and derives a practical data dependent complexity measure for learning the new complexity measure is a function of the observed margin distribution of the data and can be used a we show a a model selection criterion we then present the margin distribution optimization mdo learning algorithm that directly optimizes this complexity measure empirical evaluation of mdo demonstrates that it consistently outperforms svm 
mutual boosting is a method aimed at incorporating contextual information to augment object detection when multiple detector of object and part are trained in parallel using adaboost object detector might use the remaining intermediate detector to enrich the weak learner set this method generalizes the efficient feature suggested by viola and jones thus enabling information inference between part and object in a compositional hierarchy in our experiment eye nose mouthand face detector are trained using the mutual boosting framework result show that the method outperforms application overlooking contextual information we suggest that achieving contextual integration is a step toward human like detection capability 
recent proposal to apply data mining system to problem in law enforcement national security and fraud detection have attracted both medium attention and technical critique of their expected accuracy and impact on privacy unfortunately the majority of technical critique have been based on simplistic assumption about data classifier inference procedure and the overall architecture of such system we consider these critique in detail and we construct a simulation model that more closely match realistic system we show how both the accuracy and privacy impact of a hypothetical system could be substantially improved and we discus the necessary and sufficient condition for this improvement to be achieved this analysis is neither a defense nor a critique of any particular system concept rather our model suggests alternative technical design that could mitigate some concern but also raise more specific condition that must be met for such system to be both accurate and socially desirable 
many classification algorithm including the support vector machine boosting and logistic regression can be viewed a minimum contrast method that minimize a convex surrogate of the loss function we characterize the statistical consequence of using such a surrogate by providing a general quantitative relationship between the risk a assessed using the loss and the risk a assessed using any nonnegative surrogate loss function we show that this relationship give nontrivial bound under the weakest possible condition on the loss function that it satisfy a pointwise form of fisher consistency for classification the relationship is based on a variational transformation of the loss function that is easy to compute in many application we also present a refined version of this result in the case of low noise finally we present application of our result to the estimation of convergence rate in the general setting of function class that are scaled hull of a finite dimensional base class 
many difficult visual perception problem like d human motion estimation can be formulated in term of inference using complex generative model defined over high dimensional state space despite progress optimizing such model is difficult because prior knowledge cannot be flexibly integrated in order to reshape an initially designed representation space nonlinearities inherent sparsity of high dimensional training set and lack of global continuity make dimensionality reduction challenging and low dimensional search inefficient to address these problem we present a learning and inference algorithm that restricts visual tracking to automatically extracted non linearly embedded low dimensional space this formulation produce a layered generative model with reduced state representation that can be estimated using efficient continuous optimization method our prior flattening method allows a simple analytic treatment of low dimensional intrinsic curvature constraint and allows consistent interpolation operation we analyze reduced manifold for human interaction activity and demonstrate that the algorithm learns continuous generative model that are useful for tracking and for the reconstruction of d human motion in monocular video 
merchant selling product on the web often ask their customer to review the product that they have purchased and the associated service a e commerce is becoming more and more popular the number of customer review that a product receives grows rapidly for a popular product the number of review can be in hundred or even thousand this make it difficult for a potential customer to read them to make an informed decision on whether to purchase the product it also make it difficult for the manufacturer of the product to keep track and to manage customer opinion for the manufacturer there are additional difficulty because many merchant site may sell the same product and the manufacturer normally produce many kind of product in this research we aim to mine and to summarize all the customer review of a product this summarization task is different from traditional text summarization because we only mine the feature of the product on which the customer have expressed their opinion and whether the opinion are positive or negative we do not summarize the review by selecting a subset or rewrite some of the original sentence from the review to capture the main point a in the classic text summarization our task is performed in three step mining product feature that have been commented on by customer identifying opinion sentence in each review and deciding whether each opinion sentence is positive or negative summarizing the result this paper proposes several novel technique to perform these task our experimental result using review of a number of product sold online demonstrate the effectiveness of the technique 
we consider multi agent system whose agent compete for resource by striving to be in the minority group the agent adapt to the environment by reinforcement learning of the preference of the policy they hold diversity of preference of policy is introduced by adding random bias to the initial cumulative payoff of their policy we explain and provide evidence that agent cooperation becomes increasingly important when diversity increase analysis of these mechanism yield excellent agreement with simulation over nine decade of data 
the problem of finding a specified pattern in a time series database i e query by content ha received much attention and is now a relatively mature field in contrast the important problem of enumerating all surprising or interesting pattern ha received far le attention this problem requires a meaningful definition of surprise and an efficient search technique all previous attempt at finding surprising pattern in time series use a very limited notion of surprise and or do not scale to massive datasets to overcome these limitation we introduce a novel technique that defines a pattern surprising if the frequency of it occurrence differs substantially from that expected by chance given some previously seen data 
we argue that k mean and deterministic annealing algorithm for geometric clustering can be derived from the more general information bottleneck approach if we cluster the identity of data point to preserve information about their location the set of optimal solution is massively degenerate but if we treat the equation that define the optimal solution a an iterative algorithm then a set of smooth initial condition selects solution with the desired geometrical property in addition to conceptual unification we argue that this approach can be more efficient and robust than classic algorithm 
most learning method assume that the training set is drawn randomly from the population to which the learned model is to be applied however in many application this assumption is invalid for example lending institution create model of who is likely to repay a loan from training set consisting of people in their record to whom loan were given in the past however the institution approved loan application previously based on who wa thought unlikely to default learning from only approved loan yield an incorrect model because the training set is a biased sample of the general population of applicant the issue of including rejected sample in the learning process or alternatively using rejected sample to adjust a model learned from accepted sample only is called reject inference the main contribution of this paper is a systematic analysis of different case that arise in reject inference with explanation of which case arise in various real world situation we use bayesian network to formalize each case a a set of conditional independence relationship and identify eight case including the familiar missing completely at random mcar missing at random mar and missing not at random mnar case for each case we present an overview of available learning algorithm these algorithm have been published in separate field of research including epidemiology econometrics clinical trial evaluation sociology and credit scoring our second major contribution is to describe these algorithm in a common framework 
this article address the issue of colour classification and collision detection a they occur in the legged league robot soccer environment of robocup we show how the method of one class classification with support vector machine svms can be applied to solve these task satisfactorily using the limited hardware capacity of the prescribed sony aibo quadruped robot the experimental evaluation show an improvement over our previous method of ellipse fitting for colour classification and the statistical approach used for collision detection 
in this paper we present a graphical model for protein secondary structure prediction this model extends segmental semi markov model ssmm to exploit multiple sequence alignment profile which contain information from evolutionarily related sequence a novel parameterized model is proposed a the likelihood function for the ssmm to capture the segmental conformation by incorporating the information from long range interaction in sheet this model is capable of carrying out inference on contact map the numerical result on benchmark data set show that incorporating the profile result in substantial improvement and the generalization performance is promising 
we consider the problem of reconstructing pattern from a feature map learning algorithm using kernel to operate in a reproducing kernel hilbert space rkhs express their solution in term of input point mapped into the rkhs we introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding pattern in the input space aka pre image and review it performance in several application requiring the construction of pre im age the introduced technique avoids difficult and or unstable numerical optimization is easy to implement and unlike previous method permit the computation of pre image in discrete input space 
we investigate a general semi markov decision process smdp framework for modeling concurrent decision making where agent learn optimal plan over concurrent temporally extended action we introduce three type of parallel termination scheme all any and continue and theoretically and experimentally compare them 
boosting is a popular approach for building accurate classifier despite the initial popular belief boosting algorithm do exhibit overfitting and are sensitive to label noise part of the sensitivity of boosting algorithm to outlier and noise can be attributed to the unboundedness of the margin based loss function that they employ in this paper we describe two leveraging algorithm that build on boosting technique and employ a bounded loss function of the margin the first algorithm interleaf the expectation maximization em algorithm with boosting step the second algorithm decomposes a non convex loss into a difference of two convex loss we prove that both algorithm converge to a stationary point we also analyze the generalization property of the algorithm using the rademacher complexity we describe experiment with both synthetic data and natural data ocr and text that demonstrate the merit of our framework in particular robustness to outlier 
in this paper we show that many kernel method can be adapted to deal with indefinite kernel that is kernel which are not positive semidefinite they do not satisfy mercer s condition and they induce associated functional space called reproducing kernel kre icaron n space rkks a generalization of reproducing kernel hilbert space rkhs machine learning in rkks share many nice property of learning in rkhs such a orthogonality and projection however since the kernel are indefinite we can no longer minimize the loss instead we stabilize it we show a general representer theorem for constrained stabilization and prove generalization bound by computing the rademacher average of the kernel class we list several example of indefinite kernel and investigate regularization method to solve spline interpolation some preliminary experiment with indefinite kernel for spline smoothing are reported for truncated spectral factorization landweber fridman iteration and mr ii 
we develop a system theoretical treatment of a behavioural system that interacts with it environment in a closed loop situation such that it motor action influence it sensor input the simplest form of a feedback is a reflex reflex occur always too late i e only after a unpleasant painful dangerous reflex eliciting sensor event ha occurred this defines an objective problem which can be solved if another sensor input exists which can predict the primary reflex and can generate an earlier reaction in contrast to previous approach our linear learning algorithm allows for an analytical proof that this system learns to apply feedforward control with the result that slow feedback loop are replaced by their equivalent feed forward controller creating a forward model in other word learning turn the reactive system into a pro active system by mean of a robot implementation we demonstrate the applicability of the theoretical result which can be used in a variety of different area in physic and engineering 
many real world domain are relational in nature consisting of a set of object related to each other in complex way this paper focus on predicting the existence and the type of link between entity in such domain we apply the relational markov networkframework of taskar et al to define a joint probabilistic model over the entire link graph entity attribute and link the application of the rmn algorithm to this task requires the definition of pr obabilistic pattern over subgraph structure we apply this method to two new relational datasets one involving university webpage and the other a social network we show that the collective classification approach of rmns and the intr oduction of subgraph pattern over link label provide significant improvement s in accuracy over flat classification which attempt to predict each link in isola tion 
many interesting multiclass problem can be cast in the general framework of label ranking defined on a given set of class the eva luation for such a ranking is generally given in term of the number of violated order constraint between class in this paper we propose the preference learning modelas a unifying framework to model and solve a large class of multiclass problem in a large margin perspective in addition an original kernel based method is proposed and evaluated on a ranking dataset with state of the art result 
belief propagation on cyclic graph is an efficient algorithm for computing approximate marginal probability distribution over single node and neighboring node in the graph in this paper we propose two new algorithm for approximating joint probability of arbitrary pair of node and prove a number of desirable property that these estimate fulfill the first algorithm is a propagation algorithm which is shown to converge if belief propagation converges to a stable fixed point the second algorithm is based on matrix inversion experiment compare a number of competing method 
the issue of cross channel integration and customer life time value modeling are two of the most important topic surrounding customer relationship management crm today in the present paper we describe and evaluate a novel solution that treat these two important issue in a unified framework of markov decision process mdp in particular we report on the result of a joint project between ibm research and saks fifth avenue to investigate the applicability of this technology to real world problem the business problem we use a a testbed for our evaluation is that of optimizing direct mail campaign mailing for maximization of profit in the store channel we identify a problem common to cross channel crm which we call the cross channel challenge due to the lack of explicit linking between the marketing action taken in one channel and the customer response obtained in another we provide a solution for this problem based on old and new technique in reinforcement learning our in laboratory experimental evaluation using actual customer interaction data show that a much a to per cent increase in the store profit can be expected by employing a mailing policy automatically generated by our methodology these result confirm that our approach is valid in dealing with the cross channel crm scenario in the real world 
steganography involves hiding message in innocuous medium such a image while steganalysis is the field of detecting these secret message the ultimate goal of steganalysis is two fold making a binary classification of a file a stego bearing or innocent and secondly locating the hidden message with an aim to extracting sterilizing or manipulating it almost all steganalysis approach known a attack focus on the first of these two issue in this paper we explore the difficult related problem given that we know an image file contains steganography locate which pixel contain the message we treat the hidden message location problem a outlier detection using probability energy measure of image motivated by the image restoration community pixel contributing the most to the energy calculation of an image are deemed outlier typically of the top third of one percent of most energized pixel outlier we find that are stego bearing in color image and in grayscale image in all image type only of all pixel are stego bearing indicating our technique provides a substantial lift over random guessing 
we describe a method for learning sparse multiscale image representation using a sparse prior distribution over the basis function coecien t the prior consists of a mixture of a gaussian and a dirac delta function and thus encourages coecien t to have exact zero value coecien t for an image are computed by sampling from the resulting posterior distribution with a gibbs sampler the learned basis is similar to the steerable pyramid basis and yield slightly higher snr for the same number of active coecien t denoising using the learned image model is demonstrated for some standard test image with result that compare favorably with other denoising method 
this paper analyzes generalization of the classic rescorla wagner rw learning algorithm and study their relationship to maximum likelihood estimation of causal parameter we prove that the parameter of two popular causal model p and pc can be learnt by the same generalized linear rescorla wagner glrw algorithm provided genericity condition apply we characterize the fixed point of these glrw algorithm and calculate the fluctuation about them assuming that the input is a set of i i d sample from a fixed unknown distribution we describe how to determine convergence condition and calculate convergence rate for the glrw algorithm under these condition 
this paper present a neuromorphic model of two olfactory signalprocessing primitive chemotopic convergence of olfactory receptor neuron and center on off surround lateral inhibition in the olfactory bulb a self organizing model of receptor convergence onto glomerulus is used to generate a spatially organized map an olfactory image this map serf a input to a lattice of spiking neuron with lateral connection the dynamic of this recurrent network transforms the initial olfactory image into a spatio temporal pattern that evolves and stabilizes into odorand intensity coding attractor the model is validated using experimental data from an array of temperature modulated gas sensor our result are consistent with recent neurobiological finding on the antennal lobe of the honeybee and the locust 
predictive state representation psrs have recently been proposed a an alternative to partially observable markov decision process pomdps for representing the state of a dynamical system littman et al we present a learning algorithm that learns a psr from observational data our algorithm produce a variant of psrs called transformed predictive state representation tpsrs we provide an efficient principal component based algorithm for learning a tpsr and show that tpsrs can perform well in comparison to hidden markov model learned with baum welch in a real world robot tracking task for low dimensional representation and long prediction horizon 
the problem of learning a sparse conic combination of kernel function or kernel matrix for classification or regression can be ac hieved via the regularization by a block norm in this paper we present an algorithm that computes the entire regularization path for th ese problem the path is obtained by using numerical continuation technique and involves a running time complexity that is a constant time the complexity of solving the problem for one value of the regularization parameter working in the setting of kernel linear regression and kernel logistic regression we show empirically that the effect of the block norm regularization differs notably from the non block norm regularization commonly used for variable selection and that the regularization path is of particular value in the block case 
we consider a statistical framework for learning in a class of network of spiking neuron our aim is to show how optimal local learningrulescanbereadilyderivedoncetheneuraldynamicsand desired functionality of the neural assembly have been specifled in contrast to other model which assume sub optimal learning rule withinthisframeworkwederivelocalrulesforlearningtemporalsequencesinamodelofspikingneuronsanddemonstrateits superior performance to correlation hebbian based approach we further show how to include mechanism such a synaptic depression and outline how the framework is readily extensible to learninginnetworksofhighlycomplexspikingneurons astochasticquantalvesiclereleasemechanismisconsideredandimplications on the complexity of learning discussed 
a the complexity of distributed computing system increase system management task require significantly higher level of automation example include diagnosis and prediction based on real time stream of computer event setting alarm and performing continuous monitoring the core of autonomic computing a recently proposed initiative towards next generation it system capable of self healing is the ability to analyze data in real time and to predict potential problem the goal is to avoid catastrophic failure through prompt execution of remedial action this paper describes an attempt to build a proactive prediction and control system for large cluster we collected event log containing various system reliability availability and serviceability ra event and system activity report sars from a node cluster system for a period of one year the raw system health measurement contain a great deal of redundant event data which is either repetitive in nature or misaligned with respect to time we applied a filtering technique and modeled the data into a set of primary and derived variable these variable used probabilistic network for establishing event correlation through prediction algorithm we also evaluated the role of time series method rule based classification algorithm and bayesian network model in event prediction based on historical data our result suggest that it is feasible to predict system performance parameter sars with a high degree of accuracy using time series model rule based classification technique can be used to extract machine event signature to predict critical event with up to accuracy 
there exists a huge demand for multimedia good and service in the internet currently available bandwidth speed can support sale of downloadable content like cd e book etc a well a service like video on demand in the future such service will be prevalent in the internet since cost are typically fixed maximizing revenue can maximize profit a primary determinant of revenue in such e content market is how much value the customer associate with the content though marketing survey are useful they cannot adapt to the dynamic nature of the internet market in this work we examine how to learn customer valuation in close to real time our contribution in this paper are threefold we develop a probabilistic model to describe customer behavior we develop a framework for pricing e content based on basic economic principle and we propose a price discovering algorithm that learns customer behavior parameter and suggests price to an e content provider we validate our algorithm using simulation our simulation indicate that our algorithm generates revenue close to the maximum expectation further they also indicate that the algorithm is robust to transient customer behavior 
abstract we describe a method that computes provably exact map estimate for a subclass of problem on graph with cycle the basic idea is to represent the original problem on the graph with cycle a a convex combination of tree structured problem a convexity argument the n guarantee that the optimal value of the original problem i e the log probability of the map assignment is upper bounded by the combined optimal value of the tree problem we prove that this upper bound is met with equality if and only if the tree problem share an optimal con figuration in common an important implication is that any such shared configuration must also be the map configuration for the original pr oblem next we develop a tree reweighted max product algorithm for attempting to find convex combination of tree structured problem that share a common optimum we prove that these message passing update always have a fixed point and give necessary and sufficient cond itions for the fixed point to yield the exact map estimate an attractive feature of our analysis is that it generalizes naturally to convex co mbinations of hypertree structured distribution 
defining outlier by their distance to neighboring example is a popular approach to finding unusual example in a data set recently much work ha been conducted with the goal of finding fast algorithm for this task we show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used we test our algorithm on real high dimensional data set with million of example and show that the near linear scaling hold over several order of magnitude our average case analysis suggests that much of the efficiency is because the time to process non outlier which are the majority of example doe not depend on the size of the data set 
we present a framework for sparse gaussian process gp method which us forward selection with criterion based on informationtheoretic principle previously suggested for active learning our goal is not only to learn d sparse predictor which can be evaluated in o d rather than o n d n n the number of training point but also to perform training under strong restriction on time and memory requirement the scaling of our method is at most o n d and in large real world classication experiment we show that it can match prediction performance of the popular support vector machine svm yet can be signican tly faster in training in contrast to the svm our approximation produce estimate of predictive probability error bar allows for bayesian model selection and is le complex in implementation 
we study an explicit parametric model of document query and relevancy assessment for information retrieval ir mean field method are applied to analyze the model and derive efficient practical algorithm to estimate the parameter in the problem the hyperparameters are estimated by a fast approximate leave one out cross validation procedure based on the cavity method the algorithm is further evaluated on several benchmark database by comparing with standard algorithm in ir 
a boosting algorithm seek to minimize empirically a loss function in a greedy fashion the resulted estimator take an additive function form and is built iteratively by applying a base estimator or learner to updated sample depending on the previous iteration this paper study convergence of boosting when it is carried out over the linear span of a family of basis function for general loss function we prove the convergence of boosting s greedy optimization to the infinimum of the loss function over the linear span a a side product these result reveal the importance of restricting the greedy search step size a known in practice through the work of friedman and others 
in pattern classification task error are introduced because of difference between the true generative model and the one obtained via model estimation using likelihood ratio based classification it is possible to correct for this discrepancy by finding class pair specific term to adjust the likelihood ratio directly and that can make class pair preference relationship intransitive in this work we introduce new methodology that make necessary correction to the likelihood ratio specifically those that are necessary to achieve perfect classification but not likelihoodratio correction which is overkill the new correction while weaker than previously reported such adjustment are analytically challenging since they involve discontinuous function we therefore make several approximation one of which involves a novel measure of decision boundary error sensitivity we test a number of these new scheme on an isolated word speech recognition task a well a on the uci machine learning data set result show that by using the bias term calculated in this new way classification accuracy can substantially improve over both the baseline and over our previous result 
there is growing evidence from psychophysical and neurophysiological study that the brain utilizes bayesian principle for inference and decision making an important open question is how bayesian inference for arbitrary graphical model can be implemented in network of spiking neuron in this paper we show that recurrent network of noisy integrate and fire neuron can perform approximate bayesian inference for dynamic and hierarchical graphical model the membrane potential dynamic of neuron is used to implement belief propagation in the log domain the spiking probability of a neuron is shown to approximate the posterior probability of the preferred state encoded by the neuron given past input we illustrate the model using two example a motion detection network in which the spiking probability of a direction selective neuron becomes proportional to the posterior probability of motion in a preferred direction and a two level hierarchical network that produce attentional effect similar to those observed in visual cortical area v and v the hierarchical model offer a new bayesian interpretation of attentional modulation in v and v 
this paper compare the ability of human observer to detect target image curve with that of an ideal observer the target curve are sampled from a generative model which specifies probabilistically the geometry and local intensity property of the curve the ideal observer performs bayesian inference on the generative model using map estimation varying the probability model for the curve geometry enables u investigate whether human performance is best for target curve that obey specific shape statistic in particular those observed on natural shape experiment are performed with data on both rectangular and hexagonal lattice our result show that human observer performance approach that of the ideal observer and are in general closest to the ideal for condition where the target curve tends to be straight or similar to natural statistic on curve this suggests a bias of human observer towards straight curve and natural statistic 
airline routinely overbook flight based on the expectation that some fraction of booked passenger will not show for each flight accurate forecast of the expected number of no show for each flight can increase airline revenue by reducing the number of spoiled seat empty seat that might otherwise have been sold and the number of involuntary denied boarding at the departure gate conventional no show forecasting method typically average the no show rate of historically similar flight without the use of passenger specific information we develop two class of model to predict cabin level no show rate using specific information on the individual passenger booked on each flight the first of these model computes the no show probability for each passenger using both the cabin level historical forecast and the extracted passenger feature a explanatory variable this passenger level model is implemented using three different predictive method a c decision tree a segmented naive bayes algorithm and a new aggregation method for an ensemble of probabilistic model the second cabin level model is formulated using the desired cabin level no show rate a the response variable input to this model include the predicted cabin level no show rate derived from the various passenger level model a well a simple statistic of the feature of the cabin passenger population the cabin level model is implemented using either linear regression or a a direct probability model with explicit incorporation of the cabin level no show rate derived from the passenger level model output the new passenger based model are compared to a conventional historical model using train and evaluation data set taken from over million passenger name record standard metric such a lift curve and mean square cabin level error establish the improved accuracy of the passenger based model over the historical model all model are also evaluated using a simple revenue model and it is shown that the cabin level passenger based model can produce between and revenue gain over the conventional model depending on the revenue model parameter 
prediction suffix tree pst provide a popular and effectiv e tool for task such a compression classification and language modeling in this paper we take a decision theoretic view of pst generalizing the notion of margin to pst we present an online pst learning algorithm and derive a mistake bound for it we then describe a self bounded enhancement of our learning algorithm for which the learning process aut omatically grows a bounded depthpst we also prove a similar mistake bound for the self bounded algorithm the result is an efficient algor ithm that neither relies on a priori assumption on the shape or maximal d epth of the target pst nor doe it require any parameter to our knowledge this is the first provably correct pst learning algorithm which g enerates a bounded depth pst while being competitive with any fixed pst determined in hindsight 
many machine learning algorithm for clustering or dimensionality reduction take a input a cloud of point in euclidean space and construct a graph with the input data point a vertex this graph is then partitioned clustering or used to redefine metric information dimensionality reduction there ha been much recent work on new method for graph based clustering and dimensionality reduction but not much on constructing the graph itself graph typically used include the fullyconnected graph a local fixed grid graph for image segmentation or a nearest neighbor graph we suggest that the graph should adapt locally to the structure of the data this can be achieved by a graph ensemble that combine multiple minimum spanning tree each fit to a perturbed version of the data set we show that such a graph ensemble usually produce a better representation of the data manifold than standard method and that it provides robustness to a subsequent clustering or dimensionality reduction algorithm based on the graph 
discrete fourier transforms and other related fourier method have been practically implementable due to the fast fourier transform fft however there are many situation where doing fast fourier transforms without complete data would be desirable in this paper it is recognised that formulating the fft algorithm a a belief network allows suitable prior to be set for the fourier coecien t furthermore ecien t generalised belief propagation method between cluster of four node enable the fourier coecien t to be inferred and the missing data to be estimated in near to o n log n time where n is the total of the given and missing data point this method is compared with a number of common approach such a setting missing data to zero or to interpolation it is tested on generated data and for a fourier analysis of a damaged audio signal 
information source on the web are controlled by dierent organization or people utilize dierent text format and have varying inconsistency therefore any system that integrates information from dierent data source must consolidate data from these source data from many data source on the web may not contain enough information to accurately consolidate the data even using state of the art object consolidation system we present an approach to accurately and automatically consolidate data from various data source by utilizing a state of the art object consolidation system in conjunction with a mediator system the mediator system is able to automatically determine which secondary source need to be queried in case where the object consolidation system is unable to confidently determine whether two record refer to the same entity in turn the object consolidation system is then able to utilize this additional information to improve the accuracy of the consolidation between datasets 
semantic understanding of multimedia content is critical in enabling effective access to all form of digital medium data by making large medium repository searchable semantic content description greatly enhance the value of such data automatic semantic understanding is a very challenging problem and most medium database resort to describing content in term of low level feature or using manually ascribed annotation recent technique focus on detecting semantic concept in video such a indoor outdoor face people nature etc this approach work for a fixed lexicon for which annotated training example exist in this paper we consider the problem of using such semantic concept detection to map the video clip into semantic space this is done by constructing a model vector that act a a compact semantic representation of the underlying content we then present experiment in the semantic space leveraging such information for enhanced semantic retrieval classification visualization and data mining purpose we evaluate these idea using a large video corpus and demonstrate significant performance gain in retrieval effectiveness 
advance in satellite technology and availability of downloaded image constantly increase the size of remote sensing image archive automatic content extraction classification and content based retrieval have become highly desired goal for the development of intelligent remote sensing database the common approach for mining these database us rule created by analyst however incorporating gi information and human expert knowledge with digital image processing improves remote sensing image analysis we developed a system that us decision tree classifier for interactive learning of land cover model and mining of image archive decision tree provide a promising solution for this problem because they can operate on both numerical continuous and categorical discrete data source and they do not require any assumption about neither the distribution nor the independence of attribute value this is especially important for the fusion of measurement from different source like spectral data dem data and other ancillary gi data furthermore using surrogate split provides the capability of dealing with missing data during both training and classification and enables handling instrument malfunction or the case where one or more measurement do not exist for some location quantitative and qualitative performance evaluation showed that decision tree provide powerful tool for modeling both pixel and region content of image and mining of remote sensing image archive 
we present a statistical analysis of the auc a an evaluation criterion for classification scoring model first we consider significance test for the difference between auc score of two algorithm on the same test set we derive exact moment under simplifying assumption and use them to examine approximate practical method from the literature we then compare auc to empirical misclassification error when the prediction goal is to minimize future error rate we show that the auc may be preferable to empirical error even in this case and discus the tradeoff between approximation error and estimation error underlying this phenomenon 
we develop a framework based on bayesian model averaging to explain how animal cope with uncertainty about contingency in classical conditioning experiment traditional account of conditioning fit parameter within a fixed generative model of reinforcer delivery uncertainty over the model structure is not considered we apply the theory to explain the puzzling relationship between second order conditioning and conditioned inhibition two similar conditioning regime that nonetheless result in strongly divergent behavioral outcome according to the theory second order conditioning result when limited experience lead animal to prefer a simpler world model that produce spurious correlation conditioned inhibition result when a more complex model is justified by additional experience 
in this paper we analyze the relationship between the computational capability of randomly connected network of threshold gate in the timeseries domain and their dynamical property in particular we propose a complexity measure which we find to assume it highest value near the edge of chaos i e the transition from ordered to chaotic dynamic furthermore we show that the proposed complexity measure predicts the computational capability very well only near the edge of chaos are such network able to perform complex computation on time series additionally a simple synaptic scaling rule for self organized criticality is presented and analyzed 
agent learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing i e different state that appear similar but require different response this problem is exacerbated when the agent s sensor are noisy i e sensor may produce different observation in the same state we show that many well known reinforcement learning method designed to deal with perceptual aliasing such a utile suffix memory finite size history window eligibility trace and memory bit do not handle noisy sensor well we suggest a new algorithm noisy utile suffix memory nusm based on usm that us a weighted classification of observed trajectory we compare nusm to the above method and show it to be more robust to noise 
inductive learning of first order theory based on example ha serious bottleneck in the enormous hypothesis search space needed making existing learning approach perform poorly when compared to the propositional approach moreover in order to choose the appropiate candidate all inductive logic programming ilp system only use quantitive information e g number of example covered and length of rule which is insufficient for search space having many similar candidate this paper introduces a novel approach to improve ilp by incorporating the qualitative information into the search heuristic by focusing only on a kind of data where one instance consists of several part a well a relation among part this approach aim to find the hypothesis describing each class by using both individual and relational characteristic of part of example this kind of data can be found in various domain especially in representing chemical compound structure each compound is composed of atom a part and bond a relation between two atom we apply the proposed approach for discovering rule describing the activity of compound from their structure from two real world datasets mutagenicity in nitroaromatic compound and dopamine antagonist compound the result were compared to the existing method using ten fold cross validation and we found that the proposed method significantly produced more accurate result in prediction 
we study gender discrimination of human face using a combination of psychophysical classification and discrimination exper iments together with method from machine learning we reduce the dimensionality of a set of face image using principal component analysis and then train a set of linear classifier on this reduced representation li near support vector machine svms relevance vector machine rvms fisher linear discriminant fld and prototype prot classifier usin g human classification data because we combine a linear preprocessor wi th linear classifier the entire system act a a linear classifier al lowing u to visualise the decision imagecorresponding to the normal vector of the separating hyperplanes sh of each classifier we predict that th e female tomaleness transition along the normal vector for classifier closely mimicking human classification svm and rvm should be faste r than the transition along any other direction a psychophysical discrimination experiment using the decision image a stimulus is consistent with this prediction 
we propose a general framework for support vector machine svm based on the principle of multi objective optimization the learning of svms is formulated a a multiobjective program by setting two competing goal to minimize the empirical risk and minimize the model capacity distinct approach to solving the mop introduce various svm formulation the proposed framework enables a more effective minimization of the vc bound on the generalization risk we develop a feature selection approach based on the mop framework and demonstrate it effectiveness on hand written digit data 
we present a principled and efficient planning algorithm for cooperative multiagent dynamic system a striking feature of our method is that the coordination and communication between the agent is not imposed but derived directly from the system dynamic and function approximation architecture we view the entire multiagent system a a single large markov decision process mdp which we assume can be represented in a factored way using a dynamic bayesian network dbn the action space of the resulting mdp is the joint action space of the entire set of agent our approach is based on the use of factored linear value function a an approximation to the joint value function this factorization of the value function allows the agent to coordinate their action at runtime using a natural message passing scheme we provide a simple and efficient method for computing such an approximate value function by solving a single linear program whose size is determined by the interaction between the value function structure and the dbn we thereby avoid the exponential blowup in the state and action space we show that our approach compare favorably with approach based on reward sharing we also show that our algorithm is an efficient alternative to more complicated algorithm even in the single agent case 
this paper present an approach to automatically optimizing the retrieval quality of search engine using clickthrough data intuitively a good information retrieval system should present relevant document high in the ranking with le relevant document following below while previous approach to learning retrieval function from example exist they typically require training data generated from relevance judgment by expert this make them difficult and expensive to apply the goal of this paper is to develop a method that utilizes clickthrough data for training namely the query log of the search engine in connection with the log of link the user clicked on in the presented ranking such clickthrough data is available in abundance and can be recorded at very low cost taking a support vector machine svm approach this paper present a method for learning retrieval function from a theoretical perspective this method is shown to be well founded in a risk minimization framework furthermore it is shown to be feasible even for large set of query and feature the theoretical result are verified in a controlled experiment it show that the method can effectively adapt the retrieval function of a meta search engine to a particular group of user outperforming google in term of retrieval quality after only a couple of hundred training example 
we present an algorithm to extract feature from high dimensional gene expression profile based on the knowledge of a graph which link together gene known to participate to successive reaction in metabolic pathway motivated by the intuition that biologically relevant feature are likely to exhibit smoothness with respect to the graph topology the algorithm involves encoding the graph and the set of expression profile into kernel function and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel hilbert space function prediction experiment for the gene of the yeast s cerevisiae validate this approach by showing a consistent increase in performance when a state of the art classifier us the vector of feature instead of the original expression profile to predict the functional class of a gene 
a new kernel function between two labeled graph is presented feature vector are defined a the count of label path produced by random walk on graph the kernel computation finally boil down to obtaining the stationary state of a discrete time linear system thus is eciently performed by solving simultaneous linear equation our kernel is based on an infinite dimensional feature space so it is fundamentally dierent from other string or tree kernel based on dynamic programming we will present promising empirical result in classification of chemical compound 
the computation of classical higher order statistic such a higher order moment or spectrum is difficult for image due to the huge number of term to be estimated and interpreted we propose an alternative approach in which multiplicative pixel interaction are described by a series of wiener functionals since the functionals are estimated implicitly via polynomial kernel the combinatorial explosion associated with the classical higher order statistic is avoided first result show that image structure such a line or corner can be predicted correctly and that pixel interaction up to the order of five play an important role in natural image 
we describe a new algorithmic framework for learning multiclass categorization problem in this framework a multiclass predictor is composed of a pair of embeddings that map both instance and label into a common space in this space each instance is assigned the label it is nearest to we outline and analyze an algorithm termed bunching for learning the pair of embeddings from labeled data a key construction in the analysis of the algorithm is the notion of probabilistic output code a generalization of error correcting output code ecoc furthermore the method of multiclass categorization using ecoc is shown to be an instance of bunching we demonstrate the advantage of bunching over ecoc by comparing their performance on numerous categorization problem 
we present a general modeling method for optimal probability prediction over future observation in which model dimensionality is determined a a natural by product this new method yield several estimator and we establish theoretically that they are optimal either overall or under stated restriction when the number of free parameter is innite a a case study we investigate the problem of tting logistic model in nite sample situation simulation result on both articial and practical datasets are supportive 
we present a graphical model for beat tracking in recorded music using a probabilistic graphical model allows u to incorporate local information and global smoothness constraint in a principled manner we evaluate our model on a set of varied and difficult example and achieve impressive result by using a fast dual tree algorithm for graphical model inference our system run in le time than the duration of the music being processed 
abstract when we model a higher order function such a learning and memory we face a difficulty of comparing neural activity with hidden variable that depend on the history of sensory and motor signal and the dynamic of the network here we propose novel method for estimating hidden variable of a learning agent such a connection weight from sequence of observable variable bayesian estimation is a method to estimate the posterior probability of hidden variable from observable data sequence using a dynamic model of hidden and observable variable in this paper we apply particle filter for estimating internal parameter and metaparameters of a reinforcement learning model we verified the effectiveness of the method using both artificial data and real animal behavioral data 
data clustering ha attracted a lot of research attention in the field of computational statistic and data mining in most related study the dissimilarity between two cluster is defined a the distance between their centroid or the distance between two closest or farthest data point however all of these measurement are vulnerable to outlier and removing the outlier precisely is yet another difficult task in view of this we propose a new similarity measurement referred to a cohesion to measure the inter cluster distance by using this new measurement of cohesion we design a two phase clustering algorithm called cohesion based self merging abbreviated a csm which run in linear time to the size of input data set combining the feature of partitional and hierarchical clustering method algorithm csm partition the input data set into several small subclusters in the first phase and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase a shown by our performance study the cohesion based clustering is very robust and posse the excellent tolerance to outlier in various workload more importantly algorithm csm is shown to be able to cluster the data set of arbitrary shape very efficiently and provide better clustering result than those by prior method index term data mining data clustering hierarchical clustering partitional clustering 
we seek to both detect and segment object in image to exploit both local image data a well a contextual information we introduce boosted random field brfs which us boosting to learn the graph structure and local evidence of a conditional random field crf t he graph structure is learned by assembling graph fragment in an additive model the connection between individual pixel are not very informative but by using dense graph we can pool information from large region of the image dense model also support efficient inference we show how contextual information from other object can improve detection performance both in term of accuracy and speed by using a computational cascade we apply our system to detect stuff and thing in offi ce and street scene 
we describe and analyze an online algorithm for supervised learning of pseudo metric the algorithm receives pair of instance and predicts their similarity according to a pseudo metric the pseudo metric we use are quadratic form parameterized by positive semi definite matrix the core of the algorithm is an update rule that is based on successive projection onto the positive semi definite cone and onto half space constraint imposed by the example we describe an efficient procedure for performing these projection derive a worst case mistake bound on the similarity prediction and discus a dual version of the algorithm in which it is simple to incorporate kernel operator the online algorithm also serf a a building block for deriving a large margin batch algorithm we demonstrate the merit of the proposed approach by conducting experiment on mnist dataset and on document filtering 
in this paper we propose a scaling up method that is applicable to essentially any induction algorithm based on discrete search the result of applying the method to an algorithm is that it running time becomes independent of the size of the database while the decision made are essentially identical to those that would be made given infinite data the method work within pre specified memory limit and a long a the data is iid only requires accessing it sequentially it give anytime result and can be used to produce batch stream time changing and active learning version of an algorithm we apply the method to learning bayesian network developing an algorithm that is faster than previous one by order of magnitude while achieving essentially the same predictive performance we observe these gain on a series of large database generated from benchmark network on the kdd cup e commerce data and on a web log containing million request 
this paper present a flexible mixture model fmm for collaborative filtering fmm extends existing partitioning clustering algorithm for collaborative filtering by clustering both user and item together simultaneously without assuming that each user and item should only belong to a single cluster furthermore with the introduction of preference node the proposed framework is able to explicitly model how user rate item which can vary dramatically even among the user with similar taste on item empirical study over two datasets of movie rating ha shown that our new algorithm outperforms five other collaborative filtering algorithm substantially 
privacy consideration often constrain data mining project this paper address the problem of association rule mining where transaction are distributed across source each site hold some attribute of each transaction and the site wish to collaborate to identify globally valid association rule however the site must not reveal individual transaction data we present a two party algorithm for efficiently discovering frequent itemsets with minimum support level without either site revealing individual transaction value 
we describe a model of short term synaptic depression that is derived from a silicon circuit implementation the dynamic of this circuit model are similar to the dynamic of some present theoretical model of shortterm depression except that the recovery dynamic of the variable describing the depression is nonlinear and it also depends on the presynaptic frequency the equation describing the steady state and transient response of this synaptic model fit the experimental result obtained from a fabricated silicon network consisting of leaky integrate and fire neuron and different type of synapsis we also show experimental data demonstrating the possible computational role of depression one possible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the resting potential 
this paper present a method for learning a distance metric from relative comparison such a a is closer to b than a is to c taking a support vector machine svm approach we develop an algorithm that provides a flexible way of describing qualitative training data a a set of constraint we show that such constraint lead to a convex quadratic programming problem that can be solved by adapting standard method for svm training we empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text document 
the data in many real world problem can be thought of a a graph such a the web co author network and biological network we propose a general regularization framework on graph which is applicable to the classication ranking and link prediction problem we also show that the method can be explained a lazy random walk we evaluate the method on a number of experiment 
in this paper we introduce a new underlying probabilistic model for principal component analysis pca our formulation interprets pca a a particular gaussian process prior on a mapping from a latent space to the observed data space we show that if the prior s covariance function constrains the mapping to be linear the model is equivalent to pca we then extend the model by considering le restrictive covariance function which allow non linear mapping this more general gaussian process latent variable model gplvm is then evaluated a an approach to the visualisation of high dimensional data for three different data set additionally our non linear algorithm can be further kernelised leading to twin kernel pca in which a mapping between feature space occurs 
this paper introduces support envelope a new tool for analyzing association pattern and illustrates some of their property application and possible extension specifically the support envelope for a transaction data set and a specified pair of positive integer m n consists of the item and transaction that need to be searched to find any association pattern involving m or more transaction and n or more item for any transaction data set with m transaction and n item there is a unique lattice of at most m n support envelope that capture the structure of the association pattern in that data set because support envelope are not encumbered by a support threshold this support lattice provides a complete view of the association structure of the data set including association pattern that have low support furthermore the boundary of the support lattice the support boundary ha at most min m n envelope and is especially interesting since it bound the maximum size of potential association pattern not only for frequent closed and maximal itemsets but also for pattern such a error tolerant itemsets that are more general the association structure can be represented graphically a a two dimensional scatter plot of the m n value associated with the support envelope of the data set a feature that is useful in the exploratory analysis of association pattern finally the algorithm to compute support envelope is simple and computationally efficient and it is straightforward to parallelize the process of finding all the support envelope 
we consider the question of predicting nonlinear time series kernel dynamical modeling kdm a new method based on kernel is proposed a an extension to linear dynamical model the kernel trick is used twice first to learn the parameter of the model and second to compute preimages of the time series predicted in the feature space by mean of support vector regression our model show strong connection with the classic kalman filter model with the kernel feature space a hidden state space kernel dynamical modeling is tested against two benchmark time series and achieves high quality prediction 
we present an extension of isomap nonlinear dimension reduction tenenbaum et al for data with both spatial and temporal relationship our method st isomap augments the existing isomap framework to consider temporal relationship in local neighborhood that can be propagated globally via a shortest path mechanism two instantiation of st isomap are presented for sequentially continuous and segmented data result from applying st isomap to real world data collected from human motion performance and humanoid robot teleoperation are also presented 
the goal of clustering is to identify distinct group in a dataset the basic idea of model based clustering is to approximate the data density by a mixture model typically a mixture of gaussians and to estimate the parameter of the component density the mixing fraction and the number of component from the data the number of distinct group in the data is then taken to be the number of mixture component and the observation are partitioned into cluster estimate of the group using bayes rule if the group are well separated and look gaussian then the resulting cluster will indeed tend to be distinct in the most common sense of the word contiguous densely populated area of feature space separated by contiguous relatively empty region if the group are not gaussian however this correspondence may break down an isolated group with a non elliptical distribution for example may be modeled by not one but several mixture component and the corresponding cluster will no longer be well separated we present method for assessing the degree of separation between the component of a mixture model and between the corresponding cluster we also propose a new clustering method that can be regarded a a hybrid between model based and nonparametric clustering the hybrid clustering algorithm prune the cluster tree generated by hierarchical model based clustering starting with the tree corresponding to the mixture model chosen by the bayesian information criterion it progressively merges cluster that do not appear to correspond to different mode of the data density 
recently there ha been increasing interest in the issue of cost sensitive learning and decision making in a variety of application of data mining a number of approach have been developed that are effective at optimizing cost sensitive decision when each decision is considered in isolation however the issue of sequential decision making with the goal of maximizing total benefit accrued over a period of time instead of immediate benefit ha rarely been addressed in the present paper we propose a novel approach to sequential decision making based on the reinforcement learning framework our approach attempt to learn decision rule that optimize a sequence of cost sensitive decision so a to maximize the total benefit accrued over time we use the domain of targeted marketing a a testbed for empirical evaluation of the proposed method we conducted experiment using approximately two year of monthly promotion data derived from the well known kdd cup donation data set the experimental result show that the proposed method for optimizing total accrued benefit out performs the usual targeted marketing methodology of optimizing each promotion in isolation we also analyze the behavior of the targeting rule that were obtained and discus their appropriateness to the application domain 
a information volume in enterprise system and in the web grows rapidly how to accurately retrieve information is an important research area several corpus based smoothing technique have been proposed to address the data sparsity and synonym problem faced by information retrieval system such smoothing technique are often unable to discover and utilize the correlation among term we propose cv a correlation verification based smoothing method that considers co occurrence information in smoothing strongly correlated term in a document are identified by their co occurrence frequency in the document to avoid missing correlated term with low co occurrence frequency but specific to the theme of the document the joint distribution of term in the document are compared with those in the corpus for statistical significance a common approach to apply corpus based smoothing technique to information retrieval is by refining the vector representation of document this paper investigates the effect of corpus based smoothing on information retrieval by query expansion using term cluster generated from a term clustering process the result can also be viewed in light of the effect of smoothing on clustering empirical study show that our approach outperforms previous corpus based smoothing technique it improves retrieval effectiveness by the result demonstrate that corpus based smoothing can be used for query expansion by term clustering 
developer of artificial agent commonly take the view that we can only specify agent behavior via the expensive process of implementing new skill this paper offer an alternative expressed by the separation hypothesis that the behavioral difference among individual are due to the action of distinct preference over the same set of skill we test this hypothesis in a simulated automotive domain by using a reinforcement learning algorithm to induce vehicle control policy given a structured skill for driving that contains option and a user supplied reward function we show that qualitatively distinct reward function produce agent with qualitatively distinct behavior over the same set of skill this lead to a new development metaphor we call programming by reward 
a framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neuron upon simultaneous presentation of multiple stimulus minimal square estimation error are obtained from a fisher information analysis in an abstract compound space comprising the feature of all stimulus even for the simplest case of linear superposition of response and gaussian tuning the symmetry in the compound space are very dieren t from those in the case of a single stimulus the analysis allows for a quantitative description of attentional eects and can be extended to include neural nonlinearities such a nonclassical receptive eld 
choice based conjoint analysis build model of consumer preference over product with answer gathered in questionnaire our main goal is to bring tool from the machine learning community to solve this problem more efficiently thus we propose two algorithm to quickly and accurately estimate consumer preference 
we present a learning framework for markovian decision process that is based on optimization in the policy space instead of using relatively slow gradient based optimization algorithm we use the fast cross entropy method the suggested framework is described for several reward criterion and it eectiveness is demonstrated for a grid world navigation task and for an inventory control problem 
multiagent learning is a key problem in ai in the presence of multiple nash equilibrium even agent with non conflicting interest may not be able to learn an optimal coordination policy the problem is exaccerbated if the agent do not know the game and independently receive noisy payoff so multiagent reinforfcement learning involves two interrelated problem identifying the game and learning to play in this paper we present optimal adaptive learning the first algorithm that converges to an optimal nash equilibrium with probability in any team markov game we provide a convergence proof and show that the algorithm s parameter are easy to set to meet the convergence condition 
over the last few year the network community ha started to rely heavily on the use of novel concept such a fractal self similarity long range dependence power law especially evidence of fractal self similarity and long range dependence in network trac have been widely observed despite their wide use there is still much confusion regarding the identification of such phenomenon in real network trac 
mining frequent tree is very useful in domain like bioinformatics web mining mining semistructured data and so on we formulate the problem of mining embedded subtrees in a forest of rooted labeled and ordered tree we present treeminer a novel algorithm to discover all frequent subtrees in a forest using a new data structure called scope list we contrast treeminer with a pattern matching tree mining algorithm patternmatcher we conduct detailed experiment to test the performance and scalability of these method we find that treeminer outperforms the pattern matching approach by a factor of to and ha good scaleup property we also present an application of tree mining to analyze real web log for usage pattern 
we describe method for computing an implicit model of a hypersurface that is given only by a finite sampling the method work by mapping the sample point into a reproducing kernel hilbert space and then determining region in term of hyperplanes 
this paper give a new iterative algorithm for kernel logistic regression it is based on the solution of a dual problem using idea similar to those of the sequential minimal optimization algorithm for support vector machine asymptotic convergence of the algorithm is proved computational experiment show that the algorithm is robust and fast the algorithmic idea can also be used to give a fast dual algorithm for solving the optimization problem arising in the inner loop of gaussian process classifier 
this paper investigates a new learning model in which the input data is corrupted with noise we present a general statistical framework to tackle this problem based on the statistical reasoning we propose a novel formulation of support vector classification which allows uncertainty in input data we derive an intuitive geometric interpretation of the proposed formulation and develop algorithm to efficiently solve it empirical result are included to show that the newly formed method is superior to the standard svm for problem with noisy input 
the paper present a method for pruning frequent itemsets based on background knowledge represented by a bayesian network the interestingness of an itemset is defined a the absolute difference between it support estimated from data and from the bayesian network efficient algorithm are presented for finding interestingness of a collection of frequent itemsets and for finding all attribute set with a given minimum interestingness practical usefulness of the algorithm and their efficiency have been verified experimentally 
in this paper linear multilayer ica lmica is proposed for extracting independent component from quite high dimensional observed signal such a large size natural scene there are two phase in each layer of lmica one is the mapping phase where a one dimensional mapping is formed by a stochastic gradient algorithm which make more highlycorrelated non independent signal be nearer incrementally another is the local ica phase where each neighbor namely highly correlated pair of signal in the mapping is separated by the maxkurt algorithm because lmica separate only the highly correlated pair instead of all one it can extract independent component quite efficiently from appropriate observed signal in addition it is proved that lmica always converges some numerical experiment verify that lmica is quite efficient and effective in large size natural image processing 
semi supervised clustering employ a small amount of labeled data to aid unsupervised learning previous work in the area ha utilized supervised data in one of two approach constraint based method that guide the clustering algorithm towards a better grouping of the data and distance function learning method that adapt the underlying similarity metric used by the clustering algorithm this paper provides new method for the two approach a well a present a new semi supervised clustering algorithm that integrates both of these technique in a uniform principled framework experimental result demonstrate that the unified approach produce better cluster than both individual approach a well a previously proposed semi supervised clustering algorithm 
semi structured data such a xml and html is attracting considerable attention it is important to develop various kind of data mining technique that can handle semistructured data in this paper we discus application of kernel method for semistructured data we model semi structured data by labeled ordered tree and present kernel for classifying labeled ordered tree based on their tag structure by generalizing the convolution kernel for parse tree introduced by collins and duy we give algorithm to eciently compute the kernel for labeled ordered tree we also apply our kernel to node marking problem that are special case of information extraction from tree preliminary experiment using artificial data and real html document show encouraging result 
this paper provides a foundation for multi task learning using reproducing kernel hilbert space of vector valued function in this setting the kernel is a matrix valued function some explicit example will be described which go beyond our earlier result in in particular we characterize class of matrixvalued kernel which are linear and are of the dot product or the translation invariant type we discus how these kernel can be used to model relation between the task and present linear multi task learning algorithm finally we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation 
this paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy this is significant for many application such a image classification where obtaining classification label is expensive while large unlabeled example are easily available we investigate an expectation maximization em algorithm for learning from labeled and unlabeled data the reason why unlabeled data boost learning accuracy is because it provides the information about the joint probability distribution a theoretical argument show that the more unlabeled example are combined in learning the more accurate the result we then introduce b em algorithm based on the combination of em with bootstrap method to exploit the large unlabeled data while avoiding prohibitive i o cost experimental result over both synthetic and real data set that the proposed approach ha a satisfactory performance 
in recent year the technological advance in mapping gene have made it increasingly easy to store and use a wide variety of biological data such data are usually in the form of very long string for which it is difficult to determine the most relevant feature for a classification task for example a typical dna string may be million of character long and there may be thousand of such string in a database in many case the classification behavior of the data may be hidden in the compositional behavior of certain segment of the string which cannot be easily determined apriori another problem which complicates the classification task is that in some case the classification behavior is reflected in global behavior of the string whereas in others it is reflected in local pattern given the enormous variation in the behavior of the string over different data set it is useful to develop an approach which is sensitive to both the global and local behavior of the string for the purpose of classification for this purpose we will exploit the multi resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristic at different level of granularity the resulting scheme turn out to be very effective in practice on a wide range of problem 
prior knowledge in the form of multiple polyhedral set each belonging to one of two category is introduced into a reformulation of a linear support vector machine classier the resulting formulation lead to a linear program that can be solved ecien tly real world example from dna sequencing and breast cancer prognosis demonstrate the eectiv ene of the proposed method numerical result show improvement in test set accuracy after the incorporation of prior knowledge into ordinary data based linear support vector machine classiers one experiment also show that a linear classier based solely on prior knowledge far outperforms the direct application of prior knowledge rule to classify data keywords use and renement of prior knowledge support vector machine linear programming 
we present two result which arise from a model based approach to hierarchical agglomerative clustering first we show formally that the common heuristic agglomerative clustering algorithm single link complete link groupaverage and ward s method are each equivalent to a hierarchical model based method this interpretation give a theoretical explanation of the empirical behavior of these algorithm a well a a principled approach to resolving practical issue such a number of cluster or the choice of method second we show how a model based approach can be used to extend these basic agglomerative algorithm we introduce adjusted complete link mahalanobis link and line link a variant of the classical agglomerative method and demonstrate their utility 
in sequence modeling we often wish to represent complex interaction between label such a when performing multiple cascaded labeling task on the same sequence or when long range dependency exist we present dynamic conditional random field dcrfs a generalization of linear chain conditional random field crfs in which each time slice contains a set of state variable and edge a distributed state representation a in dynamic bayesian network dbns and parameter are tied across slice since exact inference can be intractable in such model we perform approximate inference using several schedule for belief propagation including tree based reparameterization trp on a natural language chunking task we show that a dcrf performs better than a series of linear chain crfs achieving comparable performance using only half the training data 
in the multi armed bandit problem an online algorithm must choose from a set of strategy in a sequence of n trial so a to minimize the total cost of the chosen strategy while nearly tight upper and lower bound are known in the case when the strategy set is flnite much le is known when there is an inflnite strategy set here we consider the case when the set of strategy is a subset ofrd and the cost function are continuous in the d case we improve on the best known upper and lower bound closing the gap to a sublogarithmic factor we also consider the case where d and the cost function are convex adapting a recent online convex optimization algorithm of zinkevich to the sparser feedback model of the multi armed bandit problem 
several unsupervised learning algorithm based on an eigendecomposition provide either an embedding or a clustering only for given training point with no straightforward extension for out of sample example short of recomputing eigenvectors this paper provides algorithm for such an extension for local linear embedding lle isomap laplacian eigenmaps multi dimensional scaling all algorithm which provide lower dimensional embedding for dimensionality reduction a well a for spectral clustering which performs non gaussian clustering these extension stem from a unified framework in which these algorithm are seen a learning eigenfunctions of a kernel lle and isomap pose special challenge a the kernel is training data dependent numerical experiment on real data show that the generalization performed have a level of error comparable to the variability of the embedding algorithm to the choice of training data 
we present in this paper the problem of discovering set of attribute value pair in high dimensional data set that are of interest not because of co occurrence alone but due to their value in serving a core for potential classifier of cluster we present our algorithm in the context of a gene expression dataset gene expression data in most situation is insufficient for clustering algorithm and any statistical inference because for gene typically only s and at most s of data point become available it is difficult to use statistical technique to design a classifier for such immensely under specified data the observed data though statistically insufficient contains some information about the domain our goal is to discover a much information about all potential classifier a possible from the data and then summarize this knowledge this summarization provides insight into the composition of potential classifier we present here algorithm and method for mining a high dimensional data set exemplified by a gene expression data set for mining such information 
the assumption behind linear classifier for categorical data are examined and reformulated in the context of the multinomial manifold the simplex of multinomial model furnished with the riemannian structure induced by the fisher information this lead to a new view of hyperplane classifier which together with a generalized margin concept show how to adapt existing margin based hyperplane model to multinomial geometry experiment show the new classification framework to be effective for text classification where the categorical structure of the data is modeled naturally within the multinomial family 
we apply the replica method of statistical physic combined with a variational method to the approximate analytical computation of bootstrap average for estimating the generalization error we demonstrate our approach on regression with gaussian process and compare our result with average obtained by monte carlo sampling 
recently mining data stream with concept drift for actionable insight ha become an important and challenging task for a wide range of application including credit card fraud protection target marketing network intrusion detection etc conventional knowledge discovery tool are facing two challenge the overwhelming volume of the streaming data and the concept drift in this paper we propose a general framework for mining concept drifting data stream using weighted ensemble classifier we train an ensemble of classification model such a c ripper naive beyesian etc from sequential chunk of the data stream the classifier in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time evolving environment thus the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification our empirical study show that the proposed method have substantial advantage over single classifier approach in prediction accuracy and the ensemble framework is effective for a variety of classification model 
this paper explores the computational consequence of simultaneous intrinsic and synaptic plasticity in individual model neuron it proposes a new intrinsic plasticity mechanism for a continuous activation model neuron based on low order moment of the neuron s ring rate distribution the goal of the intrinsic plasticity mechanism is to enforce a sparse distribution of the neuron s activity level in conjunction with hebbian learning at the neuron s synapsis the neuron is shown to discover sparse direction in the input we consider an individual continuous activation model neuron with a non linear transfer function that ha adjustable parameter we are proposing a simple intrinsic learning mechanism based on estimate of low order moment of the activity distribution that allows the model neuron to adjust the parameter of it non linear transfer function to obtain an approximately exponential distribution of it activity we then show that if combined with a standard hebbian learning rule employing multiplicative weight normalization this lead to the extraction of sparse feature from the input this is in sharp contrast to standard hebbian learning in linear unit with multiplicative weight normalization which lead to 
many technique in the social science and graph theory deal with the problem of examining and analyzing pattern found in the underlying structure and association of a group of entity however much of this work assumes that this underlying structure is known or can easily be inferred from data which may often be an unrealistic assumption for many real world problem below we consider the problem of learning and querying a graph based model of this underlying structure the model is learned from noisy observation linking set of entity we explicitly allow different type of link representing different type of relation and temporal information indicating when a link wa observed we quantitatively compare this representation and learning method against other algorithm on the task of predicting future link and new friendship in a variety of real world data set 
data on individual and entity are being collected widely these data can contain information that explicitly identifies the individual e g social security number data can also contain other kind of personal information e g date of birth zip code gender that are potentially identifying when linked with other available data set data are often shared for business or legal reason this paper address the important issue of preserving the anonymity of the individual or entity during the data dissemination process we explore preserving the anonymity by the use of generalization and suppression on the potentially identifying portion of the data we extend earlier work in this area along various dimension first satisfying privacy constraint is considered in conjunction with the usage for the data being disseminated this allows u to optimize the process of preserving privacy for the specified usage in particular we investigate the privacy transformation in the context of data mining application like building classification and regression model second our work improves on previous approach by allowing more flexible generalization for the data lastly this is combined with a more thorough exploration of the solution space using the genetic algorithm framework these extension allow u to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraint 
distance based method in machine learning and pattern recognition have to rely on a metric distance between point in the input space instead of specifying a metric a priori we seek to learn the metric from data via kernel method and multidimensional scaling md technique under the classification setting we define discriminant kernel on the joint space of input and output space and present a specific family of discriminant kernel this family of discriminant kernel is attractive because the induced metric are euclidean and fisher separable and md technique can be used to find the lowdimensional euclidean representation also called feature vector of the induced metric since the feature vector incorporate information from both input point and their corresponding label and they enjoy fisher separability they are appropriate to be used in distance based classifier 
to analyze the effect of the ocean and atmosphere on land climate earth scientist have developed climate index which are time series that summarize the behavior of selected region of the earth s ocean and atmosphere in the past earth scientist have used observation and more recently eigenvalue analysis technique such a principal component analysis pca and singular value decomposition svd to discover climate index however eigenvalue technique are only useful for finding a few of the strongest signal furthermore they impose a condition that all discovered signal must be orthogonal to each other making it difficult to attach a physical interpretation to them this paper present an alternative clustering based methodology for the discovery of climate index that overcomes these limitiations and is based on cluster that represent region with relatively homogeneous behavior the centroid of these cluster are time series that summarize the behavior of the ocean or atmosphere in those region some of these centroid correspond to known climate index and provide a validation of our methodology other centroid are variant of known index that may provide better predictive power for some land area and still other index may represent potentially new earth science phenomenon finally we show that cluster based index generally outperform svd derived index both in term of area weighted correlation and direct correlation with the known index 
estimation of gaussian mixture model is an ecien t and popular technique for clustering and density estimation an em procedure is widely used to estimate the model parameter in this paper we show how side information in the form of equivalence constraint can be incorporated into this procedure leading to improved clustering result equivalence constraint are prior knowledge concerning pair of data point indicating if the point arise from the same source positive constraint or from dieren t source negative constraint such constraint can be gathered automatically in some learning problem and are a natural form of supervision in others we present a closed form em procedure for handling positive constraint and a generalized em procedure using a markov net for the incorporation of negative constraint using publicly available data set we demonstrate that such side information may lead to considerable improvement in clustering task and that our algorithm is preferable to another suggested method using this type of side information 
abstract density estimation with gaussian mixture model is a popular gener ative technique used also for clustering we develop a framework to incorporate side information in the form of equivalence constraint into the model estimation procedure equivalence constraint are defined on pair of data point indicating whether the point arise from the same source positive constraint or from different source negative con straints such constraint can be gathered automatically in some learn ing problem and are a natural form of supervision in others for the estimation of model parameter we present a closed form em procedure which handle positive constraint and a gene 
abstract there are several reinforcement learning algorithm that yield approximatesolutions for the problem of policy evaluation when thevalue function is represented with a linear function approximator 
privacy is an important issue in data mining and knowledge discovery in this paper we propose to use the randomized response technique to conduct the data mining computation specially we present a method to build decision tree classifier from the disguised data we conduct experiment to compare the accuracy of our decision tree with the one built from the original undisguised data our result show that although the data are disguised our method can still achieve fairly high accuracy we also show how the parameter used in the randomized response technique affect the accuracy of the result 
the benzodiazepine midazolam cause dense but temporary anterograde amnesia similar to that produced by hippocampal damage doe the action of midazolam on the hippocampus cause le storage or le accurate storage of information in episodic long term memory we used a simple variant of the rem model to fit data collected by hirshman fisher henthorn arndt and passannante on the effect of midazolam study time and normative word frequency on both yes no and remember know recognition memory that a simple strength model fit well wa contrary to the expectation of hirshman et al more important within the bayesian based rem modeling framework the data were consistent with the view that midazolam cause le accurate storage rather than le storage of information in episodic memory 
nested dichotomy are a standard statistical technique for tackling certain polytomous classification problem with logistic regression they can be represented a binary tree that recursively split a multi class classification task into a system of dichotomy and provide a statistically sound way of applying two class learning algorithm to multi class problem assuming these algorithm generate class probability estimate however there are usually many candidate tree for a given problem and in the standard approach the choice of a particular tree is based on domain knowledge that may not be available in practice an alternative is to treat every system of nested dichotomy a equally likely and to form an ensemble classifier based on this assumption we show that this approach produce more accurate classification than applying c and logistic regression directly to multi class problem our result also show that ensemble of nested dichotomy produce more accurate classifier than pairwise classification if both technique are used with c and comparable result for logistic regression compared to error correcting output code they are preferable if logistic regression is used and comparable in the case of c an additional benefit is that they generate class probability estimate consequently they appear to be a good general purpose method for applying binary classifier to multi class problem 
in this paper we propose a bayesian framework which construct shared state triphone hmms based on a variational bayesian approach and recognizes speech based on the bayesian prediction classification variational bayesian estimation and clustering for speech recognition vbec an appropriate model structure with high recognition performance can be found within a vbec framework unlike conventional method including bic or mdl criterion based on the maximum likelihood approach the proposed model selection is valid in principle even when there are insufficient amount of data because it doe not use an asymptotic assumption in isolated word recognition experiment we show the advantage of vbec over conventional method especially when dealing with small amount of data 
computer animated agent and robot bring a social dimension to human computer interaction and force u to think in new way about how computer could be used in daily life face to face communication is a real time process operating at a time scale of le than a second in this paper we present progress on a perceptual primitive to automatically detect frontal face in the video stream and code them with respect to dimension in real time neutral anger disgust fear joy sadness surprise the face finder employ a cascade of feature detector trained with boosting technique the expression recognizer employ a novel combination of adaboost and svm s the generalization performance to new subject for a way forced choice wa and correct on two publicly available datasets the output of the classifier change smoothly a a function of time providing a potentially valuable representation to code facial expression dynamic in a fully automatic and unobtrusive manner the system wa deployed and evaluated for measuring spontaneous facial expression in the field in an application for automatic assessment of human robot interaction computer animated agent and robot bring a social dimension to human computer interaction and force u to think in new way about how computer could be used in daily life face to face communication is a real time process operating at a time scale of le than a second thus fulfilling the idea of machine that interact face to face with u requires development of robust real time perceptive primitive in this paper we present first step towards the development of one such primitive a system that automatically find face in the visual video stream and code facial expression dynamic in real time the system automatically detects frontal face and code them with respect to dimension joy sadness surprise anger disgust fear and neutral speed and accuracy are enhanced by a novel technique that combine feature selection based on adaboost with feature integration based on support vector machine we host an online demo of the system at http mplab ucsd edu 
we discus an idea for collecting data in a relatively efficient manner our point of view is bayesian and information theoretic on any given trial we want to adaptively choose the input in such a way that the mutual information between the unknown state of the system and the stochastic output is maximal given any prior information including data collected on any previous trial we prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative example comparing the performance of this adaptive technique to the more usual nonadaptive experimental design 
in this paper we focus on the adaptation of boosting to grammatical inference we aim at improving the performance of state merging algorithm in the presence of noisy data by using in the update rule additional information provided by an oracle this strategy requires the construction of a new weighting scheme that take into account the confidence in the label of the example we prove that our new framework preserve the theoretical property of boosting using the state merging algorithm rpni we describe an experimental study on various datasets showing a dramatic improvement of performance 
a standard method for approximating average in probabilistic model is to construct a markov chain in the product space of the random variable with the desired equilibrium distribution since the number of configuration in this space grows exponentially with the number of random variable we often need to represent the distribution with sample in this paper we show that if one is interested in average over single variable only an alternative markov chain defined on the much smaller union space which can be evolved exactly becomes feasible the transition kernel of this markov chain is based on conditional distribution for pair of variable and we present way to approximate them using approximate inference algorithm such a mean field factorized neighbor and belief propagation robustness to these approximation and error bound on the estimate follow from stability analysis for markov chain we also present idea on a new class of algorithm that iterate between increasingly accurate estimate for conditional and marginal distribution experiment validate the proposed method 
we use clustering to derive new relation which augment database schema used in automatic generation of predictive feature in statistical relational learning entity derived from cluster increase the expressivity of feature space by creating new first class concept which contribute to the creation of new feature for example in citeseer paper can be clustered based on word or citation giving topic and author can be clustered based on document they co author giving community such cluster derived concept become part of more complex feature expression out of the large number of generated feature those which improve predictive accuracy are kept in the model a decided by statistical feature selection criterion we present result demonstrating improved accuracy on two task venue prediction and link prediction using citeseer data 
we propose a new set of criterion for learning algorithm in multi agent system one that is more stringent and we argue better justified than previous proposed criterion our criterion which apply most straightforwardly in repeated game with average reward consist of three requirement a against a specified class of opponent this class is a parameter of the criterion the algorithm yield a payoff that approach e the payoff of the best response b against other opponent the algori thm s payoff at least approach and possibly exceed the security level payoff or maximin value and c subject to these requirement the algo rithm achieve a close to optimal payoff in self play we furthermore require that these average payoff be achieved quickly we then present a novel algorithm and show that it meet these new criterion for a particular par ameter class the class of stationary opponent finally we show that the algorithm is effective not only in theory but also empirically using a recently introduced comprehensive game theoretic test suite we show that the algorithm almost universally outperforms previous learning algorithm 
the scrubber system monitor problem in the local loop of the telephonenetwork making automated decision on ten of million of case a year many of which lead to automated action scrubber save bell atlantic millionsof dollar annually by reducing the number of inappropriate technician dispatch scrubber s core knowledge base the trouble isolation module tim is a probability estimation tree constructed via several data mining process tim currently is deployed in the 
in this paper we present result of a study on brain computer interfacing we adopted an approach of farwell donchin which we tried to improve in several aspect the main objective wa to improve the transfer rate based on offline analysis of eeg data but within a more realistic setup closer to an online realization than in the original study the objective wa achieved along two different track on the one hand we used state of the art machine learning technique for signal classification and on the other hand we augmented the data space by using more electrode for the interface for the classification task we utilized svms and a motivated by recent finding on the learning of discriminative density we accumulated the value of the classification function in order to combine several classification which finally lead to significantly improved rate a compared with technique applied in the original work in combination with the data space augmentation we achieved competitive transfer rate at an average of bit min and with a maximum of bit min 
learning in a multiagent system is a challenging problem due to two key factor first if other agent are simultaneously learning then the environment is no longer stationary thus undermining convergence guarantee second learning is often susceptible to deception where the other agent may be able to exploit a learner s particular dynamic in the worst case this could result in poorer performance than if the agent wa not learning at all these challenge are identifiable in the two most common evaluation criterion for multiagent learning algorithm convergence and regret algorithm focusing on convergence or regret in isolation are numerous in this paper we seek to address both criterion in a single algorithm by introducing giga wolf a learning algorithm for normalform game we prove the algorithm guarantee at most zero average regret while demonstrating the algorithm converges in many situation of self play we prove convergence in a limited setting and give empirical result in a wider variety of situation these result also suggest a third new learning criterion combining convergence and regret which we call negative non convergence regret nnr 
current model of the classification problem do not effectively handle burst of particular class coming in at different time in fact the current model of the classification problem simply concentrate on method for one pas classification modeling of very large data set our model for data stream classification view the data stream classification problem from the point of view of a dynamic approach in which simultaneous training and testing stream are used for dynamic classification of data set this model reflects real life situation effectively since it is desirable to classify test stream in real time over an evolving training and test stream the aim here is to create a classification system in which the training model can adapt quickly to the change of the underlying data stream in order to achieve this goal we propose an on demand classification process which can dynamically select the appropriate window of past training data to build the classifier the empirical result indicate that the system maintains a high classification accuracy in an evolving data stream while providing an efficient solution to the classification task 
intelligence agency are under increasing pressure to connect the dot between fragment of evidence from disparate source to enable preemption of potential threat such a terrorist attack most system for threat detection in use today provide only data visualization tool for manual link analysis leading to method that do not scale to massive data set the cadre system continuous analysis and discovery from relational evidence address this deficiency by automating the link analysis process cadre combine an expressive knowledge representation of threat pattern with efficient constraint based abductive reasoning algorithm to automatically infer link and construct coherent threat hypothesis from structured data a compact factored representation of multiple hypothesis avoids redundant storage and enables scaling to large data set cadre efficiently manages the growth of the hypothesis using probabilistic evaluation model and a consistency checking algorithm to prune unlikely hypothesis 
the equivalent kernel is a way of understanding how gaussian process regression work for large sample size based on a continuum limit in this paper we show how to approximate the equivalent kernel of the widely used squared exponential or gaussian kernel and related kernel and how analysis using the equivalent kernel help to understand the learning curve for gaussian process 
much of the work in machine learning ha focused on demonstrating the efficacy of learning technique using training and testing phase on line learning over the long term place different demand on symbolic machine learning technique and raise a different set of question for symbolic learning than for empirical learning we have instrumented soar to collect data and characterize the long term learning behavior of soar and demonstrate an effective approach to the utility problem in this paper we describe our approach and provide result 
we present a new approach to estimating mixture model based on a new inference principle we have proposed the latent maximum entropy principle lme lme is different both from jaynes maximum entropy principle and from standard maximum likelihood estimation we demonstrate the lme principle by deriving new algorithm for mixture model estimation and show how robust new variant of the em algorithm can be developed our experiment show that estimation based on lme generally yield better result than maximum likelihood estimation particularly when inferring latent variable model from small amount of data 
traditional non parametric statistical learning technique are often computationally attractive but lack the same generalization and model selection ability a state of the art bayesian algorithm which however are usually computationally prohibitive this paper make several important contribution that allow bayesian learning to scale to more complex real world learning scenario firstly we show that backfitting a traditional non parametric yet highly efficient regression tool can be derived in a novel formulation within an expectation maximization em framework and thus can finally be given a probabilistic interpretation secondly we show that the general framework of sparse bayesian learning and in particular the relevance vector machine rvm can be derived a a highly efficient algorithm using a bayesian version of backfitting at it core a we demonstrate on several regression and classification benchmark bayesian backfitting offer a compelling alternative to current regression method especially when the size and dimensionality of the data challenge computational resource 
we propose a soft greedy learning algorithm for building small conjunction of simple threshold function called ray deflned on single real valued attribute we also propose a pac bayes risk bound which is minimized for classiflers achieving a non trivial tradeofi between sparsity the number of ray used and the magnitude of the separating margin of each ray finally we test the soft greedy algorithm on four dna micro array data set 
pairwise coupling is a popular multi class classification method that combine all comparison for each pair of class this paper present two approach for obtaining class probability both method can be reduced to linear system and are easy to implement we show conceptually and experimentally that the proposed approach are more stable than the two existing popular method voting and the method by hastie and tibshirani 
abstract in reinforcement learning rl there ha been some experimental evidence that the residual gradient algorithm converges slower than the td algorithm in this paper we use the concept of asymptotic convergence rate to prove that under certain condition the synchronous o policy td algorithm converges faster than the synchronous o policy residual gradient algorithm if the value function is represented in tabular form this is the rst theoretical result comparing the convergence behaviour of two rl algorithm we also show that a soon a linear function approximation is involved no general state ment concerning the superiority of one of the algorithm can be made 
we present an account of human concept learning that is learning of category from example based on the principle of minimum description length mdl in support of this theory we tested a wide range of two dimensional concept type including both regular simple and highly irregular complex structure and found the mdl theory to give a good account of subject performance this suggests that the intrinsic complexity of a concept that is it description length systematically influence it learnability 
this work is concerned with the question of how to combine online an ensemble of active learner so a to expedite the learning progress in pool based active learning we develop an active learning master algorithm based on a known competitive algorithm for the multi armed bandit problem a major challenge in successfully choosing top performing active learner online is to reliably estimate their progress during the learning session to this end we propose a simple maximum entropy criterion that provides effective estimate in realistic setting we study the performance of the proposed master algorithm using an ensemble containing two of the best known active learning algorithm a well a a new algorithm the resulting active learning master algorithm is empirically shown to consistently perform almost a well a and sometimes outperform the best algorithm in the ensemble on a range of classification problem 
we investigate the explore exploit trade off in reinforcement learning using competitive analysis applied to an abstract model we state and prove lower and upper bound on the competitive ratio the essential conclusion of our analysis is that optimizing the explore exploit trade off is much easier with a few piece of extra knowledge such a the stopping time or upper and lower bound on the value of the optimal exploitation policy 
we are developing a technique to predict travel time of a vehicle for an objective road section based on real time traffic data collected through a probe car system in the area of intelligent transport system it travel time prediction is an important subject probe car system is an upcoming data collection method in which a number of vehicle are used a moving sensor to detect actual traffic situation it can collect data concerning much larger area compared with traditional fixed detector our prediction technique is based on statistical analysis using ar model with seasonal adjustment and mdl minimum description length criterion seasonal adjustment is used to handle periodicity of hour in traffic data alternatively we employ state space model which can handle time series with periodicity it is important to select really effective data for prediction among the data from widespread area which are collected via probe car system we do this using mdl criterion that is we find the explanatory variable that really have influence on the future travel time in this paper we experimentally show effectiveness of our method using probe car data collected in nagoya metropolitan area in 
we present analog neuromorphic circuit for implementing bistable synapsis with spike timing dependent plasticity stdp property in these type of synapsis the short term dynamic of the synaptic efficacy are governed by the relative timing of the preand post synaptic spike while onlongtime scale the efficacy tendasymptotically to eithera potentiated state or to a depressed one test result from a prototype vlsi chip containing a learning synapse connected to a preand post synaptic integrate andfire neuron demonstrate the synapse s stdp learning property and it long term bistable characteristic 
brain computer interface bci are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intention into a control signal for device like computer or neuroprostheses if this can be done bypassing the usual human output pathway like peripheral nerve and muscle it can ultimately become a valuable tool for paralyzed patient most activity in bci research is devoted to finding suitable feature and algorithm to increase information transfer rate itrs the present paper study the implication of using more class e g left v right hand v foot for operating a bci we contribute by a theoretical study showing under some mild assumption that it is practically not useful to employ more than three or four class two extension of the common spatial pattern csp algorithm one interestingly based on simultaneous diagonalization and controlled eeg experiment that underline our theoretical finding and show excellent improved itrs 
a text corpus become larger tradeos between speed and accuracy become critical slow but accurate method may not complete in a practical amount of time in order to make the training data a manageable size a data reduction technique may be necessary subsampling for example speed up a classier by randomly removing training point in this paper we describe an alternate method for reducing the number of training point by combining training point such that important statistical information is retained our algorithm keep the same statistic that fast linear time text algorithm like rocchio and naive bayes use we provide empirical result that show our data reduction technique compare favorably to three other data reduction technique on four standard text corpus 
most information extraction ie system treat separate potential extraction a independent however in many case considering inuences between different potential extraction could improve overall accuracy statistical method based on undirected graphical model such a conditional random eld crfs have been shown to be an effective approach to learning accurate ie system we present a new ie method that employ relational markov network which can represent arbitrary dependency between extraction this allows for collective information extraction that exploit the mutual inuence between possible extraction experiment on learning to extract protein name from biomedical text demonstrate the advantage of this approach 
face detection is a canonical example of a rare event detection problem in which target pattern occur with much lower frequency than nontargets out of million of face sized window in an input image for example only a few will typically contain a face viola and jones recently proposed a cascade architecture for face detection which successfully address the rare event nature of the task a central part of their method is a feature selection algorithm based on adaboost we present a novel cascade learning algorithm based on forward feature selection which is two order of magnitude faster than the viola jones approach and yield classifier of equivalent quality this faster method could be used for more demanding classification task such a on line learning 
the application of latent hidden variable dynamic bayesian network is constrained by the complexity of marginalising over latent variable for this reason either small latent dimension or gaussian latent conditional table linearly dependent on past state are typically considered in order that inference is tractable we suggest an alternative approach in which the latent variable are modelled using deterministic conditional probability table this specialisation ha the advantage of tractable inference even for highly complex non linear non gaussian visible conditional probability table this approach enables the consideration of highly complex latent dynamic whilst retaining the benet of a tractable probabilistic model 
abstract we consider the general problem of utilizing both labeled and unlabeleddata to improve classification accuracy under the assumptionthat the data lie on a submanifold in a high dimensional space we develop an algorithmic framework to classify a partially labeleddata set in a principled manner the central idea of our approach isthat classification function are naturally defined only on the submanifoldin question rather than the total ambient space using thelaplace beltrami 
advance in imaging technique have led to large repository of image there is an increasing demand for automated system that can analyze complex medical image and extract meaningful information for mining pattern here we describe a real life image mining application to the problem of tumour cell counting the quantitative analysis of tumour cell is fundamental to characterizing the activity of tumour cell existing approach are mostly manual time consuming and subjective effort to automate the process of cell counting have largely focused on using image processing technique only our study indicate that image processing alone is unable to give accurate result in this paper we examine the use of extracted feature rule to aid in the process of tumor cell counting we propose a robust local adaptive thresholding and dynamic water immersion algorithm to segment region of interesting from background meaningful feature are then extracted from the segmented region a number of base classifier are built to generate feature rule to help identify the tumor cell two voting strategy are implemented to combine the base classifier into a meta classifier experiment result indicate that this process of using extracted feature rule to help identify tumor cell lead to better accuracy than pure image processing technique alone 
many clustering algorithm fail when dealing with high dimensional data principal component analysis pca is a popular dimensionality reduction algorithm however it assumes a single multivariate gaussian model which provides a global linear projection of the data mixture of probabilistic principal component analyzer ppca provides a better model to the clustering paradigm it provides a local linear pca projection for each multivariate gaussian cluster component we extend this model to build hierarchical mixture of ppca hierarchical clustering provides a flexible representation showing relationship among cluster in various perceptual level we introduce an automated hierarchical mixture of ppca algorithm which utilizes the integrated classification likelihood a a criterion for splitting and stopping the addition of hierarchical level an automated approach requires automated method for initialization determining the number of principal component dimension and determining when to split cluster we address each of these in the paper this automated approach result in a coarse to fine local component model with varying projection and with different number of dimension for each cluster 
this paper address the problem of classification in situation where the data distribution is not homogeneous data instance might come from different location or time and therefore are sampled from related but different distribution in particular feature s may appear in some part of the data that are rarely or never seen in others in most situation with nonhomogeneous data the training data is not representative of the distribution under which the classifier must operate we propose a method based on probabilistic graphical model for utilizing unseen feature during classification our method introduces for each such unseen feature a continuous hidden variable describing it influence on the class whether it tends to be associated with some label we then use probabilistic inference over the test data to infer a distribution over the value of this hidden variable intuitively we learn the role of this unseen feature from the test set generalizing from those instance whose label we are fairly sure about our overall probabilistic model is learned from the training data in particular we also learn model for characterizing the role of unseen feature these model use meta feature of those feature such a word in the neighborhood of an unseen feature to infer it role we present result for this framework on the task of classifying news article and web page showing significant improvement over model that do not use unseen feature 
this paper study the problem of categorical data clustering especially for transactional data characterized by high dimensionality and large volume starting from a heuristic method of increasing the height to width ratio of the cluster histogram we develop a novel algorithm clope which is very fast and scalable while being quite effective we demonstrate the performance of our algorithm on two real world datasets and compare clope with the state of art algorithm 
we describe an approach to building brain computer interface bci based on graphical model for probabilistic inference and learning we show how a dynamic bayesian network dbn can be used to infer probability distribution over brainand body state during planning and execution of action the dbn is learned directly from observed data and allows measured signal such a eeg and emg to be interpreted in term of internal state such a intent to move preparatory activity and movement execution unlike traditional classification based approach to bci the proposed approach allows continuous tracking and prediction of internal state over time and generates control signal based on an entire probability distribution over state rather than binary yes no decision we present preliminary result of brainand body state estimation using simultaneous eeg and emg signal recorded during a self paced left right hand movement task 
although discriminatively trained classifi er are usually more accurate when labeled training data is abundant previous work ha shown that when training data is limited generative classifi er can out perform them this paper describes a hybrid model in which a high dimensional subset of the parameter are trained to maximize generative likelihood and another small subset of parameter are discriminatively trained to maximize conditional likelihood we give a sample complexity bound showing that in order to fi t the discriminative parameter well the number of training example required depends only on the logarithm of the number of feature occurrence and feature set size experimental result show that hybrid model can provide lower test error and can produce better accuracy coverage curve than either their purely generative or purely discriminative counterpart we also discus several advantage of hybrid model and advocate further work in this area 
clustering is a very well studied problem that attempt to group similar data point most traditional clustering algorithm assume that the data is provided without measurement error often however real world data set have such error and one can obtain estimate of these error we present a clustering method that incorporates information contained in these error estimate we present a new distance function that is based on the distribution of error in data using a gaussian model for error the distance function follows a chi square distribution and is easy to compute this distance function is used in hierarchical clustering to discover meaningful cluster the distance function is scale invariant so that clustering result are independent of unit of measuring data in the special case when the error distribution is the same for each attribute of data point the rank order of pair wise distance is the same for our distance function and the euclidean distance function the clustering method is applied to the seasonality estimation problem and experimental result are presented for the retail industry data a well a for simulated data where it outperforms classical clustering method 
the naive classifier is a well established mathematical model whose simplicity speed and accuracy have made it a popular choice for classification in ai and engineering in this paper we show that given n feature of interest it is possible to perform tractable exact model averaging ma over all n possible feature set model in fact we show that it is possible to calculate parameter for a single naive classifier c such that c produce prediction equivalent to those obtained by the full model averaging and we show that c can be constructed using the same time and space complexity required to construct a single naive classifier with map parameter we present experimental result which show that on average the ma classifier typically outperforms the map classifier on simulated data and we characterize how the relative performance varies with number of variable number of training record and complexity of the generating distribution finally we examine the performance of the ma naive model on the real world alarm and hepar network and show ma improved classification here a well 
we present a geometric approach to statistical shape analysis of closed curve in image the basic idea is to specify a space of closed curve satisfying given constraint and exploit the differential geometry of this space to solve optimization and inference problem we demonstrate this approach by i defining and computing statistic of observed shape ii defining and learning a parametric probability model on shape space and iii designing a binary hypothesis test on this space 
over the last year significant effort have been made to develop kernel that can be applied to sequence data such a dna text speech video and image the fisher kernel and similar variant have been suggested a good way to combine an underlying generative model in the feature space and discriminant classifier such a svm s in this paper we suggest an alternative procedure to the fisher kernel for systematically finding kernel function that naturally handle variable length sequence data in multimedia domain in particular for domain such a speech and image we explore the use of kernel function that take full advantage of well known probabilistic model such a gaussian mixture and single full covariance gaussian model we derive a kernel distance based on the kullback leibler kl divergence between generative model in effect our approach combine the best of both generative and discriminative method and replaces the standard svm kernel we perform experiment on speaker identification verification and image classification task and show that these new kernel have the best performance in speaker verification and mostly outperform the fisher kernel based svm s and the generative classifier in speaker identification and image classification 
regularization play a central role in the analysis of modern data where non regularized fitting is likely to lead to over fitted model useless for both prediction and interpretation we consider the design of incremental algorithm which follow path of regularized solution a the regularization varies these approach often result in method which are both efficient and highly flexible we suggest a general path following algorithm based on second order approximation prove that under mild condition it remains very close to the path of optimal solution and illustrate it with example 
astronomy increasingly face the issue of massive unwieldly data set the sloan digital sky survey sd ha so far generated ten of million of image of distant galaxy of which only a tiny fraction have been morphologically classified morphological classification in this context is achieved by fitting a parametric model of galaxy shape to a galaxy image this is a nonlinear regression problem whose challenge are threefold blurring of the image caused by atmosphere and mirror imperfection large number of local minimum and massive data set our strategy is to use the eigenimages of the parametric model to form a new feature space and then to map both target image and the model parameter into this feature space in this low dimensional space we search for the best image to parameter match to search the space we sample it by creating a database of many random parameter vector prototype and mapping them into the feature space the search problem then becomes one of finding the best prototype match so the fitting process a nearest neighbor search in addition to the saving realized by decomposing the original space into an eigenspace we can use the fact that the model is a linear sum of function to reduce the prototype further the only prototype stored are the component of the model function a modified form of nearest neighbor is used to search among them additional complication arise in the form of missing data and heteroscedasticity both of which are addressed with weighted linear regression compared to existing technique speed ups ach ieved are between and order of magnitude this should enable the analysis of the entire sd dataset 
the connectivity of the nervous system of the nematode caenorhabditis elegans ha been described completely but the analysis of the neuronal basis of behavior in this system is just beginning here we used an optimization algorithm to search for pattern of connectivity sufficient to compute the sensorimotor transformation underlying c elegans chemotaxis a simple form of spatial orientation behavior in which turning probability is modulated by the rate of change of chemical concentration optimization produced differentiator network with inhibitory feedback among all neuron further analysis showed that feedback regulates the latency between sensory input and behavior common pattern of connectivity between the model and biological network suggest new function for previously identified connection in the c elegans nervous system 
we have implemented a real time front end for detecting voiced speech and estimating it fundamental frequency the front end performs the signal processing for voice driven agent that attend to the pitch contour of human speech and provide continuous audiovisual feedback the algorithm we use for pitch tracking ha several distinguishing feature it make no use of ffts or autocorrelation at the pitch period it update the pitch incrementally on a sample by sample basis it avoids peak picking and doe not require interpolation in time or frequency to obtain high resolution estimate and it work reliably over a four octave range in real time without the need for postprocessing to produce smooth contour the algorithm is based on two simple idea in neural computation the introduction of a purposeful nonlinearity and the error signal of a least square fit the pitch tracker is used in two real time multimedia application a voice to midi player that synthesizes electronic music from vocalized melody and an audiovisual karaoke machine with multimodal feedback both application run on a laptop and display the user s pitch scrolling across the screen a he or she sings into the computer 
there exist many approach to clustering but the important issue of feature selection i e selecting the data attribute that are relevant for clustering is rarely addressed feature selection for clustering is difficult due to the absence of class label we propose two approach to feature selection in the context of gaussian mixture based clustering in the first one instead of making hard selection we estimate feature saliency an expectation maximization em algorithm is derived for this task the second approach extends koller and sahami s mutual informationbased feature relevance criterion to the unsupervised case feature selection is then carried out by a backward search scheme this scheme can be classified a a wrapper since it wrap mixture estimation in an outer layer that performs feature selection experimental result on synthetic and real data show that both method have promising performance 
we study how to learn to play a pareto optimal strict nash equilibrium when there exist multiple equilibrium and agent may have different preference among the equilibrium we focus on repeated coordination game of non identical interest where agent do not know the game structure up front and receive noisy payoff we design efficient near optimal algorithm for both the perfect monitoring and the imperfect monitoring setting where the agent only observe their own payoff and the joint action 
abstract we propose an approach to learning the semantics of image which allows u to automatically annotate an image with keywords and to retrieve image based on text query we do this using a formalism that model the generation of annotated image we assume that every image is divided into region each described by a continuous valued feature vector given a training set of image with annotation we compute a joint probabilistic model of image feature and word which allow u to predict the probability of generating a word given the image region this may be used to automatically annotate and retrieve image given a word a a query experiment show that our model significantly outperforms the best of the previously reported result on the task of automatic image annotation and retrieval 
one of the central challenge in reinforcement learning is to balance the exploration exploitation tradeoff while scaling up to large problem although model based reinforcement learning ha been le prominent than value based method in addressing these challenge recent progress ha generated renewed interest in pursuing modelbased approach theoretical work on the exploration exploitation tradeoff ha yielded provably sound model based algorithm such a e and rmax while work on factored mdp representation ha yielded model based algorithm that can scale up to large problem recently the benefit of both achievement have been combined in the factored e algorithm of kearns and koller in this paper we address a significant shortcoming of factored e namely that it requires an oracle planner that cannot be feasibly implemented we propose an alternative approach that us a practical approximate planner approximate linear programming that maintains desirable property further we develop an exploration strategy that is targeted toward improving the performance of the linear programming algorithm rather than an oracle planner this lead to a simple exploration strategy that visit state relevant to tightening the lp solution and achieves sample efficiency logarithmic in the size of the problem description our experimental result show that the targeted approach performs better than using approximate planning for implementing either factored e or factored rmax 
we consider the problem of unsupervised learning from a matrix of data vector where in each row the observed value are randomly permuted in an unknown fashion such problem arise naturally in area such a computer vision and text modeling where measurement need not be in correspondence with the correct feature we provide a general theoretical characterization of the dicult y of unscrambling the value of the row for such problem and relate the optimal error rate to the well known concept of the bayes classication error rate for known parametric distribution we derive closed form expression for the optimal error rate that provide insight into what make this problem difcult in practice finally we show how the expectation maximization procedure can be used to simultaneously estimate both a probabilistic model for the feature a well a a distribution over the correspondence of the row value 
online mechanism design md considers the problem of providing incentive to implement desired system wide outcome in system with self interested agent that arrive and depart dynamically agent can choose to misrepresent their arrival and departure time in addition to information about their value for dieren t outcome we consider the problem of maximizing the total longterm value of the system despite the self interest of agent the online md problem induces a markov decision process mdp which when solved can be used to implement optimal policy in a truth revealing bayesian nash equilibrium 
we are developing technology for generating english textual summary of time series data in three domain weather forecast gas turbine sensor reading and hospital intensive care data our weather forecast generator is currently operational and being used daily by a meteorological company we generate summary in three step a selecting the most important trend and pattern to communicate b mapping these pattern onto word and phrase and c generating actual text based on these word and phrase in this paper we focus on the first step a selecting the information to communicate and describe how we perform this using modified version of standard data analysis algorithm such a segmentation the modification arose out of empirical work with user and domain expert and in fact can all be regarded a application of the gricean maxim of quality quantity relevance and manner which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text the gricean maxim are perhaps a key element of adapting data analysis algorithm for effective communication of information to human user and should be considered by other researcher interested in communicating data to human user 
inference and adaptation in noisy and changing rich sensory environment are rife with a variety of specific sort of variability experimental and theoretical study suggest that these different form of variability play different behavioral neural and computational role and may be reported by different notably neuromodulatory system here we refine our previous theory of acetylcholine s role in cortical inference in the oxymoronic term of expected uncertainty and advocate a theory for norepinephrine in term of unexpected uncertainty we suggest that norepinephrine report the radical divergence of bottom up input from prevailing top down interpretation to influence inference and plasticity we illustrate this proposal using an adaptive factor analysis model 
in this paper we explore the use of random forest rf in the structured language model slm which us rich syntactic information in predicting the next word based on word already seen the goal in this work is to construct rf by randomly growing decision tree dts using syntactic information and investigate the performance of the slm modeled by the rf in automatic speech recognition rf which were originally developed a classifier are a combination of decision tree classifier each tree is grown based on random training data sampled independently and with the same distribution for all tree in the forest and a random selection of possible question at each node of the decision tree our approach extends the original idea of rf to deal with the data sparseness problem encountered in language modeling rf have been studied in the context of n gram language modeling and have been shown to generalize well to unseen data we show in this paper that rf using syntactic information can also achieve better performance in both perplexity ppl and word error rate wer in a large vocabulary speech recognition system compared to a baseline that us kneser ney smoothing 
source separation is an important problem at the intersection of several field including machine learning signal processing and speech technology here we describe new separation algorithm which are based on probabilistic graphical model with latent variable in contrast with existing method these algorithm exploit detailed model to describe source property they also use subband filtering idea to model the reverberant environment and employ an explicit model for background and sensor noise we leverage variational technique to keep the computational complexity per em iteration linear in the number of frame 
privacy and security concern can prevent sharing of data derailing data mining project distributed knowledge discovery if done correctly can alleviate this problem the key is to obtain valid result while providing guarantee on the non disclosure of data we present a method for k mean clustering when different site contain different attribute for a common set of entity each site learns the cluster of each entity but learns nothing about the attribute at other site 
we examine the problem of generating state space compression of pomdps in a way that minimally impact decision quality we analyze the impact of compression on decision quality observing that compression tha t allow accurate policy evaluation prediction of expected future reward will not affect decision quality we derive a set of sufficient condition that ensure accu rate prediction in this respect illustrate interesting mathematical property these confer on lossless linear compression and use these to derive an iterative proce dure for finding good linear lossy compression we also elaborate on how structured representation of a pomdp can be used to find such compression 
we empirically evaluate several state of theart method for constructing ensemble of heterogeneous classiers with stacking and show that they perform at best comparably to selecting the best classier from the ensemble by cross validation we then propose a new method for stacking that us multi response model tree at the meta level and show that it clearly outperforms existing stacking approach and selecting the best classier by cross validation the work presented in this paper is set in the stacking framework we argue that selecting the best of the classiers in an ensemble generated by applying different learning algorithm should be considered a a baseline to which the stacking performance should be compared our empirical evaluation of several recent stacking approach show that they perform comparably to the best of the individual classiers a selected by cross validation but not better we then propose a new stacking method based on classication by using model tree and show that this method doe perform better than other combining approach a well a better than selecting the best individual classier section rst summarizes the stacking framework then survey some recent result and nally introduces our stacking approach based on classication via model tree the setup for the experimental comparison of several stacking method voting and selecting the best classier is described in section section present and discus the experimental result and section concludes 
the standard norm svm is known for it good performance in twoclass classification in this paper we consider the norm svm we argue that the norm svm may have some advantage over the standard norm svm especially when there are redundant noise feature we also propose an efficient algorithm that computes the whole s olution path of the norm svm hence facilitates adaptive selection of the tuning parameter for the norm svm 
we present a new type of multi class learning algorithm called a linear max algorithm linearmax algorithm learn with a special type of attribute called a sub expert a sub expert is a vector attribute that ha a value for each output class the goal of the multi class algorithm is to learn a linear function combining the sub expert and to use this linear function to make correct class prediction the main contribution of this work is to prove that in the on line mistake bounded model of learning a multi class sub expert learning algorithm ha the same mistake bound a a related two class linear threshold algorithm we apply these technique to three linear threshold algorithm perceptron winnow and romma we show these algorithm give good performance on artificial and real datasets 
in this paper we present a new co training strategy that make use of unlabelled data it train two predictor in parallel with each predictor labelling the unlabelled data for training the other predictor in the next round both predictor are support vector machine one trained using data from the original feature space the other trained with new feature that are derived by clustering both the labelled and unlabelled data hence unlike standard co training method our method doe not require a priori the existence of two redundant view either of which can be used for classification nor is it dependent on the availability of two different supervised learning algorithm that complement each other we evaluated our method with two classifier and three text benchmark webkb reuters newswire article and newsgroups our evaluation show that our co training technique improves text classification accuracy especially when the number of labelled example are very few 
this paper present a novel local feature selection approach for text categorization it construct a feature set for each category by first selecting a set of term highly indicative of membership a well a another set of term highly indicative of non membership then unifying the two set the size ratio of the two set wa empirically chosen to obtain optimal performance this is in contrast with the standard local feature selection approach that either only select the term most indicative of membership or implicitly but not optimally combine the term most indicative of membership with non membership the experimental comparison between the proposed approach and standard approach wa conducted on four feature selection metric chisquare correlation coefficient odds ratio and g coefficient the result show that the proposed approach improves text categorization performance 
we propose a new statistical approach to extracting personal name from a corpus one of the key point of our approach is that it can both automatically learn the characteristic of personal name from a large training corpus and make good use of human empirical knowledge e g context free grammar furthermore our approach also assigns confidence measure to the extracted personal name compared with traditional simple true false determination another main contribution of this work is that we have applied the personal name extraction technology into a real application which is a chinese inputting system and have achieved an approximately error rate reduction for all character and error rate reduction for personal name 
randomization is an economical and efficient approach for privacy preserving data mining ppdm in order to guarantee the performance of data mining and the protection of individual privacy optimal randomization scheme need to be employed this paper demonstrates the construction of optimal randomization scheme for privacy preserving density estimation we propose a general framework for randomization using mixture model the impact of randomization on data mining is quantified by performance degradation and mutual information loss while privacy and privacy loss are quantified by interval based metric two different type of problem are defined to identify optimal randomization for ppdm illustrative example and simulation result are reported 
abstract high retrieval precision in content based image retrieval can be attained by adopting relevance feedback mechanism these mechanism require that the user judge the quality of the result of the query by marking all the retrieved image a being either relevant or not then the search engine exploit this information to adapt the search to better meet user s need at present the vast majority of proposed relevance feedback mechanism are formulated in term of search model that ha to be optimized such an optimization involves the modification of some search parameter so that the nearest neighbor of the query vector contains the largest number of relevant image in this paper a different approach to relevance feedback is proposed after the user provides the first feedback following retrieval are not based on knn search but on the computation of a relevance score for each image of the database this score is computed a a function of two distance namely the distance from the nearest non relevant image and the distance from the nearest relevant one image are then ranked according to this score and the top k image are displayed reported result on three image data set show that the proposed mechanism outperforms other state of the art relevance feedback mechanism in t rod u ct i on 
we show that two important property of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequence the property are simple cell like receptive field and complex cell like pooling of simple cell output which emerge when we apply two different approach to temporal coherence in the first approach we extract receptive field whose output are a temporally coherent a possible this approach yield simple cell like receptive field oriented localized multiscale thus temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive field the second approach is based on a two layer statistical generative model of natural image sequence in addition to modeling the temporal coherence of individual simple cell this model includes inter cell temporal dependency estimation of this model from natural data yield both simple cell like receptive field and complex cell like pooling of simple cell output in this completely unsupervised learning both layer of the generative model are estimated simultaneously from scratch this is a significant improvement on earlier statistical model of early vision where only one layer ha been learned and others have been fixed a priori 
although the v support vector machine v svm schslkopf et al ha the advantage of using a single parameter v to control both the number of support vector and the fraction of margin error there are two issue that prevent it from being used in many real world application first unlike the c svm that allows asymmetric misclassification cost v svm us a symmetric misclassification cost while lower error rate is promoted by this symmetric mi qclassification cost it is not always the preferred measure in many application second the additional constraint from vsvm make it training more difficult sequential minimal optimization smo algorithm that are very easy to implement and scalable to very large problem do not exist in a good form for v svm in this paper we proposed two new v svm formulation these formulation introduce mean to control the misclassification cost ratio between false positive and false negative while preserving the intuitive parameter v we also propose a smo algorithm for the v svm classification problem experiment show that our new v svm formulation is effective in incorporating asymmetric misclassification cost and the smo algorithm for v svm is comparable in speed to that for c svm 
this paper devise a novel kernel function for structured natural language data in the field of natural language processing feature extraction consists of the following two step syntactically and semantically analyzing raw data i e character string then representing the result a discrete structure such a parse tree and dependency graph with part of speech tag creating possibly high dimensional numerical feature vector from the discrete structure the new kernel called hierarchical directed acyclic graph hdag kernel directly accept dag whose node can contain dag hdag data structure are needed to fully reflect the syntactic and semantic structure that natural language data inherently have in this paper we define the kernel function and show how it permit efficient calculation experiment demonstrate that the proposed kernel are superior to existing kernel function e g sequence kernel tree kernel and bag of word kernel 
we develop and test new machine learning method for the predictionof topological representation of protein structure in the formof coarseor fine grained contact or distance map that are translationand rotation invariant the method are based on generalizedinput output hidden markov model giohmms and generalizedrecursive neural network grnns the method are used to predicttopology directly in the fine grained case and in the coarsegrainedcase indirectly by first 
a new distance measure between probability density function pdfs is introduced which we refer to a the laplacian pdf distance the laplacian pdf distance exhibit a remarkable connection to mercer kernel based learning theory via the parzen window technique for density estimation in a kernel feature space dened by the eigenspectrum of the laplacian data matrix this pdf distance is shown to measure the cosine of the angle between cluster mean vector the laplacian data matrix and hence it eigenspectrum can be obtained automatically based on the data at hand by optimal parzen window selection we show that the laplacian pdf distance ha an interesting interpretation a a risk function connected to the probability of error 
in this paper we define and study a novel text mining problem which we refer to a comparative text mining ctm given a set of comparable text collection the task of comparative text mining is to discover any latent common theme across all collection a well a summarize the similarity and difference of these collection along each common theme this general problem subsumes many interesting application including business intelligence and opinion summarization we propose a generative probabilistic mixture model for comparative text mining the model simultaneously performs cross collection clustering and within collection clustering and can be applied to an arbitrary set of comparable text collection the model can be estimated efficiently using the expectation maximization em algorithm we evaluate the model on two different text data set i e a news article data set and a laptop review data set and compare it with a baseline clustering method also based on a mixture model experiment result show that the model is quite effective in discovering the latent common theme across collection and performs significantly better than our baseline mixture model 
biochemical signal transduction network are the biological information processing system by which individual cell from neuron to amoeba perceive and respond to their chemical environment we introduce a simplified model of a single biochemical relay and analyse it capacity a a communication channel a diffusible ligand is released by a sending cell and received by binding to a transmembrane receptor protein on a receiving cell this receptor ligand interaction creates a nonlinear communication channel with non gaussian noise we model this channel numerically and study it response to input signal of different frequency in order to estimate it channel capacity stochastic effect introduced in both the diffusion process and the receptor ligand interaction give the channel low pas characteristic we estimate the channel capacity using a water filling formula adapted from the additive white noise gaussian channel 
variational inference method including mean field method and loopy belief propagation have been widely used for approximate probabilistic inference in graphical model while often le accurate than mcmc variational method provide a fast deterministic approximation to marginal and conditional probability such approximation can be particularly useful in high dimensional problem where sampling method are too slow to be effective a limitation of current method however is that they are restricted to parametric probabilistic model mcmc doe not have such a limitation indeed mcmc sampler have been developed for the dirichlet process dp a nonparametric distribution on distribution ferguson that is the cornerstone of bayesian nonparametric statistic escobar west neal in this paper we develop a mean field variational approach to approximate inference for the dirichlet process where the approximate posterior is based on the truncated stick breaking construction ishwaran james we compare our approach to dp sampler for gaussian dp mixture model 
we present an algorithm to overcome the local maximum problem in estimating the parameter of mixture model it combine existing approach from both em and a robust tting algorithm ransac to give a data driven stochastic learning scheme minimal subset of data point sufcient to constrain the parameter of the model are drawn from proposal density to discover new region of high likelihood the proposal density are learnt using em and bias the sampling toward promising solution the algorithm is computationally efcient a well a effective at escaping from local maximum we compare it with alternative method including em and ransac on both challenging synthetic data and the computer vision problem of alpha matting 
in this paper we introduce new algorithm for unsupervised learningbased on the use of a kernel matrix all the information requiredby such algorithm is contained in the eigenvectors of thematrix or of closely related matrix we use two dierent but relatedcost function the alignment and the cut cost the rstone is discussed in a companion paper the second one is basedon graph theoretic concept both function measure the level ofclustering of a labeled dataset or 
complex object can often be conveniently represented by fin ite set of simpler component such a image by set of patch or text by bag of word we study the class of positive definite p d kerne l for two such object that can be expressed a a function of the merger of their respective set of component we prove a general integral representation of such kernel and present two particular example one of them lead to a kernel for set of point living in a space endowed itself with a positive definite kernel we provide experimental result o n a benchmark experiment of handwritten digit image classification whic h illustrate the validity of the approach 
we describe a self configuring neuromorphic chip that us a model of activity dependent axon remodeling to automatically wire topographic map based solely on input correlation axon are guided by growth cone which are modeled in analog vlsi for the first time growth cone migrate up neurotropin gradient which are represented by charge diffusing in transistor channel virtual axon move by rerouting address event we refined an initially gross topographic projection by simulating retinal wave input 
a unied biophysically motivated calcium dependent learning model ha been shown to account for various rate based and spike time dependent paradigm for inducing synaptic plasticity here we investigate the property of this model for a multi synapse neuron that receives input with dieren t spike train statistic in addition we present a physiological form of metaplasticity an activity driven regulation mechanism that is essential for the robustness of the model a neuron thus implemented develops stable and selective receptive eld given various input statistic 
we present a novel strategy for automatically debugging program given sampled data from thousand of actual user run our goal is to pinpoint those feature that are most correlated with crash this is accomplished by maximizing an appropriately defined utility function it ha analogy with intuitive debugging heuristic and a we demonstrate is able to deal with various type of bug that occur in real program 
we investigate the generalization performance of some learning problem in hilbert function space we introduce a concept of scalesensitive effective data dimension and show that it characterizes the convergence rate of the underlying learning problem using this concept we can naturally extend result for parametric estimation problem in finite dimensional space to non parametric kernel learning method we derive upper bound on the generalization performance and show that the resulting convergent rate are optimal under various circumstance 
government regulation are semi structured text document that are often voluminous heavily cross referenced between provision and even ambiguous multiple source of regulation lead to difficulty in both understanding and complying with all applicable code in this work we propose a framework for regulation management and similarity analysis an online repository for legal document is created with the help of text mining tool and user can access regulatory document either through the natural hierarchy of provision or from a taxonomy generated by knowledge engineer based on concept our similarity analysis core identifies relevant provision and brings them to the user s attention and this is performed by utilizing both the hierarchical and referential structure of regulation to provide a better comparison between provision preliminary result show that our system reveals hidden similarity that are not apparent between provision based on node content comparison 
we present a fast iterative algorithm for identifying the support vector of a given set of point our algorithm work by maintaining a candidate support vector set it us a greedy approach to pick point for inclusion in the candidate set when the addition of a point to the candidate set is blocked because of other point already present in the set we use a backtracking approach to prune away such point to speed up convergence we initialize our algorithm with the nearest pair of point from opposite class we then use an optimization based approach to increment or prune the candidate support vector set the algorithm make repeated pass over the data to satisfy the kkt constraint the memory requirement of our algorithm scale a o s in the average case where s is the size of the support vector set we show that the algorithm is extremely competitive a compared to other conventional iterative algorithm like smo and the npa we present result on a variety of real life datasets to validate our claim 
we have recently embarked on a three year project funded by the nasa astep program to develop robotic astrobiology in the process of learning the limit of life in the atacama desert of chile we see this a an opportunity to develop a more science aware rover one that on encountering a new area can select interesting feature perform initial experiment and selectively return relevant data all before receiving feedback from the science team several component of the proposed science autonomy system can make use of classiers is this the kind of rock we are looking for and clustering algorithm is this rock like anything we have already sampled the unknown character of unexplored area motivates use of on line learning technique 
we introduce an information theoretic method for nonparametric nonlinear dimensionality reduction based on the infinite cluster limit of rate distortion theory by constraining the information available to manifold coordinate a natural probabilistic map emerges that assigns original data to corresponding point on a lower dimensional manifold with only the information distortion trade off a a parameter our method determines the shape of the manifold it dimensionality the probabilistic map and the prior that provide optimal description of the data 
tracking human motion is an integral part to developing powerful human computer interface several successful tracking algorithm were developed that model human body a an articulated tree we propose a learning based method for creating such articulated model from observation of multiple rigid motion this paper is concerned with recovering topology of the articulated model when the rigid motion of constituent segment is known our approach is based on finding the maximum likelihood tree shaped factorization of the joint probability density function pdf of rigid segment motion the topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body we demonstrate the performance of our algorithm both on synthetic and real motion capture data 
in this paper we investigate the general problem of discovering recurrent pattern that are embedded in categorical sequence an important real world problem of this nature is motif discovery in dna sequence we investigate the fundamental aspect of this data mining problem that can make discovery easy or hard we present a general framework for characterizing learning in this context by deriving the bayes error rate for this problem under a markov assumption the bayes error framework demonstrates why certain pattern are much harder to discover than others it also explains the role of different parameter such a pattern length and pattern frequency in sequential discovery we demonstrate how the bayes error can be used to calibrate existing discovery algorithm providing a lower bound on achievable performance we discus a number of fundamental issue that characterize sequential pattern discovery in this context present a variety of empirical result to complement and verify the theoretical analysis and apply our methodology to real world motif discovery problem in computational biology 
we introduce a family of classifier based on a physical analogy to an electrostatic system of charged conductor the family called coulomb classifier includes the two best known support vector machine svms the svm and the c svm in the electrostatics analogy a training example corresponds to a charged conductor at a given location in space the classification function corresponds to the electrostatic potential function and the training objective function corresponds to the coulomb energy the electrostatic framework provides not only a novel interpretation of existing algorithm and their interrelationship but it suggests a variety of new method for svms including kernel that bridge the gap between polynomial and radial basis function objective function that do not require positive definite kernel regularization technique that allow for the construction of an optimal classifier in minkowski space based on the framework we propose novel svms and perform simulation study to show that they are comparable or superior to standard svms the experiment include classification task on data which are represented in term of their pairwise proximity where a coulomb classifier outperformed standard svms 
one of the major hurdle in maintaining long lived electronic system is that electronic part become obsolete no longer available from the original supplier when this occurs an engineer is tasked with resolving the problem by finding a replacement that is a similar a possible to the original part the current approach involves a laborious manual search through several electronic portal and data book the search is difficult because potential replacement may differ from the original and from each other by one or more parameter worse still the cumbersome nature of this process may cause the engineer to miss appropriate solution amid the many thousand of part listed in industry catalog in this paper we address this problem by introducing the notion of a parametric distance between electronic component we use this distance to search a large part data set and recommend likely replacement recommendation are based on an adaptive nearest neighbor search through the parametric data set for each user we learn how to scale the ax of the feature space in which the nearest neighbor are sought this allows the system to learn each user s judgment of the phrase a similar a possible 
we present ongoing work on a project for automatic recognition of spontaneous facial action spontaneous facial expression differ substantially from posed expression similar to how continuous spontaneous speech differs from isolated word produced on command previous method for automatic facial expression recognition assumed image were collected in controlled environment in which the subject deliberately faced the camera since people often nod or turn their head automatic recognition of spontaneous facial behavior requires method for handling out of image plane head rotation here we explore an approach based on d warping of image into canonical view we evaluated the performance of the approach a a front end for a spontaneous expression recognition system using support vector machine and hidden markov model this system employed general purpose learning mechanism that can be applied to recognition of any facial movement the system wa tested for recognition of a set of facial action defined by the facial action coding system facs we showed that d tracking and warping followed by machine learning technique directly applied to the warped image is a viable and promising technology for automatic facial expression recognition one exciting aspect of the approach presented here is that information about movement dynamic emerged out of filter which were derived from the statistic of image 
the problem of structure from motion is a central problem in vision given the d location of certain point we wish to recover the camera motion and the d coordinate of the point under simplied camera model the problem reduces to factorizing a measurement matrix into the product of two low rank matrix each element of the measurement matrix contains the position of a point in a particular image when all element are observed the problem can be solved trivially using svd but in any realistic situation many element of the matrix are missing and the one that are observed have a dieren t directional uncertainty under these condition most existing factorization algorithm fail while human perception is relatively unchanged in this paper we use the well known em algorithm for factor analysis to perform factorization this allows u to easily handle missing data and measurement uncertainty and more importantly allows u to place a prior on the temporal trajectory of the latent variable the camera position we show that incorporating this prior give a signican t improvement in performance in challenging image sequence 
in recent year variational method have become a popular tool for approximate inference and learning in a wide variety of probabilistic model for each new application however it is currently necessary rst to derive the variational update equation and then to implement them in application specic code each of these step is both time consuming and error prone in this paper we describe a general purpose inference engine called vibe variational inference for bayesian network which allows a wide variety of probabilistic model to be implemented and solved variationally without recourse to coding new model are specied either through a simple script or via a graphical interface analogous to a drawing package vibe then automatically generates and solves the variational equation we illustrate the power and exibilit y of vibe using example from bayesian mixture modelling 
we present a semi parametric latent variable model based technique for density modelling dimensionality reduction and visualization unlike previous method we estimate the latent distribution non parametrically which enables u to model data generated by an underlying low dimensional multimodal distribution in addition we allow the component of latent variable model to be drawn from the exponential family which make the method suitable for special data type for example binary or count data simulation on real valued binary and count data show favorable comparison to other related scheme both in term of separating different population and generalization to unseen sample 
we present a modified version of the perceptron learning algorithm pla which solves semidefinite program sdps in polynomial time the algorithm is based on the following three observation i semidefinite program are linear program with infinitely many linear constraint ii every linear program can be solved by a sequence of constraint satisfaction problem with linear constraint iii in general the perceptron learning algorithm solves a constraint satisfaction problem with linear constraint in finitely many update combining the pla with a probabilistic rescaling algorithm which on average increase the size of the feasable region result in a probabilistic algorithm for solving sdps that run in polynomial time we present preliminary result which demonstrate that the algorithm work but is not competitive with state of the art interior point method 
inner product operator often referred to a kernel in statistical learning define a mapping from some input space into a feature space the focus of this paper is the construction of biologically motivated kernel for cortical activity the kernel we derive termed spikernels map spike count sequence into an abstract vector space in which we can perform various prediction task we discus in detail the derivation of spikernels and describe an efficient algorithm for computing their value on any two sequence of neural population spike count we demonstrate the merit of our modeling approach using the spikernel and various standard kernel for the task of predicting hand movement velocity from cortical recording in all of our experiment all the kernel we tested outperform the standard scalar product used in regression with the spikernel consistently achieving the best performance 
this paper present an algorithm for learning the time varying shape of a non rigid d object from uncalibrated d tracking data we model shape motion a a rigid component rotation and translation combined with a nonrigid deformation reconstruction is ill posed if arbitrary deformation are allowed we constrain the problem by assuming that the object shape at each time instant is drawn from a gaussian distribution based on this assumption the algorithm simultaneously estimate d shape and motion for each time frame learns the parameter of the gaussian and robustly fill in missing data point we then extend the algorithm to model temporal smoothness in object shape thus allowing it to handle severe case of missing data 
one fundamental challenge for mining recurring subgraphs from semi structured data set is the overwhelming abundance of such pattern in large graph database the total number of frequent subgraphs can become too large to allow a full enumeration using reasonable computational resource in this paper we propose a new algorithm that mine only maximal frequent subgraphs i e subgraphs that are not a part of any other frequent subgraphs this may exponentially decrease the size of the output set in the best case in our experiment on practical data set mining maximal frequent subgraphs reduces the total number of mined pattern by two to three order of magnitude our method first mine all frequent tree from a general graph database and then reconstructs all maximal subgraphs from the mined tree using two chemical structure benchmark and a set of synthetic graph data set we demonstrate that in addition to decreasing the output size our algorithm can achieve a five fold speed up over the current state of the art subgraph mining algorithm 
address event representation aer originally proposed a a mean to communicate sparse neural event between neuromorphic chip ha proven efficient in implementing large scale network with arbitrary configurable synaptic connectivity in this work we further extend the functionality of aer to implement arbitrary configurable synaptic plasticity in the address domain a proof of concept we implement a biologically inspired form of spike timing dependent plasticity stdp based on relative timing of event in an aer framework experimental result from an analog vlsi integrate and fire network demonstrate address domain learning in a task that requires neuron to group correlated input 
a fast growing body of research in the ai and machine learning community address learning in game where there are multiple learner with different interest this research add to more established research on learning in game conducted in economics in part because of a clash of field there are widely varying requirement on learning algorithm in this domain the goal of this paper is to demonstrate how communication complexity can be used a a lower bound on the required learning time or cost because this lower bound doe not assume any requirement on the learning algorithm it is universal applying under any set of requirement on the learning algorithm we characterize exactly the communication complexity of various solution concept from game theory namely nash equilibrium iterated dominant strategy both strict and weak and backwards induction this give the tighest lower bound on learning in game that can be obtained with this method 
abstract we describe a nonparametric bayesian approach to generalizing from few labeled example guided by a larger set of unlabeled object and the assumption of a latent tree structure to the domain the tree or a distribution over tree may be inferred using the unlabeled data a prior over concept generated by a mutation process on the inferred tree s allows efficient computation of the optimal bayesian classification function from the labeled example we test our approach on eight real world datasets 
traditional intrusion detection system id detect attack by comparing current behavior to signature of known attack one main drawback is the inability of detecting new attack which do not have known signature in this paper we propose a learning algorithm that construct model of normal behavior from attack free network traffic behavior that deviate from the learned normal model signal possible novel attack our id is unique in two respect first it is nonstationary modeling probability based on the time since the last event rather than on average rate this prevents alarm flood second the id learns protocol vocabulary at the data link through application layer in order to detect unknown attack that attempt to exploit implementation error in poorly tested feature of the target software on the darpa id evaluation data set we detect of attack with false alarm about evenly divided between user behavioral anomaly ip address and port a modeled by most other system and protocol anomaly because our method are unconventional there is a significant non overlap of our id with the original darpa participant which implies that they could be combined to increase coverage 
we present and evaluate a nic based network intrusion detection system intrusion detection at the nic make the system potentially tamper proof and is naturally extensible to work in a distributed setting simple anomaly detection and signature detection based model have been implemented on the nic firmware which ha it own processor and memory we empirically evaluate such system from the perspective of quality and performance bandwidth of acceptable message under varying condition of host load the preliminary result we obtain are very encouraging and lead u to believe that such nic based security scheme could very well be a crucial part of next generation network security system 
manycontrolproblemstakeplaceincontinuousstate actionspaces e g a in manipulator robotics where the control objective is oftendeflnedasflndingadesiredtrajectorythatreachesaparticular goalstate whilereinforcementlearningofiersatheoreticalframeworktolearnsuchcontrolpoliciesfromscratch itsapplicabilityto higher dimensional continuous state action space remains rather limited to date instead of learning from scratch in this paper we suggest to learn a desired complex control policy by transforming an existing simple canonical control policy for this purpose we represent canonical policy in term of difierential equation with well deflned attractor property by nonlinearly transforming the canonicalattractordynamicsusingtechniquesfromnonparametric regression almost arbitrary new nonlinear policy can be generated without losing the stability property of the canonical system we demonstrate our technique in the context of learning a set of movement skill for a humanoid robot from demonstration of a human teacher policy are acquired rapidly and due to the propertiesofwellformulateddifierentialequations canbere used and modifled on line under dynamic change of the environment thelinearparameterizationofnonparametricregressionmoreover lends itself to recognize and classify previously learned movement skill evaluation in simulation and on an actual degree offreedom humanoid robot exemplify the feasibility and robustness of our approach 
recent development in grid based and point based approximation algorithm for pomdps have greatly improved the tractability of pomdp planning these approach operate on set of belief point by individually learning a value function for each point in reality belief point exist in a highly structured metric simplex but current pomdp algorithm do not exploit this property this paper present a new metric tree algorithm which can be used in the context of pomdp planning to sort belief point spatially and then perform fast value function update over group of point we present result showing that this approach can reduce computation in point based pomdp algorithm for a wide range of problem 
most data mining algorithm require the setting of many input parameter two main danger of working with parameter laden algorithm are the following first incorrect setting may cause an algorithm to fail in finding the true pattern second a perhaps more insidious problem is that the algorithm may report spurious pattern that do not really exist or greatly overestimate the significance of the reported pattern this is especially likely when the user fails to understand the role of parameter in the data mining process data mining algorithm should have a few parameter a possible ideally none a parameter free algorithm would limit our ability to impose our prejudice expectation and presumption on the problem at hand and would let the data itself speak to u in this work we show that recent result in bioinformatics and computational theory hold great promise for a parameter free data mining paradigm the result are motivated by observation in kolmogorov complexity theory however a a practical matter they can be implemented using any off the shelf compression algorithm with the addition of just a dozen or so line of code we will show that this approach is competitive or superior to the state of the art approach in anomaly interestingness detection classification and clustering with empirical test on time series dna text video datasets 
quantum learning hold great promise for the field of machine intelligence the most studied quantum learning algorithm is the quantum neural network many such model have been proposed yet none ha become a standard in addition these model usually leave out many detail often excluding how they intend to train their network this paper discus one approach to the problem and what advantage it would have over classical network 
abstract we consider the problem of deriving class size independent generalization bound for some regularized discriminative multi category classification method in particular we obtain an expected generalization bound for a standard formulation of multi category support vector machine based on the theoretical result we argue that the formulation over penalizes misclassification error which in theory may lead to poor generalization performance a remedy based on a generalization of multi category logistic regression conditional maximum entropy is then proposed and it theoretical property are examined 
discovering coherent gene expression pattern in time series gene expression data is an important task in bioinformatics research and biomedical application in this paper we propose an interactive exploration framework for mining coherent expression pattern in time series gene expression data we develop a novel tool coherent pattern index graph to give user highly confident indication of the existence of coherent pattern to derive a coherent pattern index graph we devise an attraction tree structure to record the gene in the data set and summarize the information needed for the interactive exploration we present fast and scalable algorithm to construct attraction tree and coherent pattern index graph from gene expression data set we conduct an extensive performance study on some real data set to verify our design the experimental result strongly show that our approach is more effective than the state of the art method in mining real gene expression data and is scalable in mining large data set 
this paper analysis the contrastive divergence algorithm for learning statistical parameter we relate the algorithm to the stochastic approximation literature this enables u to specify condition under which the algorithm is guaranteed to converge to the optimal solution with probability this includes necessary and sufficient condition for the solution to be unbiased 
linear discriminant analysis lda is a well known scheme for feature extraction and dimension reduction it ha been used widely in many application involving high dimensional data such a face recognition and image retrieval an intrinsic limitation of classical lda is the so called singularity problem that is it fails when all scatter matrix are singular a well known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using principal component analysis pca before lda the algorithm called pca lda is used widely in face recognition however pca lda ha high cost in time and space due to the need for an eigen decomposition involving the scatter matrix in this paper we propose a novel lda algorithm namely dlda which stand for dimensional linear discriminant analysis dlda overcomes the singularity problem implicitly while achieving efficiency the key difference between dlda and classical lda lie in the model for data representation classical lda work with vectorized representation of data while the dlda algorithm work with data in matrix representation to further reduce the dimension by dlda the combination of dlda and classical lda namely dlda lda is studied where lda is preceded by dlda the proposed algorithm are applied on face recognition and compared with pca lda experiment show that dlda and dlda lda achieve competitive recognition accuracy while being much more efficient 
we consider the problem of eliminating redundant boolean feature for a given data set where a feature is redundant if it separate the class le well than another feature or set of feature lavra ccaron et al proposed the algorithm reduce that work by pairwise comparison of feature i e it eliminates a feature if it is redundant with respect to another feature their algorithm operates in an ilp setting and is restricted to two class problem in this paper we improve their method and extend it to multiple class central to our approach is the notion of a neighbourhood of example a set of example of the same class where the number of different feature between example is relatively small redundant feature are eliminated by applying a revised version of the reduce method to each pair of neighbourhood of different class we analyse the performance of our method on a range of data set 
this paper present vlsi circuit with continuous valued probabilistic behaviour realized by injecting noise into each computing unit neuron interconnecting the noisy neuron form a continuous restricted boltzmann machine crbm which ha shown promising performance in modelling and classifying noisy biomedical data the minimising contrastive divergence learning algorithm for crbm is also implemented in mixed mode vlsi to adapt the noisy neuron parameter on chip 
large datasets arise in various application such a market basket analysis and information retrieval we concentrate on the study of topic model aiming at result which indicate why certain method succeed or fail we describe simple algorithm for finding topic model from data we give theoretical result showing that the algorithm can discover the epsilon separable topic model of papadimitriou et al we present empirical result showing that the algorithm find natural topic in real world data set we also briefly discus the connection to matrix approach including nonnegative matrix factorization and independent component analysis 
many real world domain are relational in nature consisting of a set of entity linked to each other in complex way two important task in such data are predicting entity label and link between entity we present a flexible framework that build on conditional markov network and successfully address both task by capturing complex dependency in the data these model can compactly represent probabilistic pattern over subgraph structure and use them to predict label and link effectively we show how to train these model and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entity and link we evaluate our framework on several relational datasets including university webpage and social network our approach achieves significantly better performance than flat classification which attempt to predict each label and link in isolation 
simple lexicographic decision heuristic that consider cue one at a time in a particular order and stop searching for cue a soon a a decision can be made have been shown to be both accurate and frugal in their use of information but much of the simplicity and success of these heuristic come from using an appropriate cue order for instance the take the best heuristic us validity order for cue which requires considerable computation potentially undermining the computational advantage of the simple decision mechanism but many cue order can achieve good decision performance and study of sequential search for data record have proposed a number of simple ordering rule that may be of use in constructing appropriate decision cue order a well here we consider a range of simple cue ordering mechanism including tallying swapping and move to front rule and show that they can find cue order that lead to reasonable accuracy and considerable frugality when used with lexicographic decision heuristic 
we propose a novel method of dimensionality reduction for supervised learning given a regression or classification problem in which we wish to predict a variable y from an explanatory vector x we treat the problem of dimensionality reduction a that of finding a low dimensional effective subspace ofx which retains the statistical relationship between x and y we show that this problem can be formulated in term of conditional independence to turn this formulation into an optimization problem we characterize the notion of conditional independence using covariance operator on reproducing kernel hilbert space this allows u to derive a contrast function for estimation of the effective subspace unlike many conventional method the proposed method requires neither assumption on the marginal distribution of x nor a parametric model of the conditional distribution of y 
this paper address the problem of constructing good action selection policy for agent acting in partially observable environment a class of problem generally known a partially observable markov decision process we present a novel approach that us a modification of the well known baum welch algorithm for learning a hidden markov model hmm to predict both percept and utility in a non deterministic world this enables an agent to make decision based on it previous history of action observation and reward our algorithm called utile distinction hidden markov model udhmm handle the creation of memory well in that it tends to create perceptual and utility distinction only when needed while it can still discriminate state based on history of arbitrary length the experimental result in highly stochastic problem domain show very good performance 
we propose a method called rule based esp resp for utilizing prior knowledge evolving artificial neural network anns first kbann like technique are used to transform a set of rule into an ann then the ann is trained using the enforced subpopulation esp neuroevolution method empirical result in the prey capture domain show that resp can reach higher level of performance than esp the result also suggest that incremental learning is not necessary with resp and it is often easier to design a set of rule than an incremental evolution scheme in addition an experiment with some of the rule deleted suggests that resp is robust even with an incomplete knowledge base i esp therefore provides a robust methodology for scaling up neuroevolution to harder task by utilizing existing knowledge about the domain 
this paper present two new formulation of multiple instance learning a a maximum margin problem the proposed extension of the support vector machine svm learning approach lead to mixed integer quadratic program that can be solved heuristically our generalization of svms make a state of the art classication technique including non linear classication via kernel available to an area that up to now ha been largely dominated by special purpose method we present experimental result on a pharmaceutical data set and on application in automated image indexing and document categorization 
rule mining is an important data mining task that ha been applied to numerous real world application often a rule mining system generates a large number of rule and only a small subset of them is really useful in application although there exist some system allowing the user to query the discovered rule they are le suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rule in this paper we propose a new powerful rule query language rule ql for querying multiple rulebases that is modeled after sql and ha rigorous theoretical foundation of a rule based calculus in particular we first propose a rule based calculus rc based on the first order logic and then present the language rule ql that is at least a expressive a the safe fragment of rc we also propose a number of efficient query evaluation technique for rule ql and test them experimentally on some representative query to demonstrate the feasibility of rule ql 
during initial development kdd solution often focus heavily on algorithm architecture software hardware and system engineering challenge without first thoroughly exploring how end user will employ the new kdd technology a a result of such system centered design many useless feature are implemented that prolong development and significantly add to life cycle cost while making the system hard to operate and use this presentation will describe an alternate user centered approach borrowed from the consumer product industry that can produce kdd solution with shorter development cycle lower cost and much better usability 
from the standpoint of the automated extraction of scientific knowledge an important but little studied part of scientific publication are the figure and accompanying caption caption are dense in information but also contain many extra grammatical construct making them awkward to process with standard information extraction method we propose a scheme for understanding caption in biomedical publication by extracting and classifying image pointer reference to the accompanying image we evaluate a number of automated method for this task including hand coded method method based on existing learning technique and method based on novel learning technique the best of these method lead to a usefully accurate tool for caption understanding with both recall and precision in excess of on the most important single class in a combined extraction classification task 
we discus the problem of ranking instance with the use of a large margin principle we introduce two main approach the first is the fixed margin policy in which the margin of the closest neighboring class is being maximized which turn out to be a direct generalization of svm to ranking learning the second approach allows for different margin where the sum of margin is maximized this approach is shown to reduce to svm when the number of class both approach are optimal in size of where is the total number of training example experiment performed on visual classification and collaborative filtering show that both approach outperform existing ordinal regression algorithm applied for ranking and multi class svm applied to general multi class classification 
in complicated interacting auction a fundamental problem is the prediction of price of good in the auction and more broadly the modeling of uncertainty regarding these price in this paper we present a machine learning approach to this problem the technique is based on a new and general boosting based algorithm for conditional density estimation problem of this kind i e supervised learning problem in which the goal is to estimate the entire conditional distribution of the real valued label this algorithm which we present in detail is at the heart of a top scoring agent in the recent trading agent competition tac we describe how work the result of the competition and controlled experiment evaluating the effectiveness of price prediction in auction 
in this work we consider the task of relaxing the i i d assumption in online pattern recognition or classification aiming to make existing learning algorithm applicable to a wider range of task online pattern recognition is predicting a sequence of label based on object given for each label and on example pair of object and label learned so far traditionally this task is considered under the assumption that example are independent and identically distributed however it turn out that many result of pattern recognition theory carry over under a much weaker assumption namely under the assumption of conditional independence and identical distribution of object only while the only condition on the distribution of label is that the rate of occurrence of each label should be above some positive threshold we find a broad class of learning algorithm for which estimation of the probability of a classification error achieved under the classical i i d assumption can be generalised to the similar estimate for the case of conditionally i i d distributed example 
conditional random field crfs lafferty mccallum pereira provide a flexible and powerful model for learning to assign label to element of sequence in such application a part of speech tagging text to speech mapping protein and dna sequence analysis and information extraction from web page however existing learning algorithm are slow particularly in problem with large number of potential input feature this paper describes a new method for training crfs by applying friedman s gradient tree boosting method in tree boosting the crf potential function are represented a weighted sum of regression tree regression tree are learned by stage wise optimization similar to adaboost but with the objective of maximizing the conditional likelihood p y x of the crf model by growing regression tree interaction among feature are introduced only a needed so although the parameter space is potentially immense the search algorithm doe not explicitly consider the large space a a result gradient tree boosting scale linearly in the order of the markov model and in the order of the feature interaction rather than exponentially like previous algorithm based on iterative scaling and gradient descent 
the majority of the existing algorithm for learning decision tree are greedy a tree is induced top down making locally optimal decision at each node in most case however the constructed tree is not globally optimal furthermore the greedy algorithm require a fixed amount of time and are not able to generate a better tree if additional time is available to overcome this problem we present two lookahead based algorithm for anytime induction of decision tree thus allowing tradeoff between tree quality and learning time the first one is depth k lookahead where a larger time allocation permit larger k the second algorithm us a novel strategy for evaluating candidate split a stochastic version of id is repeatedly invoked to estimate the size of the tree in which each split result and the one that minimizes the expected size is preferred experimental result indicate that for several hard concept our proposed approach exhibit good anytime behavior and yield significantly better decision tree when more time is available 
association rule have received a lot of attention in the data mining community since their introduction the classical approach to find rule whose item enjoy high support appear in a lot of the transaction in the data set is however filled with shortcoming it ha been shown that support can be misleading a an indicator of how interesting the rule is alternative measure such a lift have been proposed more recently a paper by dumouchel et al proposed the use of all two factor loglinear model to discover set of item that cannot be explained by pairwise association between the item involved this approach however ha it limitation since it stop short of considering higher order interaction other than pairwise among the item in this paper we propose a method that examines the parameter of the fitted loglinear model to find all the significant association pattern among the item since fitting loglinear model for large data set can be computationally prohibitive we apply graph theoretical result to divide the original set of item into component set of item that are statistically independent from each other we then apply loglinear modeling to each of the component and find the interesting association among item in them the technique is experimentally evaluated with a real data set insurance data and a series of synthetic data set the result show that the technique is effective in finding interesting association among the item involved 
we describe the application of probabilistic model based learning to the problem of automatically identifying class of galaxy based on both morphological and pixel intensity characteristic the em algorithm can be used to learn how to spatially orient a set of galaxy so that they are geometrically aligned we augment this ordering model with a mixture model on object and demonstrate how class of galaxy can be learned in an unsupervised manner using a two level em algorithm the resulting model provide highly accurate classi cation of galaxy in cross validation experiment 
the sequential information bottleneck sib algorithm cluster co occurrence data such a text document v word we introduce a variant that model sparse co occurrence data by a generative process this turn the objective function of sib mutual information into a bayes factor while keeping it intact asymptotically for non sparse data experimental performance of the new algorithm is comparable to the original sib for large data set and better for smaller sparse set 
in this paper we obtain convergence bound for the concentration of bayesian posterior distribution around the true distribution using a novel method that simplifies and enhances previous result based on the analysis we also introduce a generalized family of bayesian posterior and show that the convergence behavior of these generalized posterior is completely determined by the local prior structure around the true distribution this important and surprising robustness property doe not hold for the standard bayesian posterior in that it may not concentrate when there exist bad prior structure even at place far away from the true distribution 
we consider the problem of decentralized detection under constraint on the number of bit that can be transmitted by each sensor in contrast to most previous work in which the joint distribution of sensor observation is assumed to be known we address the problem when only a set of empirical sample is available we propose a novel algorithm using the framework of empirical risk minimization and marginalized kernel and analyze it computational and statistical property both theoretically and empirically we provide an efficient implementation of the algorithm and demonstrate it performance on both simulated and real data set 
an important issue in reinforcement learning is how to incorporate expert knowledge in a principled manner especially a we scale up to real world task in this paper we present a method for incorporating arbitrary advice into the reward structure of a reinforcement learning agent without altering the optimal policy this method extends the potentialbased shaping method proposed by ng et al to the case of shaping function based on both state and action this allows for much more specic information to guide the agent which action to choose without requiring the agent to discover this from the reward on state alone we develop two qualitatively dieren t method for converting a potential function into advice for the agent we also provide theoretical and experimental justications for choosing between these advice giving algorithm based on the property of the potential function 
when mining temporal sequence knowledge discovery technique can be applied that discover interesting pattern of interaction existing approach use frequency and sometimes length a measurement for interestingness because these are temporal sequence additional characteristic such a periodicity may also be interesting we propose that information theoretic principle can be used to evaluate interesting characteristic of time ordered input sequence in this paper we present a novel data mining technique based on the minimum description length principle that discovers interesting feature in a time ordered sequence we discus feature of our real time mining approach show application of the knowledge mined by the approach and present a technique to bootstrap a decision maker from the mined pattern 
the direct neural control of external device such a computer display or prosthetic limb requires the accurate decoding of neural activity rep resenting continuous movement we develop a real time control system using the spiking activity of approximately neuron recorded with an electrode array implanted in the arm area of primary motor cortex in contrast to previous work we develop a control theoretic approach that explicitly model the motion of the hand and the probabilistic re lationship between this motion and the mean ring rate of the cell in m bin we focus on a realistic cursor control task in which the sub ject must move a cursor to hit randomly placed target on a computer monitor encoding and decoding of the neural data is achieved with a kalman lter which ha a number of advantage over previous linear ltering technique in particular the kalman lter reconstruction of hand trajectory in off line experiment are more accurate than previ ously reported result and the model provides insight into the nature of the neural coding of movement 
caching the result of frequent query pattern can improve the performance of query evaluation this paper describes a pas mining algorithm called pxminer to discover frequent xml query pattern we design data structure to expedite the mining process experiment result indicate that pxminer is both efficient and scalable 
covariance and correlation estimate have important application in data mining in the presence of outlier classical estimate of covariance and correlation matrix are not reliable a small fraction of outlier in some case even a single outlier can distort the classical covariance and correlation estimate making them virtually useless that is correlation for the vast majority of the data can be very erroneously reported principal component transformation can be misleading and multidimensional outlier detection via mahalanobis distance can fail to detect outlier there is plenty of statistical literature on robust covariance and correlation matrix estimate with an emphasis on affine equivariant estimator that posse high breakdown point and small worst case bias all such estimator have unacceptable exponential complexity in the number of variable and quadratic complexity in the number of observation in this paper we focus on several variant of robust covariance and correlation matrix estimate with quadratic complexity in the number of variable and linear complexity in the number of observation these estimator are based on several form of pairwise robust covariance and correlation estimate the estimator studied include two fast estimator based on coordinate wise robust transformation embedded in an overall procedure recently proposed by we show that the estimator have attractive robustness property and give an example that us one of the estimator in the new insightful miner data mining product 
we propose a new clustering algorithm called symp which is based on synchronization of pulse coupled oscillator symp represents each data point by an integrate and fire oscillator and us the relative similarity between the point to model the interaction between the oscillator symp is robust to noise and outlier determines the number of cluster in an unsupervised manner identifies cluster of arbitrary shape and can handle very large data set the robustness of symp is an intrinsic property of the synchronization mechanism to determine the optimum number of cluster symp us a dynamic resolution parameter to identify cluster of various shape symp model each cluster by multiple gaussian component the number of component is automatically determined using a dynamic intra cluster resolution parameter cluster with simple shape would be modeled by few component while cluster with more complex shape would require a larger number of component the scalable version of symp us an efficient incremental approach that requires a simple pas through the data set the proposed clustering approach is empirically evaluated with several synthetic and real data set and it performance is compared with cure 
there ha been a surge of interest in learning using a mix of labeled and unlabeled data general approach include semi supervised learning and tranductive inference in this paper we look at some of the unique way in which unlabeled data can improve performance when doing link based classification the classification of object making use of both object description and the link between object 
this paper present an energy normalization transform a a method to reduce system error in the lf asd brain computer interface the energy normalization transform ha two major benefit to the system performance first it can increase class separation between the active and idle eeg data second it can desensitize the system to the signal amplitude variability for four subject in the study the benefit resulted in the performance improvement of the lf asd in the range from to while for the fifth subject who had the highest non normalized accuracy of the performance did not change notably with normalization 
classifying m example using a support vector machine containing l support vector traditionally requires exactly m l kernel computation we introduce a computational geometry method for which classification cost becomes roughly proportional to the difficulty of each example e g distance from the discriminant hyperplane it produce exactly the same classification while typically requiring much e g time fewer kernel computation than ma l related educed set method e g burges scholkopf et al scholkopf et al similarly lower the effective l but provide neither proportionality with difficulty nor guaranteed preservation of classification 
we present an algorithm based on convex optimization for constructing kernel for semi supervised learning the kernel matrix are derived from the spectral decomposition of graph laplacians and combine labeled and unlabeled data in a systematic fashion unlike previous work using diffusion kernel and gaussian random field kernel a nonparametric kernel approach is presented that incorporates order constraint during optimization this result in flexible kernel and av oids the need to choose among different parametric form our approach relies on a quadratically constrained quadratic program qcqp and is computationally feasible for large datasets we evaluate the ker nels on real datasets using support vector machine with encouraging result 
markov chain monte carlo mcmc technique revolutionized statistical practice in the s by providing an essential toolkit for making the rigor and flexibility of bayesian analysis computationally practical at the same time the increasing prevalence of massive datasets and the expansion of the field of data mining ha created the need to produce statistically sound method that scale to these large problem except for the most trivial example current mcmc method require a complete scan of the dataset for each iteration eliminating their candidacy a feasible data mining technique in this article we present a method for making bayesian analysis of massive datasets computationally feasible the algorithm simulates from a posterior distribution that condition on a smaller more manageable portion of the dataset the remainder of the dataset may be incorporated by reweighting the initial draw using importance sampling computation of the importance weight requires a single scan of the remaining observation while importance sampling increase efficiency in data access it come at the expense of estimation efficiency a simple modification based on the rejuvenation step used in particle filter for dynamic system model sidestep the loss of efficiency with only a slight increase in the number of data access to show proof of concept we demonstrate the method on a mixture of transition model that ha been used to model web traffic and robotics for this example we show that estimation efficiency is not affected while offering a reduction in data access 
in this paper we describe the development of a fielded application for detecting malicious executables in the wild we gathered benign and malicious executables and encoded each a a training example using n gram of byte code a feature such processing resulted in more than million distinct n gram after selecting the most relevant n gram for prediction we evaluated a variety of inductive method including naive bayes decision tree support vector machine and boosting ultimately boosted decision tree outperformed other method with an area under the roc curve of result also suggest that our methodology will scale to larger collection of executables to the best of our knowledge ours is the only fielded application for this task developed using technique from machine learning and data mining 
the probability estimate of a naive bayes classifier are inaccurate if some of it underlying independence assumption are violated the decision criterion for using these estimate for classification therefore ha to be learned from the data this paper proposes the use of roc curve for this purpose for two class the algorithm is a simple adaptation of the algorithm for tracing a roc curve by sorting the instance according to their predicted probability of being positive a there is no obvious way to upgrade this algorithm to the multi class case we propose a hillclimbing approach which adjusts the weight for each class in a pre defined order experiment on a wide range of datasets show the proposed method lead to significant improvement over the naive bayes classifier s accuracy finally we discus an method to find the global optimum and show how it computational complexity would make it untractable 
model for the process by which idea and influence propagate through a social network have been studied in a number of domain including the diffusion of medical and technological innovation the sudden and widespread adoption of various strategy in game theoretic setting and the effect of word of mouth in the promotion of new product recently motivated by the design of viral marketing strategy domingo and richardson posed a fundamental algorithmic problem for such social network process if we can try to convince a subset of individual to adopt a new product or innovation and the goal is to trigger a large cascade of further adoption which set of individual should we target we consider this problem in several of the most widely studied model in social network analysis the optimization problem of selecting the most influential node is np hard here and we provide the first provable approximation guarantee for efficient algorithm using an analysis framework based on submodular function we show that a natural greedy strategy obtains a solution that is provably within of optimal for several class of model our framework suggests a general approach for reasoning about the performance guarantee of algorithm for these type of influence problem in social network we also provide computational experiment on large collaboration network showing that in addition to their provable guarantee our approximation algorithm significantly out perform node selection heuristic based on the well studied notion of degree centrality and distance centrality from the field of social network 
we study a new model free form of approximate policy iteration which us sarsa update with linear state action value function approximation for policy evaluation and a policy improvement operator to generate a new policy based on the learned state action value we prove that if the policy improvement operator produce soft policy and is lipschitz continuous in the action value with a constant that is not too large then the approximate policy iteration algorithm converges to a unique solution from any initial policy to our knowledge this is the first convergence result for any form of approximate policy iteration under similar computational resource assumption 
a product price become increasingly available on the world wide web consumer attempt to understand how corporation vary these price over time however corporation change price based on proprietary algorithm and hidden variable e g the number of unsold seat on a flight is it possible to develop data mining technique that will enable consumer to predict price change under these condition this paper report on a pilot study in the domain of airline ticket price where we recorded over price observation over a day period when trained on this data hamlet our multi strategy data mining algorithm generated a predictive model that saved simulated passenger by advising them when to buy and when to postpone ticket purchase remarkably a clairvoyant algorithm with complete knowledge of future price could save at most in our simulation thus hamlet s saving were of optimal the algorithm s saving of represents an average saving of for the passenger for whom saving are possible overall hamlet saved of the ticket price averaged over the entire set of simulated passenger our pilot study suggests that mining of price data available over the web ha the potential to save consumer substantial sum of money per annum 
super resolution aim to produce a high resolution image from a set of one or more low resolution image by recovering or inventing plausible high frequency image content typical approach try to reconstruct a high resolution image using the sub pixel displacement of several lowresolution image usually regularized by a generic smoothness prior over the high resolution image space other method use training data to learn low to high resolution match and have been highly successful even in the single input image case here we present a domain specific image prior in the form of a p d f based upon sampled image and show that for certain type of super resolution problem this sample based prior give a significant improvement over other common multiple image super resolution technique 
we show how to build hierarchical reduced rank representation for large stochastic matrix and use this representation to design an efficient algorithm for computing the largest eigenvalue and the corresponding eigenvectors in particular the eigen problem is first solved at the coarsest level of the representation the approximate eigen solution is then interpolated over successive level of the hierarchy a small number of power iteration are employed at each stage to correct the eigen solution the typical speedup obtained by a matlab implementation of our fast eigensolver over a standard sparse matrix eigensolver are at least a factor of ten for large image size the hierarchical representation ha proven to be effective in a min cut based segmentation algorithm that we proposed recently 
we describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic meg measurement in real time at it core is a multilayer perceptron mlp trained to map sensor signal and head position to dipole location including head position overcomes the previous need to retrain the mlp for each subject and session the training dataset wa generated by mapping randomly chosen dipole and head position through an analytic model and adding noise from real meg recording after training a localization took m with an average error of cm a few iteration of a levenberg marquardt routine using the mlp s output a it initial guess took m and improved the accuracy to cm only slightly above the statistical limit on accuracy imposed by the noise we applied these method to localize single dipole source from meg component isolated by blind source separation and compared the estimated location to those generated by standard manually assisted commercial software 
schema learning is a way to discover probabilistic constructivist predictive action model schema from experience it includes method for finding and using hidden state to make prediction mor e accurate we extend the original schema mechanism to handle arbitrary discrete valued sensor improve the original learning cr iteria to handle pomdp domain and better maintain hidden state by using schema prediction these extension show large improvement over the original schema mechanism in several rewardless pomdps and achievevery low prediction error in a difficult speech modeling task furthe r we compare the extended schema learner to the recently introduced predictive state representation and find their prediction of next ste p action effect to be approximately equal in accuracy this work lay the foundation for a schema based system of integrated learning and planning 
recently proposed algorithm for nonlinear dimensionality reduction fall broadly into two category which have different advantage and disadvantage global isomap and local locally linear embedding laplacian eigenmaps we present two variant of isomap which combine the advantage of the global approach with what have previously been exclusive advantage of local method computational sparsity and the ability to invert conformal map 
the proliferation of objectionable information on the internet ha reached a level of serious concern to empower end user with the choice of blocking undesirable and offensive website we propose a multimodal information filter named morf in this paper we present morf s core component it confidence based classifier a cross bagging ensemble scheme and multimodal classification algorithm empirical study and initial statistic collected from the morf filter deployed at site in the u s and asia show that morf is both efficient and effective due to our classification method 
world steel trade becomes more competitive every day and new high international quality standard and productivity level can only be achieved by applying the latest computational technology data driven analysis of complex process is necessary in many industrial application where analytical modeling is not possible this paper present the deployment of kdd technology in one real industrial problem the development of new tinplate quality diagnostic model the electrodeposition of tin on steel strip is the most critical stage of a complex process that involves a great amount of variable and operating condition it optimization is not only a great commercial and economic challenge but also a compulsion due to the social impact of the tinplate product more than of the production is used for food packaging the necessary certification with standard like iso requires the use of diagnostic model to minimize the cost and the environmental impact this aim ha been achieved following the multi stage dm methodology crisp dm and a novel application of pro active maintenance method a fmea for the identification of the specific process anomaly three dm tool have been used for the development of the model the final result include two ann tinplate quality diagnostic model that provide the estimated quality of the final product just second after it production and only based on the process data the result have much better performance than the classical faraday s model widely used for the estimation 
abstract dimensionality reduction of empirical co occurrence data is a fundamental problem in unsuper vised learning it is also a well studied problem in statistic known a the analysis of cross classified data one principled approach to this problem is to represent the data in low dimension with min imal loss of mutual information contained in the original data in this paper we introduce an information theoretic nonlinear method for finding such a most informative dimension reduction in contrast with previously introduced clustering based approach here we extract continuous fea ture function directly from the co occurrence matrix in a sense we automatically extract function of the variable that serve a approximate sufficient statistic for a sample of one variable about the other one our method is different from dimensionality reduction method which are based on a specific sometimes arbitrary metric or embedding another interpretation of our method is a generalized multi dimensional non linear regression where rather than fitting one regression function through two dimensional data we extract d regression function whose expectation val ues capture the information among the variable it thus present a new learning paradigm that unifies aspect from both supervised and unsupervised learning the resulting dimension reduction can be described by two conjugate d dimensional differential manifold that are coupled through maximum entropy i projection the riemannian metric of these manifold are determined by the observed expectation value of our extracted feature following this geometric interpretation we present an iterative information projection algorithm for finding such feature and prove it convergence our algorithm is similar to the method of association analysis in statistic though the feature extraction context a well a the information theoretic and geometric interpretation are new the algorithm is illustrated by various synthetic co occurrence data it is then demonstrated for text categorization and information retrieval and prof effective in selecting a small set of feature often improving performance over the original feature set 
the minimax probability machine classification mpmc framework lanckriet et al build classifier by minimizing the maximum probability of misclassification and give direct estimate of the probabilistic accuracy bound the only assumption that mpmc make is that good estimate of mean and covariance matrix of the class exist however a with support vector machine mpmc is computationally expensive and requires extensive cross validation experiment to choose kernel and kernel parameter that give good performance in this paper we address the computational cost of mpmc by proposing an algorithm that construct nonlinear sparse mpmc smpmc model by incrementally adding basis function i e kernel one at a time greedily selecting the next one that maximizes the accuracy bound smpmc automatically chooses both kernel parameter and feature weight without using computationally expensive cross validation therefore the smpmc algorithm simultaneously address the problem of kernel selection and feature selection i e feature weighting based solely on maximizing the accuracy bound experimental result indicate that we can obtain reliable bound a well a test set accuracy that are comparable to state of the art classification algorithm 
layout analysis is the process of extracting a hierarchical structure describing the layout of a page in the system wisdom the layout analysis is performed in two step firstly the global analysis determines possible area containing paragraph section column figure and table and secondly the local analysis group together block that possibly fall within the same area the result of the local analysis process strongly depends on the quality of the result of the first step we investigate the possibility of supporting the user during the correction of the result of the global analysis this is done by automatically generating training example of action selection from the sequence of user action and then by learning action selection rule for layout correction rule are expressed a a logic program whose induction demand the careful application of ilp technique experimental result on a set of multi page document shed evidence on the difficulty of the learning task tackled and pose new problem in learning control rule for adaptive interface 
the bayesian paradigm provides a natural and effective mean of exploiting prior knowledge concerning the time frequency structure of sound signal such a speech and music something which ha often been overlooked in traditional audio signal processing approach here after constructing a bayesian model and prior distribution capable of taking into account the time frequency characteristic of typical audio waveform we apply markov chain monte carlo method in order to sample from the resultant posterior distribution of interest we present speech enhancement result which compare favourably in objective term with standard time varying filtering technique and in several case yield superior performance both objectively and subjectively moreover in contrast to such method our result are obtained without an assumption of prior knowledge of the noise power 
we introduce a framework which we call divide by db for extending support vector machine svm to multi class problem db offer an alternative to the standard one against one and one against rest algorithm for an n class problem db produce an n node binary decision tree where node represent decision boundary formed by n svm binary classifier this tree structure allows u to present a generalization and a time complexity analysis of db our analysis and related experiment show that db is faster than one against one and one against rest algorithm in term of testing time significantly faster than one against rest in term of training time and that the cross validation accuracy of db is comparable to these two method 
we have constructed a second generation cpg chip capable of generating the necessary timing to control the leg of a walking machine we demonstrate improvement over a previous chip by moving toward a significantly more versatile device this includes a larger number of silicon neuron more sophisticated neuron including voltage dependent charging and relative and absolute refractory period and enhanced programmability of neural network this chip build on the basic result achieved on a previous chip and expands it versatility to get closer to a self contained locomotion controller for walking robot 
a key challenge facing it organization today is their evolution towards adopting e business practice that give rise to the need for reengineering their underlying software system any reengineering effort ha to be aware of the functional requirement of the subject system in order not to violate the integrity of it intended us however a software system get regularly maintained throughout their lifecycle the documentation of their requirement often become obsolete or get lost to address this problem of software requirement loss we have developed an interaction pattern mining method for the recovery of functional requirement a usage scenario our method analyzes trace of the run time system user interaction to discover frequently recurring pattern these pattern correspond to the functionality currently exercised by the system user represented a usage scenario the discovered scenario provide the basis for reengineering the software system into web accessible component each one supporting one of the discovered scenario in this paper we describe ipm our interaction pattern discovery algorithm we illustrate it with a case study from a real application and we give an overview of the reengineering process in the context of which it is employed 
in this paper we present a hybrid system combining technique from symbolic planning and reinforcement learning planning is used to automatically construct task hierarchy for hierarchical reinforcement learning based on abstract model of the behaviour purpose and to perform intelligent termination improvement when an executing behaviour is no longer appropriate reinforcement learning is used to produce concrete implementation of abstractly defined behaviour and to learn the best possible choice of behaviour when plan are ambiguous two new hierarchical reinforcement learning algorithm are presented planned hierarchical semi markov q learning p hsmq a variant of the hsmq algorithm dietterich b which us plan built task hierarchy and teleoreactive q learning trq a more complex algorithm which implement hierarchical reinforcement learning with teleo reactive execution semantics nilsson each algorithm is demonstrated in a simple grid world domain localised goal policy are learnt in term of these abstract behaviour rather than directly in term of primitive action simply adding behaviour blindly doe not solve the problem an agent with a diverse repertoire of behaviour with overlapping applicability space may have just a much trouble learning a policy a an agent learning a primitive policy directly most hierarchical reinforcement learning algorithm are model free they require no prior model of their behaviour effect nor do they build one and the agent must explore them to learn their effect yet behaviour are designed with a purpose this purpose serf a an abstract model which tell u that from all the applicable behaviour in a particular state some are appropriate and some are not to save the learning agent from blindly exploring inappropriate behaviour we need to implement some of this knowledge most existing algorithm achieve this by including some kind of task hierarchy which structure the agent s decision process limiting the set of choice it can make to those that might be productive essentially this is a function which map the agent s state to a set of appropriate behaviour at present this function is implemented by hand by the trainer a more ambitious problem are tackled this is likely to become an increasingly difficult task in this paper we provide a mean to specify abstract symbolic model of an agent s behaviour and goal behaviour are represented by planning operator symbolic description are used to allow the trainer to specify behaviour in a high level language these operator are then used to automatically construct taskhierarchies through planning this hybrid of planning and learning allows u to have the best of both world using background knowledge to automatically structure our policy through planning and reinforcement learning to produce concrete policy for behaviour and optimise choice in the plan 
accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue section in this paper we present the first automated system for performing this decomposition we compare the performance of our system with ground truth data and report favorable result 
kernel k mean and spectral clustering have both been used to identify cluster that are non linearly separable in input space despite significant research these method have remained only loosely related in this paper we give an explicit theoretical connection between them we show the generality of the weighted kernel k mean objective function and derive the spectral clustering objective of normalized cut a a special case given a positive definite similarity matrix our result lead to a novel weighted kernel k mean algorithm that monotonically decrease the normalized cut this ha important implication a eigenvector based algorithm which can be computationally prohibitive are not essential for minimizing normalized cut b various technique such a local search and acceleration scheme may be used to improve the quality a well a speed of kernel k mean finally we present result on several interesting data set including diametrical clustering of large gene expression matrix and a handwriting recognition data set 
in this paper we revisit the problem of inducing a process model from time series data we illustrate this task with a realistic ecosystem model review an initial method for it induction then identify three challenge that require extension of this method these include dealing with unobservable variable finding numeric condition on process and preventing the creation of model that overfit the training data we describe response to these challenge and present experimental evidence that they have the desired effect after this we show that this extended approach to inductive process modeling can explain and predict time series data from battery on the international space station in closing we discus related work and consider direction for future research 
we propose a method that allows for a rigorous statistical analysis of neural response to natural stimulus which are non gaussian and exhibit strong correlation we have in mind a model in which neuron are selective for a small number of stimulus dimension out of the high dimensional stimulus space but within this subspace the response can be arbitrarily nonlinear therefore we maximize the mutual information between the sequence of elicited neural response and an ensemble of stimulus that ha been projected on trial direction in the stimulus space the procedure can be done iteratively by increasing the number of direction with respect to which information is maximized those direction that allow the recovery of all of the information between spike and the full unprojected stimulus describe the relevant subspace if the dimensionality of the relevant subspace indeed is much smaller than that of the overall stimulus space it may become experimentally feasible to map out the neuron s input output function even under fully natural stimulus condition this contrast with method based on correlation function reverse correlation spike triggered covariance which all require simplified stimulus statistic if we are to use them rigorously 
the representation of acoustic signal at the cochlear nerve must serve a wide range of auditory task that require exquisite sensitivity in both time and frequency lewicki demonstrated that many of the filtering property of the cochlea could be explained in term of efficient coding of natural sound this model however did not account for property such a phase locking or how sound could be encoded in term of action potential here we extend this theoretical approach with algorithm for learning efficient auditory code using a spiking population code here we propose an algorithm for learning efficient auditory code using a theoretical model for coding sound in term of spike in this model each spike encodes the precise time position and magnitude of a localized time varying kernel function by adapting the kernel function to the statistic natural sound we show that compared to conventional signal representation the spike code achieves far greater coding efficiency furthermore the inferred kernel show both striking similarity to measured cochlear filter and a similar bandwidth versus frequency dependence 
machine learning method are often applied to the problem of learning a map from a robot s sensor data but they are rarely applied to the problem of learning a robot s motion model the motion model which can be influenced by robot idiosyncrasy and terrain property is a crucial aspect of current algorithm for simultaneous localization and mapping slam in this paper we concentrate on generating the correct motion model for a robot by applying em method in conjunction with a current slam algorithm in contrast to previous calibration approach we not only estimate the mean of the motion but also the interdependency between motion term and the variance in these term this can be used to provide a more focused proposal distribution to a particle filter used in a slam algorithm which can reduce the resource needed for localization while decreasing the chance of losing track of the robot s position we validate this approach by recovering a good motion model despite initialization with a poor one further experiment validate the generality of the learned model in similar circumstance 
language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next that make it possible for language to adapt to the particularity of the learner in this paper i show that this type of language change ha important consequence for model of the evolution and acquisition of syntax 
the so called expert algorithm constitute a methodology for choosing action repeatedly when the reward depend both on the choice of action and on the unknown current state of the environment an expert algorithm ha access to a set of strategy expert each of which may recommend which action to choose the algorithm learns how to combine the recommendation of individual expert so that in the long run for any fixed sequence of state of the environment it doe a well a the best expert would have done relative to the same sequence this methodology may not be suitable for situation where the evolution of state of the environment depends on past chosen action a is usually the case for example in a repeated non zero sum game a new expert algorithm is presented and analyzed in the context of repeated game it is shown that asymptotically under certain condition it performs a well a the best available expert this algorithm is quite different from previously proposed expert algorithm it represents a shift from the paradigm of regret minimization and myopic optimization to consideration of the long term effect of a player s action on the opponent s action or the environment the importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated prisoner s dilemma game whereas previous expert algorithm converge to the suboptimal non cooperative play 
periodicy detection in time series data is a challenging problem of great importance in many application most previous work focused on mining synchronous periodic pattern and did not recognize the misaligned presence of a pattern due to the intervention of random noise in this paper we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrence may be shifted due to disturbance two parameter min rep and max dis are employed to specify the minimum number of repetition that is required within each segment of nondisrupted pattern occurrence and the maximum allowed disturbance between any two successive valid segment upon satisfying these two requirement the longest valid subsequence of a pattern is returned a two phase algorithm is devised to first generate potential period by distance based pruning followed by an iterative procedure to derive and validate candidate pattern and locate the longest valid subsequence we also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency 
we present a novel method for learning with gaussian process regression in a hierarchical bayesian framework in a first step kernel matrix on a fixed set of input point are learned from data using a simple and efficient em algorithm this step is nonparametric in that it doe not require a parametric form of covariance function in a second step kernel function are fitted to approximate the learned covariance matrix using a generalized nystr om method which result in a complex data driven kernel we evaluate our approach a a recommendation engine for art image where the proposed hierarchical bayesian method lead to excellent prediction performance 
predictive state representation psrs are a recently proposed way of modeling controlled dynamical system psr based model use prediction of observable outcome of test that could be done on the system a their state representation and have model parameter that define how the predictive state representation change over time a action are taken and observation noted learning psr based model requires solving two subproblems discovery of the test whose prediction constitute state and learning the model parameter that define the dynamic so far there have been no result available on the discovery subproblem while for the learning subproblem an approximate gradient algorithm ha been proposed singh et al with mixed result it work on some domain and not on others in this paper we provide the first discovery algorithm and a new learning algorithm for linear psrs for the special class of controlled dynamical system that have a reset operation we provide experimental verification of our algorithm finally we also distinguish our work from prior work by jaeger on observable operator model ooms 
abstract we consider bayesian mixture approach where a predictor is constructedbyformingaweightedaverageofhypothesesfromsome space of function while such procedure are known to lead to optimalpredictorsinseveralcases wheresu cientlyaccurateprior information is available it ha not been clear how they perform whensomeofthepriorassumptionsareviolated inthispaperwe establish data dependent bound for such procedure extending previous randomized approach such a the gibbs algorithm to a fully bayesian setting the flnite sample guarantee established inthisworkenabletheutilizationofbayesianmixtureapproaches in agnostic setting where the usual assumption of the bayesian paradigmfailtohold moreover theboundsderivedcanbedirectly applied to non bayesian mixture approach such a bagging and boosting 
pattern discovery ha emerged a a direct result of increased data storage and analytic capability available to the data analyst without a massive amount of data we do not have the evidence to support the discovery of the local deterministic structure that we call pattern a such pattern discovery is one of the few area of data mining that cannot be considered simply a a scaling up of current statistical methodology to analyze large data set however the philosophy of hypothesis testing and modeling in traditional statistic do lend themselves to forming a framework for pattern discovery and we can also draw from idea relating to outlier discovery and residual analysis to discover pattern we illustrate an iterative strategy in a statistical framework by way of it application to one simulated and two real data set 
we consider learning to classify cognitive state of human subject based on their brain activity observed via functional magnetic resonance imaging fmri this problem is important because such classifier constitute virtual sensor of hidden cognitive state which may be useful in cognitive science research and clinical application in recent work mitchell et al have demonstrated the feasibility of training such classifier for individual human subject e g to distinguish whether the subject is reading an ambiguous or unambiguous sentence or whether they are reading a noun or a verb here we extend that line of research exploring how to train classifier that can be applied across multiple human subject including subject who were not involved in training the classifier we describe the design of several machine learning approach to training multiple subject classifier and report experimental result demonstrating the success of these method in learning cross subject classifier for two different fmri data set 
the bootstrap ha become a popular method for exploring model structure uncertainty our experiment with articial and realworld data demonstrate that the graph learned from bootstrap sample can be severely biased towards too complex graphical model accounting for this bias is hence essential e g when exploring model uncertainty we nd that this bias is intimately tied to well known spurious dependence induced by the bootstrap the leading order bias correction equal one half of akaike s penalty for model complexity we demonstrate the eect of this simple bias correction in our experiment we also relate this bias to the bias of the plug in estimator for entropy a well a to the dierence between the expected test and training error of a graphical model which asymptotically equal akaike s penalty rather than one half 
spectral clustering us eigenvectors of the laplacian of the similarity matrix they are most conveniently applied to way clustering problem when applying to multi way clustering either the way spectral clustering is recursively applied or an embedding to spectral space is done and some other method are used to cluster the point here we propose and study a k way cluster assignment method the method transforms the problem to find valley and peak of a d quantity called cluster crossing which measure the symmetric cluster overlap across a cut point along a linear ordering of the data point the method can either determine k cluster in one shot or recursively split a current cluster into several smaller one we show that a linear ordering based on a distance sensitive objective ha a continuous solution which is the eigenvector of the laplacian showing the close relationship between clustering and ordering the method relies on the connectivity matrix constructed a the truncated spectral expansion of the similarity matrix useful for revealing cluster structure the method is applied to newsgroups to illustrate introduced concept experiment show it outperforms the recursive way clustering and the standard k mean clustering 
various constrained frequent pattern mining problem formulation and associated algorithm have been developed that enable the user to specify various itemset based constraint that better capture the underlying application requirement and characteristic in this paper we introduce a new class of block constraint that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern s item and it associated set of transaction block constraint provide a natural framework by which a number of important problem can be specified and make it possible to solve numerous problem on binary and real valued datasets however developing computationally efficient algorithm to find these block constraint pose a number of challenge a unlike the different itemset based constraint studied earlier these block constraint are tough a they are neither anti monotone monotone nor convertible to overcome this problem we introduce a new class of pruning method that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called cbminer to find the closed itemsets that satisfy the block constraint 
we address the issue of discovering significant binary relationship in transaction datasets in a weighted setting traditional model of association rule mining is adapted to handle weighted association rule mining problem where each item is allowed to have a weight the goal is to steer the mining focus to those significant relationship involving item with significant weight rather than being flooded in the combinatornal explosion of insignificant relationship we identify the challenge of using weight in the iterative process of generating large itemsets the problem of invalidation of the downward closure property in the weighted setting is solved by using an improved model of weighted support measurement and exploiting a weighted downward closure property a new algorithm called warm weighted association rule mining is developed based on the improved model the algorithm is both scalable and efficient in discovering significant relationship in weighted setting a illustrated by experiment performed on simulated datasets 
there ha been substantial progress in the past decade in the development of object classifier for image for example of face human s and vehicle here we address the problem of contamination e g occlusion shadow in test image which have not explicitly been encountered in training data the variational ising classifier vic algor ithm model contamination a a mask a field of binary variable with a st rong spatial coherence prior variational inference is used to marg inalize over contamination and obtain robust classification in this way the vic approach can turn a kernel classifier for clean data into one tha t can tolerate contamination without any specific training on contaminat ed positive 
web personalization is the process of customizing a web site to the need of each specific user or set of user taking advantage of the knowledge acquired through the analysis of the user s navigational behavior integrating usage data with content structure or user profile data enhances the result of the personalization process in this paper we present sewep a system that make use of both the usage log and the semantics of a web site s content in order to personalize it web content is semantically annotated using a conceptual hierarchy taxonomy we introduce c log an extended form of web usage log that encapsulates knowledge derived from the link semantics c log are used a input to the web usage mining process resulting in a broader yet semantically focused set of recommendation 
in kernel method an interesting recent development seek to learn a good kernel from empirical data automatically in this paper by regarding the transductive learning of the kernel matrix a a missing data problem we propose a bayesian hierarchical model for the problem and devise the tanner wong data augmentation algorithm for making inference on the model the tanner wong algorithm is closely related to gibbs sampling and it also bear a strong resemblance to the expectation maximization em algorithm for an efficient implementation we propose a simplified bayesian hierarchical model and the corresponding tanner wong algorithm we express the relationship between the kernel on the input space and the kernel on the output space a a symmetric definite generalized eigenproblem based on this eigenproblem an efficient approach to choosing the base kernel matrix is presented the effectiveness of our bayesian model with the tanner wong algorithm is demonstrated through some classification experiment showing promising result 
abstract classification with partially labeled data requires using a large number of unlabeled example or an estimated marginal p x to further constrain the conditional p y x beyond a few available labeled example we formulate a regularization approach to linking the marginal and the conditional in a general way the regularization penalty measure the information that is implied about the label over covering region no parametric assumption are required and the approach remains tractable even for continuous marginal density p x we develop algorithm for solving the regularization problem for finite cover establish a limiting differential equation and exemplify the behavior of the new regularization approach in simple case 
in order to effectively use machine learning algorithm e g neural network for the analysis of survival data the correct treatment of censored data is crucial the concordance index ci is a typical metric for quantifying the predictive ability of a survival model we propose a new algorithm that directly us the ci a the objective function to train a model which predicts whether an event will eventually occur or not directly optimizing the ci allows the model to make complete use of the information from both censored and non censored observation in particular we approximate the ci via a differentiable function so that gradient based method can be used to train the model we applied the new algorithm to predict the eventual recurrence of prostate cancer following radical prostatectomy compared with the traditional cox proportional hazard model and several other algorithm based on neural network and support vector machine our algorithm achieves a significant improvement in being able to identify high risk and low risk group of patient 
we consider the bias and variance of value function estimation that are caused by using an empirical model instead of the true model we analyze these bias and variance for markov process from a classical frequentist statistical point of view and in a bayesian setting using a second order approximation we provide explicit expression for the bias and variance in term of the transition count and the reward statistic we present supporting experiment with artificial markov chain and with a large transactional database provided by a mail order catalog firm 
we propose to study link between three important classification algorithm perceptrons multi layer perceptrons mlps and support vector machine svms we first study way to control the capacity of perceptrons mainly regularization parameter and early stopping using the margin idea introduced with svms after showing that under simple condition a perceptron is equivalent to an svm we show it can be computationally expensive in time to train an svm and thus a perceptron with stochastic gradient descent mainly because of the margin maximization term in the cost function we then show that if we remove this margin maximization term the learning rate or the use of early stopping can still control the margin these idea are extended afterward to the case of mlps moreover under some assumption it also appears that mlps are a kind of mixture of svms maximizing the margin in the hidden layer space finally we present a very simple mlp based on the previous finding which yield better performance in generalization and speed than the other model 
automatically segmenting unstructured text string into structured record is necessary for importing the information contained in legacy source and text collection into a data warehouse for subsequent querying analysis mining and integration in this paper we mine table present in data warehouse and relational database to develop an automatic segmentation system thus we overcome limitation of existing supervised text segmentation approach which require comprehensive manually labeled training data our segmentation system is robust accurate and efficient and requires no additional manual effort thorough evaluation on real datasets demonstrates the robustness and accuracy of our system with segmentation accuracy exceeding state of the art supervised approach 
we propose an information theoretic clustering approach that incorporates a pre known partition of the data aiming to identify common cluster that cut across the given partition in the standard clustering setting the formation of cluster is guided by a single source of feature information the newly utilized pre partition factor introduces an additional bias that counterbalance the impact of the feature whenever they become correlated with this known partition the resulting algorithmic framework wa applied successfully to synthetic data a well a to identifying text based cross religion correspondence 
automatic document classification dc is essential for the management of information and knowledge this paper explores two practical issue in dc each document ha it context of discussion and both the content and vocabulary of the document database is intrinsically evolving the issue call for adaptive document classification adc that adapts a dc system to the evolving contextual requirement of each document category so that input document may be classified based on their context of discussion we present an incremental context mining technique to tackle the challenge of adc theoretical analysis and empirical result show that given a text hierarchy the mining technique is efficient in incrementally maintaining the evolving contextual requirement of each category based on the contextual requirement mined by the system higher precision dc may be achieved with better efficiency 
many real life sequence database grow incrementally it is undesirable to mine sequential pattern from scratch each time when a small set of sequence grow or when some new sequence are added into the database incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database update however it is nontrivial to mine sequential pattern incrementally especially when the existing sequence grow incrementally because such growth may lead to the generation of many new pattern due to the interaction of the growing subsequence with the original one in this study we develop an efficient algorithm incspan for incremental mining of sequential pattern by exploring some interesting property our performance study show that incspan outperforms some previously proposed incremental algorithm a well a a non incremental one with a wide margin 
this paper introduces rankopt a linear binary classifier which optimises the area under the roc curve the auc unlike standard binary classifier rankopt adopts the auc statistic a it objective function and optimises it directly using gradient descent the problem with using the auc statistic a an objective function are that it is non differentiable and of complexity o n in the number of data observation rankopt us a differentiable approximation to the auc which is accurate and computationally efficient being of complexity o n this enables the gradient descent to be performed in reasonable time the performance of rankopt is compared with a number of other linear binary classifier over a number of different classification problem in almost all case it is found that the performance of rankopt is significantly better than the other classifier tested 
this paper is about a variant of k nearest neighbor classification on large many class high dimensional datasets k nearest neighbor remains a popular classification technique especially in area such a computer vision drug activity prediction and astrophysics furthermore many more modern classifier such a kernel based bayes classifier or the prediction phase of svms require computational regime similar to k nn we believe that tractable k nn algorithm therefore continue to be important this paper relies on the insight that even with many class the task of finding the majority class among the k nearest neighbor of a query need not require u to explicitly find those k nearest neighbor this insight wa previously used in liu et al in two algorithm called kns and kns which dealt with fast classification in the case of two class in this paper we show how a different approach ioc standing for the international olympic committee can apply to the case of n class where n ioc assumes a slightly different processing of the datapoints in the neighborhood of the query this allows it to search a set of metric tree one for each class during the search it is possible to quickly prune away class that cannot possibly be the majority we give experimental result on datasets of up to x record and x attribute frequently showing an order of magnitude acceleration compared with each of i conventional linear scan ii a well known independent sr tree implementation of conventional k nn and iii a highly optimized conventional k nn metric tree search 
the correction of bias in magnetic resonance image is an important problem in medical image processing most previous approach have used a maximum likelihood method to increase the likelihood of the pixel in a single image by adaptively estimating a correction to the unknown image bias field the pixel likelihood are defined either in t erms of a pre existing tissue model or non parametrically in term of the image s own pixel value in both case the specific location of a pix el in the image is not used to calculate the likelihood we suggest a new approach in which we simultaneously eliminate the bias from a set of image of the same anatomy but from different patient we use the statistic from the same location across different image rather than within an image to eliminate bias field from all of the image simultaneously the method build a multi resolution non parametric tissue model conditioned on image location while eliminating the bias field associated with the original image set we present experiment on both synthetic and real mr data set and present comparison with other method 
kernel conditional random field kcrfs are introduced a a framework for discriminative modeling of graph structured data a representer theorem for conditional graphical model is given which show how kernel conditional random field arise from risk minimization procedure defined using mercer kernel on labeled graph a procedure for greedily selecting clique in the dual representation is then proposed which allows sparse representation by incorporating kernel and implicit feature space into conditional graphical model the framework enables semi supervised learning algorithm for structured data through the use of graph kernel the framework and clique selection method are demonstrated in synthetic data experiment and are also applied to the problem of protein secondary structure prediction 
we address the problem of learning distance metric using side information in the form of group of similar point we propose to use the rca algorithm which is a simple and efficient algorithm for learning a full ranked mahalanobis metric shental et al we first show that rca obtains the solution to an interesting optimization problem founded on an information theoretic basis if the mahalanobis matrix is allowed to be singular we show that fisher s linear discriminant followed by rca is the optimal dimensionality reduction algorithm under the same criterion we then show how this optimization problem is related to the criterion optimized by another recent algorithm for metric learning xing et al which us the same kind of side information we empirically demonstrate that learning a distance metric using the rca algorithm significantly improves clustering performance similarly to the alternative algorithm since the rca algorithm is much more efficient and cost effective than the alternative a it only us closed form expression of the data it seems like a preferable choice for the learning of full rank mahalanobis distance 
eigenvoice speaker adaptation ha been shown effective when only a small amount of adaptation data is available at the heart of the method is principal component analysis pca employed to find the most important eigenvoices in this paper we postulate that nonlinear pca in particular kernel pca may be even more effective one major challenge is on how to map the feature space eigenvoices back to the observation space so that the state observation likelihood during estimation of eigenvoice weight and subsequent decoding can be computed our solution is to compute kernel pca using composite kernel and we will call our new method kernel eigenvoice on the tidigits corpus we found that compared with a speaker independent model our kernel eigenvoice adaptation method can reduce the word error rate by while the conventional eigenvoice approach can only match the performance of the speaker independent model 
we introduce a new perceptron based discriminative learning algorithm for labeling structured data such a sequence tree and graph since it is fully kernelized and us pointwise label prediction large feature including arbitrary number of hidden variable can be incorporated with polynomial time complexity this is in contrast to existing labelers that can handle only feature of a small number of hidden variable such a maximum entropy markov model and conditional random field we also introduce several kernel function for labeling sequence tree and graph and efficient algorithm for them 
this paper considers the use of computational stylistics for performing authorship attribution of electronic message addressing categorization problem with a many a different class author effective stylistic characterization of text is potentially useful for a variety of task a language style contains cue regarding the authorship purpose and mood of the text all of which would be useful adjunct to information retrieval or knowledge management task we focus here on the problem of determining the author of an anonymous message based only on the message text several multiclass variant of the winnow algorithm were applied to a vector representation of the message text to learn model for discriminating different author we present result comparing the classification accuracy of the different approach the result show that stylistic model can be accurately learned to determine an author s identity 
we present a new technique for achieving source separation when given only a single channel recording the main idea is based on exploiting the inherent time structure of sound source by learning a priori set of basis filter in time domain that encode the source in a statistically efficient manner we derive a learning algorithm using a maximum likelihood approach given the observed single channel data and set of basis filter for each time point we infer the source signal and their contribution factor this inference is possible due to the prior knowledge of the basis filter and the associated coefficient density a fle xible model for density estimation allows accurate modeling of the observation and our experimental result exhibit a high level of separation performance for mixture of two music signal a well a the separation of two voice signal 
using visualization technique to explore and understand high dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays several visualization technique have been developed to study the cluster structure of data i e the existence of distinctive group in the data and how these cluster are related to each other however only few of these technique lend themselves to studying how this structure change if the feature describing the data are changed understanding this relationship between the feature and the cluster structure mean understanding the feature themselves and is thus a useful tool in the feature extraction phase in this paper we present a novel approach to visualizing how modification of the feature with respect to weighting or normalization change the cluster structure we demonstrate the application of our approach in two music related data mining project 
we describe a neuromorphic chip that utilizes transistor heterogeneity introduced by the fabrication process to generate orientation map similar to those imaged in vivo our model consists of a recurrent network of excitatory and inhibitory cell in parallel with a push pull stage similar to a previous model the recurrent network display hotspot of activity that give rise to visual feature map unlike previous work however the map for orientation doe not depend on the sign of contrast instead signindependent cell driven by both on and off channel anchor the map while push pull interaction give rise to sign preserving cell these two group of orientation selective cell are similar to complex and simple cell observed in v orientation m aps neuron in visual area and v and v are selectively tuned for a number of visual feature the most pronounced feature being orientation orientation preference of individual cell varies across the two dimensional surface of the cortex in a stereotyped manner a revealed by electrophysiology and optical imaging study the origin of these preferred orientation po map is debated but experiment demonstrate that they exist in the absence of visual experience to the dismay of advocate of hebbian learning these result suggest that the initial appearance of po map rely on neural mechanism oblivious to input correlation here we propose a model that account for observed po map based on innate noise in neuron threshold and synaptic current the network is implemented in silicon where heterogeneity is a ubiquitous a it is in biology 
a fundamental problem in text data mining is to extract meaningful structure from document stream that arrive continuously over time e mail and news article are two natural example of such stream each characterized by topic that appear grow in intensity for a period of time and then fade away the published literature in a particular research field can be seen to exhibit similar phenomenon over a much longer time scale underlying much of the text mining work in this area is the following intuitive premise that the appearance of a topic in a document stream is signaled by a burst of activity with certain feature rising sharply in frequency a the topic emerges 
high dimensional data that lie on or near a low dimensional manifold can be described by a collection of local linear model such a descri ption however doe not provide a global parameterization of the manifold arguably an important goal of unsupervised learning in this paper we show how to learn a collection of local linear model that solves this more difficult proble m our local linear model are represented by a mixture of factor analyzer and the global coordination of these model is achieved by adding a regularizing term to the standard maximum likelihood objective function the regularizer break a degeneracy in the mixture model s parameter space favoring model who se internal coordinate system are aligned in a consistent way a a result t he internal coordinate change smoothly and continuously a one traverse a connected path on the manifold even when the path cross the domain of many different local model the regularizer take the form of a kullback leibler divergence and illustrates an unexpected application of variational meth od not to perform approximate inference in intractable probabilistic model but to learn more useful internal representation in tractable one 
we abstract out the core search problem of active learning scheme to better understand the extent to which adaptive labeling can improve sample complexity we give various upper and lower bound on the number of label which need to be queried and we prove that a popular greedy active learning rule is approximately a good a any other strategy for minimizing this number of label 
the nip workshop included a feature selection competition organized by the author we provided participant with five datasets from dierent application domain and called for classification result using a minimal number of feature the competition took place over a period of week and attracted research group participant were asked to make on line submission on the validation and test set with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participant at the workshop in total entry were made on the validation set during the development period and entry on all test set for the final competition the winner used a combination of bayesian neural network with ard prior and dirichlet diusion tree other top entry used a variety of method for feature selection which combined filter and or wrapper or embedded method using random forest kernel method or neural network a a classification engine the result of the benchmark including the prediction made by the participant and the feature they selected and the scoring software are publicly available the benchmark is available at www nipsfsc ec soton ac uk for post challenge submission to stimulate further research 
high dimensional data pose a severe challenge for data mining feature selection is a frequently used technique in pre processing high dimensional data for successful data mining traditionally feature selection is focused on removing irrelevant feature however for high dimensional data removing redundant feature is equally critical in this paper we provide a study of feature redundancy in high dimensional data and propose a novel correlation based approach to feature selection within the filter model the extensive empirical study using real world data show that the proposed approach is efficient and effective in removing redundant and irrelevant feature 
we propose probabilistic generative model called parametric mixture model pmms for multiclass multi labeled text categorization problem conventionally the binary classication approach ha been employed in which whether or not text belongs to a category is judged by the binary classier for every category in contrast our approach can simultaneously detect multiple category of text using pmms we derive ecien t learning and prediction algorithm for pmms we also empirically show that our method could signican tly outperform the conventional binary method when applied to multi labeled text categorization using real world wide web page 
we study the synthesis of neural coding selective attention and perceptual decision making a hierarchical neural architecture is proposed which implement bayesian integration of noisy sensory input and topdown attentional prior leading to sound perceptual discrimination the model offer an explicit explanation for the experimentally observed modulation that prior information in one stimulus feature location can have on an independent feature orientation the network s intermediate level of representation instantiate known physiological property of visual cortical neuron the model also illustrates a possible reconciliation of cortical and neuromodulatory representation of uncertainty 
cluster analysis is a fundamental problem and technique in many area related to machine learning in this paper we consider rearrangement clustering which is the problem of finding set of object that share common or similar feature by arranging the row object of a matrix specifying object feature in such a way that adjacent object are similar to each other based on a similarity measure of the feature so a to maximize the overall similarity based on formulating this problem a the traveling salesman problem tsp we develop a new tsp based optimal clustering algorithm called tspcluster we overcome a flaw that is inherent in previous approach by relaxing restriction on dissimilarity between cluster our new algorithm ha three important feature finding the optimal k cluster for a given k automatically detecting cluster border and ascertaining a set of most viable clustering result that make good balance among maximizing the overall similarity within cluster and dissimilarity between cluster we apply tspcluster to cluster and display gene of flowering plant arabidopsis which are regulated under various abiotic stress condition we compare tspcluster to the bond energy algorithm and two existing clustering algorithm our tspcluster code is available at climer zhang 
a more and more activity are carried out using computer and computer network the amount of potentially sensitive data stored by business government and other party increase different party may wish to benefit from cooperative use of their data but privacy regulation and other privacy concern may prevent the party from sharing their data privacy preserving data mining provides a solution by creating distributed data mining algorithm in which the underlying data is not revealed in this paper we present a privacy preserving protocol for a particular data mining task learning the bayesian network structure for distributed heterogeneous data in this setting two party owning confidential database wish to learn the structure of bayesian network on the combination of their database without revealing anything about their data to each other we give an efficient and privacy preserving version of the k algorithm to construct the structure of a bayesian network for the party joint data 
in this paper we pose a novel research problem for machine learning that involves constructing a process model from continuous data we claim that casting learned knowledge in term of process with associated equation is desirable for scienti c and engineering domain where such notation are commonly used we also argue that existing induction method are not well suited to this task although some technique hold a partial solution in response we describe an approach to learning 
inside information come in many form knowledge of a corporate takeover a terrorist attack unexpectedly poor earnings the fda s acceptance of a new drug etc anyone who know some piece of soon to break news posse inside information historically insider trading ha been detected after the news is public but this is often too late fraud ha been perpetrated innocent investor have been disadvantaged or terrorist act have been carried out this paper explores early detection of insider trading detection before the news break data mining hold great promise for this emerging application but the problem also pose significant challenge we present the specific problem of insider trading in option market compare decision tree logistic regression and neural net result to result from an expert model and discus insight that knowledge discovery technique shed upon this problem 
the paper explores a very simple agent design method called q decomposition wherein a complex agent is built from simpler subagents each subagent ha it own reward function and run it own reinforcement learning process it supply to a central arbitrator the q value according to it own reward function for each possible action the arbitrator selects an action maximizing the sum of q value from all the subagents this approach ha advantage over design in which subagents recommend action it also ha the property that if each subagent run the sarsa reinforcement learning algorithm to learn it local q function then a globally optimal policy is achieved on the other hand local q learning lead to globally suboptimal behavior in some case this form of agent decomposition allows the local q function to be expressed by muchreduced state and action space these result are illustrated in two domain that require effective coordination of behavior 
recent biological experimental finding have shown that synaptic plasticity depends on the relative timing of the preand postsynaptic spike this determines whether long term potentiation ltp or long term depression ltd is induced this synaptic plasticity ha been called temporally asymmetric hebbian plasticity tah many author have numerically demonstrated that neural network are capable of storing spatiotemporal pattern however the mathematical mechanism of the storage of spatiotemporal pattern is still unknown and the effect of ltd is particularly unknown in this article we employ a simple neural network model and show that interference between ltp and ltd disappears in a sparse coding scheme on the other hand the covariance learning rule is known to be indispensable for the storage of sparse pattern we also show that tah ha the same qualitative effect a the covariance rule when spatiotemporal pattern are embedded in the network 
handling massive datasets is a difficult problem not only due to prohibitively large number of entry but in some case also due to the very high dimensionality of the data often severe feature selection is performed to limit the number of attribute to a manageable size which unfortunately can lead to a loss of useful information feature space reduction may well be necessary for many stand alone classifier but recent advance in the area of ensemble classifier technique indicate that overall accurate classifier aggregate can be learned even if each individual classifier operates on incomplete feature view training data i e such where certain input attribute are excluded in fact by using only small random subset of feature to build individual component classifier surprisingly accurate and robust model can be created in this work we demonstrate how these type of architecture effectively reduce the feature space for submodels and group of sub model which lends itself to efficient sequential and or parallel implementation experiment with a randomized version of adaboost are used to support our argument using the text classification task a an example 
structural information such a layout and look and feel ha been extensively used in the literatuce for extraction of interesting or relevant data efficient storage and query optimization traditionally tree model such a dom tree have been used to represent structural information especially in the case of html and xml document however computation of structural similarity between document based on the tree model is computationally expensive in this paper we propose an alternative scheme for representing the structural information of document based on the path contained in the corresponding tree model since the model includes partial information about parent child and sibling it allows u to define a new family of meaningful and at the same time computationally simple structural similarity measure our experimental result based on the sigmod xml data set a well a html document collection from ibm com dell com and amazon com show that the representation is powerful enough to produce good cluster of structurally similar page 
eigenvoice speaker adaptation ha been shown to be effective when only a small amount of adaptation data is available at the heart of the method is principal component analysis pca employed to find the most important eigenvoices in this paper we postulate that nonlinear pca in particular kernel pca may be even more effective one major challenge is to map the feature space eigenvoices back to the observation space so that the state observation likelihood can be computed during the estimation of eigenvoice weight and subsequent decoding our solution is to compute kernel pca using composite kernel and we will call our new method kernel eigenvoice speaker adaptation on the tidigits corpus we found that compared with a speaker independent model our kernel eigenvoice adaptation method can reduce the word error rate by while the standard eigenvoice approach can only match the performance of the speaker independent model 
theoretical and experimental analysis of bagging indicate that it is primarily a variance reduction technique this suggests that bagging should be applied to learning algorithm tuned to minimize bias even at the cost of some increase in variance we test this idea with support vector machine svms by employing out of bag estimate of bias and variance to tune the svms experiment indicate that bagging of low bias svms the lobag algorithm never hurt generalization performance and often improves it compared with well tuned single svms and to bag of individually well tuned svms 
we develop a family of upper and lower bound on the worst case expected kl loss for estimating a discrete distribution on a fin ite numberm of point given n i i d sample our upper bound are approximationtheoretic similar to recent bound for estimating discret e entropy the lower bound are bayesian based on average of the kl loss under dirichlet distribution the upper bound are convex in their parameter and thus can be minimized by descent method to provide estimator with low worst case error the lower bound are indexed by a one dimensional parameter and are thus easily maximized asymptotic analysis of the bound demonstrates the uniform kl consistency of a wide class of estimator a c n m no matter how slowly and show that no estimator is consistent for c bounded in contrast to entropy estimation moreover the bound are asymptotically tight a c or and are shown numerically to be tight within a factor of two for all c finally in the sparse data limit c we find that the dirichlet bayes add constant estimator with parameter scaling like clog c optimizes both the upper and lower bound suggesting an optimal choice of the add constant parameter in this regime 
we describe a pattern acquisition algorithm that learns in an unsupervised fashion a streamlined representation of linguistic structure from a plain natural language corpus this paper address the issue of learning structured knowledge from a large scale natural language data set and of generalization to unseen text the implemented algorithm represents sentence a path on a graph whose vertex are word or part of word significant pattern determined by recursive context sensitive statistical inference form new vertex linguistic construction are represented by tree composed of significant pattern and their associated equivalence class an input module allows the algorithm to be subjected to a standard test of english a a second language esl proficiency the result are encouraging the model attains a level of performance considered to be intermediate for th grade student despite having been trained on a corpus childes containing transcribed speech of parent directed to small child 
knowledge about local invariance with respect to given pattern transformation can greatly improve the accuracy of classification previous approach are either based on regularisation or on the generation of virtual transformed example we develop a new framework for learning linear classifier under known transformation based on semidefinite programming we present a new learning algorithm the semidefinite programming machine sdpm which is able to find a maximum margin hyperplane when the training example are polynomial trajectory instead of single point the solution is found to be sparse in dual variable and allows to identify those point on the trajectory with minimal real valued output a virtual support vector extension to segment of trajectory to more than one transformation parameter and to learning with kernel are discussed in experiment we use a taylor expansion to locally approximate rotational invariance in pixel image from usps and find improvement over known method 
we describe a three dimensional geometric hand model suitable for visual tracking application the kinematic constraint implied by the model s joint have a probabilistic structure which is well described by a graphical model inference in this model is complicated by the hand s many degree of freedom a well a multimodal likelihood caused by ambiguous image measurement we use nonparametric belief propagation nbp to develop a tracking algorithm which exploit the graph s structure to control complexity while avoiding costly dis cretization while kinematic constraint naturally have a local structur e selfocclusions created by the imaging process lead to complex interpendencies in color and edge based likelihood function however we show that local structure may be recovered by introducing binary hidden variable describing the occlusion state of each pixel we augment the nbp algorithm to infer these occlusion variable in a distribut ed fashion and then analytically marginalize over them to produce hand position estimate which properly account for occlusion event we provide simulation showing that nbp may be used to refine inaccurate model i nitializations a well a track hand motion through extended image sequence 
the standard approach to the classification of object is to consider the example a independent and identically distributed iid in many real world setting however this assumption is not valid because a topographical relationship exists between the object in this contribution we consider the special case of image segmentation where the object are pixel and where the underlying topography is a d regular rectangular grid we introduce a classification method which not only us measured vectorial feature information but also the label configuration within a topographic neighborhood due to the resulting dependence between the label of neighboring pixel a collective classification of a set of pixel becomes necessary we propose a new method called topographic support vector machine tsvm which is based on a topographic kernel and a self consistent solution to the label assignment shown to be equivalent to a recurrent neural network the performance of the algorithm is compared to a conventional svm on a cell image segmentation task 
we describe a novel method for simultaneously detecting face and estimating their pose in real time the method employ a convolutional network to map image of face to point on a lowdimensional manifold parametrized by pose and image of non face to point far away from that manifold given an image detecting a face and estimating it pose is viewed a minimizing an energy function with respect to the face non face binary variable and the continuous pose parameter the system is trained to minimize a loss function that drive correct combination of label and pose to be associated with lower energy value than incorrect one the system is designed to handle very large range of pose without retraining the performance of the system wa tested on three standard data set for frontal view rotated face and profile is comparable to previous system that are designed to handle a single one of these data set we show that a system trained simuiltaneously for detection and pose estimation is more accurate on both task than similar system trained for each task separately 
classification algorithm typically induce population wide model that are trained to perform well on average on expected future instance we introduce a bayesian framework for learning instance specific model from data that are optimized to predict well for a particular instance based on this framework we present a lazy instance specific algorithm called isa that performs selective model averaging over a restricted class of bayesian network on experimental evaluation this algorithm show superior performance over model selection we intend to apply such instance specific algorithm to improve the performance of patient specific predictive model induced from medical data 
a fundamental problem in business and other application is ranking item with respect to some notion of profit based on historical transaction the difficulty is that the profit of one item not only come from it own sale but also from it influence on the sale of other item i e the cross selling effect in this paper we draw an analogy between this influence and the mutual reinforcement of hub authority web page based on this analogy we present a novel approach to the item ranking problem we apply this ranking approach to solve two selection problem in size constrained selection the maximum number of item that can be selected is fixed in cost constrained selection there is no maximum number of item to be selected but there is some cost associated with the selection of each item in both case the question is what item should be selected to maximize the profit empirically we show that this method find profitable item in the presence of cross selling effect 
existing axis scaling and dimensionality method focus on preserving structure usually determined via the euclidean distance in other word they inherently assume that the euclidean distance is already correct we instead propose a novel nonlinear approach driven by an information theoretic viewpoint which we show is also strongly linked to intrinsic dimensionality or degree of freedom and uniformity nonlinear transformation based on common probability distribution combined with information driven selection simultaneously reduce the number of dimension required and increase the value of those we retain experiment on real data confirm that this approach reveals correlation find novel attribute and scale well 
in many reinforcement learning application the set of possible action can be partitioned by the programmer into subset of similar action this paper present a technique for exploiting this form of prior information to speed up model based reinforcement learning we call it an action renement method because it treat each subset of similar action a a single abstract action early in the learning process and then later renes the abstract action into individual action a more experience is gathered our method estimate the transition probability p s j a for an action a by combining the result of execution of action a with execution of other action in the same subset of similar action this is a form of smoothing of the probability estimate that trade increased bias for reduced variance the paper derives a formula for optimal smoothing which show that the degree of smoothing should decrease a the amount of data increase experiment show that probability smoothing is better than two simpler action renement method on a synthetic maze problem action renement is most useful in problem such a robotics where training experience are expensive 
a new large margin classifier named maxi min margin machine m is proposed in this paper this new classifier is constructed based on both a local and a global view of data while the most popular large margin classifier support vector machine svm and the recently proposed important model minimax probability machine mpm consider data only either locally or globally this new model is theoretically important in the sense that svm and mpm can both be considered a it special case furthermore the optimization of m can be cast a a sequential conic programming problem which can be solved efficiently we describe the m model definition provide a clear geometrical interpretation present theoretical justification propose efficient solving method and perform a series of evaluation on both synthetic data set and real world benchmark data set it comparison with svm and mpm also demonstrates the advantage of our new model 
because of practical limit in characterizing the safety profile of therapeutic product prior to marketing manufacturer and regulatory agency perform post marketing surveillance based on the collection of adverse reaction report pharmacovigilance the resulting database while rich in real world information are notoriously difficult to analyze using traditional technique each report may involve multiple medicine symptom and demographic factor and there is no easily linked information on drug exposure in the reporting population kdd technique such a association finding are well matched to the problem but are difficult for medical staff to apply and interpret to deploy kdd effectively for pharmacovigilance lincoln technology and glaxosmithkline collaborated to create a webbased safety data mining web environment the analytical core is a high performance implementation of the mgps multi item gamma poisson shrinker algorithm described previously by dumouchel and pregibon with several significant extension and enhancement the environment offer an interface for specifying data mining run a batch execution facility tabular and graphical method for exploring association and drilldown to case detail substantial work wa involved in preparing the raw adverse event data for mining including harmonization of drug name and removal of duplicate report the environment can be used to explore both drug event and multi way association interaction syndrome it ha been used to study age gender effect to predict the safety profile of proposed combination drug and to separate contribution of individual drug to safety problem in polytherapy situation 
computation without stable state is a computing paradigm different from turing s and ha been demonstrated for various type of simulated neural network this publication transfer this to a hardware implemented neural network result of a software implementation are reproduced showing that the performance peak when the network exhibit dynamic at the edge of chaos the liquid computing approach seems well suited for operating analog computing device such a the used vlsi neural network 
heterogeneous type of gene expression may provide a better insight into the biological role of gene interaction with the environment disease development and drug effect at the molecular level in this paper for both exploring and prediction purpose a time lagged recurrent neural network with trajectory learning is proposed for identifying and classifying the gene functional pattern from the heterogeneous nonlinear time series microarray experiment the proposed procedure identify gene functional pattern from the dynamic of a state trajectory learned in the heterogeneous time series and the gradient information over time also the trajectory learning with back propagation through time algorithm can recognize gene expression pattern vary over time this may reveal much more information about the regulatory network underlying gene expression the analyzed data were extracted from spotted dna microarrays in the budding yeast expression measurement produced by eisen et al the gene matrix contained experiment over a variety of heterogeneous experiment condition the number of recognized gene pattern in our study ranged from two to ten and were divided into three case optimal network architecture with different memory structure were selected based on akaike and bayesian information statistical criterion using two way factorial design the optimal model performance wa compared to other popular gene classification algorithm such a nearest neighbor support vector machine and self organized map the reliability of the performance wa verified with multiple iterated run 
this paper applies fast sparse multidimensional scaling md to a large graph of music similarity with k vertex that represent artist album and track and m edge that represent similarity between those entity once vertex are assigned location in a euclidean space the location can be used to browse music and to generate playlist md on very large sparse graph can be effectively performed by a family of algorithm called rectangular dijsktra rd md algorithm these rd algorithm operate on a dense rectangular slice of the distance matrix created by calling dijsktra a constant number of time two rd algorithm are compared landmark md which us the nystrm approximation to perform md and a new algorithm called fast sparse embedding which us fastmap these algorithm compare favorably to laplacian eigenmaps both in term of speed and embedding quality 
this paper proposes a novel decision tree for a data set with time series attribute our time series tree ha a value i e a time sequence of a time series attribute in it internal node and split example based on dissimilarity between a pair of time sequence our method selects for a split test a time sequence which exists in data by exhaustive search based on class and shape information experimental result confirm that our induction method construct comprehensive and accurate decision tree moreover a medical application show that our time series tree is promising for knowledge discovery 
reinforcement learning and q learning inparticular encounter two major problemswhen dealing with large state space first learning the q function in tabular form maybe infeasible because of the excessive amountof memory needed to store the table andbecause the q function only converges aftereach state ha been visited multiple time 
instance selection and feature selection are two orthogonal method for reducing the amount and complexity of data feature selection aim at the reduction of redundant feature in a dataset whereas instance selection aim at the reduction of the number of instance so far these two method have mostly been considered in isolation in this paper we present a new algorithm which we call fis feature and instance selection that target both problem simultaneously in the context of text classificationour experiment on the reuters and newsgroups datasets show that fis considerably reduces both the number of feature and the number of instance the accuracy of a range of classifier including na ve bayes tan and lb considerably improves when using the fis preprocessed datasets matching and exceeding that of support vector machine which is currently considered to be one of the best text classification method in all case the result are much better compared to mutual information based feature selection the training and classification speed of all classifier is also greatly improved 
learning in many multi agent setting is in herently repeated play this call into ques tion the naive application of single play nash equilibrium in multi agent learning and sug gests instead the application of give and take principle of bargaining we modify and analyze a satis cing algorithm based on karandikar et al that is compat ible with the bargaining perspective this algorithm is a form of relaxation search that converges to a satis cing equilibrium without knowledge of game payofis or other agent action we then develop an m action n player social dilemma that encodes the key element of the prisoner s dilemma this game is instructive because it characterizes social dilemma with more than two agent and more than two choice we show how several difierent multi agent learning algo rithms behave in this social dilemma and demonstrate that the satis cing algorithm converges with high probability to a pareto e cient solution in self play and to the single play nash equilibrium against sel sh agent finally we present theoretical result that characterize the behavior of the algorithm 
introduction a mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in it action outcome indeed almost all interesting sequential decision making domain involve large state space and large stochastic action set we investigate a way to act intelligently a quickly a possible in domain where finding a complete policy would take a hopelessly long time this approach relational envelope based planning rebp tackle large noisy problem along two ax first describing a domain a a relational mdp instead of a an atomic or propositionally factored mdp allows problem structure and dynamic to be captured compactly with a small set of probabilistic relational rule second an envelope based approach to planning let an agent begin acting quickly within a restricted part of the full state space and to judiciously expand it envelope a resource permit the case for both probability and logical structure quickly generating generating usable plan when the world abounds with uncertainty is an important and difficult enterprise consider the classic block world domain the number of way to make a stack of a certain height grows exponentially with the number of block on the table and if the outcome of action are uncertain the task becomes even more daunting we want planning technique that can deal with large state space and large stochastic action set since most compelling realistic domain have these characteristic we are investigating a method for planning in very large domain by using expressive rule to restrict attention to high utility subset of the state space 
in a multi view problem the feature of thedomain can be partitioned into disjoint subset view that are sufficient to learn the target concept 
text categorization algorithm usually represent document a bag of word and consequently have to deal with huge number of feature most previous study found that the majority of these feature are relevant for classification and that the performance of text categorization with support vector machine peak when no feature selection is performed we describe a class of text categorization problem that are characterized with many redundant feature even though most of these feature are relevant the underlying concept can be concisely captured using only a few feature while keeping all of them ha substantially detrimental effect on categorization accuracy we develop a novel measure that capture feature redundancy and use it to analyze a large collection of datasets we show that for problem plagued with numerous redundant feature the performance of c is significantly superior to that of svm while aggressive feature selection allows svm to beat c by a narrow margin 
extensive effort have been devoted to developing efficient algorithm for mining frequent pattern however frequent pattern mining remains a time consuming process especially for very large datasets it is therefore desirable to adopt a mining once and using many time strategy unfortunately there ha been little work reported on managing and organizing a large set of pattern for future use in this paper we propose a disk based data structure cfp tree condensed frequent pattern tree for organizing frequent pattern discovered from transactional database in addition to an efficient algorithm for cfp tree construction we also developed algorithm to efficiently support two important type of query namely query with minimum support constraint and query with item constraint against the stored pattern a these two type of query are basic building block for complex frequent pattern related mining task comprehensive experimental study ha been conducted to demonstrate the effectiveness of cfp tree and efficiency of related algorithm 
we describe an open source java toolkit of method for matching name and record we summarize result obtained from using various string distance metric on the task of matching entity name these metric include distance function proposed by several different community such a edit distance metric fast heuristic string comparators token based distance metric and hybrid method we then describe an extension to the toolkit which allows record to be compared we discus some issue involved in performing a similar comparision for record matching technique and finally present result for some baseline record matching algorithm that aggregate string comparison between field 
we consider the problem of geometrical surface reconstruction from one or several image using learned shape model while human can effortlessly retrieve d shape information this inverse problem ha turned out to be difcult to perform automatically we introduce a framework based on level set surface reconstruction and shape model for achieving this goal through this merging we obtain an efcient and robust method for reconstructing surface of an object category of interest the shape model includes surface cue such a point curve and silhouette feature based on idea from active shape model we show how both the geometry and the appearance of these feature can be modelled consistently in a multi view context the complete surface is obtained by evolving a level set driven by a pde which try to t the surface to the inferred d feature in addition an a priori d surface model is used to regularize the solution in particular where surface feature are sparse experiment are demonstrated on a database of real face image 
when automatically extracting information from the world wide web most established method focus on spotting single html document however the problem of spotting complete web site is not handled adequately yet in spite of it importance for various application therefore this paper discus the classification of complete web site first we point out the main difference to page classification by discussing a very intuitive approach and it weakness this approach treat a web site a one large html document and applies the well known method for page classification next we show how accuracy can be improved by employing a preprocessing step which assigns an occurring web page to it most likely topic the determined topic now represent the information the web site contains and can be used to classify it more accurately we accomplish this by following two direction first we apply well established classification algorithm to a feature space of occurring topic the second direction treat a site a a tree of occurring topic and us a markov tree model for further classification to improve the efficiency of this approach we additionally introduce a powerful pruning method reducing the number of considered web page our experiment show the superiority of the markov tree approach regarding classification accuracy in particular we demonstrate that the use of our pruning method not only reduces the processing time but also improves the classification accuracy 
we present and empirically analyze a machine learning approach for detecting intrusion on individual computer our winnow based algorithm continually monitor user and system behavior recording such property a the number of byte transferred over the last second the program that currently are running and the load on the cpu in all hundred of measurement are made and analyzed each second using this data our algorithm creates a model that represents each particular computer s range of normal behavior parameter that determine when an alarm should be raised due to abnormal activity are set on a per computer basis based on an analysis of training data a major issue in intrusion detection system is the need for very low false alarm rate our empirical result suggest that it is possible to obtain high intrusion detection rate and low false alarm rate le than one per day per computer without stealing too many cpu cycle le than we also report which system measurement are the most valuable in term of detecting intrusion a surprisingly large number of different measurement prove significantly useful 
part number are widely used within an enterprise throughout the manufacturing process the point of entry of such part number into this process is normally via a bill of material or bom sent by a contact manufacturer or supplier each line of the bom provides information about one part such a the supplier part number the bom receiver s corresponding internal part number an unstructured textual part description the supplier name etc however in a substantial number of case the bom receiver s internal part number is absent hence before this part can be incorporated into the receiver s manufacturing process it ha to be mapped to an internal part of the bom receiver based on the information of the part in the bom historically this mapping process ha been done manually which is a highly time consuming labor intensive and error prone process this paper describes a system for automating the mapping of bom part number the system us a two step modeling and mapping approach first the system us historical bom data receiver s part specification data and receiver s part taxonomic data along with domain knowledge to automatically learn classification model for mapping a given bom part description to successively lower level of the receiver s part taxonomy to reduce the set of potential internal part to which the bom part could map to then information about various part parameter is extracted from the bom part description and compared to the specification data of the potential internal part to choose the final mapped internal part mapping done by the system are very accurate and the system is currently being deployed within ibm for mapping boms received by the corporate procurement manufacturing division 
the microeconomic framework for data mining assumes that an enterprise chooses a decision maximizing the overall utility over all customer where the contribution of a customer is a function of the data available on that customer in catalog segmentation the enterprise want to design k product catalog of size r that maximize the overall number of catalog product purchased however there are many application where a customer once attracted to an enterprise would purchase more product beyond the one contained in the catalog therefore in this paper we investigate an alternative problem formulation that we call customer oriented catalog segmentation where the overall utility is measured by the number of customer that have at least a specified minimum interest t in the catalog we formally introduce the customer oriented catalog segmentation problem and discus it complexity then we investigate two different paradigm to design efficient approximate algorithm for the customer oriented catalog segmentation problem greedy deterministic and randomized algorithm since greedy algorithm may be trapped in a local optimum and randomized algorithm crucially depend on a reasonable initial solution we explore a combination of these two paradigm our experimental evaluation on synthetic and real data demonstrates that the new algorithm yield catalog of significantly higher utility compared to classical catalog segmentation algorithm 
this paper extends previous work on the skewing algorithm a promising approach that allows greedy decision tree induction algorithm to handle problematic function such a parity function with a lower run time penalty than lookahead a deficiency of the previously proposed algorithm is it inability to scale up to high dimensional problem in this paper we describe a modified algorithm that scale better with increasing number of variable we present experiment with randomly generated boolean function that evaluate the algorithm s response to increasing dimension we also evaluate the algorithm on a challenging real world biomedical problem that of sh domain binding our result indicate that our algorithm almost always outperforms an information gain based decision tree learner 
speech dereverberation is desirable with a view to achieving for example robust speech recognition in the real world however it is still a challenging problem especially when using a single microphone although blind equalization technique have been exploited they cannot deal with speech signal appropriately because their assumption are not satisfied by speech signal we propose a new dereverberation principle based on an inherent property of speech signal namely quasi periodicity the present method learn the dereverberation filter from a lot of speech data with no prior knowledge of the data and can achieve high quality speech dereverberation especially when the reverberation time is long 
most machine learning researcher perform quantitative experiment to estimate generalization error and compare the performance of different algorithm in particular their proposed algorithm in order to be able to draw statistically convincing conclusion it is important to estimate the uncertainty of such estimate this paper study the very commonly used k fold cross validation estimator of generalization performance the main theorem show that there exists no universal valid under all distribution unbiased estimator of the variance of k fold cross validation the analysis that accompanies this result is based on the eigen decomposition of the covariance matrix of error which ha only three different eigenvalue corresponding to three degree of freedom of the matrix and three component of the total variance this analysis help to better understand the nature of the problem and how it can make naive estimator that don t take into account the error correlation due to the overlap between training and test set grossly underestimate variance this is confirmed by numerical experiment in which the three component of the variance are compared when the difficulty of the learning problem and the number of fold are varied 
we investigate how to learn a kernel matrix for high dimensional data that lie on or near a low dimensional manifold noting that the kernel matrix implicitly map the data into a nonlinear feature space we show how to discover a mapping that unfolds the underlying manifold from which the data wa sampled the kernel matrix is constructed by maximizing the variance in feature space subject to local constraint that preserve the angle and distance between nearest neighbor the main optimization involves an instance of semidefinite programming a fundamentally different computation than previous algorithm for manifold learning such a isomap and locally linear embedding the optimized kernel perform better than polynomial and gaussian kernel for problem in manifold learning but worse for problem in large margin classification we explain these result in term of the geometric property of different kernel and comment on various interpretation of other manifold learning algorithm a kernel method 
in many real world application active selection of training example can significantly reduce the number of labelled training example to learn a classification function different strategy in the field of support vector machine have been proposed that iteratively select a single new example from a set of unlabelled example query the corresponding class label and then perform retraining of the current classifier however to reduce computational time for training it might be necessary to select batch of new training example instead of single example strategy for single example can be extended straightforwardly to select batch by choosing the h example that get the highest value for the individual selection criterion we present a new approach that is especially designed to construct batch and incorporates a diversity measure it ha low computational requirement making it feasible for large scale problem with several thousand of example experimental result indicate that this approach provides a faster method to attain a level of generalization accuracy in term of the number of labelled example 
the link between genetic algorithm and population based markov chain monte carlo mcmc method are explored genetic algorithm gas are well known for their capability to optimize function of discretevalued variable but the mcmc interpretation allows ga variant to be used for sampling discrete space e g in bayesian inference for machine learning the ga crossover and mutation operator are modied to provide valid mcmc sample and a new exclusive or operator is introduced a an alternative way to recombine population member this is shown to improve sampling performance in a medical diagnostic problem domain the sampler can also be used within simulated annealing to provide a global optimizer that is similar to a ga in structure but ha known convergence property 
the minimax probability machine mpm considers a binary classiflcation problem where mean and covariance matrix of each class are assumed to be known without making any further distributional assumption the mpm minimizes the worst case probability fi of misclassiflcation of future data point however the validity of the upper bound fi depends on the accuracy of the estimate of the real but unknown mean and covariance first we show how to make this minimax approach robust against certain estimation error for unknown but bounded mean and covariance matrix we guarantee a robust upper bound secondly the robust minimax approach for supervised learning is extended in a very natural way to the unsupervised learning problem of quantile estimation computing a minimal region in input space where at least a fraction fi of the total probability mass life mercer kernel can be exploited in this setting to obtain nonlinear region positive empirical result are obtained when comparing this approach to single class svm and a class svm approach 
this paper address three question is it useful to attempt to learn a bayesian network structure with hundred of thousand of node how should such structure search proceed practically the third question arises out of our approach to the second how can frequent set agrawal et al which are extremely popular in the area of descriptive data mining be turned into a probabilistic model large sparse datasets with hundred of thousand of record and attribute appear in social network warehousing supermarket transaction and web log the complexity of structural search made learning of factored probabilistic model on such datasets unfeasible we propose to use frequent set to significantly speed up the structural search unlike previous approach we not only cache n way sufficient statistic but also exploit their local structure we also present an empirical evaluation of our algorithm applied to several massive datasets 
linear discriminant analysis lda is a well known method for feature extraction and dimension reduction it ha been used widely in many application such a face recognition recently a novel lda algorithm based on qr decomposition namely lda qr ha been proposed which is competitive in term of classification accuracy with other lda algorithm but it ha much lower cost in time and space however lda qr is based on linear projection which may not be suitable for data with nonlinear structure this paper first proposes an algorithm called kda qr which extends the lda qr algorithm to deal with nonlinear data by using the kernel operator then an efficient approximation of kda qr called akda qr is proposed experiment on face image data show that the classification accuracy of both kda qr and akda qr are competitive with generalized discriminant analysis gda a general kernel discriminant analysis algorithm while akda qr ha much lower time and space cost 
we investigate how random projection can best be used for clustering high dimensional data random projection ha been shown to have promising theoretical property in practice however we nd that it result in highly unstable clustering performance our solution is to use random projection in a cluster ensemble approach empirical result show that the proposed approach achieves better and more robust clustering performance compared to not only single run of random projection clustering but also clustering with pca a traditional data reduction method for high dimensional data to gain insight into the performance improvement obtained by our ensemble method we analyze and identify the influence of the quality and the diversity of the individual clustering solution on the nal ensemble performance 
decision tree are commonly used for classification we propose to use decision tree not just for classification but also for the wider purpose of knowledge discovery because visualizing the decision tree can reveal much valuable information in the data we introduce paintingclass a system for interactive construction visualization and exploration of decision tree paintingclass provides an intuitive layout and convenient navigation of the decision tree paintingclass also provides the user the mean to interactively construct the decision tree each node in the decision tree is displayed a a visual projection of the data through actual example and comparison with other classification method we show that the user can effectively use paintingclass to construct a decision tree and explore the decision tree to gain additional knowledge 
abstract standard approach to object detection focus on local patch of theimage and try to classify them a background or not we propose touse the scene context image a a whole a an extra source of global information to help resolve local ambiguity we present a conditionalrandom field for jointly solving the task of object detection and sceneclassification 
the instability problem of decision tree classification algorithm is that small change in input training sample may cause dramatically large change in output classification rule different rule generated from almost the same training sample are against human intuition and complicate the process of decision making in this paper we present fundamental theorem for the instability problem of decision tree classifier the first theorem give the relationship between a data change and the resulting tree structure change i e split change the second theorem instability theorem provides the cause of the instability problem based on the two theorem algorithmic improvement can be made to lessen the instability problem empirical result illustrate the theorem statement the tree constructed by the proposed algorithm are more stable noise tolerant informative expressive and concise our proposed sensitivity measure can be used a a metric to evaluate the stability of splitting predicate the tree sensitivity is an indicator of the confidence level in rule and the effective lifetime of rule 
the multiple instance learning mil model ha been very successful in application area such a drug discovery and content based image retrieval recently a generalization of this model and an algorithm for this generalization were introduced showing significant advantage over the conventional mil model in certain application area unfortunately this algorithm is inherently inefficient preventing scaling to high dimension we reformulate this algorithm using a kernel for a support vector machine reducing it time complexity from exponential to polynomial computing the kernel is equivalent to counting the number of axis parallel box in a discrete bounded space that contain at least one point from each of two multisets p and q we show that this problem is p complete but then give a fully polynomial randomized approximation scheme fpras for it finally we empirically evaluate our kernel 
viral marketing take advantage of network of influence among customer to inexpensively achieve large change in behavior our research seek to put it on a firmer footing by mining these network from data building probabilistic model of them and using these model to choose the best viral marketing plan knowledge sharing site where customer review product and advise each other are a fertile source for this type of data mining in this paper we extend our previous technique achieving a large reduction in computational cost and apply them to data from a knowledge sharing site we optimize the amount of marketing fund spent on each customer rather than just making a binary decision on whether to market to him we take into account the fact that knowledge of the network is partial and that gathering that knowledge can itself have a cost our result show the robustness and utility of our approach 
constraint based mining of itemsets for question such a find all frequent itemsets where the total price is at least ha received much attention recently two class of constraint monotone and antimonotone have been identified a very useful there are algorithm that efficiently take advantage of either one of these two class but no previous algorithm can efficiently handle both type of constraint simultaneously in this paper we present the first algorithm called dualminer that us both monotone and antimonotone constraint to prune it search space we complement a theoretical analysis and proof of correctness of dualminer with an experimental study that show the efficacy of dualminer compared to previous work 
abstract in dynamic programming convergence of algorithm such a value iteration or policy iteration result in discounted problemsfrom a contraction property of the back up operator guaranteeing convergence to it fixedpoint when approximation is considered known result in approximate policy iteration provide bound on the closeness to optimality of the approximate value function obtained by successive policy improvement step a a function of the maximum norm of value determination error during policy evaluation step unfortunately such result have limited practical range since most function approximators such a linear regression select the best fit in a given class of parameterized function by minimizing some weighted quadratic norm in this paper we provide error bound for approximate policy iteration using quadratic norm and illustrate those result in the case of feature based linear function approximation 
hill climbing search is the most commonly used search algorithm in ilp system because it permit the generation of theory in short running time however a well known drawback of this greedy search strategy is it myopia macro operator or macro for short a recently proposed technique to reduce the search space explored by exhaustive search can also be argued to reduce the myopia of hill climbing search by automatically performing a variable depth look ahead in the search space surprisingly macro have not been employed in a greedy learner in this paper we integrate macro into a hill climbing learner in a detailed comparative study in several domain we show that indeed a hill climbing learner using macro performs significantly better than current state of the art system involving other technique for reducing myopia such a fixed depth look ahead template based look ahead beam search or determinate literal in addition macro in contrast to some of the other approach can be computed fully automatically and do not require user involvement nor special domain property such a determinacy 
entropy type measure for the heterogeneity of cluster have been used for a long time this paper study the entropy based criterion in clustering categorical data it first show that the entropy based criterion can be derived in the formal framework of probabilistic clustering model and establishes the connection between the criterion and the approach based on dissimilarity co efficients an iterative monte carlo procedure is then presented to search for the partition minimizing the criterion experiment are conducted to show the effectiveness of the proposed procedure 
abstract convergence for iterative reinforcement learning algorithm liketd depends on the sampling strategy for the transition however in practical application it is convenient to take transitiondata from arbitrary source without losing convergence in thispaper we investigate the problem of repeated synchronous updatesbased on axed set of transition our main theorem yield su cient condition of convergence for combination of reinforcementlearning algorithm and linear 
abstract given any generative classier based on an inexact density model we can dene a discriminative counterpart that reduces it asymptotic error rate we introduce a family of classiers that interpolate the two approach thus providing a new way to compare them and giving an estimation procedure whose classication performance is well balanced between the bias of generative classiers and the variance of discriminative one we show that an intermediate trade o between the two strategy is often preferable both theoretically and in experiment on real data 
we consider supervised learning in the presence of very many irrelevant feature and study two different regularization method for preventing overfitting focusing on logistic regression we show that using l regularization of the parameter the sample complexity i e the number of training example required to learn well grows only logarithmically in the number of irrelevant feature this logarithmic rate match the best known bound for feature selection and indicates that l regularized logistic regression can be effective even if there are exponentially many irrelevant feature a there are training example we also give a lower bound showing that any rotationally invariant algorithm including logistic regression with l regularization svms and neural network trained by backpropagation ha a worst case sample complexity that grows at least linearly in the number of irrelevant feature 
we introduce population based markov chain monte carlo sampling algorithm that use proposal density obtained by a novel method direct search optimization technique downhill simplex method and differential evolution operate in real valued space using a population of state vector and geometric operation to generate proposal similar geometric proposal are used here for mcmc sampling but are modified to meet the strict requirement for unbiased sampling of the target density we 
significant plasticity in sensory cortical representation can be driven in mature animal either by behavioural task that pair sensory stimulus with reinforcement or by electrophysiological experiment that pair sensory input with direct stimulation of neuromodulatory nucleus but usually not by sensory stimulus presented alone biologically motivated theory of representational learning however have tended to focus on unsupervised mechanism which may play a significant role on evolutionary or developmental timescales but which neglect this essential role of reinforcement in adult plasticity by contrast theoretical reinforcement learning ha generally dealt with the acquisition of optimal policy for action in an uncertain world rather than with the concurrent shaping of sensory representation this paper develops a framework for representational learning which build on the relative success of unsupervised generativemodelling account of cortical encoding to incorporate the effect of reinforcement in a biologically plausible way a remarkable feature of the brain is it ability to adapt to and learn from experience this learning ha measurable physiological correlate in term of change in the stimulusresponse property of individual neuron in the sensory system of the brain a well a in many other area while passive exposure to sensory stimulus can have profound effect on the developing sensory cortex significant plasticity in mature animal tends to be observed only in situation where sensory stimulus are associated with either behavioural or electrical reinforcement considerable theoretical attention ha been paid to unsupervised learning of representation adapted to natural sensory statistic and to the learning of optimal policy of action for decision process however relatively little work particularly of a biological bent ha sought to understand the impact of reinforcement task on representation to be complete understanding of sensory plasticity must come at two different level at a mechanistic level it is important to understand how synapsis are modified and how synaptic modification can lead to observed change in the response property of cell numerous experiment and model have addressed these question of how sensory plastic 
gaussian process regression allows a simple analytical treatment of exact bayesian inference and ha been found to provide good performance yet scale badly with the number of training data in this paper we compare several approach towards scaling gaussian process regression to large data set the subset of representers method the reduced rank approximation online gaussian process and the bayesian committee machine furthermore we provide theoretical insight into some of our experimental result we found that subset of representers method can give good and particularly fast prediction for data set with high and medium noise level on complex low noise data set the bayesian committee machine achieves significantly better accuracy yet at a higher computational cost 
privacy preserving data mining ha concentrated on obtaining valid result when the input data is private an extreme example is secure multiparty computation based method where only the result are revealed however this still leaf a potential privacy breach do the result themselves violate privacy this paper explores this issue developing a framework under which this question can be addressed metric are proposed along with analysis that those metric are consistent in the face of apparent problem 
a data stream is a massive unbounded sequence of data element continuously generated at a rapid rate consequently the knowledge embedded in a data stream is more likely to be changed a time go by identifying the recent change of a data stream specially for an online data stream can provide valuable information for the analysis of the data stream in addition monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge however most of mining algorithm over a data stream do not differentiate the information of recently generated transaction from the obsolete information of old transaction which may be no longer useful or possibly invalid at present this paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream the effect of old transaction on the mining result of the data steam is diminished by decaying the old occurrence of each itemset a time go by furthermore several optimization technique are devised to minimize processing time a well a main memory usage finally the proposed method is analyzed by a series of experiment 
a typical neuron in visual cortex receives most input from other cortical neuron with a roughly similar stimulus preference doe this arrangement of input allow efficient readout of sensory information by the target cortical neuron we address this issue by using simple modelling of neuronal population activity and information theoretic tool we find that efficient synaptic information transmission requires that the tuning curve of the afferent neuron is approximately a wide a the spread of stimulus preference of the afferent neuron reaching the target neuron by meta analysis of neurophysiological data we found that this is the case for cortico cortical input to neuron in visual cortex we suggest that the organization of v cortico cortical synaptic input allows optimal information transmission 
using a markov chain perspective of spectral clustering we present an algorithm to automatically find the number of stable cluster in a dataset the markov chain s behaviour is characterized by the spectral property of the matrix of transition probability from which we derive eigenflows along with their halflives an eigenflow describes the flow of probability mass due to the markov chain and it is characterized by it eigenvalue or equivalently by the halflife of it decay a the markov chain is iterated a ideal stable cluster is one with zero eigenflow and infinite half life the key insight in this paper is that bottleneck between weakly coupled cluster can be identified by computing the sensitivity of the eigenflow s halflife to variation in the edge weight we propose a novel eigencuts algorithm to perform clustering that remove these identified bottleneck in an iterative fashion 
psychophysical data suggest that temporal modulation of stimulus amplitude envelope play a prominent role in the perceptual segregation of concurrent sound in particular the detection of an unmodulated signal can be significantly improved by adding amplitude modulation to the spectral envelope of a competing masking noise this perceptual phenomenon is known a comodulation masking release cmr despite the obvious influence of temporal structure on the perception of complex auditory scene the physiological mechanism that contribute to cmr and auditory streaming are not well known a recent physiological study by nelken and colleague ha demonstrated an enhanced cortical representation of auditory signal in modulated noise our study evaluates these cmr like response pattern from the perspective of a hypothetical auditory edge detection neuron it is shown that this simple neural model for the detection of amplitude transient can reproduce not only the physiological data of nelken et al but also in light of previous result a variety of physiological and psychoacoustical phenomenon that are related to the perceptual segregation of concurrent sound 
in the analysis of natural image gaussian scale mixture gsm have been used to account for the statistic of lter response and to inspire hierarchical cortical representational learning scheme gsms pose a critical assignment problem working out which lter response were generated by a common multiplicative factor we present a new approach to solving this assignment problem through a probabilistic extension to the basic gsm and show how to perform inference in the model using gibbs sampling we demonstrate the efcac y of the approach on both synthetic and image data understanding the statistical structure of natural image is an important goal for visual neuroscience neural representation in early cortical area decompose image and likely other sensory input in a way that is sensitive to sophisticated aspect of their probabilistic structure this structure also play a key role in method for image processing and coding a striking aspect of natural image that ha reections in both top down and bottom up modeling is coordination across nearby location scale and orientation from a topdown perspective this structure ha been modeled using what is known a a gaussian scale mixture model gsm gsms involve a multi dimensional gaussian each dimension of which capture local structure a in a linear lter multiplied by a spatialized collection of common hidden scale variable or mixer variable which capture the coordination gsms have wide implication in theory of cortical receptive eld development eg the comprehensive bubble framework of hyv arinen the mixer variable provide the top down account of two bottom up characteristic of natural image statistic namely the bowtie statistical dependency and the fact that the marginal distribution of receptive eld lik e lters have high kurtosis in hindsight these idea also bear a close relationship with ruderman and bialek s multiplicative bottom up image analysis framework and statistical model for divisive gain control coordinated structure ha also been addressed in other image work and in other domain such a speech and nance 
we propose a fast iterative classification algorithm for kernel fisher discriminant kfd using heterogeneous kernel model in contrast with the standard kfd that requires the user to predefine a kernel function we incorporate the task of choosing an appropriate kernel into the optimization problem to be solved the choice of kernel is defined a a linear combination of kernel belonging to a potentially large family of different positive semidefinite kernel the complexity of our algorithm doe not increase significantly with respect to the number of kernel on the kernel family experiment on several benchmark datasets demonstrate that generalization performance of the proposed algorithm is not significantly different from that achieved by the standard kfd in which the kernel parameter have been tuned using cross validation we also present result on a real life colon cancer dataset that demonstrate the efficiency of the proposed method 
we introduce nashprop an iterative and local message passing algorithm for computing nash equilibrium in multi player game represented by arbitrary undirected graph we provide a formal analysis and experimental evidence demonstrating that nashprop performs well on large graphical game with many loop often converging in just a dozen iteration on graph with hundred of node nashprop generalizes the tree algorithm of kearns et al and can be viewed a similar in spirit to belief propagation in probabilistic inference and thus complement the recent work of vickrey and koller who explored a junction tree approach thus a for probabilistic inference we have at least two promising general purpose approach to equilibrium computation in graph 
the decision function constructed by support vector machine svm s usually depend only on a subset of the training set the so called support vector we derive asymptotically sharp lower and upper bound on the number of support vector for several standard type of svm s in particular we show for the gaussian rbf kernel that the fraction of support vector tends to twice the bayes risk for the l svm to the probability of noise for the l svm and to for the l svm 
we describe the ra scanner a novel system for the examination of patient suffering from rheumatoid arthritis the ra scanner is based on a novel laser based imaging technique which is sensitive to the optical characteristic of finger joint tissue based on the laser image finger joint are classified according to whether the inflammatory status ha improved or worsened to perform the classification task various linear and kernel based system were implemented and their performance were compared special emphasis wa put on measure to reliably perform parameter tuning and evaluation since only a very small data set wa available based on the result presented in this paper it wa concluded that the ra scanner permit a reliable classification of pathological finger joint thus paving the way for a further development from prototype to product stage 
standard approach to object detection focus on local patch of the image and try to classify them a background or not we propose to use the scene context image a a whole a an extra source of global information to help resolve local ambiguity we present a conditional random field for jointly solving the task of object detection and scene classification 
we prove generalization error bound for predicting entry in a partially observed matrix by approximating the observed entry with a low rank matrix to do so we bound the number of sign configuration of lowrank matrix using a result about realizable oriented matroids 
we propose a probabilistic generative account of configura l learning phenomenon in classical conditioning configural learning e xperiments probe how animal discriminate and generalize between pattern of simultaneously presented stimulus such a tone and light that are differentially predictive of reinforcement previous model of these issue have been successful more on a phenomenological than an explanatory level they reproduce experimental finding but lacking fo rmal foundation provide scant basis for understanding why animal behave a they do we present a theory that clarifies seemingly arbitrary a pects of previous model while also capturing a broader set of data key pattern of data e g concerning animal readiness to distinguish pattern with varying degree of overlap are shown to follow from statistical inference 
abstract in slow feature analysis sfa it ha been demonstrated thathigh order invariant property can be extracted by projecting inputsinto a nonlinear space and computing the slowest changingfeatures in this space this ha been proposed a a simple generalmodel for learning nonlinear invariance in the visual system however this method is highly constrained by the curse of dimensionalitywhich limit it to simple theoretical simulation this paperdemonstrates that by using a 
device mismatch in vlsi degrades the accuracy of analog arithmetic circuit and lower the learning performance of large scale neural network implemented in this technology we show compact low power on chip calibration technique that compensate for device mismatch our technique enable large scale analog vlsi neural network with learning performance on the order of bit we demonstrate our technique on a synapse linear perceptron learning with the least mean square lm algorithm and fabricated in a m cmos process 
we investigate algebraic logical and geometric property of concept recognized by various class of probabilistic classifier for this we introduce a natural hierarchy of probabilistic classifier the lowest level of which comprises the naive bayesian classifier we show that the expressivity of classifier on the different level in the hierarchy is characterized algebraically by separability with polynomial of different degree a consequence of this result is that every linearly separable concept can be recognized by a naive bayesian classifier we contrast this result with negative result about the naive bayesian classifier previously reported in the literature and point out that these result only pertain to specific learning scenario for naive bayesian classifier we also present some logical and geometric characterization of linearly separable concept thus providing additional intuitive insight into what concept are recognizable by naive bayesian classifier 
in this paper sparse representation factorization of a data matrix is first discussed an overcomplete basis matrix is estimated by using the k mean method we have proved that for the estimated overcomplete basis matrix the sparse solution coefficient matrix with minimum l norm is unique with probability of one which can be obtained using a linear programming algorithm the comparison of the l norm solution and the l norm solution are also presented which can be used in recoverability analysis of blind source separation bs next we apply the sparse matrix factorization approach to bs in the overcomplete case generally if the source are not sufficiently sparse we perform blind separation in the time frequency domain after preprocessing the observed data using the wavelet packet transformation third an eeg experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate it performance two almost independent component obtained by the sparse representation method are selected for phase synchronization analysis and their period of significant phase synchronization are found which are related to task finally concluding remark review the approach and state area that require further study 
cost sensitive learning address the issue of classification in the presence of varying cost associated with different type of misclassification in this paper we present a method for solving multi class cost sensitive learning problem using any binary classification algorithm this algorithm is derived using hree key idea iterative weighting expanding data space and gradient boosting with stochastic ensemble we establish some theoretical guarantee concerning the performance of this method in particular we show that a certain variant posse the boosting property given a form of weak learning assumption on the component binary classifier we also empirically evaluate the performance of the proposed method using benchmark data set and verify that our method generally achieves better result than representative method for cost sensitive learning in term of predictive performance cost minimization and in many case computational efficiency 
this paper is about non approximate acceleration of high dimensional nonparametric operation such a k nearest neighbor classifier and the prediction phase of support vector machine classifier we a ttempt to exploit the fact that even if we want exact answer to nonparametric query we usually do not need to explicitly find the datapoi nt close to the query but merely need to ask question about the property about that set of datapoints this offer a small amount of computational leeway and we investigate how much that leeway can be exploited for clarity this paper concentrate on pure k nn classification and the prediction phase of svms we introduce new ball tree algorithm that on real world datasets give acceleration of fold up to fold compared against highly optimized traditional ball tree based k nn these result include datasets with up to dimension and record and show non trivial speedup while giving exact answer 
an open problem in reinforcement learning is discovering hierarchical structure hexq an algorithm which automatically attempt to decompose and solve a model free factored mdp hierarchically is described by searching for aliased markov sub space region based on the state variable the algorithm us temporal and state abstraction to construct a hierarchy of interlinked smaller mdps 
many technique for complex speech processing such a denoising and deconvolution time frequency warping multiple speaker separation and multiple microphone analysis operate on sequence of short time power spectrum spectrogram a representation which is often well suited to these task however a significant problem with algorithm that manipulate spectrogram is that the output spectrogram doe not include a phase component which is needed to create a time domain signal that ha good perceptual quality here we describe a generative model of time domain speech signal and their spectrogram and show how an efficient optimizer can be used to find the maximum a posteriori speech signal given the spectrogram in contrast to technique that alternate between estimating the phase and a spectrally consistent signal our technique directly infers the speech signal thus jointly optimizing the phase and a spectrally consistent signal we compare our technique with a standard method using signal to noise ratio but we also provide audio file on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offer 
cortical synaptic plasticity depends on the relative timing of preand postsynaptic spike and also on the temporal pattern of presynaptic spike and of postsynaptic spike we study the hypothesis that cortical synaptic plasticity doe not associate individual spike but rather whole firing episode and depends only on when these episode start and how long they last but a little a possible on the timing of individual spike here we present the mathematical background for such a study standard method from hidden markov model are used to define what firing episode are estimating the probability of being in such an episode requires not only the knowledge of past spike but also of future spike we show how to construct a causal learning rule which depends only on past spike but associate preand postsynaptic firing episode a if it also knew future spike we also show that this learning rule agrees with some feature of synaptic plasticity in superficial layer of rat visual cortex froemke and dan nature 
directed graphical model with one layer of observed random variable and one or more layer of hidden random variable have been the dominant modelling paradigm in many research field although this approach ha met with considerable success the causal semantics of these model can make it difficult to infer the posterior distribution over the hidden variable in this paper we propose an alternative two layer model based on exponential family distribution and the semantics of undirected model inference in these exponential family harmonium is fast while learning is performed by minimizing contrastive divergence a member of this family is then studied a an alternative probabilistic model for latent semantic indexing in experiment it is shown that they perform well on document retrieval task and provide an elegant solution to searching with keywords 
a linear model tree is a decision tree with a linear functional model in each leaf previous model tree induction algorithm have operated on the entire training set however there are many situation when an incremental learner is advantageous in this paper we demonstrate that model tree can be induced incrementally using an algorithm that scale linearly with the number of example an incremental node splitting rule is presented together with incremental method for stopping the growth of the tree and pruning empirical testing in three domain where the emphasis is on learning a dynamic model of the environment show that the algorithm can learn a more accurate approximation from fewer example than other incremental method in addition the induced model are smaller and the learner requires le prior knowledge about the domain 
mark recapture model have for many year been used to estimate the unknown size of animal and bird population in this article we adapt a finite mixture mark recapture model in order to estimate the number of active telephone line in the usa the idea is to use the calling pattern of line that are observed on the long distance network to estimate the number of line that do not appear on the network we present a bayesian approach and use markov chain monte carlo method to obtain inference from the posterior distribution of the model parameter at the state level our result are in fairly good agreement with recent published report on line count for line that are easily classified a business or residence the estimate have low variance when the classification is unknown the variability increase considerably result are insensitive to change in the prior distribution we discus the significant computational and data mining challenge caused by the scale of the data approximately million call detail record per day observed over a number of week 
the tangential neuron in the fly brain are sensitive to the typical optic flow pattern generated during self motion in this study we examine whether a simplified linear model of these neuron can be used to estimate self motion from the optic flow we present a theory for the construction of an estimator consisting of a linear combination of optic flow vector that incorporates prior knowledge both about the distance distribution of the environment and about the noise and self motion statistic of the sensor the estimator is tested on a gantry carrying an omnidirectional vision sensor the experiment show that the proposed approach lead to accurate and robust estimate of rotation rate whereas translation estimate turn out to be le reliable a recent study ha shown that a simplified computational model of the tangential neuron a a weighted sum of flow measurement wa able to reproduce the observed response field the weight were chosen according to an optimality principle which minimizes the output variance of the model caused by noise and distance variability between different scene the question on how the output of such processing unit could be used for self motion estimation wa left open however in this paper we want to fill a part of this gap by presenting a classical linear estimation approach that extends a special case of the previous model to the complete self motion problem we again use linear combination of local flow measurement but instead of prescribing a fixed motion axis and minimizing the output variance we require that the quadratic error in the estimated self motion parameter be a small a possible from this 
the area under the roc curve auc ha been advocated a an evaluation criterion for the bipartite ranking problem we study large deviation property of the auc in particular we derive a distribution free large deviation bound for the auc which serf to bound the expected accuracy of a ranking function in term of it empirical auc on an independent test sequence a comparison of our result with a corresponding large deviation result for the classification error rate suggests that the test sample size required to obtain an accurate estimate of the expected accuracy of a ranking function with confidence is larger than that required to obtain an accurate estimate of the expected error rate of a classification function with the same confidence a simple application of the union bound allows the large deviation bound to be extended to learned ranking function chosen from finite function class 
the problem of approximating the product of several gaussian mixture distribution arises in a number of context including the nonparametric belief propagation nbp inference algorithm and the training of product of expert model this paper develops two multiscale algorithm for sampling from a product of gaussian mixture and compare their performance to existing method the first is a multiscale variant of previously proposed monte carlo technique with comparable theoretical guarantee but improved empirical convergence rate the second make use of approximate kernel density evaluation method to construct a fast approximate sampler which is guaranteed to sample point to within a tunable parameter of their true probability we compare both multiscale sampler on a set of computational example motivated by nbp demonstrating significant improvement over existing method 
reinforcement learning ha been used for training game playing agent the value function for a complex game must be approximated with a continuous function because the number of state becomes too large to enumerate temporal difference learning with self play is one method successfully used to derive the value approximation function coevolution of the value function is also claimed to yield good result this paper report on a direct comparison between an agent trained to play gin rummy using temporal difference learning and the same agent trained with co evolution coevolution produced superior result 
this paper introduces an approach for identifying predictive structure in relational data using the multiple instance framework by a predictive structure we mean a structure that can explain a given labeling of the data and can predict label of unseen data multiple instance learning ha previously only been applied to flat or propositional data and we present a modification to the framework that allows multiple instance technique to be used on relational data we present experimental result using a relational modification of the diverse density method maron maron lozano perez and of a method based on the chi squared statistic mcgovern jensen we demonstrate that multipleinstance learning can be used to identify predictive structure on both a small illustrative data set and the internet movie database we compare the classification result to a k nearest neighbor approach 
in this paper we present a family of algorithm that can simultaneously align and cluster set of multidimensional curve defined on a discrete time grid our approach us the expectation maximization em algorithm to recover both the mean curve shape for each cluster and the most likely shift offset and cluster membership for each curve we demonstrate how bayesian estimation method can improve the result for small sample size by enforcing smoothness in the cluster mean curve we evaluate the methodology on two real world data set time course gene expression data and storm trajectory data experimental result show that model that incorporate curve alignment systematically provide improvement in predictive power and within cluster variance on test data set the proposed approach provides a non parametric computationally efficient and robust methodology for clustering broad class of curve data 
we investigate how stack filter function class like weighted order statistic can be applied to classification problem this lead to a new design criterion for linear classifier when input are binary valued and weight are positive we present a rank based measure of margin that is directly optimized a a standard linear program and investigate it relationship to regularization our approach can robustly combine large number of base hypothesis and ha similar performance to other type of regularization 
dominant set are a new graph theoretic concept that ha proven to be relevant in pairwise data clustering problem such a image segmentation they generalize the notion of a maximal clique to edgeweighted graph and have intriguing non trivial connection to continuous quadratic optimization and spectral based grouping we address the problem of grouping out of sample example after the clustering process ha taken place this may serve either to drastically reduce the computational burden associated to the processing of very large data set or to efficiently deal with dynamic situation whereby data set need to be updated continually we show that the very notion of a dominant set offer a simple and efficient way of doing this numerical experiment on various grouping problem show the effectiveness of the approach 
we interpret non negative matrix factorization geometrically a the problem of finding a simplicial cone which contains a cloud of data point and which is contained in the positive orthant we show that under certain condition basically requiring that some of the data are spread across the face of the positive orthant there is a unique such simplicial cone we give example of synthetic image articulation database which obey these condition these require separated support and factorial sampling for such database there is a generative model in term of part and nmf correctly identifies the part we show that our theoretical result are predictive of the performance of published nmf code by running the published algorithm on one of our synthetic image articulation database 
abstract in this paper we show that it is possible to model sensory impressionsof consumer about beef meat this is not a straightforward task thereason is that when we are aiming to induce a function that map objectdescriptions into rating we must consider that consumer rating arejust a way to express their preference about the product presented inthe same testing session therefore we had to use a special purposesvm polynomial kernel the training data set used collect the 
like many purely data driven machine learning method support vector machine svm classifier are learned exclusively from the evidence presented in the training dataset thus a larger training dataset is required for better performance in some application there might be human knowledge available that in principle could compensate for the lack of data in this paper we propose a simple generalization of svm weighted margin svm wmsvms that permit the incorporation of prior knowledge we show that sequential minimal optimization can be used in training wmsvm we discus the issue of incorporating prior knowledge using this rather general formulation the experimental result show that the proposed method of incorporating prior knowledge is effective 
feature selection a a preprocessing step to machine learning ha been eective in reducing dimensionality removing irrelevant data increasing learning accuracy and improving comprehensibility however the recent increase of dimensionality of data pose a severe challenge to many existing feature selection method with respect to eciency and eectiveness in this work we introduce a novel concept predominant correlation and propose a fast filter method which can identify relevant feature a well a redundancy among relevant feature without pairwise correlation analysis the eciency and eectiveness of our method is demonstrated through extensive comparison with other method using real world data of high dimensionality 
this paper provides a blueprint for constructing collaborative and distributed knowledge discovery system within grid based computing environment the need for such system is driven by the quest for sharing knowledge information and computing resource within the boundary of single large distributed organisation or within complex virtual organisation vo created to tackle specific project the proposed architecture is built on top of a resource federation management layer and is composed of a set of different resource we show how this architecture will behave during a typical kdd process design and deployment how it enables the execution of complex and distributed data mining task with high performance and how it provides a community of e scientist with mean to collaborate retrieve and reuse both kdd algorithm discovery process and knowledge in a visual analytical environment 
clustering time series is a problem that ha application in a wide variety of field and ha recently attracted a large amount of research in this paper we focus on clustering data derived from autoregressive moving average arma model using k mean and k medoids algorithm with the euclidean distance between estimated model parameter we justify our choice of clustering technique and distance metric by reproducing result obtained in related research our research aim is to ass the affect of discretising data into binary sequence of above and below the median a process known a clipping on the clustering of time series it is known that the fitted ar parameter of clipped data tend asymptotically to the parameter for unclipped data we exploit this result to demonstrate that for long series the clustering accuracy when using clipped data from the class of arma model is not significantly different to that achieved with unclipped data next we show that if the data contains outlier then using clipped data produce significantly better clustering we then demonstrate that using clipped series requires much le memory and operation such a distance calculation can be much faster finally we demonstrate these advantage on three real world data set 
in the authorship verification problem we are given example of the writing of a single author and are asked to determine if given long text were or were not written by this author we present a new learning based method for adducing the depth of difference between two example set and offer evidence that this method solves the authorship verification problem with very high accuracy the underlying idea is to test the rate of degradation of the accuracy of learned model a the best feature are iteratively dropped from the learning process 
the distributional principle according to which morpheme that occur in identical context belong in some sense to the same category ha been advanced a a mean for extracting syntactic structure from corpus data we extend this principle by applying it recursively and by using mutual information for estimating category coherence the resulting model learns in an unsupervised fashion highly structured distributed representation of syntactic knowledge from corpus it also exhibit promising behavior in task usually thought to require representation anchored in a grammar such a systematicity 
co clustering is a powerful data mining technique with varied application such a text clustering microarray analysis and recommender system recently an information theoretic co clustering approach applicable to empirical joint probability distribution wa proposed in many situation co clustering of more general matrix is desired in this paper we present a substantially generalized co clustering framework wherein any bregman divergence can be used in the objective function and various conditional expectation based constraint can be considered based on the statistic that need to be preserved analysis of the co clustering problem lead to the minimum bregman information principle which generalizes the maximum entropy principle and yield an elegant meta algorithm that is guaranteed to achieve local optimality our methodology yield new algorithm and also encompasses several previously known clustering and co clustering algorithm based on alternate minimization 
we introduce a novel active learning scenario in which a user want to work with a learning algorithm to identify useful anomaly these are distinguished from the traditional statistical denition of anomaly a outlier or merely ill modeled point our distinction is that the usefulness of anomaly is categorized subjectively by the user we make two additional assumption first there exist extremely few useful anomaly to be hunted down within a massive dataset second both useful and useless anomaly may sometimes exist within tiny class of similar anomaly the challenge is thus to identify rare category record in an unlabeled noisy set with help in the form of class label from a human expert who ha a small budget of datapoints that they are prepared to categorize we propose a technique to meet this challenge which assumes a mixture model t to the data but otherwise make no assumption on the particular form of the mixture component this property promise wide applicability in real life scenario and for various statistical model we give an overview of several alternative method highlighting their strength and weakness and conclude with a detailed empirical analysis we show that our method can quickly zoom in on an anomaly set containing a few ten of point in a dataset of hundred of thousand 
motivation modern sequencing technology now permit the sequencing of entire genome leading to thousand of new gene sequence in need of detailed annotation it is too time consuming to predict the property of each protein sequence manually and to organize the result of many prediction tool by hand the prediction process must be automated so the prediction can be automatically organized but the prediction must also be transparent that is the rationale for each prediction should be easily examinable by anyone that wish to use the prediction result proteome analyst pa is a web based system for predicting the property of each protein in a proteome pa ha three interesting feature first it provides a single web based tool that allows the user to select a wide range of analytic tool and automatically apply them to each protein in a proteome in essence pa provides one stop automatic high throughput analysis second pa ha the ability to explain it prediction to user pa is based on established machine learning technique but make every prediction transparent to it user third pa allows user to easily create their own transparent custom predictor without programming availability http www c ualberta ca bioinfo pa supplementary information http www c ualberta ca bioinfo pa walkthrough http www c ualberta ca bioinfo pa experiment contact bioinfo c ualberta ca 
in this paper we introduce an efficient replanning algorithm for nondeterministic domain namely what we believe to be the first incremental heuristic minimax search algorithm we apply it to the dynamic discretization of continuous domain resulting in an efficient implementation of the parti game reinforcement learning algorithm for control in high dimensional domain 
many real world classification task involve the prediction of multiple inter dependent class label a prototypical case of this sort deal with prediction of a sequence of label for a sequence of observation such problem arise naturally in the context of annotating and segmenting observation sequence this paper generalizes gaussian process classification to predict multiple label by taking dependency between neighboring label into account our approach is motivated by the desire to retain rigorous probabilistic semantics while overcoming limitation of parametric method like conditional random field which exhibit conceptual and computational difficulty in high dimensional input space experiment on named entity recognition and pitch accent prediction task demonstrate the competitiveness of our approach 
we consider the problem of computing low rank approximation of matrix the novelty of our approach is that the low rank approximation are on a sequence of matrix unlike the problem of low rank approximation of a single matrix which wa well studied in the past the proposed algorithm in this paper doe not admit a closed form solution in general we did extensive experiment on face image data to evaluate the effectiveness of the proposed algorithm and compare the computed low rank approximation with those obtained from traditional singular value decomposition based method 
in this paper we focus on methodology of finding a classifier with a minimal cost in presence of additional performance constraint rocch analysis where accuracy and cost are intertwined in the solution space wa a revolutionary tool for two class problem we propose an alternative formulation a an optimization problem commonly used in operation research this approach extends the rocch analysis to allow for locating optimal solution while outside constraint are present similarly to the rocch analysis we combine cost and class distribution while defining the objective function rather than focusing on slope of the edge in the convex hull of the solution space however we treat cost a an objective function to be minimized over the solution space by selecting the best performing classifier s one or more vertex in the solution space the linear programming framework provides a theoretical and computational methodology for finding the vertex classifier which minimizes the objective function 
in this paper we study the problem of applying data mining to facilitate the investigation of money laundering crime mlcs we have identified a new paradigm of problem that of automatic community generation based on uni party data the data in which there is no direct or explicit link information available consequently we have proposed a new methodology for link discovery based on correlation analysis ldca we have used mlc group model generation a an exemplary application of this problem paradigm and have focused on this application to develop a specific method of automatic mlc group model generation based on timeline analysis using the ldca methodology called coral a prototype of coral method ha been implemented and preliminary testing and evaluation based on a real mlc case data are reported the contribution of this work are identification of the uni party data community generation problem paradigm proposal of a new methodology ldca to solve for problem in this paradigm formulation of the mlc group model generation problem a an example of this paradigm application of the ldca methodology in developing a specific solution coral to the mlc group model generation problem and development evaluation and testing of the coral prototype in a real mlc case data 
we present a novel bayesian approach to the problem of value function estimation in continuous state space we deflne a probabilistic generative model for the value function by imposing a gaussian prior over value function and assuming a gaussian noise model due to the gaussian nature of the random process involved the posterior distribution of the value function is also gaussian and is therefore described entirely by it mean and covariance we derive exact expression for the posterior process moment and utilizing an e cient sequential sparsiflcation method we describe an on line algorithm for learning them we demonstrate the operation of the algorithm on a dimensional continuous spatial navigation domain 
abstract this paper present a kernel method that allows to combine colorand shape information for appearance based object recognition itdoesn t require to dene a new common representation but use thepower of kernel to combine dierent representation together in aneective manner these result are achieved using result of statisticalmechanics of spin glass combined with markov randomeldsvia kernel function experiment show an increase in recognitionrate up to with 
the design of cooperative multi robot system is a highly active research area in robotics two line of research in particular have generated interest the solution of large weakly coupled mdps and the design and implementation of market architecture we propose a new algorithm which join together these two line of research for a class of coupled mdps our algorithm automatically design a market architecture which cause a decentralized multi robot system to converge to a consistent policy we can show that this policy is the same a the one which would be produced by a particular centralized planning algorithm we demonstrate the new algorithm on three simulation example multi robot towing multi robot path planning with a limited fuel resource and coordinating behavior in a game of paint ball 
this paper describes an efficient method for learning the parameter of a gaussian process gp the parameter are learned from multiple task which are assumed to have been drawn independently from the same gp prior an efficient algorithm is obtained by extending the informative vector machine ivm algorithm to handle the multi task learning case the multi task ivm mtivm save computation by greedily selecting the most informative example from the separate task the mt ivm is also shown to be more efficient than random sub sampling on an artificial data set and more effective than the traditional ivm in a speaker dependent phoneme recognition task 
finding effective method for developing an ensemble of model ha been an active research area of large scale data mining in recent year model learned from data are often subject to some degree of uncertainty for a variety of resoans in classification ensemble of model provide a useful mean of averaging out error introduced by individual classifier hence reducing the generalization error of prediction the plurality voting method is often chosen for bagging because of it simplicity of implementation however the plurality approach to model reconciliation is ad hoc there are many other voting method to choose from including the anti plurality method the plurality method with elimination the borda count method and condorcet s method of pairwise comparison any of these could lead to a better method for reconciliation in this paper we analyze the use of these voting method in model reconciliation we present empirical result comparing performance of these voting method when applied in bagging these result include some surprise and among other thing suggest that plurality is not always the best voting method the number of class can affect the performance of voting method and the degree of dataset noise can affect the performance of voting method while it is premature to make final judgment about specific voting method the result of this work raise interesting question and they open the door to the application of voting theory in classification theory 
we present a generative model and stochastic filtering algor ithm for si multaneous tracking of d position and orientation non rigid motion object texture and background texture using a single camera we show that the solution to this problem is formally equivalent to stochastic fil tering of conditionally gaussian process a problem for which well known approach exist we propose an approach based on monte carlo sampling of the nonlinear component of the process object mo tion and exact filtering of the object and background textur e given the sampled motion the smoothness of image sequence in time and space is exploited by using laplace s method to generate proposal distribution for importance sampling the resulting inference algorithm encom pass both optic flow and template based tracking a specia l case and elucidates the condition under which these method are optimal we demonstrate an application of the system to d non rigid face tracking 
convex programming involves a convex set f rn and a convex cost function c f r the goal of convex programming is to nd a point in f which minimizes c in online convex programming the convex set is known in advance but in each step of some repeated optimization problem one must select a point inf before seeing the cost function for that step this can be used to model factory production farm production and many other industrial optimization problem where one is unaware of the value of the item produced until they have already been constructed we introduce an algorithm for this domain we also apply this algorithm to repeated game and show that it is really a generalization of innitesimal gradient ascent and the result here imply that generalized innitesimal gradient ascent giga is universally consistent 
abstract the goal of low level vision is to estimate an underlying scene given an observed image real world scene eg albedo or shape can be very complex conventionally requiring high dimensional representation which are hard to estimate and store we propose a low dimensional rep resentation called a scene recipe that relies on the image itself to de scribe the complex scene configuration shape recipe are an example these are the regression coefficient that predict the bandpassed shape from image data we describe the benefit of this representation and show two us illustrating their property we improve stereo shape estimate by learning shape recipe at low resolution and applying them at full resolution shape recipe implicitly contain information about lighting and material and we use them for material segmentation 
this paper address the problem of untangling hidden graph from a set of noisy detection of undirected edge we present a model of the generation of the observed graph that includes degree based structure prior on the hidden graph exact inference in the model is intractable we present an efficien t approximate inference algorithm to compute edge appearance posterior we evaluate our model and algorithm on a biological graph inference problem 
in the wrapper approach for feature selection a popular criterion used is the leave one out estimate of the classification error while being relatively unbiased the leave one out error estimate is nonetheless known to exhibit a large variance which can be detrimental especially for small sample we propose reducing it variance i e smoothing at two level at the first level we smooth the error count using estimate of posterior probability while at the second level we smooth the posterior probability estimate themselves using bayesian estimation with conjugate prior furthermore we propose using the jackknife to reduce the bias inherent in bayesian estimator we then show empirically that smoothing the error estimate give improved performance in feature selection 
we describe semi markov conditional random field semi cr f a conditionally trained version of semi markov chain intuitively a semicrf on an input sequence x output a segmentation of x in which label are assigned to segment i e subsequence of x rather than to individual element xi of x importantly feature for semi crfs can measure property of segment and transition within a segment can be non markovian in spite of this additional power exact learning and inference algorithm for semi crfs are polynomial time often only a small constant factor slower than conventional crfs in experiment on five named entity recognition problem semi crfs genus lly outperform conventional crfs 
transformation of both the response variable and the predictor is commonly used in fitting regression model however these transformation method do not always provide the maximum linear correlation between the response variable and the predictor especially when there are non linear relationship between predictor and the response such a the medical data set used in this study a spline based transformation method is proposed that is second order smooth continuous and minimizes the mean squared error between the response and each predictor since the computation time for generating this spline is o n the processing time is reasonable with massive data set in contrast to cubic smoothing spline the resulting transformation equation also display a high level of efficiency for scoring data used for predicting health outcome contains an abundance of non linear relationship between predictor and the outcome requiring an algorithm for modeling them accurately thus a transformation that fit an adaptive cubic spline to each of a set of variable is proposed these curve are used a a set of transformation function on the predictor a case study of how the transformed variable can be fed into a simple linear regression model to predict risk outcome is presented the result show significant improvement over the performance of the original variable in both linear and non linear model 
principal component analysis pca is a widely used statistical technique for unsupervised dimension reduction k mean clustering is a commonly used data clustering for performing unsupervised learning task here we prove that principal component are the continuous solution to the discrete cluster membership indicator for k mean clustering new lower bound for k mean objective function are derived which is the total variance minus the eigenvalue of the data covariance matrix these result indicate that unsupervised dimension reduction is closely related to unsupervised learning several implication are discussed on dimension reduction the result provides new insight to the observed effectiveness of pca based data reduction beyond the conventional noise reduction explanation that pca via singular value decomposition provides the best low dimensional linear approximation of the data on learning the result suggests effective technique for k mean data clustering dna gene expression and internet newsgroups are analyzed to illustrate our result experiment indicate that the new bound are within of the optimal value 
we introduce a class of nonstationary covariance function for gaussian process gp regression nonstationary covariance function allow the model to adapt to function whose smoothness varies with the input the class includes a nonstationary version of the mat rn stationary covariance in which the differentiability of the regression function is controlled by a parameter freeing one from fixing the differentiability in advance in experiment the nonstationary gp regression model performs well when the input space is two or three dimension outperforming a neural network model and bayesian free knot spline model but is outperformed in one dimension by a state of the art bayesian free knot spline model the model readily generalizes to non gaussian data use of computational method for speeding gp fitting may allow for implementation of the method on larger datasets 
a commercial web page typically contains many information block apart from the main content block it usually ha such block a navigation panel copyright and privacy notice and advertisement for business purpose and for easy user access we call these block that are not the main content block of the page the noisy block we show that the information contained in these noisy block can seriously harm web data mining eliminating these noise is thus of great importance in this paper we propose a noise elimination technique based on the following observation in a given web site noisy block usually share some common content and presentation style while the main content block of the page are often diverse in their actual content and or presentation style based on this observation we propose a tree structure called style tree to capture the common presentation style and the actual content of the page in a given web site by sampling the page of the site a style tree can be built for the site which we call the site style tree sst we then introduce an information based measure to determine which part of the sst represent noise and which part represent the main content of the site the sst is employed to detect and eliminate noise in any web page of the site by mapping this page to the sst the proposed technique is evaluated with two data mining task web page clustering and classification experimental result show that our noise elimination technique is able to improve the mining result significantly 
this paper proposes neural mechanism of transcranial magnetic stimulation tm tm can stimulate the brain non invasively through a brief magnetic pulse delivered by a coil placed on the scalp interfering with specific cortical function with a high temporal resolution due to these advantage tm ha been a popular experimental tool in various neuroscience field however the neural mechanism underlying tmsinduced interference are still unknown a theoretical basis for tm ha not been developed this paper provides computational evidence that inhibitory interaction in a neural population not an isolated single neuron play a critical role in yielding the neural interference induced by tm 
in this paper we extend previous result providing a theoretical analysis of a new monte carlo ensemble classifier the framework allows u to characterize the condition under which the ensemble approach can be expected to outperform the single hypothesis classifier moreover we provide a closed form expression for the distribution of the true ensemble accuracy a well a of it mean and variance we then exploit this result in order to analyze the expected error behavior in a particularly interesting case 
support vector machine and other kernel machine offer robust modern machine learning method for nonlinear classification however relative to other alternative such a linear method decision tree and neural network they can be order of magnitude slower at query time unlike existing method that attempt to speedup querytime such a reduced set compression e g burges and anytime bounding e g decoste we propose a new and efficient approach based on treating the kernel machine classifier a a special form of k nearest neighbor our approach improves upon a traditional k nn by determining at query time a good k for each query based on pre query analysis guided by the original robust kernel machine we demonstrate effectiveness on high dimensional benchmark mnist data observing a greater than fold reduction in the number of svs required per query amortized over all pairwise mnist digit classifier with no extra test error in fact it happens to make fewer 
principal component analysis pca is a widely used statistical technique for unsupervised dimension reduction k mean clustering is a commonly used data clustering for performing unsupervised learning task here we prove that principal component are the continuous solution to the discrete cluster membership indicator for k mean clustering new lower bound for k mean objective function are derived which is the total variance minus the eigenvalue of the data covariance matrix these result indicate that unsupervised dimension reduction is closely related to unsupervised learning several implication are discussed on dimension reduction the result provides new insight to the observed effectiveness of pca based data reduction beyond the conventional noise reduction explanation that pca via singular value decomposition provides the best low dimensional linear approximation of the data on learning the result suggests effective technique for k mean data clustering dna gene expression and internet newsgroups are analyzed to illustrate our result experiment indicate that the new bound are within of the optimal value 
a novel feature selection algorithm is presented based on the global minimization of a data dependent generalization error bound feature selection and scaling algorithm often lead to non convex optimization problem which in many previous approach were addressed through gradient descent procedure that can only guarantee convergence to a local minimum we propose an alternative approach whereby the global solution of the non convex optimization problem is derived via an equivalent optimization problem moreover the convex optimization task is reduced to a conic quadratic programming problem for which efficient solver are available highly competitive numerical result on both artificial and real world data set are reported 
we consider the situation in semi supervised learning where the label sampling mechanism stochastically depends on the true response a well a potentially on the feature we suggest a method of moment for estimating this stochastic dependence using the unlabeled data this is potentially useful for two distinct purpose a a an input to a supervised learning procedure which can be used to de bias it result using labeled data only and b a a potentially interesting learning task in itself we present several example to illustrate the practical usefulness of our method 
most prevalent technique in support vector machine svm feature selection are based on the intuition that the weight of feature that are close to zero are not required for optimal classification in this paper we show that indeed in the sample limit the irrelevant variable in a theoretical and optimal sense will be given zero weight by a linear svm both in the soft and the hard margin case however svm based method have certain theoretical disadvantage too we present example where the linear svm may assign zero weight to strongly relevant variable i e variable required for optimal estimation of the distribution of the target variable and where weakly relevant feature i e feature that are superfluous for optimal feature selection given other feature may get non zero weight we contrast and theoretically compare with markov blanket based feature selection algorithm that do not have such disadvantage in a broad class of distribution and could also be used for causal discovery 
many nlp task rely on accurately estimating word dependency probability p where the word w and w have a particular relationship such a verb object because of the sparseness of count of such dependency smoothing and the ability to use multiple source of knowledge are important challenge for example if the probability p n v of noun n being the subject of verb v is high and v take similar object to v and v is synonymous to v then we want to conclude that p n v should also be reasonably high even when those word did not cooccur in the training data to capture these higher order relationship we propose a markov chain model whose stationary distribution is used to give word probability estimate unlike the manually defined random walk used in some link analysis algorithm we show how to automatically learn a rich set of parameter for the markov chain s transition probability we apply this model to the task of prepositional phrase attachment obtaining an accuracy of 
we present the software architecture of a robotic system for mapping abandoned mine the software is capable of acquiring consistent d map of large mine with many cycle represented a markov random eld d c space map are acquired from local d range scan which are used to identify navigable path using a search our system ha been deployed in three abandoned mine two of which inaccessible to people where it ha acquired map of unprecedented detail and accuracy 
recent work ha examined the estimation of model of stimulus driven neural activity in which some linear filtering process is followed by a nonlinear probabilistic spiking mechanism we analyze the estimation of one such model for which this nonlinear step is implemented by a noisy leaky integrate and fire mechanism specifically we formulate the problem in term of maximum likelihood estimation and show that the computational problem of optimizing this cost function is tractable our main contribution is an algorithm and a proof that this algorithm is guaranteed to find the global optimum we demonstrate the effectiveness of our estimator with numerical simulation 
in many application domain there is a large amount of unlabeled data but only a very limited amount of labeled training data one general approach that ha been explored for utilizing this unlabeled data is to construct a graph on all the data point based on distance relationship among example and then to use the known label to perform some type of graph partitioning one natural partitioning to use is the minimum cut that agrees with the labeled data blum chawla which can be thought of a giving the most probable label assignment if one view label a generated according to a markov random field on the graph zhu et al propose a cut based on a relaxation of this field and joachim give an algorithm based on finding an approximate min ratio cut in this paper we extend the mincut approach by adding randomness to the graph structure the resulting algorithm address several short coming of the basic mincut approach and can be given theoretical justification from both a markov random field perspective and from sample complexity consideration in case where the graph doe not have small cut for a given classification problem randomization may not help however our experiment on several datasets show that when the structure of the graph support small cut this can result in highly accurate classifier with good accuracy coverage tradeoff in addition we are able to achieve good performance with a very simple graph construction procedure 
we present a probabilistic approach to learning a gaussian process classier in the presence of unlabeled data our approach involves a null category noise model ncnm inspired by ordered categorical noise model the noise model reects an assumption that the data density is lower between the class conditional density we illustrate our approach on a toy problem and present comparative result for the semi supervised classication of handwritten digit 
abstract given a directed graph in which some of the node are labeled we investigate the question of how to exploit the link structure of the graph to infer the label of the remaining unlabeled node to that extent we propose a regularization framework for function dened over node of a directed graph that force the classication function to change slowly on densely linked sub graph a powerful yet computationally simple classication algorithm is derived within the proposed framework the experimental evaluation on real world web classication problem demonstrates encouraging result that validate our approach 
this paper analyzes the performance of semisupervised learning of mixture model we show that unlabeled data can lead to an increase in classification error even in situation where additional labeled data would decrease classification error we present a mathematical analysis of this degradation phenomenon and show that it is due to the fact that bias may be adversely affected by unlabeled data we discus the impact of these theoretical result to practical situation 
moment before the launch of every space vehicle engineering discipline specialist must make a critical go no go decision the cost of a false positive allowing a launch in spite of a fault or a false negative stopping a potentially successful launch can be measured in the ten of million of dollar not including the cost in morale and other more intangible detriment the aerospace corporation is responsible for providing engineering assessment critical to the go no go decision for every department of defense space vehicle these assessment are made by constantly monitoring streaming telemetry data in the hour before launch we will introduce viztree a novel time series visualization tool to aid the aerospace analyst who must make these engineering assessment viztree wa developed at the university of california riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry the use of a single tool for both aspect of the task allows a natural and intuitive transfer of mined knowledge to the monitoring task our visualization approach work by transforming the time series into a symbolic representation and encoding the data in a modified suffix tree in which the frequency and other property of pattern are mapped onto color and other visual property we demonstrate the utility of our system by comparing it with state of the art batch algorithm on several real and synthetic datasets 
we report and compare the performance of different learning algorithm based on data from cortical recording the task is to predict the orientation of visual stimulus from the activity of a population of simultaneously recorded neuron we compare several way of improving the coding of the input i e the spike data a well a of the output i e the orientation and report the result obtained using different kernel algorithm 
the bradley terry model for paired comparison ha been popular in many area we propose a generalized version in which paired individual comparison are extended to paired team comparison we introduce a simple algorithm with convergence proof to solve the model and obtain individual skill a useful application to multi class prob ability estimate using error correcting code is demonstrated 
we derive multiplicative update for solving the nonnegative quadratic programming problem in support vector machine svms the update have a simple closed form and we prove that they converge monotonically to the solution of the maximum margin hyperplane the update optimize the traditionally proposed objective function for svms they do not involve any heuristic such a choosing a learning rate or deciding which variable to update at each iteration they can be used to adjust all the quadratic programming variable in parallel with a guarantee of improvement at each iteration we analyze the asymptotic convergence of the update and show that the coefficient of non support vector decay geometrically to zero at a rate that depends on their margin in practice the update converge very rapidly to good classifier 
when the goal is to achieve the best correct classification rate cross entropy and mean squared error are typical cost function used to optimize classifier performance however for many real world classification problem the roc curve is a more meaningful performance measure we demonstrate that minimizing cross entropy or mean squared error doe not necessarily maximize the area under the roc curve auc we then consider alternative objective function for training a classifier to maximize the auc directly we propose an objective function that is an approximation to the wilcoxon mann whitney statistic which is equivalent to the auc the proposed objective function is dierentiable so gradient based method can be used to train the classifier we apply the new objective function to real world customer behavior prediction problem for a wireless service provider and a cable service provider and achieve reliable improvement in the roc curve 
we explore the application of machine learningtechniques to the problem of contentbasedimage retrieval cbir unlike mostexisting cbir system in which only globalinformation is used or in which a user mustexplicitly indicate what part of the image isof interest we apply the multiple instance mi learning model to use a small numberof training image to learn what image fromthe database are of interest to the user 
important information when mining temporal sequence knowledge discovery technique can be applied that discover interesting pattern of interaction existing approach use frequency and sometimes length a measurement for interestingness because these are temporal sequence additional characteristic such a periodicity may also be interesting we propose that information theoretic principle can be used to evaluate interesting characteristic of time ordered input sequence in this paper we present a novel data mining technique based on the minimum description length principle that discovers interesting feature in a time ordered sequence we discus feature of our real time mining approach show application of the knowledge mined by the approach and present a technique to bootstrap a decision maker from the mined pattern 
this paper proposes a data mining approach to modeling relationship among category in image collection in our approach with image feature grouping a visual dictionary is created for color texture and shape feature attribute respectively labeling each training image with the keywords in the visual dictionary a classification tree is built based on the statistical property of the feature space we define a structure called semantics graph to discover the hidden semantic relationship among the semantic category embodied in the image collection with the semantics graph each semantic category is modeled a a unique fuzzy set to explicitly address the semantic uncertainty and semantic overlap among the category in the feature space the model is utilized in the semantics intensive image retrieval application an algorithm using the classification accuracy measure is developed to combine the built classification tree with the fuzzy set modeling method to deliver semantically relevant image retrieval for a given query image the experimental evaluation have demonstrated that the proposed approach model the semantic relationship effectively and the image retrieval prototype system utilizing the derived model is promising both in effectiveness and efficiency 
p nbsp p div what make a neural microcircuit computationally powerful or more precisely which measurable quantity could explain why one microcircuit img width height border align bottom src http www igi tugraz at abstract maassetal img png alt c is better suited for a particular family of computational task than another microcircuit img width height border align bottom src http www igi tugraz at abstract maassetal img png alt c textquoteright we propose in this article quantitative measure for evaluating the computational power and generalization capability of a neural microcircuit and apply them to generic neural microcircuit model drawn from different distribution we validate the proposed measure by comparing their prediction with direct evaluation of the computational performance of these microcircuit model this procedure is applied first to microcircuit model that differ with regard to the spatial range of synaptic connection and with regard to the scale of synaptic efficacy in the circuit and then to microcircuit model that differ with regard to the level of background input current and the level of noise on the membrane potential of neuron in this case the proposed method allows u to quantify difference in the computational power and generalization capability of circuit in different dynamic regime upand down state that have been demonstrated through intracellular recording in vivo div p nbsp p 
