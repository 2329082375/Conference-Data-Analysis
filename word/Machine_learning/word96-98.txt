monotonicity is a constraint which arises in many application domain we present a machine learning model the monotonic network for which monotonicity can be enforced exactly i e by virtueof functional form a straightforward method for implementingandtraining a monotonic network is described monotonic networksare proven to be universal approximators of continuous differentiablemonotonic function we apply monotonic network to areal world task in corporate bond rating prediction 
the nearest neighbor algorithm and it derivative are often quite successful at learning a concept from a training set and providing good generalization on subsequent input vector however these technique often retain the entire training set in memory resulting in large memory requirement and slow execution speed a well a a sensitivity to noise this paper provides a discussion of issue related to reducing the number of instance retained in memory while maintaining and sometimes improving generalization accuracy and mention algorithm other researcher have used to address this problem it present three intuitive noise tolerant algorithm that can be used to prune instance from the training set in experiment on application the algorithm that achieves the highest reduction in storage also result in the highest generalization accuracy of the three method 
prioritized sweeping is a model based reinforcement learning method that attempt to focus the agent s limited computational resource to achieve a good estimate of the v alue of environment state the classic account of prioritized sweeping us an explicit state b ased representation of the value reward and model parameter such a representation is unwieldy for dealing with complex environment and there is growing interest in learning with more compact representation we claim that classic prioritized sweeping is ill suited for learning wit h such representation to overcome this deficiency we introducegeneralized prioritized sweeping a principled method for generating representation specific algorithm for model based reinforcement learning we then apply this method for several representation including state based mod el and generalized model approximators such a bayesian network we describe preliminary experiment that compare our approach with classical prioritized sweeping 
optimal brain damage obd is a method for reducing the number of weight in a neural network obd estimate the increase in cost function if weight are pruned and is a valid approximation if the learning algorithm ha converged into a local minimum on the other hand it is often desirable to terminate the learning process before a local minimum is reached early stopping in this paper we show that obd estimate the increase in cost function incorrectly if the network is not in a local minimum we also show how obd can be extended such that it can be used in connection with early stopping we call this new approach early brain damage ebd ebd also allows to revive already pruned weight we demonstrate the improvement achieved by ebd using three publicly available data set 
a new method to calculate the full training process of a neural networkis introduced no sophisticated method like the replica trickare used the result are directly related to the actual number oftraining step some result are presented here like the maximallearning rate an exact description of early stopping and the necessarynumber of training step further problem can be addressedwith this approach introductiontraining guided by empirical risk minimization doe not 
in this paper the technique of stacking previously only used for supervisedlearning is applied to unsupervised learning specifically it is usedfor non parametric multivariate density estimation to combine finite mixturemodel and kernel density estimator experimental result on bothsimulated data and real world data set clearly demonstrate that stackeddensity estimation outperforms other strategy such a choosing the singlebest model based on cross validation combining with 
the boosting algorithm adaboost developedby freund and schapire ha exhibitedoutstanding performance on severalbenchmark problem when using c a the quot weak quot algorithm to be quot boosted quot like other ensemble learning approach adaboost construct a composite hypothesisby voting many individual hypothesis in practice the large amount ofmemory required to store these hypothesescan make ensemble method hard to deployin application this paper show that byselecting a 
dnf system divide the input space into decision surface defined by conjunction of condition based on attribute value pair this mean that the decision surface are orthogonal t o the axis of the tested attribute and p arallel t o all t he other ax in this paper we present t he system ltree that i s able to define decision surface both orthogonal and oblique to the ax defined by the attribute of the input space this is done by combining a decision tree with a linear discriminant by mean of constructive induction at each decision node ltree defines a new instance space by the insertion of new attribute those are projection of the instance that fall at this node over the hyper plane given by a linear discriminant function this new instance space propagates downwards through the tree test based on those new attribute are oblique in the input space we have c arried out experiment on sixteen benchmark d atasets and compared our system with other well known decision tree system oblique and orthogonal like c oc and lmdt from t hese experiment we claim that our system ha advantage concerning accuracy and tree size at statistically significant confidence level 
if globally high dimensional data ha locally only low dimensional distribution it is advantageous to perform a local dimensionality reduction before further processing the data in this paper we examine several technique for local dimensionality reduction in the context of locally weighted linear r e 
many classification algorithm are quot passive quot in that they assign a class label to each instancebased only on the description given even if that description is incomplete in contrast an active classifier can at some cost obtain the value of missing attribute before deciding upon a class label the expectedutility of using an active classifier dependson both the cost required to obtain theadditional attribute value and the penaltyincurred if it output the wrong 
the hierarchical representation of data ha various application in domain such a data mining machine vision or information retrieval i n this paper we introduce an extension of the expectation maximization em algorithm that learns mixture hierarchy in a computationally efficient manner efficiency is achieved by progressing in a bottom up fashion i e by clustering the mixture component of a given level in the hierarchy to obtain those of the level above this clustering requires on ly knowledge of the mixture parameter there being no need to resort to intermediate sample in addition to practical application the algorith m allows a new interpretation of em that make clear the relationship with non parametric kernel based estimation method provides explicit control over the trade off between the bias and variance of em estimate and offer new insight about the behavior of deterministic annealing met hod commonly used with em to escape local minimum of the likelihood 
we present an empirical analysis of symbolic prototype learnersfor synthetic and real domain the prototype are learned by modifyingthe minimum distance classifier to solve problem with symbolicattributes attribute weighting and it inability to learn multiple prototypesfor a class these extension are implemented in snmc in thesecond half of this paper we provide empirical analysis characterizingsituations where symbolic prototype have advantage over traditionalmethods such 
by now it is widely accepted that learning a task from scratch i e without any prior knowledge is a daunting undertaking human however rarely attempt to learn from scratch they extract initial bias a well a strategy how to approach a learning problem from instruction and or demonstration of other human for learning control this paper investigates how learning from demonstration can be applied in the context of reinforcement learning we consider priming the q function the value function the policy and the model of the task dynamic a possible area where demonstration can speed up learning in general nonlinear learning problem only model based reinforcement learning show significant speed up after a demonstration while in the special case of linear quadratic regulator lqr problem all method profit from the demonstration in an implementation of pole balancing on a complex anthropomorphic robot arm we demonstrate that when facing the complexity of real signal processing modelbased reinforcement learning offer the most robustness for lqr problem using the suggested method the robot learns pole balancing in just a single trial after a second long demonstration of the human instructor 
cluster analysis is a fundamental principle in exploratory data analysis providing the user with a description of the group structure of given data a key problem in this context is the interpretation and visualization of clustering solution in high dimensional or abstract data space in particular probabilistic description of the group structure essential to capture inter cluster relationship are hardly assessable by simple inspection of the probabilistic assignment variable we 
in this paper we present a method for discovering approximately common motif also known a ective motif in three dimensional d molecule each node in a molecule is represented by a d point in the euclidean space and each edge is represented by an undirected line segment connecting two node in the molecule motif are rigid substructure which may occur in a molecule after allowing for an arbitrary number of rotation and translation a well a a small number specified by the user of node insert delete operation in the motif or the molecule we call this lapproximate occurrence the proposed method combine the geometric hashing technique and block detection algorithm for undirected graph to demonstrate the utility of our algorithm we discus their application to classifying three family of molecule pertaining to antibacterial sulfa drug anti anxiety agent benzodiazepine and sntiadrenergic agent receptor experimental result indicate the good performance of our algorithm and the high quality of the discovered motif 
in integrated service communication network an important problem is to exercise call admission control and routing so a to optim ally use the network resource this problem is naturally formulated a a dynamic programming problem which however is too complex to be solved exactly we use method of reinforcement learning rl together with a decomposition approach to find call admission control and r outing policy the performance of our policy for a network with approximately different feature configuration is compared with a commonl y used heuristic policy 
current method to avoid overfitting are eitherdata oriented using separate data forvalidation or representation oriented penalizingcomplexity in the model this paperproposes process oriented evaluation wherea model s expected generalization error iscomputed a a function of the search processthat led to it the paper developsthe necessary theoretical framework and appliesit to one type of learning rule induction a process oriented version of the cn rule learner is 
the identification of relevant attribute is an important and difficult task in data mining application where induction is used a the primary tool for knowledge extraction this paper introduces a new rule induction algorithm ritio which eliminates attribute iu order of decreasing irrelevancy the rule produced by ritio are shown to be largely based on only the most relevant attribute experimental result with and without feature selection preprocessing confirm that ritio achieves high level of predictive accuracy 
on large problem reinforcement learning system must use parameterized function approximators such a neural network in order to generalize between similar situation and action in these case there are no strong theoretical result on the accuracy of convergence and computational result have been mixed in particular boyan and moore reported at last year s meeting a series of negative result in attempting to apply dynamic programming together with function approximation to simple control problem with continuous state space in this paper we present positive result for all the control task they attempted and for one that is significantly larger the most important dierences are that we used sparse coarse coded function approximators cmacs whereas they used mostly global function approximators and that we learned online whereas they learned oine boyan and moore and others have suggested that the problem they encountered could be solved by using actual outcome rollouts a in classical monte carlo method and a in the td algorithm when however in our experiment this always resulted in substantially poorer performance we conclude that reinforcement learning can work robustly in conjunction with function approximators and that there is little justification at present for avoiding the case of general 
this paper present experiment with datasets and decision tree pruning algorithmsthat show that increasing trainingset size often result in a linear increase intree size even when that additional complexityresults in no significant increase inclassification accuracy said differently removingrandomly selected training instancesoften result in tree that are substantiallysmaller and just a accurate a those builton all available training instance this impliesthat decrease 
the observed distribution of natural image is far from uniform on the contrary real image have complex and important structure that can be exploited for image processing recognition and analysis there have been many proposed approach to the principled statistical modeling of image but each ha been limited in either the complexity of the model or the complexity of the image we present a non parametric multi scale statistical model for image that can be used for recognition image de noising and in a generative mode to synthesize high quality texture 
this paper defines a formal approach to learning from example described by labelled graph we propose a formal model based upon lattice theory and in particular with the use of galois lattice we enlarge the domain of formal concept analysis by the use of the galois lattice model with structural description of example and concept our implementation called graal for graph and learning construct a galois lattice for any description language provided that the two operation of comparison and generalization are determined for that language we prove that these operation exist in the case of labelled graph 
the ability to rely on similarity metric invariant to image transform ations is an important issue for image classification task such a face or character recognition we analyze an invariant metric that ha performed well for the latter the tangent distance and study it limitation when applied to regular image showing that the most significant among these convergence to local minimum can be drastically reduced by computing the distance in a multiresolution setting this lead to the multiresolution tangent distance which exhibit significantly higher invariance to image transformation and can be easily combined with robust estimation procedure 
we have developed a computational theory of rodent navigation that includes analog of the place cell system the head direction system and path integration in this paper we present simulation result showing how interaction between the place and head direction system can account for recent observation about hippocampal place cell response to doubling and or rotation of cue card in a cylindrical arena sharp et al 
this paper describes a new approach to automatically learning linguistic knowledge for spelling correction a major feature of this approach is the fact that the acquired knowledge is captured in a small set of easily understood rule a opposed to a large set of opaque feature and weight a perspicuous representation is advantageous in order to best exploit human intuition to understand and improve upon the acquired knowledge of the system 
the goal of active template research is to create a single unified environment that a data analyst can use to carry out a knowledge discovery project and to deliver the resulting solution in the form of an active template an active template is a hyper linked information structure that tightly integrates action executable program and command result model datasets prediction report and documentation explanation of decision action and result the use of active template provides a number of benefit including user guidance improved documentation of action and result and increased reuse of previous work 
practical approach to clustering use an iterative procedure e g k mean em which converges to one of numerous local minimum it is known that these iterative technique are especially sensitive to initial starting condition we present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the mode of a distribution the refined initial starting condition allows the iterative algorithm to converge to a better local minimum the procedure is applicable to a wide class of clustering algorithm for both discrete and continuous data we demonstrate the application of this method to the popular k mean clustering algorithm and show that refined initial starting point indeed lead to improved solution refinement run time is considerably lower than the time required to cluster the full database the method is scalable and can be coupled with a scalable clustering algorithm to address the large scale clustering problem in data mining 
scalability determines the potential in distributing 
we consider neural network model for stochastic nonlinear dynamical system where measurement of the variable of interest are only avail able at irregular interval i e most realization are missing dif culties arise since the solution for prediction and maximum likelihood learn ing with missing data lead to complex integral which even for simple case cannot be solved analytically in this paper we propose a spe ci c combination of a nonlinear recurrent neural predictive model and a linear error model which lead to tractable prediction and maximum likelihood adaptation rule in particular the recurrent neural network can be trained using the real time recurrent learning rule and the linear error model can be trained by an em adaptation rule implemented u ing forward backward kalman lter equation the model is applied to predict the glucose insulin metabolism of a diabetic patient where blood glucose measurement are only available a few time a day at irregular interval the new model show considerable improvement with respect to both recurrent neural network trained with teacher forcing or in a free running mode and various linear model 
a new approach for clustering is proposed this method is basedon an analogy to a physical model the ferromagnetic potts modelat thermal equilibrium is used a an analog computer for this hardoptimization problem we do not assume any structure of the underlyingdistribution of the data phase space of the potts model isdivided into three region ferromagnetic super paramagnetic andparamagnetic phase the region of interest is that correspondingto the super paramagnetic one where 
a linear architectural model of cortical simple cell is presented the model evidence how mutual inhibition occurring throughsynaptic coupling function asymmetrically distributed in space can be a possible basis for a wide variety of spatio temporal simplecell response property including direction selectivity and velocitytuning while spatial asymmetry are included explicitly in thestructure of the inhibitory interconnection temporal asymmetriesoriginate from the 
to monitor or control a stochastic dynamic system we need to reason about it current state exact inference for this task requires that we maintain a complete joint probability distribution over the possible state an impossible requirement for most process stochastic simulation algorithm provide an alternative solution by approximating the distribution at time via a relatively small set of sample the time sample are used a the basis for generating the sample at time however since only existing sample are used a the basis for the next sampling phase new part of the space are never explored we propose an approach whereby we try to generalize from the time sample to unsampled region of the state space thus these sample are used a data for learning a distribution over the state at time which is then used to generate the time sample we examine different representation for a distribution including density tree bayesian network and tree structured bayesian network and evaluate their appropriateness to the task the machine learning perspective allows u to examine issue such a the tradeoff of using more complex model and to utilize important technique such a regularization and prior we validate the performance of our algorithm on both artificial and real domain and show significant improvement in accuracy over the existing approach 
in this paper we propose recurrent neural network with feedback into the inputunits for handling two type of data analysis problem on the one hand thisscheme can be used for static data when some of the input variable are missing on the other hand it can also be used for sequential data when some of theinput variable are missing or are available at different frequency unlike in thecase of probabilistic model e g gaussian of the missing variable the networkdoes not 
we evaluate inductive logic programming ilp method for predicting fault density inc class in this problem each trainingexample is a c class definition representedas a calling tree and labeled a quot positive quot iff fault i e error were discoveredin it implementation we compare two ilpsystems foil and flipper and explorethe reason for their differing performance using both natural and artificial data wethen propose two extension to flipper a 
a simple linear averaging of the output of several network ase g in bagging seems to follow naturally from a bias variancedecomposition of the sum squared error the sum squared error ofthe average model is a quadratic function of the weighting factorsassigned to the network in the ensemble suggesting a quadraticprogramming algorithm for finding the quot optimal quot weighting factor if we interpret the output of a network a a probability statement the sum squared error 
monotonicity is a constraint which arises in many application domain we present a machine learning model the monotonic network for which monotonicity can be enforced exactly i e by virtueof functional form a straightforward method for implementingandtraining a monotonic network is described monotonic networksare proven to be universal approximators of continuous differentiablemonotonic function we apply monotonic network to areal world task in corporate bond rating prediction 
the nearest neighbor algorithm and it derivative are often quite successful at learning a concept from a training set and providing good generalization on subsequent input vector however these technique often retain the entire training set in memory resulting in large memory requirement and slow execution speed a well a a sensitivity to noise this paper provides a discussion of issue related to reducing the number of instance retained in memory while maintaining and sometimes improving generalization accuracy and mention algorithm other researcher have used to address this problem it present three intuitive noise tolerant algorithm that can be used to prune instance from the training set in experiment on application the algorithm that achieves the highest reduction in storage also result in the highest generalization accuracy of the three method 
prioritized sweeping is a model based reinforcement learning method that attempt to focus the agent s limited computational resource to achieve a good estimate of the v alue of environment state the classic account of prioritized sweeping us an explicit state b ased representation of the value reward and model parameter such a representation is unwieldy for dealing with complex environment and there is growing interest in learning with more compact representation we claim that classic prioritized sweeping is ill suited for learning wit h such representation to overcome this deficiency we introducegeneralized prioritized sweeping a principled method for generating representation specific algorithm for model based reinforcement learning we then apply this method for several representation including state based mod el and generalized model approximators such a bayesian network we describe preliminary experiment that compare our approach with classical prioritized sweeping 
optimal brain damage obd is a method for reducing the number of weight in a neural network obd estimate the increase in cost function if weight are pruned and is a valid approximation if the learning algorithm ha converged into a local minimum on the other hand it is often desirable to terminate the learning process before a local minimum is reached early stopping in this paper we show that obd estimate the increase in cost function incorrectly if the network is not in a local minimum we also show how obd can be extended such that it can be used in connection with early stopping we call this new approach early brain damage ebd ebd also allows to revive already pruned weight we demonstrate the improvement achieved by ebd using three publicly available data set 
a new method to calculate the full training process of a neural networkis introduced no sophisticated method like the replica trickare used the result are directly related to the actual number oftraining step some result are presented here like the maximallearning rate an exact description of early stopping and the necessarynumber of training step further problem can be addressedwith this approach introductiontraining guided by empirical risk minimization doe not 
in this paper the technique of stacking previously only used for supervisedlearning is applied to unsupervised learning specifically it is usedfor non parametric multivariate density estimation to combine finite mixturemodel and kernel density estimator experimental result on bothsimulated data and real world data set clearly demonstrate that stackeddensity estimation outperforms other strategy such a choosing the singlebest model based on cross validation combining with 
the boosting algorithm adaboost developedby freund and schapire ha exhibitedoutstanding performance on severalbenchmark problem when using c a the quot weak quot algorithm to be quot boosted quot like other ensemble learning approach adaboost construct a composite hypothesisby voting many individual hypothesis in practice the large amount ofmemory required to store these hypothesescan make ensemble method hard to deployin application this paper show that byselecting a 
dnf system divide the input space into decision surface defined by conjunction of condition based on attribute value pair this mean that the decision surface are orthogonal t o the axis of the tested attribute and p arallel t o all t he other ax in this paper we present t he system ltree that i s able to define decision surface both orthogonal and oblique to the ax defined by the attribute of the input space this is done by combining a decision tree with a linear discriminant by mean of constructive induction at each decision node ltree defines a new instance space by the insertion of new attribute those are projection of the instance that fall at this node over the hyper plane given by a linear discriminant function this new instance space propagates downwards through the tree test based on those new attribute are oblique in the input space we have c arried out experiment on sixteen benchmark d atasets and compared our system with other well known decision tree system oblique and orthogonal like c oc and lmdt from t hese experiment we claim that our system ha advantage concerning accuracy and tree size at statistically significant confidence level 
if globally high dimensional data ha locally only low dimensional distribution it is advantageous to perform a local dimensionality reduction before further processing the data in this paper we examine several technique for local dimensionality reduction in the context of locally weighted linear r e 
many classification algorithm are quot passive quot in that they assign a class label to each instancebased only on the description given even if that description is incomplete in contrast an active classifier can at some cost obtain the value of missing attribute before deciding upon a class label the expectedutility of using an active classifier dependson both the cost required to obtain theadditional attribute value and the penaltyincurred if it output the wrong 
the hierarchical representation of data ha various application in domain such a data mining machine vision or information retrieval i n this paper we introduce an extension of the expectation maximization em algorithm that learns mixture hierarchy in a computationally efficient manner efficiency is achieved by progressing in a bottom up fashion i e by clustering the mixture component of a given level in the hierarchy to obtain those of the level above this clustering requires on ly knowledge of the mixture parameter there being no need to resort to intermediate sample in addition to practical application the algorith m allows a new interpretation of em that make clear the relationship with non parametric kernel based estimation method provides explicit control over the trade off between the bias and variance of em estimate and offer new insight about the behavior of deterministic annealing met hod commonly used with em to escape local minimum of the likelihood 
we present an empirical analysis of symbolic prototype learnersfor synthetic and real domain the prototype are learned by modifyingthe minimum distance classifier to solve problem with symbolicattributes attribute weighting and it inability to learn multiple prototypesfor a class these extension are implemented in snmc in thesecond half of this paper we provide empirical analysis characterizingsituations where symbolic prototype have advantage over traditionalmethods such 
by now it is widely accepted that learning a task from scratch i e without any prior knowledge is a daunting undertaking human however rarely attempt to learn from scratch they extract initial bias a well a strategy how to approach a learning problem from instruction and or demonstration of other human for learning control this paper investigates how learning from demonstration can be applied in the context of reinforcement learning we consider priming the q function the value function the policy and the model of the task dynamic a possible area where demonstration can speed up learning in general nonlinear learning problem only model based reinforcement learning show significant speed up after a demonstration while in the special case of linear quadratic regulator lqr problem all method profit from the demonstration in an implementation of pole balancing on a complex anthropomorphic robot arm we demonstrate that when facing the complexity of real signal processing modelbased reinforcement learning offer the most robustness for lqr problem using the suggested method the robot learns pole balancing in just a single trial after a second long demonstration of the human instructor 
cluster analysis is a fundamental principle in exploratory data analysis providing the user with a description of the group structure of given data a key problem in this context is the interpretation and visualization of clustering solution in high dimensional or abstract data space in particular probabilistic description of the group structure essential to capture inter cluster relationship are hardly assessable by simple inspection of the probabilistic assignment variable we 
in this paper we present a method for discovering approximately common motif also known a ective motif in three dimensional d molecule each node in a molecule is represented by a d point in the euclidean space and each edge is represented by an undirected line segment connecting two node in the molecule motif are rigid substructure which may occur in a molecule after allowing for an arbitrary number of rotation and translation a well a a small number specified by the user of node insert delete operation in the motif or the molecule we call this lapproximate occurrence the proposed method combine the geometric hashing technique and block detection algorithm for undirected graph to demonstrate the utility of our algorithm we discus their application to classifying three family of molecule pertaining to antibacterial sulfa drug anti anxiety agent benzodiazepine and sntiadrenergic agent receptor experimental result indicate the good performance of our algorithm and the high quality of the discovered motif 
in integrated service communication network an important problem is to exercise call admission control and routing so a to optim ally use the network resource this problem is naturally formulated a a dynamic programming problem which however is too complex to be solved exactly we use method of reinforcement learning rl together with a decomposition approach to find call admission control and r outing policy the performance of our policy for a network with approximately different feature configuration is compared with a commonl y used heuristic policy 
current method to avoid overfitting are eitherdata oriented using separate data forvalidation or representation oriented penalizingcomplexity in the model this paperproposes process oriented evaluation wherea model s expected generalization error iscomputed a a function of the search processthat led to it the paper developsthe necessary theoretical framework and appliesit to one type of learning rule induction a process oriented version of the cn rule learner is 
the identification of relevant attribute is an important and difficult task in data mining application where induction is used a the primary tool for knowledge extraction this paper introduces a new rule induction algorithm ritio which eliminates attribute iu order of decreasing irrelevancy the rule produced by ritio are shown to be largely based on only the most relevant attribute experimental result with and without feature selection preprocessing confirm that ritio achieves high level of predictive accuracy 
on large problem reinforcement learning system must use parameterized function approximators such a neural network in order to generalize between similar situation and action in these case there are no strong theoretical result on the accuracy of convergence and computational result have been mixed in particular boyan and moore reported at last year s meeting a series of negative result in attempting to apply dynamic programming together with function approximation to simple control problem with continuous state space in this paper we present positive result for all the control task they attempted and for one that is significantly larger the most important dierences are that we used sparse coarse coded function approximators cmacs whereas they used mostly global function approximators and that we learned online whereas they learned oine boyan and moore and others have suggested that the problem they encountered could be solved by using actual outcome rollouts a in classical monte carlo method and a in the td algorithm when however in our experiment this always resulted in substantially poorer performance we conclude that reinforcement learning can work robustly in conjunction with function approximators and that there is little justification at present for avoiding the case of general 
this paper present experiment with datasets and decision tree pruning algorithmsthat show that increasing trainingset size often result in a linear increase intree size even when that additional complexityresults in no significant increase inclassification accuracy said differently removingrandomly selected training instancesoften result in tree that are substantiallysmaller and just a accurate a those builton all available training instance this impliesthat decrease 
the observed distribution of natural image is far from uniform on the contrary real image have complex and important structure that can be exploited for image processing recognition and analysis there have been many proposed approach to the principled statistical modeling of image but each ha been limited in either the complexity of the model or the complexity of the image we present a non parametric multi scale statistical model for image that can be used for recognition image de noising and in a generative mode to synthesize high quality texture 
this paper defines a formal approach to learning from example described by labelled graph we propose a formal model based upon lattice theory and in particular with the use of galois lattice we enlarge the domain of formal concept analysis by the use of the galois lattice model with structural description of example and concept our implementation called graal for graph and learning construct a galois lattice for any description language provided that the two operation of comparison and generalization are determined for that language we prove that these operation exist in the case of labelled graph 
the ability to rely on similarity metric invariant to image transform ations is an important issue for image classification task such a face or character recognition we analyze an invariant metric that ha performed well for the latter the tangent distance and study it limitation when applied to regular image showing that the most significant among these convergence to local minimum can be drastically reduced by computing the distance in a multiresolution setting this lead to the multiresolution tangent distance which exhibit significantly higher invariance to image transformation and can be easily combined with robust estimation procedure 
we have developed a computational theory of rodent navigation that includes analog of the place cell system the head direction system and path integration in this paper we present simulation result showing how interaction between the place and head direction system can account for recent observation about hippocampal place cell response to doubling and or rotation of cue card in a cylindrical arena sharp et al 
this paper describes a new approach to automatically learning linguistic knowledge for spelling correction a major feature of this approach is the fact that the acquired knowledge is captured in a small set of easily understood rule a opposed to a large set of opaque feature and weight a perspicuous representation is advantageous in order to best exploit human intuition to understand and improve upon the acquired knowledge of the system 
the goal of active template research is to create a single unified environment that a data analyst can use to carry out a knowledge discovery project and to deliver the resulting solution in the form of an active template an active template is a hyper linked information structure that tightly integrates action executable program and command result model datasets prediction report and documentation explanation of decision action and result the use of active template provides a number of benefit including user guidance improved documentation of action and result and increased reuse of previous work 
practical approach to clustering use an iterative procedure e g k mean em which converges to one of numerous local minimum it is known that these iterative technique are especially sensitive to initial starting condition we present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the mode of a distribution the refined initial starting condition allows the iterative algorithm to converge to a better local minimum the procedure is applicable to a wide class of clustering algorithm for both discrete and continuous data we demonstrate the application of this method to the popular k mean clustering algorithm and show that refined initial starting point indeed lead to improved solution refinement run time is considerably lower than the time required to cluster the full database the method is scalable and can be coupled with a scalable clustering algorithm to address the large scale clustering problem in data mining 
scalability determines the potential in distributing 
we consider neural network model for stochastic nonlinear dynamical system where measurement of the variable of interest are only avail able at irregular interval i e most realization are missing dif culties arise since the solution for prediction and maximum likelihood learn ing with missing data lead to complex integral which even for simple case cannot be solved analytically in this paper we propose a spe ci c combination of a nonlinear recurrent neural predictive model and a linear error model which lead to tractable prediction and maximum likelihood adaptation rule in particular the recurrent neural network can be trained using the real time recurrent learning rule and the linear error model can be trained by an em adaptation rule implemented u ing forward backward kalman lter equation the model is applied to predict the glucose insulin metabolism of a diabetic patient where blood glucose measurement are only available a few time a day at irregular interval the new model show considerable improvement with respect to both recurrent neural network trained with teacher forcing or in a free running mode and various linear model 
a new approach for clustering is proposed this method is basedon an analogy to a physical model the ferromagnetic potts modelat thermal equilibrium is used a an analog computer for this hardoptimization problem we do not assume any structure of the underlyingdistribution of the data phase space of the potts model isdivided into three region ferromagnetic super paramagnetic andparamagnetic phase the region of interest is that correspondingto the super paramagnetic one where 
a linear architectural model of cortical simple cell is presented the model evidence how mutual inhibition occurring throughsynaptic coupling function asymmetrically distributed in space can be a possible basis for a wide variety of spatio temporal simplecell response property including direction selectivity and velocitytuning while spatial asymmetry are included explicitly in thestructure of the inhibitory interconnection temporal asymmetriesoriginate from the 
to monitor or control a stochastic dynamic system we need to reason about it current state exact inference for this task requires that we maintain a complete joint probability distribution over the possible state an impossible requirement for most process stochastic simulation algorithm provide an alternative solution by approximating the distribution at time via a relatively small set of sample the time sample are used a the basis for generating the sample at time however since only existing sample are used a the basis for the next sampling phase new part of the space are never explored we propose an approach whereby we try to generalize from the time sample to unsampled region of the state space thus these sample are used a data for learning a distribution over the state at time which is then used to generate the time sample we examine different representation for a distribution including density tree bayesian network and tree structured bayesian network and evaluate their appropriateness to the task the machine learning perspective allows u to examine issue such a the tradeoff of using more complex model and to utilize important technique such a regularization and prior we validate the performance of our algorithm on both artificial and real domain and show significant improvement in accuracy over the existing approach 
in this paper we propose recurrent neural network with feedback into the inputunits for handling two type of data analysis problem on the one hand thisscheme can be used for static data when some of the input variable are missing on the other hand it can also be used for sequential data when some of theinput variable are missing or are available at different frequency unlike in thecase of probabilistic model e g gaussian of the missing variable the networkdoes not 
we evaluate inductive logic programming ilp method for predicting fault density inc class in this problem each trainingexample is a c class definition representedas a calling tree and labeled a quot positive quot iff fault i e error were discoveredin it implementation we compare two ilpsystems foil and flipper and explorethe reason for their differing performance using both natural and artificial data wethen propose two extension to flipper a 
a simple linear averaging of the output of several network ase g in bagging seems to follow naturally from a bias variancedecomposition of the sum squared error the sum squared error ofthe average model is a quadratic function of the weighting factorsassigned to the network in the ensemble suggesting a quadraticprogramming algorithm for finding the quot optimal quot weighting factor if we interpret the output of a network a a probability statement the sum squared error 
automating the learning of causal model from sample data is a key step toward incorporatingmachine learning in the automation of decision making and reasoning under uncertainty this paper present a bayesian approach to the discovery of causal model using a minimummessage length mml method we have developed encoding and search method for discoveringlinear causal model the initial experimental result presented in this paper showthat the mml induction approach can recover causal 
abstract most computational engineering based loosely on biology us continuous variable to represent neural activity yet most neuron communicate with action potential the engineering view is equivalent to using a rate code for representing information and for computing an increasing number of example are being discovered in which biology may not be using rate code information can be represented using the timing of action potential and efficiently computed with in this representation the analog match problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an underlying rhythm by using adapting unit to effect a fundamental change of representation of a problem we map the recognition of word having uniform time warp in connected speech into the same analog match problem we describe the architecture and preliminary result of such a recognition system using the fast event of biology in conjunction with an underlying rhythm is one way to overcome the limit of an eventdriven view of computation when the intrinsic hardware is much faster than the time scale of change of input this approach can greatly increase the effective computation per unit time on a given quantity of hardware spike timing 
the problem of assigning m point in the n dimensional real spacernto k cluster is formulated a that of determining k center inrnsuch that the sum of distance of each point to the nearestcenter is minimized if a polyhedral distance is used the problemcan be formulated a that of minimizing a piecewise linear concavefunction on a polyhedral set which is shown to be equivalent toa bilinear program minimizing a bilinear function on a polyhedralset a fast finite k median 
for neural network with a wide class of weight prior it can be shown that in the limit of an infinite number of hidden unit the prior over function tends to a gaussian process in this paper analytic form are derived for the covariance function of the gaussian process corresponding to network with sigmoidal and gaussian hidden unit this allows prediction to be made efficiently using network with an infinite number of hidden unit and show that somewhat paradoxically it may be 
we discus a strategy for polychotomous classification that involvesestimating class probability for each pair of class and then couplingthe estimate together the coupling model is similar to thebradley terry method for paired comparison we study the natureof the class probability estimate that arise and examine theperformance of the procedure in real and simulated datasets classifiersused include linear discriminants nearest neighbor and thesupport vector machine 
hidden markov model hmms have proven to be one of the most widelyused tool for learning probabilistic model of time series data inan hmm information about the past is conveyedthrough a single discrete variable the hidden state we discus ageneralization of hmms in which this state is factored into multiplestate variable and is therefore represented in a distributed manner we describe an exact algorithm for inferring the posteriorprobabilities of the hidden state variable given the observation and relate it to the forward backward algorithm for hmms and toalgorithms for more general graphical model due to the combinatorialnature of the hidden state representation this exact algorithm isintractable a in other intractable system approximate inferencecan be carried out using gibbs sampling or variational method within the variational framework wepresent a structured approximation in which the the statevariables are decoupled yielding a tractablealgorithm for learning the parameter of the model empiricalcomparisons suggest that these approximation are efficient andprovide accurate alternative to the exact method finally we use thestructured approximation to model bach s chorale and show thatfactorial hmms can capture statistical structure in this data setwhich an unconstrained hmm cannot 
an image is often represented by a set of detected feature we getan enormous compression by representing image in this way furthermore we get a representation which is little affected by smallamounts of noise in the image however feature are typicallychosen in an ad hoc manner we show how a good set of featurescan be obtained using sufficient statistic the idea of sparsedata representation naturally arises we treat the dimensionaland dimensional signal reconstruction 
association rule algorithm can produce a very large number of output pattern this ha raised question of whether the set of discovered rule quot overfit quot the data because all the pattern that satisfy some constraint are generated the bonferroni effect in other word the question is whether some of the rule are quot false discovery quot that are not statistically significant we present a novel approach for estimating the number of quot false discovery quot at any cutoff level empirical evaluation 
the goal of pattern classification can be approached from two point of view informative where the classifier learns the class density or discriminative where the focus is on learning the class boundary without regard to the underlying class density we review and synthesize the tradeoff between these two approach for simple classifier and extend the result to modern technique such a naive bayes and generalized additive model data mining application ofi a ll s cl l l m no nn ron ijell uyaraw u ijui u lllcz u i lup ullll di ii iii igi lr uig where the tradeoff between informative and discriminative classifier are especially relevant experimental result are provided for simulated and real data 
abstract av pierre mend s france bron cedex france zighed rakotoma ffeschet univ lyon fr in this paper we propose an extension of fischer s 
we consider multi criterion sequential decision making problem where the vector valued evaluation are compared by a given fixed total ordering condition for the optimality of stationary policy and the bellman optimality equation are given for a special but important class of problem when the evaluation of policy can be computed for the criterion independently of each other the analysis requires special care a the topology introduced by pointwise convergence and the order topology introduced by the preference order are in general incompatible reinforcement learning algorithm are proposed and analyzed preliminary computer experiment confirm the validity of the derived algorithm these type of multi criterion problem are most useful when there are several optimal solution to a problem and one want to choose the one among these which is optimal according to another fixed criterion possible application in robotics and repeated game are outlined 
motivated by the finding of modular structure in the associationcortex we study a multi modular model of associative memory thatcan successfully store memory pattern with different level of activity we show that the segregation of synaptic conductance intointra modular linear and inter modular nonlinear one considerablyenhances the network s memory retrieval performance comparedwith the conventional single module associative memory network the multi modular network ha two main 
the softassign quadratic assignment algorithm ha recently emerged a an effectivestrategy for a variety of optimization problem in pattern recognition and combinatorialoptimization while the effectiveness of the algorithm wa demonstrated inthousands of simulation there wa no known proof of convergence here we providea proof of convergence for the most general form of the algorithm introductionrecently a new neural optimization algorithm ha emerged for solving quadratic 
a mature data mining system ha to interact with standard dbms a crucial factor in the performance of such a data mining system lie in this interaction the keso project aim at the deveiopment of such a tool and it interaction with the database is restricted to two way table query a special kind of aggregate query this restriction give rise to ample possibility to optimize the computation of such two way table e g by using parallelisation or by temporary storage of intermediate result however the size of these two way table put a large communication overhead on the database interaction of keso in this paper we propose to compute certain aggregate in the database this approach low 
the recent emergence of data mining a amajor application of machine learning ha ledto increased interest in fast rule induction algorithm these are able to efficiently processlarge number of example under the constraintof still achieving good accuracy if eis the number of example many rule learnershave o e asymptotic time complexity innoisy domain and c rule ha been empiricallyobserved to sometimes require o e recent advance have brought this bound downto 
in many application it is necessary to determine the similarity of two string a widely used notion of string similarity is the edit distance the minimum number of insertion deletion and substitution required to transform one string into the other in this report we provide a stochastic model for string edit distance our stochastic model allows u to learn a string edit distance function from a corpus of example we illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of word in conversational speech in this application we learn a string edit distance with nearly one fifth the error rate of the untrained levenshtein distance our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototype 
predicting item a user would like on the basis of other user rating for these item ha become a well established strategy adopted by many recommendation service on the internet although this can be seen a a classification problem algorithm proposed thus far do not draw on result from the machine learning literature we propose a representation for collaborative filtering task that allows the application of virtually any machine learning algorithm we identify the shortcoming of current collaborative filtering technique and propose the use of learning algorithm paired with feature extraction technique that specifically address the limitation of previous approach our best performing algorithm is based on the singular value decomposition of an initial matrix of user rating exploiting latent structure that essentially eliminates the need for user to rate common item in order to become predictor for one another s preference we evaluate the proposed algorithm on a large database of user rating for motion picture and find that our approach significantly outperforms current collaborative filtering algorithm 
if globally high dimensional data ha locally only low dimensional distribution it is advantageous to perform a local dimensionality reduction beforefurther processing the data in this paper we examine several technique forlocal dimensionality reduction in the context of locally weighted linear regression a possible candidate we derive local version of factor analysisregression principle component regression principle component regressionon joint distribution and partial 
standard technique eg yule walker are available for learningauto regressive process model of simple directly observable dynamicalprocesses when sensor noise mean that dynamic areobserved only approximately learning can still been achieved viaexpectation maximisation em together with kalman filtering however this doe not handle more complex dynamic involvingmultiple class of motion for that problem we show here howem can be combined with the condensation algorithm 
in this paper we suggest determination a a representationof knowledge that should be easy to understand we briefly review determination which can bedisplayed in a tabular format and their use in prediction which involves a simple matching process wedescribe condet an algorithm that us feature selectionto construct determination from training data augmented by a condensation process that collapsesrows to produce simpler structure we report experimentsthat show condensation 
we study a time series model that can be viewed a a decisiontree with markov temporal structure the model is intractable forexact calculation thus we utilize variational approximation weconsider three different distribution for the approximation one inwhich the markov calculation are performed exactly and the layersof the decision tree are decoupled one in which the decision treecalculations are performed exactly and the time step of the markovchain are decoupled and one in 
the bayesian analysis of neural network is difficult because a simpleprior over weight implies a complex prior distribution overfunctions in this paper we investigate the use of gaussian processpriors over function which permit the predictive bayesian analysisfor fixed value of hyperparameters to be carried out exactlyusing matrix operation two method using optimization and averaging via hybrid monte carlo over hyperparameters have beentested on a number of challenging 
when triangulating a belief network we aim to obtain a junction tree of minimum state space searching for the optimal triangulation can be cast a a search over all the permutation of the network s vaeriables our approach is to embed the discrete set of permutation in a convex continuous domain d by suitably extending the cost function over d and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost in this paper we introduce an upper bound to the total junction tree weight a the cost function the appropriatedness of this choice is discussed and explored by simulation then we present two way of embedding the new objective function into continuous domain and show that they perform well compared to the best known heuristic 
in this paper we examine a method for featuresubset selection based on informationtheory initially a framework for definingthe theoretically optimal but computationallyintractable method for feature subset selectionis presented we show that our goalshould be to eliminate a feature if it givesus little or no additional information beyondthat subsumed by the remaining feature inparticular this will be the case for both irrelevantand redundant feature we thengive an efficient 
this paper discus the unsupervised learningproblem an important part of the unsupervisedlearning problem is determining thenumber of constituent group component orclasses which best describes some data weapply the minimum message length mml criterion to the unsupervised learning problem modifying an earlier such mml application we give an empirical comparison ofcriteria prominent in the literature for estimatingthe number of component in a dataset we conclude that the 
we introduce a an optimization measure for ranking problem such a information retrieval ir is an intuitive differentiable measure which is somewhat correlated with standard ir measure of performance a set of experiment is described in which the weight used to combine the result of two ir system is chosen by optimizing this technique work about a well a optimizing exact precision a traditional ir performance measure these result suggest the use of for training more complex neural net model for solving ranking problem 
the goal of robot learning from demonstrationis to have a robot learn from watching ademonstration of the task to be performed in our approach to learning from demonstrationthe robot learns a reward functionfrom the demonstration and a task modelfrom repeated attempt to perform the task a policy is computed based on the learnedreward function and task model lessonslearned from an implementation on an anthropomorphicrobot arm using a pendulumswing up task include simply 
a relational instance based learning algorithm called ribl is motivated and developedin this paper we argue that instancebasedmethods offer solution to the oftenunsatisfactory behavior of current inductivelogic programming ilp approach in domainswith continuous attribute value andin domain with noisy attribute and or example three research issue that emergewhen a propositional instance based learneris adapted to a first order representation areidentified construction 
may be the preferred variable fordescribing the response of a spiking neuron key word neural coding retinal ganglion cell spike generator refractory period reproducibility poisson processthere ha been considerable speculation about the code used byspiking neuron to transmit information ferster and spruston sejnowski stevens and zador the spectrum ofproposed theory range from the quot rate code quot in which the firingrates of many neuron are averaged to 
we have investigated the possibility that rapid processing in the visual system could be achieved by using the order of firing in different neurones a a code rather than more conventional firing rate scheme using spikenet a neural net simulator based on integrate and fire neurones and in which neurones in the input layer function a analogto delay converter we have modeled the initial stage of visual processing initial result are extremely promising even with activity in retinal output cell limited to one spike per neuron per image effectively ruling out any form of rate coding sophisticated processing based on asynchronous activation wa nonetheless possible 
conditioning experiment probe the way that animal make prediction about reward and punishment and use those prediction to control their behavior one standard model of conditioning paradigm which involve many conditioned stimulus suggests that individual prediction should be added together various key result show that this model fails in some circumstance and motivate an alternative model in which there is attentional selection between different available stimulus the new model is a form of mixture of expert ha a close relationship with some other existing psychological suggestion and is statistically well founded 
abstract traditional instance based learning methodsbase their prediction directly on training data that ha been stored in the memory thepredictions are based on weighting the contributionsof the individual stored instance bya distance function implementing a domaindependentsimilarity metric this basic approachsuffers from three drawback computationallyexpensive prediction when thedatabase grows large overfitting in the presenceof noisy data and sensitivity to the 
we describe a practical algorithm for learningaxis parallel high dimensional box frommulti instance example the first solutionto this practical learning problem arising indrug design wa given by dietterich lathrop and lozano perez a theoretical analysiswas performed by auer long srinivasan and tan in this work we derive a competitivealgorithm from theoretical considerationswhich is completely different from theapproach taken by dietterich et al our algorithmuses for 
the error rate of decision tree and other classificationlearners can often be much reduced bybagging learning multiple model from bootstrapsamples of the database and combining them byuniform voting in this paper we empirically testtwo alternative explanation for this both basedon bayesian learning theory bagging worksbecause it is an approximation to the optimalprocedure of bayesian model averaging with anappropriate implicit prior bagging work becauseit 
this paper address the problem of determining an object s d location from a sequence of camera image recorded by a mobile robot the approach presented here allows people to train robot to recognize specific object by presenting it example of the object to be recognized a decision tree method is used to learn significant feature of the target object from individual camera image individual estimate are integrated over time using bayes rule into a probabilistic d model of the robot s environment experimental result illustrate that the method enables a mobile robot to robustly estimate the d location of object from multiple camera image 
we study on line generalized linear regression with multidimensional output i e neural network with multiple output node but no hidden node we allow at the final layer transfer function such a the softmax function that need to consider the linear activation to all the output neuron the weight vector used to produce the linear activation are represented indirectly by maintaining separate parameter vector we get the weight vector by applying a particular parameterization function to the parameter vector updating the parameter vector upon seeing new example is done additively a in the usual gradient descent update however by using a nonlinear parameterization function between the parameter vector and the weight vector we can make the resulting update of the weight vector quite different from a true gradient descent update to analyse such update we define a notion of a matching loss function and apply it both to the transfer function and to the parameterization function the loss function that match the transfer function is used to measure the goodness of the prediction of the algorithm the loss function that match the parameterization function can be used both a a measure of divergence between model in motivating the update rule of the algorithm and a a measure of progress in analyzing it relative performance compared to an arbitrary fixed model a a result we have a unified treatment that generalizes earlier result for the gradient descent and exponentiated gradient algorithm to multidimensional output including multiclass logistic regression 
hubel and wiesel proposed that complex cell in visual cortexare driven by a pool of simple cell with the same preferredorientation but different spatial phase however a wide variety ofexperimental result over the past two decade have challenged thepure hierarchical model primarily by demonstrating that manycomplex cell receive monosynaptic input from unoriented lgncells or do not depend on simple cell input we recently showed usinga detailed biophysical model that 
dynamic programming q learning and other discrete markov decisionprocess solver can be applied to continuous d dimensional state space byquantizing the state space into an array of box this is often problematicabove two dimension a coarse quantization can lead to poor policy andfine quantization is too expensive possible solution are variable resolutiondiscretization or function approximation by neural net a third option which ha been little studied in the reinforcement 
finding the quot right quot number of cluster k for a dataset is a difficult and often ill posed problem ina probabilistic clustering context likelihood ratio penalized likelihood and bayesian technique areamong the more popular technique in this papera new cross validated likelihood criterion is investigatedfor determining cluster structure a practicalclustering algorithm based on monte carlo crossvalidation mccv is introduced the algorithm permitsthe data analyst to judge if 
decision tree are an important data mining tool with many application like manyclassification technique decision tree process the entire data base in order to producea generalization of the data that can be used subsequently for classification large complex data base are not always amenable to such a global approach to generalization this paper explores several method for extracting data that is local to a query point and then using the local data to build generalization these 
finding articulated object like people in picture present a particularlydifficult object recognition problem we show how tofind people by finding putative body segment and then constructingassemblies of those segment that are consistent with the constraintson the appearance of a person that result from kinematicproperties since a reasonable model of a person requires at leastnine segment it is not possible to present every group to a classifier instead the search can be 
we study the spatial data mining problem of how to extract a special type of proximity relationship namely that of distinguishing two cluster of point based on the type of their neighbouring feature the point in the cluster may represent house on a map and the feature may represent spatial entity such a school park golf course etc class of feature are organized into concept hierarchy we develop algorithm gendis which us concept generalization to identify the distinguishing feature or concept which serve a discriminator furthermore we study the issue of which discriminator axe better than others by introducing the notion of maximal discriminator and by using a ranking system to quantitatively weigh maximal discriminator from different concept hierarchy 
abstract we seek the scene interpretation that best explains image data for example we may want to infer the projected velocity scene which best explain two consecutive image frame image from synthetic data we model the relationship between image and scene patch and between a scene patch and neighboring scene patch given a new image we propagate likelihood in a markov network ignoring the e ect of loop to infer the underlying scene this yield an e cient method to form low level scene interpretation we demonstrate the technique for motion analysis and estimating high resolution image from low resolution one 
learning accuracy depends on concept variation the accuracy of six learning system c grove greedy fringe lfc andmrp is compared using a set of forty testconcepts the selection of these concept wasguided by the existence of structured conceptsthat appear in difficult real world domain such a protein folding such conceptsoften have embedded implicit structure which may be revealed through explicitrelations experiment using these benchmarkconcepts show that 
knowledge discovery in database kdd focus on the computerized exploration of large amount of data and on the discovery of interesting pattern within them while most work on kdd ha been concerned with structured database there ha been little work on handling the huge amount of information that is available only in unstructured document collection this paper describes a new method a for cotiiptirmg co occuttetice rreyuencm tnr various keywords labeling the document this method is based on computing maximal association rule regular association are based on the notion offrequent set set of attribute which appear in many record in analogy maximal association are based on the notion of frequent maximal set conceptually a frequent maximal set is a set of attribute which appear alone or maximally in many record for the definition of maximality we use an underlying taxonomy t of the attribute this allows u to obtain the interesting correlation between attribute from different category frequent maximal set are useful for cg l jz rl a a a l aa rra tr wl gjllllpallly lllluurl abci llall i lu gb ullil iiili u ie cx attribute we provide an experimental evaluation of our methodology on the reuters document collection 
the paper present an approach to data mining involving search for complete or nearly complete domain classification in term of attribute value our objective is to find classification based on interacting attribute that provide a good characterization of the concept of interest by maximizing predefined quality criterion the paper introduces the notion of the classification complexity and several other measure to evalu a abe yuauby 
we present an algorithm for fast stochastic gradient descent thatuses a nonlinear adaptive momentum scheme to optimize the latetime convergence rate the algorithm make effective use of curvatureinformation requires only o n storage and computation and delivers convergence rate close to the theoretical optimum we demonstrate the technique on linear and large nonlinear backpropnetworks improving stochastic searchlearning algorithm that perform gradient descent on a cost 
many popular learning rule are formulated in term of continu ous analog input and output biological system however use action potential which are digital amplitude event that encode analog information in the inter event interval action potential representation are now being used to advantage in neuromorphic vlsi system a well we report on a simple learning rule based on the riccati equation described by kohonen modi ed for action potential neuronal output we demonstrate this learning rule in an analog vlsi chip that us volatile capacitive storage for synaptic weight we show that our time dependent learning rule is su cient to achieve approximate weight normalization and can detect temporal correlation in spike train 
in this paper we describe two different learningtasks for relational structure whenlearning a classifier for structure the relationalstructures in the training set are classifiedas a whole contrarily when learninga context dependent classifier for elementaryobjects the elementary object of the relationalstructures in the training set are classified in general the class of an elementaryobject will not only depend on it elementaryproperties but also on it 
we propose a model for early visual processing in primate themodel consists of a population of linear spatial filter which interactthrough non linear excitatory and inhibitory pooling statisticalestimation theory is then used to derive human psychophysicalthresholds from the response of the entire population of unit themodel is able to reproduce human threshold for contrast and orientationdiscrimination task and to predict contrast threshold inthe presence of mask of varying 
this paper present an algorithm for discovering conjunction rule with high reliability from data set the discovery of conjunction rule each of which is a restricted form of a production rule is well motivated by various useflll application such a semantic query optimization and automatic development of a knowledge base in a discovery algorithm a production rule is evaluated according to it generality and accuracy since these are widely accepted a criterion in learning from example here reliability evaluation for these criterion is mandatory in distinguishing reliable rule from unreliable pattern without annoying the user however previous discovery approach have either ignored reliability evaluation or have only evaluated the reliability of generality and consequently tend to discover a huge number of rule in order to circumvent these difficulty we propose an approach based on a simultaneous estimation our approach discovers the rule that exceed pre specified threshold for generality and accuracy with high reliability a novel pruning method is employed for improving time efficiency without changing the discovery outcome the proposed approach ha been validated experimentally using benchmark data set from the uci repository 
feature selection ha proven to be a valuable technique in supervised learning for improving predictive accuracy while reducing the number of attribute considered in a task we investigate the potential for similar benefit in an unsupervised learning task conceptual clustering the issue raised in feature selection by the absence of class label are discussed and an implementation of a sequential feature selection algorithm based on an existing conceptual clustering system is described additionally we present a second implementation which employ a technique for improving the efficiency of the search for an optimal description and compare the performance of both algorithm 
learning how to adjust to an opponent s position is critical tothe success of having intelligent agent collaborating towards theachievement of specific task in unfriendly environment this paperdescribes our work on a memory based technique for to choosean action based on a continuous valued state attribute indicatingthe position of an opponent we investigate the question of how anagent performs in nondeterministic variation of the training situation our experiment indicate that 
similarity is an important and widely used concept previous definition of similarity are tied to a particular application or a form of knowledge representation we present an informationtheoretic definition of similarity that is applicable a long a there is a probabilistic model we demonstrate how our definition can be used to measure the similarity in a number of different domain 
user modeling is employed by application that need to maintain explicit model of their user in order to exhibit individua lized behaviour the user modeling task involves representation and acquisition of assumption about the user particularly user model acquisition is closely related to the machine learning task of automatical ly acquiring new information a well a new representation of existing information this paper show how and for which purpose machine learning technique have been and could be employed in user modeling also usage modeling a more action centered approach to user modeling is considered finally the labour approach to user modeling is sketched which regard user modeling a learning problem 
we have constructed an inexpensive video based motorized trackingsystem that learns to track a head it us real time graphicaluser input or an auxiliary infrared detector a supervisory signalsto train a convolutional neural network the input to the neuralnetwork consist of normalized luminance and chrominance imagesand motion information from frame difference subsampled imagesare also used to provide scale invariance during the onlinetraining phase the neural network rapidly 
most of the reinforcement learning rl algorithmsassume that the learning processesof embedded agent can be formulated asmarkov decision process mdps however the assumption is not valid for many realisticproblems therefore research on rltechniques for non markovian environmentsis gaining more attention recently we havedeveloped a bayesian approach to rl in nonmarkovianenvironments in which the environmentis modeled a a history tree model a stochastic model with 
gain control by divisive inhibition a k a divisive normalization ha been proposed to be a general mechanism throughout the visualcortex we explore in this study the statistical propertiesof this normalization in the presence of noise using simulation we show that divisive normalization is a close approximation to amaximum likelihood estimator which in the context of populationcoding is the same a an ideal observer we also demonstrate analyticallythat this is a general 
both vertebrate and invertebrate retina are highly ecient in extracting contrast independent of the background intensity over ve or more decade this eciency ha been rendered possible by the adaptation of the dc operating point to the background intensity while maintaining high gain transient response the centersurround property of the retina allows the system to extract information at the edge in the image this silicon retina model the adaptation property of the receptor and the antagonistic centersurround property of the laminar cell of the invertebrate retina and the outer plexiform layer of the vertebrate retina we also illustrate the spatio temporal response of the silicon retina on moving bar the chip ha x pixel on a x mm die and it is fabricated in m n well technology 
abstract we present a methodology for tightly coupling data mining application to database system to build high performance application without requiring any change to the database software 
singular value decomposition svd can be viewed a a method for unsupervised training of a network that associate two class of event reciprocally by linear connection through a single hidden layer svd wa used to learn and represent relation among very large number of word k k and very large number of natural text passage k k in which they occurred the result wa dimensional semantic space in which any trained or newly added word or passage could be represented a a vector and similarity were measured by the cosine of the contained angle between vector good accuracy in simulating human judgment and behavior ha been demonstrated by performance on multiple choice vocabulary and domain knowledge test emulation of expert essay evaluation and in several other way example are also given of how the kind of knowledge extracted by this method can be applied 
we derive and analyse robust optimization scheme for noisy vector quantization on the basis of deterministic annealing star ting from a cost function for central clustering that incorporates dis tortions from channel noise we develop a soft topographic vector quantization algorithm stvq which is based on the maximum entropy principle and which performs a maximum likelihood estimate in an expectationmaximization em fashion annealing in the temperature parameter lead to phase transition in the existing code vector repre sentation during the cooling process for which we calculate critical temp eratures and mode a a function of eigenvectors and eigenvalue of the covariance matrix of the data and the transition matrix of the channel no ise a whole family of vector quantization algorithm is derived from stvq among them a deterministic annealing scheme for kohonen s self o rganizing map som this algorithm which we call ssom is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel the algorithm s performance is compared to those o f lbg and stvq while it is naturally superior to lbg which doe not take into account channel noise it result compare very well to tho e of stvq which is computationally much more demanding 
this paper discus a fairly general adaptation algorithm whichaugments a standard neural network to increase it recognition accuracyfor a specific user the basis for the algorithm is that the outputof a neural network is characteristic of the input even when the outputis incorrect we exploit this characteristic output by using an outputadaptation module oam which map this output into the correctuser dependent confidence vector the oam is a simplified resourceallocating network 
this paper show that if a large neural network is used for a patternclassification problem and the learning algorithm find a networkwith small weight that ha small squared error on the trainingpatterns then the generalization performance depends on the sizeof the weight rather than the number of weight more specifically consider an layer feed forward network of sigmoid unit inwhich the sum of the magnitude of the weight associated witheach unit is bounded by a the 
we present a connectionist method for representing image that explicitlyaddresses their hierarchical nature it blend data from neuroscienceabout whole object viewpoint sensitive cell in inferotemporalcortex and attentional basis field modulation in v withideas about hierarchical description based on microfeatures the resulting model make critical use of pathway for both analysisand synthesis we illustrate the model with a simple exampleof representing 
this paper introduces and evaluates a naturalextension of linear exponentiated gradientmethods that make them applicableto reinforcement learning problem just asthese method speed up supervised learning we find that they can also increase the efficiencyof reinforcement learning comparisonsare made with conventional reinforcementlearning method on two test problemsusing cmac function approximators and replacingtraces on a small prediction task exponentiated gradient 
an important and difficult prediction taskin many domain particularly medical decisionmaking is that of prognosis prognosispresents a unique set of problem to alearning system when some of the outputsare unknown this paper present a new approachto prognostic prediction using ideasfrom nonparametric statistic to fully utilizeall of the available information in a neural architecture the technique is applied to breastcancer prognosis resulting in flexible accuratemodels that 
we previously proposed a quantitative model of early visual processingin primate based on non linearly interacting visual filtersand statistically efficient decision we now use this model to interpretthe observed modulation of a range of human psychophysicalthresholds with and without focal visual attention our model calibrated by an automatic fitting procedure simultaneously reproducesthresholds for four classical pattern discrimination task performed while attention wa 
we address the problem of learning structure in nonlinear markov network with continuous variable this can be viewed a non gaussian multidi mensional density estimation exploiting certain conditional independency in the variable markov network are a graphical way of describing con ditional independency well suited to model relationship which do not ex hibit a natural causal ordering we use neural network structure to model the quantitative relationship between variable the main focus in this pa per will be on learning the structure for the purpose of gaining insight into the underlying process using two data set we show that interesting struc tures can be found using our approach inference will be brie y addressed 
in many real world task only a small fraction of the available input are important at any particular time this paper present a method for ascertaining the relevance of input by exploiting temporal coherence and predictability the method proposed in this paper dynamically allocates relevance to input by using expectation of their future value a a model of the task is learned the model is simultaneously extended to create task specific prediction of the future value of input input which are either not relevant and therefore not accounted for in the model or those which contain noise will not be predicted accurately these input can be de emphasized and in turn a new improved model of the task created the technique presented in this paper have yielded significant improvement for the vision based autonomous control of a land vehicle vision based hand tracking in cluttered scene and the detection of fault in the etching of semiconductor wafer 
this paper present the p lanmine sequence mining algorithm to extract pattern of event that predict failure in database of plan execution new technique were needed because previous data mining algorithm were overwhelmed by the staggering number of very frequent but entirely unpredictive pattern that exist in the plan database this paper combine several technique for pruning out unpredictive and redundant pattern which reduce the size of the returned rule set by more than three order of magnitude p lanmine ha also been fully integrated into two real world planning system we experimentally evaluate the rule discovered by planmine and show that they are extremely useful for understanding and improving plan a well a for building monitor that raise alarm before failure happen this paper describes planmine the data mining component of the above two application we show that one cannot simply apply previous sequence discovery algorithm srikant zaki for mining execution trace due to the complicated structure and redundancy in the data simple application of the known algorithm generates an enormous number of highly frequent but unpredictive rule we developed a three step pruning strategy for selecting only the most predictive rule first we eliminate normative rule that are consistent with background knowledge that corresponds to the normal operation of a successful plan second we eliminate those redundant pattern that have the same frequency a at least one of their proper subsequence finally we keep only dominatingsequences that are more predictive than all of their proper subsequence to experimentally validate our approach we show that improve doe not work well if the planmine component is replaced by le sophisticated method for choosing which part of the plan to repair we also show that the output of p lanmine can be used to build execution monitor which predict failure in a plan before they occur we were able to produce monitor with precision that signal of all the failure that occur a more detailed version of this paper appears in zaki 
the kernel parameter is one of the few tunable parameter in supportvector machine and control the complexity of the resulting hypothesis the choice of it value amount to model selection and is usually performedby mean of a validation set we present an algorithm which can automatically perform model selectionand learning with no additional computational cost and with no need of avalidation set theoretical result motivating this approach providing upperbounds on the 
program execution speed on modern computer is sensitive by a factor of two or more to the order in which instruction are presented to the processor to realize potential execution efcienc y an optimizing compiler must employ a heuristic algorithm for instruction scheduling such algorithm are painstakingly hand crafted which is expensive and time consuming we show how to cast the instruction scheduling problem a a learning task obtaining the heuristic scheduling algorithm automatically our focus is the narrower problem of scheduling straight line code also called basic block of instruction our empirical result show that just a few feature are adequate for quite good performance at this task for a real modern processor and that any of several supervised learning method perform nearly optimally with respect to the feature used 
for large customer electricit de france the french national electric power company store every the amount of electric power they consume for each customer these measure lead to curve called electric load curve clustering of electric load curve is a key problem for understanding the behavior of these customer several method have been used but kohonen map give a very nice solution to this problem thanks to the visualization of the map the work we present here describes a software for interactive construction and interpretation of a kohonen map clustering in the case of curve the user can run the kohonen map clustering visualize the map see external characteristic of curve linked to each cell of the map find the cell figuring curve having some chosen external characteristic define class of cell add comment on cell user interaction is largely based on mouse clicking on the map cell and on bar of barcharts figuring external characteristic of the curve this software is not dedicated to electric load curve analysis but can be used on any type of curve for instance to analyze time series in finance 
we introduce arc lh a new algorithm for improvement of ann classifierperformance which measure the importance of pattern byaggregated network output error on several artificial benchmarkproblems this algorithm compare favorably with other resampleand combine technique introductionthe training of artificial neural network anns is usually a stochastic and unstableprocess a the weight of the network are initialized at random and trainingpatterns are presented in random 
unsupervised learning algorithm based on convex and conic encoders are proposed the encoders find the closest convex or conic combination of basis vector to the input the learning algorithm produce basis vector that minimize the reconstruction error of the encoders the convex algorithm develops locally linear model of the input while the conic algorithm discovers feature both algorithm are used to model handwritten digit and compared with vector quantization and principal component 
conventional binary classification tree such a cart either splitthe data using axis aligned hyperplanes or they perform a computationallyexpensive search in the continuous space of hyperplaneswith unrestricted orientation we show that the limitation of theformer can be overcome without resorting to the latter for everypair of training data point there is one hyperplane that is orthogonalto the line joining the data point and bisects this line suchhyperplanes are plausible 
bayda is a software package for flexible data analysis in predictive data mining task the mathematical model underlying the program is based on a simple bayesian network the naive bayes classifier it is well known that the naive bayes classifier performs well in predictive data mining ta k when compared to approach using more complex model however the model make strong independence assumption that are frequently violated in practice for this reason t he bayda software also provides a feature selection scheme which can be used for analyzing the problem domain and for improving the prediction accuracy of the model constructed by bayda the scheme is based on a novel bayesian feature selection criterion introduced in this paper the suggeste d criterion is inspired by the cheeseman stutz approximation for computing the marginal likelihood of bayesian network with hidden variable the empirical result with several widel yused data set demonstrate that the automated bayesian feature selection scheme can dramatically decrease the number of relevant feature and lead to substantial improvement in prediction accuracy 
this paper investigates a brute force technique for mining classification rule from large data set we emplo y an association rule miner enha nced with new pruning strategy to control combinatorial explosion in th e number of candidate counted with each database pa s the approach effectively and efficiently extract h igh confidence classification rule that apply to most if not all of the data in several classification benchmark 
foveal vision feature imagers with graded acuity coupled with context sensitive sensor gaze control analogous to that prevalent throughout vertebrate vision foveal vision operates more efficiently than uniform acuity vision because resolution is treated a a dynamically allocatable resource but requires a more refined visual attention mechanism we demonstrate that reinforcement learning rl significantly improves the performance of foveal visual attention and of the overall vision system for the task of model based target recognition a simulated foveal vision system is shown to classify target with fewer fixation by learning strategy for the acquisition of visual information relevant to the task and learning how to generalize these strategy in ambiguous and unexpected scenario condition 
image are ambiguous at each of many level of a contextual hierarchy nevertheless the high level interpretation of most scenesis unambiguous a evidenced by the superior performance of human this observation argues for global vision model such a deformabletemplates unfortunately such model are computationallyintractable for unconstrained problem we propose a compositionalmodel in which primitive are recursively composed subjectto syntactic restriction to form 
many reinforcement learning algorithm likeq learning or r learning correspond toadaptative method for solving markoviandecision problem in infinite horizon whenno model is available in this article weconsider the particular framework of nonstationaryfinite horizon markov decisionprocesses after establishing a relationshipbetween the finite horizon total reward criterionand the average reward criterion infinite horizon we define qh learning andrh learning for finite horizon 
the number and the size of spatial database e g for geomarketing traffic control or environmental study are rapidly growing which result in an increasing need for spatial data mining in this paper we present new algorithm for spatial characterization and spatial trend analysis for spatial characterization it is important that class membership of a database object is not only determined by it non spatial attribute but also by the attribute of object in it neighborhood in spatial trend analysis pattern of change of some non spatial attribute in the neighborhood of a database object are determined we present several algorithm for these task these algorithm were implemented within a general framework for spatial data mining providing a small set of database primitive on top of a commercial spatial database management system a performance evaluation using a real geographic database demonstrates the effectiveness of the proposed algorithm furthermore we show how the algorithm can be combined to discover even more interesting spatial knowledge 
the comparison of two data set can reveal a great deal of information about the time varying nature of an observed process for example suppose that the point in a data set represent a customer s activity by their location in n dimensional space a comparison of the distribution of point in two such data se t can indicate how the customer activity ha changed between the observation period other application include data integrity checking an unexpected change in a data set can indicate a problem in the data collection process we propose a fast inexpensive method for comparing massive high dimensional data set that doe not make any distributional assumption the method adapts the power of classical statistic for use on complex high dimensional data set we generate a map of the data set a datasphere and compare data set by comparing their dataspheres the datasphere can be generated in two pass over the data set stored in a database and aggregated at multiple level we illustrate the use of our set comparison technique with an example analysis of data set drawn from atg t data warehouse 
we propose a novel approach to automatically growing and pruninghierarchical mixture of expert the constructive algorithm proposedhere enables large hierarchy consisting of several hundredexperts to be trained effectively we show that hme s trained byour automatic growing procedure yield better generalization performancethan traditional static and balanced hierarchy evaluationof the algorithm is performed on vowel classificationand within a hybrid version of the janus 
in recent year there ha been a flurry of work on learning probabilistic belief network current state of the art method have been shown to be successful for two learning scenario learning both network structure and parameter from completedata and learning parameter for a fixed network from incomplete data that is in the presence of missing value or hidden variable however no method ha yet been demonstrated to effectively learn network structure from incomplete data in this paper we propose a new method for learning network structure from incomplete data this method is based on an extension of the expectation maximization em algorithm for model selection problem that performs search for the best structure inside the em procedure we prove the convergence of this algorithm and adapt it for learning belief network we then describe how to learn network in two scenario when the data contains missing value and in the presence of hidden variable we provide experimental result that show the effectiveness of our procedure in both scenario 
the limitation of using self organizing map som for eitherclustering vector quantization vq or multidimensional scaling md are being discussed by reviewing recent empirical findingsand the relevant theory som s remaining ability of doing both vqand md at the same time is challenged by a new combined techniqueof online k mean clustering plus sammon mapping of thecluster centroid som are shown to perform significantly worse interms of quantization error in recovering the 
in cellular telephone system an important problem is to dynamically allocatethe communication resource channel so a to maximize service ina stochastic caller environment this problem is naturally formulated a adynamic programming problem and we use a reinforcement learning rl method to find dynamic channel allocation policy that are better thanprevious heuristic solution the policy obtained perform well for a broadvariety of call traffic pattern we present result on a 
very large database with skewed class distribution and non unlform cost per error are not uncommon in real world data mining task we devised a multi classifier meta learning approach to address these three issue our empirical result from a credit card fraud detection task indicate that the approach can significantly reduce loss due to illegitimate transaction 
attribute oriented induction is a set oriented database mining method which generalizes thetask relevant subset of data attribute by attribute compress it into a generalized relation andextracts from it the general feature of data in this chapter the power of attribute orientedinduction is explored for the extraction from relational database of different kind of pattern including characteristic rule discriminant rule cluster description rule and multiple levelassociation 
in this paper we analyze and extend a class of adaptive networksfor second order blind decorrelation of instantaneous signal mixture firstly we compare the performance of the decorrelationneural network employing global knowledge of the adaptive coefficientsin with a similar structure whose coefficient areadapted via local output connection in through statisticalanalyses the convergence behavior and stability bound for thealgorithms step size are studied and derived 
feature selection is a data preprocessing step for classification and data mining task traditionally feature selection is done by selecting a minimum number of feature that determine the class label i e by the horizontal compactness of data in this paper we propose a new selection criterion that aim at the vertical compactness of data in particular we select a subset of feature that yield the least number of projected instance while determining the class label a hybrid search that is partially dfs and partially bfs is proposed to exploit the pruning potential of the problem we compare the result induced by c before and after the feature selection 
clustering is important in many field including manufacturing biology finance and astronomy mixture model are a popular approachdue to their statistical foundation and em is a very popular methodfor finding mixture model em however requires many access ofthe data and thus ha been dismissed a impractical e g zhang ramakrishnan amp livny for data mining of enormous datasets we present a new algorithm based on the multiresolution kd treesof moore schneider amp 
recent experimental data indicate that the strengthening or weakening ofsynaptic connection between neuron depends on the relative timing ofpreand postsynaptic action potential ahebbian synaptic modificationrule based on these data lead to a stable state in which the excitatory andinhibitory input to a neuron are balanced producing an irregular patternof firing it ha been proposed that neuron in vivo operate in such amode introductionhebbian modification of network 
prediction of lifetime of dynamically allocated object can be usedto improve time and space efficiency of dynamic memory managementin computer program barrett and zorn used a simplelifetime predictor and demonstrated this improvement on a varietyof computer program in this paper we use decision tree to dolifetime prediction on the same program and show significantlybetter prediction our method also ha the advantage that duringtraining we can use a large number of 
in this paper we propose a technique to incorporate contextual inform ation into object classification in the real world there are case where the identity of an object is ambiguous due to the noise in the measurement based on which the classification should be made it is helpful to reduce the ambiguity by utilizing extra information referred to a conte xt which in our case is the identity of the accompanying object this technique is applied to white blood cell classification comparison are made against no context approach which demonstrates the superior classification performance achieved by using context in our particular application it significantly reduces false alarm rate and thus greatly reduces the cost due to expensive clinical test 
a rich body of data exists showing that recollection of specific information make an important contribution to recognition memory which is distinct from the contribution of familiarity and is not adequately captured by existing unitary memory model furthermore neuropsychological evidence indicates that recollection is subserved by the hippocampus we present a model based largely on known feature of hippocampal anatomy and physiology that account for the following key characteristic of recollection false recollection is rare i e participant rarely claim to recollect having studied nonstudied item and increasing interference lead to le recollection but apparently doe not compromise the quality of recollection i e the extent to which recollected information veridically reflects event that occurred at study 
binocular rivalry is the alternating percept that can result when the two eye see different scene recent psychophysical evidence support an account for one component of binocular rivalry similar to that for other bistable percept we test the hypothesis that alternation can be generated by competition between top down cortical explanation for the input rather than by direct competition between the input recent neurophysiological evidence show that some binocular neuron are modulated with the changing percept others are not even if they are selective between the stimulus presented to the eye we extend our model to a hierarchy to address these effect 
this paper describes a new technique for solving multiclass learning problem by combining freund and schapire s boosting algorithm with the main idea of dietterich and bakiri s method of error correcting output code s ecoc boosting is a general method of improving the accuracy of a given base or weak learning algorithm ecoc is a robust method of solving multiclass learning problem by reducing to a sequence of two class problem we show that our new hybrid method ha advantage of both like ecoc our method only requires that the base learning algorithm work on binary labeled data like boosting we prove that the method come with strong theoretical guarantee on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing although previous method were known for boosting multiclass problem the new method may be significantly faster and require le programming effort in creating the base learning algorithm we also compare the new algorithm experimentally to other voting method 
our work offer both a solution to the problem of finding functional dependenciesthat are distorted by noise and to the open problem of efficiently finding strong i e highly compressive partial determination per se briefly we introduce a restrictedform of search for partial determination which is based on functional dependency focusing attention on solely partial determination derivable from overfittingfunctional dependency enables efficient search for strong partial 
we introduce a knowledge based approach to deep knowledge discovery from real world natural language text data mining data interpretation and data cleaning are all incorporated in cycle of quality based terminological reasoning process the methodology we propose identifies new knowledge item and assimilates them into a continuously updated domain knowledge base 
we analyze critically the use of classification accuracy to compare classifier on natural data set providing a thorough investigation using roc analysis standard machine learning algorithm and standard benchmark data set the result raise serious concern about the use of accuracy for comparing classifier and drawinto question the conclusion that can be drawn from such study in the course of the presentation we describe and demonstrate what we believe to be the proper use of roc 
two literature or set of article are complementary if considered together they can reveal useful information of scientik interest not apparent in either of the two set alone of particular interest are complementary literature that are also mutually isolated and noninteractive they do not cite each other and are not co cited in that case the intriguing possibility akrae that thm tfnrmnt nn n wd hv mwnhxno them nnvnl lyww u c yll i l su l uy s u b s y ayj a y u during the past decade we have identified seven example of complementary noninteractive structure in the biomedical literature each structure led to a novel plausible and testable hypothesis that in several case wa subsequently corroborated by medical researcher through clinical or laboratory investigation we have also developed tested and described a systematic computer sided approach to iinding and identifying complementary noninteractive literature 
many factory optimization problem frominventory control to scheduling and reliability can be formulated a continuous timemarkov decision process a primary goalin such problem is to find a gain optimalpolicy that minimizes the long run averagecost this paper describes a new averagerewardalgorithm called smart for findinggain optimal policy in continuous timesemi markov decision process the paperpresents a detailed experimental study ofsmart on a large unreliable 
rise domingo in press is a rule induction algorithm that proceeds by gradually generalizing rule starting with one rule per example this ha several advantage compared to the more common strategy of gradually specializing initially null rule and ha been shown to lead to significant accuracy gain over algorithm like cgrules and cn in a large number of application domain however rise s running time like that of other rule induction algorithm is quadratic in the number of example making it r l l a 
this paper argues that for many domain wecan expect credit assignment method thatuse actual return to be more effective forreinforcement learning than the more commonlyused temporal difference method wepresent analysis and empirical evidence fromthree set of experiment in different domainsto support this claim a new algorithm wecall c trace a variant of the p trace rl algorithmis introduced and some possible advantagesof using algorithm of this type are 
almost all the work in average reward reinforcementlearning arl so far ha focusedon table based method which do notscale to domain with large state space inthis paper we propose two extension to amodel based arl method called h learningto address the scale up problem we extendh learning to learn action model and rewardfunctions in the form of bayesian network and approximate it value function using locallinear regression we test our algorithmson several scheduling task 
many system that learn from example expressthe learned concept a a disjunction thosedisjuncts that cover only a few example arereferred to a small disjuncts the problem withsmall disjuncts is that they have a much highererror rate than large disjuncts but are necessary toachieve a high level of predictive accuracy thispaper investigates the effect of noise on smalldisjuncts in particular we show that when noiseis added to two real world domain a significant and 
multiplicative weight updating algorithm such a winnow have been studied extensivelyin the colt literature but only recently have people started to use them in application in this paper we apply a winnow based algorithm to a task in natural language contextsensitivespelling correction this is the task of fixing spelling error that happen to resultin valid word such a substituting to for too casual for causal and so on previousapproaches to this problem have been 
in the near future nasa intends to explore various region of our solar system using robotic device such a rover spacecraft airplane and or balloon such platform will carry imaging device and a variety of analytical instrument intended to evaluate the chemical and mineralogical nature of the environment s that they encounter the imaging and or spectroscopic device will acquire tremendous volume of data the communication band width are restrictive enough so that only a small portion of these data can actually be sent to earth the aim of this research wa to develop a system which analysis rock spectrum to automatically determine which spectrum are interesting and to compress the spectral data for communication to earth in the research we report here we classify laboratory data using clustering technique acpro an enhanced version of autoclass and provide the planetary scientist with a rapid visually oriented method of evaluating the underlying chemical and mineralogical information contained within the cluster we show how clustering can be used to identify interesting rock sample and estimate the compression that using such a system can achieve 
in this paper we study the combination of two powerful approach evolutionary topology optimization enzo and tempoal difference learning td which is up to our knowledge the first time temporal difference learning wa proven to be a well suited technique for learning strategy for solving reinforcement problem based on neural network model whereas evolutionary topology optimization is concurrently the most efficient network optimization technique on two benchmark a labyrinth 
we introduce a novel framework for simultaneous structure and parameter learning in hidden variable conditional probability model based on an entropic prior and a solution for it maximum a posteriori map estimator the map estimate minimizes uncertainty in all respect cross entropy between model and data entropy of the model entropy of the data s descriptive statistic iterative estimation extinguishes weakly supported parameter compressing and sparsifying the model trimming operator accelerate this process by removing excess parameter and unlike most pruning scheme guarantee an increase in posterior probability entropic estimation take a overcomplete random model and simplifies it inducing the structure of relation between hidden and observed variable applied to hidden markov model hmms it find a concise finite state machine representing the hidden structure of a signal we entropically model music handwriting and video time series and show that the resulting model are highly concise structured predictive and interpretable surviving state tend to be highly correlated with meaningful partition of the data while surviving transition provide a low perplexity model of the signal dynamic an entropic prior in entropic estimation we seek to maximize the information content of parameter for conditional probability parameter value near chance add virtually no information to the model and are therefore wasted degree of freedom in contrast parameter near the extremum are informative because they impose strong constraint on the class of signal accepted by the model in bayesian term our prior should assert that parameter that do not reduce uncertainty are improbable we can capture this intuition in a surprisingly simple form for a model of conditional probability we write 
many real world kdd expedition involve investigation of relationship between variable in different heterogeneous database we present a dynamic programming technique for linking record in multiple heterogeneous database using loosely defined field that allow free style verbatim entry we develop an interestingness measure based on non parametric randomization test which can be used for mining potentially useful relationship among variable this measure us distributional characteristic of historical event hence accommodating variable length record in a natural way a an illustration we include a successful application of the proposed methodology to a real world data mining problem at lucent technology 
most learning algorithm work most effectivelywhen their training data contain completelyspecified labeled sample in manydiagnostic task however the data will includethe value of only some of the attribute we model this a a blocking processthat hide the value of those attribute fromthe learner while blocker that remove thevalues of critical attribute can handicap alearner this paper instead focus on blockersthat remove only irrelevant attribute value i e value 
we introduce coactive learning a a distributed learning approach to data mining in networked and distributed database the coactive learning algorithm act on independent data set and cooperate by communicating training information which is used to guide the algorithm hypothesis construction the exchanged training information is limited to example and response to example it is shown that coactive learning can offer a solution to learning on very large data set by allowing multiple coacting algorithm to learn in parallel on subset of the data even if the subset are distributed over a network coactive learning support the construction of global concept description even when the individual learning algorithm are provided with training set having biased class distribution finally the capability of coactive learning are demonstrated on artificial noisy domain and on real world domain data with sparse class representation and unknown attribute value 
the support vector sv method wa recently proposed for estimatingregressions constructing multidimensional spline andsolving linear operator equation vapnik in this presentationwe report result of applying the sv method to these problem introductionthe support vector method is a universal tool for solving multidimensional functionestimation problem initially it wa designed to solve pattern recognition problem where in order to find a decision rule with good 
we present aa algorithm for mining association rule from relational table containing numeric and categorical attribute the approach is to merge adjacent interval of numeric value in a bottom up manner on the basis of maximizing the interestingness of a set of association rule a modification of the b tree is adopted for performing this task efficiently the algorithm take o kn i o time where k is the number of attribute and n is the number of row in the table we evaluate the effectiveness of producing good interval 
we present new algorithm for parameter estimation of hmms by adapting a framework used for supervised learning we construct iterative algorithm that maximize the likelihood of the observation while also attempting to stay close to the current estimated paramet er we use a bound on the relative entropy between the two hmms a a distance measure between them the result is new iterative training algo rithms which are similar to the em baum welch algorithm for training hmms the proposed algorithm are composed of a step similar to the expectation step of baum welch and a new update of the parameter which replaces the maximization re estimation step the algorithm take only negligibly more time per iteration and an approximated version us the same expectation step a baum welch we evaluate experimentally the new algorithm on synthetic and natural speech pronunciation data for sparse model i e model with relatively small number of non zero parameter the proposed algorithm require significantly fewer iteration preliminary we use the number from to to name the state of an hmm state is a special initial state and state is a special final state any state sequence denoted by start with the initial state but never return to it and end in the final state observation symbol are also number in and observation sequence are denoted by a discrete output hidden markov model hmm is parameterized by two matrix and the first matrix is of dimension and denotes the probability of moving from state to state the second matrix is of dimension and is the probability of outputting symbol at state the set of parameter of an hmm is denoted by the initial state distribution vector is represented by t he first row of 
we present exact analytical equilibrium solution for a class of recurrent neural network model with both sequential and parallel neuronal dynamic in which there is a tunable competition between nearestneighbour and long range synaptic interaction this competition is found to induce novel coexistence phenomenon a well a discontinuous transition between pattern recall state cycle and non recall state 
this paper present a first step towards a unifying framework for knowledge discovery in database we describe fink between data milfing knowledge discovery and other related field we then define the kdd process and basic data mining algorithm discus application issue and conclude with an analysis of challenge facing practitioner in the field 
multiple instance learning is a variation on supervised learning where the task is to learn a concept given positive and negative bag of instance each bag may contain many instance but a bag is labeled positive even if only one of the instance in it fall within the concept a bag is labeled negative only if all the instance in it are negative we describe a new general framework called diverse density for solving multiple instance learning problem we apply this framework to learn a simple description of a person from a series of image bag containing that person to a stock selection problem and to the drug activity prediction problem 
many practical real life application of conceptlearning are impossible to address withouttaking into consideration the backgroundof the concept it frame of reference andthe particual situation and circumstancesof it occurence shortly it context eventhough the phenomenon of context ha beentreated by philosopher and cognitive scientist it deserves more attention in themachine learning community introductionsuppose that we are dealing with a classification task and that 
exploratory data analysis is a process of sifting through data in search of interesting information or pattern analyst current tool for exploring data include database management system statistical analysis package data mining tool visualization tool and report generator since the exploration process seek the unexpected in a data driven manner it is crucial that these tool are seamlessly integrated so analyst can flexibly select and compose tool to use at each stage of analysis few system have integrated all these capability either architecturally or at the user interface level visage s information centric approach allows coordination among multiple application user interface it us an architecture that keep track of the mapping of visual object to information in shared database one result is the ability to perform direct manipulation operation such a drag and drop transfer of data among application this paper describes visage s visual query language and visualization tool and illustrates their application to several stage of the exploration process creating the target dataset data cleaning and preprocessing data reduction and projection and visualization of the reduced data unlike previous integrated kdd system interface direct manipulation is used pervasively and the visualization are more diverse and can be customized automatically a needed coordination among all interface object simplies iterative modication of decision at any stage 
learning to predict rare event from sequence of e vent with categorical feature is an important real wor ld problem that existing statistical and machine learn ing method are not well suited to solve this paper d escribes timeweaver a genetic algorithm based machine learning system that predicts rare event by identifying pre dictive temporal and sequential pattern timeweaver is ap plied to the task of predicting telecommunication equipment failure from alarm message and is shown to outperf orm existing learning method 
it is frequently the case that data mining is carried out in an environment which contains noisy and missing data this a particularly likely to be true when the data were originally collected for a different purpose a is often the case in data warehousing the provision of tool to handle such imperfection in data ha been identified a a challenging area for knowledge discovery in database fayyad et al previous work ha provided some method of handling such data using machine learning or statistical method to predict likely value to replace the missing or noisy value generalised database have been proposed to provide intelligent way of storing and retrieving data frequently data are imprecise i e one is not certain about the specific value of an attribute but only that it take a value which is a member of a set of possible value such data have previously been discussed a a basis of attribute oriented induction for data mining han and fu this approach ha been shown to provide a powerful methodology for the extraction of different kind of pattern from relational database it is therefore important that appropriate functionality is provided for database system to handle such information the author consider the problem of aggregation for such data 
when using machine learning technique for knowledgediscovery output that is comprehensible to a humanis a important a predictive accuracy we introducea new algorithm set gen that improves thecomprehensibility of decision tree grown by standardc without reducing accuracy it doe this by usinggenetic search to select the set of input featuresc is allowed to use to build it tree we test setgen on a wide variety of real world datasets and showthat set gen tree are 
partially observable markov decision problem pomdps recently received a lot of attentionin the reinforcement learning community no attention however ha been paidto levin s universal search through programspace l which is theoretically optimal forum wide variety of search problem includingmany pomdps experiment in this paperfirst show that l can solve partially observablemazes pom involving many morestates and obstacle than those solved by variousprevious author 
locally weighted polynomial regression lwpr is a popular instance based algorithmfor learning continuous non linearmappings for more than two or three inputsand for more than a few thousand datapointsthe computational expense of predictionsis daunting we discus drawbackswith previous approach to dealing with thisproblem and present a new algorithm basedon a multiresolution search of a quicklyconstructibleaugmented kd tree withoutneeding to rebuild the tree we can 
we have discovered a new scheme to represent the fisher informationmatrix of a stochastic multi layer perceptron based onthis scheme we have designed an algorithm to compute the inverseof the fisher information matrix when the input dimensionn is much larger than the number of hidden neuron the complexityof this algorithm is of order o n while the complexity ofconventional algorithm for the same purpose is of order o n the inverse of the fisher information matrix is 
completely parallel object recognition is np complete achievinga recognizer with feasible complexity requires a compromise betweenparallel and sequential processing where a system selectivelyfocuses on part of a given image one after another successivefixations are generated to sample the image and these sample areprocessed and abstracted to generate a temporal context in whichresults are integrated over time a computational model based on apartially recurrent feedforward network 
efficient discover of association rule in large database is a we studied problem and several ap y proaches have been proposed however it is non trivial to maintain the association rule current when the database is updated since such update could invalidate existing rule or introduce new rule in this paper we propose an incremental updating technique btied on tie ittive border for the maintenance of association ru e when new transaction data is added to f or deleted from a transaction database an important feature of our algorithm is that it requires a full scan exactly one of the whole database only if the database update cause the negative border of the set of large itemsets to expand 
severe contamination of electroencephalographic eeg activity by eye movement blink muscle heart and line noise is a serious problem for eeg interpretation and analysis rejecting contaminated eeg segment result in a considerable loss of information and may be imlractical for clinical data manv method have been proposed to remove eye movement and blink artifact from eeg recording often regression in the time or frequency domain is performed on simultaneous eeg and electrooculographic eog recording to derive parameter characterizing the appearance and spread of eog artifact in the eeg channel however eog record also contain brain signal i so regressing out eog activity inevitably involves subtracting a portion of the relevant eeg signal from each recording a well regression cannot be used to remove muscle noise or line noise since these have no reference channel here we propose a new and generally applicable method for removing a wide varietv of artifact from eeg record the method is based on an extended version of a previous independent component analysis ica algorithm for performing blind source separation on linear mixture of indewendent source signal with either sub gaussian or super gaussian distribution our result show that ica can effectiveiy detect separate and remove activity in eeg record from a wide variety of artifactual source with result comparing favorably to those obtained using regression based method 
a safe control of genetic evolution consists inpreventing past error of evolution from beingrepeated this could be done by keepingtrack of the history of evolution but maintainingand exploiting the complete historyis intractable this paper investigates the use of machinelearning ml in order to extract manageableinformation from this history more precisely induction from example of past trialsand error provides rule discriminating errorsfrom successful trial such rule 
the problem of discovering association rule in large database ha received considerable research attention much research ha examined the exhaustive discovery of all association rule involving positive binary literal e g agrawal et al other research ha concerned finding complex association rule for high arity attribute such a cn clark and niblett complex association rule are capable of representing concept such a purchasedchips true and purchasedsoda false and area northeast and customertype occasional agerange young but their generality come with severe computational penalty intractable number of precondition can have large support here we introduce new algorithm by which a sparse data structure called the adtree introduced in moore and lee can accelerate the finding of complex association rule from large datasets the adtree us the algebra of probability table to cache a dataset s sufficient statistic within a tractable amount of memory we first introduce a new adtree algorithm for quickly counting the number of record that match a precondition we then show how this can be used in accelerating exhaustive search for rule and for accelerating cn type algorithm result are presented on a variety of datasets involving many record and attribute 
the main aim of this paper is to suggest multi criterion based metric that can be used a comparators for an objective evaluation of data mining algorithm m algorithm each dm algorithm is characterized generally by some positive and negative property when it is applied to certain domain example of property are the accuracy rate understandability interpretability of the generated result and stability space and time complexity and maintenance cost can be considered a negative property by now there is no methodology to consider all of these property simultaneously and use them for a comprehensive evaluation of dm algorithm most of available study in literature use only the accuracy rate a a unique criterion to compare the performance of dmalgorithms and ignore the other property our suggested approach however can take into account all available positive and negative characteristic of dm algorithm and can combine them to construct a unique evaluation metric this new approach is based on dea data envelopment analysis we have applied this approach to evaluate dm algorithm in domain the result are analyzed and compared with the result of alternative approach 
the simple bayesian classifier sbc is commonly thought to assume that attribute are independent given the class but this is apparently contradicted by the surprisingly good performance it exhibit in many domain that contain clear attribute dependence no explanation for this ha been proposed so far in this paper we show that the sbc doe not in fact assume attribute independence and can be optimal even when this assumption is violated by a wide margin the key to this finding lie in 
until recently artificial intelligence researcher have frowned uponthe application of probability propagation in bayesian belief networksthat have cycle the probability propagation algorithm isonly exact in network that are cycle free however it ha recentlybeen discovered that the two best error correcting decoding algorithmsare actually performing probability propagation in beliefnetworks with cycle in advance in neural information processing system mit press cambridge 
this statistical method compare in real time the sequence of command given by each user to a profile of that user s past behavior we use the fisher score statistic to test the null hypothesis that the observed command transition probability come from a profiled transition matrix the alternative hypothesis is formed from a principal component analysis of historical difference between the transition probability of all other user and those of the user being tested the calculation can be structured so that only a few dozen arithmetic operation are needed to update an online test statistic after each submitted command the theoretical statistical property of the test such a false positive and false negative rate are computable under the assumption of the markov process model based on a population of research user on a single computer test data from each user are used to challenge the profile of every user the test had sufficient statistical power to successfully discriminate between almost every pair of user based on a sample size equivalent to a single day s usage of an average user 
in tanner and mead implemented an interesting constraint satisfaction circuit for global motion sensing in avlsi we report here a new and improved avlsi implementation that provides smooth optical flow a well a global motion in a two dimensional visual field the computation of optical flow is an ill posed problem which express itself a the aperture problem however the optical flow can be estimated by the use of regularization method in which additional constraint are introduced in term of a global energy functional that must be minimized we show how the algorithmic constraint of horn and schunck on computing smooth optical flow can be mapped onto the physical constraint of an equivalent electronic network motivation the perception of apparent motion is crucial for navigation knowledge of local motion of the environment relative to the observer simplifies the calculation of important task such a time to contact or focus of expansion there are several method to compute optical flow they have the common problem that their computational load is large this is a severe disadvantage for autonomous agent whose computational power is restricted by energy size and weight here we show how the global regularization approach which is necessary to solve for the ill posed nature of computing optical flow can be formulated a a local feedback constraint and implemented a a physical analog device that is computationally efficient 
we describe an application of dogma a ga based theory revision system to mdl based rule enhancement in supervised concept learning the system take a input classification data and a rule based classification theory produced by some rule based learner and build a second hopefully more accurate model of the data unlike most theory revision system dogma doesn t revise the initial rule but build instead a completely new theory using stochastic sampling and adaptation of the initial rule the search for the new model is guided by a mdl based complexity measure the proposed methodology offer a partial solution both to the local mimima trap of fast greedy rule based concept learner and to the time complexity problem of ga based concept learner a an example we show how the system improves rule produced by c rule 
in this paper we examine the problem of estimating the parameter of a multinomial distribution over a large number of discrete outcome most of which do not appear in the training data we analyze this problem from a bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcome constitute only a small subset of the possible outcome we show how to efficiently perform exact inference with this form of hierarchical prior and compare our method to standard approach and demonstrate it merit 
kernel pca a a nonlinear feature extractor ha proven powerful a apreprocessing step for classification algorithm but it can also be consideredas a natural generalization of linear principal component analysis this give rise to the question how to use nonlinear feature fordata compression reconstruction and de noising application commonin linear pca this is a nontrivial task a the result provided by kernelpca live in some high dimensional feature space and need not 
in this paper we present tdleaf a variation on the td algorithm that enables it to be used in conjunction with minimax search we present some experiment in which our chess program knightcap used tdleaf to learn it evaluation function while playing on the free ineternet chess server fics fics onenet net it improved from a rating to a rating in just game and day of play we discus some of the reason for this success and also the relationship between our result and tesauro s result in backgammon 
we study several statistically and biologically motivated learning rule using the same visual environment one made up of natural scene and the same single cell neuronal architecture this allows u to concentrate on the feature extraction and neuronal coding property of these rule included in these rule are kurtosis and skewness maximization the quadratic form of the bienenstock cooper munro bcm learning rule and single cell independent component analysis using a structure removal method we demonstrate that receptive field developed using these rule depend on a small portion of the distribution we find that the quadratic form of the bcm rule behaves in a manner similar to a kurtosis maximization rule when the distribution contains kurtotic direction although the bcm modification equation are computationally simpler 
the work of mannila et al of finding frequent episode in sequence is extended to finding temporal logic pattern in temporal database it is argued that temporal logic provides an appropriate formalism for expressing temporal pattern defined over categorical data it is also proposed to use temporal logic programming a a mechanism for the discovery of frequent pattern expressible in temporal logic it is explained in the paper how frequent temporal pattern can be discovered by constructing temporal logic program to test these method temporal logic program were constructed for certain class of pattern and were implemented in ops 
practical clustering algorithm require multiple data scan to achieve convergence for large database these scan become prohibitively expensive we present a scalable clustering framework applicable to a wide class of iterative clustering we require at most one scan of the database in this work the framework is instantiated and numerically justified with the popular k mean clustering algorithm the method is based on identifying region of the data that are compressible region that must be maintained in memory and region that are discardable the algorithm operates within the confines of a limited memory buffer empirical result demonstrate that the scalable scheme outperforms a sampling based approach in our scheme data resolution is preserved to the extent possible based upon the size of the allocated memory buffer and the fit of current clustering model to the data the framework is naturally extended to update multiple clustering model simultaneously we empirically evaluate on synthetic and publicly available data set 
tile analysis of tile massive data set collected by scientific instrument demand automation a a prerequisite to analysis there is an urgent need to create an intermediate level at which scientist can operate effectively isolating them from the massive size and harnessing human analysis capability to focus on task in which machine do not even renmtely approach human namely creative data analysis theory and hypothesis formation and drawing insight into underlying phe mmena we give an overview of the main issue in the exploitation of scientific data set present five c se study where kdd tool play important and enabling role and conclude with fi ture challenge for data mining and kdd technique in science data analysis 
this paper present a neural network architecture that can manage structureddata and refine knowledge base expressed in a first order logic language thepresented framework is well suited to classification problem in which concept descriptionsdepend upon numerical feature of the data in fact the main goal ofthe neural architecture is that of refining the numerical part of the knowledge base without changing it structure in particular we discus a method to translate a setof 
multiple instance learning is a way of modelingambiguity in supervised learning example each example is a bag of instance butonly the bag is labeled not the individualinstances a bag is labeled negative if all theinstances are negative and positive if at leastone of the instance in positive we applythe multiple instance learning framework tothe problem of learning how to classify naturalimages image are inherently ambiguoussince they can represent many different 
this paper discus a probabilistic model based approach to clustering sequence using hidden markov model hmms the problemcan be framed a a generalization of the standard mixturemodel approach to clustering in feature space two primary issuesare addressed first a novel parameter initialization procedure isproposed and second the more difficult problem of determiningthe number of cluster k from the data is investigated experimentalresults indicate that the proposed 
we explore method for incorporating prior knowledge about a problem at hand in support vector learning machine we show that both invariance under group transformation and prior knowledge about locality in image can be incorporated by constructing appropriate kernel function 
the w s wake sleep algorithm is a simple learning rule for the model with hidden variable it is shown that this algorithm can be applied to a factor analysis model which is a linear version of the helmholtz machine but even for a factor analysis model the general convergence is not proved theoretically in this article we describe the geometrical understanding of the w s algorithm in contrast with the em expectationmaximization algorithm and the em algorithm a the result we prove the convergence of the w s algorithm for the factor analysis model we also show the condition for the convergence in general model 
many combinatorial optimization algorithm have no mechanism for capturing inter parameter dependency however modeling such dependency may allow an algorithm to concentrate it sampling more effectively on region of the search space which have appeared promising in the past we present an algorithm which incrementally learns pairwise probability distribution from good solution seen so far us these statistic to generate optimal in term of maximum likelihood dependency tree to model these distribution and then stochastically generates new candidate solution from these tree we test this algorithm on a variety of optimization problem our result indicate superior performance over other tested algorithm that either do not explicitly use these dependency or use these dependency to generate a more restricted class of dependency graph 
in this paper we describe our experience of using simulatedannealing and genetic algorithm to performdata mining for a large financial service sector company we first explore the requirement that datamining system must meet to be useful in most realcommercial environment we then look at some ofthe available data mining technique including ourown heuristic technique and how they perform withrespect to those requirement the result of applyingthe technique to two 
we generalize a recent formalism to describe the dynamic of supervised learning in layered neural network in the regime where data recycling is inevitable to the case of noisy teacher our theory generates prediction for the evolution in time of trainingand generalization error s and extends the class of mathematically solvable learning process in large neural network to those complicated situation where overfitting occurs 
we present a method for automatically constructing macro action from scratch from primitive action during the reinforcement learning process the overall idea is to reinforce the tendency to perform action after action if such a pattern of action ha been rewarded we test the method on a bicycle task the car on the hill task the race track task and some grid world task for the bicycle and race track task the use of macro action approximately half the learning time while for one of the grid world task the learning time is reduced by a factor of the method did not work for the car on the hill task for reason we discus s in the conclusion 
adding example of the majority class to thetraining set can have a detrimental effect onthe learner s behavior noisy or otherwise unreliableexamples from the majority class canoverwhelm the minority class the paper discussescriteria to evaluate the utility of classifiersinduced from such imbalanced trainingsets give explanation of the poor behaviorof some learner under these circumstance and suggests a a solution a simple techniquecalled one sided selection of 
abstract we derive the correspondence between regularization operator used in regularization network and hilbert schmidt kernel appearing in support vector machine more specifically we prove that the green s function associated with regularization operator are suitable support vect or kernel with equivalent regularization property a a by product we show that a large number of radial basis function namely conditionally positive definite function may be used a support vector kernel 
generating good production quality plan isan essential element in transforming plannersfrom research tool into real world application but one that ha been frequentlyoverlooked in research on machine learningfor planning this paper describes quality an architecture that automatically acquiresoperational quality improving controlknowledge given a domain theory a domainspecificmetric of plan quality and problemswhich provide planning experience theframework includes two 
feature selection can be defined a a problemof finding a minimum set of m relevant attributesthat describes the dataset a well asthe original n attribute do where m n after examining the problem with both theexhaustive and the heuristic approach to featureselection this paper proposes a probabilisticapproach the theoretic analysis andthe experimental study show that the proposedapproach is simple to implement andguaranteed to find the optimal if resourcespermit 
data mining ha informally been introduced a large scale search for interesting pattern in data it is often an explorative task iteratively performed within the process of knowledge discovery in database in this process interactive visualization technique are also successfully applied for data exploration we deal with the synergy of these two complemental approach whereas datamining typically relies on strategy for systematic search in large hypothesis space guided by the autonomous evaluation of statistical test interactive visualization activates the visual capacity of an analyst to identify pattern that may also stimulate the further direction of the exploration process we demonstrate some possibility to combine these approach for the area of data mining in document collection document explorer is a system that offer various preprocessing tool to prepare collection of text or multimedia document which are available in distributed environment e g internet and intranet for data mining application and includes data mining method based on searching for pattern like frequent set or association rule keyword graph are used in this system a an highly interactive technique to present the mining result the user can operate on the visualized result either to redirect the data mining process to filter and structure the result to link several graph or to browse into the document collection thus in the keyword graph the relation between interesting set of keywords are presented the set may also be regarded a retrieval query to be posed to the collection and made operable to the analyst 
we present a new approximate learning algorithm for boltzmannmachines using a systematic expansion of the gibbs free energy tosecond order in the weight the linear response correction to thecorrelations is given by the hessian of the gibbs free energy thecomputational complexity of the algorithm is cubic in the numberof neuron we compare the performance of the exact bm learningalgorithm with first order wei mean field theory and secondorder tap mean field theory the learning 
in this paper we show that for discounted mdps with discount factor gamma the asymptotic rate of convergence of q learning is o t r gamma if r gamma and o sqrt log log t t otherwise provided that the state action pair are sampled from a fixed probability distribution here r p min p max is the ratio of the minimum and maximum state action occupation frequency the result extend to convergent on line learning provided that p min where p min and p max now become the minimum and maximum state action occupation frequency corresponding to the stationary distribution 
in this paper we show that for discounted mdps with discount factor the asymptotic rate of convergence of q learning is o i tr l o if r i and o jlog log t t otherwise provided that the state action pair are sampled from a fixed prob ability distribution here r pmin pmrx is the ratio of the min imum and rnaximurn state action occupation frequency the re sults extend to convergent oil line learning provided that pmin where pmin and pmax now become the minimum and maximum state action occupation frequency corresponding to the station ary distribution 
we exhibit a novel way of simulating sigmoidal neural net by networksof noisy spiking neuron in temporal coding furthermore it is shown thatnetworks of noisy spiking neuron with temporal coding have a strictly largercomputational power than sigmoidal neural net with the same number ofunits 
in order to process incoming sound efficiently it is advantageousfor the auditory system to be adapted to the statistical structure ofnatural auditory scene a a first step in investigating the relationbetween the system and it input we study low order statisticalproperties in several sound ensemble using a filter bank analysis focusing on the amplitude and phase in different frequency band we find simple parametric description for their distribution andpower spectrum that are 
many classification algorithm are designed towork with datasets that contain only discrete attribute discretization is the process of convertingthe continuous attribute of the dataset intodiscrete one in order to apply some classificationalgorithm in this paper we first review previouswork in discretization then we propose anew discretization method based on a distanceproposed by l opez de m antaras and show thatit can be easily implemented in parallel with ahigh improvement in 
there is strong evidence that face processing is localized in the brain the double dissociation between prosopagnosia a face recognition deficit occurring after brain damage and visual object agnosia difficulty recognizing other kind of complex object indicates that face and nonface object recognition may be served by partially independent mechanism in the brain is neural specialization innate or learned we suggest that this specialization could be the result of a competitive learning mechanism that during development devotes neural resource to the task they are best at performing further we suggest that the specialization arises a an interaction between task requirement and developmental constraint in this paper we present a feed forward computational model of visual processing in which two module compete to classify input stimulus when one module receives low spatial frequency information and the other receives high spatial frequency information and the task is to identify the face while simply classifying the object the low frequency network show a strong specialization for face no other combination of task and input show this strong specialization we take these result a support for the idea that an innately specified face processing module is unnecessary background study of the preserved and impaired ability in brain damaged patient pro vide important clue on how the brain is organized case of prosopagnosia a face recogniti on deficit often sparing recognition of non face object and visual object agnosia an object recognition deficit that can occur without appreciable impairment of face recognition prov ide evidence that face recognition is served by a special mechanism for a recent review of this 
arachnid is a distributed algorithm for information discovery in large dynamic distributed environment such a the world wide web the approach is based on a distributed adaptive population of intelligent agent making local decision the behavior of the algorithm is analyzed using a simplified model of the web environment this analysis highlight an interesting feature of the web environment that bodes well for arachnid s search method the performance of the algorithm is illustrated 
the problem of transforming the knowledge base of expert system using induced rule or decision tree into comprehensible knowledge structure is addressed a knowledge structure is developed that generalizes and subsumes production rule decision tree and rule with exception it give rise to a natural complexity measure that allows them to be understood analyzed and compared on a uniform basis the structure is a directed acyclic graph with the semantics that node are premise some of which have attached conclusion and the arc are inheritance link with disjunctive multiple inheritance a detailed example is given of the generation of a range of such structure of equivalent performance for a simple problem and the complexity measure of a particular structure is shown to relate to it perceived complexity the simplest structure are generated by an algorithm that factor common sub premise from the premise of rule a more complex example of a chess dataset is used to show the value of this technique in generating comprehensible knowledge structure 
the problem of discovering association rule ha received considerable research attention and several fast algorithm for mining association rule have been developed in practice user are often interested in a subset of association rule for example they may only want rule that contain a specific item or rule that contain child of a specific item in a hierarchy while such constraint can be applied a a postprocessing step integrating them into the mining algorithm can dramatically reduce the execution time we consider the problem of integrating constraint that n 
several pattern discovery method proposed in the data mining literature have the drawback that they discover too many obvious or irrelevant pattern and that they do not leverage to a full extent valuable prior domain knowledge that decision maker have in this paper we propose a new method of discovery that address these drawback in particular we propose a new method of discovering unexpected pattern that take into consideration prior background knowledge of decision maker this prior knowledge constitutes a set of expectation or belief about the problem domain our proposed method of discovering unexpected pattern us these belief to seed the search for pattern in data that contradict the belief to evaluate the practicality of our approach we applied our algorithm to consumer purchase data from a major market research company and to web logfile data tracked at an academic web site and present our finding in the paper 
nonlinear dimensionality reduction is formulated here a the problem of trying to find a euclidean feature space embedding of a set of observation that preserve a closely a possible their intrinsic metric structure the distance between point on the observation manifold a measured along geodesic path our isometric feature mapping procedure or isomap is able to reliably recover low dimensional nonlinear structure in realistic perceptual data set such a a manifold of face image where conventional global mapping method find only local minimum the recovered map provides a canonical set of globally meaningful feature which allows perceptual transformation such a interpolation extrapolation and analogy highly nonlinear transformation in the original observation space to be computed with simple linear operation in feature space 
a circuit for fast compact and low power focal plane motion centroid localization is presented this chip which us mixed signal cmos component to implement photodetection edge detection on set detection and centroid localization model the retina and superior colliculus the centroid localization circuit us time windowed asynchronously triggered row and column address event and two linear resistive grid to provide the analog coordinate of the motion centroid this vlsi chip is used to realize fast lightweight autonavigating vehicle the obstacle avoiding line following algorithm is discussed 
we explore the possibility of importing blackbox model learned over data source at remote site to improve model learned over locally available data source in this way we may be able to learn more accurate knowledge from globally available data than would otherwise be possible from partial locally available data proposed meta learning strategy in our previous work are extended to integrate local and remote model we also investigate the effect on accuracy performance when data overlap among different site 
human use visual a well a auditory speech signal to recognizespoken word a variety of system have been investigated for performingthis task the main purpose of this research wa to systematicallycompare the performance of a range of dynamic visualfeatures on a speechreading task we have found that normalizationof image to eliminate variation due to translation scale and planar rotation yielded substantial improvement in generalizationperformance regardless of the visual 
visual cognition depends critically on the ability to make rapid eye movementsknown a saccade that orient the fovea over target of interest in a visualscene saccade are known to be ballistic the pattern of muscle activationfor foveating a prespecified target location is computed prior to the movementand visual feedback is precluded despite these distinctive property therehas been no general model of the saccadic targeting strategy employed bythe human visual system during 
we develop a recursive node elimination formalismfor efficiently approximating large probabilisticnetworks no constraint are set on thenetwork topology yet the formalism can bestraightforwardly integrated with exact methodswhenever they are become applicable theapproximations we use are controlled theymaintain consistently upper and lower boundson the desired quantity at all time we showthat boltzmann machine sigmoid belief network or any combination i e chain 
in interactive data mining it is advantageous to have condensed representation of data that can be used to efficiently answer different query in this paper we show how frequent set can be used a a condensed representation for answering various type of query given a table r with value and a threshold oe a frequent set of r is a set x of column of r such that at least a fraction oe of the row of r have a in all the column of x finding frequent set is a first step in finding 
business user and analyst commonly use spreadsheet and d plot to analyze and understand their data on line analytical processing olap provides these user with added flexibility in pivoting data around dierent attribute and drilling up and down the multi dimensional cube of aggregation machine learning researcher however have concentrated on hypothesis space that are foreign to most user hyperplanes perceptrons neural network bayesian network decision tree nearest neighbor etc in this paper we advocate the use of decision table classiers that are easy for line of business user to understand we describe several variant of algorithm for learning decision table compare their performance and describe a visualization mechanism that we have implemented in mineset the performance of decision table is comparable to other known algorithm such a c c yet the resulting classiers use fewer attribute and are more comprehensible 
pruning is a common technique to avoid over tting in decision tree most pruning technique do not ac count for one important factor multiple compar isons multiple comparison occur when an induction algorithm examines several candidate model and se lects the one that best accord with the data mak ing multiple comparison produce incorrect inference about model accuracy we examine a method that ad justs for multiple comparison when pruning decision tree bonferroni pruning in experiment with ar ti cial and realistic datasets bonferroni pruning pro duce smaller tree that are at least a accurate a 
in many database marketing application the goal is to predict the customer behavior based on their previous action a usual approach is to develop model which maximize accuracy on the training and test set and then apply these model on the unseen data we show that in order to maximize business payoff accuracy optimization is insufficient by itself and explore different strategy to take the customer value into account we propose a framework for comparing payoff of different model and use it to compare a number of different approach for selecting the most valuable subset of customer for the two datasets that we consider we find that explicit use of value information during the training process and stratified modelling based on value both perform better than post processing strategy base rate giving the model a zifc of for a large customer base even small improvement in prediction accuracy can yield large improvement in lift in this paper we argue that lift measure by itself is not sufficient and that we should take the customer value into account in order to determine the model payoff using the predicted behavior and a simple business model we estimate the payoff from different model and examine different strategy to arrive at an optimal model that maximizes overall business value rather than just accuracy or lift in the rest of this paper we explain the business problem and model of business payoff present the main experimental hypothesis result and conclusion 
autonomous mobile robot need good model of their environment sensor and actuator to navigate reliably and efficiently while this information can be supplied by human or learned from scratch through active exploration such approach are tedious and time consuming our approach is to provide the robot with the topological and geometrical constraint that are easily obtainable by human and have the robot learn the rest while in the course of performing it task we present grow bw an unsupervised and passive distance learning algorithm that overcomes the problem that the robot can never be sure about it location if it is not allowed to reduce it uncertainty by asking a teacher or executing localization action advantage of grow bw include that the robot can be used immediately to perform navigation task and improves it performance over time focusing it attention to route that are more relevant for it task we demonstrate that grow bw can learn good distance sensor and actuator model with only a small amount of experience 
in data mining the goal is to develop method for discovering previously unknown regularity from database the resulting model are interpreted and evaluated by domain expert but some model evaluation criterion is needed also for the model construction process the optimal choice would be to use the same criterion a the human expert but this is usually impossible a the expert are not capable of expressing their evaluation criterion formally on the other hand it seems reasonable to assume that any model posnp nn cl nn nl a ef l mfl n nw nlo ulg ia l lqja urvy i uanul u ilxmal uu am capture some structure of the reality for this reason in predictive data mining the search for good model is guided by the expected predictive error of the model in this paper we describe the bayesian approach to predictive data mining in the finite mixture modeling framework the finite mixture model family is a 
we propose local error estimate together with algorithm for adaptivea posteriori grid and time refinement in reinforcement learning we consider a deterministic system with continuous state andtime with infinite horizon discounted cost functional for grid refinementwe follow the procedure of numerical method for thebellman equation for time refinement we propose a new criterion based on consistency estimate of discrete solution of the bellmanequation we demonstrate that an 
we describe a new iterative method for parameter estimation of gaussian mixture the new method is based on a framework developed by kivinen and warmuth for supervised on line learning in contrast to gradient descent and em which estimate the mixture s covariance matrix the proposed method estimate the inverse of the covariance matrix furthermore the new parameter estimation procedure can be applied in both on line and batch setting we show experimentally that it is typically faster than em and usually requires about half a many iteration a em we also describe experiment with digit recognition that demonstrate the merit of the on line version 
in this paper we propose a method for learning bayesian belief network from data the method usesartificial neural network a probability estimator thus avoiding the need for making prior assumptionson the nature of the probability distribution governing the relationship among the participating variable this new method ha the potential for being applied to domain containing both discrete andcontinuous variable arbitrarily distributed we compare the learning performance of this 
it is well known that for markov decision process the policy stable under policy iteration and the standard reinforcement learning method are exactly the optimal policy in this paper we investigate the condition for policy stability in the more general situation when the markov property cannot be assumed we show that for a general class of non markov decision process if actual return monte carlo credit assignment is used with undiscounted return we are still guaranteed the optimal observation based policy will be equilibrium point in the policy space when using the standard direct reinforcement learning approach however if either discounted reward or a temporal difference style of credit assignment method is used this is not the case 
the prediction of rna secondary structure on the basis of sequence information is an important tool in biosequence analysis however it ha typically been restricted to molecule containing no more than nucleotide due to the computational complexity of the underlying dynamic programming algorithm used we desribe here an approach to rna sequence analysis based upon scalable computer which enables molecule containing up to nucleotide to be analysed we apply the approach to investigation of the entire hiv genome illustrating the power of these method to perform knowledge discovery by identification of important secondary structure motif within rna sequence family the molecule a documented by their extensive use for the interpretation of molecular evolution data 
database integration of mining is becoming increasingly important with tile installation of larger and larger data warehouse built around relational database technology most of the commercially available mining system integrate loosely typically through an odbc or sql cursor interface with data stored in dbms in case where the mining algorithm make nmltiple pass over the data it is also possible to cache the data in fiat file rather than retrieve multiple time from the dbms to achieve better performance recent study have found that for association rule mining with carefully tuned sql forinulations it is possible to achieve performance comparable to system that cache the data in file outside the dbms the sql implementation ha potential for offering other qualitauve advantage like automatic parallehzation development ease portability and inter operability with relational operator in this paper we present several alternative for formulating a sql query association rule generalized to handle item with hierarchy on them and sequential pattern mining this work illustrates that it is possible to express computation that are significantly more complicated than simple boolean assom tions in sql using essentially the same franmwork 
multilayer architecture such a those used in bayesian belief network and helmholtz machine provide a powerful framework for representing and learning higher order statistical relation among input because exact probability calculation with these model are often intractable there is much interest in finding approximate algorithm we present an algorithm that efficiently discovers higher order structure using em and gibbs sampling the model can be interpreted a a stochastic recurrent 
we are frequently called upon to perform multiple task that compete for our attention and resource often we know the optimal solution to each task in isolation in this paper we describe how this knowledge can be exploited to efficiently find good solution for doing the task in parallel we formulate this problem a that of dynamically merging multiple markov decision process mdps into a composite mdp and present a new theoretically sound dynamic programming algorithm for finding an optimal policy for the composite mdp we analyze various aspect of our algorithm and illustrate it use on a simple merging problem every day we are faced with the problem of doing multiple task in parallel each of which competes for our attention and resource if we are running a job shop we must decide which machine to allocate to which job and in what order so that no job miss their deadline if we are a mail delivery robot we must find the intended recipient of the mail while simultaneously avoiding fixed obstacle such a wall and mobile obstacle such a people and still manage to keep ourselves sufficiently charged up frequently we know how to perform each task in isolation this paper considers how we can take the information we have about the individual task and combine it to efficiently find an optimal solution for doing the entire set of task in parallel more importantly we describe a theoretically sound algorithm for doing this merging dynamically new task such a a new job arrival at a job shop can be assimilated online into the solution being found for the ongoing set of simultaneous task 
in this paper we describe the jam system a distributed scalable and portable agent based data mining system that employ a general approach to scaling data mining application that we call meta learning jam provides a set of learning program implemented either a java applet or application that compute model over data stored locally at a site jam also provides a set of meta learning agent for combining multiple model that were learned perhaps at different site it employ a special distribution mechanism which allows the migration of the derived model or classifier agent to other remote site we describe the overall architecture of the jam system and the specific implementation currently under development at columbia university one of jam s target application is fraud and intrusion detection in financial information system a brief description of this learning task and jam s applicability are also described interested user may download jam from http www c columbia edu sal jam pro ject 
adaptive ridge is a special form of ridge regression balancing the quadratic penalization on each parameter of the model it wa shown to be equivalent to lasso least absolute shrinkage and select ion operator in the sense that both procedure produce the same estimate lasso can thus be viewed a a particular quadratic penalizer from this observation we derive a fixed point algorithm to compute the lasso solution the analogy provides also a new hyperparameter for tuning the model complexity we finally present a series of possible extension of lasso to non linear regression problem kernel re gression additive modeling and neural net training 
parti game moore a moore b moore and atkeson is a reinforcement learning rl algorithm that ha a lot of promise in overcoming the curse of dimensionality that can plague rl algorithm when applied to high dimensional problem in this paper we introduce modification to the algorithm that further improve it performance and robustness in addition while parti game solution can be improved locally by standard local path improvement technique we introduce an add on 
this paper present a new approach to speech recognition with hybrid hmm ann technology while the standard approach to hybrid hmm ann system is based on the use of neural network a posterior probability estimator the new approach is based on the use of mutual information neural network trained with a special learning algorithm in order to maximize the mutual information between the input class of the network and it resulting sequence of firing output neuron during training it is shown in this paper that such a neural network is an optimal neural vector quantizer for a discrete hidden markov model system trained on maximum likelihood principle one of the main advantage of this approach is the fact that such neural network can be easily combined with hmm s of any complexity with context dependent capability it is shown that the resulting hybrid system achieves very high recognition rate which are now already on the same level a the best conventional hmm system with continuous parameter and the capability of the mutual information neural network are not yet entirely exploited hybrid hmm ann system deal with the optimal combination of artificial neural network ann and hidden markov model hmm for dynamic pattern recognition task especially in the area of automatic speech recognition it ha been shown that hybrid approach can lead to very powerful and efficient system combining the discriminative capability of neural network and the superior dynamic time warping ability of hmm s the most popular hybrid approach is described in hochberg and replaces the component modeling the emission probability of the hmm by a neural net this is possible because it is shown in bourlard that neural network can be trained so that the output of the m th neuron approximates the posterior probability p m x in this paper an alternative method for constructing a hybrid system is presented it is based on the use of discrete hmm s which are combined with a neural vector quantizer vq in order to form a hybrid system each speech feature vector is 
a data mining system dbminer ha been developed for interactive mining of multiple level knowledge in large relational database the system implement a wide spectrum of data mining function including generalization characterization association classification and prediction by incorporating several interesting data mining technique including attributeoriented induction statistical analysis progressive deepening for mining multiple level knowledge and meta rule guided mining the system provides a userfriendly interactive data mining environment with good performance 
this paper investigates learning in a lifelong context lifelong learning address situation in which a learner face a whole stream of learning task such scenario provide the opportunity to transfer knowledge across multiple learning task in order to generalize more accurately from le training data in this paper several different approach to lifelong learning are described and applied in an object recognition domain it is shown that across the board lifelong learning approach generalize consistently more accurately from le training data by their ability to transfer knowledge across learning task 
the parameter space of neural network ha the riemannian metricstructure the natural riemannian gradient should be used insteadof the conventional gradient since the former denotes the steepestdescent direction of a loss function in the riemannian space thebehavior of the stochastic gradient learning algorithm is much moreeffective if the natural gradient is used the present paper studiesthe information geometrical structure of perceptrons and othernetworks and prove that the 
in this work we present a new bottom up algorithm for decision tree pruningthat is very efficient requiring only a single pas through the given tree andprove a strong performance guarantee for the generalization error of the resultingpruned tree we work in the typical setting in which the given tree t may havebeen derived from the given training sample s and thus may badly overfit s in this setting we give bound on the amount of additional generalization errorthat our 
in supervised learning there is usually a clear distinction betweeninputs and output input are what you will measure outputsare what you will predict from those measurement this papershows that the distinction between input and output is not thissimple some feature are more useful a extra output than asinputs by using a feature a an output we get more than just thecase value but can learn a mapping from the other input to thatfeature for many feature this mapping 
the problem of maximi ing the expected total discounted reward in a completely observable markovian environmen t i e a markov decision process mdp model a particular class of sequential decision problem algorithm have been developed for making optimal decision in mdps given either an mdp specification or the opportunity to interact with the mdp over time recently other sequential decision making prob lem have been studied prompting the development of new algorithm and analysis we describe a new generalized model that subsumes mdps a well a many of the recent variation we prove some basic result concerning this model and develop general izations of value iteration policy iteration model based reinforcement le arning and 
application of inductive learning algorithm to realworlddata mining problem have shown repeatedlythat using accuracy to compare classifier is not adequatebecause the underlying assumption rarely hold we present a method for the comparison of classifierperformance that is robust to imprecise class distributionsand misclassification cost the roc convexhull method combine technique from roc analysis decision analysis and computational geometry andadapts them to the particular 
fleximine is a kdd system designed a a testbed for data mining research a well a a generic knowledge discovery tool for varied database domain flexibility is achieved by an open ended design for extensibility enabling integration of existing data mining algorithm new locally developed algorithm and support function such a visualization and preprocessing support for new database is simple currently via sql query to an informix database server with a view of serving remote a well a local user internet availability wa a design goal by implementing the system in java minor modification allow u to run the user end of the system either a a java application or a a java applet 
a modification is described to the use of mean field approximationsin the e step of em algorithm for analysing data from latentstructure model a described by ghahramani among others the modification involves second order taylor approximationsto expectation computed in the e step the potential benefit ofthe method are illustrated using very simple latent profile model introductionghahramani advocated the use of mean field method a a mean to avoid theheavy 
this paper describes how to increase the efficiency of inductive data mining algorithm by replacing the central matching operation with a marker propagation technique breadth first marker propagation is most beneficial when the data are linked to hierarchical background knowledge e g tree structured attribute or when the attribute describing the data have many value we support our claim analytically with complexity argument and empirically on several large data set we also point out other efficiency gain including reduced memory management overhead which facilitate mining massive tape archive 
backpropagation learning algorithm typically collapse the network s structure into a single vector of weight parameter to be optimized we suggest that their performance may be improved by utilizing the structural information instead of discarding it and introduce a framework for tempering each weight accordingly in the tempering model activation and error signal are treated a approximately independent random variable the characteristic scale of weight change is then matched to that of the residual allowing structural prop erties such a a node s fan in and fan out to affect the local learning rate and backpropagated error the model also permit calculation of an upper bound on the global learning rate for batch update which in turn lead to different update rule for bias v non bias weight this approach yield hitherto unparalleled performance on the family relation benchmark a deep multi layer network for both batch learning with momentum and the delta bar delta algorithm convergence at the optimal learning rate is sped up by more than an order of magnitude 
in an earlier paper we introduced a new boosting algorithm called adaboost which theoretically can be used to significantly reduce the error of any learning algorithm that consistently generates classifier whose performance is a little better than random guessing we also introduced the related notion of a pseudo loss which is a method for forcing a learning algorithm of multi label concept to concentrate on the label that are hardest to discriminate in this paper we describe experiment we carried out to ass how well adaboost with and without pseudo loss performs on real learning problem we performed two set of experiment the first set compared boosting to breiman s bagging method when used to aggregate various classifier including decision tree and single attribute value test we compared the performance of the two method on a collection of machine learning benchmark in the second set of experiment we studied in more detail the performance of boosting using a nearest neighbor classifier on an ocr problem 
the execution order of a block of computer instruction on a pipelined machine can make a difference in it running time by a factor of two or more in order to achieve the best possible speed compiler use heuristic scheduler appropriate to each specific architecture implementation however these heuristic scheduler are time consuming and expensive to build we present empirical result using both rollouts and reinforcement learning to construct heuristic for scheduling basic block in simulation the rollout scheduler outperformed a commercial scheduler and the reinforcement learning scheduler performed almost a well a the commercial scheduler 
principal curve have been defined a self consistent smooth curve which pas through the middle of a d dimensional probability distribution or data cloud recently we have offered a new approach by defining principal curve a continuous curve of a given length which minimize the expected squared distance between the curve and point of the space randomly chosen according to a given distribution the new definition made it possible to carry out a theoretical analysis of learning principal curve from training data in this paper we propose a practical construction based on the new definition simulation result demonstrate that the new algorithm compare favorably with previous method both in term of performance and computational complexity 
we compare the generalization performance of three distinct rep resentation scheme for facial emotion using a single classi cation strategy neural network the face image presented to the clas si er are represented a full face projection of the dataset onto their eigenvectors eigenfaces a similar projection constrained to eye and mouth area eigenfeatures and nally a projection of the eye and mouth area onto the eigenvectors obtained from x random image patch from the dataset the latter system achieves generalization on novel face image individual the network were not trained on drawn from a database in which human sub jects consistently identify a single emotion for the face 
metapatterns also known a metaqueries have beenproposed a a new approach to integrated data mining and applied to several real world application successfully however designing the right metapatterns forum given application still remains a difficulty task inthis paper we present a metapattern generator thatcan automatically generate metapatterns from newdatabases by integrating this generator with the existingmetapattern based discovery loop our systemhas now become both 
this paper describes the application of a hybrid neural expert system network to the task of finding significant event in a market research data base neural network trained by backward error propagation are used to classify trend in the time series data a rule system then us these classification knowledge of market research analysis technique and external event which influence the time series to infer the significance of the data the system achieved recall and precision on a test set of month of survey data this wa significantly better than could be achieved by a system using linear regression together with a rule system both system were able to perform analysis of the test data in under minute the manual analysis of the same data took a human expert over four working day 
we introduce a method for learning bayesian network that handle the discretization of continuous variable a an integral part of the learning process the main ingredient in this method is a new metric based on the minimal description length principle for choosing the threshold value for the discretization while learning the bayesian network structure this score balance the complexity of the learned discretization and the learned network structure against how well they model the training data this ensures that the discretization of each variable introduces just enough interval to capture it interaction with adjacent variable in the network we formally derive the new metric study it main property and propose an iterative algorithm for learning a discretization policy finally we illustrate it behavior in application to supervised learning 
we study the effect of correlated noise on the accuracy of populationcoding using a model of a population of neuron that arebroadly tuned to an angle in two dimension the fluctuation inthe neuronal activity is modeled a a gaussian noise with pairwisecorrelations which decay exponentially with the difference betweenthe preferred orientation of the pair by calculating the fisher informationof the system we show that in the biologically relevantregime of parameter positive 
based on our analysis and experiment using real world datasets we find that the greediness of forward feature selection algorithm doe not severely corrupt the accuracy of function approximation using the selected input feature but improves the efficiency significantly hence we propose three greedier algorithm in order to further enhance the efficiency of the feature selection processing we provide empirical result for linear regression locally weighted regression and k nearestneighbor model we also propose to use these algorithm to develop an off line chinese and japanese handwriting recognition system with automatically configured local model 
this paper present first result of an interdisciplinary project in scientific data mining we analyze data about the carcinogenicity of chemical derived from the carcinogenesis bioassay program performed by the u national institute of environmental health science the database contains detailed description of test performed with more than compound and animal of different specie strain and sex the chemical structure are described at the atom and bond level and in term of various relevant strnctural property the goal of this paper is to investigate the effect that various level of detail and amount of information have on the resulting hypothesis both quantitativel and qualitatively we apply relational and propositional machine learning algorithm to learning problem formulated a regression or a classification task in addition these experiment have been conducted with two learning problem which are at different level of detail quantitatively our experiment indicate that additional information nob necessarily improves accuracy q itatively a number of potential discovery have been made by the algorithm for relational regression because it can utilize aii the information contained in the relation of the database a weli a in the numerical dependent variable 
this study uncovers trading style in the transactionrecords of u treasury bond future we usestatistical clustering technique to group togethertrades that are similar trade profit wa heldback in the clustering process result show thatclusters differ significantly in their profit and riskcharacteristics some cluster uncover quot technical quot trading rule using the information about the individualaccounts we describe the assignment ofaccounts to cluster by entropy and model the 
in order to grasp an object we need to solve the inverse kinematicsproblem i e the coordinate transformation from the visualcoordinates to the joint angle vector coordinate of the arm althoughseveral model of coordinate transformation learning havebeen proposed they suffer from a number of drawback in humanmotion control the learning of the hand position error feedbackcontroller in the inverse kinematics solver is important this paperproposes a novel model of the 
research in theory refinement ha shown that biasing a learner with initial approximately correct knowledge produce more accurate result than learning from data alone while technique have been developed to revise logical and connectionist representation little ha been done to revise probabilistic representation bayesian network are well established a a sound formalism for representing and reasoning with probabilistic knowledge and are widely used there ha been a growing interest in the problem of learning bayesian network from data however there is no existing technique for learning or revising bayesian network with hidden variable i e variable not represented in the data that ha been shown to be efficient effective and scalable through evaluation on real data the few technique that exist for revising such network perform a blind search through a large space of revision and are therefore computationally expensive this dissertation present banner a technique for using data to revise a given bayesian network with noisy or and noisy and node to improve it classification accuracy additionally the initial network can be derived directly from a logical theory expressed a propositional horn clause rule banner can revise network with hidden variable and add hidden variable when necessary unlike previous approach to this problem banner employ mechanism similar to those used in logical theory refinement technique for using the data to focus the search for effective modification to the network it can also be used to learn network with hidden variable from data alone we also introduce banner pr a technique for revising the parameter of a bayesian network with noisy or and node that directly exploit the computational efficiency afforded by these model experiment on several real world learning problem in domain such a molecular biology and intelligent tutoring system demonstrate that banner can effectively and efficiently revise network to significantly improve their accuracy and thus learn highly accurate classifier comparison with the naive bayes algorithm show that using the theory refinement approach give banner a substantial edge over learning from data alone we also show that banner pr converges faster and produce more accurate classifier than an existing algorithm for learning the parameter of a network 
the s map is a network with a simple learning algorithm that combinesthe self organization capability of the self organizing map som and the probabilistic interpretability of the generative topographicmapping gtm the simulation suggest that the smapalgorithm ha a stronger tendency to self organize from randominitial configuration than the gtm the s map algorithmcan be further simplified to employ pure hebbian learning withoutchanging the qualitative behaviour of the network 
this paper describes the use of ai planning technique to represent scientific image processing and software tool knowledge to automate knowledge discovery and data mining e g science data analysis of large image database in particular we describe two fielded system the multimission vicar planner mvp which ha been deployed for year and is currently supporting science product generation for the galileo mission mvp ha reduced time to fill certain class of request from hour to minute the automated sar image processing system asip which is currently in use by the dept of geology at asu supporting aeolian science analysis of synthetic aperture radar image asip reduces the number of manual input in science product generation by fold 
this paper address the problem of handlingskewed class distribution within thecase based learning cbl framework wefirst present a a baseline an informationgain weighted cbl algorithm and apply it tothree data set from natural language processing nlp with skewed class distribution although overall performance of thebaseline cbl algorithm is good we show thatthe algorithm exhibit poor performance onminority class instance we then presenttwo cbl algorithm designed to 
we describe the problem of finding deviation in largedata base normally explicit information outside thedata like integrity constraint or predefined pattern is used for deviation detection in contrast we approachthe problem from the inside of the data usingthe implicit redundancy of the data we give a formal description of the problem andpresent a linear algorithm for detecting deviation our solution simulates a mechanism familiar to humanbeings after seeing a series of 
we solve the dynamic of hopfield type neural network which store sequence of pattern close to saturation the asymmetry of the interaction matrix in such model lead to violation of detailed balance ruling out an equilibrium statistical mechanical analysis using generating functional method we derive exact closed equation for dynamical order parameter viz the sequence overlap and correlation and response function in the limit of an infinite system size we calculate the time translation invariant solution of these equation describing stationary limit cycle which lead to a phase diagram the effective retarded self interaction usually appearing in symmetric model is here found to vanish which cause a significantly enlarged storage capacity of compared to for hopfield network storing static pattern our result are tested against extensive computer simulation and excellent agreement is found 
this paper present an efficient approach to address the task of learning from large number of learning example in structural domain while in attribute value representation only one mapping is possible between description in first order logic representation there are potentially many mapping classic approach consider all mapping and then define a restricted hypothesis space to cope with the intractability of exploring all mapping our approach is to select one particular type of mapping at a time and use it a a basis to define a new hypothesis space we show that such a hypothesis space called a matching space may be represented using attribute value pair in a matching space it is therefore possible to use propositional learner the concept description found may then be mapped back into the initial first order logic representation it appears that characterizing a matching space is equivalent to shifting the representation of example the new learning example represent only a part of the initial example based on a taxonomy of elementary part provided by the user we consider a particular set of composite part called morion that are used to automatically and iteratively change the representation of example experimental result obtained with an implemented system remo show the benefit of this approach we have used remo to learn characteristic description of concept related to the pronunciation of chinese character from a corpus of more than three thousand character 
we implement a model of obstacle avoidance in flying insect on a small monocular robot the result is a system that is capable of rapid navigation through a dense obstacle field the key to the system is the use of zigzag behavior to articulate the body during movement it is shown that this behavior compensates for a parallax blind spot surrounding the focus of expansion normally found in system without parallax behavior the system model the cooperation of several behavior halter ocular response similar to vor optomotor response and the parallax field computation and mapping to motor system the resulting system is neurally plausible very simple and should be easily hosted on avlsi hardware 
recent research on hidden state reinforcement learning rl problem ha concentrated on overcoming partial observability by using memory to estimate state however such method are computationally extremely expensive and thus have very limited applicability this emphasis on state estimation ha come about because it ha been widely observed that the presence of hidden state or partial observability render popular rl method such a q learning and sarsa useless however this observation is misleading in two way first the theoretical result supporting it only apply to rl algorithm that do not use eligibility trace and second these result are worst case result which leaf open the possibility that there may be large class of hidden state problem in which rl algorithm work well without any state esti 
finding structure in multiple stream of data is an important problem consider the stream of data owing from a robot s sen sors the monitor in an intensive care unit or periodic measurement of various indica tor of the health of the economy there is clearly utility in determining how current and past value in those stream are related to future value we formulate the prob lem of nding structure in multiple stream of categorical data a search over the space of dependency unexpectedly frequent or infrequent co occurrence between complex pattern of value that can appear in the stream based on that formulation we de velop the multi stream dependency detec tion msdd algorithm that performs an e cient systematic search over the space of all possible dependency dependency strength is evaluated with a statistical measure of non independence and bound that we derive for the value of that measure allow the search to be pruned due to the pruning msdd can nd the k strongest dependency in the stream by examining only a fraction of the search space 
this paper describes a bayesian graph matching algorithm fordata mining from large structural data base the matching algorithmuses edge consistency and node attribute similaritytodeterminethe aposteriori probability of a query graph for eachofthecandidate match in the data base the node feature vector areconstructed by computing normalised histogram of pairwise geometricattributes attribute similarity is assessed by computingthe bhattacharyya distance between the 
in many optimization problem the structure of solution reflects complex relationship between the different input parameter for example experience may tell u that certain parameter are closely related and should not be explored independently similarly experience may establish that a subset of parameter must take on particular value any search of the cost landscape should take advantage of these relationship we present mimic a framework in which we analyze the global structure of the optimization landscape a novel and efficient algorithm for the estimation of this structure is derived we use knowledge of this structure to guide a randomized search through the solution space and in turn to refine our estimate of the structure our technique obtains significant speed gain over other randomized optimization procedure advance in neural information processing system mit press cambridge ma 
this paper formulates the problem of visual search a bayesian inference and denes a bayesian ensemble of problem instance in particular we address the problem of the detection of visual contour in noise clutter by optimizing a global criterion which combine local intensity and geometry information we analyze the convergence rate of a search algorithm using result from information theory to bound the probability of rare event within the bayesian ensemble this analysis determines characteristic of the domain which we call order parameter that determine the convergence rate in particular we present a specic admissible a algorithm with pruning which converges with high probability with expected time o n in the size of the problem in addition we briey summarize extension of this work which address fundamental limit of target contour detectability i e algorithm independent result and the use of non admissible heuristic 
this paper develops the concept of usefulness in the context of supervised learning we argue that usefulness can be used to improve the performance of classification rule a measured by error rate a well to reduce their storage or their derivation we also indicate how usefulness can be applied in a dynamic setting in which the distribution of at least one class is changing with time three algorithm are used to exemplify our proposal we first review a dynamic nearest neighbour classifier and then develop dynamic version of learning vector quantization and a radial basis function network all the algorithm are adapted to capture dynamic aspect of real world data set by keeping a record of usefulness a well a considering the age of the observation these method are tried out on real data from the credit industry 
this paper introduces a new technique for discretization of continuous variable based on zeru a measure of strength of association between nominal variable developed for this purpose zeta is defined a the maximal accuracy achievable if each value of an independent variable must predict a different value of a dependent variable we describe both how a continuous variable may be dichotomised by searching for a maximum value of zeta and how a heuristic extension of tbis method can partition a continuous variable into more than two category experimental comparison with other published method show that zeta discretization run considerably faster than other technique without any loss of accuracy 
in this paper we investigate a number of ensemble method forimproving the performance of phoneme classification for use in aspeech recognition system we discus boosting and mixture ofexperts both in isolation and in combination we present resultson an isolated word database the result show that principledensemble method such a boosting and mixture provide superiorperformance to more naive ensemble method when used in combination boosting and mixture provide a further 
in this paper we present a computationallyefficient method for inducing selectivebayesian network classifier our approachis to use information theoretic metric to efficientlyselect a subset of attribute fromwhich to learn the classifier we explorethree conditional information theoretic metricsthat are extension of metric used extensivelyin decision tree learning namely quinlan s gain and gain ratio metric and mantaras s distance metric we experimentallyshow that the 
we apply a well known bayesian probabilistic model to textual information retrieval the classification of document based on their relevance to a query this model wa previously used with supervised training data for a fixed query when only noisy unsupervised training data generated from a heuristic relevance scoring formula are available two crucial adaptation are needed severe smoothing of the model built on the training data and adding a prior probability to the model we have shown that with these adaptation the probabilistic model is able to improve the retrieval precision of the heuristic model the experiment wa performed using the trec corpus and query and the evaluation of the model wa submitted a an official entry ibms b to trec 
the successfulapplication of data mining technique ideally requires both system support for the entire knowledge discovery process and the right analysis algorithm for the particular task at hand while there are a number of successful data mining system that support the entire mining process they usually are limited to a fixed selection of analysis algorithm in this paper we argue in favor of extensibility a a key feature of data mining system and discus the requirement that this entail for system architecture we identify in which point existing data mining system fail to meet these requirement and then describe a new integration architecture for data mining system that address these problem based on the concept of plug in kepler our data mining system built according to this architecture is presented and discussed 
with the increase in information on the world wide web it ha become difficult to quickly find desired information without using multiple query or using a topic specific search engine one way to help in the search is by grouping html page together that appear in some way to be related in order to better understand this task we performed an initial study of human clustering of web page in the hope that it would provide some insight into the difficulty of automating this task our result show that subject did not cluster identically in fact on average any two subject had little similarity in their web page cluster we also found that subject generally created rather small cluster and those with access only to url created fewer cluster than those with access to the full text of each web page generally the overlap of document between cluster for any given subject increased when given the full text a did the percentage of document clustered when analyzing individual subject we found that each had different behavior across query both in term of overlap size of cluster and number of cluster these result provide a sobering note on any quest for a single clearly correct clustering method for web page 
neural network ensemble have been shown to be very accurateclassification technique previous work ha shown that an effectiveensemble should consist of network that are not only highlycorrect but one that make their error on different part of theinput space a well most existing technique however only indirectlyaddress the problem of creating such a set of network in this paper we present a technique called addemup that usesgenetic algorithm to directly search for an 
we have designed fabricated and tested an adaptive winner take all wta circuit based upon the classic wta of lazzaro et al we have added a time dimension adaptation to thiscircuit to make the input derivative an important factor in winnerselection to accomplish this we have modified the classic wtacircuit by adding floating gate transistor which slowly null theirinputs over time we present a simplified analysis and experimentaldata of this adaptive wta fabricated in a 
a new policy iteration algorithm for partially observable markov decisionprocesses is presented that is simpler and more efficient thanan earlier policy iteration algorithm of sondik the keysimplification is representation of a policy a a finite state controller this representation make policy evaluation straightforward the paper s contribution is to show that the dynamic programming updateused in the policy improvement step can be interpreted a the transformation 
cross language latent semantic indexing isa method that learns useful languageindependentvector representation of termsthrough a statistical analysis of a documentalignedtext this is accomplished by takinga collection of say english paragraphsand their translation in spanish and processingthem by singular value decompositionto yield a high dimensional vector representationfor each term in the collection theseterm vector have the property that semanticallysimilar term have 
in human object recognition converging evidence ha shown that subject performance depends on their familiarity with an object s appearance the extent of such dependence is a function of the inter object similarity the more similar the object are the stronger this dependence will be and the more dominant the two dimensional d image based information will be however the degree to which three dimensional d model based information is used remains an area of strong debate previously the author showed that all model with independent d template that allowed d rotation in the image plane cannot account for human performance in discriminating novel object view here the author derive an analytic formulation of a bayesian model that give rise to the best possible performance under d affine transformation and demonstrate that this model cannot account for human performance in d object discrimination relative to this model human statistical efficiency is higher for novel view than for learned view suggesting that human observer have used some d structural information 
some learning technique for classification task work indirectly by first trying to fit a full probabilistic model to the observed data whether this is a good idea or not depends on the robustness with respect to deviation from the postulated model we study this question experimentally in a restricte d yet non trivial and interesting case we consider a conditionally independent attribute cia model which postulate a single binary valued hidden variable on which all other attribute i e the target and the observables depend i n this model finding the most likely value of any one variable given known value for the others reduces to testing a linear function of the observed value we learn cia with two technique the standard em algorithm and a new algorithm we develop based on covariance we compare these in a controlled fashion against an algorithm a version of winnow that attempt to find a good linear classifier directly our conclusion help delimit the fragility of using the cia model for classification once the data departs from thismodel performance quickly degrades and drop below that of the directly learn ed linear classifier 
this paper survey the growing number of indu trial application of data mining and knowledge discovery we look at the existing tool describe some representative application and discus the major issue and problem for building and deploying successful application and their adoption by business user finally we examine how to ass the potential of a knowledge discovery application 
voting method s uch a boosting and b agging provide substantial improvement in classification p erformance in many problem domain however the resulting prediction can p rove inscrutable to end user this is especially problematic in do main s uch a medicine where end user acceptance often depends on the ability of a c lassifier to explain it reasoning here we propose a variant of the boosted na ve bayes classifier that facilitates explanation while retaining predictive performance 
the prediction of survival time or recurrence time is an important learning problemin medical domain the recurrence surface approximation rsa method is anatural effective method for predicting recurrence time using censored input data this paper introduces the survival curve rsa sc rsa an extension to the rsaapproach which produce accurate predicted rate of recurrence while maintaining accuracyon individual predicted recurrence time the method is applied to the problem 
with the rapid expansion of computer network during the past few year security ha become a crucial issue for modern computer system a good way to detect illegitimate use is through monitoring unusual user activity method of intrusion detection based on hand coded rule set or predicting command on line are laborous to build or not very reliable this paper proposes a new way of applying neural network to detect intrusion we believe that a user leaf a print when using the system a neural network can be used to learn this print and identify each user much like detective use thumbprint to place people at crime scene if a user s behavior doe not match his her print the system administrator can be alerted of a possible security breech a backpropagation neural network called nnid neural network intrusion detector wa trained in the identification task and tested experimentally on a system of user the system wa accurate in detecting unusual activity with false alarm rate these result suggest that learning user profile is an effective way for detecting intrusion 
the task in the computer security domain of anomaly detection is to characterize the behavior of a computer user the valid or normal user so that unusual occurrence can be detected by comparison of the current input stream to the valid user s profile this task requires an online leaming system that can respond to concept drift and handle discrete non metric time sequence data we present an architecture for online learning in the anomaly detection domain and address the issue of incremental updating of system parameter and instance selection we demonstrate a method for measuring direction and magnitude of concept drift in the classification space and present approach to the above stated issue which make use of the drift measurement an empirical evaluation demonstrates the relative strength and weakness of these technique in comparison to a number of baseline technique we show that for some user our drift adaptive technique are advantageous 
we present a new approach to reinforcement learning in which the policy considered by the learning process are constrained by hierarchy of partially specified machine this allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problem and in which component solution can be recombined to solve larger and more complicated problem our approach can be seen a providing a link between reinforcement learning and behavior based or teleo reactive approach to control we present provably convergent algorithm for problem solving and learning with hierarchical machine and demonstrate their effectiveness on a problem with several thousand state 
this paper describes our work in learning online model that forecast real valued variable in a high dimensional space a gb database wa collected by sampling real valued sensor in a cement manufacturing plant once every minute for several month the goal is to learn model that every minute forecast the value of all sensor for the next hour the underlying process is highly non stationary there are abrupt change in sensor behavior time frame minute semi periodic behavior time frame hour day and slow long term drift in plant dynamic timeframe week month therefore the model need to adapt on line a new data is received all learning and prediction must occur in realtime i e one minute the learning method must also deal with two form of data corruption large amount of data are missing and what is recorded is very noisy we have developed a framework with multiple level of adaptation in which several thousand incremental learning algorithm that adapt on line are automatically evaluated also on line to arrive at the best prediction we present experimental result to show that by combining multiple learning method we can automatically learn good model for timeseries prediction without being provided with any physical model of the underlying dynamic 
performing policy iteration in dynamic programming should only requireknowledge of relative rather than absolute measure of the utilityof action what baird call the advantage of action atstates nevertheless existing method in dynamic programming includingbaird s compute some form of absolute utility function forsmooth problem advantage satisfy two differential consistency condition including the requirement that they be free of curl and we showthat 
most data mining system to date have used variant of traditional machine learning algorithm to tackle the task of directed knowledge discovery this paper present an approach which a well a being useful for such directed data mining can also be applied to the further task of undirected data mining and hypothesis refinement this approach exploit parallel genetic algorithm a the search mechanism and seek to evolve explicit rule for maximum comprehensibility example rule found in real commercial datasets are presented 
in the regression context boosting and bagging are technique to build a committee of regressors that may be superior to a single regressor we use regression tree a fundamental building block in bagging committee machine and boosting committee machine performance is analyzed on three non linear function and the boston housing database in all case boosting is at least equivalent and in most case better than bagging in term of prediction error 
in many application such a credit default prediction and medical image recognition test input are available in addition to the labeled training example we propose a method to incorporate the test input into learning our method result in solution having smaller test error than that of simple training solution especially for noisy problem or small training set 
while it is generally agreed that neuron transmit informationabout their synaptic input through spike train the code by whichthis information is transmitted is not well understood an upperbound on the information encoded is obtained by hypothesizingthat the precise timing of each spike conveys information here wedevelop a general approach to quantifying the information carriedby spike train under this hypothesis and apply it to the leakyintegrate and fire if model of 
sparse coding is a method for finding a representation of data in which each of the component of the representation is only rarely s ignificantly active such a representation is closely related to redunda ncy reduction and independent component analysis and ha some neurophysiological plausibility in this paper we show how sparse coding can be used for denoising using maximum likelihood estimation of nongaussian variable corrupted by gaussian noise we show how to apply a shrinkage nonlinearity on the component of sparse coding so a to reduce noise furthermore we show how to choose the optimal sparse coding basis for denoising our method is closely related to the method of wavelet shrinkage but ha the important benefit over wavelet method s that both the feature and the shrinkage parameter are estimated dir ectly from the data 
a variety of technique from statistic signal processing pattern recognition machine learning and neural network have been proposed to understand data by discovering useful category however research in data mining ha not paid attention to the cognitive factor that make learned category intelligible to human user we ohnwr hn a cnnte the nfl nmnao thn ntczll h l r nf cu w ullal ii d lcllrl l ulal i iiucii u j lllu iiiltaii iiicj i 
many important classification problem are polychotomies i e the data are organizedinto k class with k given an unknown function f omega f kg representing apolychotomy an algorithm aimed at quot learning quot this polychotomy will produce an approximationof f based on the knowledge of a set of pair f xp f xp gpp although in the wide variety oflearning tool there exist some learning algorithm capable of handling polychotomies many of the 
one of the surprising recurring phenomenon observed in experiment with boosting is that the test error of the generated classifier usually doe not increase a it size becomes very large and often is observed to decrease even after the training error reach zero in this paper we show that this phenomenon is related to the distribution of margin of the training example with respect to the generated voting classification rule where the margin of an example is simply the difference between the number of correct vote and the maximum number of vote received by any incorrect label we show that technique used in the analysis of vapnik s support vector classifier and of neural network with small weight can be applied to voting method to relate the margin distribution to the test error we also show theoretically and experimentally that boosting is especially effective at increasing the margin of the training example finally we compare our explanation to those based on the bias variance 
the discovery of the relationship between chemical structure and biological function is central to biological science and medicine in this paper we apply data mining to the problem of predicting chemical carcinogenicity this toxicology application wa launched at ijcai a a research challenge for artificial intelligence our approach to the problem is descriptive rather than based on classification the goal being to find common substructure and property in chemical compound and in this way to contribute to scientific insight this approach contrast with previous machine learning research on this problem which ha mainly concentrated on predicting the toxicity of unknown chemical our contribution to the field of data mining is the ability to discover useful frequent pattern that are beyond the complexity of association rule or their known variant this is vital to the problem which requires the discovery of pattern that are out of the reach of simple transformation to frequent itemsets we present a knowledge discovery method for structured data where pattern reflect the one tomany and many to many relationship of several table background knowledge represented in a uniform manner in some of the table ha an essential role here unlike in most data mining setting for the discovery of frequent pattern 
this paper describes the fact system for knowledge discovery from text it discovers association pattern of co occurrence amongst keywords labeling the item in a c ollection o f textual document in addition fact is able to use background knowledge about the keywords labeling the document in it discovery process fact take a query centered view of knowledge discovery in which a discovery request i s viewed a a query over the implicit set of possible result supported by a collection of document and where background knowledge is used to specify constraint on the desired result of this query process execution of a knowledge discovery query is s tructured so that t hese background knowledge c onstraints can b e e xploited in the search for possible result finally rather than requiring a user to specify an explicit query expression in the knowledge discovery query language fact present the user with a simple to use graphical interface to the query language with the language providing a well defined semantics for the discovery action performed b y a user through the interface 
we show how a concise representation of active recognition behaviorwhat observation to make to detect a given objectcan be derived from hidden state reinforcement learning technique these learning technique can solve decision process task which include perceptual observation defined formally a partially observable markov decision process pomdp we define recognition within a pomdp context with an action indicating recognition of the target a well a action for adjusting the perceptual apparatus or other effector an explicit supervised reward signal is provided to the decision process whenever the accept action is performed with sufficient experience a memory based approach to reinforcement learning can find optimal policy which discriminate target from distractor pattern despite considerable perceptual aliasing at any given instant to avoid perceptual aliasing while learning all similar experience are combined when computing the utility of a possible action including experience with both target and distractor pattern by discarding the representation of negative region of the utility space when learning is complete and collapsing duplicate representation of positive region a representation similar to an augmented finite state machine is obtained we show application of our method for the task of recognizing human gesture performance that occurs at multiple spatial scale 
we consider recurrent analog neural net where each gate is subject to gaussian noise or any other common noise distribution whose probability density function is nonzero on a large set we show that many regular language cannot be recognized by network of this type for example the language begin with and we give a precise characterization of those language which can be recognized this result implies severe constraint on possibility for constructing recurrent analog neural net that are robust against realistic type of analog noise on the other hand we present a method for constructing feedforward analog neural net that are robust with regard to analog noise of this type 
explanation based reinforcement learning ebrl wa introduced by dietterich andflann a a way of combining the ability ofreinforcement learning rl to learn optimalplans with the generalization abilityof explanation based learning ebl dietterich amp flann we extend thiswork to domain where the agent must orderand achieve a sequence of subgoals inan optimal fashion hierarchical ebrl caneffectively learn optimal policy in some ofthese sequential task domain even when the 
conventional document retrieval system e g alta vista return long list of ranked document in response to user query recently document clustering ha been put forth a an alternative method of organizing retrieval result cutting et al a person browsing the cluster can discover pattern that could be overlooked in the traditional presentation this paper describes two novel clustering method that intersect the document in a cluster to determine the set of word or phrase shared h thn l n chn nlrrd nv ixtn x a j cala yllg u buuta l u au llg biuu gl cj r p r on experiment that evaluate these intersectionbased clustering method on collection of snippet returned from web search engine first we show that word intersection clustering produce superior cluster and doe so faster than standard technique second we show that our o nlog n time phrase intersection clustering method produce comparable cluster and doe so more than two order of magnitude faster than all method tested 
we compare different method to combine prediction from neuralnetworks trained on different bootstrap sample of a regression problem one of these method introduced in and which we here callbalancing is based on the analysis of the ensemble generalization errorinto an ambiguity term and a term incorporating generalizationperformances of individual network we show how to estimate theseindividual error from the residual on validation pattern weightingfactors for the different 
in this paper we present a novel hybrid architecture for continuous speech recognition system it consists of a continuous hmm system extended by an arbitrary neural network that is used a a preprocessor that take several frame of the feature vector a input to produce more discriminative feature vector with respect to the underlying hmm system this hybrid system is an extension of a state of the art continuous hmm system and in fact it is the first hybrid system that really is capable of outperforming these standard system with respect to the recognition accuracy experimental result show an relative error reduction of about that we achieved on a remarkably good recognition system based on continuous hmms for the resource management word continuous speech recognition task 
the mortality related to cervical cancer can be substantially reduced through early detection and treatment however current detection technique such a pap smear and colposcopy fail to achieve a concurrently high sensitivity and specificity in vivo fluorescence spectroscopy is a technique which quickly noninvasively and quantitatively probe the biochemical and morphological change that occur in pre cancerous tissue rbf ensemble algorithm based on such spectrum provide automated and 
this paper is to push this interactionfurther in light of these recent development in particular we perform experiment suggested by the formalresults for adaboost and c within the weaklearning framework we concentrate on two particularlyintriguing issue first the theoretical boosting result for top downdecision tree algorithm such a c suggest thata new splitting criterion may result in tree that aresmaller and more accurate than those obtained usingthe usual 
in this paper we employ a novel approach to metarule guided multi dimensional association rule mining which explores a data cube structure we propose algorithm for metarule guided mining given a metarule containing p predicate we compare mining on an n dimensional n d cube structure where p n with mining on smaller multiple pdimensional cube in addition we propose an efficient method for precomputing the cube which take into account the constraint imposed by the given metarule 
in this paper we discus a data mining framework for constructing intrusion detection model the key idea are to mine system audit data for consistent and useful pattern of program and user behavior and use the set of relevant system feature presented in the pattern to compute inductively learned classiers that can recognize anomaly and known intrusion our past experiment showed that classiers can be used to detect intrusion provided that sucient audit data is available for training and the right set of system feature are selected we propose to use the association rule and frequent episode computed from audit data a the basis for guiding the audit data gathering and feature selection process we modify these two basic algorithm to use axis attribute s a a form of item constraint to compute only the relevant useful pattern and an iterative level wise approximate mining procedure to uncover the low frequency but important pattern we report our experiment in using these algorithm on real world audit data 
we present new algorithm for reinforcement learning and prove that they have polynomial bound on the resource required to achieve near optimal return in general markov decision process after observing that the number of action required to approach the optimal return is lower bounded by the mixing time t of the optimal policy in the undiscounted case or by the horizon time t in the discounted case we then give algorithm requiring a number of action and total computation time that 
similarity based fault tolerant retrieval in neural associative memory nam ha not lead to wiedespread application a drawbackof the efficient willshaw model for sparse pattern ste wblh is that the high asymptotic information capacity is oflittle practical use because of high cross talk noise arising in theretrieval for finite size here a new bidirectional iterative retrievalmethod for the willshaw model is presented called crosswise bidirectional cb retrieval providing 
support vector learning machine svm are finding applicationin pattern recognition regression estimation and operator inversionfor ill posed problem against this very general backdrop any method for improving the generalization performance or forimproving the speed in test phase of svms are of increasing interest in this paper we combine two such technique on a patternrecognition problem the method for improving generalization performance the quot virtual support vector quot method 
the technique of bayesian inference have been applied with greatsuccess to many problem in neural computing including evaluationof regression function determination of error bar on prediction and the treatment of hyper parameter however the problem ofmodel comparison is a much more challenging one for which currenttechniques have significant limitation in this paper we show howan extended form of markov chain monte carlo called chaining is able to provide effective estimate 
we develop a generalised form of the independent component analysis ica algorithm introduced by bell and sejnowski amari et al and lately by pearlmutter and parra and also mackay motivated by information theoretic index for exploratory projection pursuit epp we show that maximisation by natural gradient ascent of the divergence of a multivariate distribution from normality using the negentropy a a distance measure yield a generalised ica we introduce a form of nonlinearity which h a an inherently simple form and exhibit the bussgang property within the a lgorithm we show that t his is sufficient to perform ica on data which ha latent variable exhibiting either unimodal or bimodal probability density function pdf or both kurtosis ha been used a a moment based projection pursuit index and a a c ontrast for i ca we introduce a simple a daptive nonlinearity which is formed by on line estimation of the latent variable kurtosis and demonstrate the removal of the standard ica constraint of latent variable pdf modality uniformity 
we study the number of hidden layer required by a multilayer neural network withthreshold unit to compute a function f from rdto f g in spite of similarity withthe characterization of linearly separable boolean function this problem present ahigher level of complexity gibson characterized the function of r which are computablewith just one hidden layer under the assumption that there is no quot multipleintersection point quot and that f is only defined on a compact set we 
in this paper we present a method for discovering approximately common motif also known a active motif in multiple rna secondary structure the secondary structure can be represented a ordered tree i e the order among sibling matter motif in these tree are connected subgraphs that can differ in both substitution and deletion insertion the proposed method consists of two step find candidate motif in a small sample of the secondary structure search all of the secondary structure to determine how frequently these motif occur within the allowed approximation in the secondary structure to reduce the running time we develop two optimization heuristic based on sampling and pattern matching technique experimental result obtained by running these algorithm on both generated data and rna secondary structure show the good performance of the algorithm to demonstrate the utility of our algorithm we discus their application to conducting the phylogenetic study of rna sequence obtained from genbank 
in previous work we advanced a new technique for direct visual matching of image for the purpose of face recognition and image retrieval using a probabilistic measure of similarity based primarily on a bayesian map analysis of image difference leading to a quot dual quot basis similar to eigenfaces the performance advantage of this probabilistic matching technique over standard euclidean nearest neighbor eigenface matching wa recently demonstrated using result from darpa s 
we present an on line investment algorithm which achieves almost the same wealth a thebest constant rebalanced portfolio determined in hindsight from the actual market outcome the algorithm employ a multiplicative update rule derived using a framework introduced bykivinen and warmuth our algorithm is very simple to implement and requires only constantstorage and computing time per stock in each trading period we tested the performance of ouralgorithm on real stock data from the new 
classification of finite sequence without explicit knowledge of theirstatistical nature is a fundamental problem with many importantapplications we propose a new information theoretic approachto this problem which is based on the following ingredient i sequencesare similar when they are likely to be generated by the samesource ii cross entropy can be estimated via quot universal compression quot iii markovian sequence can be asymptotically optimallymerged with these ingredient 
in this paper we adopt general sum stochasticgames a a framework for multiagent reinforcementlearning our work extends previouswork by littman on zero sum stochasticgames to a broader framework we designa multiagent q learning method underthis framework and prove that it convergesto a nash equilibrium under specified condition this algorithm is useful for finding theoptimal strategy when there exists a uniquenash equilibrium in the game when thereexist multiple nash equilibrium 
we describe and analyze a mixture model for supervised learning of probabilistic transducer we devise an online learning algorithm that efficiently infers the structure and estimate the parameter of each probabilistic transducer in the mixture theoretical analysis and comparative simulation indicate that the learning algorithm track the best transducer from an arbitrarily large possibly infinite pool of model we also present an application of the model for inducing a noun phrase recognizer 
we propose an active learning method with hidden unit reduction which is devised specially for multilayer perceptrons mlp first we review our active learning method and point out that many fisher information based method applied to mlp have a critical problem the information matrix may be singular to solve this problem we derive the singularity condition of an information matrix and propose an active learning technique that is applicable to mlp it effectiveness is verified through 
inference is a key component in learning probabilistic model from partially observable data when learning temporal model each of the many inference phase requires a complete traversal over a potentially very long sequence furthermore the data structure propagated in this procedure can be extremely large making the whole process very demanding in we describe an approximate inference algorithm for monitoring stochastic process and prove bound on it approximation error in this paper we apply this algorithm a an approximate forward propagation step in an em algorithm for learning temporal bayesian network we also provide a related approximation for the backward step and prove error bound for the combined algorithm we show that em using our inference algorithm is much faster than em using exact inference with no degradation of the quality of the learned model we then extend our analysis to the online learning task showing a bound on the error resulting from restricting attention to a small window of observation we present an online em learning algorithm for dynamic system and show that it learns much faster than standard offline em 
we present an analog vlsi cellular architecture implementing a simplified version of the boundary contour system bcs for real time image processing inspired by neuromorphic model across several layer of visual cortex the design integrates in each pixel the function of simple cell complex cell hyper complex cell and bipole cell in three orientation interconnected on a hexagonal grid analog current mode cmos circuit are used throughout to perform edge detection local inhibition directionally selective long range diffusive kernel and renormalizing global gain control experimental result from a fabricated pixel prototype in m cmos technology demonstrate the robustness of the architecture in selecting image contour in a cluttered and noisy background 
we present a method for learning complex appearance mapping such a occur with image of articulated object traditional interpolation network fail on this case since appearance is not necessarily a smooth function nor a linear manifold for articulated object we define an appearance mapping from example by constructing a set of independently smooth interpolation network these network can cover overlapping region of parameter space a set growing procedure is used to find example cluster which are well approximated within their convex hull interpolation then proceeds only within these set of example with this method physically valid image are produced even in region of parameter space where nearby example have different appearance we show result generating both simulated and real arm image 
real valued random hidden variable can be useful for modellinglatent structure that explains correlation among observed variable i propose a simple unit that add zero mean gaussian noiseto it input before passing it through a sigmoidal squashing function such unit can produce a variety of useful behavior rangingfrom deterministic to binary stochastic to continuous stochastic ishow how quot slice sampling quot can be used for inference and learningin top down network of these 
we describe a system we developed for identifyingtrends in text document collected over a period oftime trend can be used for example to discoverthat a company is shifting interest from one domainto another our system us several data mining techniquesin novel way and demonstrates a method inwhich to visualize the trend we also give experiencesfrom applying this system to the ibm patentserver a database of u s patent introductionwe address the problem of discovering 
there is currently considerable interest in developing general nonlinear density model based on latent or hidden variable suchmodels have the ability to discover the presence of a relatively smallnumber of underlying cause which acting in combination giverise to the apparent complexity of the observed data set unfortunately to train such model generally requires large computational effort in this paper we introduce a novel latent variable algorithm which retains the 
model learning combined with dynamic programming ha been shown tobe effective for learning control of continuous state dynamic system thesimplest method assumes the learned model is correct and applies dynamicprogramming to it but many approximators provide uncertainty estimateson the fit how can they be exploited this paper address the casewhere the system must be prevented from having catastrophic failure duringlearning we propose a new algorithm adapted from the dual control 
we study the classification problem thatarises when two variable one continuous x one discrete s evolve jointly intime we suppose that the vector x tracesout a smooth multidimensional curve to eachpoint of which the variable s attache a discretelabel the trace of s thus partition thecurve into different segment whose boundariesoccur where s change value we considerhow to learn the mapping between xand s from example of segmented curve our approach is to model the 
algorithm scalability and the distributed nature of both data and computation deserve serious attention in the context of data mining this paper present padma parallel data mining agent a parallel agent based system that make an effort to address these issue padma contains module for parallel data accessing operation parallel hierarchical clustering and webbased data visualization this paper describes the general architecture of padma and experimental result 
we present result on the use of neural network based autoassociators which act a novelty or anomaly detector to detect imminent motor failure the autoassociator is trained to reconstruct spe ctra obtained from the healthy motor in laboratory test we have demonstrated that the trained autoassociator ha a small reconstruction error on measurement recorded from healthy motor but a larger error on those reco rded from a motor with a fault we have designed and built a motor monitoring system using an autoassociator for anomaly detection and are in the process of testing the system at three industrial and commercial site 
we have combined an artificial neural network ann character classifier with context driven search over character segmentation word segmentation and word recognition hypothesis to provide robust recognition of hand printed english text in new model of apple computer s newton messagepad we present some innovation in the training and use of anns a character classifier for word recognition including normalized output error frequency balancing error emphasis negative training and stroke warping a recurring theme of reducing a priori bias emerges and is discussed 
we define a gamma multi layer perceptron mlp a an mlpwith the usual synaptic weight replaced by gamma filter a proposedby de vries and principe de vries amp principe andassociated gain term throughout all layer we derive gradientdescent update equation and apply the model to the recognitionof speech phoneme we find that both the inclusion of gammafilters in all layer and the inclusion of synaptic gain improvesthe performance of the gamma mlp we compare the gamma 
coevolutionary learning which involves theembedding of adaptive learning agent ina fitness environment that dynamically respondsto their progress is a potential solutionfor many technological chicken andegg problem however several impedimentshave to be overcome in order for coevolutionarylearning to achieve continuous progressin the long term this paper present someof those problem and proposes a frameworkto address them this presentation is illustratedwith a case study 
a constructive induction model using geneticprogramming is presented the modelevolves new attribute starting from a randompopulation of possible attribute constructedas function of the original attribute the model is tested on hard supervisedlearning problem and it performanceis compared with backpropagation and c the performance of the system on learningincomplete bit parity is reported to be better introductionconstructive induction ci is an effort to improve the 
a common way to represent a time series is to divide it into shortduration block each of which is then represented by a set of basis function a limitation of this approach however is that the temporal alignment of the basis function with the underlying structure in the time series is arbitrary we present an algorithm for encoding a time series that doe not require blocking the data the algorithm find an efficient representation by inferring the best temporal position for function in a kernel basis these can have arbitrary temporal extent and are not constrained to be orthogonal this allows the model to capture structure in the signal that may occur at arbitrary temporal position and preserve the relative temporal structure of underlying event the model is shown to be equivalent to a very sparse and highly overcomplete basis under this model the mapping from the data to the representation is nonlinear but can be computed efficiently this form also allows the use of existing method for adapting the basis itself to data this approach is applied to speech data and result in a shift invariant spike like representation that resembles coding in the cochlear nerve 
sequential data this paper is about the unsupervised discovery of pattern in sequence of composite object a composite object may be described a a sequence of other simpler data in such case not only the nature of the component is important but also the order in which these component appear the present work study the problem of generalizing sequence of complex object a formal definition of generalized sequence is given and an algorithm is derived because of the excessive computational complexity of this algorithm a heuristic version is described this algorithm is then integrated in a general purpose clustering algorithm the result is a knowledge discovery system which is able to analyze any structured database on the base of a unified unsupervised mechanism 
powerful method for interactive exploration and search from collection of free form textual document are needed to manage the ever increasing flood of digital information in this article we present a method websom for automatic organization of full text document collection using the self organizing map som algorithm the document collection is ordered onto a map in an unsupervised manner utilizing statistical information of short word context the resulting ordered map where similar document lie near each other thus present a general view of the document space with the aid of a suitable wwwbased interface document in interesting area of the map can be browsed the browsing can also be interactively extended to related topic which appear in nearby area on the map along with the method we present a case study of it use 
periodicity search that is search for cyclicity in time related database is an interesting data mining problem most previous study have been on finding full cycle periodicity for all the segment in the selected sequence of the data that is if a sequence is periodic all the point or segment in the period repeat however it is often usefill to mine segment wise or point wise periodicity in time related data set in this study we integrate data cube and apriori data mining technique for mining segment wise periodicity in regard to a fixed length period and show that data cube provides an efficient structure and a convenient way for interactive mining of multiple level periodicity 
declarative bias play an important rolewhen learning in potentially huge hypothesisspaces while scientific discovery system which perform equation discovery a a subtask consider such potentially huge hypothesisspaces few if any employ declarative asopposed to hard coded bias to define and restricttheir hypothesis space we present anequation discovery system lagramge thatuses grammar to define and restrict it hypothesisspace these grammar can makeuse of 
we describe the result of performing data mining on a challenging medical diagnosis domain acute abdominal pain this domain is well known to be difficult yielding little more than predictive accuracy for most human and machine diagnostician moreover many researcher argue that one of the simplest approach the naive bayesian classifier is optimal by comparing the performance of the naive bayesian classifier to it more general cousin the bayesian network classifter and to selective bayesian classifier with just of the total attribute we show that the simplest model perform at least a well a the more complex model we argue that simple model like the selective naive bayesian classifier will perform a well a more complicated model for similarly complex domain with relatively small data set thereby calling into question the extra expense necessary to induce more complex model 
the problem of time series prediction is studied within the uniform convergence framework of vapnik and chervonenkis the dependence inherent in the temporal structure is incorporated into the analysis ther eby generalizing the available theory for memoryless process finite sample bound are calculated in term of covering number of the approximating class and the tradeoff between approximation and estimation is discussed a complexity regularization approach is outlined based on vapnik s method of structural risk minimization and shown to be applicable in the context of mixing stochastic process 
a globally convergent homotopy method is defined that is capable of sequentiallyproducing large number of stationary point of the multi layerperceptron mean squared error surface using this algorithm large subsetsof the stationary point of two test problem are found it is shown empiricallythat the mlp neural network appears to have an extreme ratioof saddle point compared to local minimum and that even small neuralnetwork problem have extremely large number of solution 
in many data mining problem the definition of what structure in the database are to be regarded a interesting or valuable is given only loosely typically this is regarded a a source of ambiguity and imprecision however we propose taking advantage of the looseness of the definition by choosing a particular definition which optimises some additional criterion we illustrate using a consumer credit data set where the definition of what constitutes a bad risk customer is somewhat arbitrary instead of adopting the common strategy of freely choosing some definition we choose that which optimises predictability that is we choose to define our class on the ground that they are the one amongst those which can be most accurately predicted 
we present a new algorithm for associative reinforcement learning the algorithm is based upon the idea of matching a network s output probability with a probability distribution derived from the environment s reward signal this probability matching algorithm is shown to perform faster and be le susceptible to local minimum than previously existing algorithm we use probability matching to train mixture of expert network an architecture for which other reinforcement learning rule fail to converge reliably on even simple problem this architecture is particularly well suited for our algorithm a it can compute arbitrarily complex function yet calculation of the output probability is simple 
sequence of event are an important special form of data that arises in several context including telecommunication user interface study and epidemiology we present a general and flexible framework of specifying class of generalized episode these are recurrent combination of event satisfying certain condition the framework can be instantiated to a wide variety of application by selecting suitable primitive condition we present algorithm for discovering frequently occurring episode and episode rule the algorithm are based on the use of minimal occurrence of episode this make it possible to evaluate confidence of a wide variety of rule using only a single analysis pas we present empirical result on t he behavior of t he algorithm on event stemming from a www log 
this paper introduces a probability model the mixture of tree that can account for sparse dynamically changing dependence relationship we present a family of efficient algorithm that use emand the minimum spanning tree algorithm to find the ml and map mixtureof tree for a variety of prior including the dirichlet and the mdl prior 
reinforcement learning method can be used to improve the performance of local search algorithm for combinatorial optimization by learning an evaluation function that predicts the outcome of search the evaluation function is therefore able to guide search to low cost solution better than can the original cost function we describe a reinforcement learning method for enhancing local search that combine aspect of previous work by zhang and dietterich and boyan and moore boyan in an off line learning phase a value function is learned that is useful for guiding search for multiple problem size and instance we illustrate our technique by developing several such function for the dial a ride problem our learning enhanced local search algorithm exhibit an improvement of more then over a standard local search algorithm 
in this paper we discus regularisation in online sequential learningalgorithms in environment where data arrives sequentially technique such a cross validation to achieve regularisation ormodel selection are not possible further bootstrapping to determinea confidence level is not practical to surmount theseproblems a minimum variance estimation approach that make useof the extended kalman algorithm for training multi layer perceptronsis employed the novel contribution 
abstract several clustering algorithm can be applied to clustering in large multimedia database the e ectiveness and e ciency of the existing algorithm however is somewhat limited since clustering in multimedia database requires cluster ing high dimensional feature vector and since multimedia database often contain large amount of noise in this pa per we therefore introduce a new algorithm to clustering in large multimedia database called denclue density based clustering the basic idea of our new approach is to model the overall point density analytically a the sum of in uence function of the data point cluster can then be identi ed by determining density attractor and cluster of arbitrary shape can be easily described by a simple equa tion based on the overall density function the advantage of our new approach are it ha a rm mathematical basis it ha good clustering property in data set with large amount of noise it allows a compact mathematical de scription of arbitrarily shaped cluster in high dimensional data set and it is signi cantly faster than existing algo rithms to demonstrate the e ectiveness and e ciency of denclue we perform a series of experiment on a num ber of di erent data set from cad and molecular biology a comparison with dbscan show the superiority of our new approach keywords clustering algorithm density based clus tering clustering of high dimensional data clustering in multimedia database clustering in the presence of noise introduction because of the fast technological progress the amount of data which is stored in database increase very fast the type of data which are stored in the computer be come increasingly complex in addition to numerical data complex d and d multimedia data such a im age cad geographic and molecular biology data are stored in database for an e cient retrieval the com plex data is usually transformed into high dimensional feature vector example of feature vector are color histogram sh shape descriptor jag mg fourier vector ww text descriptor kuk etc in many of the mentioned application the database are very large and consist of million of data object with several ten to a few hundred of dimension automated knowledge discovery in large multimedia database is an increasingly important research issue clustering and trend detection in such database how ever is di cult since the database often contain large amount of noise and sometimes only a small portion of the large database account for the clustering in addition most of the known algorithm do not work ef ciently on high dimensional data the method which 
this paper present a study about functional model for r egression tree leaf we e valuate experimentally several alternative to the average commonly used in regression tree we have implemented a regression tree learner htl that i s able to use several alternative model in the tree leaf we study the effect on accuracy and the c omputational cost of these alternative the e xperiments carried out on data set revealed that it is possible to significantly outperform the naive average of regression tree among the four alternative model that we evaluated kernel regressors were usually the best in term of accuracy our study also indicates that by integrating regression tree with other regression approach we are able to overcome the limitation of individual m ethods both in term of accuracy a well a in computational efficiency 
we consider the problem of finding rule relating pattern in a time series to other pattern in that series or pattern in one series to pattern in another series a simple example is a rule such a a period of low telephone call activity is usually followed by a sharp rise ill call vohune example of rule relating two or more time series are if the microsoft stock price go up and lntel fall then ibm go up the next day and if microsoft go up strongly fro one day then decline strongly on the next day and on the same day intel stay about level then ibm stay about level our emphasis is in the discovery of local pattern in multivariate time series in contrast to traditional time series analysis which largely focus on global model thus we search for rule whose condition refer to pattern in time series however we do not want to define beforehand which pattern are to be used rather we want the pattern to be formed fl om the data in the context of rule discovery we describe adaptive method for finding rule of the above type fi om time series data the method are based on discretizing the sequence hy method resembling vector quantization ve first form subsequence by sliding window through the time series and then cluster these subsequence by using a suitable measure of time series similarity the discretized version of the time series is obtained by taldng the cluster identifier corresponding to the subsequence once tl e time series is discretized we use simple rule finding method to obtain rifle from the sequence vve present empmcal resuh s on the behavior of the method 
this paper introduces a new algorithm q for optimizing the expectedoutput of a multi input noisy continuous function q is designed toneed only a few experiment it avoids strong assumption on the formof the function and it is autonomous in that it requires little problemspecifictweaking these capability are directly applicable to industrial process andmay become increasingly valuable elsewhere a the machine learningfield expands beyond prediction and function 
we derive a first order approximation of the density of maximum entropy for a continuous d random variable given a number of simple constraint this result in a density expansion which is somewhat similar to the classical polynomial density expansion by gram charlier and edgeworth using this approximation of density an approximation of d differential entropy is derived the approximation of entropy is both more exact and more robust against outlier than the classical approximation 
we show finite time regret bound for the multiarmed bandit problem under the assumption that all reward come from a bounded and fixed range our regret bound after any number of pull are of the form where and are positive constant not depending on these bound are shown to hold for variant of the popular greedy and boltzmann allocation rule and for a new simple deterministic allocation rule moreover our result also apply to an extension of the basic bandit problem in which reward distribution can depend to some extent from previous pull and observed reward finally we discus the empirical performance of our algorithm with respect to specific choice of the reward distribution 
most decision tree algorithm focus on univariate i e axis parallel test at each internalnode of a tree oblique decision treesuse multivariate linear test at each non leafnode this paper report a novel approach tothe construction of non linear decision tree the crux of this method consists of the generationof new feature and the augmentationof the primitive feature with these newones the resulted non linear decision treesare more accurate than their axis parallel 
we introduce two new technique for density estimation our approachposes the problem a a supervised learning task which canbe performed using neural network we introduce a stochasticmethod for learning the cumulative distribution and an analogousdeterministic technique we demonstrate convergence of ourmethods both theoretically and experimentally and provide comparisonswith the parzen estimate our theoretical result demonstratebetter convergence property than the parzen 
we describe a reinforcement learning algorithm for partially observable environment using short term memory which we call blht since blht learns a stochastic model based on bayesian learning the overfitting problem is reasonably solved moreover blht ha an efficient implementation this paper show that the model learned by blht converges to one which provides the most accurate prediction of percept and reward given short term memory 
many real life problem require a partial classificationof the data we use the term quot partial classification quot to describe the discovery of model that show characteristicsof the data class but may not cover allclasses and all example of any given class completeclassification may be infeasible or undesirable whenthere are a very large number of class attribute mostattributes value are missing or the class distributionis highly skewed and the user is interested in understanding 
we propose a new method to compute prediction interval especiallyfor small data set the width of a prediction interval doe not only dependon the variance of the target distribution but also on the accuracyof our estimator of the mean of the target i e on the width of the confidenceinterval the confidence interval follows from the variation inan ensemble of neural network each of them trained and stopped onbootstrap replicates of the original data set a second improvement is 
we derive a learning algorithm for inferring an overcomplete basisby viewing it a probabilistic model of the observed data overcompletebases allow for better approximation of the underlyingstatistical density using a laplacian prior on the basis coefficientsremoves redundancy and lead to representation that are sparseand are a nonlinear function of the data this can be viewed asa generalization of the technique of independent component analysisand provides a method for blind 
we present a general encoding decoding framework for interpreting the activity of a populationof unit a standard population code interpretation method the poisson model startsfrom a description a to how a single value of an underlying quantity can generate the activitiesof each unit in the population in casting it in the encoding decoding framework we findthat this model is too restrictive to describe fully the activity of unit in population code inhigher processing area such 
we describe two parallel analog vlsi architecture that integrateoptical flow data obtained from array of elementary velocity sensorsto estimate heading direction and time to contact for headingdirection computation we performed simulation to evaluate themost important qualitative property of the optical flow field anddetermine the best functional operator for the implementation ofthe architecture for time to contact we exploited the divergencetheorem to integrate data 
geoscience study produce data from various observation experiment and simulation at anenormous rate exploratory data mining extract quot content information quot from massive geoscientificdatasets to extract knowledge and provide a compactsummary of the dataset in this paper wediscuss how database query processing and distributedobject management technique can beused to facilitate geoscientific data mining andanalysis some special requirement of large scalegeoscientific data 
coarse code are widely used throughout the brain to encode sensoryand motor variable method designed to interpret thesecodes such a population vector analysis are either inefficient i e the variance of the estimate is much larger than the smallest possiblevariance or biologically implausible like maximum likelihood moreover these method attempt to compute a scalar or vectorestimate of the encoded variable neuron are faced with a similarestimation problem they must read 
because of the distance between the skull and brain and their differentresistivities electroencephalographic eeg data collected fromany point on the human scalp includes activity generated withina large brain area this spatial smearing of eeg data by volumeconduction doe not involve significant time delay however suggestingthat the independent component analysis ica algorithmof bell and sejnowski is suitable for performing blind source separationon eeg data the ica 
classification rule mining aim to discover a small set of rule in the database that form an accurate classifier association rule mining find all the rule existing in the database that satisfy some minimum support and minimum confidence constraint for association rule mining the target of discovery is not pre determined while for classification rule mining there is one and only one pre determined target in this paper we propose to integrate these two mining technique the integration is done by focusing on mining a special subset of association rule called class association rule car an efficient algorithm is also given for building a classifier based on the set of discovered car experimental result show that the classifier built this way is in general more accurate than that produced by the state of the art classification system c in addition this integration help to solve a number of problem that exist in the current classification system 
cumulative training margin distributionsfor adaboost versusour quot direct optimization ofmargins quot doom algorithm the dark curve is adaboost thelight curve is doom doomsacrifices significant training errorfor improved test error horizontalmarks on margin line introductionmany learning algorithm for pattern classification minimize some cost function ofthe training data with the aim of minimizing error the probability of misclassifyingan example one example of such 
several researcher have proposed modeling temporally abstract action in reinforcement learningby the combinationof a policyand a termination condition which we refer to a an option value function over option and model of option can be learned using method designed for semi markov decision process smdps however all these method require an option to be executed to termination in this paper we explore method that learn about an option from small fragment of experience consistent with that option even if the option itself is not executed we call these method intra option learning method because they learn from experience within an option intra optionmethodsare sometimes much more efficient than smdp method because they can use off policy temporaldifference mechanism to learn simultaneously about all the option consistent with an experience not just the few that were actually executed inthispaperwepresentintra optionlearningmethodsforlearningvaluefunctionsoveroptions and for learning multi time model of the consequence of option we present computational example in which these new method learn much faster than smdp method and learn effectively when smdp method cannot learn at all we also sketch a convergenceprooffor intraoption value learning 
the vestibulo ocular reflex vor stabilizes image on the retina during rapid head motion the gain of the vor i e the ratio of eye to head rotation velocity measured with the eye focused at a distance is typically around however to stabilize image accurately the vestibulo ocular reflex response must be modulated by the context in which the response take place the context studied in this paper includes eye position eye vergence and head translation we first describe a kinematic model of the vestibulo ocular reflex response given rotational and translational information of the head motion we then show how the model can be modified to rely solely on sensory information available from the semicircular canal head rotation the otoliths head translation and neural correlate of eye position and vergence angle we then suggest a dynamical model and compare it to the eye velocity response measured in monkey snyder and king the vor response is nearly identical to an ideal theoretical response taking into account sensory input delay the model capture the dynamical modulation of the vor for this context it defines which neural signal should be combined and suggests one possible way to perform the required computation in time it therefore provides a theoretical explanation for the sensitivity of neuron to multiple input head rotation and translation eye position etc in the pathway of the vor 
this paper introduces adhoc automatic discoverer of higher order correlation an algorithm that combine the advantage of both filter and feedback model to enhance the understanding of the given data and to increase the efficiency of the feature selection process adhoc partition the observed feature into a number of group called factor that reflect the major dimension of the phenomenon under consideration the set of learned factor define the starting point of the search of the best performing feature subset a genetic algorithm is used to explore the feature space originated by the factor and to determine the set of most informative feature configuration the feature subset evaluation function is the performance of the induction algorithm this approach offer three main advantage i the likelihood of selecting good performing feature grows ii the complexity of search diminishes consistently iii the possibility of selecting a bad feature subset due to over fitting problem decrease extensive experiment on real world data have been conducted to demonstrate the effectiveness of adhoc a data reduction technique a well a feature selection method 
we present a framework for characterizing bayesianclassification method this framework can bethought of a a spectrum of allowable dependence in agiven probabilistic model with the naive bayes algorithmat the most restrictive end and the learning offull bayesian network at the most general extreme while much work ha been carried out along the twoends of this spectrum there ha been surprising littledone along the middle we analyze the assumptionsmade a one move along this 
we study the dynamic of supervised learning in layered neural network in the regime where the size p of the training set is proportional to the number n of input here the local field are no longer described by gaussian probability distribution we show how dynamical replica theory can be used to predict the evolution of macroscopic observables including the relevant performance measure incorporating the old formalism in the limit p n a a special case for simplicity we restrict ourselves to single layer network and realizable task 
within valiant s model of learning a formalizedby kearns we show that computable totalpredicates for two formally uncomputableproblems the classical halting problem andthe halting problem relative to a specifiedoracle are formally learnable from example to arbitrarily high accuracy with arbitrarilyhigh confidence under any probability distribution the halting problem relative tothe oracle is learnable in time polynomial inthe measure of accuracy confidence and the 
it is known that human can make finer discrimination betweenfamiliar sound e g syllable than between unfamiliar one e g different noise segment here we show that a corresponding enhancementis present in early auditory processing stage based onprevious work which demonstrated that natural sound had robuststatistical property that could be quantified we hypothesize thatthe auditory system exploit those property to construct efficientneural code to test this 
this paper present an innovative application of the disciple learning agent shell to the building of an educational agent that generates history test for middle school student to assist in the assessment of their understanding and use of higher order thinking skill disciple ha been taught by an educator to generate and answer basic test question and to explain the answer from it interaction with the educational expert disciple ha learned general rule that allow it to generate a large number of new test question for student together with hint answer and explanation of the answer a a result it can guide the student during their practice of higher order thinking skill a they would be directly guided by the educator it can also be used by the educator to generate a different exam for each student in the class disciple ha been experimentally evaluated by history expert student and tea chers with very promising result the work on developing this educational agent illustrates an integration of machine learning knowledge acquisition problem solving and intelligent tu toring system in the context of computer based assessment involving multimedia document 
information extraction ie is the problemof filling out pre defined structured summariesfrom text document we are interestedin performing ie in non traditionaldomains where much of the text is oftenungrammatical such a electronic bulletinboard post and web page we suggest thatthe best approach is one that take into accountmany different kind of information and argue for the suitability of a multistrategyapproach we describe learner for iedrawn from three separate machine 
we present a new machine learning method that given a set of training example induces a denition of the target concept in term of a hierarchy of intermediate concept and their denitions this eectively decomposes the problem into smaller le complex problem the method is inspired by the boolean function decomposition approach to the design of digital circuit to cope with high time complexity of nding an optimal decomposition we propose a suboptimal heuristic algorithm the method implemented in program hint hierarchy induction tool is experimentally evaluated using a set of articial and real world learning problem it is shown that the method performs well both in term of classication accuracy and discovery of meaningful concept hierarchy 
this paper address large scale regression tasksusing a novel combination of greedy input selectionand asymmetric cost our primary goalis learning envelope function suitable for automateddetection of anomaly in future sensordata we argue that this new approachcan be more effective than traditional technique such a static red line limit variance based errorbars and general probability density estimation introduction this paper explores the combination of a 
smoothing regularizers for radial basis function have been studied extensively but no general smoothing regularizers for projective basis function pbfs such a the widely used sigmoidal pbfs have heretofore been proposed we derive new class of algebraically simple m th order smoothing regularizers for network of projective basis function our simple algebraic form enable the direct enforcement of smoothness without the need for e g costly monte carlo integration of the smoothness functional we show that our regularizers are highly correlated with the value of standard smoothness functionals and thus suitable for enforcing smoothness constraint onto pbf network the regularizers are tested on illustrative sample problem and compared to quadratic weight decay the new regularizers are shown to yield better generalization error than weight decay when the implicit assumption in the latter are wrong unlike weight decay the new regularizers distinguish between the role of the input and output weight and capture the interaction between them short version of report long version is cse 
the task in text retrieval is to find the subset of a collection of document relevant to a user s information request usually expressed a a set of word classically document and query are represented a vector of word count in it simplest form relevance is defined to be the dot product between a document and a query vector a measure of the number of common term a central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query linear dimensionality reduction ha been proposed a a technique for extracting underlying structure from the document collection in some domain such a vision dimensionality reduction reduces computational complexity in text retrieval it is more often used to improve retrieval performance we propose an alternative and novel technique that produce em sparse representation constructed from set of highly related word document and query are represented by their distance to these set and relevance is measured by the number of common cluster this technique significantly improves retrieval performance is efficient to compute and share property with the optimal linear projection operator and the independent component of document 
huge mass of digital data about product customer and competitor have become available for company in the service sector in order to exploit it inherent and often hidden knowledge for improving business process the application of data mining technology is the only way for reaching good and efficient result a opposed to purely manual and interactive data exploration this paper report on a project initiated at swiss life for mining it data resource from the life insurance business based on the data warehouse masy collecting all relevant data from the oltp system for the processing of private life insurance contract a data mining environment is set up which integrates a palette of tool for automatic data analysis in particular machine learning approach special emphasis lie on establishing comfortable data preprocessing support for normalised relational database and on the management of meta data 
we report on our development of a high performance system for neural network and other signal processing application we have designed and implemented a vector microprocessor and packaged it a an attached processor for a conventional workstation we present performance comparison with workstation on neural network backpropagation training the spert ii system demonstrates roughly time the performance of a mid range workstation and five time the performance of a high end workstation with extensive hand optimization of both workstation version 
we predict stock market using information contained in article published on the web mostly textual article appearing in the leading and the most influential financial newspaper are taken a input from those article the daily closing value of major stock market index in asia europe and america are predicted textual statement contain not only the effect e g stock down but also the possible cause of the event e g stock down because of weakness in the dollar and consequently a weakening of the treasury bond exploiting textual information therefore increase the quality of the input the forecast are available real time via www c ust hk beat predict daily at am hong kong time hence all prediction are available before the major asian market start trading several technique such a rule based k nn algorithm and neural net have been employed to produce the forecast those technique are compared with one another a trading strategy based on the system s forecast is suggested 
gaussian process provide good prior model for spatial data but can be too smooth in many physical situation there are discontinuity along bounding surface for example front in near surface wind field we describe a modelling method for such a constrained discontinuity and demonstrate how to infer the model parameter in wind fiel d with mcmc sampling 
like model selection in statistic the choice of appropriate data mining algorithm dm algorithm is a very important task in the process of knowledge discovery due to this fact it is necessary to have sophisticated metric that can be used a comparators to evaluate alternative dmalgorithms it ha been shown in literature that data envelopment analysis dea is an appropriate platform to develop multi criterion evaluation metric that can consider in contrary to mono criterion metric all positive and negative property of dm algorithm we discus different extension of dea that enable consideration of qualitative property of dm algorithm and consideration of user preference in development of evaluation metric the result open new discussion in the general debate on model selection in statistic and machine learning 
perceptron decision tree also known a linear machine dts etc are analysed in order that data dependent structural risk minimizationcan be applied data dependent analysis is performed which indicates thatchoosing the maximal margin hyperplanes at the decision node will improvethe generalization the analysis us a novel technique to bound thegeneralization error in term of the margin at individual node experimentsperformed on real data set confirm the validity of the 
in this paper we consider learning first order horn programsfrom entailment in particular we show that any subclass of first orderacyclic horn program with constant arity is exactly learnable fromequivalence and entailment membership query provided it allows apolynomial time subsumption procedure and satisfies some closure condition one consequence of this is that first order acyclic determinatehorn program with constant arity are exactly learnable from equivalenceand 
this paper present a new approach to hierarchical reinforcement learning based on the maxq decomposition of the value function the maxq decomposition ha both a procedural semantics a a subroutine hierarchy and a declarative semantics a a representation of the value function of a hierarchical policy maxq unifies and extends previous work on hierarchical reinforcement learning by singh kaelbling and dayan and hinton condition under which the maxq decomposition can represent the optimal value function are derived the paper defines a hierarchical q learning algorithm prof it convergence and show experimentally that it can learn much faster than ordinary flat q learning finally the paper discus some interesting issue that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non hierarchical execution of the maxq hierarchy 
recent experiment show that the neural code at work in a wide range of creature share some common feature at first sight these observation seem unrelated however we show that these feature arise naturally in a linear filtered threshold crossing model when we set the threshold to maximize the transmitted information this maximization process requires neural adaptation to not only the dc signal level a in conventional light and dark adaptation but also to the statistical structure of the signal and noise distribution we also present a new approach for calculating the mutual information between a neuron s output spike train and any aspect of it input signal which doe not require reconstruction of the input signal this formulation is valid provided the correlation in the spike train are small and we provide a procedure for checking this assumption this paper is based on joint work deweese m optimization principle for the neural code dissertation princeton university preliminary result from the linear filtered threshold crossing model appeared in a previous proceeding deweese m and bialek w information flow in sensory neuron nuovo cimento d and the conclusion we reached at that time have been reaffirmed by further analysis of the model 
naive bayes induction algorithm were previously shown to be surprisingly accurate on many classification task even when the conditional independence assumption on which they are based is violated however most study were done on small database we show that in some larger database the accuracy of naive bayes doe not scale up a well a decision tree we then propose a new algorithm nbtree which induces a hybrid of decision tree classifier and naive bayes classifier the 
in recent year the interest of investor ha shifted to computerized asset allocation portfolio management to exploit the growing dynamic of the capital market in this paper asset allocation is formalized a a markovian decision problem which can be optimized by applying dynamic programming or reinforcement learning based algorithm using an artificial exchange rate the asset allocation strategy optimized with reinforcement learning q learning is shown to be equivalent to a policy 
a data warehouse grow to the point where one hundred gigabyte is considered small the computational efficiency of data mining algorithm on large database becomes increasingly important using a sample from the database can speed up the datamining process but this is only acceptable if it doe not reduce the quality of the mined knowledge to this end we introduce the probably close enough criterion to describe the desired property of a sample sampling usually refers to the use of static statistical test to decide whether a sample is sufficiently similar to the large database in the absence of any knowledge of the tool the data miner intends to use we discus dyrz mic sampling method which take into account the mining tool being used and can thus give better sample we describe dynamic scheme that observe a mining tool s performance on training sample of increasing size and use these result to determine when a sample is sufficiently large we evaluate these sampling method on data from the uc repository and conclude that dynamic sampling is preferable 
if it is to qualify a knowledge a learner soutput should be accurate stable and comprehensible learning multiple model canimprove significantly on the accuracy andstability of single model but at the costof losing their comprehensibility when theypossess it a do for example simple decisiontrees and rule set this paper proposes andevaluates cmm a meta learner that seek toretain most of the accuracy gain of multiplemodel approach while still producinga single 
this paper present splice a batch metalearningsystem designed to learn locally stableconcepts in domain with hidden changesin context the majority of machine learningalgorithms assume that target concept remainstable over time in many domain thisassumption is invalid for example financialprediction medical diagnosis and networkperformance are domain in which targetconcepts may not remain stable unstabletarget concept are often due to changesin a hidden context 
field ha suggested that neuron with line and edge selectivitiesfound in primary visual cortex of cat and monkey form a sparse distributedrepresentation of natural scene and barlow ha reasonedthat such response should emerge from an unsupervised learning algorithmthat attempt to find a factorial code of independent visual feature weshow here that non linear infomax when applied to an ensemble of naturalscenes produce set of visual filter that are localised 
estimating motion in scene containing multiple moving object remains a di cult problem in computer vision a promising ap proach to this problem involves using mixture model where the motion of each object is a component in the mixture however ex isting method typically require specifying in advance the number of component in the mixture i e the number of object in the scene 
in a recent paper friedman geiger and goldszmidt introduced a classifier based on bayesian network called tree augmented naive bayes tan that outperforms naive bayes and performs competitively with c and other state of the art method this classifier ha several advantage including robustness and polynomial computational complexity one limitation of the tan classifier is that it applies only to discrete attribute and thus con tinuous attribute must be prediscretized in this paper we extend tan to deal with continuous attribute directly via parametric e g gaussians and semiparametric e g mixture of gaussians conditional probability the resu lt is a classifier that can represent and combine both discrete and continuous attribute in addition we propose a new method that take advantage of the modeling language of bayesian network in order to represent attribute both in discrete and continuous form simultaneously and use both version in the classification this automates the process of deciding which form of the attribute is most relevant to the classification task it also avoids the commitment to either a discretized or a semi parametric form since different attribute may correlate better with one version or the other our empirical result show that this latter method usually achieves classification performance that is a good a or better than either the purely discrete or the purely continuous tan model 
an approach to clustering is presented that adapts the basic top down induction of decision tree method towards clustering to this aim it employ the principle of instance based learning the resulting methodology is implemented in the tic top down induction of clustering tree system for first order clustering the tic system employ the first order logical decision tree representation of the inductive logic programming system tilde various experiment with tic are presented in both propositional and relational domain 
this paper describes a simple and efficient method to make template based object classification invariant to in plane rotation the task is divided into two part orientation discrimination and classification the key idea is to perform the orientation discrimination before the classification this can be accomplished by hypothesizing in turn that the input image belongs to each class of interest the image can then be rotated to maximize it similarity to the training image in each class these contain the prototype object in an upright orientation this process yield a set of image at least one of which will have the object in an upright position the resulting image can then be classified by model which have been trained with only upright example this approach ha been successfully applied to two real world vision based task rotated handwritten digit recognition and rotated face detection in cluttered scene 
we study generalization capability of the mixture of expert learningfrom example generated by another network with the samearchitecture when the number of example is smaller than a criticalvalue the network show a symmetric phase where the expertsdoes not specialize unon crossing the critical point the systemundergoes a continuous phase transition to a symmetry breakingphase where the gating network partition the input space effectivelyand each expert is assigned to an 
we study the integration of background knowledge and concept learning genetic algorithm and show how they have been integrated in the system dogma our emphasis is in speeding up the inductive learning process by using suggestion from the background knowledge to direct genetic search we don t do theory revision by patching the old theory rather we build a new theory by using part of the background knowledge result show that the methodology can lead to better result a well a to clear saving in computational effort compared to learning with purely inductive gas 
an important success factor for the field of kdd lie in the development and integration of method for supporting the construction and execution of kdd process crucial aspect in this context are the incremental development of a precise problem description a decomposition of this top level problem description into manageable and compatible subtasks which can be reused and a selection and combination of adequate algorithm for solving these subtasks in this paper we describe an approach 
in this paper we propose a machine learningsolution to problem consisting of many similarprediction task each of the individualtasks ha a high risk of overfitting we combinetwo type of knowledge transfer betweentasks to reduce this risk multi task learningand hierarchical bayesian modeling multitasklearning is based on the assumption thatthere exist feature typical to the task athand to find these feature we train a hugetwo layered neural network each task hasits own 
data mining over large data set is important due to it obvious commercial potential however it is also a major challenge due to it computational complexity exploiting the inherent parallelism of data mining algorithm provides a direct solution by utilising the large data retrieval and processing power of parallel architecture in this paper we present some result of our intensive research on paralielising data mining algorithm in particular we also present a methodology for determining the proper parallelisation strategy based on the idea of algorithmic skeleton and performance modelling this research aim to provide a systematic way to develop parallel data mining algorithm and application 
we investigate the structure of model selection problem via the bias variance decomposition in particular we characterize the essential structure of a model selection task by the bias and variance profile it generates over the sequence of hypothesis class this lead to a new understanding of complexity penalization method first the penalty term in effect postulate a particular profile for the variance a a function of model complexity if the postulated and true profile do not match then systematic under fitting or over fitting result depending on whether the penalty term are too large or too small second it is usually best to penalize according to the true variance of the task and therefore no fixed penalization strategy is optimal across all problem we then use this bias variance characterization to identify the notion of easy and hard model selection problem in particular we show that if the variance profile grows too rapidly in relation to the bias then standard model selection technique become prone to significant error this can happen for example in regression when the independent variable are drawn from wide tailed distribution finally we discus a new model selection strategy that dramatically outperforms standard complexity penalization and hold out method on these hard task 
we discus the temporal difference learning algorithm a applied to approximating the cost to go function of an infinite horizon discounted markov chain the algorithm we analyze update parameter of a linear function approximator on line during a single endless trajectory of an irreducible aperiodic markov chain with a finite or infinite state space we present a proof of convergence with probability a characterization of the limit of convergence and a bound on the resulting 
we introduce an extended representation of time series that allows fast accurate classification and clustering in addition to the ability to explore time series data in a relevance feedback framework the representation consists of piecewise linear segment to represent shape and a weight vector that contains the relative importance of each individual linear segment in the classification context the weight are learned automatically a part of the training cycle in the relevance feedback context the weight are determined by an interactive and iterative process in which user rate various choice presented to them our representation allows a user to define a variety of similarity measure that can be tailored to specific domain we demonstrate our approach on space telemetry medical and synthetic data 
this paper present probabilistic modeling method to solve the problem of discriminating between five facial orientation with very little labeled data three model are explored the first model maintains no inter pixel dependency the second model is capable of modeling a set of arbitrary pair wise dependency and the last model allows dependency only between neighboring pixel we show that for all three of these model the accuracy of the learned model can be greatly improved by augmenting a small number of labeled training image with a large set of unlabeled image using expectation maximization this is important because it is often difficult to obtain image label while many unlabeled image are readily available through a large set of empirical test we examine the benefit of unlabeled data for each of the model by using only two randomly selected labeled example per class we can discriminate between the five facial orientation with an accuracy of with six labeled example we achieve an accuracy of 
no company so far achieved the ultimate goal of zero fault in manufacturing even high quality product occasionally show problem that must be handled a warranty case in this paper we report work done during the development of an early warning system for a large quality information database in the automotive industry we present a multi strategy approach to flexible prediction of upcoming quality problem we used existing technique and combined them in a novel way to solve a concrete application problem the basic idea is to identify sub population that at an early point in time behave like the whole population at a later time such sub population act a early indicator for future development we present our method in the context of a concrete application and present experimental result at the end of the paper we outline how this method can be generalised and transferred to other kdd application problem 
discretization is the process of dividing a continuousvalued base attribute into discrete interval which highlight distinct pattern in the behavior of a related goal attribute in this paper we present an integrated visual framework in which several discretization strategy can be experimented with and which visually assist the user in intuitively determining the appropriate number and location of interval in addition to featuring method based on minimizing classification error or entropy we introduce i an optimal algorithm that minimizes the approximation in 
we have been working on two different kdd system for scientific data one system involves comparative genomics where the database contains more than plant gene and protein sequence plus result extracted from similarity search against public sequence database the second system support a several decade s long longitudinal field study of chimpanzee behavior both system have component for the storing of raw data and for cleaning data before querying begin and for displaying data extraction both system use a relational dbms in this paper we report on a the extension we made to the dbms to support our analysis of the data and b the way that we used those extension a with user we developed a thought from an initial idea to a richer analysis we have found that a a user s initial thought develops he or she make finer distinction and look to explain anomaly seen in coarse calculation in the query to accomplish those exploration we have found it valuable to move piece of sql command into attribute value and to accomplish several smaller query all at once via a command relation thus there is a blurring of the distinction between command and data this blurring allowed u to formulate and accomplish more sophisticated analysis than we had been doing previously background a part of our collaboration with scientist who must explore and analyze large data collection we are developing an environment to support their labor intensive analysis effort our scientist user currently include researcher who are conducting a several decade s long field study of the behavior of chimpanzee and others who are conducting comparative genomics study across several plant specie in both of these project we have gathered data from various source and generated new data then cleaned and stored it using oracle relational dbms we reported on an automated method for this part of the process for the genomics project in we have developed visualization method for these data source and are working on method to closely couple them to the dbms a described in for the genomics project we are incorporating clustering technique for determining pattern of similarity between gene and protein thus our 
the paper introduces the use of the multiple cause mixture model for automatic text category assignment although much research ha been done on text categorization this algorithm is novel in that it is unsupervised i e it doe not require pre labeled training example and it can assign multiple category label to document we present very preliminary result of the application of this model to a standard test collection evaluating it in supervised mode in order to facilitate comparison with other method and showing initial result of it use in unsupervised mode 
we report on the development of the modular neural system quot seeeagle quot for the visual guidance of robot pick and place action several neural network are integrated to a single system that visuallyrecognizes human hand pointing gesture from stereo pairsof color video image the output of the hand recognition stage isprocessed by a set of color sensitive neural network to determinethe cartesian location of the target object that is referenced by thepointing gesture finally this 
an approach to defining actionability a a measure of interestingness of pattern is proposed this approach is based on the concept of an action hierarchy which is defined a a tree of action with pattern and pattern template data mining query assigned to it node a method for discovering actionable pattern is presented and various technique for optimizing the discovery process are proposed 
cortical amplification ha been proposed a a mechanism for enhancing the selectivity of neuron in the primary visual cortex le appreciated is the fact that the same form of amplification can also be used to de tune or broaden selectivity using a network model with recurrent cortical circuitry we propose that the spatial phase invariance of complex cell response arises through recurrent amplification of feedforward input neuron in the network respond like simple cell at low gain and complex cell at high gain similar recurrent mechanism may play a role in generating invariant representation of feedforward input elsewhere in the visual processing pathway 
the value of extracting knowledge from semi structured data is readily apparent with the explosion of the www and the advent of digital library this paper proposes a versatile system architecture for text mining that maintains structured data component in a relational database and unstructured concept in a concept library after a detailed explanation of our system architecture we briefly describe iris our prototype rule generation system 
this paper we present a new method for studying auditory system based on m sequence the methodallows u to perturbatively study the linear response of the system in the presence of various stimulus thisallows one to construct linear kernel at the same time that other stimulus are being presented using themethod we calculate the modulation transfer function of single neuron in the inferior colliculus of the cat 
in this paper we explore the use of machine learning and data mining to improve the prediction of travel time in an automobile we consider two formulation of this problem one that involves predicting speed at different stage along the route and another that relies on direct prediction of transit time we focus on the second formulation which we apply to data collected from the san diego freeway system we report experiment on these data with k nearest neighbout combined with a wrapper to select useful feature and normalization parameter the result suggest that nearest neighbour when using information from freeway sensor substantially outperforms prediction available from existing digital map analysis also reveal some surprise about the usefulness of other feature like the time and day of the trip 
analog electronic cochlear model need exponentially scaled filter cmos compatible lateral bipolar transistor clbts can createexponentially scaled current when biased using a resistive line with avoltage difference between both end of the line since these clbtsare independent of the cmos threshold voltage current sourcesimplemented with clbts are much better matched than currentsources created with mo transistor operated in weak inversion measurement from integrated 
recent theoretical result for pattern classification with thresholdedreal valued function such a support vector machine sigmoidnetworks and boosting give bound on misclassificationprobability that do not depend on the size of the classifier andhence can be considerably smaller than the bound that follow fromthe vc theory in this paper we show that these technique canbe more widely applied by representing other boolean functionsas two layer neural network thresholded 
consider a large collection of object each of which ha a large number of attribute of several dierent sort we assume that there are data attribute representing data attribute which are to be statistically estimated from these and attribute which can be controlled or set a motivating example is to assign a credit score to a credit card prospect indicating the likelihood that the prospect will make credit card payment and then to set a credit limit for each prospect in such a way a to maximize the over all expected revenue from the entire collection of prospect in the terminology above the credit score is called a statistical attribute and the credit limit a control attribute the methodology we describe in the paper us data mining to provide more accurate estimate of the statistical attribute and to provide more optimal setting of the control attribute we briefly describe how to parallelize these computation we also briefly comment on some of data management issue which arise for these type of problem in practice we propose using object 
the proliferation of topic hierarchy for text document ha resulted in a need for tool that automatically classify new document within such hierarchy existing classification scheme which ignore the hierarchical structure and treat the topic a separate class are often inadequate in text classification where the there is a large number of class and a huge number of relevant feature needed to distinguish between them we propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problem one at each node in the classification tree a we show each of these smaller problem can be solved accurately by focusing only on a very small set of feature those relevant to the task at hand this set of relevant feature varies widely throughout the hierarchy so that while the overall relevant feature set may be large each classifier only examines a small subset the use of reduced feature set allows u to utilize more complex probabilistic model without encountering many of the standard computational and robustness difficulty 
to measure the quality of a set of vector quantization point a mean of measuring the distance between a random point and it quantization is required common metric such a the hamming and euclidean metric while mathematically simple are inappropriate for comparing natural signal such a speech or image in this paper it is shown how an environment of function on an input space induces a canonical distortion measure cdm on x the depiction canonical is justified because it is shown that optimizing the reconstruction error of x with respect to the cdm give rise to optimal piecewise constant approximation of the function in the environment the cdm is calculated in closed form for several different function class an algorithm for training neural network to implement the cdm is presented along with some encouraging experimental result 
we present and solve a real world problem of learning to drive a bicycle we solve the problem by online reinforcement learning using the sarsa algorithm then we solve the composite problem of learning to balance a bicycle and then drive to a goal in our approach the reinforcement function is independent of the task the agent try to learn to solve 
heuristic measure for estimating the quality of attribute mostly assume the independence of attribute so in domain with strong dependency between attribute their performance is poor relief and it extension relieff are capable of correctly estimating the quality of attribute in classification problem with strong dependency between attribute by exploiting local information provided by different context they provide a global view we present the analysis of relieff which lead u to it adaptation to regression continuous class problem the experiment on artificial and real world data set show that regressional relieff correctly estimate the quality of attribute in various condition and can be used for non myopic learning of the regression tree regressional relieff and relieff provide a unified view on estimating the attribute quality in regression and classification 
this paper describes a new technique for object recognition based on learning appearance model the image is decomposed into local region which are described by a new texture representation called generali zed second moment that are derived from the output of multiscale multi orientation filter bank class characteristic local texture feature and th eir global composition is learned by a hierarchical mixture of expert architectur e jordan jacob the technique is applied to a vehicle database consisting of general car category sedan van with back door van without back door old sedan and volkswagen bug this is a difficult problem with considerable in class variation the new technique ha a misclassification rate compared to eigen image which give misclassification rate and nearest neighbor which give misclassification rate 
we have already shown that extracting long term dependency from sequentialdata is difficult both for deterministic dynamical system suchas recurrent network and probabilistic model such a hidden markovmodels hmms or input output hidden markov model iohmms inpractice to avoid this problem researcher have used domain specifica priori knowledge to give meaning to the hidden or state variable representingpast context in this paper we propose to use a more general 
abstract the task in the computer security domain of anomaly detection is to characterize the behavior of a computer user the valid or normal user so that unusual occurrence can be detected by comparison of the current input stream to the valid user s profile this task requires an online learning system that can respond to concept drift and handle discrete non metric time sequence data we present an architecture for online learning in the anomaly detection domain and address the issue of incremental updating of system parameter and instance selection we demonstrate a method for measuring direction and magnitude of concept drift in the classification space and present and evaluate approach to the above stated issue which make use of the drift measurement 
the large amount of data collected today is quickly rwrcm mrhnlminn rocolrrhara qhilitinc tn intornrot the l l lx y ucw ii i u ll ly u i y data and discover interesting pattern knowledge discovery and data mining approach hold the potential to automate the interpretation process but these approach frequently utilize computationally expensive algorithm this research outline a general approach for scaling kdd system using parallel and clistribut erl resource and applies the suggested strategy to the subdue knowledge discovery system subdue ha been used to discover interesting and repetitive concept in graph based database from a variety of dom nn h rom wx a nrrhct nt al camnrrnt nf n occ lyclllll vu r yulrs u cj o wi aw c laa ui y i yl a m ing time experiment that demonstrate scalability of parallel version of the subdue system are performed using cad circuit database and artificially generated database and potential achievement and obstacle are discussed 
in high energy physic experiment one ha to sort through a highflux of event at a rate of ten of mhz and select the few that areof interest in making this decision one relies on the location ofthe vertex where the interaction that led to the event took place here we present a solution to this problem based on two feedforwardneural network with fixed architecture whose parametersare chosen so a to obtain a high accuracy the system is testedon many data set and is shown to 
this paper is concerned with alleviating thechoice of learning bias via a two step process gamma the set of all hypothesis that are consistentwith the data and cover at least onetraining example is given an implicit characterizationof polynomial complexity theonly bias governing this induction phase isthat of the language of hypothesis gamma classification of further example is donevia interpreting this implicit theory the interpretationmechanism allows one to relaxthe 
fly are capable of rapidly detecting and integrating visual motion information in behaviorly relevant way the first stage of visual mot ion processing in fly is a retinotopic array of functional unit known a el ementary motion detector emds several decade ago reichardt and colleague developed a correlation based model of motion detection that described the behavior of these neural circuit we have implemented a variant of this model in a analog cmos vlsi process the result is a low power continuous time analog circuit with integrated photoreceptors that responds to motion in real time the response of the circuit to drifting sinusoidal grating qualitatively resemble the t emporal frequency response spatial frequency response and direction selectivity of motion sensitive neuron observed in insect in addition to it possible engineering application the circuit could potentially be used a a building block for constructing hardware model of higher level in ect motion integration 
a serious problem in mining industrial data base is that they are often incomplete and a significant amount of data is missing or erroneously entered this paper explores the use of machine learning based alternative to standard statistical data completion data imputation method for dealing with missing data we have approached the data completion problem using two well known machine learning technique the first is an unsupervised clustering strategy which us a bayesian approach to cluster the data into class the class so obtained are then used to predict multiple choice for the attribute of interest the second technique involves modeling missing variable by supervised induction of a decision tree based classifier this predicts the most likely value for the attribute of interest empirical test using extract from industrial database maintained by tioneywell customer have been done in order to compare the two technique these test show both approach are useful and have advantage and disadvantage we argue that the choice between unsupervised and supervised classification technique should be influenced by the motivation for solving the missing data problem and discus potential application for the procedure we are developing 
this paper present a direct reinforcementlearning algorithm called finite elementreinforcement learning in the continuouscase i e continuous state space and time the evaluation of the value function enablesthe generation of an optimal policy for reinforcementcontrol problem such a target orobstacle problem viability problem or optimizationproblems we propose a continuous formalism for thestudying of reinforcement learning using thecontinuous optimal control framework 
although td gammon is one of the major success in machine learning it ha not led to similar impressive breakthrough in temporal difference learning for other application or even other game we were able to replicate some of the success of td gammon developing a competitive evaluation function on a parameter feed forward neural network without using back propagation reinforcement or temporal difference learning method instead we apply simple hill climbing in a relative fitness environment these result and further analysis suggest that the surprising success of tesauro s program had more to do with the co evolutionary structure of the learning task and the dynamic of the backgammon game itself 
a recent neural model of illusory contour formation is based on a distribution of natural shape traced by particle moving with constant speed in direction given by brownian motion the input to that model consists of pair of position and direction constraint and the output consists of the distribution of contour joining all such pair in general these contour will not be closed and their distribution will not be scaleinvariant in this article we show how to compute a scale invariant distribution of closed contour given position constraint alone and use this result to explain a well known illusory contour effect 
exact interference in densely connected bayesian network is computationally intractable and so there is considerable interest in developing effective approximation scheme one approach which ha been adopted is to bound the log likelihood using a mean field approximating distribution while this lead to a tractable algorithm the mean field distribution is assumed to be factorial and hence unimodal in this paper we demonstrate the feasibility of using a richer class of approximating distribution based on mixture of mean field distribution we derive an efficient algorithm for updating the mixture parameter and apply it to the problem of learning in sigmoid belief network our result demonstrate a systematic improvement over simple mean field theory a the number of mixture component is increased 
when mining large database the data extraction problem and the interface between the database and data mining algorithm become important issue rather than giving a mining algorithm full access to a database by extracting to a flat file or other directlyaccessible data structure we propose the sql interface protocol sip which is a framework for interaction between a mining algorithm and a database the data continues to reside entirely within the database management system dbms but the query interface to the database give the data mining algorithm sufficient information to discover the same pattern it would have found with direct access to the data this model of interaction brine several advantage for ex ample it allows a mining algorithm to be parallelized automatically just by using a parallelized dbms to answer query we show how two family of mining algorithm may be implemented a sipper and we discus related work in database that should further enhance performance in the future 
we employed a white noise velocity signal to study the dynamic of the response of single neuron in the cortical area mt to visual motion response were quantied using reverse correlation optimal linear reconstruction lters and reconstruction signal to noise ratio snr the snr and lower bound estimate of information rate were lower than we expected ninety percent of the information wa transmitted below hz and the highest lower bound on bit rate wa bit s a simulated opponent motion energy subunit with poisson spike statistic wa able to out perform the mt neuron the temporal integration window measured from the reverse correlation half width ranged from m the window wa narrower when a stimulus moved faster but did not change when temporal frequency wa held constant 
this paper explores unexpected result that lie atthe intersection of two common theme in the kddcommunity large datasets and the goal of buildingcompact model experiment with many differentdatasets and several model construction algorithm including tree learning algorithm such a c withthree different pruning method and rule learning algorithmssuch a c rule and ripper show thatincreasing the amount of data used to build a model oftenresults in a linear increase in 
given a multidimensional data set and a model of it density we considerhow to define the optimal interpolation between two point thisis done by assigning a cost to each path through space based on twocompeting goal one to interpolate through region of high density theother to minimize arc length from this path functional we derive theeuler lagrange equation for extremal motion given two point the desiredinterpolation is found by solving a boundary value problem weshow 
low two different approach to hypothesis production mi and clint for instance identify the target at we study the learn ability of inductive logic t e limit whereas most others use polynomial heurisprogramming ilp concept class with retics for concept induction consequently these sysspect to robust learning we first investigate tems are generally efficient learner but to our knowlthe class of k horn clause and show that it edge none can be formally shown to find the target is not learnable in that model we prove this concept in polynomial time using a reduction on which we impose a few simultaneously theoretical work ha allowed to estabconstraints a possible from this proof we lish learnability result for some subclass of first orthen show how we can also derive negative reder horn clause early study were undertaken in the suit for some pac learnable class finally identification in the limit model gold which we end by discussing the applicational consedescribes learning a converging towards the target quences of our work and it link with other concept in finite time but given an unbounded amount learnability study regarding new learnabilof example schapiro schapiro identified a ity model for ilp most general class learnable in this model by a consistent algorithm mi and other study have since been carried out in this framework banerji 
to formulate a meaningful query on semistructured data such a on the web that match some of the source s structure we need first to discover something about how the information is represented in the source this is referred to a schema discovery and wa considered for a single object recently in the case of multiple object the task of schema discovery is to identify typical structurine information of those object a a whole we mitivate the schema discovery in this general setting and propose a framework and algorithm for it we apply the framework to a real 
abstract a new algorithm is presented which approximates the perceived visual similarity between image the image are initially transformed into a feature space which capture visual structure texture and color using a tree of filter similarity is the inverse of the distance in this perceptual feature space using this algorithm we have constructed an image database system which can perform example based retrieval on large image database using carefully constructed target set which limit variation to only a single visual characteristic retrieval rate are quantitatively compared to those of standard method accepted advanced in neural information processing 
in this paper we investigate the method of stacked generalization in combining model derived from different subset of a training dataset by a single learning algorithm a well a different algorithm the simplest way to combine prediction from competing model is majority vote and the effect of the sampling regime used to generate training subset ha already been studied in this context when bootstrap sample are used the method is called bagging and for disjoint sample we call it 
abstract 
a learner s performance doe not rely only onthe representation language and on the algorithminducing a hypothesis in this language also the way the induced hypothesis is interpretedfor the need of concept recognitionis of interest a flexible methodology for hypothesisinterpretion is offered by the philosophyof a learner s second tier a originallysuggested by michalski here thepotential of this general approach is demonstratedin the framework of numeric decisiontrees the 
a support vector machine svm is a universal learning machine whose decision surfaceis parameterized by a set of support vector and by a set of corresponding weight an svm is also characterized by a kernel function choice of the kernel determines whether the resulting svm is a polynomial classifier a two layer neural network a radialbasis function machine or some otherlearning machine svms are currently considerably slower intest phase than other approach with similar 
in this paper we first explore an intrinsicproblem that exists in the theory inducedby learning algorithm regardless of theselected algorithm search methodology andhypothesis representation by which the theoryis induced one would expect the theoryto make better prediction in some region ofthe description space than others we termthe fact that an induced theory will havesome region of relatively poor performancethe problem of locally low predictive accuracy having 
mineset tm silicon graphic interactive system for data mining integrates three powerful technology database access analytical data mining and data visualization it support the knowledge discovery process from data access and preparation through iterative analysis and visualization to deployment mineset is based on a client server architecture that scale to large database the database access component provides a rich set of operator that can be used to preprocess and transform 
of item the intuitive meaning of such a rule is that transaction of the database which contain x tend to contain y an example of an association rule is of transaction that contain beer also contain diaper of all transaction contain both of these item here is called the condence of the rule and the support of the rule the problem is to nd all association rule that satisfy user specied minimum support and minimum con dence constraint application include discovering a nities for market ba ket analysis and cross marketing catalog design loss leader analysis store layout customer segmentation based on buying pattern etc see nearhos roth man viveros for a case study of a successful application in health insurance 
closed loop control relies on sensory feedback that is usually assumedto be free but if sensing incurs a cost it may be costeffectiveto take sequence of action in open loop mode we describea reinforcement learning algorithm that learns to combineopen loop and closed loop control when sensing incurs a cost althoughwe assume reliable sensor use of open loop control meansthat action must sometimes be taken when the current state ofthe controlled system is uncertain this is 
face recognition is a class problem where is the number of known individual and support vector machine svms are a binary classification method by reformulating the face recognition problem and reinterpreting the output of the svm classifier we developed a svm based face recognition algorithm the face recognition problem is formulated a a problem in difference space which model dissimilarity between two facial image in difference space we formulate face recognition a a two class problem the class are dissimilarity between face of the same person and dissimilarity between face of different people by modifying the interpretation of the decision surface generated by svm we generated a similarity metric between face that is learned from example of difference between face the svm based algorithm is compared with a principal component analysis pca based algorithm on a difficult set of image from the feret database performance wa measured for both verification and identification scenario the identification performance for svm is versus for pca for verification the equal error rate is for svm and for pca 
we present a novel data mining approach basedon decomposition in order to analyze a givendataset the method decomposes it to a hierarchyof smaller and le complex datasets that canbe analyzed independently the method is experimentallyevaluated on a real world housingloans allocation dataset showing that the decompositioncan discover meaningful intermediateconcepts decompose a relatively complexdataset to datasets that are easy to analyze andcomprehend and derive a 
the result discussed in this paper are relevant to a large database consisting of consumer profile information together with behavioral transaction pattern the focus of this paper is on the problem of onhne mining of profile association rule in this large database the profile association rule problem is closely related to the quantitative association rule problem we show how to use multidimensional indexing structure in order to perform the mining the use of multidimensional indexing structure to perform profile mining provides considerable advantage in term of the ability to perform very generic range based onhne query 
three factor enter into analysis of performance curve such a learning curve the amount of training the learning algorithm and performance often we want to know whether the algorithm affect performance whether the effect of training on performance depends on the algorithm and whether these effect are localized in region of the curve analysis of variance is adapted to answer these question the carryover effect of learning violate the assumption of parametric analysis of variance but they are rendered harmless by a novel randomized version of the analysis after a brief outline of the statistical preliminary we present the procedure along with some example on real learning curve discus power and type i error and give some example of how our method can be applied to answer more advanced question in comparing performance curve 
in this paper we study a dual version of the ridge regression procedure it allows u to perform non linear regression by constructing a linear regression function in a high dimensional feature space the feature space representation can result in a large increase in the number of parameter used by the algorithm in order to combat this curse of dimensionality the algorithm allows the use of kernel function a used in support vector method we also discus a powerful family of kernel function which is constructed using the anova decomposition method from the kernel corresponding to spline with an infinite number of node this paper introduces a regression estimation algorithm which is a combination of these two element the dual version of ridge regression is applied to the anova enhancement of the infinitenode spline experimental result are then presented based on the boston housing data set which indicate the performance of this algorithm relative to other algorithm 
this paper report about an application of bayes inferred neuralnetwork classifier to the field of automatic sleep staging up toour current knowledge this is one of the first real world applicationsof bayesian inference we therefore want to share our experienceof this learning paradigm with a wider audience the reason forusing bayesian learning for this task is two fold first bayesianinference is known to embody regularization automatically second a side effect of bayesian 
the choice of an input representation for a neural network can havea profound impact on it accuracy in classifying novel instance however neural network are typically computationally expensiveto train making it difficult to test large number of alternativerepresentations this paper introduces fast quality measure forneural network representation allowing one to quickly and accuratelyestimate which of a collection of possible representationsfor a problem is the best we show 
recently there ha been an increased interest in lifelong machine learning method that transfer knowledge across multiple learning task such method have repeatedly been found to outperform conventional single task learning algorithm when the learning task are appropriately related to increase robustness of such approach method are desirable that can reason about the relatedness of individual learning task in order to avoid the danger arising from task that are unrelated and thus potentially misleading this paper describes the task clustering tc algorithm tc cluster learning task into class of mutually related task when facing a new learning task tc first determines the most related task cluster then exploit information selectively from this task cluster only an empirical study carried out in a mobile robot domain show that tc outperforms it non selective counterpart in situation where only a small number of task is relevant 
association rule discovery ha emerged a an important problem in knowledge discovery and data mining the association mining task consists of identifying the frequent itemsets and then forming conditional implication rule among them in this paper we present efficient algorithm for the discovery of frequent itemsets which form the compute intensive phase of the task the algorithm utilize the structural property of frequent itemsets to facilitate fast discovery the related database item are grouped together into cluster representing the potential maximal frequent itemsets in the database each cluster induces a sub lattice of the itemset lattice efficient lattice traversal technique are presented which quickly identify all the true maximal frequent itemsets and all their subset if desired we also present the effect of using different database layout scheme combined with the proposed clustering and traversal technique the proposed algorithm scan a pre processed database only once addressing the open question in association mining whether all the rule can be efficiently extracted in a single database pas we experimentally compare the new algorithm against the previous approach obtaining improvement of more than an order of magnitude for our test database 
an asynchronous pdm pulse density modulating digital neural network system ha been developed in our laboratory it consists of one thousand neuron that are physically interconnected via one million bit synapsis it can solve one thousand simultaneous nonlinear rst order dieren tial equation in a fully parallel and continuous fashion the performance of this system wa measured by a winner take all network with one thousand neuron although the magnitude of the input and network parameter were identical for each competing neuron one of them won in millisecond this processing speed amount to billion connection per second a broad range of neural network including spatiotemporal ltering feedforward and feedback network can be run by loading appropriate network parameter from a host system 
performing the complex task of knowledge discoveryin database kdd requires a break down ofthe task complexity to enable the possibilityofperformingthe kdd task since even more techniqueswill appear in the future that can solveavarietyofkdd problem a domain expert that want to analysehis domain should have the mean to work withtools that integrate several of these technique a wellas the technique themselves in this paper a frameworkis proposed for a strategy 
most technique for attribute selection indecision tree are biased towards attributeswith many value and several ad hoc solutionsto this problem have appeared in themachine learning literature statistical testsfor the existence of an association with aprespecified significance level provide a wellfoundedbasis for addressing the problem however many statistical test are computedfrom a chi squared distribution which is onlya valid approximation to the actual distributionin the 
bayesian algorithm for neural network are known toproduce classifier which are very resistent to overfitting it is often claimed that one of the main distinctivefeatures of bayesian learning algorithm is thatthey don t simply output one hypothesis but ratheran entire distribution of probability over an hypothesisset the bayes posterior an alternative perspective isthat they output a linear combination of classifier whose coefficient are given by bayes theorem oneof the 
often when learning from data one attachesa penalty term to a standard error term inan attempt to prefer simple model and preventoverfitting current penalty term forneural network however often do not takeinto account weight interaction this is acritical drawback since the effective numberof parameter in a network usually differsdramatically from the total number of possibleparameters in this paper we presenta penalty term that us principal componentanalysis to help 
several researcher have demonstrated howneural network can be trained to compensatefor nonlinear signal distortion in e g digitalsatellite communication system thesenetworks however require that both theoriginal signal and it distorted version areknown therefore they have to be trainedoff line and they cannot adapt to changingchannel characteristic in this paper a noveldual reinforcement learning approach is proposedthat can adapt on line while the systemis 
to evolve structured program we introduceh pipe a hierarchical extension ofprobabilistic incremental program evolution pipe structure is induced by quot hierarchicalinstructions quot his limited to top level structuring program part quot skip node quot sn allow for switching program part onand off they facilitate synthesis of certainstructured program in our experiment hpipeoutperforms pipe structural bias canspeed up program synthesis keywords probabilistic incremental program 
discovery of association rule from large database of item set is an important data mining problem association rule are usually stored in relational database for future use in decision support system in this paper the problem of association rule retrieval and item set retrieval is recognized a the subset search problem in relational database the subset search is not well supported by sql query language and traditional database indexing technique we introduce a new index structure called group bitmap index and compare it performance with traditional indexing method b tree and bitmap index we show experimentally that proposed index enables faster subset search and significantly outperforms traditional indexing method set item set satisfies the rule if x y is contained in the set we say that the rule is violated by a given item set item set violates the rule if the set contains x but doe not contain y each rule ha two measure of it statistical importance and strength support and confidence the support of the rule is the number of item set that satisfy the rule divided by the number of all item set the rule confidence is the number of item set that satisfy the rule divided by the number of item set that contain x transaction id item 
a a classifier a set enumeration se tree can beviewed a a generalization of decision tree it canbe shown that at the cost of a higher complexity asingle se tree encapsulates many alternative decisiontree structure an se tree enjoys several advantagesover decision tree it allows for domain based userspecifiedbias it support a flexible tradeoff betweenthe resource allocated to learning and the resultingaccuracy and it can combine knowledge induced fromexamples with other 
we address the problem of finding useful region for two dimensional association rule and decision tree in a previous paper we presented efficient algorithm for computing optimized x monotone region whose intersection with any vertical line are always undivided in practice however the quality of x monotone region is not ideal because the boundary of an xmonotone region tends to be notchy and the region is likely to overfit a training dataset too much to give a good prediction for an unseen test dataset t l r a a t c mn d u a yc pcx yt i ioll z u p p ug cur udrj igllllinear region whose intersection with any vertical line and whose intersection with any horizontal line are both undivided so that the boundary of any rectilinear region is never notchy this property is studied from a theoretical viewpoint experimental test confirm that the rectilinear region le overfits a training database and thefore provides a better prediction for unseen test data we also present a novel efficient algorithm for computing optimized rectilinear region 
for a wide variety of classification algorithm scalability to large database can be achieved by observing that most algorithm are driven by a set of sufficient statistic that are significantly smaller than the data by relying on a sql backend to compute the sufficient statistic we leverage the query processing system of sql database and avoid the need for moving data to the client we present a new sql operator unpivot that enables efficient gathering of statistic with minimal change to the sql backend our approach result in significant increase in performance without requiring any change to the physical layout of the data we show analytically how this approach outperforms an alternative that requires changing in the data layout we also compare effect of data representation and show that a dense representation may be preferred to a sparse one even when the data are fairly sparse 
local disparity information is often sparse and noisy which createstwo conflicting demand when estimating disparity in an image region the need to spatially average to get an accurate estimate andthe problem of not averaging over discontinuity we have developeda network model of disparity estimation based on disparityselectiveneurons such a those found in the early stage of processingin visual cortex the model can accurately estimate multipledisparities in a region which may 
this paper describes the automatic design of methodsfor detecting fraudulent behavior much of the designis accomplished using a series of machine learningmethods in particular we combine data mining andconstructive induction with more standard machinelearning technique to design method for detectingfraudulent usage of cellular telephone based on profilingcustomer behavior specifically we use a rulelearningprogram to uncover indicator of fraudulentbehavior from a large database 
we compare the ability of three exemplar based memory model each using three different face stimulus representation to account for the probability a human subject responded old in an old new facial memory experiment the model are the generalized context model simsample a probabilistic sampling model and dbm a novel model related to kernel density estimation that explicitly encodes stimulus distinctiveness the representation are posi tions of stimulus in md face space projection of test face onto the eigenfaces of the study set and a representation based on response to a grid of gabor filter jet of the model representation combinat ion only the distinctiveness model in md space predicts the observed morph familiarity inversion effect in which the subject false alarm rate for morphs between similar face is higher than their hit rate for many of the studied face this evidence is consistent with the hypothesis that human memory for face is a kernel density estimation task with the caveat that distinctive face require larger kernel than d o typical face 
we propose and examine a method of approximate dynamic programming for markov decision process based on structured problem representation we assume an mdp is represented using a dynamic bayesian network and construct value function using decision tree a our function representation the size of the representation is kept within acceptable limit by pruning these value tree so that leaf represent possible range of value thus approximating the value function produced during optimization we propose a method for detecting convergence prove error bound on the resulting approximately optimal value function and policy and describe some preliminary experimental result 
we study the characteristic of learning with ensemble solvingexactly the simple model of an ensemble of linear student wefind surprisingly rich behaviour for learning in large ensemble it is advantageous to use under regularized student which actuallyover fit the training data globally optimal performance canbe obtained by choosing the training set size of the student appropriately for smaller ensemble optimization of the ensembleweights can yield significant improvement 
a model of motion detection is presented the model containsthree stage the first stage is unoriented and is selective for contrastpolarities the next two stage work in parallel a phaseinsensitive stage pool across different contrast polarity througha spatiotemporal filter and thus can detect first and second ordermotion a phase sensitive stage keep contrast polarity separate each of which is filtered through a spatiotemporal filter and thusonly first order motion can be 
several clustering algorithm have been proposed for class identification in spatial database such a earth observation database the effectivity of the well known algorithm such a dbscan however is somewhat limited because they do not fully exploit the richness of the different type of data contained in a spatial database in this paper we introduce the concept of density connected set and present a significantly generalized version of dbscan the major property of this algorithm are a follows any symmetric predicate can be used to define the neighborhood of an object allowing a natural definition in the case of spatially extended object such a polygon and the cardinality function for a set of neighboring object may take into account the non spatial attribute of the object a a mean of assigning application specific weight density connected set can be used a a basis to discover trend in a spatial database we define trend in spatial database and show how to apply the generalized dbscan algorithm for the task of discovering such knowledge to demonstrate the practical impact of our approach we performed experiment on a geographical information system on bavaria which is representative for a broad class of spatial database 
we propose a model of efficient on line reinforcementlearning based on the expectedmistake bound framework introduced byhaussler littlestone and warmuth the measure of performance we use is theexpected difference between the total rewardreceived by the learning agent and that receivedby an agent behaving optimally fromthe start we call this expected differencethe cumulative mistake of the agent and werequire that it quot level off quot at a reasonably fastrate a the learning 
this paper introduces a method for regularization of hmm system that avoids parameter overfitting caused by insufficient training data regularization is done by augmenting the em training method by a penalty term that favor simple and smooth hmm system the penalty term is constructed a a mixture model of negative exponential distribution that is assumed to generate the state dependent emission probability of the hmms this new method is the successful transfer of a well known regularization approach in neural network to the hmm domain and can be interpreted a a generalization of traditional state tying for hmm system the effect of regularization is demonstrated for continuous speech recognition task by improving overfitted triphone model and by speaker adaptation with limited training data 
standard recurrent net cannot deal with long minimal time lag between relevant signal several recent nip paper propose alternative method we first show problem used to promote various previous algorithm can be solved more quickly by random weight guessing than by the proposed algorithm we then use lstm our own recent algorithm to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of 
we study the spatiotemporal correlation in natural time varyingimages and explore the hypothesis that the visual system is concernedwith the optimal coding of visual representation throughspatiotemporal decorrelation of the input signal based on themeasured spatiotemporal power spectrum the transform needed todecorrelate input signal is derived analytically and then comparedwith the actual processing observed in psychophysical experiment introductionthe visual system is 
biological control system routinely guide complex dynamical system through complicatedtasks such a running or diving conventional control technique however stumble with these problem which have complex dynamic many degree of freedom and a task which is often only partially specified e g quot move forward fast quot or quot executea one and one half somersault dive quot to address problem like these we are using abiologically inspired hierarchical control structure in which controller 
a simple learning rule is derived the vaps algorithm which canbe instantiated to generate a wide range of new reinforcementlearningalgorithms these algorithm solve a number of openproblems define several new approach to reinforcement learning and unify different approach to reinforcement learning under asingle theory these algorithm all have guaranteed convergence and include modification of several existing algorithm that wereknown to fail to converge on simple mdps these 
one approach to invariant object recognition employ a recurrent neuralnetwork a an associative memory in the standard depiction of thenetwork s state space memory of object are stored a attractive fixedpoints of the dynamic i argue for a modification of this picture if anobject ha a continuous family of instantiation it should be representedby a continuous attractor this idea is illustrated with a network thatlearns to complete pattern to perform the task of filling in 
when faced with inadequate information human often use knowledgegained from previous experienceto help them in making decision even when this knowledgeis spread thinly among manyprevious experience human areable to effectively accumulate andapply it to a current classificationtask of interest inspired byhuman knowledge reuse we havepreviously introduced a generalframework for the use of knowledgeembodied in existing classifiersto aid in a new classification 
there is a wealth of information to be mined from narrative text on the world wide web unfortunately standard natural language processing nlp extraction technique expect full grammatical sentence and perform poorly on the choppy sentence fragment that are often found on web page this paper introduces webfoot a preprocessor that par web page into logically coherent segment based on page layout cue output from webfoot is then passed on to crystal an nlp system that learns 
the new classification algorithm clef combine a version of a linear machine known a a phi machine with a non linear function approximator that construct it own feature the algorithm find non linear decision boundary by constructing feature that are needed to learn the necessary discriminant function the clef algorithm is proven to separate all consistently labelled training instance even when they are not linearly separable in the input variable the algorithm is illustrated on a variety of task 
when combining a set of learned model to form an improved estimator the issue of redundancy or multicollinearity in the set ofmodels must be addressed a progression of existing approachesand their limitation with respect to the redundancy is discussed a new approach pcr based on principal component regressionis proposed to address these limitation an evaluation of thenew approach on a collection of domain reveals that pcr wa the most robust combination method a the 
in robotics and other control application it is commonplac e to have a preexisting set of controller for solving subtasks perhaps h and crafted or previously learned or planned and still face a difficult pro blem of how to choose and switch among the controller to solve an overall task a well a possible in this paper we present a framework based on markov decision process and semi markov decision process for phrasing this problem a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controller and example application of the theorem in particular we show how an agent can plan with these high level controller and then use the result of suc h planning to find an even better plan by modifying the existing controller with negligible additional cost and no re planning in one of our example the complexity of the problem is reduced from billion state action pair to le than a million state controller pair 
it is shown that conventional computer can be exponentially faster than planar hopfield network although there are planar hopfield network that take exponential time to converge a stable state of an arbitrary planar hopfield network can be found by a conventional computer in polynomial time the theory of pls completeness give strong evidence that such a separation is unlikely for nonplanar hopfield network and it is demonstrated that this is also the case for several restricted class of nonplanar hopfield network including those who interconnection graph are the class of bipartite graph graph of degree the dual of the knight s graph the neighbor mesh the hypercube the butterfly the cube connected cycle and the shuffle exchange graph 
we consider the problem of aggregation for uncertain and imprecise data for such data we define aggregation operator and use them to provide information on property and pattern of data attribute the aggregate that we define use the kullback leibler information divergence between the aggregated probability distribution and the individual tuple data value we are thus able to provide a probability distribution for the domain value of an attribute or group of attribute using imperfect data information stored in a database is often subject to uncertainty and imprecision an extended relational data model ha previously been proposed for such data which allows u to quantify our uncertainty and imprecision about attribute value by representing them a a probability distribution our aggregation operator are defined on such a data model the provision of such operator is a central requirement in furnishing a database with the capability to perform the operation necessary for knowledge discovery in database 
abstract data reduction make datasets smaller but preserve classi cation structure of interest it is an important area of research in data mining and database in this paper we present a novel approach to data reduction based on hyper relation our method is a generalization of the data ltering method introduced by for one attribute to many attribute hyper relation are a generalization of conventional database relation in the sense that we allow set of value a tuple entry the advantage of this is that raw data and reduced data can both be represented by hyper relation the collection of hyper relation can be naturally made into a complete boolean algebra and so for any collection of hyper tuples we can nd it unique least upper bound lub a a reduction of it however the lub may not qualify a a reduced version of the given set of tuples then we turn to nd the interior cover the subset of internal element covered by the lub we establish the theoretical result that such an interior cover exists and we present a method by which we can nd it this interior serf a a reduced version of the data the proposed method wa evaluated using real world datasets with respect to it test accuracy the result were quite remarkable in that the cross validated test accuracy were substantially higher than those obtained by c in out of datasets the datasets were reduced with reduction ratio up to 
in recent paper miller goodman smyth provided condition on the cost function used for the training of a neural network in order to ensure that the output of the network approximates the conditional expectation of the desired output given the input however they only considered the single output case in this paper we provide another rather straightforward proof of the same result for the general multi output case all the development being presented in the context of estimation theory more precisely among a class of reasonable performance criterion we provide necessary and sufficient condition on the cost function so that the optimal estimate is the conditional expectation of the desired output whatever the noise characteristic affecting the data we furthermore provide a short overview of related result from estimation theory and verify numerically the development by comparing the optimal estimator of several performance criterion we must stress that while all these result are stated for a neural network they are however true in general for any learning machine that is trained in order to predict an output y in function of an input x keywords performance criterion cost function probabilistic interpretation of output unit 
we present an algorithm for identifying linear pattern on a twodimensionallattice based on the concept of an orientation selectivecell a concept borrowed from neurobiology of vision constructinga multi layered neural network with fixed architecture whichimplements orientation selectivity we define output element correspondingto different orientation which allow u to make a selectiondecision the algorithm take into account the granularityof the lattice a well a the presence of 
it ha been suggested that long range intrinsic connection in striate cortex may play a role in contour extraction gilbert et al a number of recent physiological and psychophysical study have examined the possible role of long range connection in the modulation of contrast detection threshold polat and sagi kapadia et al kov c and julesz and various pre attentive detection task kov c and julesz field et al we have developed a network architecture based on the anatomical connectivity of striate cortex a well a the temporal dynamic of neuronal processing that is able to reproduce the observed experimental result the network ha been tested on real image and ha application in term of identifying salient contour in automatic image processing system 
this paper is a comparative study of feature selection method in statistical learning of text categorization the focus is on aggressive dimensionality reduction five method were evaluated including term selection based on document frequency df information gain ig mutual information mi a test chi and term strength t we found ig and chi most effective in our experiment using ig thresholding with a k nearest neighbor classifier on the reuters corpus removal of up to removal of unique term actually yielded an improved classification accuracy measured by average precision df thresholding performed similarly indeed we found strong correlation between the df ig and chi value of a term this suggests that df thresholding the simplest method with the lowest cost in computation can be reliably used instead of ig or chi when the computation of these measure are too expensive t compare favorably with the other method with up to vocabulary reduction but is not competitive at higher vocabulary reduction level in contrast mi had relatively poor performance due to it bias towards favoring rare term and it sensitivity to probability estimation error 
there ha been much recent work on measuring image statistic and on learning probability distribution on image we observe that the mapping from image to statistic is many to one and show it can be quantified by a phase space factor this phase space approach throw light on the minimax entropy technique for learning gibbs distribution on image with potential derived from image statistic and elucidates the ambiguity that are inherent to determining the potential in addition it show that if the phase factor can be approximated by an analytic distribution then the computation time for minimax entropy learning can be vastly reduced an illustration of this concept using a gaussian to approximate the phase factor lead to a new algorithm called minutemax which give a good approximation to the result of zhu and mumford in just second of cpu time the phase space approach also give insight into the multi scale potential found by zhu and mumford and suggest that the form of the potential are influenced greatly by phase space consideration finally we prove that probability distribution learned in feature space alone are equivalent to minimax entropy learning with a multinomial approximation of the phase factor 
in this paper we propose a memory based q learning algorithmcalled predictive q routing pq routing for adaptive traffic control we attempt to address two problem encountered in q routing boyan amp littman namely the inability to fine tune routingpolicies under low network load and the inability to learn newoptimal policy under decreasing load condition unlike othermemory based reinforcement learning algorithm in which memoryis used to keep past experience to increase 
in wirth t reinartz we introduced the early indicator method a multi strategy approach for the efficient prediction of various aspect of the fault profile of a set of car in a large automotive database while successful the initial implementation wa limited in various way in this paper we report recent progress focusing on performance gain we achieved through proper process based database support we show how intelligent management of the information collected during a kdd process can both make the task of the user easier and speed up the execution the central idea is to use an object oriented schema a the central information directory to which data knowledge and process can be attached furthermore it enables the automatic exploitation of previously stored result together with the query shipping strategy this achieves efficiency and scalability in order to analyze huge database while we demonstrate the usefulness of our solution in the context of the early indicator method the approach is generally applicable and useful for any integrated comprehensive kdd system 
the neurothermostat is an adaptive controller that regulates indoorair temperature in a residence by switching a furnace on oroff the task is framed a an optimal control problem in whichboth comfort and energy cost are considered a part of the controlobjective because the consequence of control decision aredelayed in time the neurothermostat must anticipate heating demandswith predictive model of occupancy pattern and the thermalresponse of the house and furnace occupancy 
occam s razor ha been the subject of much controversy this paper argues that this is partlybecause it ha been interpreted in two quite differentways the first of which simplicity is a goalin itself is essentially correct while the second simplicity lead to greater accuracy is not thepaper review the large variety of theoretical argumentsand empirical evidence for and against the quot second razor quot and concludes that the balanceis strongly against it in particular it 
the genetic algorithm ga is a heuristic search procedure based on mechanism abstracted from population genetics in a previous paper baluja caruana we showed that much simpler algorithm such a hillclimbing and populationbased incremental learning pbil perform comparably to gas on an optimization problem custom designed to benefit from the ga s operator this paper extends these result in two direction first in a large scale empirical comparison of problem that have been reported in ga literature we show that on many problem simpler algorithm can perform significantly better than gas second we describe when crossover is useful and show how it can be incorporated into pbil implicit v explicit search statistic although there ha recently been controversy in the genetic algorithm ga community a to whether gas should be used for static function optimization a large amount of research ha been and continues to be conducted in this direction de jong since much of ga research focus on optimization most often in static environment this study examines the performance of gas in these domain in the standard ga candidate solution are encoded a fixed length binary vector the initial group of potential solution is chosen randomly at each generation the fitness of each solution is calculated this is a measure of how well the solution optimizes the objective function the subsequent generation is created through a process of selection recombination and mutation recombination operator merge the information contained within pair of selected parent by placing random subset of the information from both parent into their respective position in a member of the subsequent generation the fitness proportional selection work a selective pressure higher fitness solution string have a higher probability of being selected for recombination mutation are used to help preserve diversity in the population by introducing random change into the solution string the ga us the population to implicitly maintain statistic about the search space the selection crossover and mutation operator can be viewed a mechanism of extracting the implicit statistic from the population to choose the next set of point to sample detail of gas can be found in goldberg holland population based incremental learning pbil is a combination of genetic algorithm and competitive learning baluja the pbil algorithm attempt to explicitly maintain statistic about the search space to decide where to sample next the object of the algorithm is to create a real valued probability vector which when sampled reveals high quality solution vector with high probability for example if a good solution can be encoded a a string of alternating s and s a suitable final probability vector would be etc the pbil algorithm and parameter are shown in figure initially the value of the probability vector are initialized to sampling from this vector yield random solution vector because the probability of generating a or is equal a search progress the value in the probability vector gradually shift to represent high eval 
the solar chromosphere consists of three class plage network background which contribute differently to ultraviolet radiation reachina thn am th nl r nhv iota cama intmwatd in a y u v y yyj uy a y yivy vu iy relating plage area and intensity to uv irradiante a well a understanding the spatial and temporal evolution of plage shape we describe a data set of solar image mean of segmenting the image into constituent class and a novel high level representation for compact object based on a spatial membership function defmed via a triangulated planar graph segmentation axe found using a discrete markov random field setup and the high level representation are learned by a markov chain monte carlo birth death process on the triangulation 
one of the important problem in data mining is the evaluation of subjective interestingness of the discovered rule past research ha found that in many real life application it is easy to generate a large number of rule from the database but most of the rule are not useful or interesting to the user due to the large number of rule it is difficult for the user to analyze them manually in order to identify those interesting one whether a rule is of interest to a user depends on his her existing knowledge of the domain and his her interest in this paper we propose a technique that analyzes the discovered rule against a specific type of existing knowledge which we call general impression to help the user identify interesting rule we first propose a representation language to allow general impression to be specified we then present some algorithm to analyze the discovered classification rule against a set of general impression the result of the analysis tell u which rule conform to the general impression and which rule are unexpected unexpected rule are by definition inte resting 
some of the most successful recent applicationsof reinforcement learning have usedneural network and the td algorithm tolearn evaluation function in this paper we examine the intuition that td operatesby approximating asynchronous valueiteration we note that on the importantsubclass of acyclic task value iteration isinefficient compared with another graph algorithm dag sp which assigns value tostates by working strictly backwards from thegoal we then present rout an 
jam is a powerful and portable agent based distributed data mining system that employsmeta learning technique to integrate a number of independent classifier concept derivedin parallel from independent and possibly inherently distributed database although metalearningpromotes scalability and accuracy in a simple and straightforward manner brute forcemeta learning technique can result in large inefficient and some time inaccurate meta classifierhierarchies in this paper we 
we consider feature selection in the quot wrapper quot model of feature selection this typicallyinvolves an np hard optimization problemthat is approximated by heuristic searchfor a quot good quot feature subset first consideringthe idealization where this optimization isperformed exactly we give a rigorous boundfor generalization error under feature selection the search heuristic typically used arethen immediately seen a trying to achievethe error given in our bound and succeedingto the 
this paper center on the problem of finding commonality for a set of object belonging to an object oriented database in our approach commonality within a set of object are described by object oriented query that compute this set of object the paper discus the architecture of a knowledge discovery system called masson which employ genetic programming to find such query we also report on an experiment that evaluated the knowledge discovery capability of the masson system 
one person s noise is another person s signal for many application including the detection of credit card fraud and the monitoring of criminal activity in electronic commerce an important knowledge discovery problem is the detection of exceptional outlying event in computational statistic a depth based approach detects outlying data point in a d dataset by based on some definition of depth organizing the data point in layer with the expectation that shallow layer are more likely to contain outlying point than are the deep layer one robust notion of depth called depth contour wa introduced by tukey isodepth developed by rut and rousseeuw is an algorithm that computes d depth contour in this paper we give a fast algorithm fdc which computes the first k d depth contour by restricting the computation to a small selected subset of data point instead of examining all data point consequently fdc scale up much better than isodepth also while isodepth relies on the non existence of collinear point fdc is robust against collinear point 
this paper discus issue related to bayesian network model learning for unbalanced binary classification task in general the primary focus of current research on bayesian network learning system e g k and it variant is on the creation of the bayesian network structure that fit the database best it turn out that when applied with a specific purpose in mind such a classification the performance of these network model may be very poor we demonstrate that bayesian network model should be created to meet the specific goal or purpose intended for the model 
the following investigates the use of single neuron learning algorithmsto improve the performance of text retrieval system thataccept natural language query a retrieval process is explainedthat transforms the natural language query into the query syntaxof a real retrieval system the initial query is expanded using statisticaland learning technique and is then used for document rankingand binary classification the result of experiment suggest thatkivinen and warmuth s 
in many data mining application we are given a set of training example and asked to construct a regression machine or a classifier that ha low prediction error or low error rate on new example respectively an important issue is speed especially when there are large amount of data we show how both classification and prediction error can be reduced by using boosting technique to implement committee machine in our implementation of committee using either classification tree or regression tree we show how we can trade off speed against either error rate or prediction error 
detection of the periodicity of amplitude modulation is a major step inthe determination of the pitch of a sound in this article we willpresent a silicon model that us synchronicity of spiking neuron toextract the fundamental frequency of a sound it is based on theobservation that the so called chopper in the mammalian cochlearnucleus synchronize well for certain rate of amplitude modulation depending on the cell s intrinsic chopping frequency our siliconmodel us three 
the algorithm described in this article is based on the ob algorithmby hassibi stork and wolff and the main disadvantageof ob is it high complexity ob need to calculate theinverse hessian to delete only one weight thus needing much timeto prune a big net a better algorithm should use this matrix toremove more than only one weight because calculating the inversehessian take the most time in the ob algorithm the algorithm called unit ob described in this 
human and animal study show that mammalian brain undergoesmassive synaptic pruning during childhood removing about half ofthe synapsis until puberty we have previously shown that maintainingnetwork memory performance while synapsis are deleted requires that synapsis are properly modified and pruned removingthe weaker synapsis we now show that neuronal regulation amechanism recently observed to maintain the average neuronal inputfield result in weight dependent synaptic 
this paper describes a data mining approach for extracting enriched data from scientific data archive such a nasa s earth observing system data and information system eosdis that are stored on slow access tertiary storage this enriched data ha significantly smaller volume than the original data yet preserve sufficient property of this data such that over time many different user can repeatedly mine it for different earth science phenomenon this enriched data capture daily trend and significant deviation from trend for each bin of gridded data from an equaldegree grid covering the earth s surface a feature of this 
prior work in automated scientific discovery ha been successful in finding pattern in data given that a reasonably small set of mostly relevant feature is specified the work described in this paper place data in the context of large body of background knowledge specifically data item are connected to multiple database of background knowledge represented a inheritance network the system ha made a practical impact on botanical toxicology research which required linking example of case of plant exposure to database of botanical geographical and climate background knowledge 
this paper describes a small compact circuit the retino laminar rl circuit that capture the temporal and adaptation property both of the photoreceptor and of the laminar layer of the fly the rl circuit us only six transistor and two capacitor the circuit is operated in the subthreshold domain it ha a low dc gain and a high transient gain the adaptation time constant of the rl circuit can be controlled via an external bias it temporal filtering property change with the background intensity and with the signal to noise ratio the frequency response of the circuit show that in the frequency range of to hz the circuit response go from highpass filtering under high light level to lowpass filtering under low light level i e when the signal to noise ratio is low 
abstract published in neural information processing system nip mit press in press boosting is a general method for improving the performance of any learning algorithm that consistently generates classifier which need to perform only slightly better than random guessing a recently proposed and very promising boosting algorithm is adaboost it ha been applied with great success to several benchmark machine learning problem using rather simple learning algorithm and decision tree in this paper we use adaboost to improve the performance of neural network we compare training method based on sampling the training set and weighting the cost function our system achieves about error on a data base of online handwritten digit from more than writer adaptive boosting of a multi layer network achieved err or on the uci letter and error on the uci satellite data set 
we study the problem of learning to accurately rank a set of object by combining a given collection of ranking or preference function this problem of combining preference arises in several application such a that of combining the result of different search engine or the collaborativefiltering problem of ranking movie for a user based on the movie ranking provided by other user in this work we begin by presenting a formal framework for this general problem we then describe and analyze an efficient algorithm called rankboost for combining preference based on the boosting approach to machine learning we give theoretical result describing the algorithm s behavior both on the training data and on new test data not seen during training we also describe an efficient implementation of the algorithm for a particular restricted but common case we next discus two experiment we carried out to ass the performance of rankboost in the first experiment we used the algorithm to combine different web search strategy each of which is a query expansion for a given domain the second experiment is a collaborative filtering task for making movie recommendation 
i present a theory of mean field approximation based on information geometry this theory includes in a consistent way the naive mean field approximation a well a the tap approach and the linear response theorem in statistical physic giving clear information theoretic interpretation to them 
many data mining task e g association rule sequential pattern use complex pointer based data structure e g hash tree that typically suffer from sub optimal data locality in the multiprocessor case shared access to these data structure may also result in false sharing for these task it is commonly observed that the recursive data structure is built once and accessed multiple time during each iteration furthermore the access pattern after the build phase are highly ordered in such case locality and false sharing sensitive memory placement of these structure can enhance performance significantly we evaluate a set of placement policy for parallel association discovery and show that simple placement scheme can improve execution time by more than a factor of two more complex scheme yield additional gain our experiment show that simple placement scheme can be quite effective and for the datasets we looked at improve the execution time by a factor of two more complex scheme yield additional gain these result are directly applicable to other mining task like quantitative association srikant multi level taxonomy association srikant and sequential pattern agrawal which also use hash tree based structure most of the current work in parallel association mining ha only focused on distributed memory machine park agrawal cheung shintani han zaki where false sharing doesn t arise however our locality optimization are equally applicable to these method a detailed version of this paper appears in parthasarathy 
i present an expectation maximization em algorithm for principal component analysis pca the algorithm allows a few eigenvectors and eigenvalue to be extracted from large collection of high dimensional data it is computationally ecient in space and time and doe not require computing the sample covariance of the data it also naturally accommodates missing information i introduce a new variation of pca known a sensible principal component analysis spca which defines a proper density 
iterative refinement clustering algorithm e g k mean em converge to one of numerous local minimum it is known that they are especially sensitive to initial condition we present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the mode of a distribution the refined initial starting condition lead to convergence to better local minimum the procedure is applicable to a wide class of clustering algorithm for both discrete and continuous data we demonstrate the application of this method to the expectation maximization em clustering algorithm and show that refined initial point indeed lead to improved solution refinement run time is considerably lower than the time required to cluster the full database the method is scalable and can be coupled with a scalable clustering algorithm to address the large scale clustering in data mining 
the rocchio relevance feedback algorithm is one of the most popular and widely applied learning method from information retrieval here a probabilistic analysis of this algorithm is presented in a text categorization framework the analysis give theoretical insight into the heuristic used in the rocchio algorithm particularly the word weighting scheme and the similarity metric it also suggests improvement which lead to a probabilistic variant of the rocchio classifier the rocchio classifier it probabilistic variant and a naive bayes classifier are compared on six text categorization task the result show that the probabilistic algorithm are preferable to the heuristic rocchio classifier not only because they are more well founded but also because they achieve better performance 
we prove that the canonical distortion measure cdm is the optimal distance measure to use for nearest neighbour nn classification and show that it reduces to squared euclidean distance in feature space for function class that can be expressed a linear co mbinations of a fixed set of feature pac like bound are given on the sam plecomplexity required to learn the cdm an experiment is presented in which a neural network cdm wa learnt for a japanese ocr environment and then used to do nn classification 
an important issue in data mining is the recognition of complex dependency between attribute past technique for identifying attribute dependence include correlation coefficient scatterplots and equiwidth histogram these technique are sensitive to outlier and often are not sufficiently informative to identify the kind of attribute dependence present we propose a new approach which we call independence diagram we divide each attribute into range for each pair of attribute the combination of these range defines a two dimensional grid for each cell of this grid we store the number of data item in it we display the grid scaling each attribute axis so that the displayed width of a range is proportional to the total number of data item within that range the brightness of a cell is proportional to the density of data item in it a a result both attribute are independently normalized by frequency ensuring insensitivity to outlier and skew and allowing specific focus on attribute dependency furthermore independence diagram provide quantitative measure of the interaction between two attribute and allow formal reasoning about issue such a statistical significance 
the increasing stream of electronic informationavailable make it ever more time consuming to findinteresting information information filtering systemsthat use machine learning technique hold thepromise of relieving user of this burden by learninga model of the user s interest and using this modelto find interesting document most of these system however suffer from the problem that usersmust use the system for a long time before a usefulmodel of their interest can be learned 
here we derive measure quantifying the information loss of a synaptic signal due to the presence of neuronalnoise source a it electrotonically propagates along a weakly active dendrite we model the dendrite a an infinite linear cable with noise source distributed along it length the noise source we consider are thermal noise channel noise arising from the stochastic nature of voltage dependent ionic channel k and na andsynapticnoise duetospontaneousbackgroundactivity we ass the efficacy of information transfer using a signal detection paradigm where the objectiveis to detectthe presence absenceof a presynapticspike from thepost synapticmembranevoltage thisallows u to analyticallyassess the role of each of these noise source in information transfer for our choice of parameter we find that the synaptic noise is the dominant noise source which limit the maximum length over which information be reliably transmitted 
abstract initial experiment described here are directed toward using reinforcement learning rl to develop an automatic recovery system ar for high agility aircraft an ar is an outer loop ight control system designed to bring an aircraft from a range of initial state to straight level and non inverted ight in minimum time and while satisfying given constraint here we report on result for a simple version of of the problem involving only single axis pitch simulated recovery through simulated control experience using a medium delity aircraft simulation the rl system approximated an optimal policy for longitudinal stick input to produce minimum time transition to straight and level ight in unconstrained case a well a while meeting a pilot station acceleration constraint 
reinforcement learning is the process by which an autonomous agent us it experience interacting with an environment to improve it behavior the markov decision process mdp model is a popular way of formalizing the reinforcement learning problem but it is by no mean the only way in this paper we show how many of the important theoretical result concerning reinforcement learning in mdps extend to a generalized mdp model that includes mdps two player game and mdps under a worst case optimality criterion a special case the basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous convergence keywords reinforcement learning q learning convergence markov game 
visually guided arm reaching movement are produced by distributed neural network within parietal and frontal region of the cerebral cortex experimental data indicate that single neuron in these region are broadly tuned to parameter of movement appropriate command are elaborated by population of neuron the coordinated action of neuron can be visualized using a neuronal population vector npv however the npv provides only a rough estimate of movement parameter direction velocity and may even fail to reflect the parameter of movement when arm posture is changed we designed a model of the cortical motor command to investigate the relation between the desired direction of the movement the actual direction of movement and the direction of the npv in motor cortex the model is a two layer self organizing neural network which combine broadly tuned muscular proprioceptive and cartesian visual information to calculate angular motor command for the initial part of the movement of a two link arm the network wa trained by motor babbling in position simulation showed that the network produced appropriate movement direction over a large part of the workspace small deviation of the actual trajectory from the desired trajectory existed at the extremity of the workspace these deviation were accompanied by large deviation of the npv from both trajectory these result suggest the npv doe not give a faithful image of cortical processing during arm reaching movement 
planning and learning at multiple level of temporal abstra ction is a key problem for artificial intelligence in this paper we summar ize an approach to this problem based on the mathematical framework of markov decision process and reinforcement learning current mo del based reinforcement learning is based on one step model that cannot represent common sense higher level action such a going to lunch grasping an object or flying to denver this paper generalizes prior wor k on temporally abstract model sutton and extends it from t he prediction setting to include action control and planning we intro duce a more general form of temporally abstract model the multi time model and establish it suitability for planning and learning by virtue of it relationship to the bellman equation this paper summarizes the theoretical framework of multi time model and illustrates their potential a dvantages in a gridworld planning task a new approach to modeling at multiple time scale wa introduced by sutton based on prior work by singh dayan and sutton and pinette this approach enables model of the environment at different temporal scale to be interm ixed producing temporally abstract model however that work wa concerned only with predicting the environment this paper summarizes an extension of the approach including action and control of the environment precup sutton in particular we generalize the usual notion of a 
a one dimensional model of primate smooth pursuit mechanism hasbeen implemented in m cmos vlsi the scheme consolidatesrobinson s negative feedback model with wyatt and pola s positivefeedback scheme to produce a smooth pursuit system which zero thevelocity of a target on the retina furthermore the system us thecurrent eye motion a a predictor for future target motion analysis stability and biological correspondence of the system are discussed forimplementation at the focal 
dimension reducing feature extraction neural network technique which also preserve neighbourhood relationship in data have traditionally been the exclusive domain of kohonen self organising map recently we introduced a novel dimension reducing feature extraction process which is also topographic based upon a radial basis function architecture it ha been observed that the generalisation performance of the system is broadly insensitive to model order complexity and other smoothing 
entity identification ei is the identification and integration of all record which represent the same realworld entity and is an important task in database integration process when a common identification mechanism for similar record across heterogeneous database is not readily available ei is performed by examining the relationship between various attribute value among the record we propose the use of distance between attribute value a a measure of similarity between the record they represent recordmatching condition for ei can then be expressed a constraint on the attribute distance we show how knowledge discovery technique can be used to automatically derive these condition expressed a decision tree directly from the data using a distancebased framework 
we apply information maximization maximum likelihood blindsource separation to complex valued signal mixed with complexvalued nonstationary matrix this case arises in radio communicationswith baseband signal we incorporate known sourcesignal distribution in the adaptation thus making the algorithmsless quot blind quot this result in drastic reduction of the amount of dataneeded for successful convergence adaptation to rapidly changingsignal mixing condition such a to 
we present a bias variance decompositionof expected misclassification rate the mostcommonly used loss function in supervisedclassification learning the bias variancedecomposition for quadratic loss functionsis well known and serf a an importanttool for analyzing learning algorithm yetno decomposition wa offered for the morecommonly used zero one misclassification loss function until the recent work of kong amp dietterich and breiman their decomposition suffers 
distributed data mining system aim to discover and combine usefull information that is distributed across multiple database the jam system for example applies machine learning algorithm to compute model over distributed data set and employ meta learning technique to combine the multiple model occasionally however these model or classiers are induced from database that have moderately dieren t schema and hence are incompatible in this paper we investigate the problem of combining multiple model computed over distributed data set with dieren t schema through experiment performed on actual credit card data provided by two dieren t nancial institution we evaluate the eectiv ene of the proposed approach and demonstrate their potential utility 
we present the notion of ranking for evaluation of two class classifier ranking is based on using the ordering information contained in the output of a scoring model rather than just setting a classification threshold using this ordering information we can evaluate the model s performance with regard to complex goal function such a the cor rect identification of the k most likely and or least likely to be responder out of a group of potential customer using ranking we can also obtain increased efficiency in comparing classifier and selecting the better one even for the standard goal of achieving a minimal misclassification rate this feature of ranking is illustrated by simulation result we also discus it theoretically showing the similarity in structure between the reducible model dependent part of the linear ranking score and the standard misclassification rate score and characterizing the situation when we expect linear ranking to outperform misclassification rate a a method for model discrimination 
lateral competition within a layer of neuron sharpens and l ocalizes the response to an input stimulus here we investigate a model for the activity dependent development of ocular dominance map which allows to vary the degree of lateral competition for weak competit ion it resembles a correlation based learning model and for strong competition it becomes a self organizing map thus in the regime of weak competition the receptive field are shaped by the second order sta tistics of the input pattern whereas in the regime of strong competition the higher moment and feature of the individual pattern become important when correlated localized stimulus from two eye drive the cortical development we find that a topographic map and binocular localized receptive field emerge when the degree of competition excee d a critical value and that receptive field exhibit eye dominance beyond a second critical value for anti correlated activity between t he eye the second order statistic drive the system to develop ocular domi nance even for weak competition but no topography emerges topography is established only beyond a critical degree of competition 
a machine learning ha graduated fromtoy problem to quot real world quot application user are finding that quot real world quot problemsrequire them to perform aspect of problemsolving that are not currently addressedby much of the machine learning literature specifically user are finding that the task ofselecting a set of feature to define a problemand obtaining a set of example of the problemare often more important for a successfulmachine learning application than the 
in the developing nervous system gradient of target derived diffusible factor play an important role in guiding axon to appropriate target in this paper the shape that such a gradient might have is calculated a a function of distance from the target and the time since the start of factor production using estimate of the relevant parameter value from the experimental literature the spatiotemporal domain in which a growth cone could detect such a gradient is derived for large time a value for the maximum guidance range of about mm is obtained this value t well with experimental data for smaller time the analysis predicts that guidance over longer range may be possible this prediction remains to be tested 
in data mining similarity or distance between attribute is one of the central notion such a notion can be used to build attribute hierarchy etc similarity metric can be user defined but an important problem is defining similarity on the basis of data several method based on statistical technique exist for defining the similarity between two attribute a and b they typically consider only the value of a and b not the other attribute we describe how a similarity notion between attribute can be defined by considering the value of other attribute the basic idea is that in a relation r two attribute a and b are similar if the subrelations aa l r and ab r are similar similarity between the two relation is defined by considering the marginal frequency of a selected subset of other attribute we show that the framework produce natural notion of similarity empirical result on the reuters document dataset show for example how natural classification for country can be discovered from keyword distribution in document the similarity notion is easily computable with scalable algorithm 
dual route and connectionist single route model of reading havebeen at odds over claim a to the correct explanation of the readingprocess recent dual route model predict that subject shouldshow an increased naming latency for irregular word when the irregularityis earlier in the word e g chef is slower than glow aprediction that ha been confirmed in human experiment sincethis would appear to be an effect of the left to right reading process coltheart amp rastle claim 
this paper introduces the rl top architecturefor robot learning a hybrid systemcombining teleo reactive planning and reinforcementlearning technique the aim ofthis system is to speed up learning by decomposingcomplex task into hierarchy ofsimple behaviour which can be learnt moreeasily behaviour learnt in this way cansubsequently be re used to solve a variety ofproblems reducing the need to learn everynew task from scratch it is even possibleto learn multiple 
an adaptive on line algorithm extending the learning of learningidea is proposed and theoretically motivated relying only on gradientflow information it can be applied to learning continuousfunctions or distribution even when no explicit loss function is givenand the hessian is not available it efficiency is demonstratedfor a non stationary blind separation task of acoustic signal introductionneural network provide powerful tool to capture the structure in data by 
structured attribute have domain value set that are partially ordered set typically hierarchy such attribute allow knowledge discovery program t o incorporate background knowledge about hierarchical relationship among attribute value inductive generalization rule for structured attribute have been developed that take into consideration the type of node in the domain hierarchy anchor or non anchor and the type of decision rule to be generated characteristic discriminant or minimum complexity these generalization rule enhance the ability of knowledge discovery system inlen to exploit the semantic content of the domain knowledge in the process of generating hypothesis if the dependent attribute e g a decision attribute is structured the system generates a system of hierarchically organized rule representing relationship between the value of this attribute and independent attribute such a situation often occurs i n practice when the decision to be assigned to a situation can be at different level of abstraction e g this is a liver disease or this is a liver cancer continuous attribute e g physical measurement are quantized into a hierarchy of value range of value arranged into different level these method are illustrated by an example concerning the discovery of pattern in world economics and demographic knowledge that relates the numerical age with the higher level concept in general the structure of the domain doe not have to be fixed it may be changing with the context of the problem at hand structuring attribute can prove advantageous for a knowledge discovery system it allows fact trend and regularity to be revealed both at high and low level of abstraction and for background knowledge to be stored and generalization to be made at the appropriate level 
in the poisson neuron model the output is a rate modulated poissonprocess snyder and miller the time varying rate parameter r t is an instantaneous function g of the stimulus r t g s t in a poisson neuron then r t give the instantaneousfiring rate the instantaneous probability of firing at anyinstant t and the output is a stochastic function of the input inpart because of it great simplicity this model is widely used usuallywith the addition of a 
a said in signal processing one person s noise is another person s signal for many application such a the exploration of satellite or medical image and the monitoring of criminal activity in electronic commerce identifying exception can often lead to the discovery of truly unexpected knowledge in this paper we study an intuitive notion of outlier a key contribution of this paper is to show how the proposed notion of outlier unifies or generalizes many ex 
abstract given a set of object in the visual field how doe the the visual system learn to attend to a particular object of interest while ignoring the rest how are occlusion and background clutter so effortlessly discounted for when recognizing a familiar object in this paper we attempt to answer these question in the context of a kalman filter based model of visual recognition that ha previously been useful in explaining certain neurophysio logical phenomenon such a endstopping and related extra classical receptive field effect in the visual cortex by using result from the field of rob ust statistic we describe an extension of the kalman filter model that can handle multiple object in the visual field the resulting robust kalman filter model demonstrates how certain form of attention can be viewed a an emergent property of the interaction between top down expectation and bottom up signal the model also suggests functional interpretatio n of certain attention related effect that have been observed in visual cortical neuron experimental result are provided to help demonstrate the abil ity of the model to perform robust segmentation and recognition of object and image sequence in the presence of varying degree of occlusion and clutter 
we report here that change in the normalized electroencephalographic eeg cross spectrum can be used in conjunction withfeedforward neural network to monitor change in alertness of operatorscontinuously and in near real time previously we haveshown that eeg spectral amplitude covary with change in alertnessas indexed by change in behavioral error rate on an auditorydetection task here we report for the first time that increasesin the frequency of detection error 
combining multiple classi er is an e ective technique for improving accuracy there are many general combining algorithm such a bagging or error correcting output coding that signi cantly improve classi er like deci sion tree rule learner or neural network unfortunately many combining method do not improve the nearest neighbor classi er in this paper we present mf a combining algorithm designed to improve the accuracy of the nearest neighbor nn classi er mf combine multiple nn classi er each using only a random subset of feature the ex perimental result are encouraging on datasets from the uci repository mf sig ni cantly improved upon the nn k near est neighbor knn and nn classi er with forward and backward selection of feature mf wa also robust to corruption by irrele vant feature compared to the knn classi er finally we show that mf is able to reduce both bias and variance component of error 
reinforcement learning method for discrete and semi markov decisionproblems such a real time dynamic programming canbe generalized for controlled diffusion process the optimalcontrol problem reduces to a boundary value problem for a fullynonlinear second order elliptic differential equation of hamiltonjacobi bellman hjb type numerical analysis provides multigridmethods for this kind of equation in the case of learning control however the system of equation on the various 
the unsupervised detection of hierarchical structuresis a major topic in unsupervised learning and one ofthe key question in data analysis and representation we propose a novel algorithm for the problem of learningdecision tree for data clustering and related problem in contrast to many other method based onsuccessive tree growing and pruning we propose anobjective function for tree evaluation and we derive anon greedy technique for tree growing applying theprinciples of maximum 
we study model feed forward network a time series predictorsin the stationary limit the focus is on complex yet non chaotic behavior the main question we address is whether the asymptoticbehavior is governed by the architecture regardless the detail ofthe weight we find hierarchy among class of architectureswith respect to the attractor dimension of the long term sequencethey are capable of generating larger number of hidden unit cangenerate higher dimensional attractor 
previous resolution based approach totheory guided induction of logic programsproduce hypothesis in the form of a set of resolventsof a theory where the resolvent representallowed sequence of resolution stepsfor the initial theory there are however many characterization of allowed sequencesof resolution step that cannot be expressedby a set of resolvent one approach tothis problem is presented the system merlin which is based on an earlier techniquefor learning 
in general procedure for determining bayes optimal adaptivecontrols for markov decision process mdp s require a prohibitiveamount of computation the optimal learning problemis intractable this paper proposes an approximate approach inwhich bandit process are used to model in a certain quot local quot sense a given mdp bandit process constitute an important subclass ofmdp s and have optimal learning strategy defined in term ofgittins index that can be computed relatively 
eric b baumnec research institute independence wayprinceton nj eric research nj nec comabstract i argue that the mind should be viewed a an economy and describe an algorithm that autonomously apportionscomplex task to multiple cooperating agent insuch a way that the incentive of each agent is exactly tomaximize my reward a owner of the system a specificmodel called quot the hayek machine quot is proposedand tested on a simulated block world bw planningproblem hayek 
a central theme of computational vision research ha been the realization that reliable estimation of local scene property requires propagating measurement across the image many author have therefore suggested solving vision problem using architecture of locally connected unit updating their activity in parallel unfortunately the convergence of traditional relaxation method on such architecture ha proven to be excruciatingly slow and in general they do not guarantee that the stable 
in macaque inferotemporal cortex it neuron have been found to respond selectively to complex shape while showing broad tuning invariance with respect to stimulus transformation such a translation and scale change and a limited tuning to rotation in depth training monkey with novel paperclip like object logothetis et al could investigate whether these invariance property are due to experience with exhaustivelymany transformedinstancesofanobjectorifthereare mechanism that allow the cell to show response invariance also to previously unseen instance of that object they found object selective cell in anterior it which exhibited limited invariance to various transformation after trainingwithsingle object view while previous model accounted for the tuning of the cell for rotation in depth and for their selectivity to a specific object relative to a population of distractor object the model described here attempt to explain in a biologically plausible way the additional property of translation and size invariance using the same stimulus a in the experiment we find that model it neuron exhibit invariance property which closely parallel those of real neuron simulation show that the model is capable of unsupervised learning of view tuned neuron 
the requirement of a strict and fixed distinction between dependent variable and independent variable together with the presence of missing data typically imposes considerable problem for most standard statistical prediction procedure this paper describes a solution of these problem through the weighted effect approach in which recursive neural net are used to learn how to compensate for any main and interaction effect attributable to missing data through the use of an effect set in addition to the data of actual cxqes fxtensive simulation of the approach based on an existing psychological data base showed high predictive validity and a graceful degradation in performance with an increase in the number of unknown predictor variable moreover the method proved amenable to the use of twoparameter logistic curve to arrive at a three way low high and undecided decision scheme with a priori known error rate 
sigmoid type belief network a class of probabilistic neural network provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problem often the parameter used in these network need to be learned from example unfortunately estimating the parameter via exact probabilistic calculation i e the em algorithm is intractable even for network with fairly small number of hidden unit we propose to avoid the infeasibility of the e step by bounding likelihood instead of computing them exactly we introduce extended and complementary representation for these network and show that the estimation of the network parameter can be made fast reduced to quadratic optimization by performing the estimation in either of the alternative domain the complementary network can be used for continuous density estimation a well 
classification for very large datasets ha many practical application in data mining technique such a discretization and dataset sampling can be used to scale up decision tree classifier to large datasets unfortunately both of these technique can cause a significant loss in accuracy we present a novel decision tree classifier called cloud which sample the splitting point for numeric attribute followed by an estimation step to narrow the search space of the best split cloud reduces computation and i o complexity substantially compared to state of the art classifters while maintaining the quality of the generated tree in term of accuracy and tree size we provide experimental result with a number of real and synthetic data ets 
computational comparison is made betweentwo feature selection approach for finding aseparating plane that discriminates betweentwo point set in an n dimensional featurespace that utilizes a few of the n feature dimension a possible in the concave minimizationapproach a separating planeis generated by minimizing a weighted sum ofdistances of misclassified point to two parallelplanes that bound the set and whichdetermine the separating plane midway betweenthem 
the full version of this paper appeared aticml many problem correspond to theclassical control task of determining the appropriatecontrol action to take given some sequence of observation one standard approachto learning these control rule calledbehavior cloning involves watching a perfectoperator operate a plant and then tryingto emulate it behavior in the experimentallearning approach by contrast thelearner first guess an initial operation toaction 
a a benchmark task the spiral problem is well known in neural network unlike previous work that emphasizes learning we approachthe problem from a generic perspective that doe not involve learning we point out that the spiral problem is intrinsically connected to the inside outside problem a generic solution to both problem is proposedbased on oscillatory correlation using a time delay network our simulationresults are qualitatively consistent with human performance andwe 
time series prediction is one of the major application of neural network after a short introduction into the basic theoretical foundat ion we argue that the iterated prediction of a dynamical system may be interpreted a a model of the system dynamic by mean of rbf neural network we describe a modeling approach and extend it to be able to model instationary system a a practical test for the capability of the method we investigate the modeling of musical and speech signal and demonstrate that the model may be used for synthesis of musical and speech signal 
clustering algorithm are attractive for the task of class identification in spatial database however the application to large spatial database rise the following requirement for clustering algorithm minimal requirement of domain knowledge to determine the input parameter discovery of cluster with arbitrary shape and good efficiency on large database the well known clustering algorithm offer no solution to the combination of these requirement in this paper we present the new clustering algorithm dbscan relying on a density based notion of cluster which is designed to discover cluster of arbitrary shape dbscan requires only one input parameter and support the user in determining an appropriate value for it we performed an experimental evaluation of the effectiveness and efficiency of dbscan using synthetic data and real data of the sequoia benchmark the result of our experiment demonstrate that dbscan is significantly more effective in discovering cluster of arbitrary shape than the well known algorithm clarans and that dbscan outperforms clarans by a factor of more than in term of efficiency 
we describe the implementation of a hidden markov model state decoding system a component for a wordspotting speech recognition system the key specification for this state decoder design is microwatt power dissipation this requirement led to a continuoustime analog circuit implementation we describe the tradeo s inherent in the choice of an analog design and explain the mapping of the discrete time state decoding algorithm into the continuous domain we characterize the operation of a word state state decoder test chip 
we present a method for the analysis of nonstationary time serieswith multiple operating mode in particular it is possible todetect and to model both a switching of the dynamic and a lessabrupt time consuming drift from one mode to another this isachieved in two step first an unsupervised training method providesprediction expert for the inherent dynamical mode then the trained expert are used in a hidden markov model that allowsto model drift an application to 
we present here an approach and algorithm for mining generalized term association the problem is to find co occurrence frequency of term given a collection of document each with relevant term and a taxonomy of term we have developed an efficient count propagation algorithm cpa targeted for library application such a medline the basis of our approach is that set of term termsets can be put into a taxonomy by exploring this taxonomy cpa propagates the count of termsets to their ancestor in the taxonomy instead of separately counting individual termset we found that cpa is more efficient than other algorithm particularly for counting large termsets a benchmark on data set extracted from a medline database showed that cpa outperforms other known algorithm by up to around half the computing time at the cost of le than of additional memory to keep the taxonomy of termsets we have used discovered knowledge of term association for the purpose of improving search capability of medline 
we use the constant statistic constraint to calibrate an array ofsensors that contains gain and offset variation this algorithm hasbeen mapped to analog hardware and designed and fabricated witha um cmos technology measured result from the chip show thatthe system achieves invariance to gain and offset variation of theinput signal introductiontransistor mismatch and parameter variation cause unavoidable nonuniformitiesfrom sensor to sensor a one time calibration procedure 
in this work we develop a new criterion to performpessimistic decision tree pruning ourmethod is theoretically sound and is based ontheoretical concept such a uniform convergenceand the vapnik chervonenkis dimension we show that our criterion is very wellmotivated from the theory side and performsvery well in practice the accuracy ofthe new criterion is comparable to that of thecurrent method used in c introductionthe phenomenon of overfitting the data is well knownin 
we rst describe a hierarchical generative model that can be viewed a a non linear generalisation of factor analysis and can be implemented in a neural network the model performs per ceptual inference in a probabilistically consistent manner by using top down bottom up and lateral connection these connection can be learned using simple rule that require only locally avail able information we then show how to incorporate lateral con nections into the generative model the model extract a sparse distributed hierarchical representation of depth from simpli ed random dot stereograms and the localised disparity detector in the rst hidden layer form a topographic map when presented with image patch from natural scene the model develops topo graphically organised local feature detector 
this paper establishes common ground for researcher addressing the challenge of scaling up inductive data mining algorithm to very large database and for practitioner who want to understand the state of the art we begin with a discussion of important but often tacit issue related to scaling up we then overview existing method categorizing them into three main approach finally we use the overview to recommend how to proceed when dealing with a large problem and where future 
we consider the microscopic equation for learning problem inneural network the aligning field of an example are obtainedfrom the cavity field which are the field if that example wereabsent in the learning process in a rough energy landscape we assumethat the density of the metastable state obey an exponentialdistribution yielding macroscopic property agreeing with the firststep replica symmetry breaking solution iterating the microscopicequations provide a learning 
many knowledge discovery kdd system need to spend substantial amount of effort to search for rulesand pattern within large amount of data after some natural evolution a a consequence of updatesapplied to their database these system must update their previously discovered knowledge to reflect thecurrent state of their database the straight forward approach of re running the discovery process onthe whole updated database to re discover the rule and pattern is not 
abstract here we analyze synaptic transmission from an information theoretic perspective we derive closed form expression for the lower bound on the capacity of a simple model of a cortical synapse under two explicit coding paradigm under the signal estimation paradigm we assume the signal to be encoded in the mean firing rate of a poisson neuron the performance of an optimal linear estimator of the signal then provides a lower bound on the capacity for signal estimation under the signal detection paradigm the presence or absence of the signal ha to be detected performance of the optimal spike detector allows u to compute a lower bound on the capacity for signal detection we find that single synapsis for empirically measured parameter value transmit information poorly but significant improvement can be achieved with a small amount of redundancy 
we train recurrent network to control chemotaxis in a computermodel of the nematode c elegans the model presented is basedclosely on the body mechanic behavioral analysis neuroanatomyand neurophysiology of c elegans each imposing constraint relevantfor information processing simulated worm moving autonomouslyin simulated chemical environment display a varietyof chemotaxis strategy similar to those of biological worm introductionthe nematode c elegans provides a unique 
in poggio and edelman proposed a view based model of objectrecognition that account for several psychophysical propertiesof certain recognition task the model predicted the existence ofview tuned and view invariant unit that were later found by logothetiset al logothetis et al in it cortex of monkeystrained with view of specific paperclip object the model however doe not specify the input to the view tuned unit and theirinternal organization in this paper we 
abstract we present the cem conditional expectation maximization al gorithm a an extension of the em expectation maximization algorithm to conditional density estimation under missing data a bounding and maximization process is given to speci cally optimize conditional likelihood instead of the usual joint likelihood we ap ply the method to conditioned mixture model and use bounding technique to derive the model s update rule monotonic conver gence computational e ciency and regression result superior to em are demonstrated 
research emanating from artificial intelligencehas throughout it history contributedto technique and idea in software engineering we describe in this paper a case studyshowing the use of theory revision to the refinementof a formally specified requirementsmodel in a previous project we were contractedto create a precise model of the complexcriteria governing the separation of aircraftprofiles in atlantic airspace duringthat work it became clear that the automated 
a neural network approach to stereovision is presented based onaliasing effect of simple disparity estimator and a fast coherencedetectionscheme within a single network structure a dense disparitymap with an associated validation map and additionally the fused cyclopean view of the scene are available the networkoperations are based on simple biological plausible circuitry thealgorithm is fully parallel and non iterative introductionhumans experience the three dimensional 
support vector machine work by mappingtraining data for classification task into ahigh dimensional feature space in the featurespace they then find a maximal marginhyperplane which separate the data thishyperplane is usually found using a quadraticprogramming routine which is computationallyintensive and is non trivial to implement in this paper we propose an adaptationof the adatron algorithm for classificationwith kernel in high dimensionalspaces the algorithm is 
the handling of anomalous or outlying observation in a data set is one of the most important task in data pre processing it is important for three reason first outlying observation can have a considerable influence on the result of an analysis second although outlier are often measurement or recording error some of them can represent phenomenon of interest something significant from the viewpoint of the application domain third for many application exception identified can often lead to the discovery of unexpected knowledge 
learning difficulty can vary considerably from one algorithm to another because different approach may be biased or tuned towards a certain initial problem description reason for poor performance include noise class distribution number of attribute or example in the sample however when intrinsic accuracy is high and performance is poor the problem can be caused by feature interaction pattern are more difficult to identify because they are conditional system that attempt to learn in domain such a this can perform constructive induction to change the initial representation to one which make classification information more visible however system that attempt to reformulate example description often do so regardless of the initial representation the author present a data based detection measure that estimate concept difficulty several measure including ness variation blurring and j are compared they argue that a measure based only on the a posteriori probability of the class variable ha limited use and that disparity between concept difficulty and blurring result on some data set can be explained by employing a simple technique that average the blurring measure over subset generated by splitting on the best attribute 
the article describes research currently being carried out by the control of power system group at the queen s university of belfast into the application of data mining to the performance monitoring and optimisation of the steam generation system in thermal power plant this work is being carried out in conjunction with premier power plc which owns and operates ballylumford power station near larne in northern ireland this station consists of mw and mw gas oil fired generating unit plus mw gas oil turbine the main component of a steam generation system consist of an oil gas fired boiler a turbine and a condenser although the operation of these is conceptually simple the component are extremely complicated and due to the nature of the process involved in steam generation they are prone to degradation and failure this can lead to a reduction in the thermal efficiency of the plant increase in plant emission and the possibility of unscheduled power outage the aim of the research is twofold to develop model of the plant over the full range of operating condition and to develop and implement a system which will use the model to determine the condition of the plant accurately and which will be able to make operational suggestion to engineer operator to rectify any deviation detected the model are to be created by data mining on the large database of archived plant data which premier power ha made available to queen s university 
the author examine the total daily load data for a large region of the uk over an eight year period the objective is to examine the data and determine what factor influence the load level the approach is to assume little knowledge of the system starting with a minimal number of input and a network with few hidden neuron this way the network will formulate a relationship between the given input and the load by examining the peculiarity of those day which do not fit into the model it is possible to discover why they do not and to create extra input that convey the information required 
we outline an approach which aim to link data mining technique within an architecture to assist in understanding natural language text it is obvious that understanding language is a kind of knowledge problem and it is generally acknowledged that knowledge acquisition is costly and time consuming we suggest that rule induction and related approach can help make this particular problem more tractable paving the way for various useful and usable product it is taken a axiomatic that the information and especially textual information which is available to individual and organisation will continue to grow unfortunately the capacity of people to deal with information unaided is going to remain static therefore there is a need for tool which can summarise categorise and contextualise information 
the aim of the investigation is to develop a system which will give product designer access to data and information from a range of corporate database deemed essential to their function in particular customer complaint product material feature r d testing access to these data may point to fundamental design anomaly or inefficiency which would not have been otherwise apparent the goal of the investigation therefore are to providing a mechanism to enable information to be used from later life cycle stage by earlier one to provide this information in a format which would be understandable and useable to another product life cycle function 
pharmaceutical company are continually striving to determine the common key characteristic of compound that determine their functionality e g relief of asthma so that they may continue to provide safe and effective medicine historically this ha been carried out by visually comparing graphical representation of the structure of compound which posse the same functionality so that key substructure pharmacophores may be determined however with the advent of high throughput screening technique providing data on enormous number of compound this ha become inappropriate a human can only compare a certain number of pattern accurately in a day potential solution to this problem may appear to come from a knowledge based system approach based upon pattern matching however we suggest this solution doe lie with a knowledge based system approach but one which relies on data mining a the underpinning technology we discus our initial work which show that organic compound possessing the same functionality may be mined for common substructure using data mining technique we also discus how our prototype tool in which a selection of data mining algorithm may be chosen ha been developed in a functional programming language the functional language gofer which wa used to rapidly prototype the tool readily lends itself to the task due to it polymorphism and lazy evaluation the lazy evaluation of gofer is a particularly useful feature of the language which readily enables the common characteristic to be determined no matter how large the compound 
