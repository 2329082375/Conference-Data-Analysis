most of the research on face recognition address the match problem and it assumes a closed universe where there is no need for a reject false positive option the surveillance problem is addressed indirectly if at all through the match problem where the size of the gallery rather than that of the probe set is very large this paper address the proper surveillance problem where the size of the probe unknown image set v gallery known image set is v frontal image we developed robust face id verification classification and retrieval scheme based on hybrid classifier and showed their feasibility using the feret face database the hybrid classifier architecture consists of an ensemble of connectionist network radial basis function rbf and inductive decision tree dt experimental result prove the feasibility of our approach and yield accuracy using the probe and gallery set specified above 
this paper introduces to the finite state calculus a family of directed replace operator in contrast to the simple replace expression upper larr lower defined in karttunen the new directed version upper larr lower yield an unambiguous transducer if the lower language consists of a single string it transduces the input string from left to right making only the longest possible replacement at each point a new type of replacement expression upper larr prefix suffix yield a transducer that insert text around string that are instance of upper the symbol denotes the matching part of the input which itself remains unchanged prefix and suffix are regular expression describing the insertion expression of the type upper larr prefix suffix may be used to compose a deterministic parser for a local grammar in the sense of gross other useful application of directed replacement include tokenization and filtering of text stream 
this work considers the problem of discovering areasof convergence of line like shape in an image themotivating application is to use the convergence of theblood vessel network to automatically locate the opticnerve in an ocular fundus image a fuzzy segmentmodel is proposed based on a conjecture that line likeshapes only contribute to a perception of convergencein their near neighborhood using this model a votingtypemethod is described to compute a convergence image which can be 
based on empirical evidence from a free word order language german we propose a fundamental revision of the principle guiding the ordering of discourse entity in the forward looking center within the centering model we claim that grammatical role criterion should be replaced by indicator of the functional information structure of the utterance i e the distinction between context bound and unbound discourse element this claim is backed up by an empirical evaluation of functional centering 
this paper describes a representation for people and animal called a body plan which is adapted to segmentation and to recognition in complex environment the representation is an organized collection of grouping hint obtained from a combination of constraint on color and texture and constraint on geometric property such a the structure of individual part and the relationship between part body plan can be learned from image data using established statistical learning technique the approach is illustrated with two example of program that successfully use body plan for recognition one example involves determining whether a picture contains a scantily clad human using a body plan built by hand the other involves determining whether a picture contains a horse using a body plan learned from image data in both case the system demonstrates excellent performance on large uncontrolled test set and very large and diverse control set 
we present a new logical approach to reasoning from inconsistent information the idea is to restore modelhood of inconsistent formula by providing a third truth value tolerating inconsistency the novelty of our approach stem first from the restriction of en tailment to three valued model a similar a possi ble to two valued model and second from an implica tion connective providing a notion of restricted monotonicity after developing the semantics we present a corresponding proof system that relies on a circum scription schema furnishing the syntactic counterpart of model minimization 
chart constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical form and that measure are taken to curtail generation path containing semantically incomplete phrase 
a new region based approach to nonrigid motion tracking is described shape is defined in term of a deformable triangular mesh that capture object shape plus a color texture map that capture object appearance photometric variation are also modeled nonrigid shape registration and motion tracking are achieved by posing the problem a an energy based robust minimization procedure the approach provides robustness to occlusion wrinkle shadow and specular highlight the formulation is tailored to rake advantage of texture mapping hardware available in many workstation pc and game console this enables nonrigid tracking at speed approaching video rate 
browser l are now among the most popular software for people to surf web page and retrieve information from internet for their own interest however people usually find the information they search for are scattered in several web page and cannot specify which part of content are interesting and how these part are presented in other word today s browser are weak in personalization to enhance the capability of personalization browser must have more power in managing content of web page in ihis demo we apply natural language processing nlp technique to create a browser that can be personalized personal browser personal browser allows user to record the interesting part of web page a chunk which are then analyzed through nlp technique for future information retrieval user can also bundle these chunk coming from different web page together to suit their preference to update information in these chunk personal browser pull relevant web page filter these web page to produce the desired information according to the recorded chunk and update the information in the original page in personal browser user can organize different information chunk in the page much like composing a personal home page for example a user who is interested in australia singapore and hong kong stock quote find web page that list current price of international stock market the user can copy the text that show the three stock quote to personal browser personal browser then analyzes the html source text according to the marked text and record the result a chunk applying nlp tool to process the html source text should show that this web page is relevant to stock price and therefore the number after a national name such a australia is a price quote which change very often the url marked text and analyzed result are all recorded in a chunk for future information retrieval user can assign a priority to each information chunk in the prioritized retrieval personal browser will pull relevant web page applying information retrieval technique s to match the information chunk with the web page to produce the desired information and present the information in a single page 
monotonicity is a constraint which arises in many application domain we present a machine learning model the monotonic network for which monotonicity can be enforced exactly i e by virtueof functional form a straightforward method for implementingandtraining a monotonic network is described monotonic networksare proven to be universal approximators of continuous differentiablemonotonic function we apply monotonic network to areal world task in corporate bond rating prediction 
this paper discus the treatment of fixed word expression developed for our it frenc english translation system this treatment make a clear distinction between compound i e multiword expression of x level in which the chunk are adjacent and idiomatic phrase i e multiword expression of phrasal category where the chunk are not necessarily adjacent in our system compound are handled during the lexical analysis while idiom are treated in the syntax where they are treated a specialized lexeme once recognized an idiom can be transfered according to the specification of the bilingual dictionary we will show several case of transfer to corresponding idiom in the target language or to simple lexeme the complete system including several hundred of compound and idiom can be consulted on the internet http latl unige ch itsweb html 
the efficiency of pattern recognition is particularly crucial in two scenario whenever there are a large number of class to discriminate and whenever recognition must be performed a large number of time we propose a single technique namely pattern rejection that greatly enhances efficiency in both case a rejector is a generalization of a classifier that quickly eliminates a large fraction of the candidate class or input this allows a recognition algorithm to dedicate it effort to a much smaller number of possibility importantly a collection of rejectors may be combined to form a composite rejector which is shown to be far more effective than any of it individual component a simple algorithm is proposed for the construction of each of the component rejectors it generality is established through close relationship with the karhunen lo ve expansion and fisher s discriminant analysis composite rejectors were constructed for two representative application namely appearance matching based object recognition and local feature detection the result demonstrate substantial efficiency improvement over existing approach most notably fisher s discriminant analysis 
because observing the same action can warrant dif ferent conclusion depending on who executed the ac tions a goal recognizer that work well on one person might not work well on another two problem that arise in providing user specific recognition are how to consider the vast number of possible adaptation that might be made to the goal recognizer and how to evaluate a particular set of adaptation for the first problem we evaluate the use of hillclimbing to search the space of all combination of an input set of adaptation for the second problem we present an algorithm that estimate the accuracy and cover age of a recognizer on a set of action sequence the individual ha recently executed we use these tech niques to construct adapt a recognizer ind ependent unsupervised learning algorithm for adapting a rec ognizer to a person s idiosyncratic behavior our ex periments in two domain show that applying adapt to the boce recognizer can improve it performance by a factor of two to three 
belief revision is a ubiquitous process under lying many form of intelligent behaviour the agm paradigm is a powerful framework for modeling and implementing belief revision sys tems based on the principle of minimal change it provides a rich and rigorous foundation for computer based belief revision architecture maxi adjustment is a belief revision strategy for theory base that can be implemented using a standard theorem prover and one that ha been used successfully for several application in this paper we provide an anytime decision procedure for maxi adjustment and study it complexity furthermore we outline a set of guideline that serve a a protomethodology for building belief revision system employing a maxi adjustment the algorithm is under de velopment in the belief revision module of the cin project 
this paper describes an interactive sensor planning system that can be used to select viewpoint subject to camera visibility field of view and task constraint application area for this method include surveillance planning safety monitoring architectural site design planning and automated site modeling given a description of the sensor s characteristic the object in the d scene and the target to be viewed our algorithm compute the set of admissible view point that satisfy the constraint the system first build topologically correct solid model of the scene from a variety of data source viewing target are then selected and visibility volume and field of view cone are computed and intersected to create viewing volume where camera can be placed the user can interactively manipulate the scene and select multiple target feature to be viewed by a camera the user can also select candidate viewpoint within this volume to synthesize view and verify the correctness of the planning system we present experimental result for the planning system on an actual complex city model 
the nearest neighbor algorithm and it derivative are often quite successful at learning a concept from a training set and providing good generalization on subsequent input vector however these technique often retain the entire training set in memory resulting in large memory requirement and slow execution speed a well a a sensitivity to noise this paper provides a discussion of issue related to reducing the number of instance retained in memory while maintaining and sometimes improving generalization accuracy and mention algorithm other researcher have used to address this problem it present three intuitive noise tolerant algorithm that can be used to prune instance from the training set in experiment on application the algorithm that achieves the highest reduction in storage also result in the highest generalization accuracy of the three method 
prioritized sweeping is a model based reinforcement learning method that attempt to focus the agent s limited computational resource to achieve a good estimate of the v alue of environment state the classic account of prioritized sweeping us an explicit state b ased representation of the value reward and model parameter such a representation is unwieldy for dealing with complex environment and there is growing interest in learning with more compact representation we claim that classic prioritized sweeping is ill suited for learning wit h such representation to overcome this deficiency we introducegeneralized prioritized sweeping a principled method for generating representation specific algorithm for model based reinforcement learning we then apply this method for several representation including state based mod el and generalized model approximators such a bayesian network we describe preliminary experiment that compare our approach with classical prioritized sweeping 
a critical path in the development of natural language understanding nlu module lie in the difficulty of defining a mapping from word to semantics usually it take in the order of year of highly skilled labor to develop a semantic mapping e g in the form of a semantic grammar that is comprehensive enough for a given domain yet due to the very nature of human language such mapping invariably fail to achieve full coverage on unseen data acknowledging the impossibility of stating a priori all the surface form by which a concept can be expressed we present gsg an empathic computer system for the rapid deployment of nlu front end and their dynamic customization by non expert end user given a new domain for which an nlu front end is to be developed two stage are involved in the end user paraphrase a baseline version of gsg ha been implemented and preliminary experiment show promising result 
the vast majority of corner and edge detector measure image intensity gradient in order to estimate the position and strength of feature however many of the most popular intensity gradient estimator are inherently and significantly anisotropic in spite of this few algorithm take the anisotropy into account and so the set of feature uncovered is typically sensitive to rotation of the image compromising recognition matching e g stereo and tracking we introduce an effective technique for removing unwanted anisotropy from analytical gradient estimate by measuring local intensity gradient in four direction rather than the more traditional two in experiment using real image data our algorithm reduces the gradient anisotropy associated with conventional analytical gradient estimate by up to yeilding more consistent feature topology 
many classical image processing task can be realized a evaluation of a boolean function over subset of an image for instance the simplicity test used in d thinning requires examining the neighbor of each voxel and computing a single boolean function of these input in this article we show how binary decision diagram can be used to produce automatically very efficient and compact code for such function the total number of operation performed by a generated function is at most one test and one branching for each input value e g in the case of d thinning test and branching at each stage the function is guaranteed to examine only the pertinent input data i e the value which affect the result a an example we consider the d simplicity test in digital topology and thinning process we produce function much faster than our previously optimized implementation and than any other implementation we know of in the case of d simplicity test on average at each voxel only neighboring voxel value are examined 
this paper present a new technique for modelling object class such a face and matching the model to novel image from the object class the technique can be used for a variety of image analysis application including face recognition object verification and facial expression analysis the model called a hierarchical morphable model is learned from example image partioned into component and their correspondence this is an extension to the work on morphable model described in previous paper hierarchical morphable model are shown to find good match to novel face image and are also robust to partial occlusion 
conventional video camera have limited field of view that make them restrictive in a variety of vision application there are several way to enhance the field of view of an imaging system however the entire imaging system must have a single effective viewpoint to enable the generation of pure perspective image from a sensed image a new camera with a hemispherical field of view is presented two such camera can be placed back to back without violating the single viewpoint constraint to arrive at a truly omnidirectional sensor result are presented on the software generation of pure perspective image from an omnidirectional image given any user selected viewing direction and magnification the paper concludes with a discussion on the spatial resolution of the proposed camera 
this paper focus on the estimation of the intrinsiccamera parameter and the trajectory of the camerafrom an image sequence intrinsic camera calibrationand pose estimation are the prerequisite formany application involving navigation task scenereconstruction and merging of virtual and real environment proposed and evaluated is a technical solutionto decrease the sensitivity of self calibration byplacing easily identifiable target of known shape in theenvironment the relative 
existing method for grouping edge on the basis of localsmoothness measure fail to compute complete contour in natural image it appears that a stronger global constraint is required motivatedby growing evidence that the human visual system exploit contour closurefor the purpose of perceptual grouping we presentan algorithm for computing highly closed bounding contour from image unlike previous algorithm no restriction are placedon the 
optimal brain damage obd is a method for reducing the number of weight in a neural network obd estimate the increase in cost function if weight are pruned and is a valid approximation if the learning algorithm ha converged into a local minimum on the other hand it is often desirable to terminate the learning process before a local minimum is reached early stopping in this paper we show that obd estimate the increase in cost function incorrectly if the network is not in a local minimum we also show how obd can be extended such that it can be used in connection with early stopping we call this new approach early brain damage ebd ebd also allows to revive already pruned weight we demonstrate the improvement achieved by ebd using three publicly available data set 
a new method to calculate the full training process of a neural networkis introduced no sophisticated method like the replica trickare used the result are directly related to the actual number oftraining step some result are presented here like the maximallearning rate an exact description of early stopping and the necessarynumber of training step further problem can be addressedwith this approach introductiontraining guided by empirical risk minimization doe not 
we introduce our research approach to in vestigating real world intelligence by building remote brained robot the key idea is that of interfacing ai system with real world be haviors through wireless technology in this approach the robot system is designed to have the brain and body separate both conceptu ally and physically it allows u to tie ai di rectly to the world enabling the verification of high level ai technique which could previously only be used in simulation for robotics re search this approach open the way to the use of large scale powerful parallel computer for ai this approach allows experiment with real istic agent an essential step to the application of ai in the real world in this presentation we introduce the remote brained approach de scribe some remote brained robot and discus experiment 
in classical planning we are faced with the fol lowing formal task given a set a of permissible action a description a of initial state and a description of final state determine a plan ii i e a finite sequence of action from a such that execution of ii begun in any state satis fying is guaranteed to terminate in a state satisfying in this paper we extend the classical model of planning by admitting plan that are not a sured to succeed we address two basic prob lem connected with such plan how to determine whether a given plan is valid i e al way succeeds admissible i e may succeed or fail or inadmissible i e never succeeds given an admissible plan determine a minimal set of observation that are to be made in the initial state or in some intermediate state if the plan is in progress to validate or falsify the plan 
remote reality is an approach to providing an immersive environment via omni directional imaging the system can use a live video feed from a remote location or can use recorded data and be remote in both space and time while le interactive than traditional vr remote reality ha an important advantage there is little to no need for model building in addition the object the texture and the motion are not just realistic they are remote view of reality 
when a symmetric object in d is projected to an image the symmetry property are lost except for specific relative viewpoint for sufficiently complex object however it is still possible to infer symmetry in d from a single image of an uncalibrated camera in this paper we give a general computational procedure for computing d structure and finding image symmetry constraint from single view projection of symmetric d object for uncalibrated camera these constraint take the form of polynomial in bracket determinant and by using projectively invariant shape constraint relating d and image structure the symmetry constraint can be derived easily by considering the effect of the corresponding symmetry transformation group in d the constraint can also be given a useful geometric interpretation in term of projective incidence using the grassmann cayley algebra formalism we demonstrate that in specific situation geometric intuition and the use of gc algebra is a very simple and effective tool for finding image symmetry constraint 
in this paper the technique of stacking previously only used for supervisedlearning is applied to unsupervised learning specifically it is usedfor non parametric multivariate density estimation to combine finite mixturemodel and kernel density estimator experimental result on bothsimulated data and real world data set clearly demonstrate that stackeddensity estimation outperforms other strategy such a choosing the singlebest model based on cross validation combining with 
search is not inherent in the correspondence problem we propose a representation of image called intrinsic curve that combine the idea of associative storage of image with connectedness of the representation intrinsic curve are the path that a set of local image descriptor trace a an image scanline is traversed from left to right curve become surface when full image are considered instead of scanlines because only the path in the space of descriptor is used for matching intrinsic curve lose track of space and are invariant with respect to disparity under ideal circumstance establishing stereo correspondence then becomes a trivial lookup problem we also show how to use intrinsic curve to match real image in the presence of noise brightness bias contrast fluctuation and moderate geometric distortion and we show how intrinsic curve can be used to deal with image ambiguity and occlusion we carry out experiment on single scanline matching to prove the feasibility of the approach and illustrate it main feature 
in this paper we propose the use of mirror and a single camera for computational stereo when compared to conventional stereo system that use two camera our method ha a number of significant advantage such a wide field of view single viewpoint projection identical camera parameter and ease of calibration we propose four stereo system that use a single camera pointed towards planar ellipsoidal hyperboloidal and paraboloidal mirror in each case we present a derivation of the epipolar constraint next we attempt to understand what can be seen by each system and formalize the notion of field of view we conclude with two experiment to obtain d structure in the first we use a pair of planar mirror and in the second a pair of paraboloidal mirror the result of our experiment demonstrate the viability of stereo using mirror 
string transformation system have been introduced in brill and have several application in natural language processing in this work we consider the computational problem of automatically learning from a given corpus the set of transformation presenting the best evidence we introduce an original data structure and efficient algorithm that learn some family of transformation that are relevant for part of speech tagging and phonological rule system we also show that the same learning problem becomes np hard in case of an unbounded use of don t care symbol in a transformation 
tractable cover are introduced a a new approach to equivalence preserving compilation of propositional knowledge base first a general framework is presented then two specific case are considered in the first one partial interpretation are used to shape the knowledge base into tractable formula from several possible class in the second case they are used to derive renamable horn formula this last case is proved le space consuming than prime implicants cover compilation for every knowledge base finally experimental result show that the new approach can prove efficient w r t direct query answering and offer significant time and space saving w r t prime implicants cover 
the boosting algorithm adaboost developedby freund and schapire ha exhibitedoutstanding performance on severalbenchmark problem when using c a the quot weak quot algorithm to be quot boosted quot like other ensemble learning approach adaboost construct a composite hypothesisby voting many individual hypothesis in practice the large amount ofmemory required to store these hypothesescan make ensemble method hard to deployin application this paper show that byselecting a 
we propose an algorithm to automatically construct feature detector for arbitrary parametric feature to obtain a high level of robustness we advocate the use of realistic multi parameter feature model and incorporate optical and sensing effect each feature is represented a a densely sampled parametric manifold in a low dimensional subspace of a hilbert space during detection the brightness distribution around each image pixel is projected into the subspace if the projection lie sufficiently close to the feature manifold the feature is detected and the location of the closest manifold point yield the feature parameter the concept of parameter reduction by normalization dimension reduction pattern rejection and heuristic search are all employed to achieve the required efficiency by applying the algorithm to appropriate parametric feature model detector have been constructed for five feature namely step edge roof edge line corner and circular disc detailed experiment are reported on the robustness of detection and the accuracy of parameter estimation 
dnf system divide the input space into decision surface defined by conjunction of condition based on attribute value pair this mean that the decision surface are orthogonal t o the axis of the tested attribute and p arallel t o all t he other ax in this paper we present t he system ltree that i s able to define decision surface both orthogonal and oblique to the ax defined by the attribute of the input space this is done by combining a decision tree with a linear discriminant by mean of constructive induction at each decision node ltree defines a new instance space by the insertion of new attribute those are projection of the instance that fall at this node over the hyper plane given by a linear discriminant function this new instance space propagates downwards through the tree test based on those new attribute are oblique in the input space we have c arried out experiment on sixteen benchmark d atasets and compared our system with other well known decision tree system oblique and orthogonal like c oc and lmdt from t hese experiment we claim that our system ha advantage concerning accuracy and tree size at statistically significant confidence level 
this paper is concerned with acquiring panoramic focused image using a small field of view video camera when scene point are distributed over a range of distance from the sensor obtaining a focused composite image involves focus computation and mechanically changing some sensor parameter translation of sensor plane panning of camera etc which can be time intensive in this paper we present method to optimize the image acquisition strategy in order to reduce redundancy we show that panning a camera about a point f focal length in front of the camera eliminates redundancy the non frontal imaging camera nicam with tilted sensor plane ha been previously introduced a a sensor that can acquire focused panoramic image in this paper we also describe strategy for optimal selection of panning angle increment and sensor plane tilt for nicam experimental result are presented for panoramic image acquisition using a regular camera a well a using nicam 
ranking based on passage address some of the shortcoming ofwhole document ranking it provides convenient unit of text toreturn to the user avoids the difficulty of comparing documentsof different length and enables identification of short block ofrelevant material amongst otherwise irrelevant text in this paperwe explore the potential of passage retrieval based on anexperimental evaluation of the ability of passage to identifyrelevant document we compare our scheme of arbitrary passageretrieval to several other document retrieval and passage retrievalmethods we show experimentally that compared to these method ranking via fixed length passage is robust and effective ourexperiments also show that compared to whole document ranking ranking via fixed length arbitrary passage significantly improvesretrieval effectiveness by for trec disk and and by for the federal register collection 
an approach to normalization is presented for both the affine and the projective case the approach is based on group factorization a well a on optimizing parameter invariant integral in order to overcome the difficult problem of parameterization related work ha been carried out by and by for affine transformation and by for projective transformation to avoid some drawback inherent to projective transformation it is suitable to integrate point information or explore thick curve 
if globally high dimensional data ha locally only low dimensional distribution it is advantageous to perform a local dimensionality reduction before further processing the data in this paper we examine several technique for local dimensionality reduction in the context of locally weighted linear r e 
many classification algorithm are quot passive quot in that they assign a class label to each instancebased only on the description given even if that description is incomplete in contrast an active classifier can at some cost obtain the value of missing attribute before deciding upon a class label the expectedutility of using an active classifier dependson both the cost required to obtain theadditional attribute value and the penaltyincurred if it output the wrong 
the hierarchical representation of data ha various application in domain such a data mining machine vision or information retrieval i n this paper we introduce an extension of the expectation maximization em algorithm that learns mixture hierarchy in a computationally efficient manner efficiency is achieved by progressing in a bottom up fashion i e by clustering the mixture component of a given level in the hierarchy to obtain those of the level above this clustering requires on ly knowledge of the mixture parameter there being no need to resort to intermediate sample in addition to practical application the algorith m allows a new interpretation of em that make clear the relationship with non parametric kernel based estimation method provides explicit control over the trade off between the bias and variance of em estimate and offer new insight about the behavior of deterministic annealing met hod commonly used with em to escape local minimum of the likelihood 
we present an empirical analysis of symbolic prototype learnersfor synthetic and real domain the prototype are learned by modifyingthe minimum distance classifier to solve problem with symbolicattributes attribute weighting and it inability to learn multiple prototypesfor a class these extension are implemented in snmc in thesecond half of this paper we provide empirical analysis characterizingsituations where symbolic prototype have advantage over traditionalmethods such 
by now it is widely accepted that learning a task from scratch i e without any prior knowledge is a daunting undertaking human however rarely attempt to learn from scratch they extract initial bias a well a strategy how to approach a learning problem from instruction and or demonstration of other human for learning control this paper investigates how learning from demonstration can be applied in the context of reinforcement learning we consider priming the q function the value function the policy and the model of the task dynamic a possible area where demonstration can speed up learning in general nonlinear learning problem only model based reinforcement learning show significant speed up after a demonstration while in the special case of linear quadratic regulator lqr problem all method profit from the demonstration in an implementation of pole balancing on a complex anthropomorphic robot arm we demonstrate that when facing the complexity of real signal processing modelbased reinforcement learning offer the most robustness for lqr problem using the suggested method the robot learns pole balancing in just a single trial after a second long demonstration of the human instructor 
this paper present a new operater for corner detection this operator us a variant of the morphological closing operator which we have called asymmetrical closing it consists of the successive application of different morphological transformation using different structuring element each of these structuring element used to probe the image under study is tuned to affect corner of different orientation and brightness we found that this kind of approach based on brightness comparison lead to better quality result than others and is achieved at a lower computational cost 
a command entity ce is a goal oriented intelligent agent specifically designed to com mand and control other agent the ce niche most often a military battlefield is a complex dynamic adversarial environment in which a ce must balance the need for thorough plan ning with the need for quick reaction to chang ing condition we describe the requirement of this environment and how it constrains the de sign of ce example from several ce design are used the paper concentrate on the area of planning knowledge and teamwork 
we describe a flexible model for representing image of object of a certain class known a priori such a face and introduce a new algorithm for matching it to a novel image and thereby performing image analysis we call this model a multidimensional morphable model or just a morphable model the morphable model is learned from example image calledprototypes of object of a class in this paper we introduce an effective stochastic gradient descent algorithm that automatically match a model to a novel image by finding the parameter that minimize the error between the image generated by the model and the novelimage two example demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying morphable model our approach can provide novel solution to several vision task including the computation of image correspondence object verification image synthesis and image compression 
cluster analysis is a fundamental principle in exploratory data analysis providing the user with a description of the group structure of given data a key problem in this context is the interpretation and visualization of clustering solution in high dimensional or abstract data space in particular probabilistic description of the group structure essential to capture inter cluster relationship are hardly assessable by simple inspection of the probabilistic assignment variable we 
we present a new function that operates on fundamental matrix across a sequence of view the operation we call threading connects two consecutive fundamental matrix using the trifocal tensor a the connecting thread the threading operation guarantee that consecutive camera matrix are consistent with a unique d model without ever recovering a d model application include recovery of camera ego motion from a sequence of view image stabilization plane stabilization across a sequence and multiview imagebased rendering 
matching image based on a hausdorff measure ha become popular for computer vision application however no probabilistic model ha been used in these application this limit the formal treatment of several issue such a feature uncertainty and prior knowledge in this paper we develop a probabilistic formulation of image matching in term of maximum likelihood estimation that generalizes a version of hausdorff matching this formulation yield several benefit with respect to previous hausdorff matching formulation in addition we show that the optimal model position in a discretized pose space can be located efficiently in this formation and we apply these technique to a mobile robot self localization problem 
in face recognition literature holistic template matching system and geometrical local feature based system have been pursued in the holistic approach pca principal component analysis and lda linear discriminant analysis are popular one more recently the combination of pca and lda ha been proposed a a superior alternative over pure pca and lda in this paper we illustrate the rationale behind these method and the pro and con of applying them to pattern classification task a theoretical performance analysis of lda suggests applying lda over the principal component from the original signal space or the subspace the improved performance of this combined approach is demonstrated through experiment conducted on both simulated data and real data 
recent work on the syntax semantics interface see e g dalrymple et al us a fragment of linear logic a a glue language for assembling meaning compositionally this paper present a glue language account of how negative polarity item e g ever any get licensed within the scope of negative or downward entailing context ladusaw e g nobody ever left this treatment of licensing operates precisely at the syntax semantics interface since it is carried out entirely within the interface glue language linear logic in addition to the account of negative polarity licensing we show in detail how linear logic proof net girard gallier can be used for efficient meaning deduction within this glue language framework 
the silhouette of a smooth d object observed by a moving camera change over time past work ha shown how surface geometry can be recovered using the deformation of the silhouette when the camera motion is known this paper address the problem of estimating both the full euclidean surface structure and the camera motion from a dense set of silhouette captured under orthographic or scaled orthographic projection the approach relies on a viewpoint invariant representation of curve swept by viewpoint dependent feature such a bitangents inflection and contour point with parallel tangent feature point which form stereo frontier point between non consecutive image are matched using this representation the camera s angular velocity is computed from constraint derived from this correspondence along with the image velocity of these feature from the angular velocity the epipolar geometry is ascertained and infinitesimal motion frontier point can be detected in turn the motion of these frontier point constrains the translation component of camera motion finally the surface is reconstructed using established technique once the camera motion ha been estimated 
traditionally corner are found along step edge in this paper we present an alternative approach corner along ridge trough and local minimum point these feature seem to be more reliable for tracking a new approach for sub pixel localization of these corner is suggested using a local approximation of the image surface 
introductionmany science and engineering application require theuser to find solution to system of nonlinear constraintsover real number or to optimize a nonlinear functionsubject to nonlinear constraint this includes applicationssuch the modeling of chemical engineering processesand of electrical circuit robot kinematics chemicalequilibrium problem and design problem e g nuclearreactor design the field of global optimization isthe study of method to find all 
in this paper we present a method for discovering approximately common motif also known a ective motif in three dimensional d molecule each node in a molecule is represented by a d point in the euclidean space and each edge is represented by an undirected line segment connecting two node in the molecule motif are rigid substructure which may occur in a molecule after allowing for an arbitrary number of rotation and translation a well a a small number specified by the user of node insert delete operation in the motif or the molecule we call this lapproximate occurrence the proposed method combine the geometric hashing technique and block detection algorithm for undirected graph to demonstrate the utility of our algorithm we discus their application to classifying three family of molecule pertaining to antibacterial sulfa drug anti anxiety agent benzodiazepine and sntiadrenergic agent receptor experimental result indicate the good performance of our algorithm and the high quality of the discovered motif 
we have designed and implemented a real time binocular tracking system which us two independent cue commonly found in the primary function of biological visual system to robustly track moving target in complex environment without a priori knowledge of the target shape or texture a fast optical flow segmentation algorithm quickly locates independently moving object for target acquisition and provides a reliable velocity estimate for smooth tracking in parallel target position is generated from the output of a zero disparity filter where a phase based disparity estimation technique allows dynamic control of the camera vergence to adapt the horopter geometry to the target location the system take advantage of the optical property of our custom designed foveated wide angle lens which exhibit a wide field of view along with a high resolution fovea method to cope with the distortion introduced by the space variant resolution and a robust real time implementation on a high performance active vision head are presented 
the basic limitation of the current appearance based matching method using eigenimages are non robust estimation of coefficient and inability to cope with problem related to occlusion and segmentation in this paper we present a new approach which successfully solves these problem the major novelty of our approach lie in the way how the coefficient of the eigenimages are determined instead of computing the coefficient by a projection of the data onto the eigenimages we extract them by a hypothesize and test paradigm using subset of image point competing hypothesis are then subject to a selection procedure based on the minimum description length principle the approach enables u not only to reject outlier and to deal with occlusion but also to simultaneously use multiple class of eigenimages 
abstract one of the main problem to obtain a euclidean d reconstructionfrom multiple view is the calibration of the camera explicitcalibration is not always practical and ha to be repeated regularly sometimesit is even impossible i e for picture taken by an unknown cameraof an unknown scene the second possibility is to do auto calibration 
in integrated service communication network an important problem is to exercise call admission control and routing so a to optim ally use the network resource this problem is naturally formulated a a dynamic programming problem which however is too complex to be solved exactly we use method of reinforcement learning rl together with a decomposition approach to find call admission control and r outing policy the performance of our policy for a network with approximately different feature configuration is compared with a commonl y used heuristic policy 
in this paper we study the statistical theory of shape for ordered finite point congurations or otherwise stated the uncertainty of geometric invariant such study have been made for affine invariant in e g where in the former case a bound on error are used instead of error described by density function and in the latter case a first order approximation give an ellipsis a uncertainty region here a general approach for defining shape and finding it density expressed 
most previous corpus based algorithm disambiguate a word with a classifier trained from previous usage of the same word separate classifier have to be trained for different word we present an algorithm that us the same knowledge source to disambiguate different word the algorithm doe not require a sense tagged corpus and exploit the fact that two different word are likely to have similar meaning if they occur in identical local context 
we present virtualized reality a technique to create virtual world out of dynamic event using densely distributed stereo view the intensity image and depth map for each camera view at each time instant are combined to form a visible surface model immersive interaction with the virtualized event is possible using a dense collection of such model additionally acomplete surface model of each instant can be built by merging the depth map from different camera into a common volumetric space the corresponding model is compatible with traditional virtual model and can be interacted with immersively using standard tool because both vsms and csms are fully three dimensional virtualized model can also be combined and modi ed to build larger more complex environment an important capability for many non trivial application we present result from d dome our facility to create virtualized model 
this paper describes an evolvable hardware ehw system for generalized neural network learning we have developed an asic vlsi chip which is a building block to configure a scalable neural network hardware system in our system both the topology and the hidden layer node function of a neural network mapped on the chip are dynamically changed using a genetic algorithm thus the most desirable network topology and choice of node function e g gaussian or sigmoid for a given application can be determined adaptively this approach is particularly suited to application requiring ability to cope with time varying problem and real time timing constraint the chip consists of digital signal processor dsps whose function and interconnection are reconfigured dynamically according to the chromosome of the genetic algorithm incorporation of local learning hardware increase the learning speed significantly simulation result on adaptive equalization in digital mobile communication are also given our system is two order of magnitude faster than a sun s on the corresponding problem 
a novel feature detector the constrained phase congruency transform cpct is introduced it simultaneously detects interest point a well a their scale in various orientation the cpct is especially important in registration application the local transformation between interest point can be determined based on their orientational scale the cpct detects the feature in mach band and in sinusoidal wave this cannot be done simply by looking for local maximum in intensity gradient nor by looking for local energy maximum i conjecture that constraining the general phase congruency is sufficient for feature detection the correct detection of feature location and of their scale is demonstrated the robustness of the cpct is achieved by constraining the local phase only four easy to detect phase are used in the computation they correspond to symmetry and anti symmetry in their neighborhood the scale at any location and orientation is determined by the scale of the channel that conforms to the constraint and maximizes energy 
for language that have no explicit word boundary such a thai chinese and japanese correcting word in text is harder than in english because of additional ambiguity in locating error word the traditional method handle this by hypothesizing that every substring in the input sentence could be error word and trying to correct all of them in this paper we propose the idea of reducing the scope of spelling correction by focusing only on dubious area in the input sentence boundary of these dubious area could be obtained approximately by applying word segmentation algorithm and finding word sequence with low probability to generate the candidate correction word we used a modified edit distance which reflects the characteristic of thai ocr error finally a part of speech trigram model and winnow algorithm are combined to determine the most probable correction 
we propose a new machine learning paradigm called graph transformer network that extends the applicability of gradient based learning algorithm to system composed of module that take graph a input and produce graph a output training is performed by computing gradient of a global objective function with respect to all the parameter in the system using a kind of back propagation procedure a complete check reading system based on these concept is described the system us convolutional neural network character recognizers combined with global training technique to provide record accuracy on business and personal check it is presently deployed commercially and read million of check per month 
this paper investigates the problem of computing the fundamental matrix for a class of active stereo vision system namely with common elevation platform the fundamental matrix is derived for such a system and a number of method are proposed to simplify it computation experimental result validate the feasibility of the different method these method are then used in a real application to validate the correctness of the fundamental matrix form for an active stereo system we demonstrate that typical variation in camera intrinsic parameter do not much affect the epipolar geometry in the image this motivates u to calibrate the camera intrinsic parameter approximately and then to use the calibration result to compute the epipolar geometry directly in real time 
we derive and discus a set of parametric equation which when given a convex d feature domain k will generate affine invariant with the property that the invariant value are uniformly distributed in the region once the shape of the feature domain k is determined and fixed it is straightforward to compute the value of the parameter and thus the proposed scheme can be tuned to a specific feature domain the feature of all recognizable object model are assumed to be three dimensional point and uniformly distributed over k the scheme lead to improved discrimination power improved computational load and storage load balancing and can also be used to determine and identify bias in the database of recognizable model over represented construct of object point obvious enhancement produce rigid transformation and similarity transformation invariant with the same good distribution property making this approach generally applicable 
current method to avoid overfitting are eitherdata oriented using separate data forvalidation or representation oriented penalizingcomplexity in the model this paperproposes process oriented evaluation wherea model s expected generalization error iscomputed a a function of the search processthat led to it the paper developsthe necessary theoretical framework and appliesit to one type of learning rule induction a process oriented version of the cn rule learner is 
the identification of relevant attribute is an important and difficult task in data mining application where induction is used a the primary tool for knowledge extraction this paper introduces a new rule induction algorithm ritio which eliminates attribute iu order of decreasing irrelevancy the rule produced by ritio are shown to be largely based on only the most relevant attribute experimental result with and without feature selection preprocessing confirm that ritio achieves high level of predictive accuracy 
this paper present an approach to the recognition of articulated d object in monocular video image a hierarchical object representation model object a a composition of rigid component which are explicitly connected by specific kinematic constraint e g rotational and or translational joint the recognition task follows this tree like structure by first estimating the d pose of the static component root and afterwards determining the relative d pose of the remaining component recursively this method limit the search space for the actual correspondence between image and model feature and cope with the problem of self occlusion experiment in the context of autonomous mobile robot show the practicability of this approach 
on large problem reinforcement learning system must use parameterized function approximators such a neural network in order to generalize between similar situation and action in these case there are no strong theoretical result on the accuracy of convergence and computational result have been mixed in particular boyan and moore reported at last year s meeting a series of negative result in attempting to apply dynamic programming together with function approximation to simple control problem with continuous state space in this paper we present positive result for all the control task they attempted and for one that is significantly larger the most important dierences are that we used sparse coarse coded function approximators cmacs whereas they used mostly global function approximators and that we learned online whereas they learned oine boyan and moore and others have suggested that the problem they encountered could be solved by using actual outcome rollouts a in classical monte carlo method and a in the td algorithm when however in our experiment this always resulted in substantially poorer performance we conclude that reinforcement learning can work robustly in conjunction with function approximators and that there is little justification at present for avoiding the case of general 
this paper present experiment with datasets and decision tree pruning algorithmsthat show that increasing trainingset size often result in a linear increase intree size even when that additional complexityresults in no significant increase inclassification accuracy said differently removingrandomly selected training instancesoften result in tree that are substantiallysmaller and just a accurate a those builton all available training instance this impliesthat decrease 
this paper present a new approach for the classification and retrieval of three dimensional image and model from database a set of retrieval algorithm is introduced these algorithm are content based meaning that the input is not made out of keywords but of three dimensional model tensor of inertia distribution of normal and distribution of cord are used to describe each model the database can be searched by scale shape or color or any combination of these parameter a user friendly interface make the retrieval operation simple and intuitive and allows to edit reference model according to the specification of the user experimental result using a database of more than range image and vrml model are presented 
we describe an estimation technique which given a measurement of the depth of a target from a wide field of view wfov stereo camera pair produce a minimax risk fixed size confidence interval estimate for the target depth this work constitutes the first application to the computer vision domain of optimal fixed size confidence interval decision theory the approach is evaluated in term of theoretical capture probability and empirical capture frequency during actual experiment with a target on an optical bench the method is compared to several other procedure including the kalman filter the minimax approach is found to dominate all the other method in performance in particular for the minimax approach a very close agreement is achieved between theoretical capture probability and empirical capture frequency this allows performance to be accurately predicted greatly facilitating the system design and delineating the task that may be performed with a given system 
the observed distribution of natural image is far from uniform on the contrary real image have complex and important structure that can be exploited for image processing recognition and analysis there have been many proposed approach to the principled statistical modeling of image but each ha been limited in either the complexity of the model or the complexity of the image we present a non parametric multi scale statistical model for image that can be used for recognition image de noising and in a generative mode to synthesize high quality texture 
we present a new algorithm for computing the heading from multiple frame which improves on previous approach it exploit our discovery and analysis of a new structure from motion ambiguity allowing for this ambiguity also make two frame reconstruction more robust we show experimentally that the error landscape for planar scene ha few significant local minimum 
the distribution of object color can be effectively utilized for recognition and indexing difficulty arise in the recognition of object color distribution when there are variation in illumination color change in object pose with respect to illumination direction and specular reflection however most of the recent approach to color based recognition focus mainly on illumination color invariance we propose an approach that identifies object color distribution influenced by illumination pose illumination color and specularity we suggest the use of chromaticity distribution to achieve illumination pose invariance to characterize change in chromaticity distribution due to illumination color a set of chromaticity histogram of each object is generated for a range of lighting color based on linear model of illumination and reflectance and the histogram are represented using a small number of eigen basis vector constructed from principal component analysis since specular reflection may alter the chromaticity distribution of test object a model based specularity detection rejection algorithm called chromaticity differencing is developed to reduce these effect 
abstract the appearance of an object depends on both the viewpoint from which it is observed and the lightsources by which it is illuminated if the appearance of two object is never identical for any pose or lightingconditions then in theory the object can always be distinguished or recognized the question arises whatis the set of image of an object under all lighting condition and pose in this paper we consider only the set ofimages of an object under variable illumination 
this paper defines a formal approach to learning from example described by labelled graph we propose a formal model based upon lattice theory and in particular with the use of galois lattice we enlarge the domain of formal concept analysis by the use of the galois lattice model with structural description of example and concept our implementation called graal for graph and learning construct a galois lattice for any description language provided that the two operation of comparison and generalization are determined for that language we prove that these operation exist in the case of labelled graph 
we present a novel geometric approach for solving the stereo problem for an arbitrary number of image greater than or equal to it is based upon the definition of a variational principle that must be satisfied by the surface of the object in the scene and their image the euler lagrange equation which are deduced from the variational principle provide a set of pde s which are used to deform an initial set of surface which then move towards the object to be detected the level set 
this paper describes the application of distributional clustering to document classification this approach cluster word into group based on the distribution of class label associated with each word thus unlike some other unsupervised dimensionalityreduction technique such a latent semantic indexing we are able to compress the feature space much more aggressively while still maintaining high document classification accuracy experimental result obtained on three real world data set show that we can reduce the feature dimensional y by three order of magnitude and lose only accuracy significantly better than latent semantic indexing class based clustering l feature selection by mutual information or markov blanket based feature selection we also show that le aggressive clustering sometimes result in improved classification accuracy over classification without clustering 
for many practical application it is important to relax the self calibration condition to allow for changing internal camera parameter e g zooming focusing classical technique failed for such condition we present the available constraint that allow u to right a projective calibration to a euclidean one meanwhile we found that the estimation of the internal parameter were rather inaccurate we discus theoretically this difficulty and above all the resulting effect on the d reconstruction in fact we show that the uncertainty on the focal length estimation lead to a euclidean calibration up to a quasi anisotropic homothety whereas the error on the principal point can often be interpreted a a translation hopefully the calibration we come up with is quite acceptable for reconstruction of model 
the extraction of curvilinear structure is an important low level operation in computer vision most existing operator use a simple model for the line that is to be extracted i e they do not take into account the surroundings of a line therefore they will estimate a wrong line position whenever a line with different lateral contrast is extracted in contrast the algorithm proposed in this paper us an explicit model for line and their surroundings by analyzing the scale space behavior of a model line profile it is shown how the bias that is induced by asymmetrical line can be removed thus the algorithm is able to extract an unbiased line position and width both with sub pixel accuracy 
the ability to rely on similarity metric invariant to image transform ations is an important issue for image classification task such a face or character recognition we analyze an invariant metric that ha performed well for the latter the tangent distance and study it limitation when applied to regular image showing that the most significant among these convergence to local minimum can be drastically reduced by computing the distance in a multiresolution setting this lead to the multiresolution tangent distance which exhibit significantly higher invariance to image transformation and can be easily combined with robust estimation procedure 
this contribution describes an automatic d surface modeling system that extract dense metric d surface from an uncalibrated video sequence a static d scene is observed from multiple viewpoint by freely moving a video camera around the object no restriction on camera movement and internal camera parameter like zoom are imposed a the camera pose and intrinsic parameter are calibrated from the sequence dense surface reconstruction are obtained by first treating consecutive image of the sequence a stereoscopic pair and computing dense disparity map for all image pair all viewpoint are then linked by controlled correspondence linking for each image pixel the correspondence linking algorithm allows for accurate depth estimation a well a image texture fusion from all viewpoint simultaneously by keeping track of surface visibility and measurement uncertainty it can cope with occlusion and measurement outlier the correspondence linking is applied to increase the robustness and geometrical resolution of surface depth a well a to remove highlight and specular reflection and to create super resolution texture map for increased realism the major impact of this work is the ability to automatically generate geometrically correct and visually pleasing d surface model from image sequence alone which allows the economic model generation for a wide range of application the resulting textured d surface model are highly realistic vrml representation of the scene 
we have developed a computational theory of rodent navigation that includes analog of the place cell system the head direction system and path integration in this paper we present simulation result showing how interaction between the place and head direction system can account for recent observation about hippocampal place cell response to doubling and or rotation of cue card in a cylindrical arena sharp et al 
in stereo algorithm with more than two camera the improvement of accuracy is often reported since they are robust against noise however another important aspect of the polynocular stereo that is the ability of occlusion detection ha been paid le attention we intensively analyzed the occlusion in the camera matrix stereo sea and developed a simple but effective method to detect the presence of occlusion and to eliminate it effect in the correspondence search by considering several statistic on the occlusion and the accuracy in the sea we derived a few base mask which represent occlusion pattern and are effective for the detection of occlusion several experiment using typical indoor scene showed quite good performance to obtain dense and accurate depth map even at the occluding boundary of object 
this paper describes a new approach to automatically learning linguistic knowledge for spelling correction a major feature of this approach is the fact that the acquired knowledge is captured in a small set of easily understood rule a opposed to a large set of opaque feature and weight a perspicuous representation is advantageous in order to best exploit human intuition to understand and improve upon the acquired knowledge of the system 
the goal of active template research is to create a single unified environment that a data analyst can use to carry out a knowledge discovery project and to deliver the resulting solution in the form of an active template an active template is a hyper linked information structure that tightly integrates action executable program and command result model datasets prediction report and documentation explanation of decision action and result the use of active template provides a number of benefit including user guidance improved documentation of action and result and increased reuse of previous work 
practical approach to clustering use an iterative procedure e g k mean em which converges to one of numerous local minimum it is known that these iterative technique are especially sensitive to initial starting condition we present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the mode of a distribution the refined initial starting condition allows the iterative algorithm to converge to a better local minimum the procedure is applicable to a wide class of clustering algorithm for both discrete and continuous data we demonstrate the application of this method to the popular k mean clustering algorithm and show that refined initial starting point indeed lead to improved solution refinement run time is considerably lower than the time required to cluster the full database the method is scalable and can be coupled with a scalable clustering algorithm to address the large scale clustering problem in data mining 
scalability determines the potential in distributing 
we consider neural network model for stochastic nonlinear dynamical system where measurement of the variable of interest are only avail able at irregular interval i e most realization are missing dif culties arise since the solution for prediction and maximum likelihood learn ing with missing data lead to complex integral which even for simple case cannot be solved analytically in this paper we propose a spe ci c combination of a nonlinear recurrent neural predictive model and a linear error model which lead to tractable prediction and maximum likelihood adaptation rule in particular the recurrent neural network can be trained using the real time recurrent learning rule and the linear error model can be trained by an em adaptation rule implemented u ing forward backward kalman lter equation the model is applied to predict the glucose insulin metabolism of a diabetic patient where blood glucose measurement are only available a few time a day at irregular interval the new model show considerable improvement with respect to both recurrent neural network trained with teacher forcing or in a free running mode and various linear model 
in this paper we propose fully d active surface model for image segmentation our model are capable of fitting a diverse range of region shape they have low sensitivity to initial shape and position we design self inflation deflation force which cooperate naturally with gradient force they permit the active surface to travel a long distance without the aid of any external force they are easily controlled in both their direction and magnitude the model produce accurate segmentation when tested with synthetic and real image they manifest robustness to image noise and imperfect image data importantly they are capable of converging to the correct boundary even if the initial estimate is not close 
we present a technique for camera calibration and euclidean reconstruction from multiple image of the same scene unlike standard tsai s camera calibration from a known scene we exploited controlled known motion of the camera to obtain it calibration and euclidean reconstruction without any knowledge about the scene we consider three linearly independent translation of an uncalibrated camera mounted on a robot arm that provides u with four view of the scene the translation of the robot arm are measured in a robot coordinate system this special but still realistic arrangement allowed u to find a linear algorithm for recovering all intrinsic camera calibration parameter the rotation of the camera with respect to the robot coordinate system and proper scaling factor for all point allowing their euclidean reconstruction the experiment showed that an efficient and robust algorithm wa obtained by exploiting total least square in combination with careful normalization of image coordinate 
a new approach for clustering is proposed this method is basedon an analogy to a physical model the ferromagnetic potts modelat thermal equilibrium is used a an analog computer for this hardoptimization problem we do not assume any structure of the underlyingdistribution of the data phase space of the potts model isdivided into three region ferromagnetic super paramagnetic andparamagnetic phase the region of interest is that correspondingto the super paramagnetic one where 
automated scene recognition in dynamic environment involvesnot only object classification or recognition but also a furtherstep consisting in finding what is going on in this environment that isto say what the object are doing and what their purpose are 
a novel method for representing d object that unifies viewer and model centered object representation is presented a unified d frequency domain representation called volumetric iconic spectral signature v i encapsulates both the spatial structure of the object and a continuum of it view in the same data structure the frequency domain image of an object viewed from any direction can be directly extracted employing an extension of the projection slice theorem where each fourier transformed view is a planar slice of the volumetric frequency representation the v i representation can be employed for pose invariant recognition of complex object such a face the recognition and pose estimation is based on an efficient matching algorithm in a four dimensional fourier space experimental example of pose estimation and recognition of face are also presented 
stereo reconstruction algorithm often fail to properly deal with complex surface because there is not enough image information to overcome this problem we propose to guide the reconstruction process using a priori information about the differential geometry of the object surface we use both linear structure such a crest line or scalar field such a curvature value to generate a reconstruction of the surface which is consistent with the differential property this method improves the accuracy of the reconstruction around the discontinuity and increase the compactness of the surface representation 
the kalman filter is a very efficient optimal filter however it ha the precondition that the noise of the process and of the measurement are gaussian the author introduce the general distribution filter which is an optimal filter that can be used even where the distribution are not gaussian an efficient practical implementation of the filter is possible where the distribution are discrete and compact or can be approximated a such 
a number of vision based biometric technique have beenproposed in the past for personal identification we present a novel onebased on visual capturing of signature this paper describes a systembased on correlation and recursive prediction method that can trackthe tip of the pen in real time with sufficient spatio temporal resolutionand accuracy to enable signature verification several example and theperformance of the system are shown introduction and motivationa number of 
a linear architectural model of cortical simple cell is presented the model evidence how mutual inhibition occurring throughsynaptic coupling function asymmetrically distributed in space can be a possible basis for a wide variety of spatio temporal simplecell response property including direction selectivity and velocitytuning while spatial asymmetry are included explicitly in thestructure of the inhibitory interconnection temporal asymmetriesoriginate from the 
to monitor or control a stochastic dynamic system we need to reason about it current state exact inference for this task requires that we maintain a complete joint probability distribution over the possible state an impossible requirement for most process stochastic simulation algorithm provide an alternative solution by approximating the distribution at time via a relatively small set of sample the time sample are used a the basis for generating the sample at time however since only existing sample are used a the basis for the next sampling phase new part of the space are never explored we propose an approach whereby we try to generalize from the time sample to unsampled region of the state space thus these sample are used a data for learning a distribution over the state at time which is then used to generate the time sample we examine different representation for a distribution including density tree bayesian network and tree structured bayesian network and evaluate their appropriateness to the task the machine learning perspective allows u to examine issue such a the tradeoff of using more complex model and to utilize important technique such a regularization and prior we validate the performance of our algorithm on both artificial and real domain and show significant improvement in accuracy over the existing approach 
present a new approach to segment vessel from d angiography of the brain the author approach is based on a vessel model and us a multiscale analysis in order to extract the vessel network surrounding an aneurysm the author model allows them to choose a criterion based on the eigenvalue of the hessian matrix for selecting a subset of interesting point near the vessel center it also allows them to choose a good parameter for a normalization of the single scale response the response at one scale is obtained by integrating along a circle the first derivative of the intensity in the radial direction once the multiscale response is obtained the author create a smoothed skeleton of the vessel combined with a mip or a volume rendering to enhance their visualization the method ha been tested on a large variety of d image of the brain with excellent result vessel of various size and contrast are detected with a remarkable robustness and most junction are preserved 
this paper present an approach to measuring fluid flow from image sequence the approach center around a motion recovery algorithm that is based on principle from fluid mechanic the algorithm is constrained so that recovered flow observe conservation of mass a well a physically motivated boundary condition empirical result are presented from application of the algorithm to fluid flow captured via transmittance imagery i e radiograph in these experiment fluid seeded with tracer were driven through simple physical system the significance of this work is twofold first from a theoretical point of view it is shown how information derived from the physical behavior of fluid can be used to motivate a flow recovery algorithm second from an application point of view the developed algorithm can be used to augment the tool that are available for the measurement of fluid dynamic other imaged flow that observe compatible constraint might benefit in a similar fashion 
in this paper we propose recurrent neural network with feedback into the inputunits for handling two type of data analysis problem on the one hand thisscheme can be used for static data when some of the input variable are missing on the other hand it can also be used for sequential data when some of theinput variable are missing or are available at different frequency unlike in thecase of probabilistic model e g gaussian of the missing variable the networkdoes not 
a an alternative to planning an approach to highlevel agent control based on concurrent program execution is considered a formal definition in the situation calculus of such a programming language is presented and illustrated with a detailed example the language includes facility for prioritizing the concurrent execution interrupting the execution when certain condition become true and dealing with exogenous action the language differs from other procedural formalism for concurrency in that the initial state can be incompletely specified and the primitive action can be user defined by axiom in the situation calculus 
this paper describes a system that lead u to believe in the feasibility of constructing natural spoken dialogue system in task oriented domain it specifically address the issue of robust interpretation of speech in the presence of recognition error robustness is achieved by a combination of statistical error post correction syntacticallyand semantically driven robust parsing and extensive use of the dialogue context we present an evaluation of the system using time to completion and the quality of the final solution that suggests that most native speaker of english can use the system successfully with virtually no training 
digital video is rapidly becoming important for education entertainment and a host of multimedia application with the size of the video collection growing to thousand of hour technology is needed to effectively browse seg ments in a short time without losing the content of the video we propose a method to extract the significant audio and video information and create a skim video which represents a very short synopsis of the original the goal of this work is to show the utility of integrating lan guage and image understanding technique for video skimming by extraction of significant information such a specific object audio keywords and relevant video struc ture the resulting skim video is much shorter where com paction is a high a and yet retains the essential content of the original segment 
we present a novel new word extraction methodfrom japanese text based on expected wordfrequencies first we compute expected wordfrequencies from japanese text using a robuststochastic n best word segmenter we then extractnew word by filtering out erroneous wordhypotheses whose expected word frequency arelower than the predefined threshold the methodis derived from an approximation of the generalizedversion of the forward backward algorithm when the japanese word segmenter is 
this paper present a system for automatic extraction and tracking of d contour of the tongue surface from digital ultrasound image sequence the input to the system is provided by a head and transducer support system hat which is developed for use in ultrasound imaging of the tongue movement we developed a novel active contour snake model that us several temporally adjacent image during the extraction of the tongue surface contour for an image frame the user supply an initial contour model for a single image frame in the whole sequence using optical flow and multi resolution method this initial contour is then used to find the candidate contour point in the temporally immediate adjacent image subsequently the new snake mechanism is applied to estimate optimal contour for each image frame using these candidate point in turn the extracted contour are used a model for the extraction process of new adjacent frame finally the system us a novel postprocessing technique to refine the position of the contour we tested the system on different speech sequence each containing about image visual inspection of the detected contour by the speech expert show that the result are very promising and this system can be effectively employed in speech and swallowing research 
we evaluate inductive logic programming ilp method for predicting fault density inc class in this problem each trainingexample is a c class definition representedas a calling tree and labeled a quot positive quot iff fault i e error were discoveredin it implementation we compare two ilpsystems foil and flipper and explorethe reason for their differing performance using both natural and artificial data wethen propose two extension to flipper a 
a simple linear averaging of the output of several network ase g in bagging seems to follow naturally from a bias variancedecomposition of the sum squared error the sum squared error ofthe average model is a quadratic function of the weighting factorsassigned to the network in the ensemble suggesting a quadraticprogramming algorithm for finding the quot optimal quot weighting factor if we interpret the output of a network a a probability statement the sum squared error 
a novel local scale controlled piecewise linear diffusion for selective smoothing and edge detection is presented the diffusion stop at the place and time determined by the minimum reliable local scale and a spatial variant anisotropic local noise estimate it show anisotropic nonlinear diffusion equation using diffusion coefficient tensor that continuously depend on the gradient is not necessary to achieve sharp distorted stable edge detection across many scale the new diffusion is anisotropic and asymmetric only at place it need to be i e at significant edge it not only doe not diffuse across significant edge but also enhances edge it advance geometry driven diffusion because it is a piecewise linear model rather than a full nonlinear model thus it is simple to implement and analyze and avoids the difficulty and problem associated with nonlinear diffusion it advance local scale control by introducing spatial variant anisotropic local noise estimation and local stopping of diffusion the original local scale control wa based on the unrealistic assumption of uniformly distributed noise independent of the image signal the local noise estimate significantly improves local scale control 
this paper demonstrates the existence of a new approximate intrinsic ambiguity in euclidean structure from motion sfm which occurs a generically a the ba relief ambiguity but unlike it strengthens for scene with more depth variation the ambiguity doe not occur in projective sfm but the reason for this make projective reconstruction more likely to have large error our analysis give a semiquantitative characterization of the least square error surface over a domain complementary to that analyzed by jepson heeger and maybank a part of our analysis we show that the least square error for infinitesimal motion the optical flow error give a good approximation to the least square error for moderate finite motion we propose that many high error local minimum occur for epipoles in or near the image we also establish the existence of a new local minimum in minimizing over the rotation given the translation direction 
we study the problem of statistically correct inference in network whose basic representation are population code population code are ubiquitous in the brain and involve the simultaneous activity of many unit coding for some low dimensional quantity a classic example are place cell in the rat hippocampus these fire when the animal is at a particular place in an environment so the underlying quantity ha two dimension of spatial location we show how to interpret the activity a encoding whole probability distribution over the underlying variable rather then just single value and propose a method of inductively learning mapping between population code that are computationally tractable and yet offer good approximation to statistically optimal inference we simulate the method on some simple example to prove it competence 
this paper explores the possibility and limit of a discourse grammar applied to spontaneous speech most discourse grammar e g sdrt asher rst mann thompson tend to be descriptive theory of written discourse which presuppose a coherent structure this structure is the outcome of a goal directed planning process on the part of the producer in order to obtain a better understanding of the planning process we analyse spoken discourse elicited in an experimental setting subject describe the pixel per pixel development of sketch map on a computer screen this force the speaker to conceptualise the perceived state of affair plan their discourse and produce a description of the drawing at the same time thus we find evidence for the planning process in the recorded data and can show that the discourse structure are le globally coherent than those underlying written text in our paper we discus to what extent a flexible discourse grammar based on a tree description grammar tdg schilder can handle such data 
automating the learning of causal model from sample data is a key step toward incorporatingmachine learning in the automation of decision making and reasoning under uncertainty this paper present a bayesian approach to the discovery of causal model using a minimummessage length mml method we have developed encoding and search method for discoveringlinear causal model the initial experimental result presented in this paper showthat the mml induction approach can recover causal 
the same scene viewed under two different illuminant inducestwo different colour image if the two illuminant are the samecolour but are placed at different position then corresponding rgb pixelsare related by simple scale factor in contrast if the lighting geometryis held fixed but the colour of the light change then it is the individualcolour channel e g all the red pixel value or all the green pixel thatare a scaling apart it is well known that the image dependency 
many search tree are impractically large to explore exhaustively recently technique like limited discrepancy search have been proposed for improving the chance of finding a goal in a limited amount of search depth bounded discrepancy search offer such a hope the motivation behind depth bounded discrepancy search is that branching heuristic are more likely to be wrong at the top of the tree than at the bottom we therefore combine one of the best feature of limited discrepancy search the ability to undo early mistake with the completeness of iterative deepening search we show theoretically and experimentally that this novel combination outperforms existing technique 
this paper however our approximation of a subclass ofthese relation proved helpful for a number of query a strong example of the part whole relation occurswhen a country is mentioned in the query and aprovince or city within that country is mentioned in thedocument for example 
we describe a hierarchical appearance based method for learning recognizing and predicting arbitrary spatiotemporal sequence of image the method which implement a robust hierarchical form of the kalman filter derived from the minimum description length mdl principle includes a a special case several well known object encoding technique including eigenspace method for static recognition successive level of the hierarchical filter implement dynamic model operating over successively larger spatial and temporal scale each hierarchical level predicts the recognition state at a lower level and modifies it own recognition state using the residual error between the prediction and the actual lower level state simultaneously on a longer time scale the filter learns an internal model of input dynamic by adapting it generative and state transition matrix at each level to minimize prediction error the resulting prediction learning scheme thereby implement an on line form of the well known expectation maximization em algorithm from statistic we present experimental result demonstrating the method s efficacy in mediating robust spatiotemporal recognition in a variety of scenario containing varying degree of occlusion and clutter 
abstract most computational engineering based loosely on biology us continuous variable to represent neural activity yet most neuron communicate with action potential the engineering view is equivalent to using a rate code for representing information and for computing an increasing number of example are being discovered in which biology may not be using rate code information can be represented using the timing of action potential and efficiently computed with in this representation the analog match problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an underlying rhythm by using adapting unit to effect a fundamental change of representation of a problem we map the recognition of word having uniform time warp in connected speech into the same analog match problem we describe the architecture and preliminary result of such a recognition system using the fast event of biology in conjunction with an underlying rhythm is one way to overcome the limit of an eventdriven view of computation when the intrinsic hardware is much faster than the time scale of change of input this approach can greatly increase the effective computation per unit time on a given quantity of hardware spike timing 
the problem of assigning m point in the n dimensional real spacernto k cluster is formulated a that of determining k center inrnsuch that the sum of distance of each point to the nearestcenter is minimized if a polyhedral distance is used the problemcan be formulated a that of minimizing a piecewise linear concavefunction on a polyhedral set which is shown to be equivalent toa bilinear program minimizing a bilinear function on a polyhedralset a fast finite k median 
for neural network with a wide class of weight prior it can be shown that in the limit of an infinite number of hidden unit the prior over function tends to a gaussian process in this paper analytic form are derived for the covariance function of the gaussian process corresponding to network with sigmoidal and gaussian hidden unit this allows prediction to be made efficiently using network with an infinite number of hidden unit and show that somewhat paradoxically it may be 
previous research ha shown that aggregated predictor improve the performance of non parametric function approximation technique this paper present the result of applying aggregated predictor to a computer vision problem and show that the method of bagging significantly improves performance in fact the result are better than those previously reported on other domain this paper explains this performance in term of the variance and bias 
we discus a strategy for polychotomous classification that involvesestimating class probability for each pair of class and then couplingthe estimate together the coupling model is similar to thebradley terry method for paired comparison we study the natureof the class probability estimate that arise and examine theperformance of the procedure in real and simulated datasets classifiersused include linear discriminants nearest neighbor and thesupport vector machine 
this paper summarizes a novel logic based approach to grouping and perceptual organization presented more thoroughly in cite feldman ci and present novel efficient method for computing interpretation in this framework grouping interpretation are first defined a logical structure built out of atomic premise regularity that are derived from consideration of non accidentalness these interpretation can then be partially ordered by their degree of regularity or constraint measured numerically by their it codimension the genericity constraint the principle that interpretation should minimize coincidence in the observed configuration dictate that the preferred interpretation will be the minimum in this partial order i e the interpretation with it maximum codimension the preferred interpretation called the it qualitative parse corresponds neatly to the interpretation intuitively preferred by human observer a a side effect the most salient or most structured part of the scene can be identified a the highest codimension subtree of the qualitative parse an efficient o n method for computing the maximum codimension interpretation is presented along with example 
we give a new treatment of tabular lr parsing which is an alternative to tomita s generalized lr algorithm the advantage is twofold firstly our treatment is conceptually more attractive because it us simpler concept such a grammar transformation and standard tabulation technique also know a secondly the static and dynamic complexity of parsing both in space and time is significantly reduced 
in transformation based parsing a finite sequence of tree rewriting rule are checked for application to an input structure since in practice only a small percentage of rule are applied to any particular structure the naive parsing algorithm is rather inefficient we exploit this sparseness in rule application to derive an algorithm two to three order of magnitude faster than the standard parsing algorithm 
two new scheme are presented for finding human face in a photograph the first scheme approximates the unknown distribution of the face and the face like manifold using higher order statistic ho an ho based data clustering algorithm is also proposed in the second scheme the face to non face and non face to face transition are learnt using a hidden markov model hmm the hmm parameter are estimated corresponding to a given photograph and the face are located by examining the optimal state sequence of the hmm experimental result are presented on the performance of both the scheme 
hidden markov model hmms have proven to be one of the most widelyused tool for learning probabilistic model of time series data inan hmm information about the past is conveyedthrough a single discrete variable the hidden state we discus ageneralization of hmms in which this state is factored into multiplestate variable and is therefore represented in a distributed manner we describe an exact algorithm for inferring the posteriorprobabilities of the hidden state variable given the observation and relate it to the forward backward algorithm for hmms and toalgorithms for more general graphical model due to the combinatorialnature of the hidden state representation this exact algorithm isintractable a in other intractable system approximate inferencecan be carried out using gibbs sampling or variational method within the variational framework wepresent a structured approximation in which the the statevariables are decoupled yielding a tractablealgorithm for learning the parameter of the model empiricalcomparisons suggest that these approximation are efficient andprovide accurate alternative to the exact method finally we use thestructured approximation to model bach s chorale and show thatfactorial hmms can capture statistical structure in this data setwhich an unconstrained hmm cannot 
an image is often represented by a set of detected feature we getan enormous compression by representing image in this way furthermore we get a representation which is little affected by smallamounts of noise in the image however feature are typicallychosen in an ad hoc manner we show how a good set of featurescan be obtained using sufficient statistic the idea of sparsedata representation naturally arises we treat the dimensionaland dimensional signal reconstruction 
association rule algorithm can produce a very large number of output pattern this ha raised question of whether the set of discovered rule quot overfit quot the data because all the pattern that satisfy some constraint are generated the bonferroni effect in other word the question is whether some of the rule are quot false discovery quot that are not statistically significant we present a novel approach for estimating the number of quot false discovery quot at any cutoff level empirical evaluation 
the goal of pattern classification can be approached from two point of view informative where the classifier learns the class density or discriminative where the focus is on learning the class boundary without regard to the underlying class density we review and synthesize the tradeoff between these two approach for simple classifier and extend the result to modern technique such a naive bayes and generalized additive model data mining application ofi a ll s cl l l m no nn ron ijell uyaraw u ijui u lllcz u i lup ullll di ii iii igi lr uig where the tradeoff between informative and discriminative classifier are especially relevant experimental result are provided for simulated and real data 
abstract av pierre mend s france bron cedex france zighed rakotoma ffeschet univ lyon fr in this paper we propose an extension of fischer s 
we study an application of image registration in the medical domain based on a d hierarchical deformable registration algorithm we have developed a prototype system which automatically aligns a standard atlas to a subject s data to create a customized atlas combined with domain knowledge the registration algorithm can also detect asymmetry and abnormal variation in the subject s data that indicate the existence and location of pathology we have conducted test on mri scan of normal brain mri and i ct scan of brain with pathology with result qualitatively comparable to manual segmentation 
this paper describes a theory and a practical algorithm for the autocalibration of a moving projective camera from view of a planar scene the unknown camera calibration and up to scale the unknown scene geometry and camera motion are recovered from the hypothesis that the camera s internal parameter remain constant during the motion this work extends the various existing method for non planar autocalibration to a practically common situation in which it is not possible to bootstrap the calibration from an intermediate projective reconstruction it also extends hartley s method for the internal calibration of a rotating camera to allow camera translation and to provide d a well a calibration information the basic constraint is that the projection of orthogonal direction vector point at infinity in the plane must be orthogonal in the calibrated camera frame of each image abstractly since the two circular point of the d plane representing it euclidean structure lie on the d absolute conic their projection into each image must lie on the absolute conic s image representing the camera calibration the resulting numerical algorithm optimizes this constraint over all circular point and projective calibration parameter using the inter image homographies a a projective scene representation 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
the paper present a language model that develops syntatic structure and us it to extract meaningful information from the word history thus enabling the use of long distance dependency the model assigns probability to every joint sequence of word binary parse structure with headword annotation the model it probabilistic parametrization and a set of experiment meant to evaluate it predictive power are presented 
motion of an observer relative to object in a scene provides information about the structure of the scene changing pattern of shading due to motion relative to the light source provide information about surface structure albedo and light source one can stratify this photometric information into affine unitary and metric structure much like the stratification of structure from motion for lambertian surface if either motion or photometry give u more than affine structure the two cue can be combined to yield full metric information edge constraint plus unitary photometry also give u full metric photometry affine structure alone contains much of the quantitative structure information allowing u to judge such thing a the ordinal relationship between the albedo 
it is widely accepted that textureless surface cannot be recovered using passive sensing technique the problem is approached by viewing image formation a a fully three dimensional mapping it is shown that the lens encodes structural information of the scene within a compact three dimensional space behind it after analyzing the information content of this space and by using it property we derive necessary and sufficient condition for the recovery of textureless scene based on these condition a simple procedure for recovering textureless scene is described we experimentally demonstrate the recovery of three textureless surface namely a line a plane and a paraboloid since textureless surface represent the worst case recovery scenario all the result and the recovery procedure are naturally applicable to scene with texture 
we study the d shape similarity between closed surface we represent a curved or polyhedral d object of genus zero using a mesh representation that ha nearly uniform distribution with known connectivity among mesh node we define a shape similarity metric based on the l distance between the local curvature distribution over the mesh representation of the two object for both convex and concave object the shape metric can be computed in time o n where n is the number of tessellation of sphere or the number of mesh which approximate the surface experiment show that our method produce good shape similarity measurement 
most traditional approach to anaphora resolution rely heavily on linguistic and domain knowledge one of the disadvantage of developing a knowledgebased system however is that it is a very labourintensive and time consuming task this paper present a robust knowledge poor approach to resolving pronoun in technical manual this approach is a modification of the practical approach mitkov a and operates on text pre processed by a partof speech tagger input is checked against agreement and a number of antecedent indicator candidate are assigned score by each indicator and the candidate with the highest aggregate score is returned a the antecedent we propose this approach a a platform for multilingual pronoun resolution the robust approach wa initially developed and tested for english but we have also adapted and tested it for polish and arabic for both language we found that adaptation required minimum modification and that further even if used unmodified the approach delivers acceptable success rate preliminary evaluation report high success rate in the range of and over 
this paper discus the design of the eurowordnet database in which semantic database like wordnetl for several language are combined via a so called inter lingual index in this database language independent data is shared and language specific property are maintained a well a special interface ha been developed to compare the semantic configuration across language and to track down difference the pragmatic design of the database make it possible to gather empirical evidence for a common cross linguistic ontology 
we consider multi criterion sequential decision making problem where the vector valued evaluation are compared by a given fixed total ordering condition for the optimality of stationary policy and the bellman optimality equation are given for a special but important class of problem when the evaluation of policy can be computed for the criterion independently of each other the analysis requires special care a the topology introduced by pointwise convergence and the order topology introduced by the preference order are in general incompatible reinforcement learning algorithm are proposed and analyzed preliminary computer experiment confirm the validity of the derived algorithm these type of multi criterion problem are most useful when there are several optimal solution to a problem and one want to choose the one among these which is optimal according to another fixed criterion possible application in robotics and repeated game are outlined 
motivated by the finding of modular structure in the associationcortex we study a multi modular model of associative memory thatcan successfully store memory pattern with different level of activity we show that the segregation of synaptic conductance intointra modular linear and inter modular nonlinear one considerablyenhances the network s memory retrieval performance comparedwith the conventional single module associative memory network the multi modular network ha two main 
the softassign quadratic assignment algorithm ha recently emerged a an effectivestrategy for a variety of optimization problem in pattern recognition and combinatorialoptimization while the effectiveness of the algorithm wa demonstrated inthousands of simulation there wa no known proof of convergence here we providea proof of convergence for the most general form of the algorithm introductionrecently a new neural optimization algorithm ha emerged for solving quadratic 
a mature data mining system ha to interact with standard dbms a crucial factor in the performance of such a data mining system lie in this interaction the keso project aim at the deveiopment of such a tool and it interaction with the database is restricted to two way table query a special kind of aggregate query this restriction give rise to ample possibility to optimize the computation of such two way table e g by using parallelisation or by temporary storage of intermediate result however the size of these two way table put a large communication overhead on the database interaction of keso in this paper we propose to compute certain aggregate in the database this approach low 
we present a new algorithm for efficient matching of d polygonal arc the algorithm is based on the decomposition of the arc into set of corresponding line segment with equal length we derive a closed form solution for the transformation that give the best match between two set of corresponding line segment best in the sense of an l sub norm distance measure which enables the development of efficient arc matching algorithm we apply this algorithm to the problem of finding a match between a short are and a piece of a long arc in real and synthetic image and compare the result with alternative technique in the literature 
the recent emergence of data mining a amajor application of machine learning ha ledto increased interest in fast rule induction algorithm these are able to efficiently processlarge number of example under the constraintof still achieving good accuracy if eis the number of example many rule learnershave o e asymptotic time complexity innoisy domain and c rule ha been empiricallyobserved to sometimes require o e recent advance have brought this bound downto 
in many application it is necessary to determine the similarity of two string a widely used notion of string similarity is the edit distance the minimum number of insertion deletion and substitution required to transform one string into the other in this report we provide a stochastic model for string edit distance our stochastic model allows u to learn a string edit distance function from a corpus of example we illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of word in conversational speech in this application we learn a string edit distance with nearly one fifth the error rate of the untrained levenshtein distance our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototype 
we describe a vision system that monitor activity in a site over extended period of time the system us a distributed set of sensor to cover the site and an adaptive tracker detects multiple moving object in the sensor our hypothesis is that motion tracking is sufficient to support a range of computation about site activity we demonstrate using the tracked motion data to calibrate the distributed sensor to construct rough site model to classify detected object to learn common pattern of activity for different object class and to detect unusual activity 
predicting item a user would like on the basis of other user rating for these item ha become a well established strategy adopted by many recommendation service on the internet although this can be seen a a classification problem algorithm proposed thus far do not draw on result from the machine learning literature we propose a representation for collaborative filtering task that allows the application of virtually any machine learning algorithm we identify the shortcoming of current collaborative filtering technique and propose the use of learning algorithm paired with feature extraction technique that specifically address the limitation of previous approach our best performing algorithm is based on the singular value decomposition of an initial matrix of user rating exploiting latent structure that essentially eliminates the need for user to rate common item in order to become predictor for one another s preference we evaluate the proposed algorithm on a large database of user rating for motion picture and find that our approach significantly outperforms current collaborative filtering algorithm 
if globally high dimensional data ha locally only low dimensional distribution it is advantageous to perform a local dimensionality reduction beforefurther processing the data in this paper we examine several technique forlocal dimensionality reduction in the context of locally weighted linear regression a possible candidate we derive local version of factor analysisregression principle component regression principle component regressionon joint distribution and partial 
this paper present a method to generate non monotonic rule with exception from posi tive negative example and background knowl edge in inductive logic programming we adopt extended logic program a the form of program to be learned where two kind of negation nega tion a failure and classical negation are effectively used in the presence of incomplete information while default rule axe generated a specialization of general rule that cover positive example exception to general rule are identified from negative ex amples and are then generalized to rule for cancellation of default we implemented the learning system lelp based on the proposed method in lelp when the number of posi tive and negative example are very close ei ther parallel default rule with positive and negative consequents or nondeterminis tic rule are learned moreover hierarchical default can also be learned by recursively calling the exception identification algorithm 
standard technique eg yule walker are available for learningauto regressive process model of simple directly observable dynamicalprocesses when sensor noise mean that dynamic areobserved only approximately learning can still been achieved viaexpectation maximisation em together with kalman filtering however this doe not handle more complex dynamic involvingmultiple class of motion for that problem we show here howem can be combined with the condensation algorithm 
we study the problem of how to detect quot interesting object quot appeared in a given image i our approach is to treat it a a function approximation problem based on an over redundant basis since the basis a library of image template is over redundant there are infinitely many way to decompose i to select the quot best quot decomposition we first propose a global optimization procedure that considers a concave cost function derived from a quot weighted l p norm quot with p this concave cost 
a system to retrieve image using a syntactic description of appearance is presented a multiscaleinvariant vector representation is obtained by first filtering image in the database withgaussian derivative filter at several scale and then computing low order differential invariant the multi scale representation is indexed for rapid retrieval query are designed by the usersfrom an example image by selecting appropriate region the invariant vector corresponding tothese region are 
in this paper we suggest determination a a representationof knowledge that should be easy to understand we briefly review determination which can bedisplayed in a tabular format and their use in prediction which involves a simple matching process wedescribe condet an algorithm that us feature selectionto construct determination from training data augmented by a condensation process that collapsesrows to produce simpler structure we report experimentsthat show condensation 
many current approach to statistical language modeling rely on independence assumption between the different explanatory variable this result in model which are computationally simple but which only model the main effect of the explanatory variable on the response variable this paper present an argument in favor of a statistical approach that also model the interaction between the explanatory variable the argument rest on empirical evidence from two series of experimetns concerning automatic ambiguity resolution 
in tree search depth first search dfs often us ordering successor heuristic if the heuris tic make a mistake ordering a bad successor without goal in it subtree before good one with goal in their subtrees dfs ha to unsuc cessfully traverse the whole bad subtree before finding a goal to prevent this useless work we present a new strategy called interleaved depthfirst search idf which search depth first several subtrees called active in parallel idf assumes a single processor on which it in terleaves dfs on active subtrees when idf find a mistake it traverse partially the bad subtree idf doe not reexpand node and us a memory amount linear in search depth with a bounded number of active subtrees idf outperforms dfs if the heuristic improves from the first to the second tree level experimental result on hard solvable problem confirm the practical validity of idf 
following the theory of statistical estimation the problem of recognizing object imaged in complex real world scene is examined from a parametric perspective a scalar measure of an object s complexity which is invariant under affine transformation and change in image noise level is extracted from the object s fisher information the volume of fisher information is shown to provide an overall statistical measure of the object s recognizability in a particular image while the complexity provides an intrinsically physical measure that characterizes the object in any image an information conserving method is then developed for recognizing an object imaged in a complex scene here the term information conserving mean that the method us al the measured data pertinent to the object s recognizability attains the theoretical lower bound on estimation error for any unbiased estimate and therefore is statistically optimal this method is then successfully applied to finding object imaged in thousand of complex real world scene 
we have been developing a theory of generic d shape based on a reaction diffusion model from mathematical physic the description of a shape is derived from the singularity of a curve evolution process driven by the reaction hyperbolic term the diffusion parabolic term is related to smoothing and shape simplification however the unification of the two is problematic because the slightest amount of diffusion dominates and prevents the formation of generic first order shock the technical issue is whether it is possible to smooth a shape in any sense without destroying the shock we now report a constructive solution to this problem by embedding the smoothing term in a global metric against which a purely hyperbolic evolution is performed from the initial curve this is a new flow for shape that extends the advantage of the original one specific metric are developed which lead to a natural hierarchy of shape feature analogous to the simplification one might perceive when viewing an object from increasing distance we illustrate our new flow with a variety of example 
denote a point in the plane by z z y and a polynomial of nth degree in z by f z spl sigma sub i j spl ge o sub i j spl le n a sub ij x sup i y sup j denote by z f the set of point for which f z z f is the d curve represented by f z in this paper we present a new approach to fitting d curve to data in the plane or d surface to range data which ha significant advantage over presently known method it requires considerably le computation and the resulting curve can be forced to lie close to the data set at prescribed point provided that there is an nth degree polynomial that can reasonably approximate the data linear programming is used to do the fitting the approach can incorporate a variety of distance measure and global geometric constraint 
this paper proposes a new efficient figure from ground method at every stage the data feature are classified to either background or unknown yet class thus emphasizing the background detection task and implying the name of the method the sequential application of such classification stage creates a bootstrap mechanism which improves performance in very cluttered scene this method can be applied to many perceptual grouping cue and an application to smoothness based classification of edge point is given a fast implementation using a kd tree allows to work on large realistic image 
in this paper we propose a new distance measure for an identification problem and describe experiment on fingerprint preselection using eigenfeatures of ridge direction pattern the distance is defined by likelihood ratio of error distribution of feature vector to the whole distribution of feature vector difference in addition we introduce quality index of feature vector and make the distance adaptive to the quality index experiment on fingerprint preselection for ten print card revealed that our proposed distance is much more effective than the mahalanobis distance by combining the eigenfeatures and traditional classification feature false acceptance rate at false rejection rate and one million card sec preselection speed on a standard workstation have been achieved this make it possible to construct high performance fingerprint identification system 
we study a time series model that can be viewed a a decisiontree with markov temporal structure the model is intractable forexact calculation thus we utilize variational approximation weconsider three different distribution for the approximation one inwhich the markov calculation are performed exactly and the layersof the decision tree are decoupled one in which the decision treecalculations are performed exactly and the time step of the markovchain are decoupled and one in 
the bayesian analysis of neural network is difficult because a simpleprior over weight implies a complex prior distribution overfunctions in this paper we investigate the use of gaussian processpriors over function which permit the predictive bayesian analysisfor fixed value of hyperparameters to be carried out exactlyusing matrix operation two method using optimization and averaging via hybrid monte carlo over hyperparameters have beentested on a number of challenging 
we introduce a novel view based object representation called the saliency map graph smg which capture the salient region of an object view at multiple scale using a wavelet transform this compact representation is highly invariant to translation rotation image and depth and scaling and offer the locality of representation required for occluded object recognition to compare two saliency map graph we introduce two graph similarity algorithm the first computes the topological similarity between two smg s providing a coarse level matching of two graph the second computes the geometrical similarity betweentwo smg s providing a fine level matching of two graph we test and compare these two algorithm on a large database of model object view 
a framework for object segmentation in vector valued image is presented in this paper the first scheme proposed is based on geometric active contour moving towards the object to be detected in the vector valued image object boundary are obtained a geodesic or minimal weighted distance curve in a riemannian space the metric in this space is given by a definition of edge in vector valued image the curve flow corresponding to the proposed active contour hold formal existence uniqueness stability and correctness result the technique is applicable for example to color and texture image the scheme automatically handle change in the deforming curve topology we conclude the paper presenting an extension of the color active contour which lead to a possible image flow for vector valued image segmentation the algorithm is based on moving each one of the image level set according to the proposed color active contour this extension also show the relation of the color geodesic active contour with a number of partial differential equation based image processing algorithm a anisotropic diffusion and shock filter 
a framework for learning parameterized model of optical flow from image sequence is presented a class of motion is represented by a set of orthogonal basis flow field that are computed from a training set using principal component analysis many complex image motion can be represented by a linear combination of a small number of these basis flow the learned motion model may be used for optical flow estimation and for model based recognition for optical flow estimation we describe a robust multi resolution scheme for directly computing the parameter of the learned flow model from image derivative a example we consider learning motion discontinuity non rigid motion of human mouth and articulated human motion 
the objective of this work is to enlarge the class of camera motion for which epipolar geometry and image correspondence can be computed automatically this facilitates matching between quite disparate view wide baseline stereo two extension are made to the current small baseline algorithm first and most importantly a viewpoint invariant measure is developed for assessing the affinity of corner neighbourhood over image pair second algorithm are given for generating putative corner match between image pair using local homographies two novel infrastructure development are also described the automatic generation of local homographies and the combination of possibly conflicting set of match prior to ransac estimation the wide baseline matching algorithm is demonstrated on a number of image pair with varying relative motion and for different scene type all processing is automatic 
in a practical situation the rigid transformation relating different view is recovered with error in such a case the recovered depth of the scene contains error and consequently a distorted version of visual space is computed what then are meaningful shape representation that can be computed from the image the result presented in this paper state that if the rigid transformation between different view is estimated in a way that give rise to a minimum number of negative depth value then at the center of the image affine shape can be correctly computed this result is obtained by exploiting property of the distortion function developed in the distortion model turn out to be a very powerful tool in the analysis and design of d motion and shape estimation algorithm and a a byproduct of our analysis we present a computational explanation of psychophysical result demonstrating human visual space distortion from motion information 
we propose a variant of shape from shading which we call shape from image warping the idea is that the three dimensional shape of an object is estimated by determining how much the image of the object is warped with respect to the image of a known prototype shape we demonstrate that for a class of reflectance function there is a direct relationship between these image warp and geometric warp of the underlying three dimensional shape therefore detecting the image warp relative to a prototype of known shape allows u to reconstruct the shape of the imaged object we derive property of these shape warp and illustrate the result by recovering the shape of face this relationship between image and shape warp help u understand the relationship between image based model of object recognition and approach based on three dimensional object model 
when triangulating a belief network we aim to obtain a junction tree of minimum state space searching for the optimal triangulation can be cast a a search over all the permutation of the network s vaeriables our approach is to embed the discrete set of permutation in a convex continuous domain d by suitably extending the cost function over d and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost in this paper we introduce an upper bound to the total junction tree weight a the cost function the appropriatedness of this choice is discussed and explored by simulation then we present two way of embedding the new objective function into continuous domain and show that they perform well compared to the best known heuristic 
framenet is a three year nsf supported project in corpus based computational lexicography now in it second year nsf iri tool for lexicon building the project s key feature are a a commitment to corpus evidence for semantic and syntactic generalization and b the representation of the valence of it target word mostly noun adjective and verb in which the semantic portion make use of frame semantics the resulting database will contain a description of the semantic frame underlying the meaning of the word described and b the valence representation semantic and syntactic of several thousand word and phrase each accompanied by c a representative collection of annotated corpus attestation which jointly exemplify the observed linkings between frame element and their syntactic realization e g grammatical function phrase type and other syntactic trait this report will present the project s goal and workflow and information about the computational tool that have been adapted or created in house for this work 
image differencing is used for many application involving change detection although it is usually followed by a thresholding operation to isolate region of change there are few method available in the literature specific to and appropriate for change detection we describe four different method for selecting threshold that work on very different principle either the noise or the signal is modelled and the model cover either the spatial or intensity distribution characteristic the method are a normal model is used for the noise intensity distribution signal intensity are tested by making local intensity distribution comparison in the two image frame i e the difference map is not used the spatial property of the noise are modelled by a poisson distribution and the spatial property of the signal are modelled a a stable number of region or stable euler number 
in this paper we examine a method for featuresubset selection based on informationtheory initially a framework for definingthe theoretically optimal but computationallyintractable method for feature subset selectionis presented we show that our goalshould be to eliminate a feature if it givesus little or no additional information beyondthat subsumed by the remaining feature inparticular this will be the case for both irrelevantand redundant feature we thengive an efficient 
recent empirical research ha shown conclusive advantage of multimodal interaction over speech only interaction for map based task this paper describes a multimodal language processing architecture which support interface allowing simultaneous input from speech and gesture recognition integration of spoken and gestural input is driven by unification of typed feature structure representing the semantic contribution of the different mode this integration method allows the component modality to mutually compensate for each others error it is implemented in quick set a multimodal pen voice system that enables user to set up and control distributed interactive simulation 
in order to realize their full potential multimodal system need to support not just input from multiple mode but also synchronized integration of mode johnston et al model this integration using a unification operation over typed feature structure this is an effective solution for a broad class of system but limit multimodal utterance to combination of a single spoken phrase with a single gesture we show how the unification based approach can be scaled up to provide a full multimodal grammar formalism in conjunction with a multidimensional chart parser this approach support integration of multiple element distributed across the spatial temporal and acoustic dimension of multimodal interaction integration strategy are stated in a high level unification based rule formalism supporting rapid prototyping and iterative development of multimodal system 
this paper discus the unsupervised learningproblem an important part of the unsupervisedlearning problem is determining thenumber of constituent group component orclasses which best describes some data weapply the minimum message length mml criterion to the unsupervised learning problem modifying an earlier such mml application we give an empirical comparison ofcriteria prominent in the literature for estimatingthe number of component in a dataset we conclude that the 
image based virtual reality is emerging a a major alternative to the more traditional d based vr the main advantage of the image based vr are it photo quality realism and d illusion without any d information unfortunately creating content for image based vr is usually a very tedious process this paper proposes to use a non perspective fisheye lens to capture the spherical panorama with very few image unlike most of camera calibration in computer vision self calibration of the fisheye lens pose new question regarding the parameterization of the distortion and wrap around effect because of it unique projection model and large field of view near degree most of the ambiguity problem in self calibrating a traditional lens can be solved trivially we demonstrate that with four fisheye lens image we can seamlessly register them to create the spherical panorama while self calibrating it distortion and field of view 
we introduce a an optimization measure for ranking problem such a information retrieval ir is an intuitive differentiable measure which is somewhat correlated with standard ir measure of performance a set of experiment is described in which the weight used to combine the result of two ir system is chosen by optimizing this technique work about a well a optimizing exact precision a traditional ir performance measure these result suggest the use of for training more complex neural net model for solving ranking problem 
the goal of robot learning from demonstrationis to have a robot learn from watching ademonstration of the task to be performed in our approach to learning from demonstrationthe robot learns a reward functionfrom the demonstration and a task modelfrom repeated attempt to perform the task a policy is computed based on the learnedreward function and task model lessonslearned from an implementation on an anthropomorphicrobot arm using a pendulumswing up task include simply 
a system to retrieve image using a description of the image intensity surface is presented gaussian derivative filter at several scale are applied to the image and low order d differential invariant are computed the resulting multi scale representation is indexed for rapid retrieval query are designed by the user from an example image by selecting appropriate region the invariant vector corresponding to these region are matched with the database counter part both in feature and coordinate space this yield a match score per image image are sorted by the match score and displayed experiment conducted with over image of object embedded in arbitrary background are described it is observed that image similar in appearance and whose viewpoint is within small view variation of the query can be retrieved with an average precision of 
a relational instance based learning algorithm called ribl is motivated and developedin this paper we argue that instancebasedmethods offer solution to the oftenunsatisfactory behavior of current inductivelogic programming ilp approach in domainswith continuous attribute value andin domain with noisy attribute and or example three research issue that emergewhen a propositional instance based learneris adapted to a first order representation areidentified construction 
may be the preferred variable fordescribing the response of a spiking neuron key word neural coding retinal ganglion cell spike generator refractory period reproducibility poisson processthere ha been considerable speculation about the code used byspiking neuron to transmit information ferster and spruston sejnowski stevens and zador the spectrum ofproposed theory range from the quot rate code quot in which the firingrates of many neuron are averaged to 
we present a new type of snake in which the dimensionality of the shape is scaled appropriately for the resolution of the image in which the shape are embedded we define shape a an ordered list of control point and compute the principal component of the shape in a prior training set our energy function is based upon the mahalanobis distance of a given shape from the mean shape and on the mahalanobis distance of the image attribute from image attribute value extracted from the training set we show that the derivative of this energy function with respect to the modal weight is reduced a the image resolution is reduced and that the derivative of the energy scale with the variance associated with each mode we exploit this property to determine the subset of the mode which are relevant at a particular level of image resolution thereby reducing the dimensionality of the shape we implement a coarse to fine search procedure in the image and shape domain simultaneously and demonstrate this procedure on the identification of anatomic structure in computed tomography image 
we have investigated the possibility that rapid processing in the visual system could be achieved by using the order of firing in different neurones a a code rather than more conventional firing rate scheme using spikenet a neural net simulator based on integrate and fire neurones and in which neurones in the input layer function a analogto delay converter we have modeled the initial stage of visual processing initial result are extremely promising even with activity in retinal output cell limited to one spike per neuron per image effectively ruling out any form of rate coding sophisticated processing based on asynchronous activation wa nonetheless possible 
the paper address the concept of cognition starting from the role of a sensor basis in the de sign of robot the field of robotics force the discussion to be pragmatic which is considered to be advantageous in addition we introduce the notion of cognitive basis in order to discus the cognitive ability of an artificial creature a cognition cannot be fully separated from ac tion and acting we present finally the notion of motor schema basis this basis involves action and acting a integral part of cognition 
the literature on recursive estimation of structure and motion from monocular image sequence comprises a large number of different model and estimation technique we propose a framework that allows u to derive and compare all model by following the idea of dynamical system reduction the natural dynamic model derived by the rigidity constraint and the perspective projection is first reduced by explicitly decoupling structure depth from motion then implicit decoupling technique are explored which consist of imposing that some function of the unknown parameter is held constant by appropriately choosing such a function not only can we account for all model seen so far in the literature but we can also derive novel one casting all the different model in a common framework allows u to compare their geometric property on common experimental ground 
conditioning experiment probe the way that animal make prediction about reward and punishment and use those prediction to control their behavior one standard model of conditioning paradigm which involve many conditioned stimulus suggests that individual prediction should be added together various key result show that this model fails in some circumstance and motivate an alternative model in which there is attentional selection between different available stimulus the new model is a form of mixture of expert ha a close relationship with some other existing psychological suggestion and is statistically well founded 
in synchronous rewriting the production of two rewriting system are paired and applied synchronously in the derivation of a pair of string we present a new synchronous rewriting system and argue that it can handle certain phenomenon that are not covered by existing synchronous system we also prove some interesting formal computational property of our system 
we consider how tracking in stereo may be enhanced by coupling pair of active contour in different view via affine epipolar geometry and various subset of planar affine transformation a well a by implementing temporal constraint imposed by curve rigidity d curve tracking is achieved using a submanifold model where it is shown how the coupling mechanism can be decomposed to cater for fired and variable epipolar geometry in the case of tracking planar curve the canonical frame model is developed such that the various geometrical constraint needed in different situation may be efficiently selected the result show that coupled active contour add consistency and robustness to tracking in stereo 
we propose a method for segmenting gray value image by segmentation we mean a map from the set of pixel to a small set of level such that each connected component of the set of pixel with the same level form a relatively large and meaningful region the method find a set of level with associated gray value by first finding junction in the image and then seeking a minimum set of threshold value that preserve the junction then it find a segmentation map that map each pixel to the level with the closest gray value to the pixel data within a smoothness constraint for a convex smoothing penalty we show the global optimal solution for an energy function that fit the data can be obtained in a polynomial time by a novel use of the maximum flow algorithm our approach is in contrast to a view in computer vision where segmentation is driven by intensity gradient usually not yielding closed boundary 
we report in this paper the observation of that is the same critical fragment in different sentence from the same source almost always realize one and the same of it many possible tokenizations this observation is demonstrated very helpful in sentence tokenization practice and is argued to be with far reaching implication in natural language processing 
in this paper we propose a new approach to recovering epipolar geometry from a pair of uncalibrated image we first detect the feature point by minimizing a proposed cost function we match the feature point discard the outlier and recover the epipolar geometry in one step experiment on real image show that this approach is effective and fast 
document length normalization is an important aspect of term weight assignment in an automatic information retrieval system in this study we observe that a normalization scheme that retrieves document of all length with similar chance a their likelihood of relevance will outperform another scheme which retrieves document with chance very different from their likelihood of relevance we show that the retrieval probability for a particular normalization method deviate systematically from the relevance probability across different collection we present pivoted normalization a technique that can be used to reduce the gap between the relevance and the retrieval probability training pivoted normalization on one collection we can successfully use it on other new text collection yielding a robust collection independent normalization technique we use the idea of pivoting with the well known cosine normalization scheme we point out some shortcoming of the cosine normalization function and present two new normalization function pivoted unique normalization and pivoted byte size normalization 
many different metric exist for evaluating parsing result including viterbi crossing bracket rate zero crossing bracket rate and several others however most parsing algorithm including the viterbi algorithm attempt to optimize the same metric namely the probability of getting the correct labelled tree by choosing a parsing algorithm appropriate for the evaluation metric better performance can be achieved we present two new algorithm the labelled recall algorithm which maximizes the expected labelled recall rate and the bracketed recall algorithm which maximizes the bracketed recall rate experimental result are given showing that the two new algorithm have improved performance over the viterbi algorithm on many criterion especially the one that they optimize 
abstract traditional instance based learning methodsbase their prediction directly on training data that ha been stored in the memory thepredictions are based on weighting the contributionsof the individual stored instance bya distance function implementing a domaindependentsimilarity metric this basic approachsuffers from three drawback computationallyexpensive prediction when thedatabase grows large overfitting in the presenceof noisy data and sensitivity to the 
traditional account of quantifier scope employ qualitative constraint or rule to account for scoping preference this paper outline a feature based parsing algorithm for a grammar with multiple simultaneous level of representation one of which corresponds to a partial ordering among quantifier according to scope the optimal such ordering a well a the ranking of other ordering is determined in this grammar not by absolute constraint but by stochastic heuristic based on the degree of alignment among the representational level a prolog implementation is described and it accuracy is compared with that of other account 
a method for resolving the ellipsis that appear in japanese dialogue is proposed this method resolve not only the subject ellipsis but also those in object and other grammatical case in this approach a machine learning algorithm is used to select the attribute necessary for a resolution a decision tree is built and used a the actual ellipsis resolver the result of blind test have shown that the proposed method wa able to provide a resolution accuracy of for indirect object and for subject with a verb predicate by investigating the decision tree we found that topic dependent attribute are necessary to obtain high performance resolution and that indispensable attribute vary according to the grammatical case the problem of data size relative to decision tree training is also discussed 
we describe a practical algorithm for learningaxis parallel high dimensional box frommulti instance example the first solutionto this practical learning problem arising indrug design wa given by dietterich lathrop and lozano perez a theoretical analysiswas performed by auer long srinivasan and tan in this work we derive a competitivealgorithm from theoretical considerationswhich is completely different from theapproach taken by dietterich et al our algorithmuses for 
in this paper we address the problem of recognizing an object from a novel viewpoint given a single model view of that object a is common in model based recognition object and image are represented a set of feature point we present an efficient algorithm for determining whether two set of image point in the plane could be projection of a common object a three dimensional point set the method relies on the fact that two set of point in the plane are orthographic projection of the same three dimensional point set exactly when they have a common projection onto a line this is a form of the well known epipolar constraint used in stereopsis our algorithm can be used to recognize an object by comparing a stored two dimensional view of the object against an unknown view without requiring the correspondence between point in the view to be known a priori we provide some example illustrating the approach 
the error rate of decision tree and other classificationlearners can often be much reduced bybagging learning multiple model from bootstrapsamples of the database and combining them byuniform voting in this paper we empirically testtwo alternative explanation for this both basedon bayesian learning theory bagging worksbecause it is an approximation to the optimalprocedure of bayesian model averaging with anappropriate implicit prior bagging work becauseit 
this paper address the problem of determining an object s d location from a sequence of camera image recorded by a mobile robot the approach presented here allows people to train robot to recognize specific object by presenting it example of the object to be recognized a decision tree method is used to learn significant feature of the target object from individual camera image individual estimate are integrated over time using bayes rule into a probabilistic d model of the robot s environment experimental result illustrate that the method enables a mobile robot to robustly estimate the d location of object from multiple camera image 
we present a method to infer segmented and full volumetric description of object from intensity image we use three weakly calibrated image from closely spaced viewpoint a input deriving full volumetric description requires the development of robust inference rule the inference rule are based on local property of generalized cylinder gc we first detect group in each image based on proximity parallelism and symmetry the group in the three image are matched and their contour are labelled a true and limb edge we use the information about group and the label associated with their contour to recover visible surface and their surface ax to extract the complete volume in term of a gc we need to infer the gc axis it cross section and the scaling function the property of straight and curved axis generalized cylinder are used locally on the visible surface to obtain the gc axis the cross section is recovered if seen in the image else it is inferred using the visible surface and gc property we consider group with true edge limb edge or a combination of both the final description are volumetric and in term of part sometimes when not enough information is present to make volumetric inference the description remain at the surface level we demonstrate result on real image of moderately complex object with texture and shadow 
we study on line generalized linear regression with multidimensional output i e neural network with multiple output node but no hidden node we allow at the final layer transfer function such a the softmax function that need to consider the linear activation to all the output neuron the weight vector used to produce the linear activation are represented indirectly by maintaining separate parameter vector we get the weight vector by applying a particular parameterization function to the parameter vector updating the parameter vector upon seeing new example is done additively a in the usual gradient descent update however by using a nonlinear parameterization function between the parameter vector and the weight vector we can make the resulting update of the weight vector quite different from a true gradient descent update to analyse such update we define a notion of a matching loss function and apply it both to the transfer function and to the parameterization function the loss function that match the transfer function is used to measure the goodness of the prediction of the algorithm the loss function that match the parameterization function can be used both a a measure of divergence between model in motivating the update rule of the algorithm and a a measure of progress in analyzing it relative performance compared to an arbitrary fixed model a a result we have a unified treatment that generalizes earlier result for the gradient descent and exponentiated gradient algorithm to multidimensional output including multiclass logistic regression 
abstract we describe a face modeling system which estimate complete facial structure and texture from a real time video stream the system begin with a face tracking algorithm which detects and stabilizes live facial im age into a canonical d pose the resulting canonical texture is then processed by a statistical model to l ter imperfection and estimate unknown component such a missing pixel and underlying d structure this statistical model is a soft mixture of eigenfea ture selector which span the d deformation and texture change across a training set of laser scanned face an iterative algorithm is introduced for deter mining the dimensional partitioning of the eigenfea tures to maximize their generalization capability over a cross validation set of data the model s ability to lter and estimate absent facial component are then demonstrated over incomplete d data this ulti mately allows the model to span known and regress un known facial information from stabilized natural video sequence generated by a face tracking algorithm the resulting continuous and dynamic estimation of the model s parameter over a video sequence generates a compact temporal description of the d deformation and texture change of the face 
use of uncalibrated image ha found many application such a image synthesis however it is not easy to specify the desired position of the new image in projective or affine space this paper proposes to recover euclidean structure from uncalibrated image using domain knowledge such a distance and angle the knowledge we have is usually about an object category but not very precise for the particular object being considered the variation fuzziness is modeled a a gaussian variable six type of common knowledge are formulated once we have an euclidean description the task to specify the desired position in euclidean space becomes trivial the proposed technique is then applied to synthesis of new facial image a number of difficulty existing in image synthesis are identified and solved for example we propose to use edge point to deal with occlusion 
most of behavior recognition method proposed so far share the limitation of bottom up analysis and single object assumption the bottom up analysis can be confused by erroneous and missing image feature and the single object assumption prevents u from analyzing image sequence including multiple moving object this paper present a robust behavior recognition method free from these limitation our method is best characterized by top down image feature extraction by selective attention mechanism object discrimination by colored token propagation and integration of multi viewpoint image extensive experiment of human behavior recognition in real world environment demonstrate the soundness and robustness of our method 
hubel and wiesel proposed that complex cell in visual cortexare driven by a pool of simple cell with the same preferredorientation but different spatial phase however a wide variety ofexperimental result over the past two decade have challenged thepure hierarchical model primarily by demonstrating that manycomplex cell receive monosynaptic input from unoriented lgncells or do not depend on simple cell input we recently showed usinga detailed biophysical model that 
dynamic programming q learning and other discrete markov decisionprocess solver can be applied to continuous d dimensional state space byquantizing the state space into an array of box this is often problematicabove two dimension a coarse quantization can lead to poor policy andfine quantization is too expensive possible solution are variable resolutiondiscretization or function approximation by neural net a third option which ha been little studied in the reinforcement 
finding the quot right quot number of cluster k for a dataset is a difficult and often ill posed problem ina probabilistic clustering context likelihood ratio penalized likelihood and bayesian technique areamong the more popular technique in this papera new cross validated likelihood criterion is investigatedfor determining cluster structure a practicalclustering algorithm based on monte carlo crossvalidation mccv is introduced the algorithm permitsthe data analyst to judge if 
we present here a qualitative temporal reasoning system that take both point and duration a primitive object and allows relative and indefinite information we formaly define a point duration network a a structure formed by two point algebra pa network separately but not independently since ternary constraint are introduced for relating point and duration information we adapt some of the concept and reasoning technique developed for the point algebra network such a consistency and minimality we prove that the problem of determining consistency in a point duration network is np complete a simpler and polynomial time decision problem is introduced for a restricted kind of point duration network finally we suggest how to determine consistency and find minimal point duration network in the general case 
this paper present the lilfes system an efficient feature structure description language for hpsg the core engine of lilfes is an abstract machine for attribute value logic proposed by carpenter and qu basic design policy the current status and performance evaluation of the lilfes system are described the paper discus two implementation of the lilfes the first one is based on an emulator of the abstract machine while the second one us a native code compiler and therefore is much more efficient than the first one 
decision tree are an important data mining tool with many application like manyclassification technique decision tree process the entire data base in order to producea generalization of the data that can be used subsequently for classification large complex data base are not always amenable to such a global approach to generalization this paper explores several method for extracting data that is local to a query point and then using the local data to build generalization these 
abstract an improved technique for d head tracking under varying illumination condition is proposed the head is modeled a a texture mapped cylinder tracking is formulated a an image registration problem in the cylinder s texture map image to solve the registration problem in the presence of lighting variation and head motion the residual error of registration is modeled a a linear combination of texture warping template and orthogonal illumination template fast and stable on line tracking is then achieved via regularized weighted least square minimization of the registration error the regularization term tends to limit potential ambiguity that arise in the warping and illumination template it enables stable tracking over extended sequence tracking doe not require a precise initial fit of the model the system is initialized automatically using a simple d face detector the only assumption is that the target is facing the camera in the first frame of the sequence the warping template are computed at the first frame of the sequence illumination template are precomputed off line over a training set of face image collected under varying lighting condition experiment in tracking are reported 
we propose a framework for extracting structure from stereo which represents the scene a a collection of approximately planar layer each layer consists of an explicit d plane equation a colored image with per pixel opacity a sprite and a per pixel depth offset relative to the plane initial estimate of the layer are recovered using technique taken from parametric motion estimation these initial estimate are then refined using a re synthesis algorithm which take into account both occlusion and mixed pixel reasoning about such effect allows the recovery of depth and color information with high accuracy even in partially occluded region another important benefit of our framework is that the output consists of a collection of approximately planar region a representation which is far more appropriate than a dense depth map for many application such a rendering and video parsing 
the ai literature contains many definition of diagnostic reasoning most of which are defined in term of the logical entailment relation we use existing work on approximate entailment to define notion of approximation in diagnosis we show how such a notion of approximate diagnosis can be exploited in various diagnostic strategy we illustrate these strategy by performing diagnosis in a small car domain example motivation 
in this paper we present a method for d reconstruction of human body with application in cad system for garment design the reconstruction scheme us image information from several arbitrary view and deformable superquadrics a the model of the body part two visual cue are used occluding contour and stereo possibly aided by projected pattern our preliminary experiment show that the reconstruction is more complete than in purely stereo or structured light based method and more precise than the reconstruction from occluding contour only from the reconstructed human body the body measurement can be taken automatically and used in garment design we give an example of draping of virtual garment over the photo realistic d model of the imaged human one can easily envision the use of the described algorithm in the development of custom fit garment retail software over the internet which would include the possibility of trying the garment on in virtual reality 
communication involves more than simply spoken information typical interaction use gesture to accurately and efficiently convey idea that are more easily expressed with action than word a more intuitive interface with machine should involve not only speech recognition but gesture recognition a well one of the most frequently used and expressively powerful gesture is pointing it is far easier and more accurate to point to an object than give a verbal description of it location to produce a more efficient accurate and natural human machine interface we use the perseus architecture to interpret the pointing gesture perseus us a variety of technique to reliably solve this complex visual problem in non engineered world knowledge about the task and environment is used at all stage of processing to best interpret the scene for the current situation once the visual operator are chosen contextual knowledge is used to tune them for maximal performance redundant interpretation of the scene provides robustness to error in interpretation fusion of independent type of information result in increased tolerance when assumption about the environment fail window of attention are used to improve speed and remove distraction from the scene furthermore reuse is a major issue in the design of perseus information about the environment and task is explicitly represented so it can easily be re used in task other than pointing a clean interface to perseus is provided for symbolic higher level system like the rap reactive execution system in this paper we describe perseus in detail and show how it is used to locate object pointed to by people 
almost all work on texture in the computer visionand graphic community ha modeled the texture astangential i e lying in the tangent plane to the surface this is equivalent to thinking of the texture asa pattern painted on the surface three dimensionaltextures where the element may point out of the surface have largely been ignored we study a specialclass of d texture perpendicular texture where wecan model the element a being normal to the surface the perspective 
finding articulated object like people in picture present a particularlydifficult object recognition problem we show how tofind people by finding putative body segment and then constructingassemblies of those segment that are consistent with the constraintson the appearance of a person that result from kinematicproperties since a reasonable model of a person requires at leastnine segment it is not possible to present every group to a classifier instead the search can be 
in the dynamical system approach to robot path planning both sensed and remembered information contribute to shape a nonlinear vector field that governs the behavior of an autonomous agent such system perform well with partial knowledge of the environment and in dynamically changing environment nevertheless it is a local heuristic approach to path planning and it is not guaranteed to find existing path we describe a method of adjusting the spatial resolution of the planner using a dynamical system that operates at a faster time scale than the planning dynamic this improves the system s ability to utilize both sensed and remembered information and to solve a larger range of problem without resorting to global path planning 
we study the spatial data mining problem of how to extract a special type of proximity relationship namely that of distinguishing two cluster of point based on the type of their neighbouring feature the point in the cluster may represent house on a map and the feature may represent spatial entity such a school park golf course etc class of feature are organized into concept hierarchy we develop algorithm gendis which us concept generalization to identify the distinguishing feature or concept which serve a discriminator furthermore we study the issue of which discriminator axe better than others by introducing the notion of maximal discriminator and by using a ranking system to quantitatively weigh maximal discriminator from different concept hierarchy 
we introduce in this paper two probabilistic reasoning model ppm and prm which combine the principal component analysis pca technique and the bayes classifier and show their feasibility on the face recognition problem the conditional probability density function for each class is modeled using the within class scatter and the maximum a posteriori map classification rule is implemented in the reduced pca subspace experiment carried out using facial image corresponding to subject with subject having duplicate image from the feret database show that the prm approach compare favorably against the two well known method for face recognition the eigenfaces and fisherfaces 
abstract we seek the scene interpretation that best explains image data for example we may want to infer the projected velocity scene which best explain two consecutive image frame image from synthetic data we model the relationship between image and scene patch and between a scene patch and neighboring scene patch given a new image we propagate likelihood in a markov network ignoring the e ect of loop to infer the underlying scene this yield an e cient method to form low level scene interpretation we demonstrate the technique for motion analysis and estimating high resolution image from low resolution one 
we propose a computational scheme for uncalibrated reconstruction of scene structureup to a relief transformation from binocular disparity this scheme which wecall regional disparity correction rdc is motivated both by computational considerationsand by psychophysical observation regarding human stereoscopic depth perception we describe an implementation of rdc and demonstrate it performanceexperimentally a an example of application of rdc we show how it can be used toalign 
this paper describes robotag an advanced prototype for a machine learningbased multilingual information extraction system first we describe a general client server architecture used in learning from observation then we give a detailed description of our novel decision tree tagging approach robotag performance for the proper noun tagging task in english and japanese is compared against humantagged key and to the best hand coded pattern performance a reported in the muc and met evaluation result related work and future direction are presented 
we firstly present a variational approach such that during image restoration edge detected in the original image are being preserved and then we compare in a second part the mathematical foundation of this method with respect to some of the well known method recently proposed in the literature within the class of pde based algorithm anisotropic diffusion mean curvature motion min max flow technique the performance of our approach is carefully examined and compared to the classical method experimental result on synthetic and real image will illustrate the capability of all the studied approach 
we propose a unified framework in which to treat semantic underspecification and parallelism phenomenon in discourse the framework employ a constraint language that can express equality and subtree relation between finite tree in addition our constraint language can express the equality up to relation over tree which capture parallelism between them the constraint are solved by context unification we demonstrate the use of our framework at the example of quantifier scope ellipsis and their interaction 
this paper deal with the reference choice involved in the generation of argumentative text since a natual segmentation of discourse into attentional space is needed to carry out this task this paper first proposes an architecture for natural language generation that combine hierarchical planning and focus guided navigation a work in it own right while hierarchical planning span out an attentional hierarchy of the discourse produced local navigation fill detail into the primitive discourse space the usefulness of this architecure actually go beyond the particular domain of application for which it is developed a piece of argumentative text such a the proof of a mathematical theorem conveys a sequence of derivation for each step of derivation the premise derived in the previous context and the inference method such a the application of a particular theorem or definition must be made clear although not restricted to nominal phrase our reference decision are similar to those concerning nominal subsequent referring expression based on the work of reichmann this paper present a discourse theory that handle reference choice by taking into account both textual distance a well a the attentional hierarchy 
we address the problem of egomotion estimation of a monocular observer moving with arbitrary translation and rotation in an unknown environment using log polar image the method we propose is uniquely based on the spatio temporal image derivative or the normal flow thus we avoid computing the complete optical flow field which is an ill posed problem due to the aperture problem we use a search paradigm based on geometric property of the normal flow field and consider a family of search subspace to estimate the egomotion parameter these algorithm are particularly well suited for the log polar image geometry a we use a selection of special normal flow vector with simple representation in logpolar coordinate this approach highlight the close coupling between algorithmic aspect and the sensor geometry retina physiology often found in nature finally we present and discus a set of experiment for various kind of camera motion which show encouraging result 
in this paper we present a shape recovery technique in d and d with specific application in visualizing and measuring anatomical shape from medical image this algorithm model extremely corrugated structure like the brain is topologically adaptable is robust and run in o n logn time where n is the total number of point in the domain our two stage technique is based on the level set shape recovery scheme introduced in and the fast march ing method in for computing solution to static hamilton jacobi equation 
learning accuracy depends on concept variation the accuracy of six learning system c grove greedy fringe lfc andmrp is compared using a set of forty testconcepts the selection of these concept wasguided by the existence of structured conceptsthat appear in difficult real world domain such a protein folding such conceptsoften have embedded implicit structure which may be revealed through explicitrelations experiment using these benchmarkconcepts show that 
knowledge discovery in database kdd focus on the computerized exploration of large amount of data and on the discovery of interesting pattern within them while most work on kdd ha been concerned with structured database there ha been little work on handling the huge amount of information that is available only in unstructured document collection this paper describes a new method a for cotiiptirmg co occuttetice rreyuencm tnr various keywords labeling the document this method is based on computing maximal association rule regular association are based on the notion offrequent set set of attribute which appear in many record in analogy maximal association are based on the notion of frequent maximal set conceptually a frequent maximal set is a set of attribute which appear alone or maximally in many record for the definition of maximality we use an underlying taxonomy t of the attribute this allows u to obtain the interesting correlation between attribute from different category frequent maximal set are useful for cg l jz rl a a a l aa rra tr wl gjllllpallly lllluurl abci llall i lu gb ullil iiili u ie cx attribute we provide an experimental evaluation of our methodology on the reuters document collection 
the paper present an approach to data mining involving search for complete or nearly complete domain classification in term of attribute value our objective is to find classification based on interacting attribute that provide a good characterization of the concept of interest by maximizing predefined quality criterion the paper introduces the notion of the classification complexity and several other measure to evalu a abe yuauby 
we present an algorithm for fast stochastic gradient descent thatuses a nonlinear adaptive momentum scheme to optimize the latetime convergence rate the algorithm make effective use of curvatureinformation requires only o n storage and computation and delivers convergence rate close to the theoretical optimum we demonstrate the technique on linear and large nonlinear backpropnetworks improving stochastic searchlearning algorithm that perform gradient descent on a cost 
many popular learning rule are formulated in term of continu ous analog input and output biological system however use action potential which are digital amplitude event that encode analog information in the inter event interval action potential representation are now being used to advantage in neuromorphic vlsi system a well we report on a simple learning rule based on the riccati equation described by kohonen modi ed for action potential neuronal output we demonstrate this learning rule in an analog vlsi chip that us volatile capacitive storage for synaptic weight we show that our time dependent learning rule is su cient to achieve approximate weight normalization and can detect temporal correlation in spike train 
we present a theory and practical computation for automatically matching a police artist sketch to a set of true photograph we locate facial feature in both the sketch a well a the set of photograph image then the sketch is photometrically standardized to facilitate comparison with a photo and then both the sketch and the photo are geometrically standardized finally for matching eigenanalysis is employed result using real police sketch and arrest photo are presented 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
in this paper we describe two different learningtasks for relational structure whenlearning a classifier for structure the relationalstructures in the training set are classifiedas a whole contrarily when learninga context dependent classifier for elementaryobjects the elementary object of the relationalstructures in the training set are classified in general the class of an elementaryobject will not only depend on it elementaryproperties but also on it 
to create a more realistic soccer game derived from tv image we developed an image synthesis system that generates an image sequence from the viewpoint of a player on the field this system is based on the camera calibration theory the system first determines the camera parameter of a tv image by using the intersection point of the white line drawn on the soccer field it then extract player from each image and estimate their position in the world coordinate system finally it applies a running motion to the player in their respective position and generates computer graphic animation from the viewpoint of any player selected by a user the system wa tested over seven sequence of tv image and demonstrated satisfactory result 
system that attempt to recover the spoken word from image sequence usually require complicated model of the mouth and it motion here we describe a new approach based on a fast mathematical morphology transform called the sieve we form statistic of scale measurement in one and two dimension and these are used a a feature vector for standard hidden markov model hmms 
we describe mosaicing for a sequence of image acquired by a camera rotating about it centre the novel contribution are in two area first in the automation and estimation of image registration image are registered under a full degree of freedom homography the registration is automatic and robust and a maximum likelihood estimator is used in panicular the registration is consistent so that there are no accumulated error over a sequence this mean that it is not a problem if the sequence loop back on itself the second novel area is in enhanced resolution a region of the mosaic can be viewed at a resolution higher than any of the original frame it is shown that the degree of resolution enhancement is determined by a measure based on a matrix norm a maximum likelihood solution is given which also take account of the error in the estimated homographies an improved map estimator is also developed result of both mle and map estimation are included for sequence acquired by a camcorder and a ccd camera 
our main contribution in this paper is to describe a new class of the adaptive mesh the mesh us both split and merge operation to adapt itself to the structure of volumetric data point the adaptive behavior is controlled by the variance of the data point position about maximum likelihood quadric patch we show that the density of control point on the mesh is regulated by the curvature of the underlying surface finally we illustrate the effectiveness of the method on both real world and simulated data set 
we propose a model for early visual processing in primate themodel consists of a population of linear spatial filter which interactthrough non linear excitatory and inhibitory pooling statisticalestimation theory is then used to derive human psychophysicalthresholds from the response of the entire population of unit themodel is able to reproduce human threshold for contrast and orientationdiscrimination task and to predict contrast threshold inthe presence of mask of varying 
this paper present an algorithm for discovering conjunction rule with high reliability from data set the discovery of conjunction rule each of which is a restricted form of a production rule is well motivated by various useflll application such a semantic query optimization and automatic development of a knowledge base in a discovery algorithm a production rule is evaluated according to it generality and accuracy since these are widely accepted a criterion in learning from example here reliability evaluation for these criterion is mandatory in distinguishing reliable rule from unreliable pattern without annoying the user however previous discovery approach have either ignored reliability evaluation or have only evaluated the reliability of generality and consequently tend to discover a huge number of rule in order to circumvent these difficulty we propose an approach based on a simultaneous estimation our approach discovers the rule that exceed pre specified threshold for generality and accuracy with high reliability a novel pruning method is employed for improving time efficiency without changing the discovery outcome the proposed approach ha been validated experimentally using benchmark data set from the uci repository 
feature selection ha proven to be a valuable technique in supervised learning for improving predictive accuracy while reducing the number of attribute considered in a task we investigate the potential for similar benefit in an unsupervised learning task conceptual clustering the issue raised in feature selection by the absence of class label are discussed and an implementation of a sequential feature selection algorithm based on an existing conceptual clustering system is described additionally we present a second implementation which employ a technique for improving the efficiency of the search for an optimal description and compare the performance of both algorithm 
learning how to adjust to an opponent s position is critical tothe success of having intelligent agent collaborating towards theachievement of specific task in unfriendly environment this paperdescribes our work on a memory based technique for to choosean action based on a continuous valued state attribute indicatingthe position of an opponent we investigate the question of how anagent performs in nondeterministic variation of the training situation our experiment indicate that 
an ever increasing number of registered trademark ha created greater demand for an automatic trademark retrieval system we present a method for such a system based on the image content using shape feature zernike or pseudo zernike moment of the image are employed a a feature set to retrieve similar shape we take into account visually salient feature that dominantly affect the global shape of the trademark and ignore their minor detail experimental result on a database of trademark image demonstrate that the proposed method retrieves visually similar trademark which agree well with human perception 
most view based vision algorithm are based on strong assumption about the disposition of the object in the image to safely apply those algorithm in real world image sequence we propose that a vision system should be divided into two component the first component contains an approximate world model of the scene a low accuracy coarse description of the object and action in the world approximate world model are constructed and updated by simple vision routine and by the use of action information provided by an external source the second component employ view based algorithm to perform required perceptual task the selection and control of the view based method are determined by the information provided by the approximate world model we demonstrate the approximate world model approach in a project to control camera in a tv studio where the external context is provided by a script 
in this paper we present a car tracking system which provides quantitative and qualitative motion estimate of the tracked car simultaneously from a moving observer first we construct three motion model constant velocity constant acceleration and turning to describe the qualitative motion of a moving car then the model are incorporated into the extended kalman filter to perform quantitative tracking finally we develop an extended interacting multiple model eimm algorithm to manage the switching between model and to output both qualitative and quantitative motion estimate of the tracked car accurate motion modeling and efficient model management result in a high performance tracking system the experimental result on simulated and real data demonstrate that our tracking system is reliable and robust and run in real time the multiple motion representation make the system useful in various autonomous driving task 
similarity is an important and widely used concept previous definition of similarity are tied to a particular application or a form of knowledge representation we present an informationtheoretic definition of similarity that is applicable a long a there is a probabilistic model we demonstrate how our definition can be used to measure the similarity in a number of different domain 
user modeling is employed by application that need to maintain explicit model of their user in order to exhibit individua lized behaviour the user modeling task involves representation and acquisition of assumption about the user particularly user model acquisition is closely related to the machine learning task of automatical ly acquiring new information a well a new representation of existing information this paper show how and for which purpose machine learning technique have been and could be employed in user modeling also usage modeling a more action centered approach to user modeling is considered finally the labour approach to user modeling is sketched which regard user modeling a learning problem 
providing a machine with the ability to learn and use model of natural interaction is a challenging and largely unaddressed problem a framework is developed enabling both the acquisition of interaction behaviour from the observation of human and the use of the acquired behaviour model to simulate a plausible partner during interaction statistically based interaction behaviour model are acquired automatically from the observation of interacting human interaction with a virtual human is achieved using the model together with a stochastic tracking algorithm experimental result demonstrate the generation and use of the model for a simple human interaction 
in this work we investigate the visual appearance of real world surface and the dependence of appearance on imaging condition we present a brdf bidirectional reflectance distribution function database with reflectance measurement for over different sample each observed with over different combination of viewing and source direction we fit the brdf measurement to two recent model to obtain a brdf parameter database these brdf parameter can be directly used for both image analysis and image synthesis finally we present a btf bidirectional texture function database with image texture from over different sample each observed with over different combination of viewing and source direction each of these unique database ha important implication for a variety of vision algorithm and each is made publicly available 
we have constructed an inexpensive video based motorized trackingsystem that learns to track a head it us real time graphicaluser input or an auxiliary infrared detector a supervisory signalsto train a convolutional neural network the input to the neuralnetwork consist of normalized luminance and chrominance imagesand motion information from frame difference subsampled imagesare also used to provide scale invariance during the onlinetraining phase the neural network rapidly 
most of the reinforcement learning rl algorithmsassume that the learning processesof embedded agent can be formulated asmarkov decision process mdps however the assumption is not valid for many realisticproblems therefore research on rltechniques for non markovian environmentsis gaining more attention recently we havedeveloped a bayesian approach to rl in nonmarkovianenvironments in which the environmentis modeled a a history tree model a stochastic model with 
gain control by divisive inhibition a k a divisive normalization ha been proposed to be a general mechanism throughout the visualcortex we explore in this study the statistical propertiesof this normalization in the presence of noise using simulation we show that divisive normalization is a close approximation to amaximum likelihood estimator which in the context of populationcoding is the same a an ideal observer we also demonstrate analyticallythat this is a general 
we present a surface texture and microstructure extraction system to provide added realism in visualization and virtual reality application the system us multiple image d model and camera information in addition to knowledge about man made structure to cope with problem such a perspective distortion data deficiency and corruption caused by shadow and occlusion combined with the ascender site modeling system cite collins iuw and scene rendering algorithm the system is typically useful for urban site model refinement and visualization 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
a new generation of reactive model based executive are emerging that make extensive use of componentbased declarative model to analyze anomalous situation and generate novel sequence for the internal control of complex autonomous system burton a generative model based planner offer a core element that bridge the gap between current and target state within the reactive loop burton is a sound complete reactive planner that generates a single control action of a valid plan in average case constant time and compensates for anomaly at every step burton will not generate irreversible potentially damaging sequence except to effect repair we present model compilation causal analysis and online policy construction method that are key to burton s performance 
we identify and validate from a large corpus constraint from conjunction on the positive or negative semantic orientation of the conjoined adjective a log linear regression model us these constraint to predict whether conjoined adjective are of same or different orientation achieving accuracy in this task when each conjunction is considered independently combining the constraint across many adjective a clustering algorithm separate the adjective into group of different orientation and finally adjective are labeled positive or negative evaluation on real data and simulation experiment indicate high level of performance classification precision is more than for adjective that occur in a modest number of conjunction in the corpus 
the information used for the extraction of term can be considered a rather internal i e coming from the candidate string itself this paper present the incorporation of external information derived from the context of the candidate string it is embedded to the c value approach for automatic term recognition atr in the form of weight constructed from statistical characteristic of the context word of the candidate string 
both vertebrate and invertebrate retina are highly ecient in extracting contrast independent of the background intensity over ve or more decade this eciency ha been rendered possible by the adaptation of the dc operating point to the background intensity while maintaining high gain transient response the centersurround property of the retina allows the system to extract information at the edge in the image this silicon retina model the adaptation property of the receptor and the antagonistic centersurround property of the laminar cell of the invertebrate retina and the outer plexiform layer of the vertebrate retina we also illustrate the spatio temporal response of the silicon retina on moving bar the chip ha x pixel on a x mm die and it is fabricated in m n well technology 
abstract we present a methodology for tightly coupling data mining application to database system to build high performance application without requiring any change to the database software 
we demonstrate real time face tracking and pose estimation in an unconstrained office environment with an active foveated camera using vision routine previously implemented for an interactive environment we determine the spatial location of a user s head and guide an active camera to obtain foveated image of the face face are analyzed using a set of eigenspaces indexed over both pose and world location closed loop feedback from the estimated facial location is used to guide the camera when a face is present in the foveated view our system can detect the head pose of an unconstrained user in real time a he or she move about an open room 
singular value decomposition svd can be viewed a a method for unsupervised training of a network that associate two class of event reciprocally by linear connection through a single hidden layer svd wa used to learn and represent relation among very large number of word k k and very large number of natural text passage k k in which they occurred the result wa dimensional semantic space in which any trained or newly added word or passage could be represented a a vector and similarity were measured by the cosine of the contained angle between vector good accuracy in simulating human judgment and behavior ha been demonstrated by performance on multiple choice vocabulary and domain knowledge test emulation of expert essay evaluation and in several other way example are also given of how the kind of knowledge extracted by this method can be applied 
minimum description length mdl estimation ha proven itself of major importance in a large number of application many of which are in the field of computer vision and pattern recognition a problem is encountered in applying the associated formula however especially those associated with model cost this is because most of these are asymptotic form appropriate only for large sample size j rissanen ha recently derived sharper code length formula valid for much smaller sample size because of the importance of these result it is our intent here to present a tutorial description of them in keeping with this goal we have chosen a simple application whose relative tractability allows it to be explored more deeply than most problem the segmentation of binary string based on a piecewise bernoulli assumption by that we mean that the string are assumed to be divided into substring the bit of which are assumed to have been generated by a single within a substring bernoulli source 
we present a corpus based system to expand multi wordindex term using a part of speech tagger and a full fledgedderivational morphological system combined with a shallowparser the system ha been applied to french the uniquecontribution of the research is in using these linguisticallybased tool with safety filter in order to avoid the problemsof degradation typically associated with derivational analysisand generation the successful expansion and thus conflationof term 
we derive and analyse robust optimization scheme for noisy vector quantization on the basis of deterministic annealing star ting from a cost function for central clustering that incorporates dis tortions from channel noise we develop a soft topographic vector quantization algorithm stvq which is based on the maximum entropy principle and which performs a maximum likelihood estimate in an expectationmaximization em fashion annealing in the temperature parameter lead to phase transition in the existing code vector repre sentation during the cooling process for which we calculate critical temp eratures and mode a a function of eigenvectors and eigenvalue of the covariance matrix of the data and the transition matrix of the channel no ise a whole family of vector quantization algorithm is derived from stvq among them a deterministic annealing scheme for kohonen s self o rganizing map som this algorithm which we call ssom is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel the algorithm s performance is compared to those o f lbg and stvq while it is naturally superior to lbg which doe not take into account channel noise it result compare very well to tho e of stvq which is computationally much more demanding 
this paper present an interactive modeling system that construct d model from a collection of panoramic image mosaic a panoramic mosaic consists of a set of image taken around the same viewpoint and a transformation matrix associated with each input image our system first recovers the camera pose for each mosaic from known line direction and point and then construct the d model using all available geometrical constraint we partition constraint into soft and hard linear constraint so that the modeling process can be formulated a a linearly constrained least square problem which can be solved efficiently using qr factorization the result of extracting wire frame and texture mapped d model from single and multiple panorama are presented 
this paper discus a fairly general adaptation algorithm whichaugments a standard neural network to increase it recognition accuracyfor a specific user the basis for the algorithm is that the outputof a neural network is characteristic of the input even when the outputis incorrect we exploit this characteristic output by using an outputadaptation module oam which map this output into the correctuser dependent confidence vector the oam is a simplified resourceallocating network 
progressive processing is a resource bounded reasoning technique that allows a system to incrementally construct a solution to a problem using a hierarchy of processing level this paper focus on the problem of meta level control of progressive processing in domain characterized by rapid change and high level of duration uncertainty we show that progressive processing facilitates efficient run time monitoring and meta level control our solution is based on an incremental scheduler that can handle duration uncertainty by dynamically revising the schedule during execution time based on run time information we also show that a probabilistic representation of duration uncertainty reduces the frequency of schedule revision and thus improves the performance of the system finally an experimental evaluation show the contribution of this approach and it suitability for a data transmission application 
this paper investigates the structure of projective translation rigid translation expressed a homographies in projective space a seven parameter representation is proposed which explicitly represents the geometric entity constraining and defining the translation a practical algebraic method for estimating these parameter is developed it provides affine calibration of a stereo rig determines the translation axis and allows projective translation to be composed the practical effectiveness of the calibration is evaluated on synthetic and real image data 
this paper show that if a large neural network is used for a patternclassification problem and the learning algorithm find a networkwith small weight that ha small squared error on the trainingpatterns then the generalization performance depends on the sizeof the weight rather than the number of weight more specifically consider an layer feed forward network of sigmoid unit inwhich the sum of the magnitude of the weight associated witheach unit is bounded by a the 
we present a connectionist method for representing image that explicitlyaddresses their hierarchical nature it blend data from neuroscienceabout whole object viewpoint sensitive cell in inferotemporalcortex and attentional basis field modulation in v withideas about hierarchical description based on microfeatures the resulting model make critical use of pathway for both analysisand synthesis we illustrate the model with a simple exampleof representing 
this paper introduces and evaluates a naturalextension of linear exponentiated gradientmethods that make them applicableto reinforcement learning problem just asthese method speed up supervised learning we find that they can also increase the efficiencyof reinforcement learning comparisonsare made with conventional reinforcementlearning method on two test problemsusing cmac function approximators and replacingtraces on a small prediction task exponentiated gradient 
detection of the set of center of maximal disk is a key step toward generation of accurate skeleton on the basis of distance map algorithm using approximate distance metric are abundant and their theory ha been well established however the resulting skeleton may be inaccurate and sensitive to rotation in this paper we study method for detecting maximal disk from distance map based on the exact euclidean metric we first show that no previous algorithm identifies the exact set of discrete maximal disk under euclidean distance metric we then propose new algorithm and show that they produce the exact set of maximal disk the effectiveness of our algorithm is demonstrated with numerous example 
an important and difficult prediction taskin many domain particularly medical decisionmaking is that of prognosis prognosispresents a unique set of problem to alearning system when some of the outputsare unknown this paper present a new approachto prognostic prediction using ideasfrom nonparametric statistic to fully utilizeall of the available information in a neural architecture the technique is applied to breastcancer prognosis resulting in flexible accuratemodels that 
we previously proposed a quantitative model of early visual processingin primate based on non linearly interacting visual filtersand statistically efficient decision we now use this model to interpretthe observed modulation of a range of human psychophysicalthresholds with and without focal visual attention our model calibrated by an automatic fitting procedure simultaneously reproducesthresholds for four classical pattern discrimination task performed while attention wa 
we have addressed the problem of tracking the nonrigid motion of the heart using a sequence of velocity field and a sequence of contour the information from both the contour and the dense velocity field is integrated into a deforming mesh that is placed over the myocardium at one time frame and then tracked over the entire cardiac cycle the deformation is guided by a smoothing filter that provides a compromise between i believing the dense field velocity and the contour data when it is crisp and coherent in a local spatial and temporal sense and ii employing a temporally smooth cyclic model of cardiac motion when contour and velocity data are not trustworthy the method ha been carefully evaluated with simulated data and phantom data experiment with in vivo data have also been conducted 
we address the problem of learning structure in nonlinear markov network with continuous variable this can be viewed a non gaussian multidi mensional density estimation exploiting certain conditional independency in the variable markov network are a graphical way of describing con ditional independency well suited to model relationship which do not ex hibit a natural causal ordering we use neural network structure to model the quantitative relationship between variable the main focus in this pa per will be on learning the structure for the purpose of gaining insight into the underlying process using two data set we show that interesting struc tures can be found using our approach inference will be brie y addressed 
a novel boundary detection scheme based on edgejow is proposed in this paper this scheme utilizes a predictive coding model to identify the direction of change in color and texture at each image location at a given scale and construct an edge flow vector by iteratively propagating the edge flow the boundary can be detected at image location which encounter two opposite direction offrow in the stable state a user defined image scale is the only significant control parameter that is needed by the algorithm the scheme facilitates integration of color and texture into a single framework for boundary detection 
an approach for learning and estimating temporal flow model from image sequence is proposed the temporal flow model are represented a a set of orthogonal temporal flow base that are learned using principal component analysis of instantaneous flow measurement spatial constraint on the temporal flow are also developed for modeling the motion of region in rigid and coordinated motion the performance of these model is demonstrated on several long image sequence of rigid and articulated body in motion 
in many real world task only a small fraction of the available input are important at any particular time this paper present a method for ascertaining the relevance of input by exploiting temporal coherence and predictability the method proposed in this paper dynamically allocates relevance to input by using expectation of their future value a a model of the task is learned the model is simultaneously extended to create task specific prediction of the future value of input input which are either not relevant and therefore not accounted for in the model or those which contain noise will not be predicted accurately these input can be de emphasized and in turn a new improved model of the task created the technique presented in this paper have yielded significant improvement for the vision based autonomous control of a land vehicle vision based hand tracking in cluttered scene and the detection of fault in the etching of semiconductor wafer 
this paper introduces primitive optimality theory otp a linguistically motivated formalization of ot otp specifies the class of autosegmental representation the universal generator gen and the two simple family of permissible constraint in contrast to le restricted theory using generalized alignment otp s optimal surface form can be generated with finite state method adapted from ellison unfortunately these method take time exponential on the size of the grammar indeed the generation problem is shown np complete in this sense however technique are discussed for making ellison s approach fast in the typical case including a simple trick that alone provides a fold speedup on a grammar fragment of moderate size one avenue for future improvement is a new finite state notion factored automaton where regular language are represented compactly via formal intersection ki ai of fsas 
this paper present the p lanmine sequence mining algorithm to extract pattern of event that predict failure in database of plan execution new technique were needed because previous data mining algorithm were overwhelmed by the staggering number of very frequent but entirely unpredictive pattern that exist in the plan database this paper combine several technique for pruning out unpredictive and redundant pattern which reduce the size of the returned rule set by more than three order of magnitude p lanmine ha also been fully integrated into two real world planning system we experimentally evaluate the rule discovered by planmine and show that they are extremely useful for understanding and improving plan a well a for building monitor that raise alarm before failure happen this paper describes planmine the data mining component of the above two application we show that one cannot simply apply previous sequence discovery algorithm srikant zaki for mining execution trace due to the complicated structure and redundancy in the data simple application of the known algorithm generates an enormous number of highly frequent but unpredictive rule we developed a three step pruning strategy for selecting only the most predictive rule first we eliminate normative rule that are consistent with background knowledge that corresponds to the normal operation of a successful plan second we eliminate those redundant pattern that have the same frequency a at least one of their proper subsequence finally we keep only dominatingsequences that are more predictive than all of their proper subsequence to experimentally validate our approach we show that improve doe not work well if the planmine component is replaced by le sophisticated method for choosing which part of the plan to repair we also show that the output of p lanmine can be used to build execution monitor which predict failure in a plan before they occur we were able to produce monitor with precision that signal of all the failure that occur a more detailed version of this paper appears in zaki 
the kernel parameter is one of the few tunable parameter in supportvector machine and control the complexity of the resulting hypothesis the choice of it value amount to model selection and is usually performedby mean of a validation set we present an algorithm which can automatically perform model selectionand learning with no additional computational cost and with no need of avalidation set theoretical result motivating this approach providing upperbounds on the 
we propose a novel method of extracting a moving object region from each frame in a series of image regardless of complex changing background using statistical knowledge about the target in vision system for real world like a human motion tracker a priori knowledge about the target and environment is often limited e g only the approximate size of the target is known and is insufficient for extracting the target motion directly in our approach information about both target object and environment is extracted with a small amount of given knowledge about the target object pixel value color intensity etc distribution for both the target object and background region are adaptively estimated from the input image sequence based on the knowledge then the probability of each pixel being associated with the target object is calculated the target motion can be extracted from the calculated stochastic image we confirmed the stability of this approach through experiment 
program execution speed on modern computer is sensitive by a factor of two or more to the order in which instruction are presented to the processor to realize potential execution efcienc y an optimizing compiler must employ a heuristic algorithm for instruction scheduling such algorithm are painstakingly hand crafted which is expensive and time consuming we show how to cast the instruction scheduling problem a a learning task obtaining the heuristic scheduling algorithm automatically our focus is the narrower problem of scheduling straight line code also called basic block of instruction our empirical result show that just a few feature are adequate for quite good performance at this task for a real modern processor and that any of several supervised learning method perform nearly optimally with respect to the feature used 
for large customer electricit de france the french national electric power company store every the amount of electric power they consume for each customer these measure lead to curve called electric load curve clustering of electric load curve is a key problem for understanding the behavior of these customer several method have been used but kohonen map give a very nice solution to this problem thanks to the visualization of the map the work we present here describes a software for interactive construction and interpretation of a kohonen map clustering in the case of curve the user can run the kohonen map clustering visualize the map see external characteristic of curve linked to each cell of the map find the cell figuring curve having some chosen external characteristic define class of cell add comment on cell user interaction is largely based on mouse clicking on the map cell and on bar of barcharts figuring external characteristic of the curve this software is not dedicated to electric load curve analysis but can be used on any type of curve for instance to analyze time series in finance 
we introduce arc lh a new algorithm for improvement of ann classifierperformance which measure the importance of pattern byaggregated network output error on several artificial benchmarkproblems this algorithm compare favorably with other resampleand combine technique introductionthe training of artificial neural network anns is usually a stochastic and unstableprocess a the weight of the network are initialized at random and trainingpatterns are presented in random 
the performance of active contour in tracking is highly dependent on the availability of an appropriate model of shape and motion to use a a predictor model can be hand built but it is far more effective and le time consuming to learn them from a training set technique to do this exist both for shape and for shape and motion jointly this paper extends the range of shape and motion model in two significant way the first is to model jointly the random variation in shape arising 
unsupervised learning algorithm based on convex and conic encoders are proposed the encoders find the closest convex or conic combination of basis vector to the input the learning algorithm produce basis vector that minimize the reconstruction error of the encoders the convex algorithm develops locally linear model of the input while the conic algorithm discovers feature both algorithm are used to model handwritten digit and compared with vector quantization and principal component 
this paper address the general d rigid motion problem where the point correspondence and the motion parameter between two set of d point are to be recovered the existence of missing point in the two set is the most difficult problem we first show a mathematical symmetry in the solution of rotation parameter and point correspondence a closed form solution based on the correlation matrix eigenstructure decomposition is proposed for correspondence recovery with no missing point using a heuristic measure of point pair affinity derived from the eigenstructure a weighted bipartite matching algorithm is developed to determine the correspondence in general case where missing point occur the use of the affinity heuristic also lead to a fast outlier removal algorithm which can be run iteratively to refine the correspondence recovery simulation result and experiment on real image are shown in both ideal and general case 
conventional binary classification tree such a cart either splitthe data using axis aligned hyperplanes or they perform a computationallyexpensive search in the continuous space of hyperplaneswith unrestricted orientation we show that the limitation of theformer can be overcome without resorting to the latter for everypair of training data point there is one hyperplane that is orthogonalto the line joining the data point and bisects this line suchhyperplanes are plausible 
generalized cylinder gc are a popular representational tool in computer vision in medical imaging the curved axis gc is particularly applicable to a number of elongated physical structure such a vasculature bone and bronchus in many of these instance it is necessary to recover curved axis gc with arbitrary cross section it is also vital that these structure once recovered can be analyzed and visualized with off the shelf algorithm and software package such tool are usually designed to operate on the domain of polynomial or rational surface unfortunately most extant suitably versatile gc representation do not admit rational parameterizations we develop an entirely rational b spline representation for generalized cylinder with curved ax and arbitrary cross section function we demonstrate how our representation can be used a a deformable model by extracting a rational gc from pre segmented spinal data using a discrete dynamic surface fit 
bayda is a software package for flexible data analysis in predictive data mining task the mathematical model underlying the program is based on a simple bayesian network the naive bayes classifier it is well known that the naive bayes classifier performs well in predictive data mining ta k when compared to approach using more complex model however the model make strong independence assumption that are frequently violated in practice for this reason t he bayda software also provides a feature selection scheme which can be used for analyzing the problem domain and for improving the prediction accuracy of the model constructed by bayda the scheme is based on a novel bayesian feature selection criterion introduced in this paper the suggeste d criterion is inspired by the cheeseman stutz approximation for computing the marginal likelihood of bayesian network with hidden variable the empirical result with several widel yused data set demonstrate that the automated bayesian feature selection scheme can dramatically decrease the number of relevant feature and lead to substantial improvement in prediction accuracy 
this paper investigates a brute force technique for mining classification rule from large data set we emplo y an association rule miner enha nced with new pruning strategy to control combinatorial explosion in th e number of candidate counted with each database pa s the approach effectively and efficiently extract h igh confidence classification rule that apply to most if not all of the data in several classification benchmark 
foveal vision feature imagers with graded acuity coupled with context sensitive sensor gaze control analogous to that prevalent throughout vertebrate vision foveal vision operates more efficiently than uniform acuity vision because resolution is treated a a dynamically allocatable resource but requires a more refined visual attention mechanism we demonstrate that reinforcement learning rl significantly improves the performance of foveal visual attention and of the overall vision system for the task of model based target recognition a simulated foveal vision system is shown to classify target with fewer fixation by learning strategy for the acquisition of visual information relevant to the task and learning how to generalize these strategy in ambiguous and unexpected scenario condition 
this paper considers a specific problem of visual perception of motion namely the problem of visual detection of independent d motion most of the existing technique for solving this problem rely on restrictive assumption about the environment the observer s motion or both moreover they are based on the computation of a dense optical flow field which amount to solving the ill posed correspondence problem in this work independent motion detection is formulated a a problem of robust parameter estimation applied to the visual input acquired by a rigidly moving observer the proposed method automatically selects a planar surface in the scene and the residual planar parallax normal flow field with respect to the motion of this surface is computed at two successive time instant the two resulting normal flow field are then combined in a linear model the parameter of this model are related to the parameter of self motion ego motion and their robust estimation lead to a segmentation of the scene based on d motion the method avoids a complete solution to the correspondence problem by selectively matching subset of image point and by employing normal flow field experimental result demonstratethe effectiveness of the proposed method in detecting independent motion in scene with large depth variation and unrestrited observer motion 
image are ambiguous at each of many level of a contextual hierarchy nevertheless the high level interpretation of most scenesis unambiguous a evidenced by the superior performance of human this observation argues for global vision model such a deformabletemplates unfortunately such model are computationallyintractable for unconstrained problem we propose a compositionalmodel in which primitive are recursively composed subjectto syntactic restriction to form 
many reinforcement learning algorithm likeq learning or r learning correspond toadaptative method for solving markoviandecision problem in infinite horizon whenno model is available in this article weconsider the particular framework of nonstationaryfinite horizon markov decisionprocesses after establishing a relationshipbetween the finite horizon total reward criterionand the average reward criterion infinite horizon we define qh learning andrh learning for finite horizon 
the number and the size of spatial database e g for geomarketing traffic control or environmental study are rapidly growing which result in an increasing need for spatial data mining in this paper we present new algorithm for spatial characterization and spatial trend analysis for spatial characterization it is important that class membership of a database object is not only determined by it non spatial attribute but also by the attribute of object in it neighborhood in spatial trend analysis pattern of change of some non spatial attribute in the neighborhood of a database object are determined we present several algorithm for these task these algorithm were implemented within a general framework for spatial data mining providing a small set of database primitive on top of a commercial spatial database management system a performance evaluation using a real geographic database demonstrates the effectiveness of the proposed algorithm furthermore we show how the algorithm can be combined to discover even more interesting spatial knowledge 
weak causal relationship and small sample size pose two significant difficulty to the automatic discovery of causal model from observational data this paper examines the influence of weak causal link and varying sample size on the discovery of causal model the experimental result illustrate the effect of larger sample size for discovering causal model reliably and the relevance of the strength of causal link and the complexity of the original causal model we present indicative evidence of the superior robustness of mml minimum message length method to standard significance test in the recovery of causal link the comparative result show that the mml ci the mml causal inducer causal discovery system find better model than tetrad ii given small sample from linear causal model the experimental result also reveal that mml ci find weak link with smaller sample size than can tetrad ii 
although powerful image representation have been proposed for content based image retrieval most of the current system are rigid i e they retrieve a fixed set of image a response to a given query and an image feature in this paper our goal is to introduce tool for making image retrieval system more flexible more precisely we use multiple image feature and present in detail a new relevance feedback technique that integrates the positive and negative example provided by the user experimental result on various large database show that the proposed technique is more performant than the standard relevance feedback approach 
a new object tracking algorithm based on affine structure ha been developed and it is shown that it performance is better than that of a kalman filter based correlation tracker the algorithm is fast reliable viewpoint invariant and insensitive to occlusion and or individual corner disappearance or reappearance detailed experimental analysis on a long real image sequence is also presented 
the comparison of two data set can reveal a great deal of information about the time varying nature of an observed process for example suppose that the point in a data set represent a customer s activity by their location in n dimensional space a comparison of the distribution of point in two such data se t can indicate how the customer activity ha changed between the observation period other application include data integrity checking an unexpected change in a data set can indicate a problem in the data collection process we propose a fast inexpensive method for comparing massive high dimensional data set that doe not make any distributional assumption the method adapts the power of classical statistic for use on complex high dimensional data set we generate a map of the data set a datasphere and compare data set by comparing their dataspheres the datasphere can be generated in two pass over the data set stored in a database and aggregated at multiple level we illustrate the use of our set comparison technique with an example analysis of data set drawn from atg t data warehouse 
this paper describes a fast and flexible method for extracting text region from a document page containing text graphic and picture such region can be given a an input to an ocr system the user fix two parameter the minimum width w of the text to be detected and the precision needed both expressed a a percentage of the image width according to the implementation need the method work by subdividing the page into overlapping column whose width and inter shift depend on w and and by performing text line extraction on each column separately successively a statistical analysis of the text line element found in each column is performed and they are connected to form complete text line finally related piece of text are merged into block so that a sensible reading order is provided for the ocr system the algorithm is very fast is able to work on low resolution document page and is robust against skew the algorithm a also very flexible no assumption are made on the layout of the document the shape of the text region and the font size and style the main assumption is that the background is uniform and the text approximately horizontal despite the statistical nature of the method a single line of text of a certain font size is generally sufficient to warrant detection experimental result are shown which demonstrate the effectiveness of the method on several different kind of document 
this paper describes a prototype disambiguation module kankei which us two corpus of the train project in ambiguous verb phrase of form v np pp or v np adverb s the two corpus have very different pp and adverb attachment pattern in the first the correct attachment is to the vp of the time while in the second the correct attachment is to the np of the time kankei us various n gram pattern of the phrase head around these ambiguity and assigns parse tree with these ambiguity a score based on a linear combination of the frequency with which these pattern appear with np and vp attachment in the train corpus unlike previous statistical disambiguation system this technique thus combine evidence from bigram trigram and the gram around an ambiguous attachment in the current experiment equal weight are used for simplicity but result are still good on the train corpus and accuracy despite the large statistical difference in attachment preference in the two corpus training on the first corpus and testing on the second give an accuracy of these result suggest that our technique capture attachment pattern that are useful across corpus 
this article is concerned with the design and implementation of information retrieval system irs we show how theory and model from the domain of human computer interaction hci can be applied to the design of irs we first study the user s task by modelling the mental activity of the user while accomplishing a task adopting a system perspective we consider the processing task of an irs and organize them in a design space we then build upon the design space to consider the implication of such data processing and level of abstraction on software design finally we present pac amodeus a software architecture model and illustrate the applicability of the approach with the implementation of an irs the tiapri system 
we propose a novel approach to automatically growing and pruninghierarchical mixture of expert the constructive algorithm proposedhere enables large hierarchy consisting of several hundredexperts to be trained effectively we show that hme s trained byour automatic growing procedure yield better generalization performancethan traditional static and balanced hierarchy evaluationof the algorithm is performed on vowel classificationand within a hybrid version of the janus 
we explore the notion of a tour guide software agent for assisting user browsing the world wide web a web tour guide agent provides assistance similar to that provided by a human tour guide in a museum it guide the user along an appropriate path through the collection based on it knowledge of the user s interest of the location and relevance of various item in the collection and of the way in which others have interacted with the collection in the past this paper describes a simple 
in recent year there ha been a flurry of work on learning probabilistic belief network current state of the art method have been shown to be successful for two learning scenario learning both network structure and parameter from completedata and learning parameter for a fixed network from incomplete data that is in the presence of missing value or hidden variable however no method ha yet been demonstrated to effectively learn network structure from incomplete data in this paper we propose a new method for learning network structure from incomplete data this method is based on an extension of the expectation maximization em algorithm for model selection problem that performs search for the best structure inside the em procedure we prove the convergence of this algorithm and adapt it for learning belief network we then describe how to learn network in two scenario when the data contains missing value and in the presence of hidden variable we provide experimental result that show the effectiveness of our procedure in both scenario 
the limitation of using self organizing map som for eitherclustering vector quantization vq or multidimensional scaling md are being discussed by reviewing recent empirical findingsand the relevant theory som s remaining ability of doing both vqand md at the same time is challenged by a new combined techniqueof online k mean clustering plus sammon mapping of thecluster centroid som are shown to perform significantly worse interms of quantization error in recovering the 
in cellular telephone system an important problem is to dynamically allocatethe communication resource channel so a to maximize service ina stochastic caller environment this problem is naturally formulated a adynamic programming problem and we use a reinforcement learning rl method to find dynamic channel allocation policy that are better thanprevious heuristic solution the policy obtained perform well for a broadvariety of call traffic pattern we present result on a 
this paper present a general trainable framework for object detection in static image of cluttered scene the detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instance by learning an object class in term of a subset of an overcomplete dictionary of wavelet basis function we derive a compact representation of an object class which is used a an input to a support vector machine classifier this representation overcomes both the problem of in class variability and provides a low false detection rate in unconstrained environment we demonstr ate the capability of the technique in two domain whose inherent information content differs significantly the first system is face detection and the second is the domain of people which in contrast to face vary greatly in color texture and pattern unlike previous approach this system learns from example and doe not rely on any a priori hand crafted model or motion based segmentation the paper also present a motion based extension to enhance the performance of the detection algorithm over video sequence the result presented here suggest that this architecture may well be quite general 
image denoising and segmentation are fundamental problem in the field of image processing and computer vision with numerous application we propose a partial differential equation pde based smoothing and segmentation framework wherein the image data are smoothed via an evolution equation that is controlled by a vector field describing a viscous fluid flow image segmentation in this framework is defined by location in the image where the fluid velocity is a local maximum the nonlinear image smoothing is selectively achieved to preserve edge in the image the novelty of this approach lie in the fact that the selective term is derived from a nonlinearly regularized image gradient field unlike most earlier technique which either used a constant with respect to time selective term or a time varying nonlinearly smoothed scalar valued term implementation result on synthetic and real image are presented to depict the performance of the technique in comparison to method recently reported in literature 
very large database with skewed class distribution and non unlform cost per error are not uncommon in real world data mining task we devised a multi classifier meta learning approach to address these three issue our empirical result from a credit card fraud detection task indicate that the approach can significantly reduce loss due to illegitimate transaction 
attribute oriented induction is a set oriented database mining method which generalizes thetask relevant subset of data attribute by attribute compress it into a generalized relation andextracts from it the general feature of data in this chapter the power of attribute orientedinduction is explored for the extraction from relational database of different kind of pattern including characteristic rule discriminant rule cluster description rule and multiple levelassociation 
in this paper we analyze and extend a class of adaptive networksfor second order blind decorrelation of instantaneous signal mixture firstly we compare the performance of the decorrelationneural network employing global knowledge of the adaptive coefficientsin with a similar structure whose coefficient areadapted via local output connection in through statisticalanalyses the convergence behavior and stability bound for thealgorithms step size are studied and derived 
an autonomous vehicle ha been developed for precision application of treatment onoutdoor crop this document detail a new vision algorithm to aid navigation and crop weeddiscrimination being developed for this machine the algorithm track a model of the crop plantingpattern through an image sequence using an extended kalman filter a parallel update schemeis used to provide not only navigation information for the vehicle controller but also estimate ofplant position for the 
in this paper the special case of reconstruction from image sequence taken by camera with skew equal to and aspect ratio equal to ha been treated these type of camera here called camera with euclidean image plane represent rigid projection where neither the principal point nor the focal length is known it will be shown that it is possible to reconstruct an unknown object from image taken by a camera with euclidean image plane up to similarity transformation i e euclidean transformation plus change in the global scale an algorithm using bundle adjustment technique ha been implemented the performance of the algorithm is shown on simulated data 
this paper present a technique called genh that automatically generates search heuristic for scheduling problem the impetus for developing this technique is the growing consensus that heuristic encode advice that is at best useful in solving most or typical problem instance and at worst useful in solving only a narrowly defined set of instance in either case heuristic problem solver to be broadly applicable should have a mean of automatically adjusting to the idiosyncrasy of each problem instance genh generates a search heuristic for a given problem instance by hillclimbing in the space of possible multiattribute heuristic where the evaluation of a candidate heuristic is based on the quality of the solution found under it guidance we present empirical result obtained by applying genh to the real world problem of telescope observation scheduling these result demonstrate that genh is a simple and effective way of improving the performance of an heuristic scheduler 
feature selection is a data preprocessing step for classification and data mining task traditionally feature selection is done by selecting a minimum number of feature that determine the class label i e by the horizontal compactness of data in this paper we propose a new selection criterion that aim at the vertical compactness of data in particular we select a subset of feature that yield the least number of projected instance while determining the class label a hybrid search that is partially dfs and partially bfs is proposed to exploit the pruning potential of the problem we compare the result induced by c before and after the feature selection 
a multiple instruction multiple data mimd parallel computing platform built upon a network of tm c s c c for real time image processing of a hierarchical foveal machine vision hfmv system is described in this paper the architecture of the system the parallel algorithm development environment and strategy to map task into the computing platform are described the platform support both static and dynamic computing resource allocation the performance of the computing platform is illustrated by example 
clustering is important in many field including manufacturing biology finance and astronomy mixture model are a popular approachdue to their statistical foundation and em is a very popular methodfor finding mixture model em however requires many access ofthe data and thus ha been dismissed a impractical e g zhang ramakrishnan amp livny for data mining of enormous datasets we present a new algorithm based on the multiresolution kd treesof moore schneider amp 
the organization of image database can rely upon different aspect of image similarity here we extract silhouette from image of three dimensional object and rely upon curve similarity for image classification our scheme avoids the embedding of image in a vector space instead we propose a curve dissimilarity measure which relies upon a novel curve matching syntactic algorithm and use it to represent the database a a complete graph with node representing the image and dissimilarity value assigning weight to the edge a robust clustering algorithm which is based on a physical ferromagnet model is used to find the hierarchical structure underlying the collection of image we tested our scheme with a database of real image of object some of them very different others rather similar we get a perfect hierarchical classification of these image into class of object belonging to different family 
we use colour mixture model for real time colour based object localisation tracking and segmentation in dynamic scene within such a framework we address the issue of model order selection modelling scene background and model adaptation in time experimental result are given to demonstrate our approach in different scale and lighting condition 
this paper describes an effcient method to calculate from an image of an object configuration of a two fingered robot gripp er that form a cage to contain that object closing the finger on the object from these configuration is guaranteed to reach a given desired grasp this build on the visual grasping theory of blake taylor and cox which describes how to find optimal grasp it extends the result of rimon and blake which show how to construct such cage in two way first a more effcient algorithm for computing the cage is described second a further development deal with occlusion by solving thecaging problem within a restricted image window the new method greatly reduce the complexity of the visual caging problem making it feasible in a real time computer vision system 
recent experimental data indicate that the strengthening or weakening ofsynaptic connection between neuron depends on the relative timing ofpreand postsynaptic action potential ahebbian synaptic modificationrule based on these data lead to a stable state in which the excitatory andinhibitory input to a neuron are balanced producing an irregular patternof firing it ha been proposed that neuron in vivo operate in such amode introductionhebbian modification of network 
we present completely new very powerful solution to two fundamental problem central to computer vision given data set representing c object to be stored in a database and given a new data set for an object determine the object in the database that is most like the object measured we solve this problem through use of pims polynomial interpolated measure which is a new representation integrating implicit polynomial curve and surface explicit polynomial and discrete data set which may be sparse the method provides high accuracy at low computational cost given noisy d data along a curve or d data along a surface decompose the data into patch such that new data taken along affine transformation or euclidean transformation of the curve or surface can be decomposed into correponding patch then recognition of complex or partially occluded object can be done in term of invariantly determined patch we briefly outline a low computational cost image database indexing system based on this representation for object having complex shape geometry 
prediction of lifetime of dynamically allocated object can be usedto improve time and space efficiency of dynamic memory managementin computer program barrett and zorn used a simplelifetime predictor and demonstrated this improvement on a varietyof computer program in this paper we use decision tree to dolifetime prediction on the same program and show significantlybetter prediction our method also ha the advantage that duringtraining we can use a large number of 
we describe a local parallel method for computing the stochastic completion field introduced in an earlier paper cite williams the stochastic completion field represents the likelihood that a completion joining two contour fragment pass through any given position and orientation in the image plane it is based upon the assumption that the prior probability distribution of completion shape can be modeled a a random walk in a lattice of discrete position and orientation the local parallel method can be interpreted a a stable finite difference scheme for solving the underlying fokker planck equation identified by mumford cite mumford the resulting algorithm is significantly faster than the previously employed method which relied on convolution with large kernel filter computed by monte carlo simulation the complexity of the new method is o n m while that of the previous algorithm wa o n m for an n x n image with m discrete orientation perhaps most significantly the use of a local method allows u to model the probability distribution of completion shape using stochastic process which are neither homogenous nor isotropic for example it is possible to modulate particle decay rate by a directional function of local image brightness i e anisotropic decay the effect is that illusory contour can be made to respect the local image brightness structure finally we note that the new method is more plausible a a neural model since unlike the previous method it can be computed in a sparse locally connected network and the network dynamic are consistent with psychophysical measurement of the time course of illusory contour formation 
we introduce a typed feature logic system providing both universal implicational principle a well a definite clause over feature term we show that such an architecture support a modular encoding of linguistic theory and allows for a compact representation using underspecification the system is fully implemented and ha been used a a workbench to develop and test large hpsg grammar the technique described in this paper are not restricted to a specific implementation but could be added to many current feature based grammar development system 
in this paper we propose a technique to incorporate contextual inform ation into object classification in the real world there are case where the identity of an object is ambiguous due to the noise in the measurement based on which the classification should be made it is helpful to reduce the ambiguity by utilizing extra information referred to a conte xt which in our case is the identity of the accompanying object this technique is applied to white blood cell classification comparison are made against no context approach which demonstrates the superior classification performance achieved by using context in our particular application it significantly reduces false alarm rate and thus greatly reduces the cost due to expensive clinical test 
the three best known criterion in two view motion analysis are based respectively on the distance between point and their corresponding epipolar line on the gradient weighted epipolar error and on the distance between point and the reprojections of their reconstructed point the last one ha a better statistical interpretation but is however much slower than the first two in this paper we show that the last two criterion are equivalent when the epipoles are at infinity and differ from each other only a little even when the epipoles are in the image the first two criterion are equivalent only when the epipoles are at infinity and when the observed object ha the same scale in the two image this suggests that the second criterion is sufficient in practice because of it computational efficiency the resultis valid for both calibrated and uncalibrated image 
we describe an approach to detecting locating and normalizing road sign the approach will apply provided i the sign have stereotypical boundary shape i e rectangular or hexagonal of course we allow for these shape to be distorted by projection to unknown viewpoint ii the writingon the sign ha one uniform color and the rest of the sign ha a second uniform color we allow for the color of the illuminant to be unknown we show that the approach work even under significant illuminant color change viewpoint direction shadowing and occlusion this work is part of a project intended to help people who are blind or whose sight is impaired 
a rich body of data exists showing that recollection of specific information make an important contribution to recognition memory which is distinct from the contribution of familiarity and is not adequately captured by existing unitary memory model furthermore neuropsychological evidence indicates that recollection is subserved by the hippocampus we present a model based largely on known feature of hippocampal anatomy and physiology that account for the following key characteristic of recollection false recollection is rare i e participant rarely claim to recollect having studied nonstudied item and increasing interference lead to le recollection but apparently doe not compromise the quality of recollection i e the extent to which recollected information veridically reflects event that occurred at study 
binocular rivalry is the alternating percept that can result when the two eye see different scene recent psychophysical evidence support an account for one component of binocular rivalry similar to that for other bistable percept we test the hypothesis that alternation can be generated by competition between top down cortical explanation for the input rather than by direct competition between the input recent neurophysiological evidence show that some binocular neuron are modulated with the changing percept others are not even if they are selective between the stimulus presented to the eye we extend our model to a hierarchy to address these effect 
we investigate the effect of lexicon size and stopwords on chinese information retrieval using our method of short word segmentation based on simple language usage rule and statistic these rule allow u to employ a small lexicon of only entry and provide quite admirable retrieval result it is noticed that accurate segmentation is not essential for good retrieval larger lexicon can lead to incremental improvement the presence of stopwords do not contribute much noise to ir their removal risk elimination of crucial word in a query and adversely affect retrieval especially when the query are short short query of a few word perform more than worse than paragraph size query 
we characterise the set of subalgebras of allen s algebra which have a tractable satisfiability problem and in addition contain certain basic relation the conclusion is that no tractable subalgebra that is not known in the literature can contain more than the three basic relation b and b where b d o s f this mean that concerning algebra for specifying complete knowledge about temporal information there is no hope of finding yet unknown class with much expressivity furthermore we show that there are exactly two maximal tractable algebra which contain the relation both of these algebra can express the notion of sequentially thus we have a complete characterisation of tractable inference using that notion 
we investigate the application of support vector machine svms in computer vision svm is a learning technique developed by v vapnik and his team at t bell lab that can be seen a a new method for training polynomial neural network or radial basis function classifier the decision surface are found by solving a linearly constrained quadratic programming problem this optimization problem is challenging because the quadratic form is completely dense and the memory requirement grow with the square of the number of data point we present a decomposition algorithm that guarantee global optimality and can be used to train svm s over very large data set the main idea behind the decomposition is the iterative solution of sub problem and the evaluation of optimality condition which are used both to generate improved iterative value and also establish the stopping criterion for the algorithm we present experimental result of our implementation of svm and demonstrate the feasibility of our approach on a face detection problem that involves a data set of data point 
abstract cartography and other application of remote sensing have led to an increasedinterest in the semi automatic interpretation of structure in aerial image of urbanand suburban area building delineation and d reconstruction is a good case inpoint although urban and suburban area are particularly challenging because oftheir complexity the degree of regularity in such man made structure also helpsto tackle the problem the paper present the iterated application of the hough 
we study occluding contour artifact in area based stereo matching they are false response of the matching operator to the occlusion boundary and cause the object extend beyond their true boundary in disparity map most of the matching method suffer from these artifact the effect is so strong that it cannot be ignored we show what give rise to the artifact and design a matching criterion that accommodates the presence of occlusion a opposed to method that identify and remove the artifact this approach lead to the problem of measurement contamination studied in statistic we show that such a problem is hard given finite computational resource unless more independent measurement directly related to occluding contour is available what can be achieved is a substantial reduction of the artifact especially for large matching template reduced artifact allow for easier hierarchical matching and for easy fusion of reconstruction from different viewpoint into a coherent whole 
this paper describes a new technique for solving multiclass learning problem by combining freund and schapire s boosting algorithm with the main idea of dietterich and bakiri s method of error correcting output code s ecoc boosting is a general method of improving the accuracy of a given base or weak learning algorithm ecoc is a robust method of solving multiclass learning problem by reducing to a sequence of two class problem we show that our new hybrid method ha advantage of both like ecoc our method only requires that the base learning algorithm work on binary labeled data like boosting we prove that the method come with strong theoretical guarantee on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing although previous method were known for boosting multiclass problem the new method may be significantly faster and require le programming effort in creating the base learning algorithm we also compare the new algorithm experimentally to other voting method 
our work offer both a solution to the problem of finding functional dependenciesthat are distorted by noise and to the open problem of efficiently finding strong i e highly compressive partial determination per se briefly we introduce a restrictedform of search for partial determination which is based on functional dependency focusing attention on solely partial determination derivable from overfittingfunctional dependency enables efficient search for strong partial 
we describe here an algorithm for detecting subject boundary within text based on a statistical lexical similarity measure hearst ha already tackled this problem with good result hearst one of her main assumption is that a change in subject is accompanied by a change in vocabulary using this assumption but by introducing a new measure of word significance we have been able to build a robust and reliable algorithm which exhibit improved accuracy without sacrificing language independency 
we evaluated six algorithm for computing egomotion from image velocity we established benchmark for quantifying bias and sensitivity to noise and for quantifying the convergence property of those algorithm that require numerical search our simulation result reveal some interesting and surprising result first it is often written in the literature that the egomotion problem is difficult because translation e g along the x axis and rotation e g about the y axis produce similar image velocity we found to the contrary that the bias and sensitivity of our six algorithm are totally invariant with respect to the axis of rotation second it is also believed by some that fixating help to make the egomotion problem easier we found to the contrary that fixating doe not help when the noise is independent of the image velocity fixation doe help if the noise is proportional to speed but this is only for the trivial reason that the speed are slower under fixation third it is widely believed that increasing the field of view will yield better performance we found to the contrary that this is not necessarily true 
this paper deal with the discovery representation and use of lexical rule lr during large scale semi automatic computational lexicon acquisition the analysis is based on a set of lr implemented and tested on the basis of spanish and english businessand finance related corpus we show that though the use of lr is justified they do not come cost free semi automatic output checking is required even with blocking and preemtion procedure built in nevertheless large scope lr are justified because they facilitate the unavoidable process of large scale semi automatic lexical acquisition we also argue that the place of lr in the computational process is a complex issue 
in this paper we present result on developing robust natural language interface by combining shallow and partial interpretation with dialogue management the key issue is to reduce the effort needed to adapt the knowledge source for parsing and interpretation to a necessary minimum in the paper we identify different type of information and present corresponding computational model the approach utilizes an automatically generated lexicon which is updated with information from a corpus of simulated dialogue the grammar is developed manually from the same knowledge source we also present result from evaluation that support the approach 
we introduce a knowledge based approach to deep knowledge discovery from real world natural language text data mining data interpretation and data cleaning are all incorporated in cycle of quality based terminological reasoning process the methodology we propose identifies new knowledge item and assimilates them into a continuously updated domain knowledge base 
this paper demonstrates a new visual motion estimation technique that is able to recover high degree of freedom articulated human body configuration in complex video sequence we introduce the use of a novel mathematical technique the product of exponential map and twist motion and it integration into a differential motion estimation this result in solving simple linear system and enables u to recover robustly the kinematic degree of freedom in noise and complex self occluded configuration we demonstrate this on several image sequence of people doing articulated full body movement and visualize the result in re animating an artificial d human model we are also able to recover and re animate the famous movement of eadweard muybridge s motion study from the last century to the best of our knowledge this is the first computer vision based system that is able to process such challenging footage and recover complex motion with such high accuracy 
we analyze critically the use of classification accuracy to compare classifier on natural data set providing a thorough investigation using roc analysis standard machine learning algorithm and standard benchmark data set the result raise serious concern about the use of accuracy for comparing classifier and drawinto question the conclusion that can be drawn from such study in the course of the presentation we describe and demonstrate what we believe to be the proper use of roc 
two literature or set of article are complementary if considered together they can reveal useful information of scientik interest not apparent in either of the two set alone of particular interest are complementary literature that are also mutually isolated and noninteractive they do not cite each other and are not co cited in that case the intriguing possibility akrae that thm tfnrmnt nn n wd hv mwnhxno them nnvnl lyww u c yll i l su l uy s u b s y ayj a y u during the past decade we have identified seven example of complementary noninteractive structure in the biomedical literature each structure led to a novel plausible and testable hypothesis that in several case wa subsequently corroborated by medical researcher through clinical or laboratory investigation we have also developed tested and described a systematic computer sided approach to iinding and identifying complementary noninteractive literature 
many factory optimization problem frominventory control to scheduling and reliability can be formulated a continuous timemarkov decision process a primary goalin such problem is to find a gain optimalpolicy that minimizes the long run averagecost this paper describes a new averagerewardalgorithm called smart for findinggain optimal policy in continuous timesemi markov decision process the paperpresents a detailed experimental study ofsmart on a large unreliable 
rise domingo in press is a rule induction algorithm that proceeds by gradually generalizing rule starting with one rule per example this ha several advantage compared to the more common strategy of gradually specializing initially null rule and ha been shown to lead to significant accuracy gain over algorithm like cgrules and cn in a large number of application domain however rise s running time like that of other rule induction algorithm is quadratic in the number of example making it r l l a 
this paper argues that for many domain wecan expect credit assignment method thatuse actual return to be more effective forreinforcement learning than the more commonlyused temporal difference method wepresent analysis and empirical evidence fromthree set of experiment in different domainsto support this claim a new algorithm wecall c trace a variant of the p trace rl algorithmis introduced and some possible advantagesof using algorithm of this type are 
we derive the rhetorical structure of text by mean of two new surface form based algorithm one that identifies discourse usage of cue phrase and break sentence into clause and one that produce valid rhetorical structure tree for unrestricted natural language text the algorithm use information that wa derived from a corpus analysis of cue phrase 
almost all the work in average reward reinforcementlearning arl so far ha focusedon table based method which do notscale to domain with large state space inthis paper we propose two extension to amodel based arl method called h learningto address the scale up problem we extendh learning to learn action model and rewardfunctions in the form of bayesian network and approximate it value function using locallinear regression we test our algorithmson several scheduling task 
the use of visual representation in which retinal neuron receptive field are not constant over the visual field is universal in the visual system of higher vertebrate and is coming to play an important role in active vision application the breaking of translation symmetry that is unavoidably associated with non uniform sampling present a major algorithmic complication for image processing in this paper we use a lie group approach to derive a kernel which provides a quasi shift i e approximate shift invariant template matching capability under normal convolution in the distorted range coordinate of the non uniform mapping we work out the special case of the logpolar mapping which is of great interest in vision in this case we call the associated linear integral transform the exponential chirp transform ect the method is however general for other form of mapping or warp function 
the correspondence problem in computer vision is basically a matching task between two or more set of feature in this paper we introduce a vectorized image representation which is a feature based representation where correspondence ha been established with respect to a reference image the representation consists of two image measurement made at the feature point shape and texture feature geometry or shape is represented using the x y location of feature relative to the some standard reference shape image grey level or texture are represented by mapping image grey level onto the standard reference shape computing this representation is essentially a correspondence task and in this paper we explore an automatic technique for vectorizing face image our face vectorizer alternate back and forth between computation step for shape and texture and a key idea is to structure the two computation so that each one us the output of the other in addition to describing the vectorizer an application to the problem of facial feature detection will be presented 
many system that learn from example expressthe learned concept a a disjunction thosedisjuncts that cover only a few example arereferred to a small disjuncts the problem withsmall disjuncts is that they have a much highererror rate than large disjuncts but are necessary toachieve a high level of predictive accuracy thispaper investigates the effect of noise on smalldisjuncts in particular we show that when noiseis added to two real world domain a significant and 
we take a new look at one of the fundamental property of discrete time associative memory and show how it can be adapted for natural language processing nlp many task in nlp could benefit from such associative functionality particularly those which are traditionally regarded a being context driven such a word sense disambiguation the result describe the typical time to convergence of a hopfield network when trained on pattern representing sentence from a large corpus through numerical simulation we estimate the time order of convergence and compare this to previous finding for randomly generated unbiased and uncorrected pattern 
in this contribution we are concerned with the detection andrefined localization of d point landmark we propose multi step differentialprocedures for subvoxel localization of d point landmark moreover we address the problem of choosing an optimal size for a region ofinterest roi around point landmark that is to reliably localize thelandmark position on the one hand a much a possible image informationabout the landmark should be incorporated on the other hand the 
to properly handle concurrent access to document by update and query in information retrieval ir system effort are on to integrate ir feature with database management system dbms feature however initial research ha revealed that dbms feature optimized for traditional database display degraded performance while handling text database since efficiency is critical in ir system infrastructural extension are necessary for several dbms feature transaction support being one of them this paper focus on developing efficient transaction support for ir system where update and query arrive dynamically by exploiting the data characteristic of the index a well a of the query and update that access the index result of performance test on a prototype system demonstrate the superior performance of our algorithm 
multiplicative weight updating algorithm such a winnow have been studied extensivelyin the colt literature but only recently have people started to use them in application in this paper we apply a winnow based algorithm to a task in natural language contextsensitivespelling correction this is the task of fixing spelling error that happen to resultin valid word such a substituting to for too casual for causal and so on previousapproaches to this problem have been 
in the near future nasa intends to explore various region of our solar system using robotic device such a rover spacecraft airplane and or balloon such platform will carry imaging device and a variety of analytical instrument intended to evaluate the chemical and mineralogical nature of the environment s that they encounter the imaging and or spectroscopic device will acquire tremendous volume of data the communication band width are restrictive enough so that only a small portion of these data can actually be sent to earth the aim of this research wa to develop a system which analysis rock spectrum to automatically determine which spectrum are interesting and to compress the spectral data for communication to earth in the research we report here we classify laboratory data using clustering technique acpro an enhanced version of autoclass and provide the planetary scientist with a rapid visually oriented method of evaluating the underlying chemical and mineralogical information contained within the cluster we show how clustering can be used to identify interesting rock sample and estimate the compression that using such a system can achieve 
sokoban is a challenging single player game for both man and machine the simplicityof the rule belies the complexity of thegame this paper describes our program rolling stone a first attempt to solve sokobanproblems adapting and extending the standardsingle agent search technique in the literature we are able to optimally solve of problem from a standard test suite this resultdemonstrates how difficult a game sokobanreally is for computer to solve and underlinesthe 
in this paper we study the combination of two powerful approach evolutionary topology optimization enzo and tempoal difference learning td which is up to our knowledge the first time temporal difference learning wa proven to be a well suited technique for learning strategy for solving reinforcement problem based on neural network model whereas evolutionary topology optimization is concurrently the most efficient network optimization technique on two benchmark a labyrinth 
in recent year curve evolution ha developed into an important tool in computer vision and ha been applied to a wide variety of problem such a smoothing of shape shape analysis and shape recovery the underlying principle is the evolution of a simple closed curve whose point move in the direction of the normal with prescribed velocity a fundamental limitation of the method a it stand is that it cannot deal with important image feature such a triple point the method also requires a choice of an edge strength function defined over the image domain indicating the likelihood of an object boundary being present at any point in the image domain this implies a separate preprocessing step in essence precomputing approximate boundary in the presence of noise one also ha to choose the initial curve it is shown here that the different version of curve evolution used in computer vision together with the preprocessing step can be integrated in the form of a new segmentation functional which overcomes these limitation and extends curve evolution model moreover the numerical solution obtained retain sharp discontinuity or shock thus providing sharp demarcation of object boundary 
we introduce a novel framework for simultaneous structure and parameter learning in hidden variable conditional probability model based on an entropic prior and a solution for it maximum a posteriori map estimator the map estimate minimizes uncertainty in all respect cross entropy between model and data entropy of the model entropy of the data s descriptive statistic iterative estimation extinguishes weakly supported parameter compressing and sparsifying the model trimming operator accelerate this process by removing excess parameter and unlike most pruning scheme guarantee an increase in posterior probability entropic estimation take a overcomplete random model and simplifies it inducing the structure of relation between hidden and observed variable applied to hidden markov model hmms it find a concise finite state machine representing the hidden structure of a signal we entropically model music handwriting and video time series and show that the resulting model are highly concise structured predictive and interpretable surviving state tend to be highly correlated with meaningful partition of the data while surviving transition provide a low perplexity model of the signal dynamic an entropic prior in entropic estimation we seek to maximize the information content of parameter for conditional probability parameter value near chance add virtually no information to the model and are therefore wasted degree of freedom in contrast parameter near the extremum are informative because they impose strong constraint on the class of signal accepted by the model in bayesian term our prior should assert that parameter that do not reduce uncertainty are improbable we can capture this intuition in a surprisingly simple form for a model of conditional probability we write 
scene classification is a major open challenge in machine vision most solution proposed so far such a those based on color histogram and local texture statistic cannot capture a scene s global configuration which is critical in perceptual judgment of scene similarity we present a novel approach configural recognition for encoding scene class structure the approach s main feature is it use of qualitative spatial and photometric relationship within and across region in low resolution image the emphasis on qualitative measure lead to enhanced generalization ability and the use of low resolution image render the scheme computationally efficient we present result on a large database of natural scene we also describe how qualitative scene concept may be learned from example 
many real world kdd expedition involve investigation of relationship between variable in different heterogeneous database we present a dynamic programming technique for linking record in multiple heterogeneous database using loosely defined field that allow free style verbatim entry we develop an interestingness measure based on non parametric randomization test which can be used for mining potentially useful relationship among variable this measure us distributional characteristic of historical event hence accommodating variable length record in a natural way a an illustration we include a successful application of the proposed methodology to a real world data mining problem at lucent technology 
in this paper we describe a dynamic programming dp based search algorithm for statistical translation and present experimental result the statistical translation us two source of information a translation model and a language model the language model used is a standard bigram model for the translation model the alignment probability are made dependent on the difference in the alignment position rather than on the absolute position thus the approach amount to a first order hidden markov model hmm a they are used successfully in speech recognition for the time alignment problem under the assumption that the alignment is monotone with respect to the word order in both language an efficient search strategy for translation can be formulated the detail of the search algorithm are described experiment on the eutrans corpus produced a word error rate of 
we develop an approach to image segmentation for naturalscenes containing image texture one general methodology which showspromise for solving this problem is to characterize textured region viatheir response to a set of filter however this approach brings with itmany open question including how to combine texture and intensityinformation into a common descriptor and how to deal with the factthat filter response inside textured region are generally spatially 
most learning algorithm work most effectivelywhen their training data contain completelyspecified labeled sample in manydiagnostic task however the data will includethe value of only some of the attribute we model this a a blocking processthat hide the value of those attribute fromthe learner while blocker that remove thevalues of critical attribute can handicap alearner this paper instead focus on blockersthat remove only irrelevant attribute value i e value 
model of document indexing and document retrieval have been extensively studied the integration of these two class of model ha been the goal of several researcher but it is a very difficult problem we argue that much of the reason for this is the lack of an adequate indexing model this suggests that perhaps a better indexing model would help solve the problem however we feel that making unwarranted parametric assumption will not lead to better retrieval performance furthermore making prior assumption about the similarity of document is not warranted either instead we propose an approach to retrieval based on probabilistic language modeling we estimate model for each document individually our approach to modeling is non parametric and integrates document indexing and document retrieval into a single model one advantage of our approach is that collection statistic which are used heuristically in many other retrieval model are an integral part of our model we have implemented our model and tested it empirically our approach significantly outperforms standard tf idf weighting on two different collection and query set 
this paper present a formal model of an active recognition system that can be programmed by learning at each time step the system decides between producing an action to generate new data and stopping to issue the name of the object observed the action can be directed either towards the external environment or towards the internal perceptual system of the agent the decision strategy is based on a quantitative evaluation of the system learning experience the problem studied is the recognition of chess piece using a moving camera and a multiscale feature detector the recognition is difficult because the object are complex neither polyhedral nor smooth and rather similar between class especially in certain view configuration the system us the information obtained by observing internal state transition when the camera is moved or when the feature detector scale is changed a simulation of the agent and the environment is used for experimental measure of the model performance 
current indexing based approach build the hash table using either a large number of reference view or d model in this paper we propose building the hash table using algebraic function of view during preprocessing we consider group of model point and we represent all the view i e image that they can produce in a hash table these view are computed using algebraic function of a small number of reference view which contain the group fundamental to this procedure is a methodology based on singular value decomposition and interval arithmetic for estimating the range of value that the parameter of algebraic function can assume during recognition scene group are used to retrieve from the hash table the model group that might have produced them using algebraic function of view for indexing based recognition offer a number of advantage first of all the hash table can be built easier without requiring d model or a large number of reference view second recognition doe not rely on the similarity between new and reference view third verification becomes simpler finally the approach is more general and extendible 
we introduce coactive learning a a distributed learning approach to data mining in networked and distributed database the coactive learning algorithm act on independent data set and cooperate by communicating training information which is used to guide the algorithm hypothesis construction the exchanged training information is limited to example and response to example it is shown that coactive learning can offer a solution to learning on very large data set by allowing multiple coacting algorithm to learn in parallel on subset of the data even if the subset are distributed over a network coactive learning support the construction of global concept description even when the individual learning algorithm are provided with training set having biased class distribution finally the capability of coactive learning are demonstrated on artificial noisy domain and on real world domain data with sparse class representation and unknown attribute value 
the support vector sv method wa recently proposed for estimatingregressions constructing multidimensional spline andsolving linear operator equation vapnik in this presentationwe report result of applying the sv method to these problem introductionthe support vector method is a universal tool for solving multidimensional functionestimation problem initially it wa designed to solve pattern recognition problem where in order to find a decision rule with good 
prose rhythm is a widely observed but scarcely quantified phenomenon we describe an information theoretic model for measuring the regularity of lexical stress in english text and use it in combination with trigram language model to demonstrate a relationship between the probability of word sequence in english and the amount of rhythm present in them we find that the stream of lexical stress in text from the wall street journal ha an entropy rate of le than bit per syllable for common sentence we observe that the average number of syllable per word is greater for rarer word sequence and to normalize for this effect we run control experiment to show that the choice of word order contributes significantly to stress regularity and increasingly with lexical probability 
we present aa algorithm for mining association rule from relational table containing numeric and categorical attribute the approach is to merge adjacent interval of numeric value in a bottom up manner on the basis of maximizing the interestingness of a set of association rule a modification of the b tree is adopted for performing this task efficiently the algorithm take o kn i o time where k is the number of attribute and n is the number of row in the table we evaluate the effectiveness of producing good interval 
we present new algorithm for parameter estimation of hmms by adapting a framework used for supervised learning we construct iterative algorithm that maximize the likelihood of the observation while also attempting to stay close to the current estimated paramet er we use a bound on the relative entropy between the two hmms a a distance measure between them the result is new iterative training algo rithms which are similar to the em baum welch algorithm for training hmms the proposed algorithm are composed of a step similar to the expectation step of baum welch and a new update of the parameter which replaces the maximization re estimation step the algorithm take only negligibly more time per iteration and an approximated version us the same expectation step a baum welch we evaluate experimentally the new algorithm on synthetic and natural speech pronunciation data for sparse model i e model with relatively small number of non zero parameter the proposed algorithm require significantly fewer iteration preliminary we use the number from to to name the state of an hmm state is a special initial state and state is a special final state any state sequence denoted by start with the initial state but never return to it and end in the final state observation symbol are also number in and observation sequence are denoted by a discrete output hidden markov model hmm is parameterized by two matrix and the first matrix is of dimension and denotes the probability of moving from state to state the second matrix is of dimension and is the probability of outputting symbol at state the set of parameter of an hmm is denoted by the initial state distribution vector is represented by t he first row of 
abstract we consider the problem of assigning level number weight to hierarchically organized category during the process of text categorization these level control the ability of the category to attract document during the categorization process the level are adjusted in order to obtain a balance between recall and precision for each category if a category s recall exceeds it precision the category is too strong and it level is reduced conversely a category s level is increased to strengthen it if it precision exceeds it recall the categorization algorithm used is a supervised learning procedure that us a linear 
we present exact analytical equilibrium solution for a class of recurrent neural network model with both sequential and parallel neuronal dynamic in which there is a tunable competition between nearestneighbour and long range synaptic interaction this competition is found to induce novel coexistence phenomenon a well a discontinuous transition between pattern recall state cycle and non recall state 
this paper present a first step towards a unifying framework for knowledge discovery in database we describe fink between data milfing knowledge discovery and other related field we then define the kdd process and basic data mining algorithm discus application issue and conclude with an analysis of challenge facing practitioner in the field 
object recognition start from a set of image measurement including location of point line surface color and shading which provides access into a database where representation of object are stored we describe a complexity theory of indexing a meta analysis which identifies the best set of measurement up to algebraic transformation such that the representation of object are linear subspace and thus easy to learn direct indexing is efficient since the linear subspace are of minimal rank the index complexity is determined via a simple process equivalent to computing the rank of a matrix we readily re derive the index complexity of the few previously analyzed case we then compute the best index for new case point in one perspective image and direction in one para perspective image the most efficient representation of a color is a plane in d space for future application with any vision problem where the relation between shape and image measurement can be written down in an algebraic form we give an automatic process to construct the most efficient database that can be directly obtained by learning from example 
multiple instance learning is a variation on supervised learning where the task is to learn a concept given positive and negative bag of instance each bag may contain many instance but a bag is labeled positive even if only one of the instance in it fall within the concept a bag is labeled negative only if all the instance in it are negative we describe a new general framework called diverse density for solving multiple instance learning problem we apply this framework to learn a simple description of a person from a series of image bag containing that person to a stock selection problem and to the drug activity prediction problem 
many practical real life application of conceptlearning are impossible to address withouttaking into consideration the backgroundof the concept it frame of reference andthe particual situation and circumstancesof it occurence shortly it context eventhough the phenomenon of context ha beentreated by philosopher and cognitive scientist it deserves more attention in themachine learning community introductionsuppose that we are dealing with a classification task and that 
multiple image of a scene are related through d d view transformation and linear and non linear camera transformation in all the traditional technique to compute these transformation especially the one relying on direct intensity gradient one image and it coordinate system have been assumed to be ideal and distortion free in this paper we present a formulation and an algorithm for true multi image alignment that doe not rely on the measurement of a reference image being distortion free for instance in the presence of lens distortion none of the image can be assumed to be ideal in our formulation all the image are modeled a intensity measurement represented in their respective coordinate system each of which is related to an ideal coordinate system through an interior camera transformation and an exterior view transformation the goal of the accompanying algorithm is to compute an image in the ideal coordinate system while solving for the transformcations that relate the ideal system with each of the data image key advantage of the technique presented in this paper are i no reliance on one distortion free image ii ability to register image and compute coordinate transformation even when the multiple image are of an extended scene with no overlap between the first and last frame of the sequence and iii ability to handle linear and non linear transformation within the same framework the new algorithm is evaluated in the context of two application i correction of lens distortion and ii creation of video mosaic 
exploratory data analysis is a process of sifting through data in search of interesting information or pattern analyst current tool for exploring data include database management system statistical analysis package data mining tool visualization tool and report generator since the exploration process seek the unexpected in a data driven manner it is crucial that these tool are seamlessly integrated so analyst can flexibly select and compose tool to use at each stage of analysis few system have integrated all these capability either architecturally or at the user interface level visage s information centric approach allows coordination among multiple application user interface it us an architecture that keep track of the mapping of visual object to information in shared database one result is the ability to perform direct manipulation operation such a drag and drop transfer of data among application this paper describes visage s visual query language and visualization tool and illustrates their application to several stage of the exploration process creating the target dataset data cleaning and preprocessing data reduction and projection and visualization of the reduced data unlike previous integrated kdd system interface direct manipulation is used pervasively and the visualization are more diverse and can be customized automatically a needed coordination among all interface object simplies iterative modication of decision at any stage 
learning to predict rare event from sequence of e vent with categorical feature is an important real wor ld problem that existing statistical and machine learn ing method are not well suited to solve this paper d escribes timeweaver a genetic algorithm based machine learning system that predicts rare event by identifying pre dictive temporal and sequential pattern timeweaver is ap plied to the task of predicting telecommunication equipment failure from alarm message and is shown to outperf orm existing learning method 
it is frequently the case that data mining is carried out in an environment which contains noisy and missing data this a particularly likely to be true when the data were originally collected for a different purpose a is often the case in data warehousing the provision of tool to handle such imperfection in data ha been identified a a challenging area for knowledge discovery in database fayyad et al previous work ha provided some method of handling such data using machine learning or statistical method to predict likely value to replace the missing or noisy value generalised database have been proposed to provide intelligent way of storing and retrieving data frequently data are imprecise i e one is not certain about the specific value of an attribute but only that it take a value which is a member of a set of possible value such data have previously been discussed a a basis of attribute oriented induction for data mining han and fu this approach ha been shown to provide a powerful methodology for the extraction of different kind of pattern from relational database it is therefore important that appropriate functionality is provided for database system to handle such information the author consider the problem of aggregation for such data 
this paper present a novel approach to the detection andrecognition of qualitative part like geons from real d intensity image previous work relied on semi local property of either line drawing orgood region segmentation here in the framework of model based optimisation whole geons or substantial sub part are recognised by fittingparametric deformable contour model to the edge image by mean ofa maximum a posteriori estimation performed by adaptive simulated 
this paper describes an approach to extract the aspectual information of japanese verb phrase from a monoligual corpus we classify verb into six category by mean of the aspectual feature which are defined on the basis of the possibility of co occurrence with aspectual form and adverb a unique category could be identified for of the target verb to evaluate the result of the experiment we examined the meaning of teiru which is one of the most fundamental aspectual marker in japanese and obtained the correct recognition score of for the sentence 
we present five performance measure to evaluate grouping module in the context of constrained search and indexing based object recognition using these measure we demonstrate a sound experimental framework based on statistical anova test to compare and contrast three edge based organization module namely those of etemadi et al jacob and sarkar boyer in the domain of aerial object using image with adapted parameter the jacob module is overall the best choice for constraint based recognition for fixed parameter the sarkar boyer module is the best in term of recognition accuracy and indexing speedup etemadi et al s module performs equally well with fixed and adapted parameter while the jacob module is most sensitive to fixed and adapted parameter choice the overall performance ranking of the module is jacob sarkar boyer and etemadi et al 
we use a statistical method to select the most probable structure or parse for a given sentence it take a input the dependency structure generated for the sentence by a dependency grammar find all triple of modifier particle and modificant relation calculates mutual information of each relation and chooses the structure for which the product of the mutual information of it relation is the highest 
the view that communication is a form of action serving a variety of specific function ha had a tremendous impact on the philosophy of language and on computational linguistics yet this mode of analysis ha been applied to only a narrow range of exchange e g those whose primary purpose is transferring information or coordinating task while exchange meant to manage interpersonal relationship maintain face or simply to convey thanks sympathy and so on have been largely ignored we present a model of such social perlocutions that integrates previous work in natural language generation social psychology and communication study this model ha been implemented in a system that generates socially appropriate e mail in response to user specified communicative goal 
when using machine learning technique for knowledgediscovery output that is comprehensible to a humanis a important a predictive accuracy we introducea new algorithm set gen that improves thecomprehensibility of decision tree grown by standardc without reducing accuracy it doe this by usinggenetic search to select the set of input featuresc is allowed to use to build it tree we test setgen on a wide variety of real world datasets and showthat set gen tree are 
deformable model are related to other data representation method it wa recently proposed a class of model based on a fuzzy energy function which includes many well known algorithm snake elastic net fuzzy and hard c mean and kohonen map this paper describes a probabilistic extension of these algorithm in a bayesian framework using gibbs boltzman distribution it is shown that the new class of model minimizes an energy function with an additional term the log partition function the role of the log partition function in probabilistic version of snake c mean and elastic net is studied and analytic expression are derived in the case of probabilistic snake the log partition function produce an additional force field which improves the performance of these algorithm in some application 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
partially observable markov decision problem pomdps recently received a lot of attentionin the reinforcement learning community no attention however ha been paidto levin s universal search through programspace l which is theoretically optimal forum wide variety of search problem includingmany pomdps experiment in this paperfirst show that l can solve partially observablemazes pom involving many morestates and obstacle than those solved by variousprevious author 
this paper present a novel time based visual motion cue called the hybrid visual threat cue hvtc that provides some measure for a change in relative range a well a absolute clearance between a d surface and a moving observer it is shown that the hvtc is a linear combination of time to contact ttc visual looming and the visual threat cue vtc the visual field associated with the hvtc can be used to demarcate the region around a moving observer into safe and danger zone of varying degree which may be suitable for autonomous navigation task the hvtc is independent of the d environment and need almost no a priori information about it it is rotation independent and is measured in time sup unit several approach to extract the hvtc are suggested also a practical method to extract it from a sequence of image of a d textured surface obtained by a visually fixating fixed focus monocular camera in motion is presented this approach of extracting the hvtc is independent of the type of d surface texture and need no optical flow information d reconstruction segmentation feature tracking 
three representation method are empirically investigated for chinese information retrieval gram single character bigram two contiguous overlapping character and short word indexing based on a simple segmentation of the text the retrieval collection is the approximately mb trec chinese corpus of news article and query that are long and rich in wording evaluation show that gram indexing is good but not sufficiently competitive while bigram indexing work surprisingly well bigram indexing lead to a large index term space three time that of short word indexing but is a good a short word indexing in precision and about better in relevants retrieved the best average non interpolated precision is about better than gram indexing and quite high for a mainly statistical approach 
locally weighted polynomial regression lwpr is a popular instance based algorithmfor learning continuous non linearmappings for more than two or three inputsand for more than a few thousand datapointsthe computational expense of predictionsis daunting we discus drawbackswith previous approach to dealing with thisproblem and present a new algorithm basedon a multiresolution search of a quicklyconstructibleaugmented kd tree withoutneeding to rebuild the tree we can 
this paper examines the fundamental ambiguity and uncertainty inherent in recovering structure from motion by examining the eigenvectors associated with null or small eigenvalue of the hessian matrix we can quantify the exact nature of these ambiguity and predict how they affect the accuracy of the reconstructed shape our result for orthographic camera show that the ba relief ambiguity is significant even with many image unless a large amount of rotation is present similar result for perspective camera suggest that three or more frame and a large amount of rotation are required for metrically accurate reconstruction 
this paper will review the design of a working system that visually recognizes hand gesture for the control of a window based user interface after an overview of the system it will explore one aspect of gestural interaction in depth hand tracking and what is needed for the user to be able to interact comfortably with on screen object we describe how the location of the hand is mapped to a location on the screen and how it is both necessary and possible to smooth the camera input using a non linear physical model of the cursor the performance of the system is examined especially with respect to object selection we show how a standard hci model of object selection fitts law can be extended to model the selection performance of free hand pointing 
efficient access to information contained in video database implies that a structured representation of the content of the video is built beforehand this paper describes an approach in this direction targeted at video indexing and browsing exploiting a d motion model estimator we partition the video into shot characterize camera motion extract and track mobile object these step rely on robust motion estimation statistical test and contextual statistical labeling the content of each shot can then be viewed on a synoptic frame composed of a mosaic image of the background scene on which trajectory of mobile object are superimposed the proposed method also provides instantaneous and long term qualitative and quantitative object motion cue for content based indexing it different step and the system they form are designed to keep computational cost low while being able to cope with general video content wa aimed at we provide experimental result on real world sequence the structured output open important possible extension for instance in the direction of higher level interpretation 
ritz vector approximate eigenvectors that are a common choice for primary image in content based indexing they can be computed efficiently even when the image are accessed through slow communication such a the internet we develop an algorithm that computes ritz vector in one pas through the image when iterated the algorithm can recover the exact eigenvectors in application to image indexing and learning it may be necessary to compute primary image for indexing many sub category of the image set the proposed algorithm can compute these additional primary image offline without the image data much more costly even when access to the image is inexpensive 
we analyze the use of kinematic constraint for articulated object tracking condition for the occurrence of singularity in d model are presented and their effect on tracking are characterized we describe a novel d scaled prismatic model spm for figure registration in contrast to d kinematic model the spm ha fewer singularity problem and doe not require detailed knowledge of the d kinematics we fully characterize the singularity in the spm and illustrate tracking through singularity using synthetic and real example with d and d model our result demonstrate the significant benefit of the spm in tracking with a single source of video 
we have discovered a new scheme to represent the fisher informationmatrix of a stochastic multi layer perceptron based onthis scheme we have designed an algorithm to compute the inverseof the fisher information matrix when the input dimensionn is much larger than the number of hidden neuron the complexityof this algorithm is of order o n while the complexity ofconventional algorithm for the same purpose is of order o n the inverse of the fisher information matrix is 
completely parallel object recognition is np complete achievinga recognizer with feasible complexity requires a compromise betweenparallel and sequential processing where a system selectivelyfocuses on part of a given image one after another successivefixations are generated to sample the image and these sample areprocessed and abstracted to generate a temporal context in whichresults are integrated over time a computational model based on apartially recurrent feedforward network 
efficient discover of association rule in large database is a we studied problem and several ap y proaches have been proposed however it is non trivial to maintain the association rule current when the database is updated since such update could invalidate existing rule or introduce new rule in this paper we propose an incremental updating technique btied on tie ittive border for the maintenance of association ru e when new transaction data is added to f or deleted from a transaction database an important feature of our algorithm is that it requires a full scan exactly one of the whole database only if the database update cause the negative border of the set of large itemsets to expand 
severe contamination of electroencephalographic eeg activity by eye movement blink muscle heart and line noise is a serious problem for eeg interpretation and analysis rejecting contaminated eeg segment result in a considerable loss of information and may be imlractical for clinical data manv method have been proposed to remove eye movement and blink artifact from eeg recording often regression in the time or frequency domain is performed on simultaneous eeg and electrooculographic eog recording to derive parameter characterizing the appearance and spread of eog artifact in the eeg channel however eog record also contain brain signal i so regressing out eog activity inevitably involves subtracting a portion of the relevant eeg signal from each recording a well regression cannot be used to remove muscle noise or line noise since these have no reference channel here we propose a new and generally applicable method for removing a wide varietv of artifact from eeg record the method is based on an extended version of a previous independent component analysis ica algorithm for performing blind source separation on linear mixture of indewendent source signal with either sub gaussian or super gaussian distribution our result show that ica can effectiveiy detect separate and remove activity in eeg record from a wide variety of artifactual source with result comparing favorably to those obtained using regression based method 
antonio j colmenarez and thomas s huang in this paper we present a visual learning technique that maximizes the discrimination between positive and negative example in a training set we demonstrate our technique in the context of face detection with complex background without color or motion information which ha proven to be a challenging problem we use a family of discrete markov process to model the face and background pattern and estimate the probability model using the data statistic then we convert the learning process into an optimization selecting the markov process that optimizes the information based discrimination between the two class the detection process is carried out by computing the likelihood ratio using the probability model obtained from the learning procedure we show that because of the discrete nature of these model the detection process is by almost two order of magnitude le computationally expensive than neural network approach however no improvement in term of correct answer false alarm tradeoff is achieved 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
we describe an appearance based object recognition system using a keyed multi level context representation reminiscent of certain aspect of cubist art specifically we utilize distinctive intermediate level feature in this case automatically extracted d boundary fragment a key which are then verified within a local context and assembled within a loose global context to evoke an overall percept this system demonstrates extraordinarly good recognition of a variety of d shape ranging from sport car and fighter plane to snake and lizard with full orthographic invariance we report the result of large scale test involving over separate test image that evaluate performance with increasing number of item in the database in the presence of clutter background change and occlusion and also the result of some generic classification experiment where the system is tested on object never previously seen or modelled to our knowledge the result we report are the best in the literature for full sphere test of general shape with occlusion and clutter resistance 
we present a computational group theoretic approach to steerable function the approach is group theoretic in that the treatment involves continuous transformation group for which elementary lie group theory may be applied the approach is computational in that the theory is constructive and lead directly to a procedural implementation for function that are steerable with n finite number of basis function under a k parameter group the procedure is efficient and is guaranteed to return the minimum number of basis function if the function is not steerable a numerical implementation of the procedure could also be used to compute basis function that approximately steer the function over a range of transformation parameter example of both application are demonstrated 
we propose a new method to recognize d free form object from their apparent contour it is the extension of our established method to recognize object with fixed edge object model are compared with d boundary which are extracted by segment based stereo vision based on the local shape of the boundary candidate transformation are generated the candidate are verified and adjusted based on the whole shape of the boundary themodels are built from all around range data of the object experimental result show the effectiveness of the method 
it is important to use pattern information e g tv newscast and textual information e g newspaper together for this purpose we describe a method for aligning article in tv newscast and newspaper in order to align article the alignment system us word extracted from telops in tv newscast the recall and the precision of the alignment process are and respectively in addition using the result of the alignment process we develop a browsing and retrieval system for article in tv newscast and newspaper 
snake or active contour are used extensively in computer vision and image processing application particularly to locate object boundary problem associated with initialization and poor convergence to concave boundary however have limited their utility this paper develops a new external force for active contour largely solving both problem this external force which we call gradient vector flow gvf is computed a a diffusion of the gradient vector of a gray level or binary edge map derived from the image the resultant field ha a large capture range and force active contour into concave region example on simulated image and one real image are presented 
a safe control of genetic evolution consists inpreventing past error of evolution from beingrepeated this could be done by keepingtrack of the history of evolution but maintainingand exploiting the complete historyis intractable this paper investigates the use of machinelearning ml in order to extract manageableinformation from this history more precisely induction from example of past trialsand error provides rule discriminating errorsfrom successful trial such rule 
the problem of discovering association rule in large database ha received considerable research attention much research ha examined the exhaustive discovery of all association rule involving positive binary literal e g agrawal et al other research ha concerned finding complex association rule for high arity attribute such a cn clark and niblett complex association rule are capable of representing concept such a purchasedchips true and purchasedsoda false and area northeast and customertype occasional agerange young but their generality come with severe computational penalty intractable number of precondition can have large support here we introduce new algorithm by which a sparse data structure called the adtree introduced in moore and lee can accelerate the finding of complex association rule from large datasets the adtree us the algebra of probability table to cache a dataset s sufficient statistic within a tractable amount of memory we first introduce a new adtree algorithm for quickly counting the number of record that match a precondition we then show how this can be used in accelerating exhaustive search for rule and for accelerating cn type algorithm result are presented on a variety of datasets involving many record and attribute 
we describe progress in completely automatically recovering d scene structure together with d camera position from a sequence ofimages acquired by an unknown camera undergoing unknown movement the main departure from previous structure from motion strategy isthat processing is not sequential instead a hierarchical approach is employedbuilding from image triplet and associated trifocal tensor thisis advantageous both in obtaining correspondence and also in optimally 
this paper describes a new method for estimating optical flow that strike a balance between the flexibility of local dense computation and the robustness and accuracy of global parameterized flow model an affine model of image motion is used within local image patch while a spatial smoothness constraint on the affine flow parameter of neighboring patch enforces continuity of the motion we refer to this a a skin and bone model in which the affine patch can be thought of a rigid bone connected by a flexible skin since local image patch may contain multiple motion we use a layered representation for the affine bone to regularize this layered motion representation we develop a new framework for regularization with transparency 
the main aim of this paper is to suggest multi criterion based metric that can be used a comparators for an objective evaluation of data mining algorithm m algorithm each dm algorithm is characterized generally by some positive and negative property when it is applied to certain domain example of property are the accuracy rate understandability interpretability of the generated result and stability space and time complexity and maintenance cost can be considered a negative property by now there is no methodology to consider all of these property simultaneously and use them for a comprehensive evaluation of dm algorithm most of available study in literature use only the accuracy rate a a unique criterion to compare the performance of dmalgorithms and ignore the other property our suggested approach however can take into account all available positive and negative characteristic of dm algorithm and can combine them to construct a unique evaluation metric this new approach is based on dea data envelopment analysis we have applied this approach to evaluate dm algorithm in domain the result are analyzed and compared with the result of alternative approach 
in this paper we develop a method to find correspondence between a cranio caudal cc and a medio lateral oblique mlo x ray image of the same breast matching between such pair of image is considered essential by radiologist for more reliable diagnosis of early breast cancer the two image are taken while the breast is compressed between the cassette and plate of the x ray machine but almost always to a different extent in each direction the deformation of the breast caused by the different compression in the different direction cause corresponding point to appear far from the straight epipolar line familiar from binocular stereo vision the method developed in this paper calculates the line in a mlo image corresponding to a point in the cc image through simulation of the deformation and the projection of a d line curve corresponding to the point experiment using actual image show that the method give good prediction which can be used to find exact correspondence between point in the two image 
this paper describes a boundary estimation scheme based on a new adaptive approach to b spline curve fitting the number of control point of the spline their location and the observation parameter are all considered unknown the optimal number of control point is estimated via a new minimum description length mdl type criterion the result is an adaptive parametrically deformable contour which also estimate the observation model parameter experiment on synthetic and real medical image confirm the adequacy and good performance of the approach 
we propose anew rectification method for aligning epipolar line of a pair of stereo image taken under any camera geometry it effectively remaps both image onto the surface of a cylinder instead of a plane which is used in common rectification method for a large set of camera motion remapping to a plane ha the drawback of creating rectified image that are potentially infinitely large and present a loss of pixel information along epipolar line in contrast cylindrical rectification guarantee that the rectified image are bounded for all possible camera motion and minimizes the loss of pixel information along epipolar line the process e g stereo matching etc subsequently applied to the rectified image are thus more accurate and general since they can accommodate any camera geometry 
recognizing a target in synthetic aperture radar sar image is an important yet challenging application of the model based vision technique this paper describes a model based sar recognition system based on invariant histogram and deformable template matching technique an invariant histogram is a histogram of invariant value defined by geometric feature such a point and line in sar image although a few invariant are sufficient to recognize a target we use a histogram of all invariant value given by all possible target feature pair this redundant histogram enables robust recognition under severe occlusion typical in sar recognition scenario multi step deformable template matching examines the existence of an object by superimposing template over potential energy field generated from image or primitive feature it determines the template configuration which ha the minimum deformation and the best alignment of the template with feature the deformability of the template absorbs the instability of sar feature we have implemented the system and evaluated the system performance using hybrid sar image generated from synthesized model signature and real sar background signature 
we introduce a polynomial time algorithm for statistical machine translation this algorithm can be used in place of the expensive slow best first search strategy in current statistical translation architecture the approach employ the stochastic bracketing transduction grammar sbtg model we recently introduced to replace earlier word alignment channel model while retaining a bigram language model the new algorithm in our experience yield major speed improvement with no significant loss of accuracy 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
the paper discus objection against performance characterization of vision algorithm and explains theirmotivation short and long term argument are given which overcome these objection the methodologyfor performance characterization is sketched to demonstrate the feasibility of empirical testing staffof visionalgorithms motivationfor at least year computer vision ha been confronted with paper and discussion on the scientificvalue of it result and the difficulty in 
the simple bayesian classifier sbc is commonly thought to assume that attribute are independent given the class but this is apparently contradicted by the surprisingly good performance it exhibit in many domain that contain clear attribute dependence no explanation for this ha been proposed so far in this paper we show that the sbc doe not in fact assume attribute independence and can be optimal even when this assumption is violated by a wide margin the key to this finding lie in 
until recently artificial intelligence researcher have frowned uponthe application of probability propagation in bayesian belief networksthat have cycle the probability propagation algorithm isonly exact in network that are cycle free however it ha recentlybeen discovered that the two best error correcting decoding algorithmsare actually performing probability propagation in beliefnetworks with cycle in advance in neural information processing system mit press cambridge 
this statistical method compare in real time the sequence of command given by each user to a profile of that user s past behavior we use the fisher score statistic to test the null hypothesis that the observed command transition probability come from a profiled transition matrix the alternative hypothesis is formed from a principal component analysis of historical difference between the transition probability of all other user and those of the user being tested the calculation can be structured so that only a few dozen arithmetic operation are needed to update an online test statistic after each submitted command the theoretical statistical property of the test such a false positive and false negative rate are computable under the assumption of the markov process model based on a population of research user on a single computer test data from each user are used to challenge the profile of every user the test had sufficient statistical power to successfully discriminate between almost every pair of user based on a sample size equivalent to a single day s usage of an average user 
this paper present a new framework for segmentation of textured visual imagery the proposed method consists of a bayesian formulation for labeling similar region similarity is defined via texture feature obtained by gabor wavelet multivariate gaussian distribution are employed to model the feature class conditional density while the markov process is used to characterize the distribution of the region labeling due to each feature a coarse nearest neighbor clustering is performed over the feature space to estimate the initial labelings an iterative solution to the maximum a posteriori map estimation is developed where the parameter of the prior distribution of region label are estimated using the expectation maximization em algorithm finally for man made object segmentation a region growing procedure is used to analyze the classified texture region by incorporating measure of local shape characteristic to obtain smooth boundary and region homogeneity result of the developed algorithm on real scene image are presented 
in tanner and mead implemented an interesting constraint satisfaction circuit for global motion sensing in avlsi we report here a new and improved avlsi implementation that provides smooth optical flow a well a global motion in a two dimensional visual field the computation of optical flow is an ill posed problem which express itself a the aperture problem however the optical flow can be estimated by the use of regularization method in which additional constraint are introduced in term of a global energy functional that must be minimized we show how the algorithmic constraint of horn and schunck on computing smooth optical flow can be mapped onto the physical constraint of an equivalent electronic network motivation the perception of apparent motion is crucial for navigation knowledge of local motion of the environment relative to the observer simplifies the calculation of important task such a time to contact or focus of expansion there are several method to compute optical flow they have the common problem that their computational load is large this is a severe disadvantage for autonomous agent whose computational power is restricted by energy size and weight here we show how the global regularization approach which is necessary to solve for the ill posed nature of computing optical flow can be formulated a a local feedback constraint and implemented a a physical analog device that is computationally efficient 
we describe an application of dogma a ga based theory revision system to mdl based rule enhancement in supervised concept learning the system take a input classification data and a rule based classification theory produced by some rule based learner and build a second hopefully more accurate model of the data unlike most theory revision system dogma doesn t revise the initial rule but build instead a completely new theory using stochastic sampling and adaptation of the initial rule the search for the new model is guided by a mdl based complexity measure the proposed methodology offer a partial solution both to the local mimima trap of fast greedy rule based concept learner and to the time complexity problem of ga based concept learner a an example we show how the system improves rule produced by c rule 
in this paper we examine the problem of estimating the parameter of a multinomial distribution over a large number of discrete outcome most of which do not appear in the training data we analyze this problem from a bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcome constitute only a small subset of the possible outcome we show how to efficiently perform exact inference with this form of hierarchical prior and compare our method to standard approach and demonstrate it merit 
this paper is concerned with the analysis of d fluid motion from numerical image the interpretation of such deformable flow field can be derived from the characterization of linear motion model provided that first order approximation are considered in an adequate neighborhood of so called singular point where the velocity becomes null however locating such point delimiting this neighborhood and estimating the associated d affine motion model are intricate difficult problem we explicitly address these three joint problem according to a statistical adaptive approach in the fluid mechanic image we are dealing with the motion model can be directly inferred from a single image since the visualized form account for the underlying motion we have developed an original method which relies on an orthogonality constraint between the spatial image gradient field and the motion model velocity field while explicitly formalizing and handling both model and measurement noise this method ha been validated on several real fluid flow image 
in data oriented language processing an annotated language corpus is used a a stochastic grammar the most probable analysis of a new sentence is constructed by combining fragment from the corpus in the most probable way this approach ha been successfully used for syntactic analysis using corpus with syntactic annotation such a the penn tree bank if a corpus with semantically annotated sentence is used the same approach can also generate the most probable semantic interpretation of an input sentence the present paper explains this semantic interpretation method a data oriented semantic interpretation algorithm wa tested on two semantically annotated corpus the english atis corpus and the dutch ovis corpus experiment show an increase in semantic accuracy if larger corpus fragment are taken into consideration 
kernel pca a a nonlinear feature extractor ha proven powerful a apreprocessing step for classification algorithm but it can also be consideredas a natural generalization of linear principal component analysis this give rise to the question how to use nonlinear feature fordata compression reconstruction and de noising application commonin linear pca this is a nontrivial task a the result provided by kernelpca live in some high dimensional feature space and need not 
a method for recognition of street name phrase collected from mail piece is presented in this paper some of the challenge posed by the problem are i patron error ii non standardized way of abbreviating name and iii variable number of word in a street name image a neural network ha been designed to segment word in a phrase a street name in this case using distance between component and style of writing the network learns the type of spacing including size that one should expect between different pair of character in handwritten test experiment show perfect word segmentation performance at about of case unlike conventional method where lexicon entry are expanded to take care of all variation of prefix and size substring matching is attempted only between the main body of a lexicon entry and the word segment of an image effort to reduce computational complexity are successfully made by the sharing of character segmentation result between the segmentation and recognition phase phrase recognition accuracy is achieved on a test set 
texture ha long been regarded a spatial distribution of gray level variation and texture analysis ha generally been confined to the d image domain introducing the concept of d world texture this paper considers texture a a function of d structure and proposes a set of d textural feature the proposed d feature appear to have a great potential in terrain classification experiment have been carried out to compare the d feature with a popular traditional d feature set the result show that the d feature significantly outperform the d feature in term of classification accuracy 
automatic indexing or registration is an essential task for image database it allows to archive organise and retrieve a large amount of image by using inner property in this paper we propose an indexing technique which allows to solve indexing problem due to geometric or photometric transformation inferred by the different image acquisition this approach is based on an invariant partition of the image thanks to the use of interest point or keypoints and a characterisation with ifs parameter or barycentric moment the research process is based on a similarity measure taking in account a numerical distance and a localisation criterion this work is based on a local characterisation of the image we use the interest point to build a triangular partition or a set of triangle we associate to each polygon a vector containing it photometric property in other approach the keypoints are directly characterised by local invariant the use of the ifs parameter to index the image ha been studied in early publication the improvement robustness against rotation and scaling come from the use of the invariant partition and the barycentric coordinate the research process is particularly important it us traditional spatial relation and integrate them with a numerical distance to calculate a score associated to each image 
in this paper we present tdleaf a variation on the td algorithm that enables it to be used in conjunction with minimax search we present some experiment in which our chess program knightcap used tdleaf to learn it evaluation function while playing on the free ineternet chess server fics fics onenet net it improved from a rating to a rating in just game and day of play we discus some of the reason for this success and also the relationship between our result and tesauro s result in backgammon 
we study several statistically and biologically motivated learning rule using the same visual environment one made up of natural scene and the same single cell neuronal architecture this allows u to concentrate on the feature extraction and neuronal coding property of these rule included in these rule are kurtosis and skewness maximization the quadratic form of the bienenstock cooper munro bcm learning rule and single cell independent component analysis using a structure removal method we demonstrate that receptive field developed using these rule depend on a small portion of the distribution we find that the quadratic form of the bcm rule behaves in a manner similar to a kurtosis maximization rule when the distribution contains kurtotic direction although the bcm modification equation are computationally simpler 
a complete analysis of the statistical issue related to the estimation of a bilinear form one of the fundamental problem in computer vision is presented it is shown why already at moderate noise level most available technique fail to provide a satisfactory solution a new estimation procedure is proposed in which the nonlinear nature of the error are taken into account and the implementation us the generalized singular value decomposition for superior numerical behavior a an example the ellipse fitting problem is discussed and the performance of the new algorithm is compared with thecurrent state of the art 
the work of mannila et al of finding frequent episode in sequence is extended to finding temporal logic pattern in temporal database it is argued that temporal logic provides an appropriate formalism for expressing temporal pattern defined over categorical data it is also proposed to use temporal logic programming a a mechanism for the discovery of frequent pattern expressible in temporal logic it is explained in the paper how frequent temporal pattern can be discovered by constructing temporal logic program to test these method temporal logic program were constructed for certain class of pattern and were implemented in ops 
a compositional account of the semantics of german prefix verb in hpsg is outlined we consider only those verb that are formed by productive synchronic rule rule are fully productive if they apply to all base verb which satisfy a common description prefix can be polysemous and have separate highly underspecified lexical entry adequate base are determined via selection restriction 
practical clustering algorithm require multiple data scan to achieve convergence for large database these scan become prohibitively expensive we present a scalable clustering framework applicable to a wide class of iterative clustering we require at most one scan of the database in this work the framework is instantiated and numerically justified with the popular k mean clustering algorithm the method is based on identifying region of the data that are compressible region that must be maintained in memory and region that are discardable the algorithm operates within the confines of a limited memory buffer empirical result demonstrate that the scalable scheme outperforms a sampling based approach in our scheme data resolution is preserved to the extent possible based upon the size of the allocated memory buffer and the fit of current clustering model to the data the framework is naturally extended to update multiple clustering model simultaneously we empirically evaluate on synthetic and publicly available data set 
tile analysis of tile massive data set collected by scientific instrument demand automation a a prerequisite to analysis there is an urgent need to create an intermediate level at which scientist can operate effectively isolating them from the massive size and harnessing human analysis capability to focus on task in which machine do not even renmtely approach human namely creative data analysis theory and hypothesis formation and drawing insight into underlying phe mmena we give an overview of the main issue in the exploitation of scientific data set present five c se study where kdd tool play important and enabling role and conclude with fi ture challenge for data mining and kdd technique in science data analysis 
this paper present a neural network architecture that can manage structureddata and refine knowledge base expressed in a first order logic language thepresented framework is well suited to classification problem in which concept descriptionsdepend upon numerical feature of the data in fact the main goal ofthe neural architecture is that of refining the numerical part of the knowledge base without changing it structure in particular we discus a method to translate a setof 
multiple instance learning is a way of modelingambiguity in supervised learning example each example is a bag of instance butonly the bag is labeled not the individualinstances a bag is labeled negative if all theinstances are negative and positive if at leastone of the instance in positive we applythe multiple instance learning framework tothe problem of learning how to classify naturalimages image are inherently ambiguoussince they can represent many different 
a motion estimation algorithm using wavelet approximation a an optical flow model ha been developed to estimate accurate dense optical flow from an image sequence this wavelet motion model is particularly useful in estimating optical flow with large displacement traditional pyramid method which use the coarse to fine image pyramid by image burring in estimating optical flow often produce incorrect result when the coarse level estimate contain large error that cannot be corrected at the subsequent finer level this happens when region of low texture become flat or certain pattern result in spatial aliasing due to imageblurring our method in contrast us large to small full resolution region without blurring image and simultaneously optimizes the coarser and finer part of optical flow so that the large and small motion can be estimated correctly we compare result obtained by using our method with those obtained by using one of the leading optical flow method the szeliski pyramid spline based method the experiment include case of small displacement le than pixel under image size or equivalent displacement under other image size and those of large displacement pixel while both method produce comparableresults when the displacement are small our method out performs pyramid spline based method when the displacement are large 
this paper discus a probabilistic model based approach to clustering sequence using hidden markov model hmms the problemcan be framed a a generalization of the standard mixturemodel approach to clustering in feature space two primary issuesare addressed first a novel parameter initialization procedure isproposed and second the more difficult problem of determiningthe number of cluster k from the data is investigated experimentalresults indicate that the proposed 
this paper tackle the problem of selfcalibration of multiple camera which are very far apart given a set of feature correspondence one can determine the camera geometry the key problem we address is finding such correspondence since the camera geometry location and orientation and photometric characteristic vary considerably between image one cannot use brightness and or proximity constraint instead we propose a three step approach first we use moving object in the scene to determine a rough planar alignment next we use static feature to improve the alignment finally we use off plane feature to determine the epipolar geometry and the horizon line we do not assume synchronized camera and we show that enforcing the geometric constraint enables u to align the tracking data in time we present result on challenging outdoor scene using real time tracking data 
we explore method for incorporating prior knowledge about a problem at hand in support vector learning machine we show that both invariance under group transformation and prior knowledge about locality in image can be incorporated by constructing appropriate kernel function 
the w s wake sleep algorithm is a simple learning rule for the model with hidden variable it is shown that this algorithm can be applied to a factor analysis model which is a linear version of the helmholtz machine but even for a factor analysis model the general convergence is not proved theoretically in this article we describe the geometrical understanding of the w s algorithm in contrast with the em expectationmaximization algorithm and the em algorithm a the result we prove the convergence of the w s algorithm for the factor analysis model we also show the condition for the convergence in general model 
many combinatorial optimization algorithm have no mechanism for capturing inter parameter dependency however modeling such dependency may allow an algorithm to concentrate it sampling more effectively on region of the search space which have appeared promising in the past we present an algorithm which incrementally learns pairwise probability distribution from good solution seen so far us these statistic to generate optimal in term of maximum likelihood dependency tree to model these distribution and then stochastically generates new candidate solution from these tree we test this algorithm on a variety of optimization problem our result indicate superior performance over other tested algorithm that either do not explicitly use these dependency or use these dependency to generate a more restricted class of dependency graph 
we describe a technique for using the joint occurrence of local feature at multiple resolution to measure the similarity between texture image though superficially similar to a number of gabor style technique which recognize texture through the extraction of multi scale feature vector our approach is derived from an accurate generative model of texture which is explicitly multi scale and non parametric the resulting recognition procedure is similarly non parametric and can model complex non homogeneous texture we report result on publicly available texture database in addition experiment indicate that this approach may have sufficient discrimination power to perform target detection in synthetic aperture radar image sar 
in this paper the feasibility of self calibration in the presence of varying internal camera parameter is under investigation a self calibration method is presented which efficiently deal with all kind of constraint on the internal camera parameter within this framework a practical method is proposed which can retrieve metric reconstruction from image sequence obtained with uncalibrated zooming focusing camera the feasibility of the approach is illustrated on real and synthetic example 
this paper describes a system which us multiple visual process to detect and track face for video compression and transmission the system is based on an architecture in which a supervisor selects and activates visual process in cyclic manner control of visual process is made possible by a confidence factor which accompanies each observation fusion of result into a unified estimation for tracking is made possible by estimating a covariance matrix with each observation visual process for face tracking are described using blink detection normalized color histogram matching and cross correlation ssd and ncc ensemble of visual process are organized into processing state so a to provide robust tracking transition between state is determined by event detected by process the result of face detection is fed into recursive estimator kalman filter the output from the estimator drive a pd controller for a pan tilt zoom camera the resulting system provides robust and precise tracking which operates continuously at approximately image per second on a megahertz computer work station 
we present a technique to extract complex suburban roofsfrom set of aerial image because we combine d edge information photometric and chromatic attribute and d information we can dealwith complex house neither do we assume the roof to be flat or rectilinearnor do we require parameterized building model from only oneimage d edge and their corresponding attribute and relation areextracted using a segment stereo matching based on all available image the d 
motion segmentation involves identifying region of the image that correspond to independently moving object the number of independently moving object and type of motion model for each of the object is unknown a priori in order to perform motion segmentation the problem of model selection robust estimation and clustering must all be addressed simultaneously here we place the three problem into a common bayesian framework investigating the use of model averaging representing a motion by a combination of model a a principled way for motion segmentation of image the final result is a fully automatic algorithm for clustering that work in the presence of noise and outlier 
in this paper we describe our experience of using simulatedannealing and genetic algorithm to performdata mining for a large financial service sector company we first explore the requirement that datamining system must meet to be useful in most realcommercial environment we then look at some ofthe available data mining technique including ourown heuristic technique and how they perform withrespect to those requirement the result of applyingthe technique to two 
we generalize a recent formalism to describe the dynamic of supervised learning in layered neural network in the regime where data recycling is inevitable to the case of noisy teacher our theory generates prediction for the evolution in time of trainingand generalization error s and extends the class of mathematically solvable learning process in large neural network to those complicated situation where overfitting occurs 
we present a method for automatically constructing macro action from scratch from primitive action during the reinforcement learning process the overall idea is to reinforce the tendency to perform action after action if such a pattern of action ha been rewarded we test the method on a bicycle task the car on the hill task the race track task and some grid world task for the bicycle and race track task the use of macro action approximately half the learning time while for one of the grid world task the learning time is reduced by a factor of the method did not work for the car on the hill task for reason we discus s in the conclusion 
the extraction of figure symmetry from image contour face a number of fundamental difficulty object symmetry are distorted due to i gap in the bounding contour of a shape due to figure ground blending weak contrast edge highlight noise etc ii an introduction of part and occluders and iii spurious edge element due to surface marking texture etc a framework for extracting such symmetry from real image is proposed based on the propagation of contour orientation information and the detection of four type of singularity shock arising from the collision of propagating element in this paper we show that an additional labeling of shock based on whether the colliding wavefront carry true orientation information regular v rarefaction wave allows a division of shock into three set regular shock are the partial shock of partial contour a they remain invariant to the completion of the contour semi degenerate and degenerate shock depict potential part and gap finally shock altered due to spurious edge occlusion and gap are recovered via a simulation of inter penetrating wave generated at select shock group which with the aid of the above shock label lead to second and further generation of shock 
gaussian curvature is an invariant local descriptor of smooth surface we present an object signature which is a condensed representation of the distribution of gaussian curvature information at visible object point an invariant related to gaussian curvature at a point is derived from the covariance matrix of the photometric value in a neighborhood about that point in addition we introduce an albedo normalization method that is capable of cancelling albedo on lambertian surface we use three illumination condition two of which are unknown the three tuple of intensity value at a point is related via a one to one mapping to the surface normal at that point the determinant of the covariance matrix of the local three tuples is invariant to albedo rotation and translation the collection of determinant over mutually illuminated object point is combined into a signature distribution which is albedo rotation translation and scale invariant an object recognition methodology using these signature is proposed 
this paper describes a color region based approach to motion estimation in color image sequence the system is intended for robotic and vehicle guidance application where the task is to detect and track moving object in the scene it belongs to the class of feature based matching technique and us color region resulting from a prior color segmentation a the matching primitive in contrast to other regionbased approach it take into account the unavoidable variation in the segmentation by the extension of the matching model to multi match in order to provide extended trajectory color region that could not be matched on the feature level are matched on the pixel level by the integration of a correlation based mechanism the usage of color information and the combination of feature based and correlation based matching lead to robust and ecient algorithm the system wa applied to a motion segmentation task in vehicle guidance experiment on more than natural color outdoor image taken from a moving car show promising result 
we expect a variety of autonomous system from rover to life support system to play a critical role in the success of manned mar mission the crew and ground support personnel will want to control and be informed by these system at varying level of detail depending on the situation moreover these system will need to operate safely in the presence of people and cooperate with them effectively we call such autonomous system human centered in contrast with traditional black box autonomous system our goal is to design a framework for human centered autonomous system that enables user to interact with these system at whatever level of control is most appropriate whenever they so choose but minimize the necessity for such interaction this paper discus on going research at the nasa ames research center and the johnson space center in developing human centered autonomous system that can be used for a manned mar mission 
adding example of the majority class to thetraining set can have a detrimental effect onthe learner s behavior noisy or otherwise unreliableexamples from the majority class canoverwhelm the minority class the paper discussescriteria to evaluate the utility of classifiersinduced from such imbalanced trainingsets give explanation of the poor behaviorof some learner under these circumstance and suggests a a solution a simple techniquecalled one sided selection of 
a technique is introduced for extracting and reconstructing a wide class of building type from a registered range image and optical image an attentional focus stage followed by model indexing allows top down robust surface fittting to reconstruct the d nature of the building in the data because of the effectiveness of model selection top down processing of noisy range data still succeeds and the algorithm is capable of detecting and reconstructing several different building roof class including fiat single level fiat multi leveled peaked and curued rooftop the algorithm is applicable to range data that may have been collected from several different range sensor type we demonstrate reconstruction of different building class in the presence of large amount of noise our result underline the usefuless of range data when processed in the context of a focus of attention area derived from the monocular optical image 
abstract we derive the correspondence between regularization operator used in regularization network and hilbert schmidt kernel appearing in support vector machine more specifically we prove that the green s function associated with regularization operator are suitable support vect or kernel with equivalent regularization property a a by product we show that a large number of radial basis function namely conditionally positive definite function may be used a support vector kernel 
we have been developing name it a system that associate face and name in news video first a the only knowledge source the system is given news video which include image sequence and transcript obtained from audio track or closed caption text the system can then either infer the name of a given face and output the name candidate or can locate the face in news video by a name to accomplish this task the system extract face from image sequence and name from transcript both of which might correspond to key person in news topic the proposed system take full advantage of advanced image and natural language processing the image processing contributes to the extraction of face sequence which provide rich information for face name association the processing also help to select the best frontal view of a face in a face sequence to enhance the face identification which is required for the processing on the other hand the natural language processing effectively extract name by using lexical grammatical analysis and knowledge of the news video topic structure the success of our experiment demonstrates the benefit of the advanced image and natural language processing method and their incorporation 
generating good production quality plan isan essential element in transforming plannersfrom research tool into real world application but one that ha been frequentlyoverlooked in research on machine learningfor planning this paper describes quality an architecture that automatically acquiresoperational quality improving controlknowledge given a domain theory a domainspecificmetric of plan quality and problemswhich provide planning experience theframework includes two 
feature selection can be defined a a problemof finding a minimum set of m relevant attributesthat describes the dataset a well asthe original n attribute do where m n after examining the problem with both theexhaustive and the heuristic approach to featureselection this paper proposes a probabilisticapproach the theoretic analysis andthe experimental study show that the proposedapproach is simple to implement andguaranteed to find the optimal if resourcespermit 
in this paper the problem of generating a d octree like structure with the help of epipolar geometry within a projective framework is addressed after a brief introduction on the basic of octrees and epipolar geometry the new concept called projective octree is introduced together with an algorithm for building this projective structure finally some result of the implementation are presented in the last section together with the conclusion and future work 
data mining ha informally been introduced a large scale search for interesting pattern in data it is often an explorative task iteratively performed within the process of knowledge discovery in database in this process interactive visualization technique are also successfully applied for data exploration we deal with the synergy of these two complemental approach whereas datamining typically relies on strategy for systematic search in large hypothesis space guided by the autonomous evaluation of statistical test interactive visualization activates the visual capacity of an analyst to identify pattern that may also stimulate the further direction of the exploration process we demonstrate some possibility to combine these approach for the area of data mining in document collection document explorer is a system that offer various preprocessing tool to prepare collection of text or multimedia document which are available in distributed environment e g internet and intranet for data mining application and includes data mining method based on searching for pattern like frequent set or association rule keyword graph are used in this system a an highly interactive technique to present the mining result the user can operate on the visualized result either to redirect the data mining process to filter and structure the result to link several graph or to browse into the document collection thus in the keyword graph the relation between interesting set of keywords are presented the set may also be regarded a retrieval query to be posed to the collection and made operable to the analyst 
the problem of finding the closest point in high dimensional space is common in computational vision unfortunately the complexity of most existing search algorithm such a k d tree and r tree grows exponentially with dimension making them impractical for dimensionality above in nearly all application the closest point is of interest only if it lie within a user specified distance epsilon we present a simple and practical algorithm to efficiently search for the nearest neighbor within euclidean distance epsilon our algorithm us a projection search technique along with a novel data structure to dramatically improve performance in high dimension a complexity analysis is presented which can help determine epsilon in structured problem benchmark clearly show the superiority of the proposed algorithm for high dimensional search problem frequently encountered in machine vision such a real time object recognition 
we present a new approximate learning algorithm for boltzmannmachines using a systematic expansion of the gibbs free energy tosecond order in the weight the linear response correction to thecorrelations is given by the hessian of the gibbs free energy thecomputational complexity of the algorithm is cubic in the numberof neuron we compare the performance of the exact bm learningalgorithm with first order wei mean field theory and secondorder tap mean field theory the learning 
autonomous robot system operating in an uncertain environment have to be reactive and adaptive in order to cope with changing environment condition and task requirement to achieve this the hybrid control architecture presented in this paper us reinforcement learning on top of a discrete event dynamic system ded framework to learn to supervise a set of basis controller in order to achieve a given task the use of an abstract system model in the automatically derived supervisor reduces the complexity of the learning problem in addition safety constraint may be imposed a priori such that the system learns on line in a single trial without the need for an outside teacher to demonstrate the applicability of the approach the architecture is used to learn a turning gait on a four legged robot platform 
we compare the effectiveness of two related machine translation model applied to the same limited domain task one is a transfer model with monolingual head automaton for analysis and generation the other is a direct transduction model based on bilingual head transducer we conclude that the head transducer model is more effective according to measure of accuracy computational requirement model size and development effort 
we discus optimal rotation estimation from two set of dpoints in the presence of anisotropic and inhomogeneous noise we firstpresent a theoretical accuracy bound and then give a method that attainsthat bound which can be viewed a describing the reliability of thesolution we also show that an efficient computational scheme can beobtained by using quaternion and applying renormalization using realstereo image for d reconstruction we demonstrate that our methodis superior to 
existing object tracking algorithm generally use some form of local optimisation assuming that an object s position and shape change smoothly over time in some situation this assumption is not valid the trackable shape of an object may change discontinuously for example if it is the d silhouette of a d object in this paper we propose a novel method for modelling temporal shape discontinuity explicitly allowable shape are represented a a union of learned bounded region within a shape space discontinuous shape change are described in term of transition between these region transition probability are learned from training sequence and stored in a markov model in this way we can create wormhole in shape space tracking with such model is via an adaptation of the condensation algorithm 
we describ e an implemented technique for generating event model automatically based on qualitative reasoning and a statistical analysis of video input using an existing tracking program which generates labelled contour for object in every frame the view from a fixed camera is partitioned into semantically relevant region based on the path followed by movingobjects the path are indexed with temporal information so object moving along the same path at different speed can be distinguished using a notion of proximity based on the speed of the moving object and qualitative spatial reasoning technique event model describing the behaviour of pair of object can be built again using statistical method the system ha been tested on a traffic domain and learns various event model expressed in the qualitative calculus which represent human observable event the system can then be used to recognise subsequent selected event occurrence or unusual behaviour 
we present a d shape based object recognition system for simultaneous recognition of multiple object in scene containing clutter and occlusion recognition is based on matching surface by matching point using the spin image representation the spin image is a data level shape descriptor that is used to match surface represented a surface mesh we present a compression scheme for spin image that result in efficient multiple object recognition which we verify with result showing the simultaneous recognition of multiple object from a library of model furthermore we demonstrate the robust performance of recognition in the presence of clutter and occlusion through analysis of recognition trial on scene 
in this paper we show that for discounted mdps with discount factor gamma the asymptotic rate of convergence of q learning is o t r gamma if r gamma and o sqrt log log t t otherwise provided that the state action pair are sampled from a fixed probability distribution here r p min p max is the ratio of the minimum and maximum state action occupation frequency the result extend to convergent on line learning provided that p min where p min and p max now become the minimum and maximum state action occupation frequency corresponding to the stationary distribution 
in this paper we show that for discounted mdps with discount factor the asymptotic rate of convergence of q learning is o i tr l o if r i and o jlog log t t otherwise provided that the state action pair are sampled from a fixed prob ability distribution here r pmin pmrx is the ratio of the min imum and rnaximurn state action occupation frequency the re sults extend to convergent oil line learning provided that pmin where pmin and pmax now become the minimum and maximum state action occupation frequency corresponding to the station ary distribution 
this paper present an algorithm for extracting proposition from trained neural network the algorithm is a decompositional approach which can be applied to any neural network whose output function is monotone such a sigmoid function therefore the algorithm can be applied to multi layer neural network recurrent neural network and so on the algorithm doe not depend on training method the algorithm is polynomial in computational complexity the basic idea is that the unit of neural network are approximated by boolean function but the computational complexity of the approximation is exponential so a polynomial algorithm is presented the author have applied the algorithm to several problem to extract understandable and accurate proposition this paper show the result for vote data and mushroom data the algorithm is extended to the continuous domain where extracted proposition are continuous boolean function roughly speaking the representation by continuous boolean function mean the representation using conjunction disjunction direct proportion and reverse proportion this paper show the result for iris data 
the one dimensional image analysis method know a the sieve is extended to any finite dimensional image it preserve all the usual scale space property but ha some additional feature that we believe make it more attractive than the diffusion based method we present some simple example of how it might be used 
this study combine two useful method in recognition consensus or voting based approach and moment based representation match between image patch are generated using a gaussian weighted moment encoding of the patch and a feature indexing process each match implies an object d position and orientation pose and generates a vote for this pose recognition is accomplished by detecting significant cluster of vote in pose space this combined method is an improvement over voting and moment method in isolation using image brightness moment the idea is successfully demonstrated on example of human face undergoing full d pose change a well a change in feature such a talking and blinking the idea is then extended to moment of local texture orientation and successfully demonstrated under large variation in lighting nature and geometry 
we exhibit a novel way of simulating sigmoidal neural net by networksof noisy spiking neuron in temporal coding furthermore it is shown thatnetworks of noisy spiking neuron with temporal coding have a strictly largercomputational power than sigmoidal neural net with the same number ofunits 
based on a resulting medial axis configuration of planar shape a new shape descriptor called the chain of circle cocs is defined herein the cocs representation is directly extracted along the boundary contour of silhouette image and can be controlled in a hierarchical manner which appeal to intuition the coarsetofine hierarchy make matching of shape possible with le computational complexity and greater robustness to noise spatial quantization and local deformation of shape the dissimilarity vector calculated in the matching which is executed via the dynamic programming technique may be used to facilitate the searching process in the digital library the capability of the proposed method is shown by matching several complex shape such a map image 
we derive real time global optimization algorithm for several clustering optimization method used in unsupervised texture segmentation speed is achieved by exploiting the topological relation of feature to design a multiscale optimization technique while accuracy and global optimization property are provided by a deterministic annealing method coarse grained cost function are derived for both central and sparse pairwise clustering where the problem of coarsening sparse random graph is solved by the concept of structured randomization annealing schedule and coarse to fine optimization are tightly coupled by a statistical convergence criterion derived from computational learning theory the algorithm are benchmarked on brodatz like micro texture mondrian result are presented for an autonomous robotics application 
this paper describes the incremental generation of parse table for the lr type parsing of tree adjoining language tals the algorithm presented handle modification to the input grammar by updating the parser generated so far in this paper a lazy generation of lr type parser for tals is defined in which parse table are created by need while parsing we then describe an incremental parser generator for tals which responds to modification of the input grammar by updating parse table built so far 
inspired by the property of the humam visual system a new active vision system called escher etl stereo compact head for robot vision ha been recently implemented with foveated wide angle lens the lens exhibit a wide field of view along with a space varying resolution for facilitating both detection and close observation however to handle such optical property and achieve basic eye movement function new calibration method are needed therefore two novel and online technique are presented that in one case perform a global identification of the optical process through artificial neural technique and in the other case compute the physical parameter by using environmental feature tracking and controlled rotation of the camera self alignment of the camera is also achieved using a similar technique 
in order to process incoming sound efficiently it is advantageousfor the auditory system to be adapted to the statistical structure ofnatural auditory scene a a first step in investigating the relationbetween the system and it input we study low order statisticalproperties in several sound ensemble using a filter bank analysis focusing on the amplitude and phase in different frequency band we find simple parametric description for their distribution andpower spectrum that are 
many classification algorithm are designed towork with datasets that contain only discrete attribute discretization is the process of convertingthe continuous attribute of the dataset intodiscrete one in order to apply some classificationalgorithm in this paper we first review previouswork in discretization then we propose anew discretization method based on a distanceproposed by l opez de m antaras and show thatit can be easily implemented in parallel with ahigh improvement in 
the paper contributes to the viewpoint invariant recognition of planar pattern especially label and sign under affine deformation by their nature the information of such eye catcher is not contained in the outline or frame they often are affinely equivalent like parallelogram and ellipsis but in the intensity content within moment invariant are well suited for their recognition they need a closed bounding contour but this is comparatively easy to provide for the simple shape considered on the other hand they characterize the intensity pattern without the need for error prone feature extraction this paper us moment a the basic feature but extends the literature in two respect deliberate mix of different type of moment to keep the order of the moment and hence also the sensitivity to noise low and yet have a sufficiently large number to safeguard discriminant power and invariance with respect to photometric change is incorporated in order to find the simplest moment invariant that can cope with changing lighting condition which can hardly be avoided when changing viewpoint the paper give complete classification of such affine photometric moment invariant experiment are described that illustrate the use of some of them 
the recognition of general three dimensional object in cluttered scene is a challenging problem in particular the design of a good representation suitable to model large number of generic object that is also robust to occlusion ha been an stumbling block in achieving success in this paper we propose a representation using appearance based part and relation to overcome these problem appearance based part and relation are defined in term of closed region and the union of these region respectively the region are segmented using the mdl principle and their appearance is obtained from collection of image and compactly represented by parametric manifold in the two eigenspaces spanned by the part and the relation 
there is strong evidence that face processing is localized in the brain the double dissociation between prosopagnosia a face recognition deficit occurring after brain damage and visual object agnosia difficulty recognizing other kind of complex object indicates that face and nonface object recognition may be served by partially independent mechanism in the brain is neural specialization innate or learned we suggest that this specialization could be the result of a competitive learning mechanism that during development devotes neural resource to the task they are best at performing further we suggest that the specialization arises a an interaction between task requirement and developmental constraint in this paper we present a feed forward computational model of visual processing in which two module compete to classify input stimulus when one module receives low spatial frequency information and the other receives high spatial frequency information and the task is to identify the face while simply classifying the object the low frequency network show a strong specialization for face no other combination of task and input show this strong specialization we take these result a support for the idea that an innately specified face processing module is unnecessary background study of the preserved and impaired ability in brain damaged patient pro vide important clue on how the brain is organized case of prosopagnosia a face recogniti on deficit often sparing recognition of non face object and visual object agnosia an object recognition deficit that can occur without appreciable impairment of face recognition prov ide evidence that face recognition is served by a special mechanism for a recent review of this 
arachnid is a distributed algorithm for information discovery in large dynamic distributed environment such a the world wide web the approach is based on a distributed adaptive population of intelligent agent making local decision the behavior of the algorithm is analyzed using a simplified model of the web environment this analysis highlight an interesting feature of the web environment that bodes well for arachnid s search method the performance of the algorithm is illustrated 
we present a novel application of the expectation maximization algorithm to the global analysis of articulated motion the approach utilizes a kinematic model to constrain the motion estimate producing a segmentation of the flow field into part with different articulated motion experiment with synthetic and real image are described 
the problem of transforming the knowledge base of expert system using induced rule or decision tree into comprehensible knowledge structure is addressed a knowledge structure is developed that generalizes and subsumes production rule decision tree and rule with exception it give rise to a natural complexity measure that allows them to be understood analyzed and compared on a uniform basis the structure is a directed acyclic graph with the semantics that node are premise some of which have attached conclusion and the arc are inheritance link with disjunctive multiple inheritance a detailed example is given of the generation of a range of such structure of equivalent performance for a simple problem and the complexity measure of a particular structure is shown to relate to it perceived complexity the simplest structure are generated by an algorithm that factor common sub premise from the premise of rule a more complex example of a chess dataset is used to show the value of this technique in generating comprehensible knowledge structure 
the problem of discovering association rule ha received considerable research attention and several fast algorithm for mining association rule have been developed in practice user are often interested in a subset of association rule for example they may only want rule that contain a specific item or rule that contain child of a specific item in a hierarchy while such constraint can be applied a a postprocessing step integrating them into the mining algorithm can dramatically reduce the execution time we consider the problem of integrating constraint that n 
this paper present a method for word sense disambiguation and coherence understanding of prepositional relation the method relies on information provided by wordnet we first classify prepositional attachment according to semantic equivalence of phrase head and then apply inferential heuristic for understanding the validity of prepositional structure 
this paper describes an efficient algorithm for the segmentation of echo cluster within a dynamic d sonar image the sensor centered image is an echo management framework grouping sonar return into spherical cell and allowing real time organisation of d range data using inexpensive equipment each cell act a a spatial key to the feature related to this location the spherical representation is effectively exploited for segmentation using an approach motivated from connected component analysis in binary video image a fast algorithm linear in time complexity based on cell connectivity between sonar beam is presented including method for coping with sparse data 
several pattern discovery method proposed in the data mining literature have the drawback that they discover too many obvious or irrelevant pattern and that they do not leverage to a full extent valuable prior domain knowledge that decision maker have in this paper we propose a new method of discovery that address these drawback in particular we propose a new method of discovering unexpected pattern that take into consideration prior background knowledge of decision maker this prior knowledge constitutes a set of expectation or belief about the problem domain our proposed method of discovering unexpected pattern us these belief to seed the search for pattern in data that contradict the belief to evaluate the practicality of our approach we applied our algorithm to consumer purchase data from a major market research company and to web logfile data tracked at an academic web site and present our finding in the paper 
new representation are developed for d ip implicit polynomial curve of arbitrary degree these representation permit shape recognition and pose estimation with essentially single rather than iterative computation and extract and use all the information in the polynomial coefficient this is accomplished by decomposing polynomial coefficient space into a union of orthogonal subspace for which rotation within two dimensional subspace or identity transformation within one dimensional subspace result from rotation in x y measured data space these rotation in the two dimensional coefficient subspace are related in simple way to each other and to rotation in the x y data space by recasting this approach in term of complex polynomial i e z x iy and complex coefficient further simplification occurs for rotation and some simplification occurs for translation 
nonlinear dimensionality reduction is formulated here a the problem of trying to find a euclidean feature space embedding of a set of observation that preserve a closely a possible their intrinsic metric structure the distance between point on the observation manifold a measured along geodesic path our isometric feature mapping procedure or isomap is able to reliably recover low dimensional nonlinear structure in realistic perceptual data set such a a manifold of face image where conventional global mapping method find only local minimum the recovered map provides a canonical set of globally meaningful feature which allows perceptual transformation such a interpolation extrapolation and analogy highly nonlinear transformation in the original observation space to be computed with simple linear operation in feature space 
in this paper we describe an algorithm for object recognition that explicitly model and estimate the posterior probability function p object image we have chosen a functional form of the posterior probability function that capture the joint statistic of local appearance and position on the object a well a the statistic of local appearance in the visual world at large we lise a discrete representation of local appearance consisting of approximately pattern we compute an estimate of p object image in closed form by counting the frequency of occurrence of these pattern over various set of training image we have used this method for detecting human face from frontal and profile view the algorithm for frontal view ha shown a detection rate of with false alarm on a set of image containing face combining the mit test set of sung and poggio with the cmu test set of rowley baluja and kanade the algorithm for detection of profile view ha also demonstrated promising result 
representing shape is a significant problem for vision system that must recognize or classify object we derive a representation for a given shape by investigating it self similarity and constructing it shape axis sa and shape axis tree sa tree we start with a shape it boundary contour and two different parameterizations for the contour to measure it self similarity we consider matching pair of point and their tangent along the boundary contour i e matching the two parameterizations the matching of self similarity criterion may vary e g co circularity parallelism distance region homogeneity the locus of middle point of the pairing contour point are the shape axis and they can be grouped into a unique tree graph the sa tree the shape axis for the co circularity criterion is compared to the symmetry axis an interpretation in term of object part is also presented 
a circuit for fast compact and low power focal plane motion centroid localization is presented this chip which us mixed signal cmos component to implement photodetection edge detection on set detection and centroid localization model the retina and superior colliculus the centroid localization circuit us time windowed asynchronously triggered row and column address event and two linear resistive grid to provide the analog coordinate of the motion centroid this vlsi chip is used to realize fast lightweight autonavigating vehicle the obstacle avoiding line following algorithm is discussed 
this paper introduces a unified approach to the problem of verifying alignment hypothesis in the presence of substantial amount of uncertainty in the predicted location of projected model feature our approach is independent of whether the uncertainty is distributed or bounded and moreover incorporates information about the domain in a formally correct manner information which can be incorporated includes the error model the distribution of background feature and the position of the data feature near each predicted model feature experiment are described that demonstrate the improvement over previously used method furthermore our method is efficient in that the number of operation is on the order of the number of image feature that lie nearby the predicted model feature 
we explore the possibility of importing blackbox model learned over data source at remote site to improve model learned over locally available data source in this way we may be able to learn more accurate knowledge from globally available data than would otherwise be possible from partial locally available data proposed meta learning strategy in our previous work are extended to integrate local and remote model we also investigate the effect on accuracy performance when data overlap among different site 
a new method for the representation recognition and interpretation of parameterized gesture is presented by parameterized gesture we mean gesture that exhibit a meaniful variation one example is a point gesture where the important parameter is the dimnesional direction our approach is to extend the standard hidden markov model method of gesture recognition by including a global parametric variation in the output probability of the state of the hmm using a linear model to derive the theory we formulate an expectation maximization em method for training the parametric hmm during testing the parametric hmm simultaneously recognizes the gesture and estimate the quantifying parameter using visually derived and directly measured dimensional hand position measurement a input we present result on two different movement a size gesture and a point gesture and show robustness with respect to noise in the input feature 
in this paper we study the limitation of current verification strategy in object recognition and suggest how they may be enhanced on the whole object topologyis exploited little during verification in practice understanding the connectivity relationship between feature in the image or on the object can l ead to significantly more accurate evaluation of recognition hypothesis we study how topology reasoning allows u to hypothesize the presence of occlusion in the image analysis of these hypothesis provides information which turn out to be crucial to the quality of our overall verification result 
human use visual a well a auditory speech signal to recognizespoken word a variety of system have been investigated for performingthis task the main purpose of this research wa to systematicallycompare the performance of a range of dynamic visualfeatures on a speechreading task we have found that normalizationof image to eliminate variation due to translation scale and planar rotation yielded substantial improvement in generalizationperformance regardless of the visual 
visual cognition depends critically on the ability to make rapid eye movementsknown a saccade that orient the fovea over target of interest in a visualscene saccade are known to be ballistic the pattern of muscle activationfor foveating a prespecified target location is computed prior to the movementand visual feedback is precluded despite these distinctive property therehas been no general model of the saccadic targeting strategy employed bythe human visual system during 
a general geometrical framework for image processing is presented we consider intensity image a surface in the x i space the image is thereby a two dimensional surface in three dimensional space for gray level image the new formulation unifies many classical scheme algorithm and measure via choice of parameter in master geometrical measure more important it is a simple and efficient tool for the design of natural scheme for image enhancement segmentation and scale space here we give the basic motivation and apply the scheme to enhance image we present the concept of an image a a surface in dimension higher than the three dimensional intuitive space this will help u handle movie color and volumetric medical image 
we develop a recursive node elimination formalismfor efficiently approximating large probabilisticnetworks no constraint are set on thenetwork topology yet the formalism can bestraightforwardly integrated with exact methodswhenever they are become applicable theapproximations we use are controlled theymaintain consistently upper and lower boundson the desired quantity at all time we showthat boltzmann machine sigmoid belief network or any combination i e chain 
in interactive data mining it is advantageous to have condensed representation of data that can be used to efficiently answer different query in this paper we show how frequent set can be used a a condensed representation for answering various type of query given a table r with value and a threshold oe a frequent set of r is a set x of column of r such that at least a fraction oe of the row of r have a in all the column of x finding frequent set is a first step in finding 
abstract one of the main hurdle to improved clir ef fectiveness is resolving ambiguity associated with translation availability of resource is also a problem first we present a technique based on co occurrence statistic from unlinked cor pora which can be used to reduce the ambiguity associated with phrasal and term translation we then combine this method with other technique for reducing ambiguity and achieve more than monolingual effectiveness finally we compare the co occurrence method with parallel corpus and machine trans lation technique and show that good retrieval effectiveness can be achieved without complex resource 
business user and analyst commonly use spreadsheet and d plot to analyze and understand their data on line analytical processing olap provides these user with added flexibility in pivoting data around dierent attribute and drilling up and down the multi dimensional cube of aggregation machine learning researcher however have concentrated on hypothesis space that are foreign to most user hyperplanes perceptrons neural network bayesian network decision tree nearest neighbor etc in this paper we advocate the use of decision table classiers that are easy for line of business user to understand we describe several variant of algorithm for learning decision table compare their performance and describe a visualization mechanism that we have implemented in mineset the performance of decision table is comparable to other known algorithm such a c c yet the resulting classiers use fewer attribute and are more comprehensible 
pruning is a common technique to avoid over tting in decision tree most pruning technique do not ac count for one important factor multiple compar isons multiple comparison occur when an induction algorithm examines several candidate model and se lects the one that best accord with the data mak ing multiple comparison produce incorrect inference about model accuracy we examine a method that ad justs for multiple comparison when pruning decision tree bonferroni pruning in experiment with ar ti cial and realistic datasets bonferroni pruning pro duce smaller tree that are at least a accurate a 
we present a new method for extracting and classifying motion pattern to recognize hand gesture first motion segmentatzon of the image sequence is generated based on a muttiscale transform and attributed graph matching of region across frame this produce region correspondence and their affine transformation second color information of motion region is used to determine skin region third human head and palm region are identified based on the shape and size of skin area in motion finally affine transformation defining a region s motion between successive frame are concatenated to construct the region s motion trajectory gestural motion trajectory are then classified by a time delay neural network trained with backpropagation learning algorithm our experimental result show that hand gesture can be recognized well using motion pattern 
this paper discus research on distinguishing word meaning in the context of information retrieval system we conducted experiment with three source of evidence for making these distinction morphology part of speech and phrase we have focused on the distinction between homonymy and polysemy unrelated v related meaning our result support the need to distinguish homonymy and polysemy we found grouping morphological variant make a significant improvement in retrieval performance that more than half of all word in a dictionary that differ in part of speech are related in meaning and that it is crucial to assign credit to the component word of a phrase these experiment provide better understanding of word based method and suggest where natural language processing can provide further improvement in retrieval performance 
partial collection replication improves performance and scalability of a large scale distributed information retrieval system by distributing excessive workload reducing network latency and restricting some search to a small percentage of data in this paper we first examine query from real system log and show that there is sufficient query locality in real system to justify partial collection replication we then present a method for constructing a hierarchy of partial replica from a collection where each replica is a subset of all larger replica and extend the inference network model to rank and select partial replica we compare our new selection algorithm to previous work on collection selection over a range of tuning parameter for a given query our replica selection algorithm correctly determines the most relevant of the replica or original collection and thus maintains the highest retrieval effectiveness while searching the least data a compared with the other ranking function simulation result show that with load balancing partial replication consistently improves performance over collection partitioning on multiple disk of a shared memory multiprocessor and it requires only modest query locality 
in many database marketing application the goal is to predict the customer behavior based on their previous action a usual approach is to develop model which maximize accuracy on the training and test set and then apply these model on the unseen data we show that in order to maximize business payoff accuracy optimization is insufficient by itself and explore different strategy to take the customer value into account we propose a framework for comparing payoff of different model and use it to compare a number of different approach for selecting the most valuable subset of customer for the two datasets that we consider we find that explicit use of value information during the training process and stratified modelling based on value both perform better than post processing strategy base rate giving the model a zifc of for a large customer base even small improvement in prediction accuracy can yield large improvement in lift in this paper we argue that lift measure by itself is not sufficient and that we should take the customer value into account in order to determine the model payoff using the predicted behavior and a simple business model we estimate the payoff from different model and examine different strategy to arrive at an optimal model that maximizes overall business value rather than just accuracy or lift in the rest of this paper we explain the business problem and model of business payoff present the main experimental hypothesis result and conclusion 
autonomous mobile robot need good model of their environment sensor and actuator to navigate reliably and efficiently while this information can be supplied by human or learned from scratch through active exploration such approach are tedious and time consuming our approach is to provide the robot with the topological and geometrical constraint that are easily obtainable by human and have the robot learn the rest while in the course of performing it task we present grow bw an unsupervised and passive distance learning algorithm that overcomes the problem that the robot can never be sure about it location if it is not allowed to reduce it uncertainty by asking a teacher or executing localization action advantage of grow bw include that the robot can be used immediately to perform navigation task and improves it performance over time focusing it attention to route that are more relevant for it task we demonstrate that grow bw can learn good distance sensor and actuator model with only a small amount of experience 
in data mining the goal is to develop method for discovering previously unknown regularity from database the resulting model are interpreted and evaluated by domain expert but some model evaluation criterion is needed also for the model construction process the optimal choice would be to use the same criterion a the human expert but this is usually impossible a the expert are not capable of expressing their evaluation criterion formally on the other hand it seems reasonable to assume that any model posnp nn cl nn nl a ef l mfl n nw nlo ulg ia l lqja urvy i uanul u ilxmal uu am capture some structure of the reality for this reason in predictive data mining the search for good model is guided by the expected predictive error of the model in this paper we describe the bayesian approach to predictive data mining in the finite mixture modeling framework the finite mixture model family is a 
the use of hand gesture provides an attractive mean of interacting naturally with a computer generated display using one or more video camera the hand movement can potentially be interpreted a meaningful gesture one key problem in building such an interface without a restricted setup is the ability to localize and track the human arm robustly in image sequence this paper proposes a multiple cue based localization scheme combined with a tracking framework to reliably track the human ann dynamic in unconstrained environment the localization scheme integrates the multiple cue of motion shape and color for locating a set of key image feature using constraint fusion these feature are tracked by a modified extended kalman filter that exploit the articulated structure of the arm we also propose an interaction scheme between tracking and localization for improving the estimation process while reducing the computational requirement the performance of the framework is validated with the help of extensive experiment and simulation 
this paper describes a robust segmentation algorithm for the detection and localization of woven fabric defect the essence of the presented segmentation algorithm is the localization of those event i e defect in the input image that disrupt the global homogeneity of the background texture to this end preprocessing module based on the wavelet transform and edge fusion are employed with the objective of attenuating the background texture and accentuating the defect then texture feature are utilized to measure the global homogeneity of the output image if these image are deemed to be globally nonhomogenous i e defect are present a local roughness measure is used to localize the defect the utility of this algorithm can be extended beyond the specific application in our work that is defect segmentation in woven fabric indeed in a general sense this algorithm can be used to detect and to localize anomaly that reside in image characterized by ordered texture the efficacy of this algorithm ha been tested thoroughly under realistic condition and a a part of an on line fabric inspection system using over image offabrics containing different type of defect the overall detection rate of our approach wa with a localization accuracy of le than inch and a false alarm rate of 
dept of comput inf sci pennsylvania univ philadelphia pa usa abstract we present a new method for the d model based tracking of human body part to mitigate the difficulty arising due to occlusion among body part we employ multiple calibrated camera in a mutually orthogonal configuration in addition we develop criterion for a time varying active selection of a set of camera to track the motion of a particular human part in particular at every frame each camera track a number of part depending on the visibility of these part and the observability of their predicted motion from the specific camera to relate point on the occluding contour of the part to point on their model we apply concept from projective geometry then within the physic based framework we compute the generalized force applied from the part occluding contour to model point of the body part these force update the translational and rotational degree of freedom of the model such a to minimize the discrepancy between the sensory data and the estimated model state we present initial tracking result from a series of experiment involving the recovery of complex d motion in the presence of significant occlusion 
abstract a constrained optimization method called the lagrange hopfield lh method is presented for solving markov random field mrf based bayesian image estimation problem for restoration and segmentation the method combine the augmented lagrangian multiplier technique with the hopfield network to solve a constrained optimization problem into which the original bayesian estimation problem is reformulated the lh method effectively overcomes instability that are inherent in the penalty method e g hopfield network or the lagrange multiplier method in constrained optimization an additional advantage of the lh method is it suitability for neural like analog implementation experimental result are presented which show that lh yield good quality solution at reasonable computational cost 
this paper present a task oriented evaluation methodology for edge detector performance is measured based on the task of structure from motion eighteen real image sequence from different scene varying in the complexity and scenery type are used the task level ground truth for each image sequence is manually specified in term of the d motion and structure an automated tool computes the accuracy of the motion and structure achieved using the set of edge map parameter sensitivity and execution speed are also analyzed four edge detector are compared all implementation and data set are publicly available 
we propose local error estimate together with algorithm for adaptivea posteriori grid and time refinement in reinforcement learning we consider a deterministic system with continuous state andtime with infinite horizon discounted cost functional for grid refinementwe follow the procedure of numerical method for thebellman equation for time refinement we propose a new criterion based on consistency estimate of discrete solution of the bellmanequation we demonstrate that an 
we describe a new iterative method for parameter estimation of gaussian mixture the new method is based on a framework developed by kivinen and warmuth for supervised on line learning in contrast to gradient descent and em which estimate the mixture s covariance matrix the proposed method estimate the inverse of the covariance matrix furthermore the new parameter estimation procedure can be applied in both on line and batch setting we show experimentally that it is typically faster than em and usually requires about half a many iteration a em we also describe experiment with digit recognition that demonstrate the merit of the on line version 
in this paper we propose a method for learning bayesian belief network from data the method usesartificial neural network a probability estimator thus avoiding the need for making prior assumptionson the nature of the probability distribution governing the relationship among the participating variable this new method ha the potential for being applied to domain containing both discrete andcontinuous variable arbitrarily distributed we compare the learning performance of this 
this work focus on creating a framework for objectively evaluating the performance of range image segmentation algorithm the algorithm are evaluated in term of correct segmentation overand undersegmentation missed and noise region a set of image with ground truth wa created for this work the image were captured using a structured light scanner image used in the evaluation contain planar spherical cylindrical toroidal andconical surface patch the different surface patch in each image were manually identified to establish ground truth for performance evaluation two segmentation algorithm from the literature are compared 
we develop a vision system for highly mobile autonomous agent that is capable of dynamic obstacle avoidance we demonstrate the robust performance of the system in artificial animal with directable foveated eye situated in physic based virtual world through active perception each agent control it eye and body by continuously analyzing photorealistic binocular retinal image stream the vision system computes stereo disparity and segment looming target in the low resolution visual periphery while controlling eye movement to track an object fixated in the high resolution fovea it match segmented target against mental model of colored object of interest in order to decide whether the segmented object are harmless or represent dangerous obstacle the latter are localized enabling the artificial animal to exercise the sensorimotor control necessary to avoid collision 
it is well known that for markov decision process the policy stable under policy iteration and the standard reinforcement learning method are exactly the optimal policy in this paper we investigate the condition for policy stability in the more general situation when the markov property cannot be assumed we show that for a general class of non markov decision process if actual return monte carlo credit assignment is used with undiscounted return we are still guaranteed the optimal observation based policy will be equilibrium point in the policy space when using the standard direct reinforcement learning approach however if either discounted reward or a temporal difference style of credit assignment method is used this is not the case 
the prediction of rna secondary structure on the basis of sequence information is an important tool in biosequence analysis however it ha typically been restricted to molecule containing no more than nucleotide due to the computational complexity of the underlying dynamic programming algorithm used we desribe here an approach to rna sequence analysis based upon scalable computer which enables molecule containing up to nucleotide to be analysed we apply the approach to investigation of the entire hiv genome illustrating the power of these method to perform knowledge discovery by identification of important secondary structure motif within rna sequence family the molecule a documented by their extensive use for the interpretation of molecular evolution data 
in language modeling for speech recognition the goal is to constrain the search of the speech recognizer by providing a model which can given a context indicate what the next most likely word will be in this paper we explore how the addition of information to the text in particular part of speech and dysfluency annotation can be used to build more complex language model in particular we ask two question first in conversational speech where there is a le clear notion of sentence than in written text doe segmenting the text into linguistically or semantically based unit contribute to a better language model than merely segmenting based on broad acoustic information such a pause second is the sentence itself a good unit to be modeling or should we look at smaller unit for example dividing a sentence into a given and new portion and segmenting out acknowledgment and reply to answer these question we present a variety of kind of analysis from vocabulary distribution to perplexity on language model the next step will be modeling conversation and incorporating those model into a speech recognizer 
bilateral filtering smooth image while preserving edge by mean of a nonlinear combination of nearby image value the method is noniterative local and simple it combine gray level or color based on both their geometric closeness and their photometric similarity and prefers near value to distant value in both domain and range in contrast with filter that operate on the three band of a color image separately a bilateral filter can enforce the perceptual metric underlying the cie lab color space and smooth color and preserve edge in a way that is tuned to human perception also in contrast with standard filtering bilateral filtering produce no phantom color along edge in color image and reduces phantom color where they appear in the original image 
we report the result of an experimental study in which human subject are asked to demarcatethe contour of a sample of natural image by analysing the statistic of these contour wederive a set of empirical probability distribution for three contour grouping cue proximity goodcontinuation curvature and brightness similarity we show that of these three cue proximityis by far the most powerful followed by good continuation and finally brightness similarity usinga 
database integration of mining is becoming increasingly important with tile installation of larger and larger data warehouse built around relational database technology most of the commercially available mining system integrate loosely typically through an odbc or sql cursor interface with data stored in dbms in case where the mining algorithm make nmltiple pass over the data it is also possible to cache the data in fiat file rather than retrieve multiple time from the dbms to achieve better performance recent study have found that for association rule mining with carefully tuned sql forinulations it is possible to achieve performance comparable to system that cache the data in file outside the dbms the sql implementation ha potential for offering other qualitauve advantage like automatic parallehzation development ease portability and inter operability with relational operator in this paper we present several alternative for formulating a sql query association rule generalized to handle item with hierarchy on them and sequential pattern mining this work illustrates that it is possible to express computation that are significantly more complicated than simple boolean assom tions in sql using essentially the same franmwork 
we present a framework for recognizing isolated and continuous american sign language asl sentence from three dimensional data the data are obtained by using physic based three dimensional tracking method and then presented a input to hidden markov model hmms for recognition to improve recognition performance we model context dependent hmms and present a novel method of coupling three dimensional computer vision method and hmms by temporally segmenting the data stream with vision method we then use the geometric property of the segment to constrain the hmm framework for recognition we show in experiment with a sign vocabulary that three dimensional feature outperform two dimensional feature in recognition performance furthermore we demonstrate that context dependent modeling and the coupling of vision method and hmms improve the accuracy of continuous asl recognition 
multilayer architecture such a those used in bayesian belief network and helmholtz machine provide a powerful framework for representing and learning higher order statistical relation among input because exact probability calculation with these model are often intractable there is much interest in finding approximate algorithm we present an algorithm that efficiently discovers higher order structure using em and gibbs sampling the model can be interpreted a a stochastic recurrent 
we are frequently called upon to perform multiple task that compete for our attention and resource often we know the optimal solution to each task in isolation in this paper we describe how this knowledge can be exploited to efficiently find good solution for doing the task in parallel we formulate this problem a that of dynamically merging multiple markov decision process mdps into a composite mdp and present a new theoretically sound dynamic programming algorithm for finding an optimal policy for the composite mdp we analyze various aspect of our algorithm and illustrate it use on a simple merging problem every day we are faced with the problem of doing multiple task in parallel each of which competes for our attention and resource if we are running a job shop we must decide which machine to allocate to which job and in what order so that no job miss their deadline if we are a mail delivery robot we must find the intended recipient of the mail while simultaneously avoiding fixed obstacle such a wall and mobile obstacle such a people and still manage to keep ourselves sufficiently charged up frequently we know how to perform each task in isolation this paper considers how we can take the information we have about the individual task and combine it to efficiently find an optimal solution for doing the entire set of task in parallel more importantly we describe a theoretically sound algorithm for doing this merging dynamically new task such a a new job arrival at a job shop can be assimilated online into the solution being found for the ongoing set of simultaneous task 
we introduce an algorithm for scope resolution in underspecified semantic representation scope preference are suggested on the basis of semantic argument structure the major novelty of this approach is that while maintaining an scopally underspecified semantic representation we at the same time suggest a resolution possibility the algorithm ha been implemented and tested in a large scale system and fared quite well of the utterance were ambiguous of these were correctly interpreted leaving error in only of the utterance set 
in this paper we describe the jam system a distributed scalable and portable agent based data mining system that employ a general approach to scaling data mining application that we call meta learning jam provides a set of learning program implemented either a java applet or application that compute model over data stored locally at a site jam also provides a set of meta learning agent for combining multiple model that were learned perhaps at different site it employ a special distribution mechanism which allows the migration of the derived model or classifier agent to other remote site we describe the overall architecture of the jam system and the specific implementation currently under development at columbia university one of jam s target application is fraud and intrusion detection in financial information system a brief description of this learning task and jam s applicability are also described interested user may download jam from http www c columbia edu sal jam pro ject 
using result from robust kalman filtering we present a new kalman filter based snake model for tracking of nonrigid object in combined spatio velocity space the proposed model is the stochastic version of the velocity snake which is an active contour model for combined tracking of position and velocity of nonrigid boundary the proposed model us image gradient and optical flow measurement along the contour a system measurement an optical flow based measurement error is used to detect and reject image measurement which correspond to image clutter or to other object the method wa applied to object tracking ofboth rigid and nonrigid object resulting in good tracking result and robustness to image clutter occlusion and numerical noise 
adaptive ridge is a special form of ridge regression balancing the quadratic penalization on each parameter of the model it wa shown to be equivalent to lasso least absolute shrinkage and select ion operator in the sense that both procedure produce the same estimate lasso can thus be viewed a a particular quadratic penalizer from this observation we derive a fixed point algorithm to compute the lasso solution the analogy provides also a new hyperparameter for tuning the model complexity we finally present a series of possible extension of lasso to non linear regression problem kernel re gression additive modeling and neural net training 
in this paper we present an evaluation of a robust visual image tracker on echocardiographic image sequence we show how the tracking framework can be customised to define an appropriate shape space that describes heart shape deformation that can be learnt from a training data set we also investigate an energy based temporal boundary enhancement method to improve image feature measurement preliminary result are presented demonstrating tracking on real normal heart motion data sequence and synthesised and real abnormal heart motion data sequence we conclude by discussing some of our current research effort 
parti game moore a moore b moore and atkeson is a reinforcement learning rl algorithm that ha a lot of promise in overcoming the curse of dimensionality that can plague rl algorithm when applied to high dimensional problem in this paper we introduce modification to the algorithm that further improve it performance and robustness in addition while parti game solution can be improved locally by standard local path improvement technique we introduce an add on 
this paper present a new approach to speech recognition with hybrid hmm ann technology while the standard approach to hybrid hmm ann system is based on the use of neural network a posterior probability estimator the new approach is based on the use of mutual information neural network trained with a special learning algorithm in order to maximize the mutual information between the input class of the network and it resulting sequence of firing output neuron during training it is shown in this paper that such a neural network is an optimal neural vector quantizer for a discrete hidden markov model system trained on maximum likelihood principle one of the main advantage of this approach is the fact that such neural network can be easily combined with hmm s of any complexity with context dependent capability it is shown that the resulting hybrid system achieves very high recognition rate which are now already on the same level a the best conventional hmm system with continuous parameter and the capability of the mutual information neural network are not yet entirely exploited hybrid hmm ann system deal with the optimal combination of artificial neural network ann and hidden markov model hmm for dynamic pattern recognition task especially in the area of automatic speech recognition it ha been shown that hybrid approach can lead to very powerful and efficient system combining the discriminative capability of neural network and the superior dynamic time warping ability of hmm s the most popular hybrid approach is described in hochberg and replaces the component modeling the emission probability of the hmm by a neural net this is possible because it is shown in bourlard that neural network can be trained so that the output of the m th neuron approximates the posterior probability p m x in this paper an alternative method for constructing a hybrid system is presented it is based on the use of discrete hmm s which are combined with a neural vector quantizer vq in order to form a hybrid system each speech feature vector is 
a data mining system dbminer ha been developed for interactive mining of multiple level knowledge in large relational database the system implement a wide spectrum of data mining function including generalization characterization association classification and prediction by incorporating several interesting data mining technique including attributeoriented induction statistical analysis progressive deepening for mining multiple level knowledge and meta rule guided mining the system provides a userfriendly interactive data mining environment with good performance 
this paper investigates learning in a lifelong context lifelong learning address situation in which a learner face a whole stream of learning task such scenario provide the opportunity to transfer knowledge across multiple learning task in order to generalize more accurately from le training data in this paper several different approach to lifelong learning are described and applied in an object recognition domain it is shown that across the board lifelong learning approach generalize consistently more accurately from le training data by their ability to transfer knowledge across learning task 
fixation is defined a the ability of an active visual system to keep the projection of an environmental point stationary in the image we show in this paper that fixation enables the decoupling of the d motion parameter by projecting appropriately the spherical motion field in two latitudinal direction with respect to two different pole of the image sphere both computational step are based on one dimensional search along meridian of the image sphere we do not use the efference copy of the fixational rotation of the camera performance of the algorithm is tested on real world sequence with fixation accomplished either off line or during the recording using an active camera 
the parameter space of neural network ha the riemannian metricstructure the natural riemannian gradient should be used insteadof the conventional gradient since the former denotes the steepestdescent direction of a loss function in the riemannian space thebehavior of the stochastic gradient learning algorithm is much moreeffective if the natural gradient is used the present paper studiesthe information geometrical structure of perceptrons and othernetworks and prove that the 
in this work we present a new bottom up algorithm for decision tree pruningthat is very efficient requiring only a single pas through the given tree andprove a strong performance guarantee for the generalization error of the resultingpruned tree we work in the typical setting in which the given tree t may havebeen derived from the given training sample s and thus may badly overfit s in this setting we give bound on the amount of additional generalization errorthat our 
in supervised learning there is usually a clear distinction betweeninputs and output input are what you will measure outputsare what you will predict from those measurement this papershows that the distinction between input and output is not thissimple some feature are more useful a extra output than asinputs by using a feature a an output we get more than just thecase value but can learn a mapping from the other input to thatfeature for many feature this mapping 
a system of coupled differential equation is formulated which learns prior for modelling preattentive texture it is derived from an energy functional consisting of a linear combination of a large number of term corresponding to the feature that the system is capable of learning the system learns the parameter associated with each feature by applying gradient ascent to the log likelihood function update of each parameter is thus governed by the residual with respect to the corresponding feature a feature residual is computed from it observed value and value generated by the system the latter is calculated from a synthesized sample image which is generated by mean of a reaction diffusion equation obtained by applying gradient descent to the energy functional 
in this paper we develop a representation for the temporal structure inherent in human action and demonstrate an effective method for using that representation to detect the occurrence of action the temporal structure of the action sub action event and sensor information is described using a constraint network based on allen s interval algebra we map these network onto a simpler valued domain past now fut network a pnf network to allow fast detection of action and sub action the occurrence of an action is computed by considering the minimal domazn of it pnf network under constraint imposed by the current state of the sensor and the previous state of the network we illustrate the approach with example showing that a major advantage of pnf propagation is the detection and removal of inconsistent situation 
the problem of maximi ing the expected total discounted reward in a completely observable markovian environmen t i e a markov decision process mdp model a particular class of sequential decision problem algorithm have been developed for making optimal decision in mdps given either an mdp specification or the opportunity to interact with the mdp over time recently other sequential decision making prob lem have been studied prompting the development of new algorithm and analysis we describe a new generalized model that subsumes mdps a well a many of the recent variation we prove some basic result concerning this model and develop general izations of value iteration policy iteration model based reinforcement le arning and 
due to illumination variability the same object can appear dramatically different even when viewed in fixed pose to handle this variability an object recognition system must employ a representation that is either invariant to or model this variability this paper present an appearance based method for modeling the variability due to illumination in the image of object the method differs from past appearance based method however in that a small set of training image is used to generate a representation the illumination cone which model the complete set of image of an object with lambertian reflectance map under an arbitrary combination of point light source at infinity this method is both an implementation and extension an extension in that it model cast shadow of the illumination cone representation proposed in the method is tested on a database of image of face and the result exceed those of popular existing method 
we present a novel method of velocity field estimation for point on moving contour in an image sequence the method determines the corresponding point in the next image frame by considering curvature change at each point on a contour in previous method there are error in estimation for the point which have low curvature variation since those method compute the solution by approximatingthe normal component of optical flow the proposed method computes optical flow vector of contour point by minimizing the curvature change a a first step snake are used to locate smooth curve in d imagery then the extracted curve are tracked continuously we excluded the rearranging process in snake and allowed the snaxel distance to vary each point on a contour ha a unique corresponding point in thenext frame experimental result showed that the proposed method computes accurate optical flow vector for various moving contour 
we present a formal methodology for the integration of optical flow and deformable model the optical flow constraint equation provides a non holonomic constraint on the motion of the deformable model in this augmented system force computed from edge and optical flow are used simultaneously when this dynamic system is solved a model based least square solution for the optical flow is obtained and improved estimation result are achieved the use of a d model reduces or eliminates problem associated with optical flow computation this approach instantiates a general methodology for treating visual cue a constraint on deformable model we apply this framework to human face shape and motion estimation our d deformable face model us a small number of parameter to describe a rich variety of face shape and facial expression we present experiment in extracting the shape and motion of a face from image sequence 
the paper present an agent based surveillance system for use in monitoring scene involving both pedestrian and vehicle the surveillance system supply textual description for the dynamic activity occurring in the d world these are derived by mean of dynamic andprobabilistic inference based on geometric information provided by a vision system that track vehicle and pedestrian the symbolic scene annotation is given at two major level of description the object level and the interobject level at object level each tracked object pedestrian or vehicle is assigned a behaviour agent which us a bayesian network to infer the fundamental feature of the object trajectory and continuously update it textual description the inter object interaction level is interpreted by a situation agent which is created dynamically when two object are in close proximity 
the multilingual information retrieval system of the future will need to be able to retrieve document across language boundary this extension of the classical ir problem is particularly challenging ax significant resource are required to perform query translation at xerox we are working to build a multilingual ir system and conducting a series of experiment to understand what factor are most important in making the system work using translated query and a bilingual transfer dictionary we have learned that crosslartguage multilingual ir is feasible although performance lag considerably behind the monolingual standard the experiment suggest that correct identification and translation of multi word terminology is the single most important source of error in the system although amblguit y in translation also contributes to poor performance 
we propose a novel appr oach to programa robot by demonstrating the task multiple number of time in front of a vision system here we integrate human dexterity with sensory data using computer vision technique in a single platform a simultaneous feature detection and tracking framework is used to track various feature finger tip and the wrist joint a kalman filter doe the tracking by predicting the tentative feature location and a ho based data clustering algorithm extract the feature color information of the feature are used for establishing correspondence a fast efficient and robust algorithm for the vision system thus developed process a binocular video sequence to obtain the trajectory and the orientation information of the end effector the concept of a trajectory bundle is introduced to avoid singularity and to obtain an optimal path 
this paper present an algorithm for detecting localizingand grouping instance of repeated scene element the grouping is representedby a graph where node correspond to individual element andarcs join spatially neighboring element associated with each arc is anaffine map that best transforms the image patch at one location to theother the approach we propose consists of step detecting quot interesting quot element in the image matching element with their neighbor 
application of inductive learning algorithm to realworlddata mining problem have shown repeatedlythat using accuracy to compare classifier is not adequatebecause the underlying assumption rarely hold we present a method for the comparison of classifierperformance that is robust to imprecise class distributionsand misclassification cost the roc convexhull method combine technique from roc analysis decision analysis and computational geometry andadapts them to the particular 
fleximine is a kdd system designed a a testbed for data mining research a well a a generic knowledge discovery tool for varied database domain flexibility is achieved by an open ended design for extensibility enabling integration of existing data mining algorithm new locally developed algorithm and support function such a visualization and preprocessing support for new database is simple currently via sql query to an informix database server with a view of serving remote a well a local user internet availability wa a design goal by implementing the system in java minor modification allow u to run the user end of the system either a a java application or a a java applet 
a modification is described to the use of mean field approximationsin the e step of em algorithm for analysing data from latentstructure model a described by ghahramani among others the modification involves second order taylor approximationsto expectation computed in the e step the potential benefit ofthe method are illustrated using very simple latent profile model introductionghahramani advocated the use of mean field method a a mean to avoid theheavy 
we present a class of pde based algorithm suitable for a wide range of image processing application the technique are applicable to both saltand pepper grey scale noise and full image continuous noise present in black and white image grey scale image texture image and color image at the core the technique rely on a level set formulation of evolving curve and surface and the viscosity in profile evolution essentially the method consists of moving the isointensity contour in a image under curvature dependent speed law to achieve enhancement compared to existing technique our approach ha several distinct advantage first it contains only one enhancement parameter which in most case is automatically chosen second the scheme automatically stop smoothing at some optimal point continued application of the scheme produce no further change third the method is one of the fastest possible scheme based on a curvature controlled approach 
how similar are two corpus a measure of corpus similarity would be very useful for nlp for many purpose such a estimating the work involved in porting a system from one domain to another first we discus difculties in identifying what we mean by corpus similarity human similarity judgement are not negrained enough corpus similarity is inherently multidimensional and similarity can only be interpreted in the light of corpus homogeneity we then present an operational denition of corpus similarity which address or circumvents the problem using purposebuilt set of known similarity corpus these ksc set can be used to evaluate the measure we evaluate the measure described in the literature including three variant of the information theoretic measure perplexity a based measure using word frequency is shown to be the best of those tested 
in this paper we propose a method for locating d position of a soccer ball from monocular image sequence of soccer game toward this goal we adopted ground model to image transformation together with physic based approach that a ball follows the parabolic trajectory in the air by using the transformation the height of a ball can be easily calculated using simple triangular geometric relation given the start and the end position of the ball on the ground here the height of a ball are determined in term of a player s height even if the end position of a ball is not given on the ground due to kicking or heading of a falling ball before it touch the ground the most probable trajectory can be determined by searching based on the physical fact that the ball follows a parabolic trajectory in the air we have tested and experimented with a real image sequence the result of which seem promising 
this paper describes how to increase the efficiency of inductive data mining algorithm by replacing the central matching operation with a marker propagation technique breadth first marker propagation is most beneficial when the data are linked to hierarchical background knowledge e g tree structured attribute or when the attribute describing the data have many value we support our claim analytically with complexity argument and empirically on several large data set we also point out other efficiency gain including reduced memory management overhead which facilitate mining massive tape archive 
backpropagation learning algorithm typically collapse the network s structure into a single vector of weight parameter to be optimized we suggest that their performance may be improved by utilizing the structural information instead of discarding it and introduce a framework for tempering each weight accordingly in the tempering model activation and error signal are treated a approximately independent random variable the characteristic scale of weight change is then matched to that of the residual allowing structural prop erties such a a node s fan in and fan out to affect the local learning rate and backpropagated error the model also permit calculation of an upper bound on the global learning rate for batch update which in turn lead to different update rule for bias v non bias weight this approach yield hitherto unparalleled performance on the family relation benchmark a deep multi layer network for both batch learning with momentum and the delta bar delta algorithm convergence at the optimal learning rate is sped up by more than an order of magnitude 
we consider the problem of feature based face recognition in the setting where only a single example of each face is available for training the mixture distance technique we introduce achieves a recognition rate of on a database of people in which each face is represented by measured distance this is currently the best recorded recognition rate for a feature based system applied to a database of this size by comparison nearest neighbor search using euclidean distance yield in our work a novel distance function is constructed based on local second order statistic a estimated by modeling the training data a a mixture of normal density we report on the result from mixture of several size we demonstrate that a flat mixture of mixture performs a well a the best model and therefore represents an effective solution to the model selection problem a mixture perspective is also taken for individual gaussians to choose between first order variance and second order covariance model here an approximation to flat combination is proposed and seen to perform well in practice our result demonstrate that even in the absence of multiple training example for each class it is sometimes possible to infer from a statistical model of training data a significantly improved distance function for use in pattern recognition 
this paper present a novel technique for the estimation of global nonrigid motion without using point correspondence the completedescription of the nonrigid motion of an object involves specifying a displacement vector at each point of the object such a description provides a large amount of information which need to be processed further in order to study the global characteristic of the deformation nonrigid motion can be studied hierarchically in term of a global nonrigid motion and point by point local nonrigid motion the technique presented in this paper give a method for estimating a global a fine or polynomial transformation between two object the novelty of the technique lie in the fact that it doe not use any point correspondence our method us hyper quadric model to model the data and estimate the global deformation we show that affine or polynomial transformation between two data set can be recovered from the hyper quadric parameter the usefulness of the technique is two fold first it pave the way for viewing nonrigid motion hierarchically in term of global and local motion second it can be used a a front end to other motion analysis technique that assume small motion for instance most nonrigid motion analysis algorithm make some assumption on the type of nonrigid motion conformal motion small motion etc that are not always satisfied in practice when the motion between two data set is large our algorithm can be used to estimate the affine transformation which includes scale and shear or a polynomial transformation between the two data set which can then be used to warp the first data set closer to the second so a to satisfy the small motion assumption we present experiment result with real and synthetic and data 
in an earlier paper we introduced a new boosting algorithm called adaboost which theoretically can be used to significantly reduce the error of any learning algorithm that consistently generates classifier whose performance is a little better than random guessing we also introduced the related notion of a pseudo loss which is a method for forcing a learning algorithm of multi label concept to concentrate on the label that are hardest to discriminate in this paper we describe experiment we carried out to ass how well adaboost with and without pseudo loss performs on real learning problem we performed two set of experiment the first set compared boosting to breiman s bagging method when used to aggregate various classifier including decision tree and single attribute value test we compared the performance of the two method on a collection of machine learning benchmark in the second set of experiment we studied in more detail the performance of boosting using a nearest neighbor classifier on an ocr problem 
the new millennium remote agent nmra will be the first on board ai system to control an actual spacecraft the spacecraft domain raise a number of challenge for planning and execution ranging from extended agency and long term planning to dynamic recovery and robust concurrent execution all in the presence of tight real time deadline changing goal scarce resource constraint and a wide variety of possible failure nmra is one of the first system to integrate closed loop planning and execution of concurrent temporal plan it is also the first autonomous system that will be able to achieve a sustained multi stage multiyear mission without communication or guidance from earth 
this paper present a trainable rule based algorithm for performing word segmentation the algorithm provides a simple language independent alternative to large scale lexical based segmenters requiring large amount of knowledge engineering a a stand alone segmenter we show our algorithm to produce high performance chinese segmentation in addition we show the transformation based algorithm to be effective in improving the output of several existing word segmentation algorithm in three different language 
image sequence analysis for real time application requires high quality and highly efficient algorithm for tracking a there is no time to do the costly object recognition each time a new image is captured tracking with projection histogram revealed amazing result compared with standard correlation method tracker based on projection histogram performed up to better than the reference method on a common test set the new template based method relying on projection histogram rph is described and compared with two commonly known template based method namely the normalized cross correlation ncc and displaced frame distance dfd method the input to the system consists of live or recorded video data where filterbased preprocessing can be applied before tracking in order to enhance feature such a edge texture etc a region of interest roi is taken a a template for tracking in subsequent image tracking exploit a kalman filtered local search in order to renew correspondence between the object template and the new object location comparative test were performed with real live image sequence taken in underground station tracking with projection histogram outperformed tracking by ncc and dfd on grey level image sequence a well a on edge enhanced image sequence even the worst chosen parameter set for tracking by the new rph method resulted in better tracking a with the best one for both ncc and dfd 
a face recognition system is described which employ a fuzzy information fusion technique to increase the over all recognition rate the face image are searched for locating head area and face boundary the eye and mouth are detected using rigid and deformable template assuming a d head model the face rotation are estimated which allows for compensating rotated facial feature back to a front upright view each facial feature form a source of information for classification based on a correlation technique using eye forehead mouth and nose window three classifier are established the output of each classifier is taken a a partial evidence in classification the importance of each source is measured using a fuzzy density measure and the final classification is achieved using a fuzzy evidence aggregation method the performance of the system is evaluated using a combined match score 
we present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a lexicalized tree adjoining grammar ltag this approach capture naturally and elegantly the interaction between pragmatic and syntactic constraint on description in a sentence and the inferential interaction between multiple description in a sentence at the same time it exploit linguistically motivated declarative specification of the discourse function of syntactic construction to make contextually appropriate syntactic choice 
the execution order of a block of computer instruction on a pipelined machine can make a difference in it running time by a factor of two or more in order to achieve the best possible speed compiler use heuristic scheduler appropriate to each specific architecture implementation however these heuristic scheduler are time consuming and expensive to build we present empirical result using both rollouts and reinforcement learning to construct heuristic for scheduling basic block in simulation the rollout scheduler outperformed a commercial scheduler and the reinforcement learning scheduler performed almost a well a the commercial scheduler 
principal curve have been defined a self consistent smooth curve which pas through the middle of a d dimensional probability distribution or data cloud recently we have offered a new approach by defining principal curve a continuous curve of a given length which minimize the expected squared distance between the curve and point of the space randomly chosen according to a given distribution the new definition made it possible to carry out a theoretical analysis of learning principal curve from training data in this paper we propose a practical construction based on the new definition simulation result demonstrate that the new algorithm compare favorably with previous method both in term of performance and computational complexity 
we compare the generalization performance of three distinct rep resentation scheme for facial emotion using a single classi cation strategy neural network the face image presented to the clas si er are represented a full face projection of the dataset onto their eigenvectors eigenfaces a similar projection constrained to eye and mouth area eigenfeatures and nally a projection of the eye and mouth area onto the eigenvectors obtained from x random image patch from the dataset the latter system achieves generalization on novel face image individual the network were not trained on drawn from a database in which human sub jects consistently identify a single emotion for the face 
we devise a statistical framework for edge detection by performing a statistical analysis of zero crossing of the second derivative of an image this analysis enables u to estimate at each pixel of an image the probability that an edge pass through the pixel we present a statistical analysis of the lindeberg operator that we use to compute image derivative we also introduce a confidence probability that tell u how reliable the edge probability is given the image s noise level and the operator s scale combining the edge and confidence probability lead to a probabilistic scale selection algorithm we present the result of experiment on natural image 
we propose a concept of active diagnosis that differs from the conventional passive i e event driven diagnosis in temporal diagnosis is carried out by always monitoring normal condition a opposed to identifying faulty only when abnormal condition is detected sense a well a spatial diagnosis is carried out by agent distributed in the sensor network sense a one way of realizing active diagnosis we present immunity based agent approach based on the self creating monitoring and maintaining feature of immune system we apply the approach to process diagnosis where the agent are defined on the sensor network each agent corresponding to sensor or process constraint evaluates a kind of reliability by communicating other agent system level recognition of sensor process fault can be attained by continuously and mutually monitoring and maintaining consistency among sensor value and process constraint 
the compact description of a video sequence through a single image map and a dominant motion ha application in several domain including video browsing and retrieval compression mosaicing and visual summarization building such a representation requires the capability to register all the frame with respect to the dominant object in the scene a task which ha been in the past addressed through temporally localized motion estimate in this paper we show how the lack of temporal consistency associated with such estimate can undermine the validity of the dominant motion assumption leading to oscillation between different scene interpretation and poor registration to avoid this oscillation we augment the motion model with a generic temporal constraint which increase the robustness against competing interpretation leading to more meaningful content summarization 
the spectral property of outdoor illumination function can vary significantly due to atmospheric condition and scene geometry we show using a statistical analysis of a comprehensive physical model that the variation in outdoor illumination function over both the visible range m m and the visible near infrared range m m can be represented accurately by use of seven dimensional linear model the physical model includes solar and scattered radiation a well a the effect of atmospheric gas and aerosol the modtran code wa employed for computing radiative transfer aspect of the model we show that the new model ha strong agreement over the visible wavelength with the empirical study of judd et al we also demonstrate the accuracy of the model over the m m spectral range using measured outdoor illumination function 
in the literature we find two class of algorithm which on the basis of two view of a scene recover the rigid transformation between the view and subsequently the structure of the scene the first class contains technique which require knowledge of the correspondence or the motion field between the image and are based on the epipolar constraint the second class contains so called direct algorithm which require knowledge about the value of the flow in one direction only and are based on the positive depth constraint algorithm in the first class achieve the solution by minimizing a function representing deviation from the epipolar constraint while direct algorithm find the d motion that when used to estimate depth produce a minimum number of negative depth value this paper present a stability analysis of both class of algorithm the formulation is such that it allows comparison of the robustness of algorithm in the two class a well a within each class specifically a general statistical model is employed to express the function which measure the deviation from the epipolar constraint and the number of negative depth value and these function are studied with regard to their topographic structure specifically a regard the error in the d motion parameter at the place representing the minimum of the function the analysis show that for algorithm in both class which estimate all motion parameter simultaneously the obtained solution ha an error such that the projection of the translational and rotational error on the image plane are perpendicular to each other furthermore the estimated projection of the translation on the image lie on a line through the origin and the projection of the real translation 
metapatterns also known a metaqueries have beenproposed a a new approach to integrated data mining and applied to several real world application successfully however designing the right metapatterns forum given application still remains a difficulty task inthis paper we present a metapattern generator thatcan automatically generate metapatterns from newdatabases by integrating this generator with the existingmetapattern based discovery loop our systemhas now become both 
a new boundary detection approach for shape modeling is presented itdetects the global minimum of an active contour model s energybetween two end point initialization is made easier and the curveis not trapped at a local minimum by spurious edge we modify the snake energy by including the internal regularization term in theexternal potential term our method is based on finding a path ofminimal length in a riemannian metric we then make use of a newefficient numerical method to find this shortest path it is shown that the proposed energy though based only on apotential integrated along the curve imposes a regularization effectlike snake we explore the relation between the maximum curvaturealong the resulting contour and the potential generated from the image the method is capable to close contour given only one point on theobjects boundary by using a topology based saddle search routine we show example of our method applied to real aerial andmedical image 
an unsupervised algorithm for arranging an image database a a binary tree is described tree node are associated with image subset maintaining the property that the similarity among the image associated with the child of a node is higher than the similarity among the image associated with the parent node experiment with datasets of hundred and thousand of image show that shallow tree can produce clustering into meaningful class visual content search tree can be used to automate image retrieval by content orhelp a human to interactively search for image 
because of the size of the world wide web and it inherentlack of structure finding what one is looking for can bea challenge in fact some of the most highly visited website are search engine however while web page typicallycontain both text and image most currently availablesearch engine only index text this paper describeswebseer a system for locating image on the web webseeruses image content in addition to associated text toindex image the image analysis is designed 
this paper proposes a method for detecting obstacle on a runway by controlling their expected disparity by approximating the runway by a planar surface the initial model flow field mff corresponding to an obstacle free runway is described by the data from on board sensor ob the error variance of the initial mff is computed and used to estimate the mff obstacle are detected by comparing the expected residual flow disparity with the residual flow field rff estimated after warping or stabilizing an image using the mff expected temporal and spatial disparity are obtained from the use of the ob this allows u to control the residual disparity by increasing the temporal baseline and or by utilizing the spatial baseline if distant object cannot be detected for a given temporal baseline experimental result for two real flight image sequence are presented 
affine invariant medial ax and symmetry set of planar shape are intr oduced and studied in this paper two different approach are presented the first one is based on affine invariant distance and defines the symmetry set a set containing the medial axis a the closure of the locus of point on at least two affine normal and affine equidistant from the corresponding point on the curve the second approach is based on a affine bitangent conic in this case the symmetry set is defined a the closure of the locus of center of conic with at least three point contact with two or more distinct point on the curve this is equivalent to conic and curve having at those point the same affine tangent or the same euclidean tangent and curvature although the two analogous definition for the classical euclidean symmetry set medial axis are equivalent this is not the case for the affine group we then show how to use the symmetry set to detect affine skew symmetry proving that the contact based symmetry set is a straight line if and only if the given shape is the affine transformation of a symmetric object 
this paper report novel algorithm for the efficient localization and recognition of vehicle in traffic scene which eliminate the need for explicit symbolic feature extraction and matching the algorithm make use of two a priori source of knowledge about the scene and the object i the ground plane constraint and ii the fact that road vehicle are strongly rectilinear the algorithm are demonstrated and tested using routine outdoor traffic image success with a variety of vehicle demonstrates the efficiency and robustness of context based computer vision in road traffic scene the limitation of the algorithm are also addressed in the paper 
this paper present sitecity a semi automated building extraction system integrating photogrammetry geometric constraint and image understanding algorithm existing automated building extraction system produce mixed result and it is clear that human intervention is required to correct mistake from fully automated system sitecity give human operator the ability to construct and manipulate three dimensional building object using multiple image image understanding algorithm are integrated into sitecity to assist user the automated process in sitecity use user delineated roof boundary a cue and attempt to locate the floor of a building and match the building object in other image in addition photogrammetric cue are used to assist automated process these automated process are described and their performance is evaluated illustrating that automated process in sitecity produce comparable performance to that of human subject 
stochastic search ha recently been shown to besuccessful for solving large boolean satisfiabilityproblems however systematic method tendto be more effective in problem domain witha large number of dependent variable that is variable whose truth value are directly determinedby a smaller set of independent variable in systematic search truth value can be efficientlypropagated from the independent to thedependent variable by unit propagation suchpropagation is more 
this paper describes the application of a hybrid neural expert system network to the task of finding significant event in a market research data base neural network trained by backward error propagation are used to classify trend in the time series data a rule system then us these classification knowledge of market research analysis technique and external event which influence the time series to infer the significance of the data the system achieved recall and precision on a test set of month of survey data this wa significantly better than could be achieved by a system using linear regression together with a rule system both system were able to perform analysis of the test data in under minute the manual analysis of the same data took a human expert over four working day 
we introduce a method for learning bayesian network that handle the discretization of continuous variable a an integral part of the learning process the main ingredient in this method is a new metric based on the minimal description length principle for choosing the threshold value for the discretization while learning the bayesian network structure this score balance the complexity of the learned discretization and the learned network structure against how well they model the training data this ensures that the discretization of each variable introduces just enough interval to capture it interaction with adjacent variable in the network we formally derive the new metric study it main property and propose an iterative algorithm for learning a discretization policy finally we illustrate it behavior in application to supervised learning 
we study the effect of correlated noise on the accuracy of populationcoding using a model of a population of neuron that arebroadly tuned to an angle in two dimension the fluctuation inthe neuronal activity is modeled a a gaussian noise with pairwisecorrelations which decay exponentially with the difference betweenthe preferred orientation of the pair by calculating the fisher informationof the system we show that in the biologically relevantregime of parameter positive 
based on our analysis and experiment using real world datasets we find that the greediness of forward feature selection algorithm doe not severely corrupt the accuracy of function approximation using the selected input feature but improves the efficiency significantly hence we propose three greedier algorithm in order to further enhance the efficiency of the feature selection processing we provide empirical result for linear regression locally weighted regression and k nearestneighbor model we also propose to use these algorithm to develop an off line chinese and japanese handwriting recognition system with automatically configured local model 
this paper present first result of an interdisciplinary project in scientific data mining we analyze data about the carcinogenicity of chemical derived from the carcinogenesis bioassay program performed by the u national institute of environmental health science the database contains detailed description of test performed with more than compound and animal of different specie strain and sex the chemical structure are described at the atom and bond level and in term of various relevant strnctural property the goal of this paper is to investigate the effect that various level of detail and amount of information have on the resulting hypothesis both quantitativel and qualitatively we apply relational and propositional machine learning algorithm to learning problem formulated a regression or a classification task in addition these experiment have been conducted with two learning problem which are at different level of detail quantitatively our experiment indicate that additional information nob necessarily improves accuracy q itatively a number of potential discovery have been made by the algorithm for relational regression because it can utilize aii the information contained in the relation of the database a weli a in the numerical dependent variable 
this study uncovers trading style in the transactionrecords of u treasury bond future we usestatistical clustering technique to group togethertrades that are similar trade profit wa heldback in the clustering process result show thatclusters differ significantly in their profit and riskcharacteristics some cluster uncover quot technical quot trading rule using the information about the individualaccounts we describe the assignment ofaccounts to cluster by entropy and model the 
the paper proposes a set of principle and a general architecture that may explain how language and meaning may originate and complexify in a group of physically grounded distributed agent an experimental setup is introduced for concretising and validating specific mechanism based on these principle the setup consists of two robotic head that watch a scene in which a robot move around in it ecosystem the first result from experiment showing the emergence of distinction of a lexicon and of primitive syntactic structure are reported 
we present a new approach for resolving occlusion in augmented reality the main interest is that it doe not require d reconstruction of the considered scene our idea is to use a contour based approach and to label each contour point a being behind or in front of depending on whether it is in front of or behind the virtual object this labeling step only requires that the contour can be tracked from frame to frame a proximity graph is then built in order to group the contour that belong to the same occluding object finally we use some kind of active contour to accurately recover the mask of the occluding object 
in order to grasp an object we need to solve the inverse kinematicsproblem i e the coordinate transformation from the visualcoordinates to the joint angle vector coordinate of the arm althoughseveral model of coordinate transformation learning havebeen proposed they suffer from a number of drawback in humanmotion control the learning of the hand position error feedbackcontroller in the inverse kinematics solver is important this paperproposes a novel model of the 
we present a shape based method of indexing to model aspect from a single intensity image object are assumed to be rigid a model aspect is represented by a d edgemap and the part of the object silhouette part decomposition is derived from a codon representation of the object silhouette invariant feature extracted from each part are then used to index into a hash table to generate model aspect hypothesis knowledge about part is incorporated in voting scheme to order hypothesis for efficient verification of candidate model verification of model aspect hypothesis is carried out by an alignment algorithm that is robust to partial occlusion result of test using model aspect from object demonstrate that accurate recognition can be achieved with very few verification attempt 
research in theory refinement ha shown that biasing a learner with initial approximately correct knowledge produce more accurate result than learning from data alone while technique have been developed to revise logical and connectionist representation little ha been done to revise probabilistic representation bayesian network are well established a a sound formalism for representing and reasoning with probabilistic knowledge and are widely used there ha been a growing interest in the problem of learning bayesian network from data however there is no existing technique for learning or revising bayesian network with hidden variable i e variable not represented in the data that ha been shown to be efficient effective and scalable through evaluation on real data the few technique that exist for revising such network perform a blind search through a large space of revision and are therefore computationally expensive this dissertation present banner a technique for using data to revise a given bayesian network with noisy or and noisy and node to improve it classification accuracy additionally the initial network can be derived directly from a logical theory expressed a propositional horn clause rule banner can revise network with hidden variable and add hidden variable when necessary unlike previous approach to this problem banner employ mechanism similar to those used in logical theory refinement technique for using the data to focus the search for effective modification to the network it can also be used to learn network with hidden variable from data alone we also introduce banner pr a technique for revising the parameter of a bayesian network with noisy or and node that directly exploit the computational efficiency afforded by these model experiment on several real world learning problem in domain such a molecular biology and intelligent tutoring system demonstrate that banner can effectively and efficiently revise network to significantly improve their accuracy and thus learn highly accurate classifier comparison with the naive bayes algorithm show that using the theory refinement approach give banner a substantial edge over learning from data alone we also show that banner pr converges faster and produce more accurate classifier than an existing algorithm for learning the parameter of a network 
tracking the d contour of a moving object ha widely been used in the past year so called active contour model have been proven to be a promising approach to real time tracking of deformable object also tracking d contour which are projection of rigid d object is reduced to tracking deformable d contour there the deformation of the contour are caused by the movement in d and the changing perspective to the camera in this paper a combination of d and d shape description is presented which can be applied to the prediction of change in d contour which are caused by movement in d only coarse d knowledge is provided which is automatically acquired in a training step then the reconstructed d model of the object is used to predict the shape of the d contour thus limitation of the contour point search in the image is possible which reduces the error in the contour extraction caused by heterogenous background the experimental part show that the proposed combination of d and d shape description is efficient and accurate with respect to real time contour extraction and tracking 
the s map is a network with a simple learning algorithm that combinesthe self organization capability of the self organizing map som and the probabilistic interpretability of the generative topographicmapping gtm the simulation suggest that the smapalgorithm ha a stronger tendency to self organize from randominitial configuration than the gtm the s map algorithmcan be further simplified to employ pure hebbian learning withoutchanging the qualitative behaviour of the network 
pac is an interactive system for experimenting with scenario of agent where the agent are modelled a having both cognition and personality a well a a physical realisation the aim of the system is to provide an environment where scenario can quickly and easily be built up varying aspect of agent personality or emotion and agent cognition plan and belief this will allow u to experiment with different combination of agent in different world we can then investigate the effect of various parameter on emergent behaviour of the agent system 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
this paper describes the use of ai planning technique to represent scientific image processing and software tool knowledge to automate knowledge discovery and data mining e g science data analysis of large image database in particular we describe two fielded system the multimission vicar planner mvp which ha been deployed for year and is currently supporting science product generation for the galileo mission mvp ha reduced time to fill certain class of request from hour to minute the automated sar image processing system asip which is currently in use by the dept of geology at asu supporting aeolian science analysis of synthetic aperture radar image asip reduces the number of manual input in science product generation by fold 
this paper address the problem of handlingskewed class distribution within thecase based learning cbl framework wefirst present a a baseline an informationgain weighted cbl algorithm and apply it tothree data set from natural language processing nlp with skewed class distribution although overall performance of thebaseline cbl algorithm is good we show thatthe algorithm exhibit poor performance onminority class instance we then presenttwo cbl algorithm designed to 
this paper present an algorithm for the compilation of regular formalism with rule feature into finite state automaton rule feature are incorporated into the right context of rule this general notion can also be applied to other algorithm which compile regular rewrite rule into automaton 
in this paper we focus on using local d structure for segmentation a tensor descriptor is estimated for each neighbourhood i e for each voxel in the data set the tensor are created from a combination of the output form a set of d quadrature filter the shape of the tensor describe locally the structure of the neighborhood in term of how much it is like a plane a line and a sphere we apply this to segmentation of bone from computer tomography data ct traditional method are based purely on gray level value discrimination and have difficulty in recovering thin bone structure due to so called partial voluming a problem which is present in all such sampled data we illuminate the partial voluming problem by showing that thresholding creates complicated artifact even if the signal is densely enough sampled and can be perfectly reconstructed the unwanted effect of thresholding can be reduced by a change of the signal basis we show that by using additional local structure information can significantly reduce the degree of sampling artifact evaluation of the method on a clinical case is presented the segmentation of a human skull from a ct volume the method show that many of the thin bone structure which disappear in a pure thresholding can be recovered 
a least square method simultaneously solves for the model to sensor suite pose and sensor to sensor registration the development is for a sensor suite containing separate range and optical sensor to address outlier and more generally match finding a statistical method median filtering and a search method local search are developed sensitivity to gaussian noise and the choice of initial pose estimate is investigated on synthetic data both of the matching method are demonstrated on real data 
common object such a people and car comprisemany visual part and attribute yet image based trackingalgorithms are often keyed to only one of a target sidentifying characteristic in this paper we presenta framework for combining and sharing informationamong several state estimation process operating onthe same underlying visual object well known techniquesfor joint probabilistic data association areadapted to yield increased robustness when multipletrackers attuned to 
we describe the problem of finding deviation in largedata base normally explicit information outside thedata like integrity constraint or predefined pattern is used for deviation detection in contrast we approachthe problem from the inside of the data usingthe implicit redundancy of the data we give a formal description of the problem andpresent a linear algorithm for detecting deviation our solution simulates a mechanism familiar to humanbeings after seeing a series of 
we address the problem of locating a gray level pattern in a gray level image the pattern can have been transformed by an affine transformation and may have undergone some additional change we define a difference function based on comparing each pixel of the pattern with a window in the image and search efficiently for transformation that minimize the difference function the search is guaranteed it will always find the transformation minimizing the difference function and not get fooled by a local minimum it is also efficient in that it doe not need to examine every transformation in order to achieve this guarantee this technique can be applied to object location motion tracking optical flow or block based motion compensation in video image sequence compression e g mpeg 
recent work ha seen the emergence of a common framework for parsing categorial grammar cg formalism that fall within the type logical tradition such a the lambek calculus and related system whereby some method of linear logic theorem proving is used in combination with a system of labelling that ensures only deduction appropriate to the relevant grammatical logic are allowed the approach realising this framework however have not so far addressed the task of incremental parsing a key issue in earlier work with flexible categorial grammar in this paper the approach of hepple is modified to yield a linear deduction system that doe allow flexible deduction and hence incremental processing but that hence also suffers the problem of spurious ambiguity this problem is avoided via normalisation 
in this paper we study the problem of combininga chinese thesaurus with a chinese dictionary bylinking the word entry in the thesaurus with theword sens in the dictionary and propose asimilar word strategy to solve the problem themethod is based on the definition given in thedictionary but without any syntactic parsing orsense disambiguation on them at all a a result their combination make the thesaurus specify thesimilarity between sens which account for thesimilarity 
in this paper we study the problem of combining a chinese thesaurus with a chinese dictionary by linking the word entry in the thesaurus with the word sens in the dictionary and propose a similar word strategy to solve the problem the method is based on the definition given in the dictionary but without any syntactic parsing or sense disambiguation on them at all a a result their combination make the thesaurus specify the similarity between sens which account for the similarity between word produce a kind of semantic classification of the sens defined in the dictionary and provides reliable information about the lexical item on which the resource don t conform with each other 
this paper report on corpus based research into the relationship between intonational variation and discourse structure we examine the effect of speaking style read versus spontaneous and of discourse segmentation method text alone versus text and speech on the nature of this relationship we also compare the acoustic prosodic feature of initial medial and final utterance in a discourse segment 
we previously presented a framework for segmentation of complex scene using multiple physical hypothesis for simple image region a consequence of that framework wa a proposal for a new approach to the segmentation of complex scene into region corresponding to coherent surface rather than merely region of similar color herein we present an implementation of this new approach and show example segmentation for scene containing multi colored piece wise uniform object using our approach we are able to intelligently segment scene with object of greater complexity than previous physic based segmentation algorithm the result show that by using general physical model we obtain segmentation that correspond more closely to coherent surface in the scene than segmentation found using only color 
this panel is a celebration of artificial intelligence ai basing it claim to interest on the past accomplishment of ai it highlight some of the new exciting concept and technology that compete for the title the next big thing 
we solve the dynamic of hopfield type neural network which store sequence of pattern close to saturation the asymmetry of the interaction matrix in such model lead to violation of detailed balance ruling out an equilibrium statistical mechanical analysis using generating functional method we derive exact closed equation for dynamical order parameter viz the sequence overlap and correlation and response function in the limit of an infinite system size we calculate the time translation invariant solution of these equation describing stationary limit cycle which lead to a phase diagram the effective retarded self interaction usually appearing in symmetric model is here found to vanish which cause a significantly enlarged storage capacity of compared to for hopfield network storing static pattern our result are tested against extensive computer simulation and excellent agreement is found 
the effect of using paat query to improve automatic query expansion wa examined in the trec environment automatic feedback of document identified from similar past query wa compared with standard top document feedback and with no feedback a new query similarity metric wa used based on comparing result list and using probability of relevance our top document feedback method showed small improvement over no feedback method consistent with past study on recall precision and average precision measure past query feedback yielded performance superior to that of top document feedback the past query feedback method also lends itself to tunable threshold such that better performance can be obtained by automatically deciding when and when not to apply the expansion automatic past query feedback actually improved top document precision in this experiment 
different instance of a handwritten word consist of the same basic feature hump cusp crossing etc arranged in a deformable spatial pattern thus keywords in cursive text can be detected by looking for the appropriate feature in the correct spatial configuration a keyword can be modeled hierarchically a a set of word fragment each of which consists of lower level feature to allow flexibility the spatial configuration of keypoints within a fragment is modeled using a dryden mardia dm probability density over the shape of the configuration in a writer dependent test on a transcription of the declaration of independence word character the method detected all eleven instance of the keyword government with only four false positive 
volume intersection algorithm are used to reconstruct incomplete object from their silhouette an imagined light source is moved about the data and the cumulative amount of light seen at each point in space is interpreted a indicating the likelihood that the point is inside the object the object data need not be uniformly distributed nor exclusively surface data explicit distinction between noise surface and interior data is avoided the novel concept of a localised viewing region is introduced to overcome the inherent inability of volume intersection algorithm to reconstruct concave surface algorithm for d pixel and d voxel data are described and applied to d ultra sound data 
recently attention ha been focused on providing knowledge acquisition ka support for building practical planning system such support is needed to guide a knowledge engineer in selecting planning method a well a for building and validating the planning knowledge base for a given practical domain following current practice in knowledge acquisition developing ka tool for planning requires that a number of planning knowledge component are made explicit this includes explicating i a planning domain ontology ii a library of problem solving method psms used in planning and iii a set of domain requirement that are used to select a suitable psm in this paper we summarize the planning knowledge component which we have identified in previous work and based on these present an implementation par kap that can exploit these model to aid knowledge engineer in constructing practical planning system 
this paper present an efficient approach to address the task of learning from large number of learning example in structural domain while in attribute value representation only one mapping is possible between description in first order logic representation there are potentially many mapping classic approach consider all mapping and then define a restricted hypothesis space to cope with the intractability of exploring all mapping our approach is to select one particular type of mapping at a time and use it a a basis to define a new hypothesis space we show that such a hypothesis space called a matching space may be represented using attribute value pair in a matching space it is therefore possible to use propositional learner the concept description found may then be mapped back into the initial first order logic representation it appears that characterizing a matching space is equivalent to shifting the representation of example the new learning example represent only a part of the initial example based on a taxonomy of elementary part provided by the user we consider a particular set of composite part called morion that are used to automatically and iteratively change the representation of example experimental result obtained with an implemented system remo show the benefit of this approach we have used remo to learn characteristic description of concept related to the pronunciation of chinese character from a corpus of more than three thousand character 
the problem of non parametric probability density function pdf estimation using radial basis function rbf neural network is addressed here we investigate two criterion based on a modified kullback leibler distance that lead to an appropriate choice of the network architecture complexity in the first criterion the modification consists in the addition of a term that penalizes complex architecture mpl criterion the second strategy involves the regularization of the network through the imposition of lower bound on the standard deviation derived from condition of existence of rejection test lbsd criterion experimental result indicate that the mpl criterion outperforms the lbsd method 
a new algorithm and systematic evaluation is presented for searching a database via relevance feedback it represents a new image display strategy for the pichunter system the algorithm take feedback in the form of relative judgment item a is more relevant than item b a opposed to the stronger assumption of categorical relevance judgment item a is relevant but item b is not it also exploit a learned probabilistic model of human behavior to make better use of the feedback it obtains the algorithm can be viewed a an extension of indexing scheme like the k d tree to a stochastic setting hence the name stochastic comparison search in simulation the amount of feedback required for the new algorithm scale like log d where d is the size of the database while a simple query by example approach scale like d a where a 
we implement a model of obstacle avoidance in flying insect on a small monocular robot the result is a system that is capable of rapid navigation through a dense obstacle field the key to the system is the use of zigzag behavior to articulate the body during movement it is shown that this behavior compensates for a parallax blind spot surrounding the focus of expansion normally found in system without parallax behavior the system model the cooperation of several behavior halter ocular response similar to vor optomotor response and the parallax field computation and mapping to motor system the resulting system is neurally plausible very simple and should be easily hosted on avlsi hardware 
this paper describes a probabilistic decomposition of human dynamic at multiple abstraction and show how to propagate hypothesis across space time and abstraction level recognition in this framework is the succession of very general low level grouping mechanism to increased specific and learned model based grouping technique at higher level hard decision threshold are delayed and resolved by higher level statistical model and temporal context low level primitive are area of coherent motion found by em clustering mid level category are simple movement represented by dynamical system and high level complex gesture are represented by hidden markov model a successive phase of simple movement we show how such a representation can be learned from training data and apply it to the example of human gait recognition 
this paper is concerned with surface shape estimation by a method in which an empirically determined associative model relating appearance to surface shape is used significantly the estimated model is more accurate than the algorithm that generates the example the method presented here is a generalization of shape from shading method that doe not rely upon idealized model of the image formation process a a relative of shape from shading this method more accurately recovers small surface detail than is possible with method such a stereo and motion the present approach is a continuous analogue of pattern recognition and is closely related to method of joint space learning used in robotics experiment on real scene are used to illustrate the concept involved 
recent research on hidden state reinforcement learning rl problem ha concentrated on overcoming partial observability by using memory to estimate state however such method are computationally extremely expensive and thus have very limited applicability this emphasis on state estimation ha come about because it ha been widely observed that the presence of hidden state or partial observability render popular rl method such a q learning and sarsa useless however this observation is misleading in two way first the theoretical result supporting it only apply to rl algorithm that do not use eligibility trace and second these result are worst case result which leaf open the possibility that there may be large class of hidden state problem in which rl algorithm work well without any state esti 
finding structure in multiple stream of data is an important problem consider the stream of data owing from a robot s sen sors the monitor in an intensive care unit or periodic measurement of various indica tor of the health of the economy there is clearly utility in determining how current and past value in those stream are related to future value we formulate the prob lem of nding structure in multiple stream of categorical data a search over the space of dependency unexpectedly frequent or infrequent co occurrence between complex pattern of value that can appear in the stream based on that formulation we de velop the multi stream dependency detec tion msdd algorithm that performs an e cient systematic search over the space of all possible dependency dependency strength is evaluated with a statistical measure of non independence and bound that we derive for the value of that measure allow the search to be pruned due to the pruning msdd can nd the k strongest dependency in the stream by examining only a fraction of the search space 
a new framework and method based on image motion trajectory in spatiotemporal space x y t space are proposed to estimate image velocity from an image sequence we focus on the surface of the trajectory in the x y t space formed by the edge and contour of moving object and obtain image velocity from the orientation of the intersection line formed by tangent plane on the trajectory the proposed method includes two hough transforms to detect the most dominant orientation in all possible intersection line and reliably produce the dominant translational image velocity semi locally also the confidence measure of estimate is defined to decide the optimal size of patch that suppresses the aperture problem experimental result from several synthetic and real image sequence are presented to verify the effectiveness of the method and to confirm it robustness against noise and occlusion 
this paper describes an application of the hierarchical mixture of expert algorithm to the registration of multiple cartographic model to noisy radar data according to the hme algorithm each model is represented by a set of maximum likelihood registration parameter together with a set of matching probability this architecture can be viewed a providing simultaneous registration and hypothesis verification the map in the cartographic data base compete to account for radar data through the imposed probability normalization the resulting matching algorithm can be regarded a a generic tool for model retrieval from a data base our evaluation on radar image illustrates some of the characteristic of the algorithm our main conclusion are that the method is both robust to added image noise and poor initialization 
to make a euclidean reconstruction of the world seen through a stereo rig we can either use a calibration grid and the result will rely on the precision of the grid and the extracted point of interest or use self calibration past work on self calibration is focussed on the use of only one camera and give sometimes very unstable result in this paper we use a stereo rig which is supposed to be weakly calibrated using a method such a the one described in deriche et al then by matching two set of point of the same scene reconstructed from different point of view we try to find both the homography that map the projective reconstruction to the euclidean space and the displacement from the first set of point to the second set of point we present result of the euclidean reconstruction of a whole object from uncalibrated camera using the method proposed here 
a factorization method is proposed for recovering camera motion and object shape from point correspondence observed in multiple image with perspective projection for any factorization based approach for perspective image scaling parameter called projective depth must be estimated in order to obtain a measurement matrix that could be decomposed into motion and shape one possible approach proposed by sturm and triggs is to compute projective depth from fundamental matrix and epipoles the estimation process of the fundamental matrix however might be unstable if the measurement noise is large or the camera and the object point are nearly in critical configuration in this paper the author propose an algorithm by which the projective depth are iteratively estimated so that the measurement matrix is made to be a close a possible to rank this estimation process requires no fundamental matrix computation and is therefore robust against measurement noise camera motion and shape in d projective space are then recovered by factoring the measurement matrix computed from the obtained projective depth the author also derive metric constraint for a perspective camera model in the case where the intrinsic camera parameter are available and show that these constraint can be linearly solved for a projective transformation which relates projective and euclidean description of the scene structure using this transformation the projective motion and shape obtained in the previous factorization step is upgraded to metric description that is represented with respect to the euclidean coordinate frame the validity of the proposed method is confirmed by experiment with real image 
many multilingual nlp application need to translate word between different language but cannot afford the computational expense of inducing or applying a full translation model for thesis application we have designed a fast algorithm for estimating a partial translation model which account for translational equivalence only at the word level the model s precision recall trade off can be directly controlled via one threshold parameter this feature make the model more suitable for application that are not fully statistical the model s hidden parameter can be easily conditioned on information extrinsic to the model providing an easy way to integrate pre existing knowledge such a part of speech dictionary word order etc our model can link word token in parallel text a well a other translation model in the literature unlike other translation model it can automatically produce dictionary sized translation lexicon and it can do so with over accuracy 
we address the problem of automatically reconstructing m manifold of unknown topology from unorganized point in metric p space obtained from a noisy measurement process the point set is first approximated by a collection of oriented primitive fuzzy set over a range of resolution hierarchical multiresolution representation is then computed based on the relation of relative containment defined on the collection finally manifold structure is recovered by establishing connectivity between these primitive based on proximity compatibility of position and orientation and local topological constraint the method ha been successfully applied to the problem of surface reconstruction from polynocular stereo data with many outlier 
flexible model of object class based on linear combination of prototypical image are capable of matching novel image of the same class and have been shown to be a powerful tool to solve several fundamental vision task such a recognition synthesis and correspondence the key problem in creating a specific flexible model is the computation of pixelwise correspondence between the prototype a task done until now in a semiautomatic way in this paper we describe an algorithm that automatically bootstrap the correspondence between the prototype the algorithm which can be used for d image a well a for d model is shown to synthesize successfully a flexible model of frontal face image and a flexible model of handwritten digit 
a fast stereo algorithm based on aliasing effect of simple disparity estimator within a coherence detection scheme is presented the algorithm calculates dense disparity map with subpixel precision by performing local spatial filter operation and simple arithmetic transformation performance similar to classical area based approach is achieved but without the complicated hierarchical search structure typical for these approach the algorithm is completely parallel the disparity value are calculated independently for each pixel in addition local validation count for the disparity estimate and a fused cyclopean view of the scene are available within the proposed network structure for coherence based stereo 
a new algorithm for graph matching which us graduated assignment is presented along with experimental result demonstrating large improvement in speed and accuracy over previous technique the softassign a novel constraint satisfaction technique is applied to a new graph matching energy function that us a robust sparse distance measure between the link of the two graph the softassign which ha emerged out of the neural network statistical physic framework enforces two way assignment constraint without the use of penalty term the algorithm s low order computational complexity lm where l and m are the number of link in the two graph compare favorably with most competing approach the method not restricted to any special class of graph is applied to subgraph isomorphism weighted graph matching and attributed relational graph matching experiment on graph generated from image and on randomly generated graph including benchmark against a relation labeling algorithm and an algorithm employing potts glass dynamic are reported over twenty five thousand experiment were conducted no comparable result have been reported by any other graph matching algorithm before in the research literature 
this paper describes a bayesian graph matching algorithm fordata mining from large structural data base the matching algorithmuses edge consistency and node attribute similaritytodeterminethe aposteriori probability of a query graph for eachofthecandidate match in the data base the node feature vector areconstructed by computing normalised histogram of pairwise geometricattributes attribute similarity is assessed by computingthe bhattacharyya distance between the 
we describe a formal framework for interpretation of word and compound in a discourse context which integrates a symbolic lexicon grammar word sense probability and a pragmatic component the approach is motivated by the need to handle productive word use in this paper we concentrate on compound nominal we discus the inadequacy of approach which consider compund interpretation a either wholly lexico grammatical or wholly pragmatic and provide an alternative integrated account 
we describe a formal framework for interpretation of word and compound in a discourse context which integrates a symbolic lexicon grammar word sense probability and a pragmatic component the approach is motivated by the need to handle productive word use in this paper we concentrate on compound nominal we discus the inadequacy of approach which consider compund interpretation a either wholly lexico grammatical or wholly pragmatic and provide an alternative integrated account 
originating in allen s analysis of temporal relation the notion of a composition table ct ha become a key technique in providing an efficientinference mechanism for a wide class oftheories in this paper we challenge researchersworking with ct to give a general characterisationof the class of theory and relationalconstraint language for which a complete proofprocedure can be specified by a ct severalrelated problem and conjecture will be discussed a secondary aim is to 
a system for the automatic production of controlled index term is presented using linguistically motivated technique this includes a finite state part of speech tagger a derivational morphological processor for analysis and generation and a unification based shallow level parser using transformational rule over syntactic pattern the contribution of this research is the successful combination of parsing over a seed term list coupled with derivational morphology to achieve greater coverage of multi word term for indexing and retrieval final result are evaluated for precision and recall and implication for indexing and retrieval are discussed 
in many optimization problem the structure of solution reflects complex relationship between the different input parameter for example experience may tell u that certain parameter are closely related and should not be explored independently similarly experience may establish that a subset of parameter must take on particular value any search of the cost landscape should take advantage of these relationship we present mimic a framework in which we analyze the global structure of the optimization landscape a novel and efficient algorithm for the estimation of this structure is derived we use knowledge of this structure to guide a randomized search through the solution space and in turn to refine our estimate of the structure our technique obtains significant speed gain over other randomized optimization procedure advance in neural information processing system mit press cambridge ma 
recovering three dimensional d information of a scene from it image is a fundamental problem in computer vision there exists two major multi ocular cue for it namely the motion cue and the stereo cue this paper present a new approach of integrating the two cue when two camera that move through the scene while taking picture repeatedly are available the approach is based on the singular value decomposition svd technique with which the d structure of the scene the image projection parameter the motion parameter and the stereo geometry are all separated the approach offer the advantage of both cue simple correspondence a well a accurate reconstruction it can also work with relatively short image sequence 
this paper formulates the problem of visual search a bayesian inference and denes a bayesian ensemble of problem instance in particular we address the problem of the detection of visual contour in noise clutter by optimizing a global criterion which combine local intensity and geometry information we analyze the convergence rate of a search algorithm using result from information theory to bound the probability of rare event within the bayesian ensemble this analysis determines characteristic of the domain which we call order parameter that determine the convergence rate in particular we present a specic admissible a algorithm with pruning which converges with high probability with expected time o n in the size of the problem in addition we briey summarize extension of this work which address fundamental limit of target contour detectability i e algorithm independent result and the use of non admissible heuristic 
the past several year have seen much progressin the area of propositional reasoning and satisfiabilitytesting there is a growing consensusby researcher on the key technical challengesthat need to be addressed in order to maintainthis momentum this paper outline concretetechnical challenge in the core area of systematicsearch stochastic search problem encoding and criterion for evaluating progress in thisarea introductionpropositional reasoning is a core problem 
region based image segmentation technique make use ofsimilarity in intensity color and texture to determine the partitioning ofan image the powerful cue of contour continuity is not exploited at all in this paper we provide a way of incorporating curvilinear grouping intoregion based image segmentation soft contour information is obtainedthrough orientation energy weak contrast gap and subjective contoursare completed by contour propagation the normalized cut approach 
this paper introduces a new real time method to estimate the posture of a human from thermal image acquired by an infrared camera regardless of the background and lighting condition distance transformation is performed for the human body area extracted from the thresholded thermal image for the calculation of the center of gravity after the orientation of the upper half of the body is obtained by calculating the moment of inertia significant point such a the top of the head the tip of the hand and foot are heuristically located in addition the elbow and knee position are estimated from the detected significant point using a genetic algorithm based learning procedure the experimental result demonstrate the robustness of the proposed algorithm and real time faster than frame per second performance 
in this paper we introduce a novel geometric shape modeling scheme which allows for representation of global and local shape characteristic of an object geometric model are traditionally well suited for representing global shape but not the local detail however in this paper we propose a powerful geometric shape modeling scheme which allows for the representation of global shap e with local detail and permit model shaping a well a topological change via physic based control the proposed modeling scheme consists of representing shape by pedal curve and surface pedal curve surface are the locus of the foot of perpendicular to the tangent of a fixed curve surface from a fixed point called the pedal point by varying the location of the pedal point one can synthesize a large class of shape which exhibit both local and glob al deformation we introduce physic based control for shaping these geometric model by letting the pedal point vary and use a dynamic spline to represent the position of this varying pedal point the model dubbed a a snake pedal allows for interactive manipulation via force applied to the snake we demonstrate the applicability of this modeling scheme via example of shape synthesis and shape estimation from real image data 
this paper develops the concept of usefulness in the context of supervised learning we argue that usefulness can be used to improve the performance of classification rule a measured by error rate a well to reduce their storage or their derivation we also indicate how usefulness can be applied in a dynamic setting in which the distribution of at least one class is changing with time three algorithm are used to exemplify our proposal we first review a dynamic nearest neighbour classifier and then develop dynamic version of learning vector quantization and a radial basis function network all the algorithm are adapted to capture dynamic aspect of real world data set by keeping a record of usefulness a well a considering the age of the observation these method are tried out on real data from the credit industry 
this paper introduces a new technique for discretization of continuous variable based on zeru a measure of strength of association between nominal variable developed for this purpose zeta is defined a the maximal accuracy achievable if each value of an independent variable must predict a different value of a dependent variable we describe both how a continuous variable may be dichotomised by searching for a maximum value of zeta and how a heuristic extension of tbis method can partition a continuous variable into more than two category experimental comparison with other published method show that zeta discretization run considerably faster than other technique without any loss of accuracy 
we present a new efficient stereo algorithm addressing robust disparity estimation in the presence of occlusion the algorithm is an adaptive multi window scheme using left right consistency to compute disparity and it associated uncertainty we demonstrate and discus performance with both synthetic and real stereo pair and show how our result improve on those of closely related technique for both robustness and efficiency 
a general technique for the recovery of significant image feature is presented the technique is based on the mean shift algorithm a simple nonparametric procedure for estimating density gradient drawback of the current method including robust clustering are avoided feature space of any nature can be processed and a an example color image segmentation is discussed the segmentation is completely autonomous only it class is chosen by the user thus the same program can produce a high quality edge image or provide by extracting all the significant color a preprocessor for content based query system a spl time color image is analyzed in le than second on a standard workstation gray level image are handled a color image having only the lightness coordinate 
this paper deal with the intelligent exploration of an unknown environment by autonomous robot in particular we present an algorithm and associated analysis for collaborative exploration using two mobile robot our approach is based on robot with range sensor limited by distance by appropriate behavioural strategy we show that odometry motion error that would normally present problem for mapping can be severely reduced our analysis includes polynomial complexity bound and a discussion of possible heuristic 
in this paper we investigate a number of ensemble method forimproving the performance of phoneme classification for use in aspeech recognition system we discus boosting and mixture ofexperts both in isolation and in combination we present resultson an isolated word database the result show that principledensemble method such a boosting and mixture provide superiorperformance to more naive ensemble method when used in combination boosting and mixture provide a further 
recent research in image sensor ha produced camera with very large field of view an area of computer vision research which will benefit from this technology is the computation of camera motion ego motion from a sequence of image traditional camera suffer from the problem that the direction of translation may lie outside of the field of view making the computation of camera motion sensitive to noise in this paper we present a method for the recovery of ego motion using omnidirectional camera noting the relationship between spherical projection and wide angle imaging device we propose mapping the image velocity vector to a sphere using the jacobian of the transformation between the projection model of the camera and spherical projection once the velocity vector are mapped to a sphere we show how existing ego motion algorithm can be applied and present some experimental result these result demonstrate the ability to compute ego motion with omnidirectional camera 
in this paper we present a computationallyefficient method for inducing selectivebayesian network classifier our approachis to use information theoretic metric to efficientlyselect a subset of attribute fromwhich to learn the classifier we explorethree conditional information theoretic metricsthat are extension of metric used extensivelyin decision tree learning namely quinlan s gain and gain ratio metric and mantaras s distance metric we experimentallyshow that the 
this paper describes novel result on the characteristic of three party dialogue by quantitatively comparing them with those of two party in previous dialogue research two party dialogue are mainly focussed because data collection of multi party dialogue is difficult and there are very few theory handling them although research on multi party dialogue is expected to be of much use in building computer supported collaborative work environment and computer assisted instruction system in this paper firstly we describe our data collection method of multi party dialogue using a meeting scheduling task which enables u to compare three party dialogue with those of two party then we quantitively compare these two kind of dialogue such a the number of character and turn and pattern of information exchange lastly we show that pattern of information exchange in speaker alternation and initiative taking can be used to characterise three party dialogue 
we apply a well known bayesian probabilistic model to textual information retrieval the classification of document based on their relevance to a query this model wa previously used with supervised training data for a fixed query when only noisy unsupervised training data generated from a heuristic relevance scoring formula are available two crucial adaptation are needed severe smoothing of the model built on the training data and adding a prior probability to the model we have shown that with these adaptation the probabilistic model is able to improve the retrieval precision of the heuristic model the experiment wa performed using the trec corpus and query and the evaluation of the model wa submitted a an official entry ibms b to trec 
this paper we develop geometric relationship between the residual planar parallax displacementsof pair of point these geometric relationship address the problem of d scene analysiseven in difficult condition i e when the epipole estimation is ill conditioned when there is a smallnumber of parallax vector and in the presence of moving object we show how these relationshipscan be applied to each of the three problem outlined at the beginning of this section moreover the 
the successfulapplication of data mining technique ideally requires both system support for the entire knowledge discovery process and the right analysis algorithm for the particular task at hand while there are a number of successful data mining system that support the entire mining process they usually are limited to a fixed selection of analysis algorithm in this paper we argue in favor of extensibility a a key feature of data mining system and discus the requirement that this entail for system architecture we identify in which point existing data mining system fail to meet these requirement and then describe a new integration architecture for data mining system that address these problem based on the concept of plug in kepler our data mining system built according to this architecture is presented and discussed 
with the increase in information on the world wide web it ha become difficult to quickly find desired information without using multiple query or using a topic specific search engine one way to help in the search is by grouping html page together that appear in some way to be related in order to better understand this task we performed an initial study of human clustering of web page in the hope that it would provide some insight into the difficulty of automating this task our result show that subject did not cluster identically in fact on average any two subject had little similarity in their web page cluster we also found that subject generally created rather small cluster and those with access only to url created fewer cluster than those with access to the full text of each web page generally the overlap of document between cluster for any given subject increased when given the full text a did the percentage of document clustered when analyzing individual subject we found that each had different behavior across query both in term of overlap size of cluster and number of cluster these result provide a sobering note on any quest for a single clearly correct clustering method for web page 
neural network ensemble have been shown to be very accurateclassification technique previous work ha shown that an effectiveensemble should consist of network that are not only highlycorrect but one that make their error on different part of theinput space a well most existing technique however only indirectlyaddress the problem of creating such a set of network in this paper we present a technique called addemup that usesgenetic algorithm to directly search for an 
we have designed fabricated and tested an adaptive winner take all wta circuit based upon the classic wta of lazzaro et al we have added a time dimension adaptation to thiscircuit to make the input derivative an important factor in winnerselection to accomplish this we have modified the classic wtacircuit by adding floating gate transistor which slowly null theirinputs over time we present a simplified analysis and experimentaldata of this adaptive wta fabricated in a 
this paper present a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system streak to incrementally generate newswire sport summary the evaluation consists of searching a test corpus of stock market report for sentence pair whose semantic and syntactic structure respectively match the triggering condition and application result of each revision rule the result show that at least of all rule class are fully portable with at least another partially portable 
a new policy iteration algorithm for partially observable markov decisionprocesses is presented that is simpler and more efficient thanan earlier policy iteration algorithm of sondik the keysimplification is representation of a policy a a finite state controller this representation make policy evaluation straightforward the paper s contribution is to show that the dynamic programming updateused in the policy improvement step can be interpreted a the transformation 
cross language latent semantic indexing isa method that learns useful languageindependentvector representation of termsthrough a statistical analysis of a documentalignedtext this is accomplished by takinga collection of say english paragraphsand their translation in spanish and processingthem by singular value decompositionto yield a high dimensional vector representationfor each term in the collection theseterm vector have the property that semanticallysimilar term have 
planar pose measurement from image is an important problem for automated assembly and inspection in addition to accuracy and robustness ease of use is very important for real world application recently murase and nayar have presented the parametric eigenspace for object recognition and pose measurement based on training image although their system is easy to use it ha potential problem with background clutter and partial occlusion we present an algorithm that is robust in these term it us several small feature on the object rather than a monolithic template these eigenfeatures are matched using a median statistic giving the system robustness in the face of background clutter and partial occlusion we demonstrate our algorithm s pose measurement accuracy with a controlled test and we demonstrate it detection robustness on cluttered image with the object of interest partially occluded 
in human object recognition converging evidence ha shown that subject performance depends on their familiarity with an object s appearance the extent of such dependence is a function of the inter object similarity the more similar the object are the stronger this dependence will be and the more dominant the two dimensional d image based information will be however the degree to which three dimensional d model based information is used remains an area of strong debate previously the author showed that all model with independent d template that allowed d rotation in the image plane cannot account for human performance in discriminating novel object view here the author derive an analytic formulation of a bayesian model that give rise to the best possible performance under d affine transformation and demonstrate that this model cannot account for human performance in d object discrimination relative to this model human statistical efficiency is higher for novel view than for learned view suggesting that human observer have used some d structural information 
this paper introduces a novel user interface that integrates search and browsing of very large category hierarchy with their associated text collection a key component is the separate but simultaneous display of the representation of the category and the retrieved document another key component is the display of multapfe selected category simultaneously complete with their hierarchical context the prototype implementation us animation and a three dimensional graphical workspace to accommodate the category hierarchy and to store intermediate search result query specification in this d environment is accomplished via a novel method for painting boolean query over a combination of category label and free text example are shown on a collection of medical text 
we present a new clustering algorithm that address two major issue associated with conventional partitional clustering the difficulty in determining the number of cluster and the sensitivity to noise and outlier the proposed algorithm determines the number of cluster by a process of competitive agglomeration noise immunity is achieved by integrating concept from robust statistic into the algorithm the proposed approach can incorporate different distance measure in the objective function to find an unknown number of cluster of various type including line plane and surface 
this paper address the recovery of structure and motion from two uncalibrated image of a scene under full perspective or under affine projection epipolar geometry projective reconstruction and affine reconstruction are elaborated in a way such that everyone having knowledge of linear algebra can understand the discussion without difficulty a general expression of the fundamental matrix is derived which is valid for any projection model without lens distortion including full perspective and affine camera a new technique for affine reconstruction from two affine image is developed which consists in first estimating the affine epipolar geometry and then performing a triangulation for each point match with respect to an implicit common affine basis this technique is very efficient 
some learning technique for classification task work indirectly by first trying to fit a full probabilistic model to the observed data whether this is a good idea or not depends on the robustness with respect to deviation from the postulated model we study this question experimentally in a restricte d yet non trivial and interesting case we consider a conditionally independent attribute cia model which postulate a single binary valued hidden variable on which all other attribute i e the target and the observables depend i n this model finding the most likely value of any one variable given known value for the others reduces to testing a linear function of the observed value we learn cia with two technique the standard em algorithm and a new algorithm we develop based on covariance we compare these in a controlled fashion against an algorithm a version of winnow that attempt to find a good linear classifier directly our conclusion help delimit the fragility of using the cia model for classification once the data departs from thismodel performance quickly degrades and drop below that of the directly learn ed linear classifier 
this paper survey the growing number of indu trial application of data mining and knowledge discovery we look at the existing tool describe some representative application and discus the major issue and problem for building and deploying successful application and their adoption by business user finally we examine how to ass the potential of a knowledge discovery application 
this paper considers a specific problem of visual perception of motion namely the problem of visual detection of independent d motion most of the existing technique for solving this problem rely on restrictive assumption about the environment the observer s motion or both moreover they are based on the computation of optical flow which amount to solving the ill posed correspondence problem in this work independent motion detection is formulated a robust parameter estimation applied to the visual input acquired by a binocular rigidly moving observer depth and motion measurement are combined in a linear model the parameter of this model are related to the parameter of self motion egomotion and the parameter of the stereoscopic configuration of the observer the robust estimation of this model lead to a segmentation of the scene based on d motion the method avoids the correspondence problem by employing only normal flow field experimental result demonstrate the effectiveness of this method in detecting independent motion in scene with large depth variation without any constraint imposed on observer motion 
voting method s uch a boosting and b agging provide substantial improvement in classification p erformance in many problem domain however the resulting prediction can p rove inscrutable to end user this is especially problematic in do main s uch a medicine where end user acceptance often depends on the ability of a c lassifier to explain it reasoning here we propose a variant of the boosted na ve bayes classifier that facilitates explanation while retaining predictive performance 
the problem of model selection is relevant to many area of computer vision model selection criterion have been used in the vision literature and many more have been proposed in statistic but the relative strength of these criterion have not been analyzed in vision more importantly suitable extension to these criterion must be made to solve problem unique to computer vision using the problem of surface reconstruction a our context we analyze existing criterion using simulation and sensor data introduce new criterion from statistic develop novel criterion capable of handling unknown error distribution and outlier and extendmodel selection criterion to apply to the surface merging problem the new surface merging rule improve upon previous result and work well even at small step height h and crease discontinuity our result show that a bayesian criterion and it bootstrapped variant perform the best although for time sensitive application a variant of the akaike criterion may be a better choice unfortunately none of the criterion work reliably for small region size implying that model selection and surface merging should be avoided unless the region size is sufficiently large 
this paper describes a methodology to interpret the information from telephone company dsx assignment table drawing horizontal line are found using an efficient algorithm that work over the run length encoded representation of the image for vertical line the image is transposed using an efficient method we developed and the algorithm for horizontal line is applied again using the information about the line the tabular structure are extracted by finding biconnected component on the graph formed by the line and their intersection a methodology ha also been developed for the representation of end access to the entry inside the table 
the prediction of survival time or recurrence time is an important learning problemin medical domain the recurrence surface approximation rsa method is anatural effective method for predicting recurrence time using censored input data this paper introduces the survival curve rsa sc rsa an extension to the rsaapproach which produce accurate predicted rate of recurrence while maintaining accuracyon individual predicted recurrence time the method is applied to the problem 
with the rapid expansion of computer network during the past few year security ha become a crucial issue for modern computer system a good way to detect illegitimate use is through monitoring unusual user activity method of intrusion detection based on hand coded rule set or predicting command on line are laborous to build or not very reliable this paper proposes a new way of applying neural network to detect intrusion we believe that a user leaf a print when using the system a neural network can be used to learn this print and identify each user much like detective use thumbprint to place people at crime scene if a user s behavior doe not match his her print the system administrator can be alerted of a possible security breech a backpropagation neural network called nnid neural network intrusion detector wa trained in the identification task and tested experimentally on a system of user the system wa accurate in detecting unusual activity with false alarm rate these result suggest that learning user profile is an effective way for detecting intrusion 
the task in the computer security domain of anomaly detection is to characterize the behavior of a computer user the valid or normal user so that unusual occurrence can be detected by comparison of the current input stream to the valid user s profile this task requires an online leaming system that can respond to concept drift and handle discrete non metric time sequence data we present an architecture for online learning in the anomaly detection domain and address the issue of incremental updating of system parameter and instance selection we demonstrate a method for measuring direction and magnitude of concept drift in the classification space and present approach to the above stated issue which make use of the drift measurement an empirical evaluation demonstrates the relative strength and weakness of these technique in comparison to a number of baseline technique we show that for some user our drift adaptive technique are advantageous 
we present a new approach to reinforcement learning in which the policy considered by the learning process are constrained by hierarchy of partially specified machine this allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problem and in which component solution can be recombined to solve larger and more complicated problem our approach can be seen a providing a link between reinforcement learning and behavior based or teleo reactive approach to control we present provably convergent algorithm for problem solving and learning with hierarchical machine and demonstrate their effectiveness on a problem with several thousand state 
this paper describes a new approach for tracking rigid and articulated object using a view based representation the approach build on and extends work on eigenspace representation robust estimation technique and parameterized optical flow estimation first we note that the least square image reconstruction of standard eigenspace technique ha a number of problem and we reformulate the reconstruction problem a one of robust estimation second we define a subspace constancy assumption that allows u to exploit technique for parameterized optical flow estimation to simultaneously solve for the view of an object and the affine transformation between the eigenspace and the image to account for large affine transformation between the eigenspace and the image we define an eigenpyramid representation and a coarse to fine matching strategy finally we use these technique to track object over long image sequence in which the object simultaneously undergo both affine image motion and change of view in particular we use this eigentracking technique to track and recognize the gesture of a moving hand view based object representation have found a number of expression in the computer vision literature in particular in the work on eigenspace representation eigenspace representation provide a compact approximate encoding of a large set of training image in term of a small number of orthogonal basis image these basis image span a subspace of the training set called the eigenspace and a linear combination of these image can be used to approximately reconstruct any of the training image previous work on eigenspace representation ha focused on the problem of object recognition and ha only peripherally addressed the problem of tracking object over time additionally these eigenspace reconstruction method are not invariant to image transformation such a translation scaling and rotation previous approach have typically assumed that the object of interest can be located in the scene segmented and transformed into a canonical form for matching with the eigenspace in this paper we will present a robust statistical framework for reconstruction using the eigenspace that will generalize and extend the previous work in the area to ameliorate some of these problem the work combine line of research from object recognition using eigenspaces parameterized optical 
in this paper we undertake a systematic investigation of affine invariant object detection edge detection is first presented from the point of view of the affine invariant scale space obtained by curvature based motion of the image level set in this case affine invariant edge are obtained a a weighted difference of image at different scale we then introduce the affine gradient a the simplest possible affine invariant differential function which ha the same qualitative behavior a the euclidean gradient magnitude these edge detector are the basis both to extend the affine invariant scale space to a complete affine flow for image denoising and simplification and to define affine invariant active contour for object detection and edge integration the active contour are obtained a a gradient flow in a conformally euclidean space defined by the image on which the object is to be detected that is we show that object can be segmented in an affine invariant manner by computing a path of minimal weighted affine distance the weight being given by function of the affine edge detector the geodesic path is computed via an algorithm which allows to simultaneously detect any number of object independently of the initial curve topology 
this paper describes our work in learning online model that forecast real valued variable in a high dimensional space a gb database wa collected by sampling real valued sensor in a cement manufacturing plant once every minute for several month the goal is to learn model that every minute forecast the value of all sensor for the next hour the underlying process is highly non stationary there are abrupt change in sensor behavior time frame minute semi periodic behavior time frame hour day and slow long term drift in plant dynamic timeframe week month therefore the model need to adapt on line a new data is received all learning and prediction must occur in realtime i e one minute the learning method must also deal with two form of data corruption large amount of data are missing and what is recorded is very noisy we have developed a framework with multiple level of adaptation in which several thousand incremental learning algorithm that adapt on line are automatically evaluated also on line to arrive at the best prediction we present experimental result to show that by combining multiple learning method we can automatically learn good model for timeseries prediction without being provided with any physical model of the underlying dynamic 
in this paper the computation of medial axis is posed a a statistical inference problem not a a mathematical transform this method provides answer to two essential problem in computing the medial axis representation i prior knowledge are adopted for ax and junction so that the ax around junction become well defined ii a stochastic jump diffusion process is proposed for estimating medial axis in a markov random field we argue that the stochastic algorithm for computing medial axis is compatible with existing algorithm for image segmentation such a snake and region competition thus it provides a new direction for computing medial axis from real textured image experiment are demonstrated on both synthetic and real shape 
performing policy iteration in dynamic programming should only requireknowledge of relative rather than absolute measure of the utilityof action what baird call the advantage of action atstates nevertheless existing method in dynamic programming includingbaird s compute some form of absolute utility function forsmooth problem advantage satisfy two differential consistency condition including the requirement that they be free of curl and we showthat 
this paper concern the influence of edge direction on the estimation of edge contrast and orientation we show that the gradient estimated using radial filter is not affected by edge orientation for non radial filter the gradient can be affected by edge orientation for instance we find that the estimated edge orientation using a non radial filter may be biased even if the signal is noise free however there are non radial filter for which gradient is unaffected by edge orientation a in the case of radial filter the property of these function are given in this paper the result are illustrated by the study of the canny deriche and shen castan detector we take into account discretization error these result give a clear indication of the effect of the rotation invariance property of an edge detector on it response thus providing a more precise meaning for this property in edge detection 
a method for detection and description of rectangular building from two or more registered aerial intensity image is proposed the output is a d description of the building with an associated confidence measure for each building hierarchical perceptual grouping and matching across view is employed to increase the robustness of the system verification of selected building hypothesis is done using shadow and wall evidence of the building the system is largely feature based grouping and matching are performed in a hierarchical manner utilizing primitive of increasing complexity starting with line segment and junction and proceeding to higher level feature binocular and trinocular epipolar constraint are used to reduce the search space for matching feature 
most data mining system to date have used variant of traditional machine learning algorithm to tackle the task of directed knowledge discovery this paper present an approach which a well a being useful for such directed data mining can also be applied to the further task of undirected data mining and hypothesis refinement this approach exploit parallel genetic algorithm a the search mechanism and seek to evolve explicit rule for maximum comprehensibility example rule found in real commercial datasets are presented 
in the regression context boosting and bagging are technique to build a committee of regressors that may be superior to a single regressor we use regression tree a fundamental building block in bagging committee machine and boosting committee machine performance is analyzed on three non linear function and the boston housing database in all case boosting is at least equivalent and in most case better than bagging in term of prediction error 
in many application such a credit default prediction and medical image recognition test input are available in addition to the labeled training example we propose a method to incorporate the test input into learning our method result in solution having smaller test error than that of simple training solution especially for noisy problem or small training set 
few attention ha been paid to terminology extraction for what concern the possibility it offer to corpus linguistics and lexical acquisition the problem of detecting term in textual corpus ha been approached in a complex framework terminology is seen a the acquisition of domain specific knowledge i e semantic feature selectional restriction for complex term and or unknown word this ha useful implication on more complex text processing task e g information extraction an hybrid symbolic and probabilistic approach to terminology extraction ha been defined the proposed inductive method put a specific attention to the linguistic description of what term are a well a to the statistical characterization of term a complex unit of information typical of domain sub language experimental evidence of the proposed method are discussed 
while it is generally agreed that neuron transmit informationabout their synaptic input through spike train the code by whichthis information is transmitted is not well understood an upperbound on the information encoded is obtained by hypothesizingthat the precise timing of each spike conveys information here wedevelop a general approach to quantifying the information carriedby spike train under this hypothesis and apply it to the leakyintegrate and fire if model of 
we present a new graph theoretic approach to the problem of image segmentation our method us local criterion and yet produce result that reflect global property of the image we develop a framework that provides specific definition of what it mean for an image to be underor over segmented we then present an efficient algorithm for computing a segmentation that is neither undernor over segmented according to these definition our segmentation criterion is based on intensity difference between neighboring pixel an important characteristic of the approach is that it is able to preserve detail in low variability region while ignoring detail in high variability region which we illustrate with several example on both real and sythetic image 
this paper proposes a two layered model of dialogue structure for task oriented dialogue that process contextual information and disambiguates speech act the final goal is to improve translation quality in a speech to speech translation system 
in this paper we introduce a method for distinguishing between informative and uninformativeviewpoints a they pertain to an active observer seeking to identify anobject in a known environment the method is based on a generalized inverse theoryusing a probabilistic framework where assertion are represented by conditionalprobability density function consequently the method also permit the assessmentof the belief associated with a set of assertion based on data acquired from a 
rigid motion imposes constraint on the motion of image point between the two image the matched point must conform to one of several possible constraint such a that given by the fundamental matrix or image image homography and it is essential to know which model to fit to the data before recovery of structure matching or segmentation can be performed successfully this paper compare several model selection method with a particular emphasis on providing a method that will work fully automatically on real imagery 
in this paper a minimax entropy principle is studied based on which a novel theory called frame filter random field and minimax entropy is proposed for texture modeling frame combine attractive aspect of two important theme in texture modeling multi channel filtering and markov random field mrf modeling it incorporates the response of a set of well selected filter into the distribution over a random field and hence ha a much stronger descriptive ability than the traditional mrf model furthermore it interprets and clarifies many previous concept and method for texture analysis and synthesis from a unified point of view algorithm are proposed for probability inference stochastic simulation and filter selection experiment on a variety of texture are described to illustrate our theory and to show the performance of our algorithm these experiment demonstrate that many texture previously considered a different category can be modeled and synthesized in a common framework 
wavelet transforms are attracting increasing interest in computer vision because they provide a mathematical tool for multiscale image analysis in this paper we show that i the subsampled wavelet multiresolution representation is translationally variant and ii a wavelet transform of a signal generally confounds the phase component of the analysing wavelet associated with that scale and orientation the importance of this observation is that commonly used feature in texture analysis also depend on this phase component this not only cause unnecessary spatial variation of feature at each scale but also make it more difficult to match feature across scale 
we address the problem of d object recognition from a single d image using a model database we develop a new method calledenhanced geometric hashing this approach allows u to solve for the indexingand the matching problem in one pas with linear complexity useof quasi invariant allows u to index image in a new type of geometrichashing table they include topological information of the observedobjects inducing a high numerical stability we also introduce a more robust 
sparse coding is a method for finding a representation of data in which each of the component of the representation is only rarely s ignificantly active such a representation is closely related to redunda ncy reduction and independent component analysis and ha some neurophysiological plausibility in this paper we show how sparse coding can be used for denoising using maximum likelihood estimation of nongaussian variable corrupted by gaussian noise we show how to apply a shrinkage nonlinearity on the component of sparse coding so a to reduce noise furthermore we show how to choose the optimal sparse coding basis for denoising our method is closely related to the method of wavelet shrinkage but ha the important benefit over wavelet method s that both the feature and the shrinkage parameter are estimated dir ectly from the data 
we present a new approach to the tracking of very non rigid pattern of motion such a water flowing down a stream the algorithm is based on a disturbance map which is obtained by linearly subtracting the temporal average of the previous frame from the new frame every local motion creates a disturbance having the form of a wave with a head at the present position of the motion and a historical tail that indicates the previous location of that motion these disturbance serve a locus of attraction for tracking particle that are scattered throughout the image the algorithm is very fast and can be performed in real time we provide excellent tracking result on various complex sequence using both stabilized and moving camera showing a busy ant column waterfall rapid and flowing stream shopper in a mall and car in a traffic intersection 
this paper proposes an approach for automatic road extraction in aerial imagery which exploit the scale space behavior of road in combination with geometric constrained snake based edge extraction the approach not only ha few parameter to be adjusted but for the rst time allows for a bridging of shadow and partially occluded area using the heavily disturbed evidence in the image the road network is constructed after extracting crossing of various shape and topology reasonable result are obtained which are evaluated based on ground truth 
we define a new image feature called the color correlogram and use it for image indexing and comparison this feature distills the spatial correlation of color and is both effective and inexpensive for content based image retrieval the correlogram robustly tolerates large change in appearance and shape caused by change in viewing position camera zoom etc experimental evidence suggests that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement method for image indexing retrieval 
a variety of technique from statistic signal processing pattern recognition machine learning and neural network have been proposed to understand data by discovering useful category however research in data mining ha not paid attention to the cognitive factor that make learned category intelligible to human user we ohnwr hn a cnnte the nfl nmnao thn ntczll h l r nf cu w ullal ii d lcllrl l ulal i iiucii u j lllu iiiltaii iiicj i 
many important classification problem are polychotomies i e the data are organizedinto k class with k given an unknown function f omega f kg representing apolychotomy an algorithm aimed at quot learning quot this polychotomy will produce an approximationof f based on the knowledge of a set of pair f xp f xp gpp although in the wide variety oflearning tool there exist some learning algorithm capable of handling polychotomies many of the 
in this paper sequence of camera motion that lead to inherent ambiguity in uncalibrated euclidean reconstruction or self calibration are studied our main contribution is a complete detailed classification of these critical motion sequence cm the practically important class are identified and their degree of ambiguity are derived we also discus some practical issue especially concerning the reduction of the ambiguity of a reconstruction 
this paper proposes a method which estimate the relationship between learner s behavior and other agent one in the environment through interaction observation and action using the method of system identification in order to identify the model of each agent akaike s information criterion is applied to the result of canonical variate analysis for the relationship between the observed data in term of action and future observation next reinforcement learning based on the estimated state vector is performed to obtain the optimal behavior the proposed method is applied to a soccer playing situation where a rolling ball and other moving agent are well modeled and the learner s behavior are successfully acquired by the method computer simulation and real experiment are shown and a discussion is given 
this paper address the problem of characterizing the space formed by all image of a rigid set of n point observed by a weak perspective or paraperspective camera by taking explicitly into account the euclidean constraint associated with calibrated camera we showthat this space is a six dimensional variety embedded in ir n and parameterize it using the image position of three reference point this parameterization is constructed vialinear least square from point correspondence established across a sequence of image and it is used to synthesize new picture without any explicit three dimensional model degenerate scene and camera configuration are analyzed and experiment with real image sequence are presented 
one of the surprising recurring phenomenon observed in experiment with boosting is that the test error of the generated classifier usually doe not increase a it size becomes very large and often is observed to decrease even after the training error reach zero in this paper we show that this phenomenon is related to the distribution of margin of the training example with respect to the generated voting classification rule where the margin of an example is simply the difference between the number of correct vote and the maximum number of vote received by any incorrect label we show that technique used in the analysis of vapnik s support vector classifier and of neural network with small weight can be applied to voting method to relate the margin distribution to the test error we also show theoretically and experimentally that boosting is especially effective at increasing the margin of the training example finally we compare our explanation to those based on the bias variance 
the discovery of the relationship between chemical structure and biological function is central to biological science and medicine in this paper we apply data mining to the problem of predicting chemical carcinogenicity this toxicology application wa launched at ijcai a a research challenge for artificial intelligence our approach to the problem is descriptive rather than based on classification the goal being to find common substructure and property in chemical compound and in this way to contribute to scientific insight this approach contrast with previous machine learning research on this problem which ha mainly concentrated on predicting the toxicity of unknown chemical our contribution to the field of data mining is the ability to discover useful frequent pattern that are beyond the complexity of association rule or their known variant this is vital to the problem which requires the discovery of pattern that are out of the reach of simple transformation to frequent itemsets we present a knowledge discovery method for structured data where pattern reflect the one tomany and many to many relationship of several table background knowledge represented in a uniform manner in some of the table ha an essential role here unlike in most data mining setting for the discovery of frequent pattern 
this paper present such a model g grammar for quot group grammar quot and argues that the standard group theoretic notion of conjugacy which is central in g grammar is well suited toa uniform description of commutative and noncommutativeaspects of language the use of conjugacy provides an elegant approachto long distance dependency and scoping phenomenon both in parsing and in generation g grammar give a symmetrical account of thesemantics phonology relation from which it is 
this paper describes a novel framework for performing relational graph matching using genetic search the fitness measure is the recently reported global consistency measure of wilson and hancock the basic measure of relational distance underpinning the technique is hamming distance our standpoint is that genetic search provides a more attractive mean of performing stochastic discrete optimisation on the global consistency measure than alternative such a simulated annealing moreover the action of the optimisation process is easily understood in term of it action in the hamming distance domain we provide some experimental evaluation of the method in the matching of aerial stereograms 
this paper describes the fact system for knowledge discovery from text it discovers association pattern of co occurrence amongst keywords labeling the item in a c ollection o f textual document in addition fact is able to use background knowledge about the keywords labeling the document in it discovery process fact take a query centered view of knowledge discovery in which a discovery request i s viewed a a query over the implicit set of possible result supported by a collection of document and where background knowledge is used to specify constraint on the desired result of this query process execution of a knowledge discovery query is s tructured so that t hese background knowledge c onstraints can b e e xploited in the search for possible result finally rather than requiring a user to specify an explicit query expression in the knowledge discovery query language fact present the user with a simple to use graphical interface to the query language with the language providing a well defined semantics for the discovery action performed b y a user through the interface 
we present an algorithm which us information from both surface reflectance and illumination variation to solve for colour constancy most colour constancy algorithm assume that the illumination across a scene is constant but this is very often not valid for real image the method presented in this work identifies and remove the illumination variation and in addition us the variation to constrain the solution the constraint is applied conjunctively to constraint found from surface reflectance thus the algorithm can provide good colour constancy when there is sufficient variation in surface reflectance or sufficient illumination variation or a combination of both we present the result of running the algorithm on several real scene and the result are very encouraging 
most of the recent color recognition indexing approach concentr ate on establishing invariance to illumination color to improve the utility of color recognition however other effect caused by illumination pose and specularity on three dimensional object surface have not received notable attention we present a chromaticity recognition method that discount the effect of illumination pose illumination color and specularity it utilizes a chromaticity space based on log ratio of sensor response for illumination pose and color invariance a model based specularity detection rejection algorithm can be used to improve the chromaticity recognition and illumination estimation for object including specular reflection 
we show how a concise representation of active recognition behaviorwhat observation to make to detect a given objectcan be derived from hidden state reinforcement learning technique these learning technique can solve decision process task which include perceptual observation defined formally a partially observable markov decision process pomdp we define recognition within a pomdp context with an action indicating recognition of the target a well a action for adjusting the perceptual apparatus or other effector an explicit supervised reward signal is provided to the decision process whenever the accept action is performed with sufficient experience a memory based approach to reinforcement learning can find optimal policy which discriminate target from distractor pattern despite considerable perceptual aliasing at any given instant to avoid perceptual aliasing while learning all similar experience are combined when computing the utility of a possible action including experience with both target and distractor pattern by discarding the representation of negative region of the utility space when learning is complete and collapsing duplicate representation of positive region a representation similar to an augmented finite state machine is obtained we show application of our method for the task of recognizing human gesture performance that occurs at multiple spatial scale 
image texture can arise not only from surface albedo variation ed texture but also from surface height variation d texture since the appearance of d texture depends on the illumination and viewing direction in a complicated manner such image texture can be called a bidirectional texture function a fundamental representation of image texture is the histogram of pixel intensity since the histogram of d texture also depends on the illumination and viewing direction in a complex fashion we refer to it a a bidirectional histogram in this work we present a concise analytical model for the bidirectional histogram of lambertian isotropic randomly rough surface which are common in real world scene we demonstrate the accuracy of the histogram model by fitting to several sample from the columbia utrecht texture database the parameter obtained from the model fit are roughness measure which can be used in texture recognition scheme in addition the model ha potential application in estimating illumination direction in scene where surface of known tilt and roughness are visible we demonstrate the usefulness of our model by employing it in a novel d texture synthesis procedure 
we consider recurrent analog neural net where each gate is subject to gaussian noise or any other common noise distribution whose probability density function is nonzero on a large set we show that many regular language cannot be recognized by network of this type for example the language begin with and we give a precise characterization of those language which can be recognized this result implies severe constraint on possibility for constructing recurrent analog neural net that are robust against realistic type of analog noise on the other hand we present a method for constructing feedforward analog neural net that are robust with regard to analog noise of this type 
explanation based reinforcement learning ebrl wa introduced by dietterich andflann a a way of combining the ability ofreinforcement learning rl to learn optimalplans with the generalization abilityof explanation based learning ebl dietterich amp flann we extend thiswork to domain where the agent must orderand achieve a sequence of subgoals inan optimal fashion hierarchical ebrl caneffectively learn optimal policy in some ofthese sequential task domain even when the 
we explore the application of facial tracking to automated re animation to this end it is necessary to recover both head pose and facial expression from the facial movement of a performer however both effect are coupled this is a serious problem which previous study haven t fully considered the solution to this interaction problem proposed here is to solve explicitly at each timestep for pose and expression variable in principle this is a nonlinear inverse problem however appropriate parameterisation of pose in term of affine transformation with parallax and of expression in term of key frame reduces the problem to a bilinear one this can then be solved directly by singular value decomposition thus actor driven animation ha b een implemented in real time at video field rate using two indy desktop workstation 
conventional document retrieval system e g alta vista return long list of ranked document in response to user query recently document clustering ha been put forth a an alternative method of organizing retrieval result cutting et al a person browsing the cluster can discover pattern that could be overlooked in the traditional presentation this paper describes two novel clustering method that intersect the document in a cluster to determine the set of word or phrase shared h thn l n chn nlrrd nv ixtn x a j cala yllg u buuta l u au llg biuu gl cj r p r on experiment that evaluate these intersectionbased clustering method on collection of snippet returned from web search engine first we show that word intersection clustering produce superior cluster and doe so faster than standard technique second we show that our o nlog n time phrase intersection clustering method produce comparable cluster and doe so more than two order of magnitude faster than all method tested 
an approach for perceptual segmentation of colour image texture is described a multiscale representation of the texture image generated by a multiband smoothing algorithm based on human psychophysical measurement of colour appearance is used a the input initial segmentation is achieved by applying a clustering algorithm to the image at the coarsest level of smoothing using these isolated em core cluster d colour histogram are formed and used for probabilistic assignment of all other pixel to the core cluster to form larger cluster and categorise the rest of the image the process of setting up colour histogram and probabilistic reassignment of the pixel is then propagated through finer level of smoothing until a full segmentation is achieved at the highest level of resolution 
we present in this paper a novel calibration method that us cross ratio to compute world point falling onto any given light stripe plane of a structured light system we show that by using known non coplanar set of collinear world point the direct x image to world transformation matrix for each light stripe plane can also be recovered from plane to plane homography preliminary experiment conducted with a calibration target and a mannequin suggest that this novel calibration method is robust and is applicable to many shape measurement task 
a new method is presented for robustly estimating multiple view relation from image point correspondence there are three new contribution the first is a general purpose method of parametrizing these relation using point correspondence the second contribution is the formulation of a common maximum likelihood estimate mle for each of the multiple view relation the parametrization facilitates a constrained optimization to obtain the mle the third contribution is a new robust algorithm mlesac for obtaining the point correspondence the method is general and it use is illustrated for the estimation of fundamental matrix image to image homographics and quadratic transformation result are given for both synthetic and real image it is demonstrated that the method give result equal or superior to previous approach 
we compare different method to combine prediction from neuralnetworks trained on different bootstrap sample of a regression problem one of these method introduced in and which we here callbalancing is based on the analysis of the ensemble generalization errorinto an ambiguity term and a term incorporating generalizationperformances of individual network we show how to estimate theseindividual error from the residual on validation pattern weightingfactors for the different 
we present an approach to recognition of complex object in cluttered d scene that doe not require feature extraction or segmentation our object representation comprises descriptive image associated with each oriented point on the surface of an object using a single point basis constructed from an oriented point the position of other point on the surface of the object can be described by two parameter the accumulation of these parameter for many point on the surface of the object result in an image at each oriented point these image localized description of the global shape of the object are invariant to rigid transformation through correlation of image point correspondence between a model and scene data are established and then grouped using geometric consistency the effectiveness of our algorithm is demonstrated with result showing recognition of complex object in cluttered scene with occlusion 
in this paper we present a novel hybrid architecture for continuous speech recognition system it consists of a continuous hmm system extended by an arbitrary neural network that is used a a preprocessor that take several frame of the feature vector a input to produce more discriminative feature vector with respect to the underlying hmm system this hybrid system is an extension of a state of the art continuous hmm system and in fact it is the first hybrid system that really is capable of outperforming these standard system with respect to the recognition accuracy experimental result show an relative error reduction of about that we achieved on a remarkably good recognition system based on continuous hmms for the resource management word continuous speech recognition task 
automatic d model acquisition and d tracking of simple object under motion using a single camera is often difficult due to the sparsity of information from which to establish the model we have developed an automatic scheme that first computes a simple pointalistic euclidean model of the object and then enriches this model using hyper patch these hyper patch contain information on both the orientation and intensity pattern variation of roughly planar patch on an object this information allows both the spatial and intensity distortion of the projected patch to be modelled accurately under d object motion we show that hyper patch not only can be computed automatically during model acquisition from a monocular image sequence but that they are also extremely appropriate for the task of visual tracking 
the mortality related to cervical cancer can be substantially reduced through early detection and treatment however current detection technique such a pap smear and colposcopy fail to achieve a concurrently high sensitivity and specificity in vivo fluorescence spectroscopy is a technique which quickly noninvasively and quantitatively probe the biochemical and morphological change that occur in pre cancerous tissue rbf ensemble algorithm based on such spectrum provide automated and 
video convey information through several plane of communication encompassing what is represented in the image how the image are linked together and how the subject is imaged this feature is stressed in commercial where color editing effect rhythm and object motion are exploited to influence human purchasing habit in this paper based on research in the marketing field a link is formalized between low level feature of a commercial video and feeling that the video would inspire in the observer this link is used to define high level index capturing the main semantics of the video these index are embedded in a video retrieval system to support access to a database of video based on their semantics 
this paper is to push this interactionfurther in light of these recent development in particular we perform experiment suggested by the formalresults for adaboost and c within the weaklearning framework we concentrate on two particularlyintriguing issue first the theoretical boosting result for top downdecision tree algorithm such a c suggest thata new splitting criterion may result in tree that aresmaller and more accurate than those obtained usingthe usual 
this paper deal with the problem of characterizing and parametrizing the manifold of trifocal tensor that describe the geometry of three view like the fundamental matrix characterizes the geometry of two the paper contains two new result first a new simpler set of algebraic constrai nt that characterizes the set of trifocal tensor is presented second we give a ne w parametrization of the trifocal tensor based upon those constraint which is also simpler than previously known parametrizations some preliminary experimental result of the use of these constraint and parametrization to estimate th e trifocal tensor from image correspondence are presented 
in this paper we employ a novel approach to metarule guided multi dimensional association rule mining which explores a data cube structure we propose algorithm for metarule guided mining given a metarule containing p predicate we compare mining on an n dimensional n d cube structure where p n with mining on smaller multiple pdimensional cube in addition we propose an efficient method for precomputing the cube which take into account the constraint imposed by the given metarule 
we present an incremental focus of attention ifa architecture for adding robustnessto software based real time motion tracker the framework provides a structurewhich when given the entire camera image to search efficiently focus the attentionof the system into a narrow set of configuration that includes the target configuration ifa offer a mean for automatic tracking initialization and reinitialization whenenvironmental condition momentarily deteriorate and cause the system 
in this paper we present a unified factorization algorithm for recovering structure and motion from image sequence by using point feature line segment and plane this new formulation is based on directional uncertainty model for feature point and line segment are both described by the same probabilistic model and so can be recovered in the same way prior information on the coplanarity of feature is shown to fit naturally into the new factorization formulation and provides additional constraint for the shape recovery this formulation lead to a weighted least square motion and shape recovery problem which is solved by an efficient quasi linear algorithm the statistical uncertainty model also enables u to recover uncertainty estimate for the reconstructed three dimensional feature location 
diffuse interreflection cause effect that make current theory of shape from shading unsatisfactory we show that distant radiating surface produce radiosity effect at low spatial frequency this mean that if a shading pattern ha a small region of support unseen surface in the environment can only produce effect that vary slowly over the support region it is therefore relatively easy to construct matching process for such pattern that are robustto interreflection we call region with these pattern shading primitive fold and groove on surface provide two example of shading primitive the shading pattern is relatively independent of surface shape at a fold or a groove and the pattern is localised we show that the pattern of shading can be predicted accurately by a simple model and derive a matching process from this model both groove and fold matcher are shown to work well on image of real scene 
in this paper we discus a data mining framework for constructing intrusion detection model the key idea are to mine system audit data for consistent and useful pattern of program and user behavior and use the set of relevant system feature presented in the pattern to compute inductively learned classiers that can recognize anomaly and known intrusion our past experiment showed that classiers can be used to detect intrusion provided that sucient audit data is available for training and the right set of system feature are selected we propose to use the association rule and frequent episode computed from audit data a the basis for guiding the audit data gathering and feature selection process we modify these two basic algorithm to use axis attribute s a a form of item constraint to compute only the relevant useful pattern and an iterative level wise approximate mining procedure to uncover the low frequency but important pattern we report our experiment in using these algorithm on real world audit data 
we develop a method for recognizing color texture independent of rotation scale and illumination color texture is modeled using spatial correlation function defined within and between sensor band using a linear model for surface spectral reflectance with the same number of parameter a the number of sensor class we show that illumination and geometry change in the scene correspond to a linear transformation of the correlation function and a linear transformation of their coordinate a several step algorithm which includes scale estimation and correlation moment computation is used to achieve the invariance the key to the method is the new result that illumination and geometry change in the scene correspond to a specific transformation of correlation function zernike moment matrix these matrix can be estimated from a color image this relationship is used to derive an efficient algorithm for recognition the algorithm is substantiated using classification result on over image of color texture obtained under various illumination condition and geometric configuration 
we present new algorithm for reinforcement learning and prove that they have polynomial bound on the resource required to achieve near optimal return in general markov decision process after observing that the number of action required to approach the optimal return is lower bounded by the mixing time t of the optimal policy in the undiscounted case or by the horizon time t in the discounted case we then give algorithm requiring a number of action and total computation time that 
situation calculus is arguably the most widely studied and used formalism for reasoning about action and change the main reason for it popularity is the ability to reason about different action sequence a explicit object in particular planning can be formulated a an existence problem this paper show how these property break down when incomplete information about the initial state and nondeterministic action effect are introduced basically due to the fact that this incompleteness is not adequately manifested on the object level a version of situation calculus is presented which adequately model the alternative way the world can develop relative to a choice of action 
similarity based fault tolerant retrieval in neural associative memory nam ha not lead to wiedespread application a drawbackof the efficient willshaw model for sparse pattern ste wblh is that the high asymptotic information capacity is oflittle practical use because of high cross talk noise arising in theretrieval for finite size here a new bidirectional iterative retrievalmethod for the willshaw model is presented called crosswise bidirectional cb retrieval providing 
support vector learning machine svm are finding applicationin pattern recognition regression estimation and operator inversionfor ill posed problem against this very general backdrop any method for improving the generalization performance or forimproving the speed in test phase of svms are of increasing interest in this paper we combine two such technique on a patternrecognition problem the method for improving generalization performance the quot virtual support vector quot method 
the technique of bayesian inference have been applied with greatsuccess to many problem in neural computing including evaluationof regression function determination of error bar on prediction and the treatment of hyper parameter however the problem ofmodel comparison is a much more challenging one for which currenttechniques have significant limitation in this paper we show howan extended form of markov chain monte carlo called chaining is able to provide effective estimate 
we develop a generalised form of the independent component analysis ica algorithm introduced by bell and sejnowski amari et al and lately by pearlmutter and parra and also mackay motivated by information theoretic index for exploratory projection pursuit epp we show that maximisation by natural gradient ascent of the divergence of a multivariate distribution from normality using the negentropy a a distance measure yield a generalised ica we introduce a form of nonlinearity which h a an inherently simple form and exhibit the bussgang property within the a lgorithm we show that t his is sufficient to perform ica on data which ha latent variable exhibiting either unimodal or bimodal probability density function pdf or both kurtosis ha been used a a moment based projection pursuit index and a a c ontrast for i ca we introduce a simple a daptive nonlinearity which is formed by on line estimation of the latent variable kurtosis and demonstrate the removal of the standard ica constraint of latent variable pdf modality uniformity 
we study the number of hidden layer required by a multilayer neural network withthreshold unit to compute a function f from rdto f g in spite of similarity withthe characterization of linearly separable boolean function this problem present ahigher level of complexity gibson characterized the function of r which are computablewith just one hidden layer under the assumption that there is no quot multipleintersection point quot and that f is only defined on a compact set we 
in this paper we present a method for discovering approximately common motif also known a active motif in multiple rna secondary structure the secondary structure can be represented a ordered tree i e the order among sibling matter motif in these tree are connected subgraphs that can differ in both substitution and deletion insertion the proposed method consists of two step find candidate motif in a small sample of the secondary structure search all of the secondary structure to determine how frequently these motif occur within the allowed approximation in the secondary structure to reduce the running time we develop two optimization heuristic based on sampling and pattern matching technique experimental result obtained by running these algorithm on both generated data and rna secondary structure show the good performance of the algorithm to demonstrate the utility of our algorithm we discus their application to conducting the phylogenetic study of rna sequence obtained from genbank 
in order to take advantage of the top speed of manipulator vision can not be tightly integrated into the motion control loop past visual servo control system have performed satisfactorily with this constraint however it can be shown that the task execution time can be reduced if the vision system is de coupled from the low level motor control system for reaching there is a trade off between the accuracy of a motion and the time requir ed to execute a motion in study of human motor contr ol this trade off is quantified by fitts law a relationship between the motion time the target distance and the target width these study suggest that vision is not used tightly within the control loop i e a a sensor that is servo ed on but rather vision is used to determine where the reaching target is and whether target ha been reached successfully through a simple robotic example we demonstrate that a similar trade off exists between motion accuracy and the motion execution time for visual guided robot reaching motion 
in previous work we advanced a new technique for direct visual matching of image for the purpose of face recognition and image retrieval using a probabilistic measure of similarity based primarily on a bayesian map analysis of image difference leading to a quot dual quot basis similar to eigenfaces the performance advantage of this probabilistic matching technique over standard euclidean nearest neighbor eigenface matching wa recently demonstrated using result from darpa s 
in this paper we propose and examine non parametric statistical test to define similarity and homogeneity measure for texture the statistical test are applied to the coefficient of image filtered by a multi scale gabor filter bank we will demonstrate that these similarity measure are useful for both texture based image retrieval and for unsupervised texture segmentation and hence offer an unified approach to these closely related task we present result on brodatz like micro texture and a collection of real word image 
a general active contour formulation is considered and a convexity analysis of it energy function is presented condition under which this formulation ha a unique solution are derived these condition involve both the active contour energy potential and the regularization parameter this analysis is then applied to four particular active contour formulation revealing important characteristic of their convexity and suggesting that external potential involving center of mass computation may be better behaved than the ususal potential based on image gradient most importantly our analysis provides an explanation for the poor convergence behavior at concave boundary and suggests an alternate algorithm for approaching these type of boundary 
we present and experimentally evaluate a new model of prounciation by analogy the paradigmatic cascade model given a pronunciation lexicon this algorithm first extract the most productive paradigmatic mapping in the graphemic domain and pair them statistically with their correlate s in the phonemic domain these mapping are used to search and retrieve in the lexical database the most promising analog of unseen word we finally apply to the analog pronunciation the correlated series of mapping in the phonemic domain to get the desired pronunciation 
we present an on line investment algorithm which achieves almost the same wealth a thebest constant rebalanced portfolio determined in hindsight from the actual market outcome the algorithm employ a multiplicative update rule derived using a framework introduced bykivinen and warmuth our algorithm is very simple to implement and requires only constantstorage and computing time per stock in each trading period we tested the performance of ouralgorithm on real stock data from the new 
image taken with wide angle camera tend to have severe distortion which pull point towards the optical center this paper proposes a new method for recovering the distortion parameter without using any calibration object the distortion cause straight line in the scene to appear a curve in the image our algorithm seek to find the distortion parameter that would map the image curve to straight line the user selects a small set of point along the image curve recovery of the parameter is formulated a the minimization of an objective function which is designed to explicitly account for noise in the selected image point experimental result are provided for synthetic data with different noise level a well a for real image the computed distortion parameter are used to undistort a video stream in real time using a look up table 
multiscale approach are an invaluable tool for image segmentation a vast amount of research ha been devoted to the construction of different multiscale representation of an image in this paper we use the hyperstack a multiscale linking model for image segmentation for an in depth comparison of four different scale space generator with respect to segmentation result we consider the linear gaussian scale space both in the spatial and the fourier domain the variable conductance diffusion according to the perona malik equation and the euclidean shortening flow we have done experiment on mr image of the brain for which a gold standard is available the hyperstack proof to be rather insensitive to the underlying scale space generator 
the surface growing framework presented by besl and jain ha served a the basis for many range segmentation technique it ha been augmented with alternative fitting technique model selection criterion and solid modelling component all of these approach however require global threshold and large isolated seed region range scene typically do not satisfy the global threshold assumption since it requires data noise characteristic to be constant throughout the scene furthermore a scene complexity increase the number of surface discontinuity and outlier increase hindering the identification of large seed region we present statistical criterion based on multivariate regression to replace the traditional decision criterion used in surface growing we use local estimate and their uncertainty to construct criterion which capture the uncertainty in extrapolating estimated fit we restrict surface expansion to very localized extrapolation increasing the sensitivity to discontinuity and allowing region to refine their estimate and uncertainty our approach us a small number of parameter which are either statistical threshold or cardinality measure i e we do not use threshold defined by specific range distance or orientation angle 
a testbed for automatic face recognition show an eigenface coding of shape free texture with manually coded landmark wa more effective than correctly shaped face being dependent upon high quality representation of the facial variation by a shape free ensemble configuration also allowed recognition these measure combine to improve performance and allowed automatic measurement of the face shape caricaturing further increased performance correlation of contour of shapefree image also increased recognition suggesting extra information wa available a natural model considers face a in a manifold linearly approximated by the two factor with a separate system for local feature 
in this paper we present new algorithm for target detection segmentation in second generation forward looking infra red flir image an initial detection algorithm that model the background using weibull function is used to identify candidate target location in the image a two stage focused analysis of each candidate target location is then performed to get an accurate representation of the target boundary a region growing procedure is used to get an initial estimate of the target region which is then combined with salient edge information in the image to arrive at a more accurate representation of the target boundary the region and edge integration is done using a novel method that us a bayes minimum risk classification approach finally to reduce the false alarm rate a higher level interpretation module is used to classify the detected area a man made or natural object using geometric and flir intensity based feature extracted from the target 
several vision problem can be reduced to the problem of fitting a linear surface of low dimension to data including the problem of structure from affine motion and of characterizing the intensity image of a lambertian scene by constructing the em intensity manifold for these problem one must deal with a data matrix with some missing element in structure from motion missing element will occur if some point feature are not visible in some frame to construct the intensity manifold missing matrix element will arise when the surface normal of some scene point do not face the light source in some image we propose a novel method for fitting a low rank matrix to a matrix with missing element we show experimentally that our method produce good result in the presence of noise these result can be either used directly or can serve a an excellent starting point for an iterative method 
we present an architecture and an on line learning algorithm and apply it to the problem of part of speech tagging the architecture presented is a network of linear separator in the feature space utilizing the winnow update algorithm multiplicative weight update algorithm such a winnow have been shown to have exceptionally good behavior when applied to very high dimensional problem and especially when the target concept depend on only a small subset of the feature in the feature space in this paper we describe an architecture that utilizes this mistake driven algorithm for multi class prediction selecting the part of speech of a word the experimental analysis presented here provides more evidence to that these algorithm are suitable for natural language problem the algorithm used is an on line algorithm every example is used by the algorithm only once and is then discarded this ha significance in term of efficiency a well a quick adaptation to new context we present an extensive experimental study of our algorithm under various condition in particular it is shown that the algorithm performs comparably to the best known algorithm for po 
we present a model of contour extraction in which the perceptual salience of contour arises from long range interaction between orientation selective filter it ha been previously shown that salient contour may be extracted from noisy image by using a number heuristic feature our algorithm is based on cortical mechanism and simulation show close agreement with result from recent anatomical physiological and psychophysical study including recent result of d j field et al i kovacs et al and m k kapadia et al the performance of the algorithm is demonstrated on a range of psychophysical stimulus and real image 
the foreground group in a scene may be discovered andcomputed a a factorized approximation to the pairwise affinity of theelements in the scene a pointwise approximation of the pairwise affinityinformation may in fact be interpreted a a saliency index and theforeground of the scene may be obtained by thresholding it an algorithmcalled affinity factorization is thus obtained which may be used forgrouping 
focus of attention mechanism for robot vision are discussed a new method for neglecting low level filter response from already modelled structure is presented the method is based on a filtering technique termed normalized convolution in one experiment the robot is continuously moving it arm in the scene while tracking other object it is shown how the arm can be made invisible so that only the moving object of interest is detected this make tracking of object much simpler in another experiment the attention of the system is shifted between object by simply cancelling the mask of the object to be attended to with this strategy the low level process do not need to know the difference between a new object entering the scene and a mask being cancelled and thus a complex communication structure between high and low level is avoided 
classification of finite sequence without explicit knowledge of theirstatistical nature is a fundamental problem with many importantapplications we propose a new information theoretic approachto this problem which is based on the following ingredient i sequencesare similar when they are likely to be generated by the samesource ii cross entropy can be estimated via quot universal compression quot iii markovian sequence can be asymptotically optimallymerged with these ingredient 
we describe a system that is being used to segment gray matter and create connected cortical representation from mri the method exploit knowledge of the anatomy of the cortex and incorporates structural constraint into the segmentation first the white matterand csf region in the mr volume are segmented using some novel technique of posterior anisotropic diffusion then the user selects the cortical white matter component of interest and it structure is verified by checking for cavity and handle after this a connected representation of the gray matter is created by a constrained growing out from the white matter boundary because the connectivity is computed the segmentation can be used a input to several method of visualizing the spatial pattern of cortical activity within gray matter in our case the connected representation of gray matter is used to create a representation of theflattened cortex then fmri measurement are over laid on the flattened representation yielding a representation of the volumetric data within a single image 
little work ha been done in nlp on the subject of punctuation owing mainly to a lack of a good theory on which computational treatment could be based this paper described early work in progress to try to construct such a theory two approach to finding the syntactic function of punctuation mark are discussed and procedure are described by which the result from these approach can be tested and evaluated both against each other a well a against other work suggestion are made for the use of these result and for future work 
in this paper we present an integrated approach that solves the structure and motion problem for affine camera given image of corresponding point line and conic in any number of view a reconstruction of the scene structure and the camera motion is calculated up to an affine transformation starting with three view two novel concept are introduced the first one is a quasi tensor consisting of component and the second one is another quasitensor consisting of component these tensor describe the viewing geometry for three view taken by an affine camera it is shown how correspondence of point line and conic can be used to constrain the tensor component a set of affine camera matrix compatible with the quasi tensor can easily be calculated from the tensor component the resulting camera matrix serve a an initial guess in a factorisation method using point line and conic concurrently generalizing the well known factorisation method by tomasi kanade finally example are given that illustrate the developed method on both simulated and real data 
the author describes a new method for camera autocalibration and scaled euclidean structure and motion from three or more view taken by a moving camera with fixed but unknown intrinsic parameter the motion constancy of these is used to rectify an initial projective reconstruction euclidean scene structure is formulated in term of the absolute quadric the singular dual d quadric spl time rank matrix giving the euclidean dot product between plane normal this is equivalent to the traditional absolute conic but simpler to use it encodes both affine and euclidean structure and project very simply to the dual absolute image conic which encodes camera calibration requiring the projection to be constant give a bilinear constraint between the absolute quadric and image conic from which both can be recovered nonlinearly from m spl ge image or quasi linearly from m spl ge calibration and euclidean structure follow easily the nonlinear method is stabler faster more accurate and more general than the quasi linear one it is based on a general constrained optimization technique sequential quadratic programming that may well be useful in other vision problem 
this paper present a new similarity measure for object recognition from large library of line pattern the measure draw it inspiration from both the hausdorff distance and a recently reported bayesian consistency measure that ha been sucessfully used for graphbased correspondence matching the measure us robust error kernel to gauge the similarity of pairwise attribute relation defined on the edge of nearest neighbour graph we use the similarity measure in a recognition experiment which involves a library of over line pattern a sensitivity study reveals that the method is capable of delivering a recognition accuracy of a comparative study reveals that the method is most effective when a gaussian kernel or huber s robust kernel is used to weight the attribute relation moreover the method consistently outperforms rucklidge s median hausdorff distance 
in this paper we adopt general sum stochasticgames a a framework for multiagent reinforcementlearning our work extends previouswork by littman on zero sum stochasticgames to a broader framework we designa multiagent q learning method underthis framework and prove that it convergesto a nash equilibrium under specified condition this algorithm is useful for finding theoptimal strategy when there exists a uniquenash equilibrium in the game when thereexist multiple nash equilibrium 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
it ha been known that different representation of a query retrieve different set of document recentwork suggests that significant improvement in retrieval performance can be achieved by combiningmultiple representation of an information need however little effort ha been made to understandthe reason why combining multiple source of evidence improves retrieval effectiveness in this paperwe analyze why improvement can be achieved with evidence combination and investigate how 
we describe and analyze a mixture model for supervised learning of probabilistic transducer we devise an online learning algorithm that efficiently infers the structure and estimate the parameter of each probabilistic transducer in the mixture theoretical analysis and comparative simulation indicate that the learning algorithm track the best transducer from an arbitrarily large possibly infinite pool of model we also present an application of the model for inducing a noun phrase recognizer 
this paper present a methodology for localization of manmade object in complex scene by learning multiple feature model in image the methodology is based on a modular structure consisting of multiple classifier each of which solves the problem independently based on it input observation each classifier module is trained to detect manmade object region and a higher order decision integrator collect evidence from each of the module to delineate a final region of interest the proposed framework is applied to the problem of automatic manmade object localization detection result obtained on the detection of vehicle in color visual and infrared imagery are presented in this paper 
we propose an active learning method with hidden unit reduction which is devised specially for multilayer perceptrons mlp first we review our active learning method and point out that many fisher information based method applied to mlp have a critical problem the information matrix may be singular to solve this problem we derive the singularity condition of an information matrix and propose an active learning technique that is applicable to mlp it effectiveness is verified through 
the paper present a new method for matching individual line segment between image the method us both grey level information and the multiple view geometric relation between the image for image pair epipolar geometry facilitates the computation of a cross correlation based matching score for putative line correspondence for image triplet cross correlation matching score are used in conjunction with line transfer based on the trifocal geometry algorithm are developed for both short and long range motion in the case of long range motion the algorithm involves evaluating a one parameter family of plane induced homographies the algorithm are robust to deficiency in the line segment extraction and partial occlusion experimental result are given for image pair and triplet for varying motion between view and for different scene type the three view algorithm eliminates all mismatch 
inference is a key component in learning probabilistic model from partially observable data when learning temporal model each of the many inference phase requires a complete traversal over a potentially very long sequence furthermore the data structure propagated in this procedure can be extremely large making the whole process very demanding in we describe an approximate inference algorithm for monitoring stochastic process and prove bound on it approximation error in this paper we apply this algorithm a an approximate forward propagation step in an em algorithm for learning temporal bayesian network we also provide a related approximation for the backward step and prove error bound for the combined algorithm we show that em using our inference algorithm is much faster than em using exact inference with no degradation of the quality of the learned model we then extend our analysis to the online learning task showing a bound on the error resulting from restricting attention to a small window of observation we present an online em learning algorithm for dynamic system and show that it learns much faster than standard offline em 
most algorithm dedicated to the generation of referential description widely suffer from a fundamental problem they make too strong assumption about adjacent processing component resulting in a limited coordination with their perceptive and linguistics data that is the provider for object descriptor and the lexical expression by which the chosen descriptor is ultimately realized motivated by this deficit we present a new algorithm that allows for a widely unconstrained incremental and goal driven selection of descriptor integrates linguistic constraint to ensure the expressibility of the chosen descriptor and provides mean to control the appearance of the created referring expression hence the main achievement of our approach lie in providing a core algorithm that make few assumption about other processing component and improves the flow of control between module 
we describe a new approach to feature based object recognition using maximum a posteriori map estimation under a markov random field mrf model the main advantage of this approach is that it allows explicit modeling of dependency between individual feature of an object for instance we use the approach to model the fact that mismatched feature due to partial occlusion tend to form spatially coherent group rather than being independent efficient computation of the map estimate in our framework can be accomplished by finding a minimum cut on an appropriately defined graph an even more efficient approximation that doe not use graph cut is also presented this approximation technique which we call spatially coherent matching scm is closely related to generalized hausdorff matching we report some monte carlo experiment showing that the scm technique improves substantially on the tradeoff between correct detection and false alarm compared with previous feature matching method such a the hausdorff distance 
image mosaic are useful for a variety of task in vision and computer graphic a particularly convenient way to generate mosaic is by stitching together many ordinary photograph existing algorithm focus on capturing static scene this paper present a complete system for creating visually pleasing mosaic in the presence of moving object there are three primary contribution the first component of our system is a registration method that remains unbiased by movement the mellin transform is extended to register image related by a projective transform second an efficient method for finding a globally consistent registration of all image is developed by solving a linear system of equation derived from many pairwise registration matrix we find an optimal global registration lastly a new method of compositing image is presented blurred area due to moving object are avoided by segmenting the mosaic into disjoint region and sampling pixel in each region from a single source image 
we present an analog vlsi cellular architecture implementing a simplified version of the boundary contour system bcs for real time image processing inspired by neuromorphic model across several layer of visual cortex the design integrates in each pixel the function of simple cell complex cell hyper complex cell and bipole cell in three orientation interconnected on a hexagonal grid analog current mode cmos circuit are used throughout to perform edge detection local inhibition directionally selective long range diffusive kernel and renormalizing global gain control experimental result from a fabricated pixel prototype in m cmos technology demonstrate the robustness of the architecture in selecting image contour in a cluttered and noisy background 
model based vehicle tracking in traffic image sequence can be made more robust by matching expected displacementrates of vehicle surface point to optical flow of vector computed from an image sequence the capability to track vehicle uninterruptedly in thismanner over extended image sequence result in the ability to investigate even small error in of estimation it turn out that the of magnitude are systematically underestimated the albeit small bias can be corrected by analyzing the influence of explicitly modeled grey value noise on the precision of of value estimated by mean of the neighborhood sampling method 
we present a method for learning complex appearance mapping such a occur with image of articulated object traditional interpolation network fail on this case since appearance is not necessarily a smooth function nor a linear manifold for articulated object we define an appearance mapping from example by constructing a set of independently smooth interpolation network these network can cover overlapping region of parameter space a set growing procedure is used to find example cluster which are well approximated within their convex hull interpolation then proceeds only within these set of example with this method physically valid image are produced even in region of parameter space where nearby example have different appearance we show result generating both simulated and real arm image 
understanding observation of interacting object requires one to reason about qualitative scene dynamic for example on observing a hand lifting a can we may infer that an active hand is applying an upwards force by grasping to lift a passive can in previous work we presented a system that infers qualitative scene dynamic from the instantaneous motion of object however since that analysis only considered single frame in isolation there were often multiple interpretation for each frame in this work we show how the dynamic information inferred at each frame can be integrated over time to reduce ambiguity our approach to integrating information is to extend our representation to describe object by a set of property or capability that are assumed to persist over time given this extended representation we find interpretation that require the smallest set s of property over the whole image sequence 
the observed image texture for a rough surface ha a complex dependence on the illumination and viewing angle due to effect such a local shading interreflection and the shadowing and occlusion of surface element we introduce the dimensionality surface a a representation for the visual complexity of a material sample the dimensionality surface defines the number of basis feature that are required to represent the space of observed texture for a surface a a function of range of illumination and viewing angle basis texture are represented using multiband correlation function we study property of the dimensionality surface for real material using the columbia utrecht reflectance and texture curet database the analysis show that the dependence of the dimensionality surface on range of illumination and viewing angle is approximately linear with a slope dependent on the complexity of the sample 
we confront the theoretical and practical difficulty of computing a representation for two dimensional shape based on shock or singularity that arise a the shape s boundary is deformed first we develop subpixel local detector for finding and classifying shock second to show that shock pattern are not arbitrary but obey the rule of a grammar and in addition satisfy specific topological and geometric constraint shock hypothesis that violate the grammar or are topologically or geometrically invalid are pruned to enforce global consistency survivor are organized into a hierarchical graph of shock group computed in the reaction diffusion space where diffusion play a role of regularization to determine the significance of each shock group the shock group can be functionally related to the object s part protrusion and bend and the representation is suited to recognition several example illustrate it stability with rotation scale change occlusion and movement of part even at very low resolution 
condensation recently introduced in the computer visionliterature is a particle filtering algorithm which represents a tracked object s state using an entire probability distribution clutter can cause thedistribution to split temporarily into multiple peak each representinga different hypothesis about the object configuration when measurementsbecome unambiguous again all but one peak corresponding tothe true object position die out while several peak persist estimating 
a new approach to the recognition of temporal behavior and activity is presented the fundamental idea inspired by work in speech recognition is to divide the inference problem into two level the lower level is performed using standard independent probabilistic temporal event detector such a hidden markov model hmms to propose candidate detection of low level temporal feature the output of these detector provide the input stream for a stochastic context free grammar parsing mechanism the grammar and parser provide longer range temporal constraint disambiguate uncertain low level detection and allow the inclusion of a priori knowledge about the structure of temporal event in a given domain to achieve such a system we provide technique for generating a discrete symbol stream from continuous low level detector and for enforcing temporal exclusion constraint during parsing we demonstrate the approach in several experiment using both visual and other sensing data 
the analysis of nominal compound construction ha proven to be a recalcitrant problemfor linguistic semantics and pose serious challenge for natural language processing system 
the paper present an analysis of the stability of pose estimation the investigated pose estimation technique is based on orientation of three edge segment and provides the rotation part of object pose the specific emphasis of the analysis is on determining how the stability varies with view point relative to an object the stability investigation propagates the uncertainty in edge segment orientation to the resulting effect on the pose parameter it is shown that there is a very strong variation in noise sensitivity over the range of viewpoint and that exactly what viewpoint offer highest robustness towards noise can be determined in advance experiment on real image verify the theoretical result and show that dependent on viewpoint pose parameter variance varies from to degree squared 
this paper derives what we term the euclidean hinge constraint for projective reconstruction of object displaying articulated motion a euclidean hinge is defined here to be an articulation axis with the proviso that any plane perpendicular to the articulation axis in link ha a coincident plane which is perpendicular to the articulation ad in link and that these plane remain coincident under articulated motion this constraint permit the independent projective reconstruction of two adjacent articulated link to be placed in a common frame the constraint may be expressed mathematically by considering what we define a circular parallax additionally the existence of a euclidean hinge permit the permit the projective frame to be brought nearer to a euclidean frame for the reconstructed object a brief reprise of the method of articulation axis estimation is given together with a more extensive series of experimental result 
real valued random hidden variable can be useful for modellinglatent structure that explains correlation among observed variable i propose a simple unit that add zero mean gaussian noiseto it input before passing it through a sigmoidal squashing function such unit can produce a variety of useful behavior rangingfrom deterministic to binary stochastic to continuous stochastic ishow how quot slice sampling quot can be used for inference and learningin top down network of these 
this paper deal with the problem of motion based segmentation of image sequence such partition are multiple purpose in dynamic scene analysis we first extract a texture based partition using an unsupervised mrf approach the region obtained are then grouped according to a motion based criterion this grouping process relies on two motion estimation technique and exploit contextual information between region in contrast with clustering technique region grouping is formalized a a motion based graph labeling process within a markovian framework result on real world image sequence are shown and validate the proposed method 
this paper present an algorithm for a dense computation of the difference in blur between two image the two image are acquired by varying the intrinsic parameter of the camera the image formation system is assumed to be passive estimation of depth from the blur difference is straigh forward the algorithm is based on a local image decomposition technique using the hermite polynomial basis we show that any coefficient of the hermite polynomial computed using the more blurred image is a function of the partial derivative of the other image and the blur difference hence the blur difference can be computed by resolving a system of equation all computation required are local and carried out in the spatial domain an algorithm is presented for estimation of the blur in d and d case and it behavior is studied for constant image step edge line edge and junction the algorithm is tested using synthetic and real image the result obtained are very encouraging 
we present an extensive empirical comparisonof several smoothing technique inthe domain of language modeling includingthose described by jelinek and mercer katz and church andgale we investigate for the rsttime how factor such a training datasize corpus e g brown versus wall streetjournal and n gram order bigram versustrigram aect the relative performance ofthese method which we measure throughthe cross entropy of test data in addition we 
the aim of this paper is to provide a comparative evaluation of a number of contrasting approach to relational matching unique to this study is the way in which we show how a diverse family of algorithm relate to one another using a common bayesian framework broadly speaking there are two main aspect to this study firstly we focus on the issue of how relational inexactness may be quantified we illustrate that several popular relational distance measure can be recovered a specific limiting case of the same bayesian consistency measure the second aspect of our comparison concern the way in which structural inexactness is controlled we investigate three different realisation of the matching process which draw on contrasting control model the main conclusion of our study is that the active process of graph editing outperforms the alternative in term of it ability to effectively control a large population of contaminating clutter 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
we describe a system we developed for identifyingtrends in text document collected over a period oftime trend can be used for example to discoverthat a company is shifting interest from one domainto another our system us several data mining techniquesin novel way and demonstrates a method inwhich to visualize the trend we also give experiencesfrom applying this system to the ibm patentserver a database of u s patent introductionwe address the problem of discovering 
a new framework is presented in this paper for tracking non rigid motion in image sequence this method model non rigid motion a the deformation of connected d membrane patch local smoothness constraint for each patch are the low frequency vibration mode obtained from modal analysis these constraint are incorporated with the template matching algorithm into a local motion estimator to maintain the global topological structure different patch are connected together by hinge which introduce additional constraint in the tracking process the resulted over constrained linear system is solved efficiently using a least square estimator to improve the robustness of the tracker stochastic filtering technique is employed to take advantage of the temporal continuity of the deformation promising result on both natural and synthetic facial motion sequence are demonstrated in this paper 
this paper proposes a method for automatically constructing triangular g spline model of complex three dimensional object from a few registered photograph these model are used for pose estimation from monocular silhouette data and they form the basis for a simple recognition strategy the proposed approach is demonstrated by several experiment 
there is currently considerable interest in developing general nonlinear density model based on latent or hidden variable suchmodels have the ability to discover the presence of a relatively smallnumber of underlying cause which acting in combination giverise to the apparent complexity of the observed data set unfortunately to train such model generally requires large computational effort in this paper we introduce a novel latent variable algorithm which retains the 
model learning combined with dynamic programming ha been shown tobe effective for learning control of continuous state dynamic system thesimplest method assumes the learned model is correct and applies dynamicprogramming to it but many approximators provide uncertainty estimateson the fit how can they be exploited this paper address the casewhere the system must be prevented from having catastrophic failure duringlearning we propose a new algorithm adapted from the dual control 
this paper describes a new statistical parser which is based on probability of dependency between head word in the parse tree standard bigram probability estimation technique are extended to calculate probability of dependency between pair of word test using wall street journal data show that the method performs at least a well a spatter magerman jelinek et al which ha the best published result for a statistical parser on this task the simplicity of the approach mean the model train on sentence in under minute with a beam search strategy parsing speed can be improved to over sentence a minute with negligible loss in accuracy 
this paper is concerned with the problem of segmenting an image into region using a local measure of the difference between image pixel we develop a general framework for a broad range of segmentation problem based on pairwise comparison of region in a segmentation this framework provides precise definition of when a segmentation is too coarse or too fine within this framework we define a particular pairwise region comparison function for graph based segmentation problem then we provide an efficient algorithm for computing a segmentation using this comparison function and prove that it produce good segmentation those that are neither too coarse nor too fine by our definition we apply this algorithm to image segmentation an important characteristic of this method is it ability to preserve detail in low variability image region while ignoring detail in high variability region we illustrate the method with several example on both real and sythetic image 
we study the classification problem thatarises when two variable one continuous x one discrete s evolve jointly intime we suppose that the vector x tracesout a smooth multidimensional curve to eachpoint of which the variable s attache a discretelabel the trace of s thus partition thecurve into different segment whose boundariesoccur where s change value we considerhow to learn the mapping between xand s from example of segmented curve our approach is to model the 
algorithm scalability and the distributed nature of both data and computation deserve serious attention in the context of data mining this paper present padma parallel data mining agent a parallel agent based system that make an effort to address these issue padma contains module for parallel data accessing operation parallel hierarchical clustering and webbased data visualization this paper describes the general architecture of padma and experimental result 
this paper present fundamental theory and design of centralpanoramic camera panoramic camera combine a convex hyperbolic orparabolic mirror with a perspective camera to obtain a large field of view we show how to design a panoramic camera with a tractable geometryand we propose a simple calibration method we derive the image formationfunction for such a camera the main contribution of the paperis the derivation of the epipolar geometry between a pair of panoramiccameras we show 
we present result on the use of neural network based autoassociators which act a novelty or anomaly detector to detect imminent motor failure the autoassociator is trained to reconstruct spe ctra obtained from the healthy motor in laboratory test we have demonstrated that the trained autoassociator ha a small reconstruction error on measurement recorded from healthy motor but a larger error on those reco rded from a motor with a fault we have designed and built a motor monitoring system using an autoassociator for anomaly detection and are in the process of testing the system at three industrial and commercial site 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
three different type of classifier were investigatedin the context of a text categorization problem in the medical domain the automatic assignment of icd code to dictated inpatient discharge summary k nearest neighbor relevance feedback and bayesian independence classifier were applied individually and in combination a coknbination of different classifier produced better result than any single type of classifier for this specific medical categorization problem new query formulation and weighting method used in the k nearest neighbor classifier improved performance 
camera calibration is essential to many computer vision application in practice this often requires cumbersome calibration procedure to be carried out regularly in the last few year a lot of work ha been done on self calibration of camera ranging from weak calibration to metric calibration it ha been shown that a metric calibration of the camera setup up to scale wa possible based on the rigidity of the scene only in this paper a stratified approach is proposed which gradually retrieves the metric calibration of the camera setup starting from an uncalibrated image sequence the projective calibration is retrieved first in projective space the plane at infinity is then identified yielding the affine calibration this is achieved using a constraint which can be formulated between any two arbitrary image of the sequence once the affine calibration is known the upgrade to metric is easily obtained through linear equation 
many computer vision task rely on feature extraction inter est point are such feature this paper show that interest point are geometrically stable under different transformation and have high information content distinctiveness these two property make interest point very successful in the context of image matching to measure these two property quantitatively we introduce two evaluation criterion repeatability rate and information content the quality of the interest point depends on the detector used in this paper several detector are compared according to the criterion specified above we determine which detector give the best result and show that it satisfies the criterion well 
we propose a treatment of coordination based on the concept of functor argument and subcategorization it formalization comprises two part which are conceptually independent on one hand we have extended the feature structure unification to disjunctive and set value in order to check the compatibility and the satisfiability of subcategorization requirement by structured complement on the other hand we have considered the conjunction and the general schema of a head saturation both part have been encoded within hpsg using the same resource that is the subcategorization and it principle which we have just extended 
we introduce a method for unsupervised clustering of image of d object our method examines the space of all image and partition the image into set that form smooth and parallel surface in this space it further us sequence of image to obtain more reliable clustering finally since our method relies on a non euclidean similarity measure we introduce algebraic technique for estimating local property of these surface without first embedding the image in a euclidean space we demonstrate our method by applying it to a large database of image 
we have combined an artificial neural network ann character classifier with context driven search over character segmentation word segmentation and word recognition hypothesis to provide robust recognition of hand printed english text in new model of apple computer s newton messagepad we present some innovation in the training and use of anns a character classifier for word recognition including normalized output error frequency balancing error emphasis negative training and stroke warping a recurring theme of reducing a priori bias emerges and is discussed 
this paper present a statistical approach for detecting corner from chain encoded digital arc an arc point is declared a a corner if the estimated parameter of the two fitted line of the two arc segment immediately to the right and left of the arc point are statistically significantly different the corner detection algorithm consists of two step corner detection and optimization while corner detection involves statistically identifying the most likely corner point along an arc sequence corner optimization deal with improving the locational error of the detected corner the major contribution of this research include developing a method for analytically estimating the covariance matrix of the fitted line parameter and developing a hypothesis test statistic to statistically test the difference between the parameter of two fitted line this paper discus the theory and performance characterization of the proposed corner detector 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
we define a gamma multi layer perceptron mlp a an mlpwith the usual synaptic weight replaced by gamma filter a proposedby de vries and principe de vries amp principe andassociated gain term throughout all layer we derive gradientdescent update equation and apply the model to the recognitionof speech phoneme we find that both the inclusion of gammafilters in all layer and the inclusion of synaptic gain improvesthe performance of the gamma mlp we compare the gamma 
coevolutionary learning which involves theembedding of adaptive learning agent ina fitness environment that dynamically respondsto their progress is a potential solutionfor many technological chicken andegg problem however several impedimentshave to be overcome in order for coevolutionarylearning to achieve continuous progressin the long term this paper present someof those problem and proposes a frameworkto address them this presentation is illustratedwith a case study 
we consider the problem of determining whether two image come from different object or the same object in the same pose but under different illumination condition we show that this problem cannot be solved using hard constraint even using a lambertian reflectance model there is always an object and a pair of lighting condition consistent with any two image nevertheless we show that for point source and object with lambertian reflectance the ratio of two image from the same object is simpler than the ratio of image from different object we also show that the ratio of the two image provides two of the three distinct value in the hessian matrix of the object s surface using these observation we develop a simple measure for matching image under variable illumination comparing it performance to other existing method on a database of image of individual 
a constructive induction model using geneticprogramming is presented the modelevolves new attribute starting from a randompopulation of possible attribute constructedas function of the original attribute the model is tested on hard supervisedlearning problem and it performanceis compared with backpropagation and c the performance of the system on learningincomplete bit parity is reported to be better introductionconstructive induction ci is an effort to improve the 
this paper give a practical and accurate algorithm for the computation of the quadrifocal tensor and extraction of camera matrix from it previous method for using the quadrifocal tensor in projective scene reconstruction have not emphasized accuracy of the algorithm in condition of noise method given in this paper minimize algebraic error either through a non iterative linear algorithm or two alternative iterative algorithm it is shown by experiment with synthetic data that the iterative method though minimizing algebraic rather than more correctly geometric error measured in the image give almost optimal result 
we present a method for matching curve which accommodates large and small deformation the method preserve geometric similarity in the case of small deformation and loosens these geometric constraint when large deformation occur the approach is based on the computation of a set of geodesic path connecting the curve these two curve are defined a a source area and a destination area which can have an arbitrary number of connected component and different topology the applicative framework of the presented method is the study of the crustal deformation from a set of iso elevation curve an experiment with real curve demonstrates that the approach can be successfully applied to characterize deformation of digital elevation model 
a common way to represent a time series is to divide it into shortduration block each of which is then represented by a set of basis function a limitation of this approach however is that the temporal alignment of the basis function with the underlying structure in the time series is arbitrary we present an algorithm for encoding a time series that doe not require blocking the data the algorithm find an efficient representation by inferring the best temporal position for function in a kernel basis these can have arbitrary temporal extent and are not constrained to be orthogonal this allows the model to capture structure in the signal that may occur at arbitrary temporal position and preserve the relative temporal structure of underlying event the model is shown to be equivalent to a very sparse and highly overcomplete basis under this model the mapping from the data to the representation is nonlinear but can be computed efficiently this form also allows the use of existing method for adapting the basis itself to data this approach is applied to speech data and result in a shift invariant spike like representation that resembles coding in the cochlear nerve 
sequential data this paper is about the unsupervised discovery of pattern in sequence of composite object a composite object may be described a a sequence of other simpler data in such case not only the nature of the component is important but also the order in which these component appear the present work study the problem of generalizing sequence of complex object a formal definition of generalized sequence is given and an algorithm is derived because of the excessive computational complexity of this algorithm a heuristic version is described this algorithm is then integrated in a general purpose clustering algorithm the result is a knowledge discovery system which is able to analyze any structured database on the base of a unified unsupervised mechanism 
the color associated with an object in machine vision image is not constant under varying illuminating and viewing condition such a in outdoor image the perceived color of an object can vary significantly thus making color based recognition difficult existing method in color based recognition have been applied mostly to indoor and or constrained imagery but not to realistic outdoor data this work analyzes the variation of object color in outdoor image with respect to existing model of day light illumination and surface reflectance two approach for color recognition are then proposed the first develops context based model of daylight illumination and hybrid surface reflectance and predicts the color of object based on scene context the secondmethod show that object color can be nonparametrically learned through classification method such a neural network and multivariate decision tree the method have been successfully tested in domain such a road highway scene off road navigation and military target detection 
powerful method for interactive exploration and search from collection of free form textual document are needed to manage the ever increasing flood of digital information in this article we present a method websom for automatic organization of full text document collection using the self organizing map som algorithm the document collection is ordered onto a map in an unsupervised manner utilizing statistical information of short word context the resulting ordered map where similar document lie near each other thus present a general view of the document space with the aid of a suitable wwwbased interface document in interesting area of the map can be browsed the browsing can also be interactively extended to related topic which appear in nearby area on the map along with the method we present a case study of it use 
control structure is one of the most important issue which ha to be addressed while designing and developing a software agent system based on the research and development of several agent system this thesis describes new agent architecture and a number of general principle 
this paper examines the phenomenon of consonant spreading in arabic stem each spreading involves a local surface copying of an underlying consonant and in certain phonological context spreading alternate productively with consonant lengthening or gemination the morphophonemic trigger of spreading lie in the pattern or even in the root themselves and the combination of a spreading root and a spreading pattern cause a consonant to be copied multiple time the interdigitation of arabic stem and the realization of consonant spreading are formalized using finite state morphotactics and variation rule and this approach ha been successfully implemented in a large scale arabic morphological analyzer which is available for testing on the internet 
periodicity search that is search for cyclicity in time related database is an interesting data mining problem most previous study have been on finding full cycle periodicity for all the segment in the selected sequence of the data that is if a sequence is periodic all the point or segment in the period repeat however it is often usefill to mine segment wise or point wise periodicity in time related data set in this study we integrate data cube and apriori data mining technique for mining segment wise periodicity in regard to a fixed length period and show that data cube provides an efficient structure and a convenient way for interactive mining of multiple level periodicity 
markov random field mrf s can be used for a wide variety of vision problem in this paper we focus on mrf s with two valued clique potential which form a generalized potts model we show that the maximum a posteriori estimate of such an mrf can be obtained by solving a multiway minimum cut problem on a graph we develop efficient algorithm for computing good approximation to the minimum multiway cut the visual correspondence problem can be formulated a an mrf in our framework this yield quite promising result on real data with ground truth we also apply our technique to mrf s with linear clique potential 
declarative bias play an important rolewhen learning in potentially huge hypothesisspaces while scientific discovery system which perform equation discovery a a subtask consider such potentially huge hypothesisspaces few if any employ declarative asopposed to hard coded bias to define and restricttheir hypothesis space we present anequation discovery system lagramge thatuses grammar to define and restrict it hypothesisspace these grammar can makeuse of 
we present an integrated approach to the derivation of scene description from binocular stereo image by inferring the scene description directly from local measurement of both point and line correspondence we address both the stereo correspondence problem and the surface reconstruction problem simultaneously we introduce a robust computational technique call tensor voting for the inference of scene description in term of surface junction and region boundary the methodology is grounded in two element tensor calculus for representation and non linear voting for data communication by efficiently and effectively collecting and analyzing neighborhood information we are able to handle the task of interpolation discontinuity detection and outlier identification simultaneously the proposed method is non iterative robust to initialization and thresholding in the preprocessing stage and the only criticalfree parameter is the size of the neighborhood we illustrate the approach with result on a variety of image 
we describe the result of performing data mining on a challenging medical diagnosis domain acute abdominal pain this domain is well known to be difficult yielding little more than predictive accuracy for most human and machine diagnostician moreover many researcher argue that one of the simplest approach the naive bayesian classifier is optimal by comparing the performance of the naive bayesian classifier to it more general cousin the bayesian network classifter and to selective bayesian classifier with just of the total attribute we show that the simplest model perform at least a well a the more complex model we argue that simple model like the selective naive bayesian classifier will perform a well a more complicated model for similarly complex domain with relatively small data set thereby calling into question the extra expense necessary to induce more complex model 
this paper describes an active camera real time system for tracking shape description and classification of the human face and mouth using only an sgi indy computer the system is based on use of d blob feature which are spatially compact cluster of pixel that are similar in term of low level image property pattern of behavior e g facial expression and head movement can be classified in real time using hidden markov model hmm method the system ha been tested on hundred of user and ha demonstrated extremely reliable and accurate performance typical classification accuracy are near 
conventional edge linking method perform poorly whenmultiple response to the same edge bifurcation and nearby edge arepresent we propose a scheme for curve inference where divergent bifurcationsare initially suppressed so that the smooth part of the curve canbe computed more reliably recovery of curve singularity and gap isdeferred to a later stage when more contextual information is available introductionthe problem of curve inference from a brightness image is of 
we describe a novel technique for face recognition based on deformable intensity surface which incorporates both the shape and texture component of the d image the intensity surface of the facial image is modeled a a deformable d mesh in z y i x y space using an efficient technique for matching two surface in term of the analytic mode of vibration we obtain a dense correspondence field or d warp between two image the probability distribution of two class of warp are then estimated from training data interpersonal and extrapersonal variation these density are then used in a bayesian framework for image matching and recognition experimental result with facial data from the u army feret database demonstrate an increased recognition rate over the previous best method 
we propose a new image retrieval method based on human perceptual clustering of color image this color clustering produce for each image a small set of representative color which capture the color property of the image and a small set of sizable contiguous region which capture the spatial geometrical property of the image the proposed method outperforms the traditional histogram and it improved method not only with it richer image retrieval capability which cover a wider spectrum of user requirement but also with it powerful indexing scheme which is essential to cater for large scale image database 
the problem of time series prediction is studied within the uniform convergence framework of vapnik and chervonenkis the dependence inherent in the temporal structure is incorporated into the analysis ther eby generalizing the available theory for memoryless process finite sample bound are calculated in term of covering number of the approximating class and the tradeoff between approximation and estimation is discussed a complexity regularization approach is outlined based on vapnik s method of structural risk minimization and shown to be applicable in the context of mixing stochastic process 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
a globally convergent homotopy method is defined that is capable of sequentiallyproducing large number of stationary point of the multi layerperceptron mean squared error surface using this algorithm large subsetsof the stationary point of two test problem are found it is shown empiricallythat the mlp neural network appears to have an extreme ratioof saddle point compared to local minimum and that even small neuralnetwork problem have extremely large number of solution 
this paper develops real time tracking technology for sport broadcast application the specific sport chosen here is the game of tennis the output of the tennis tracking system are spatio temporal trajectory of motion of the player and the ball which can in turn provide a number of statistic about the game for instance the distance travelled by a player the speed and the acceleration at any instant a well a court coverage pattern can be obtained from the trajectory the statistic so obtained can be visualized in compelling way to enhance the appreciation of the athleticism and strategy involved in the sport we present technique for tracking the player and the ball in video obtained from stationary camera the problem is challenging a the tracking need to be performed outdoors player are fast moving non rigid object and the ball is a small object that can move at speed in the range of mile an hour player trajectory are obtained by dynamically clustering track of local feature ball segmentation and tracking is based on shape and color feature of the ball real time tracking result are presented on video recorded live by the author in an international tennis tournament 
this pap er present a comprehensive framework for tracking moving human in an indoor environment from sequence of synchronized monocular grayscale image captured from multiple fixed camera the proposed framework consists of three main module single view tracking svt multiple view transition tracking mvtt and automatic camera switching ac bayesian classification scheme based on motion analysis of human feature are used to track spatially and temporally a subject image of interest between consecutive frame the automatic camera switching module predicts the position of the subject along a spatial temporal domain and then selects the camera which provides the best view and requires the least switching to continue tracking limited degree of occlusion are tolerated within the system tracking is based up on the image of upper human body captured from various viewing angle and non human moving object are excluded using principal component analysis pca experimental result are presented to evaluate the performance of the tracking system 
in many data mining problem the definition of what structure in the database are to be regarded a interesting or valuable is given only loosely typically this is regarded a a source of ambiguity and imprecision however we propose taking advantage of the looseness of the definition by choosing a particular definition which optimises some additional criterion we illustrate using a consumer credit data set where the definition of what constitutes a bad risk customer is somewhat arbitrary instead of adopting the common strategy of freely choosing some definition we choose that which optimises predictability that is we choose to define our class on the ground that they are the one amongst those which can be most accurately predicted 
dynamic probabilistic network dpns are a useful tool for modeling complex stochastic process the simplest inference task in dpns is monitoringthat is computing a posterior distribution for the state variable at each time step given all observation up to that time recursive constant space algorithm are well known for monitoring in dpns and other model this paper is concerned with hindsight that is computing a posterior distribution given both past and future observation hindsight is an essential subtask of learning dpn model from data existing algorithm for hindsight in dpns use o sn space and time where n is the total length of the observation sequence and s is the state space size for each time step they are therefore impractical for hindsight in complex model with long observation sequence this paper present an o s log n space o sn log n time hindsight algorithm we demonstrates the effectiveness of the algorithm in two real world dpn learning problem we also discus the possibility of an o s space o siv time algorithm 
in this paper a novel recursive method for estimating structure and motion from image sequence is presented the novelty lie in the fact that the output of the algorithm is independent of the chosen coordinate system in the image a well a the ordering of the point it relies on subspace method and is derived from both ordinary coordinate representation and camera matrix and from a so called depth and shape analysis furthermore no initial phase is needed to start up the algorithm it start directly with the first two image and incorporates new image a soon a new corresponding point are obtained the performance of the algorithm is shown on simulated data moreover the two different approach one using camera matrix and the other using the concept of affine shape and depth are unified into a general theory of structure and motion from image sequence 
it is often necessary to handle randomness and geometry in computer vision for instance to match and fuse together noisy geometric feature such a point line or d frame or to estimate a geometric transformation from a set of matched feature however the proper handling of these geometric feature is far more difficult than for point and a number of paradox can arise we analyse in this article three basic problem what is a uniform random distribution of feature how to define a distance between feature and what is the mean feature of a number of feature measurement and we propose generic method to solve them 
we develop a face recognition algorithm which is insensitiveto gross variation in lighting direction and facial expression takinga pattern classification approach we consider each pixel in an imageas a coordinate in a high dimensional space we take advantage of theobservation that the image of a particular face under varying illuminationdirection lie in a d linear subspace of the high dimensional featurespace if the face is a lambertian surface without self shadowing 
this paper present a quantitative approach to grouping a genericgrouping method which may be applied to many domain is given and an analysisof it expected grouping quality is done the grouping method is divided into twoparts constructing a graph representation of the geometric relation in the dataset and then finding the quot best quot partition of the graph into group both stagesare implemented using known statistical tool such a wald s sprt algorithmand the maximum likelihood 
this paper proposes a mistake driven mixture method for learning a tag model the method iteratively performs two procedure constructing a tag model based on the current data distribution and updating the distribution by focusing on data that are not well predicted by the constructed model the final tag model is constructed by mixing all the model according to their performance to well reflect the data distribution we represent each tag model a a hierarchical tag i e ntt context tree by using the hierarchical tag context tree the constituent of sequential tag model gradually change from broad coverage tag e g noun to specific exceptional word that cannot be captured by general tag in other word the method incorporates not only frequent connetions but also infrequent one that are often considered to be collocational we evaluate several tag model by implementing japanese part of speech tagger that share all other condition i e dictionary and word model other than their tag model the experimental result show the proposed method significantly outperforms both hand crafted and conventional statistical method 
abstract we present a new approach to analyse the deformation of the left ventricle of the heart based on aparametric model that give a compact representation of a set of point in a d image we presenta strategy for tracking surface in a sequence of d cardiac image following tracking we theninfer quantitative parameter which characterize left ventricle motion volume of left ventricle ejection fraction amplitude and twist component of cardiac motion we explain the computationof 
in this paper it is shown how corresponding conic in two image can be used to estimate the epipolar geometry in term of the fundamental essential matrix the corresponding conic can be image of either planar conic or silhouette of quadric it is shown that one conic correspondence give two independent constraint on the fundamental matrix and a method to estimate the fundamental matrix from at least four corresponding conic is presented furthermore a new type of fundamental matrix for describing conic correspondence is introduced finally it is shown that the problem of estimating the fundamental matrix from point correspondence and conic correspondence in general ha different solution a method to calculate these solution is also given together with an experimental validation 
this paper look at representing paraphrase using the formalism of synchronous tag it look particularly at comparison with machine translation and the modification it is necessary to make to synchronous tag for paraphrasing a more detailed version is in dras a 
a novel approach to grouping symmetrical planar curve under a projective transform is described symmetric curve are important a a generic model for object recognition where an object class is defined by the set of symmetry that any object in the class obeys in this paper a new algorithm is presented for grouping curve based on their correspondence under a plane projectivity the correspondence between curve is established from an initial correspondence between two pair of distinguished line such a line tangent to inflection point this initial correspondence lead to a reduced dimensional form for the projective mapping between the curve and a natural method for establishing correspondence between all point on the curve a saliency measure is introduced which permit grouping result to be ordered in term of the degree of symmetry supported by each curve pair this saliency measure provides a basis for recognition in the case of approximate symmetry 
we present a new framework for recognizing planar object class which is based on local feature detector and a probabilistic model of the spatial arrangement of the feature the allowed object deformation are represented through shape statistic which are learned from example instance of an object in an image are detected by finding the appropriate feature in the correct spatial configuration the algorithm is robust with respect to partial occlusion detector false alarm and missed feature a success rate wa achieved for the problem of locating quasi frontal view of face in cluttered scene 
we present a new algorithm for associative reinforcement learning the algorithm is based upon the idea of matching a network s output probability with a probability distribution derived from the environment s reward signal this probability matching algorithm is shown to perform faster and be le susceptible to local minimum than previously existing algorithm we use probability matching to train mixture of expert network an architecture for which other reinforcement learning rule fail to converge reliably on even simple problem this architecture is particularly well suited for our algorithm a it can compute arbitrarily complex function yet calculation of the output probability is simple 
this paper present a method for alignment of image acquired by sensor of different modality e g eo and ir the paper ha two main contribution i it identifies an appropriate image representation for multi sensor alignment i e a representation which emphasizes the common information between the two multi sensor image suppresses the non common information and is adequate for coarse to fine processing ii it present a new alignment technique which applies global estimation to any choice of a local similarity measure in particular it is shown that when this registration technique is applied to the chosen image representation with a local normalized correlation similarity measure it providesa new multi sensor alignment algorithm which is robust to outlier and applies to a wide variety of globally complex brightness transformation between the two image our proposed image representation doe not rely on sparse image feature e g edge contour or point feature it is continuous and doe not eliminate the detailed variation within local image region our method naturally extends to coarse to fine processing and applies even in situation when the multi sensor signal are globally characterized by low statistical correlation 
sequence of event are an important special form of data that arises in several context including telecommunication user interface study and epidemiology we present a general and flexible framework of specifying class of generalized episode these are recurrent combination of event satisfying certain condition the framework can be instantiated to a wide variety of application by selecting suitable primitive condition we present algorithm for discovering frequently occurring episode and episode rule the algorithm are based on the use of minimal occurrence of episode this make it possible to evaluate confidence of a wide variety of rule using only a single analysis pas we present empirical result on t he behavior of t he algorithm on event stemming from a www log 
during evaluation of real world traffic scene we often encounterthe situation that the vehicle under scrutiny are temporarilyoccluded by dynamic or stationary scene component vision basedtracking algorithm often fail in tracking vehicle under such condition contextual knowledge about occlusion is expected to facilitate the vehicletracking process we collected a set of potential occlusion situation described by primitive predicate which are modeled by mean of fuzzysets 
this paper introduces a probability model the mixture of tree that can account for sparse dynamically changing dependence relationship we present a family of efficient algorithm that use emand the minimum spanning tree algorithm to find the ml and map mixtureof tree for a variety of prior including the dirichlet and the mdl prior 
we describe the early stage of our methodology of knowledge acquisition from technical text first a partial morpho syntactic analysis is performed to extract candidate term then the knowledge engineer assisted by an automatic clustering tool build the conceptual field of the domain we focus on this conceptual analysis stage describe the data prepared from the result of the morpho syntactic analysis and show the result of the clustering module and their interpretation we found that syntactic link represent good descriptor for candidate term clustering since the cluster are often easily interpreted a conceptual field 
we present a principled method of obtaining a weighted similarity metric for d image retrieval firmly rooted in bayes decision theory the basic idea is to determine a set of most discriminative feature by evaluating how well they perform on the task of classifying image according to predefined semantic category we propose this indirect method a a rigorous way to solve the difficult feature selection problem that come up in most content based image retrieval task the method is applied to normal and pathological neuroradiological ct image where we take advantage of the fact that normal human brain present an approximate bilateral symmetry which is often absent in pathological brain the quantitative evaluation of the retrieval system show promising result 
a real time tracking algorithm that us contextual information is described the method is capable of simultaneously tracking multiple non rigid object when erratic movement and object collision are common a closed world assumption is used to adaptively select and weight image feature used for correspondence result of algorithm testing and the limitation of the method are discussed the algorithm ha been used to track child in an interactive narrative playspace 
structure of dynamic scene can only be recovered using a real time range sensor depth from defocus offer an effective solution to fast and dense range estimation however accurate depth estimation requires theoretical and practical solution to a variety of problem including recovery of textureless surface precise blur estimation and magnification variation caused by defocusing both textured and textureless surface are recovered using an illumination pattern that is projected via the same optical path used to acquire image the illumination pattern is optimized to maximize accuracy and spatial resolution in computed depth the relative blurring in two image is computed using a narrow band linear operator that is designed by considering all the optical sensing and computational element of the depth from defocus system defocus invariant magnification is achieved by the use of an additional aperture in the imaging optic a prototype focus range sensor ha been developed that ha a workspace of cubic foot and produce up to depth estimate at hz with an average rms error of several experimental result are included to demonstrate the performance of the sensor 
image editing system are essentially pixel based in this paper we propose a novel method for image editing in which the primitive working unit is not a pixel but an edge the feasibility of this proposal is suggested by recent work showing that a gray scale image can be accurately represented by it edge map if a suitable edge model and scale selection method are employed in particular an efficient algorithm ha been reported to invert such an edge representation to yield a high fidelity reconstruction of the original image we have combined these algorithm together with an efficient method for contour grouping and an intuitive user interface to allow user to perform image editing operation crop paste delete directly in the contour domain experimental result suggest that this novel combination of vision algorithm may increase the efficiency of certain class of image editing operation 
different kind of digital image can be modelled a the sampling of a continuous surface being described and analyzed through the extraction of geometric feature from the underlying surface among them ridge and valley or generically crease have deserved special interest the computer vision community ha been relying on different crease definition some of them equivalent although they are quite valuable in a number of application they usually do not correspond to the real crease of a topographic relief these definition give rise either to algorithm that label pixel a crease point and then focus on the problem of grouping them into curve or to operator whose outcome is a creaseness image we draw our attention to the real crease definition for a landscape due to rudolf rothe which is based on the convergence of slopelines they are computed by numerically solving a system of differential equation afterwards we extract rothe crease which are part of slopelines where others converge avoiding in such a way any pixel grouping step at the same time we compute a creaseness image according to this definition 
reinforcement learning method can be used to improve the performance of local search algorithm for combinatorial optimization by learning an evaluation function that predicts the outcome of search the evaluation function is therefore able to guide search to low cost solution better than can the original cost function we describe a reinforcement learning method for enhancing local search that combine aspect of previous work by zhang and dietterich and boyan and moore boyan in an off line learning phase a value function is learned that is useful for guiding search for multiple problem size and instance we illustrate our technique by developing several such function for the dial a ride problem our learning enhanced local search algorithm exhibit an improvement of more then over a standard local search algorithm 
in this paper we discus regularisation in online sequential learningalgorithms in environment where data arrives sequentially technique such a cross validation to achieve regularisation ormodel selection are not possible further bootstrapping to determinea confidence level is not practical to surmount theseproblems a minimum variance estimation approach that make useof the extended kalman algorithm for training multi layer perceptronsis employed the novel contribution 
this paper is concerned with the retrieval of image from large database based on their shape similarity to a query image our approach is based on two dimensional histogram that encode both the local and global geometric property of the shape the pairwise attribute are the directed segment relative angle and directed relative position the novelty of the proposed approach is to simultaneously use the relational and structural constraint derived from an adjacency graph to gate histogram contribution we investiguate the retrieval cap ability of the method for various query we also investigate the robustness of the method to segmentation error we conclude that a relational histo gram of pairwise segment attributespresents a very efficient way of indexing into large database the optimal configuration is obtained when the local feature are constructed from six neighbouring segment pair moreover a sensitivity analysis reveals that segmentation error do not affect the retrieval performance 
we present in this paper a system which automatically build from real image a scene model containing both d geometric information of the scene structure and it photometric information under various illumination condition the geometric structure is recovered from image taken from distinct viewpoint structure from motion and correlation based stereo technique are used to match pixel between image of different viewpoint and to reconstruct the scene in d space the photometric property is extracted from image taken under different illumination condition orientation position and intensity of the light source this is achieved by computing a low dimensional linear space of the spatio illumination volume and is represented by a set of basis image the model that ha been built can be used to create realistic rendering from different viewpoint and illumination condition application include object recognition virtual reality and product advertisement 
abstract several clustering algorithm can be applied to clustering in large multimedia database the e ectiveness and e ciency of the existing algorithm however is somewhat limited since clustering in multimedia database requires cluster ing high dimensional feature vector and since multimedia database often contain large amount of noise in this pa per we therefore introduce a new algorithm to clustering in large multimedia database called denclue density based clustering the basic idea of our new approach is to model the overall point density analytically a the sum of in uence function of the data point cluster can then be identi ed by determining density attractor and cluster of arbitrary shape can be easily described by a simple equa tion based on the overall density function the advantage of our new approach are it ha a rm mathematical basis it ha good clustering property in data set with large amount of noise it allows a compact mathematical de scription of arbitrarily shaped cluster in high dimensional data set and it is signi cantly faster than existing algo rithms to demonstrate the e ectiveness and e ciency of denclue we perform a series of experiment on a num ber of di erent data set from cad and molecular biology a comparison with dbscan show the superiority of our new approach keywords clustering algorithm density based clus tering clustering of high dimensional data clustering in multimedia database clustering in the presence of noise introduction because of the fast technological progress the amount of data which is stored in database increase very fast the type of data which are stored in the computer be come increasingly complex in addition to numerical data complex d and d multimedia data such a im age cad geographic and molecular biology data are stored in database for an e cient retrieval the com plex data is usually transformed into high dimensional feature vector example of feature vector are color histogram sh shape descriptor jag mg fourier vector ww text descriptor kuk etc in many of the mentioned application the database are very large and consist of million of data object with several ten to a few hundred of dimension automated knowledge discovery in large multimedia database is an increasingly important research issue clustering and trend detection in such database how ever is di cult since the database often contain large amount of noise and sometimes only a small portion of the large database account for the clustering in addition most of the known algorithm do not work ef ciently on high dimensional data the method which 
this paper present a study about functional model for r egression tree leaf we e valuate experimentally several alternative to the average commonly used in regression tree we have implemented a regression tree learner htl that i s able to use several alternative model in the tree leaf we study the effect on accuracy and the c omputational cost of these alternative the e xperiments carried out on data set revealed that it is possible to significantly outperform the naive average of regression tree among the four alternative model that we evaluated kernel regressors were usually the best in term of accuracy our study also indicates that by integrating regression tree with other regression approach we are able to overcome the limitation of individual m ethods both in term of accuracy a well a in computational efficiency 
in this paper we present the first step in the development of a statistical shape model specifically a point distribution model pdm of the cortical surface of the brain this will ultimately be used to locate label and describe the cortex for visualisation diagnosis andquantification in order to produce the model it wa necessary to find and label the sulcal fissure on a series of mr image due to the complexity of the surface an automated method wa developed to facilitate development of a full surface model automating the marking process introduced the problem of identifying correspondence between example the knowledge of which is essential to the development of a pdm various method were investigated to solve this problem including simple point matching and more complex curve matching each is outlined and discussed the model obtained so far provide interesting insight into the shape and cortical pattern variation over a group of normal subject 
this paper present an efficient algorithm for generating adaptive triangular mesh from dense range image the proposed technique consists of two stage first a quadrilateral mesh is generated from the given range image the point of this mesh adapt to the surface shape represented in the range image by grouping in area of high curvature and dispersing in low variation region the second stage split each quadrilateral cell obtained before into two triangle between the two possible flip it is chosen the one whose diagonal s direction is closest to the orientation of the discontinuity present in that cell both stage avoid costly iterative optimization technique result with real range image are presented they show low cpu time and accurate triangular approximation of the given image 
we consider the problem of finding rule relating pattern in a time series to other pattern in that series or pattern in one series to pattern in another series a simple example is a rule such a a period of low telephone call activity is usually followed by a sharp rise ill call vohune example of rule relating two or more time series are if the microsoft stock price go up and lntel fall then ibm go up the next day and if microsoft go up strongly fro one day then decline strongly on the next day and on the same day intel stay about level then ibm stay about level our emphasis is in the discovery of local pattern in multivariate time series in contrast to traditional time series analysis which largely focus on global model thus we search for rule whose condition refer to pattern in time series however we do not want to define beforehand which pattern are to be used rather we want the pattern to be formed fl om the data in the context of rule discovery we describe adaptive method for finding rule of the above type fi om time series data the method are based on discretizing the sequence hy method resembling vector quantization ve first form subsequence by sliding window through the time series and then cluster these subsequence by using a suitable measure of time series similarity the discretized version of the time series is obtained by taldng the cluster identifier corresponding to the subsequence once tl e time series is discretized we use simple rule finding method to obtain rifle from the sequence vve present empmcal resuh s on the behavior of the method 
in many vision problem we want to infer two or more hidden factor which interact to produce our observation we may want to disentangle illuminant and object color in color constancy rendering condition from surface shape in shape from shading face identity and head pose in face recognition or font and letter class in character recognition we refer to these two factor generically a style and content bilinear model offer a powerful framework for extracting the two factor structure of a set of observation and are familiar in computational vision from several well known line of research this paper show how bilinear model can be used to learn the style content structure of a pattern analysis or synthesis problem which can then be generalized to solve related task using different style and or content we focus on three task extrapolating the style of data to unseen content class classifying data with known content under a novel style and translating data from novel content class and style to a known style or content we show example from color constancy face pose estimation shape from shading typography and speech 
we propose an algorithm to resolve anaphor tackling mainly the problem of intrasentential antecedent we base our methodology on the fact that such antecedent are likely to occur in embedded sentence sidner s focusing mechanism is used a the basic algorithm in a more complete approach the proposed algorithm ha been tested and implemented a a part of a conceptual analyser mainly to process pronoun detail of an evaluation are given 
we describe the geometry constraint and algorithmic implementation for metric rectification of plane the rectification allows metric property such a angle and length ratio to be measured on the world plane from a perspective image the novel contribution are first that in a stratified context the various form of providing metric information which include a known angle two equal though unknown angle and a known length ratio can all be represented a circular constraint on the parameter of an affine transformation of the plane this provides a simple and uniform framework for integrating constraint second direct rectification from right angle in the plane third it is shown that metric rectification enables calibration of the internal camera parameter fourth vanishing point are estimated using a maximum likelihood estimator fifth an algorithm for automatic rectification example are given for a number of image and application demonstrated for texture map acquisition and metric measurement 
the estimation and detection of occlusion boundary and moving bar are important and challenging problem in image sequence analysis here we model such motion feature a linear combination of steerable basis flow field these model constrain the interpretation of image motion and are used in the same way a translational or affine motion model we estimate the subspace coefficient of the motion feature model directly from spatiotemporal image derivative using a robust regression method from the subspace coefficient we detect the presence of a motion feature and solve for the orientation of the feature and the relative velocity of the surface our method doe not require the prior computation of optical flow and recovers accurate estimate of orientation and velocity 
manufacturing flaw of all type shape and size can be exhaustively detected a abnormal pixel if process and noise variation can be learned at every pixel in the inspection area this statistical template approach to automated visual inspection is extremely fast effective and flexible while achieving false negative rate critical to this approach are the following novel feature represent both geometry and process information in a model template align d surface with subpixel accuracy compensate for local deformation and texture estimate bimodal distribution robustly this novel paradigm wa applied to the automatic screening of x ray image of turbine blade it ha been validated with over image and shown to out perform regular inspector looking at high pas filtered image 
this paper describes an attentional mechanism based on the interpretation of spectral signature for detecting regular object configuration in area of an image delineated using context information the proposed global operator relies on the spectral analyse s of edge structure and exploit spatial a well a frequency domain constraint derived from known geometrical model of monitored object a decision theoretic method for learning decision region is presented application of this mechanism are demonstrated for several aerial image interpretation task specific example are described for detecting vehicle formation such a convoy qualifying the geometry of detected formation or monitoring the occupancy of region of interest such a parking area road or open area experiment and sensitivity analysis result are reported 
under binocular viewing of a scene there are inevitably region seen only in one eye or camera normally this is a source of trouble for stereopsis algorithm which must deal with these region on non correspondence this paper point out that in fact half occlusion can be a source of valuable information this is done by deriving a formula relating the displacement of an occlusion junction in the two eye image and the depth difference between the scene edge that comprise the occlusion junction this paper represents the first quantitative result on the cue of half occlusion in stereopsis 
many real life problem tackled using constraint programming are dynamic addition and deletion of constraint may lead to inconsistency when a solution is required constraint may be violated in order to fulfillthe user requirement in this paper we propose a best first search method parameterized by the domain of constraint and the associated solver forsolving over constrained problem arising in dynamic environment we proposea specialization of this approach for csp using 
we present a method for computing dense visual correspondence based on general assumption about scene geometry our algorithm doe not rely on correlation and us a variable region of support we assume that image consist of a number of connected set of pixel with the same disparity which we call em disparity component at each pixel we compute a small set of plausible disparity each of which is more likely to be the pixel s true disparity than not a pixel is assigned a disparity d based on connected component of pixel where each pixel in a component considers d to be plausible our implementation chooses the largest plausible disparity component however global contextual constraint can also be applied while the algorithm wa originally designed for visual correspondence it can also be used for other early vision problem such a image restoration it run in a few second on traditional benchmark image with standard parameter setting and give quite promising result 
this paper introduces a new algorithm q for optimizing the expectedoutput of a multi input noisy continuous function q is designed toneed only a few experiment it avoids strong assumption on the formof the function and it is autonomous in that it requires little problemspecifictweaking these capability are directly applicable to industrial process andmay become increasingly valuable elsewhere a the machine learningfield expands beyond prediction and function 
we derive a first order approximation of the density of maximum entropy for a continuous d random variable given a number of simple constraint this result in a density expansion which is somewhat similar to the classical polynomial density expansion by gram charlier and edgeworth using this approximation of density an approximation of d differential entropy is derived the approximation of entropy is both more exact and more robust against outlier than the classical approximation 
we present ordinal measure for establishing image correspondence linear correspondence measure like correlation and the sum of squared difference are known to be fragile ordinal measure which are based on relative ordering of intensity value in window have demonstrable robustness to depth discontinuity occlusion and noise the relative ordering of intensity value in each window is represented by a rank permutation which is obtained by sorting the corresponding intensity data by using a novel distance metric between the rank permutation we arrive at ordinal correlation coefficient these coefficient are independent of absolute intensity scale i e they are normalized measure further since rank permutation are invariant to monotone transformation of the intensity value the coefficient are unaffected by nonlinear effect like gamma variation between image we have developed a simple algorithm for their efficient implementation experiment suggest the superiority of ordinal measure over existing technique under non ideal condition though we present ordinal measure in the context of stereo they serve a a general tool for image matching that is applicable to other vision problem such a motion estimation and image registration 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
we show finite time regret bound for the multiarmed bandit problem under the assumption that all reward come from a bounded and fixed range our regret bound after any number of pull are of the form where and are positive constant not depending on these bound are shown to hold for variant of the popular greedy and boltzmann allocation rule and for a new simple deterministic allocation rule moreover our result also apply to an extension of the basic bandit problem in which reward distribution can depend to some extent from previous pull and observed reward finally we discus the empirical performance of our algorithm with respect to specific choice of the reward distribution 
most decision tree algorithm focus on univariate i e axis parallel test at each internalnode of a tree oblique decision treesuse multivariate linear test at each non leafnode this paper report a novel approach tothe construction of non linear decision tree the crux of this method consists of the generationof new feature and the augmentationof the primitive feature with these newones the resulted non linear decision treesare more accurate than their axis parallel 
we report on the design and evaluation of a visualization tool for information retrieval ir system that aim to help the end user in the following respect a an indicator of document relevance the tool graphically provides specific query related information about individual document a a diagnosis tool it graphically provides aggregate information about the query result that could help in identifying how the different query term influence the retrieval and ranking of document two different experiment using trec data were conducted to evaluate the effectiveness of this tool result while mixed indicate that visualization of this sort may provide useful support for judging the relevance of document in particular by enabling user to make more accurate decision about which document to inspect in detail problem in evaluation of such tool in interactive environment are discussed 
we introduce two new technique for density estimation our approachposes the problem a a supervised learning task which canbe performed using neural network we introduce a stochasticmethod for learning the cumulative distribution and an analogousdeterministic technique we demonstrate convergence of ourmethods both theoretically and experimentally and provide comparisonswith the parzen estimate our theoretical result demonstratebetter convergence property than the parzen 
a traditional approach to extracting geometric information from alarge scene is to compute multiple d depth map from stereo pairsor direct range finder and then to merge the d data however theresulting merged depth map may be subject to merging error if therelative pose between depth map are not known exactly in addition the d data may also have to be resampled before merging which addsadditional complexity and potential source of error this paper provides a mean of directly extracting d data coveringa very wide field of view thus by passing the need for numerousdepth map merging in our work cylindrical image are firstcomposited from sequence of image taken while the camera is rotated about a vertical axis by taking such image panorama atdifferent camera location we can recover d data of the sceneusing a set of simple technique feature tracking an pointstructure from motion algorithm and multibaseline stereo we alsoinvestigate the effect of median filtering on the recovered d pointdistributions and show the result of our approach applied to bothsynthetic and real scene 
we firstly present a variational approach such that during image restoration edge detected in the original image are being preserved and then we compare in a second part the mathematical foundation of this method with respect to some of the well known method recently proposed in the literature within the class of pde based algorithm anisotropic diffusion mean curvature motion min max flow technique the performance of our approach is carefully examined and compared to the classical method experimental result on synthetic and real image will illustrate the capability of all the studied approach 
we present the concept of non rigid matching based on demon by reference to maxwell s demon we contrast this concept with the more conventional viewpoint of attraction we show that demon and attractive point are clearly distinct for large deformation but also that they become similar for small displacement encompassing technique close to optical flow we describe a general iterative matching method based on demon and derive from it three different non rigid matching algorithm one using all the image intensity one using only contour and one for already segmented image at last we present result with synthesized and real deformation with application to computer vision and medical image processing 
we describe a reinforcement learning algorithm for partially observable environment using short term memory which we call blht since blht learns a stochastic model based on bayesian learning the overfitting problem is reasonably solved moreover blht ha an efficient implementation this paper show that the model learned by blht converges to one which provides the most accurate prediction of percept and reward given short term memory 
we introduce a novel method for visual homing using this method a robot can be sent to desired position and orientation in d space specified by single image taken from these position our method is based on recovering the epipolar geometry relating the current image taken by the robot and the target image using the epipolar geometry most of the parameter which specify the difference in position and orientation of the camera between the two image are recovered however since not all of the parameter can be recovered from two image we have developed specific method to bypass these missing parameter and resolve the ambiguity that exist we present two homing algorithm for two standard projection model weak and full perspective our method determines the path of the robot on line the starting position of the robot is relatively not constrained and a d model of the environment is not required the method is almost entirely memoryless in the sense that at every step the path to the target position is determined independently of the previous path taken by the robot because of this property the robot may be able while moving toward the target to perform auxiliary task or to avoid obstacle without this impairing it ability to eventually reach the target position we have performed simulation and real experiment which demonstrate the robustness of the method and that the algorithm always converge to the target pose 
there are many historical manuscript written in a single hand which it would be useful to index example include the w b dubois collection at the university of massachusetts and the early presidential library at the library of congress since optical character recognition ocr doe not work well on handwriting an alternative scheme based on matching the image of the word is proposed for indexing such text the current paper deal with the matching aspect of this process two different technique for matching word are discussed the first method match word assuming that the transformation between the word may be modelled by a translation shift the second method match word assuming that the transformation between the word may be modelled by an affine transform experiment are shown demonstrating the feasibility of the approach for indexing handwriting the method should also be applicable to retrieving previously stored material from personal digital assistant pda 
in this paper we present a novel approach to surface recovery from an image sequence of a rotating object in this approach the object is illuminated under a collinear light source where the light source lie on or near the optical axis and rotated on a controlled turntable a wire frame of d curve on the object surface is extracted by using shading and occluding contour in the image sequence then the whole object surface is recovered by interpolating the surface between curve on the wire frame the interpolation can be done by using geometric or photometric method the photometric method us shading information and is more powerful than geometric method the experimental result on real image sequence of matte and specular surface show that the technique is feasible and promising 
many real life problem require a partial classificationof the data we use the term quot partial classification quot to describe the discovery of model that show characteristicsof the data class but may not cover allclasses and all example of any given class completeclassification may be infeasible or undesirable whenthere are a very large number of class attribute mostattributes value are missing or the class distributionis highly skewed and the user is interested in understanding 
a term list is a list of content word that characterize a consistent text or a concept this paper present a new method for translating a term list by using a corpus in the target language the method first retrieves alternative translation for each input word from a bilingual dictionary it then determines the most coherent combination of alternative translation where the coherence of a set of word is defined a the proximity among multi dimensional vector produced from the word on the basis of co occurrence statistic the method wa applied to term list extracted from newspaper article and achieved translation accuracy for ambiguous word i e word with multiple translation 
this paper present a trainable object detection architecture that is applied to detecting people in static image of cluttered scene this problem pose several challenge people are highly non rigid object with a high degree of variability in size shape color and texture unlike previous approach this system learns from example and doe not rely on any a priori handcrafted model or on motion the detection technique is based on the novel idea of the wavelet template that defines the shape of an object in term of a subset of the wavelet coeficients of the image it is invariant to change in color and texture and can be used to robustly define a rich and complex class of object such a people we show how the invariant property and computational eficiency of the wavelet template make it an effective tool for object detection 
we propose a new method to compute prediction interval especiallyfor small data set the width of a prediction interval doe not only dependon the variance of the target distribution but also on the accuracyof our estimator of the mean of the target i e on the width of the confidenceinterval the confidence interval follows from the variation inan ensemble of neural network each of them trained and stopped onbootstrap replicates of the original data set a second improvement is 
we derive a learning algorithm for inferring an overcomplete basisby viewing it a probabilistic model of the observed data overcompletebases allow for better approximation of the underlyingstatistical density using a laplacian prior on the basis coefficientsremoves redundancy and lead to representation that are sparseand are a nonlinear function of the data this can be viewed asa generalization of the technique of independent component analysisand provides a method for blind 
we present a general encoding decoding framework for interpreting the activity of a populationof unit a standard population code interpretation method the poisson model startsfrom a description a to how a single value of an underlying quantity can generate the activitiesof each unit in the population in casting it in the encoding decoding framework we findthat this model is too restrictive to describe fully the activity of unit in population code inhigher processing area such 
the second order statistic of natural image can be well characterized by a self similar i f power spectrum and the bandpass decomposition in biological vision system is characterized by a self similar wavelet like structuring of the frequency channel it ha thus often been suggested that there might exist a systematic interrelationship between these two property but a complete formal derivation of this relation ha not yet been provided using rate distortion argument and a complexity measure we first show that a self similar bandpass decomposition can achieve a desired level of distortion with a le complex system structure than required for a decomposition in band of equal linear bandwidth a closer analysis reveals that the true optimum decomposition is approximately selfsimilar but show a systematic decrease of the logbandwidths with increasing center frequency of the subbands since this effect ha also been observed in neurophysiological experiment we conclude that the typical property of visual neuron may infact result from an optimized exploitation of the statistical redundancy of the natural environment 
clustering is increasing in importance but linearand even constant time clustering algorithm are often too slow for real time application a simple way to speed up clustering is to speed up the distance calculation at the heart of clustering routine we study two technique for improving the cost of distance calculation lsi and truncation and determine both how much these technique speed up clustering and how much they affect the quality of the resulting cluster we find that the 
a the field of view of a picture is much smaller than our own visual field of view it is common to paste together several picture to create a panoramic mosaic having a larger field of view image with a wider field of view can be generated by using fish eye lens or panoramic mosaic can be created by special device which rotate around the camera s optical center quicktime vr surround video or by aligning and pasting frame in a video sequence to a single reference frame existing mosaicing method have strong limitation on imaging condition and distortion are common manifold projection enables the creation of panoramic mosaic from video sequence under more general condition and in particular the unrestricted motion of a hand held camera the panoramic mosaic is a projection of the scene into a virtual manifold whose structure depends on the camera s motion this manifold is more general than the customary projection onto a single image plane or onto a cylinder in addition to being more general than traditional mosaic manifold projection is also computationally efficient a the only image deformation used are image plane translation and rotation real time software only implementation on a pentium pc prof the superior quality and speed of this approach 
we describe two parallel analog vlsi architecture that integrateoptical flow data obtained from array of elementary velocity sensorsto estimate heading direction and time to contact for headingdirection computation we performed simulation to evaluate themost important qualitative property of the optical flow field anddetermine the best functional operator for the implementation ofthe architecture for time to contact we exploited the divergencetheorem to integrate data 
we present a natural language interface system which is based entirely on trained statistical model the system consists of three stage of processing parsing semantic interpretation and discourse each of these stage is modeled a a statistical process the model are fully integrated resulting in an end to end system that map input utterance into meaning representation frame 
current method for registering image region perform well for simple transformation or large image region the author present a new method that is better able to handle small image region a they deform with nonlinear transformation he introduces difference decomposition a novel approach to solving the registration problem the method is a generalization of previous method and can better handle nonlinear transforms although the method are general he focus on projective transformation and introduces piecewise projective transformation for modeling the motion of non planar object he concludes with example from a prototype implementation 
this paper describes an on going study which applies the concept of transitivity to news discourse for text processing task the complex notion of transitivity is defined and the relatioship between transitivity and information foregrounding is explained a sample corpus of news article ha been coded for transitivity the corpus is being used in two text processing experiment 
in this paper we describe a new recognition method thatuses a subspace representation to approximate the comparison of binaryimages e g intensity edge using the hausdorff fraction the techniqueis robust to outlier and occlusion and thus can be used for recognizingobjects that are partly hidden from view and occur in clutteredbackgrounds we report some simple recognition experiment in whichnovel view of object are classified using both a standard ssd basedeigenspace method 
we derive a sensitivity analysis for moment invariant of multidimensional distribution these invariant have many us in computational system and have recently been used for illumination invariant recognition in color image in this context the sensitivity analysis predicts the response of moment invariant to partial occlusion using the result of the sensitivity analysis we develop a novel surface representation called the invariant profile which capture color distribution and spatial information while remaining invariant to the spectral content of the scene illumination unlike previous representation the recognition of invariant profile doe not require illumination correction we demonstrate the sensitivity analysis and the use of invariant profile for recognition with a set of experiment on color image 
this paper deal with the d structure estimation and exploration of a scene using active vision our method is based on the structure from controlled motion approach which consists in constraining the camera motion in order to obtain a precise and robust estimation of the d structure of a geometrical primitive since this approach involves to gaze on the considered primitive we present a method for connecting up many estimation in order to recover the complete spatial structure of scene composed of cylinder and segment we have developed perceptual strategy able to perform a succession of robust estimation without any assumption on the number and on the localization of the different object furthermore the proposed strategy ensures the completeness of the reconstruction an exploration process centered on current visual feature and on the structure of the previously studied primitive is presented this lead to a gaze planning strategy that mainly us a representation of known and unknown area a a basis for selecting viewpoint finally experiment carried out on a robotic cell have proved the validity of our approach 
geoscience study produce data from various observation experiment and simulation at anenormous rate exploratory data mining extract quot content information quot from massive geoscientificdatasets to extract knowledge and provide a compactsummary of the dataset in this paper wediscuss how database query processing and distributedobject management technique can beused to facilitate geoscientific data mining andanalysis some special requirement of large scalegeoscientific data 
the robustness of shape recovery based on deformable model dep end in general on the relative difference of position and topology of the initial model with respect to the data a close initialization with correct topology guaranty a proper recovery of the object furthermore the closeness of the initial model greatly influence the time of computation needed for the recovery in this paper we propose a method for initializing deformable model from range data or volumetric image the prop osed method solves two distinct problem first we use the topological segmentation of volumetric image in order to recover the approximate topology of the object second we use an efficient mesh sampling algorithm to control the number of vertex of the initial model the method take into account missing data and outlier 
this paper give a widely applicable technique for solving many of the parameter estimation problem encountered in geometric computer vision a commonly used approach is to minimize an algebraic error function instead of a possibly preferable geometric error function it is claimed in this paper that minimizing algebraic error will usually give excellent result and in fact the main problem with most algorithm minimizing algebraic distance is that they do not take account of mathematical constraint that should be imposed on the quantity being estimated this paper give an efficient method of minimizing algebraic distance while taking account of the constraint this provides new algorithm for the problem of resectioning a pinhole camera computing the fundamental matrix and computing the tri focal tensor evaluation result are given for the resectioning and tri focal tensor estimation algorithm 
coarse code are widely used throughout the brain to encode sensoryand motor variable method designed to interpret thesecodes such a population vector analysis are either inefficient i e the variance of the estimate is much larger than the smallest possiblevariance or biologically implausible like maximum likelihood moreover these method attempt to compute a scalar or vectorestimate of the encoded variable neuron are faced with a similarestimation problem they must read 
it is common in nlp that the category into whichtext is classified do not have fully objective definition example of such category are lexicaldistinctions such a part of speech tag and wordsensedistinctions sentence level distinction suchas phrase attachment and discourse level distinctionssuch a topic or speech act categorization this paper present an approach to analyzing theagreement among human judge for the purposeof formulating a refined and more reliable set of 
for the problem of tracking vehicle on freeway using machine vision existing system work well in free flowing traffic traffic engineer however are more interested in monitoring freeway when there is congestion and current system break down for congested traffic due to the problem of partial occlusion we have developed a feature based tracking approach for the task of tracking vehicle under congestion instead of tracking entire vehicle vehicle sub feature are tracked to make the system robust to partial occlusion in order to group together sub feature that come from the same vehicle the constraint of common motion is used here we describe the real time implementation of the system using a network of dsp chip 
because of the distance between the skull and brain and their differentresistivities electroencephalographic eeg data collected fromany point on the human scalp includes activity generated withina large brain area this spatial smearing of eeg data by volumeconduction doe not involve significant time delay however suggestingthat the independent component analysis ica algorithmof bell and sejnowski is suitable for performing blind source separationon eeg data the ica 
classification rule mining aim to discover a small set of rule in the database that form an accurate classifier association rule mining find all the rule existing in the database that satisfy some minimum support and minimum confidence constraint for association rule mining the target of discovery is not pre determined while for classification rule mining there is one and only one pre determined target in this paper we propose to integrate these two mining technique the integration is done by focusing on mining a special subset of association rule called class association rule car an efficient algorithm is also given for building a classifier based on the set of discovered car experimental result show that the classifier built this way is in general more accurate than that produced by the state of the art classification system c in addition this integration help to solve a number of problem that exist in the current classification system 
cumulative training margin distributionsfor adaboost versusour quot direct optimization ofmargins quot doom algorithm the dark curve is adaboost thelight curve is doom doomsacrifices significant training errorfor improved test error horizontalmarks on margin line introductionmany learning algorithm for pattern classification minimize some cost function ofthe training data with the aim of minimizing error the probability of misclassifyingan example one example of such 
this paper present a robust technique to detect local deterioration of old cinematographic film this method relies on spatio temporal information and combine two different detector a morphological detector which us spatial property of deterioration and a dynamic detector based on motion estimation technique our deterioration detector ha been validated on several film sequence and turned out to be a powerful tool for digital film restoration 
this paper address a problem arising in the reverse engineering of solid model from depth map we wish to identify and t surfacesofknowntypewherevertheseareagoodt thispaperpresents a set of method for the least square tting of sphere cylinder cone andtoritothree dimensionalpointdata least squaresttingofsurfaces other plane even of simple geometric type ha been little studied our method ha the particular advantage of being robust in the sense that a the principal curvature of the surface beingtted decrease or become more equal the result which are returned naturally become closerand closertothosesurfacesof simplertype i e plane cylinder cone or sphere which best describe the data unlike other method which may diverge a various parameter or their combination become innite 
a robust and accurate polarization phase based technique for material classification is presented the novelty of this technique is three fold in i it theoretical development ii it application and iii it experimental implementation the concept of phase of polarization of a light wave is introduced to computer vision for discrimination between material according to their intrinsic electrical conductivity such a distinguishing conducting metal and poorly conducting dielectric previous work ha used intensity color and polarization component ratio this new method is based on the physical principle that metal retard orthogonal component of light upon reflection while dielectric do not this method ha significant complementary advantage with respect to existing technique is computationally efficient and can be easily implemented with existing imaging technology experiment for real circuit board inspection non conductive and conductive glass and outdoor object recognition have been performed to demonstrate it accuracy and potential capability 
given three partially overlapping view of a scene from which a set of point correspondence have been extracted recover the three trifocal tensor between the three view we give a new way of deriving the trifocal tensor based on grassmann cayley algebra that shed some new light on it structure we show that our derivation lead to a complete characterization of it geometric and algebraic property which is fairly intuitive i e geometric we give a set of algebraic constraint which 
motivated by a need to define an object centered reference system determined by the most salient characteristic of the shape many method have been proposed all of which directly or indirectly involve an axis about which the shape is locally symmetric recently a function v called the edge strength function ha been successfully used to determine efficiently the ax of local symmetry of d shape the level curve of v are interpreted a succesively smoother version of the initial shape boundary the local minimum of the absolute gradient left nabla v right along the level curve of v are shown to be a robust criterion for determining the shape skeleton more generally at an extremal point of left nabla v right along a level curve the level curve is locally symmetric with respect to the gradient vector nabla v that is at such a point the level curve is approximately a conic section whose one of the principal ax coincides with the gradient vector thus the locus ofthe extremal point of left nabla v right along the level curve determines the ax of local symmetry of the shape in this paper we extend this method to shape of arbitrary dimension 
under a weak perspective camera model the imageplane coordinate in different view of a planarobject are related by an affine transformation becauseof this property researcher have attempted touse affine invariant for recognition however thereare two problem with this approach object orobject class with inherent variability cannot be adequatelytreated using invariant and in practicethe calculated affine invariant can be quite sensitiveto error in the image 
abstract there is strong empirical and theoretic evidence that combination of retrieval method can improve performance in this paper we systematically compare combination strategy in the context of document filtering using query from the tipster reference corpus we find that simple averaging strategy do indeed improve performance but that direet averaging of probability estimate is not the correet approach instead the probabijit y estimate must be renormalized using logistic regression on the known relevance judgment we examine more complex combination strat gy but find them le successful due to the high correlation among our filtering method which are optimized over the same training data and employ similar document represerttations introduction a text filtering system monitor au incoming document 
smooth surface are approximated by polyhedral surface for a number of computational purpose an inherent problem of these approximation algorithm is that the resulting polyhedral surface appear faceted within a recently introduced signal processing approach to solving this problem surface smoothing corresponds to low pas filtering in this paper we look at the filter design problem in more detail we analyze the stability property of the low pas filter described in and show how to minimize it running time we show that most classical technique used to design finite impulse response fir digital filter can also be used to design significantly faster surface smoothing filter finally we describe an algorithm to estimate the power spectrum of a signal and use it to evaluate the performance of the different filter design technique described in the paper 
we present a method for motion estimation using ordinal measure ordinal measure are based on relative ordering of intensity value in an image region called rank permutation while popular measure like the sum of squared difference ssd and normalized correlation ncc rely on linearity between corresponding intensity value ordinal measure only require them to be monotonically related so that rank permutation between corresponding region are presented this property turn out to be useful for motion estimation in tagged magnetic resonance image we study the imaging equation involved in two method of tagging and observe temporal monotonicity in intensity under certain condition though the tag themselves fade we compare our method to ssd and ncc in a rotating ring phantom image sequence we present an experiment on a real heart image sequence which suggests the suitability of our method 
this paper present a network model of the mental lexicon and it formation model of word meaning typically postulate a network of node with connection strength or distance that reflect semantic similarity but seldom explain how the network is formed or how it could be represented in the brain the model presented here is an attempt to address these question the network organizes semantically similar word into cluster when exposed to sequentially presented text lexical co occurrence information is calculated and used to create a hierarchical semantic representation the output is similar to semantic network first described by collins and loftus but is created automatically 
a novel approach is proposed to obtain a record of the patient s occlusion using computer vision data acquisition is obtained using intra oral video camera the technique utilizes shape from shading to extract d information from d view of the jaw and a novel technique for d data registration using genetic algorithm the resulting d model can be used for diagnosis treatment planning and implant purpose the overall purpose of this research is to develop a model based vision system for orthodontics to replace traditional approach this system will be flexible accurate and will reduce the cost of orthodontic treatment 
several researcher have proposed modeling temporally abstract action in reinforcement learningby the combinationof a policyand a termination condition which we refer to a an option value function over option and model of option can be learned using method designed for semi markov decision process smdps however all these method require an option to be executed to termination in this paper we explore method that learn about an option from small fragment of experience consistent with that option even if the option itself is not executed we call these method intra option learning method because they learn from experience within an option intra optionmethodsare sometimes much more efficient than smdp method because they can use off policy temporaldifference mechanism to learn simultaneously about all the option consistent with an experience not just the few that were actually executed inthispaperwepresentintra optionlearningmethodsforlearningvaluefunctionsoveroptions and for learning multi time model of the consequence of option we present computational example in which these new method learn much faster than smdp method and learn effectively when smdp method cannot learn at all we also sketch a convergenceprooffor intraoption value learning 
the vestibulo ocular reflex vor stabilizes image on the retina during rapid head motion the gain of the vor i e the ratio of eye to head rotation velocity measured with the eye focused at a distance is typically around however to stabilize image accurately the vestibulo ocular reflex response must be modulated by the context in which the response take place the context studied in this paper includes eye position eye vergence and head translation we first describe a kinematic model of the vestibulo ocular reflex response given rotational and translational information of the head motion we then show how the model can be modified to rely solely on sensory information available from the semicircular canal head rotation the otoliths head translation and neural correlate of eye position and vergence angle we then suggest a dynamical model and compare it to the eye velocity response measured in monkey snyder and king the vor response is nearly identical to an ideal theoretical response taking into account sensory input delay the model capture the dynamical modulation of the vor for this context it defines which neural signal should be combined and suggests one possible way to perform the required computation in time it therefore provides a theoretical explanation for the sensitivity of neuron to multiple input head rotation and translation eye position etc in the pathway of the vor 
this paper introduces adhoc automatic discoverer of higher order correlation an algorithm that combine the advantage of both filter and feedback model to enhance the understanding of the given data and to increase the efficiency of the feature selection process adhoc partition the observed feature into a number of group called factor that reflect the major dimension of the phenomenon under consideration the set of learned factor define the starting point of the search of the best performing feature subset a genetic algorithm is used to explore the feature space originated by the factor and to determine the set of most informative feature configuration the feature subset evaluation function is the performance of the induction algorithm this approach offer three main advantage i the likelihood of selecting good performing feature grows ii the complexity of search diminishes consistently iii the possibility of selecting a bad feature subset due to over fitting problem decrease extensive experiment on real world data have been conducted to demonstrate the effectiveness of adhoc a data reduction technique a well a feature selection method 
decoding algorithm is a crucial part in statistical machine translation we describe a stack decoding algorithm in this paper we present the hypothesis scoring method and the heuristic used in our algorithm we report several technique deployed to improve the performance of the decoder we also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process we evaluate and compare these technique model in our statistical machine translation system 
to navigate effectively an autonomous agent must be able to quickly and accurately determine it current location given an initial estimate of it position perhaps based on dead reckoning and an image taken of a known environment our agent first attempt to locate a set of landmark real world object at known location then us their angular separation to obtain an improved estimate of it current position unfortunately some landmark may not be visible or worse may be confused with other landmark resulting in both time wasted in searching for the undetected landmark and in further error in the agent s estimate of it position to address these problem we propose a method that us previous experience to learn a selection function that given the set of landmark that might be visible return the subset that can be used to reliably provide an accurate registration of the agent s position we use statistical technique to prove that the learned selection function is with high probability effectively at a local optimum in the space of such function this paper also present empirical evidence using real world data that demonstrate the effectiveness of our approach 
image quantization and dithering are fundamental image processing problem in computer vision and graphic both step are generally performed sequentially and in most case independent of each other color quantization with a pixel wise defined distortion measure and the dithering process with it local neighborhood typically optimize different quality criterion or frequently follow a heuristic approach without reference to any quality measure in this paper we propose a new model to simultaneously quantize and dither color image the method is based on a rigorous cost function approach which optimizes a quality criterion derived from a simplified model of human perception optimization are performed by an efficient multiscale procedure which substantially alleviates the computational burden the quality criterion and the optimization algorithm are evaluated on a representative set of artificial and real world image thereby showing a significant image quality improvement over standard color reduction approach 
many relaxation based smoothing method used in surface reconstruction algorithm filter out the effect of noise in image data but result in the elimination of important discontinuity information a well in this paper the inter pixel interaction during relaxation is shown to be equivalent to a multiple measurement fusion problem which can be solved using optimal estimation theory pixel in a given neighbourhood act a noisy information source combining their information to update the state of that neighbourhood by formulating discontinuity a another noise source in the image and by using the so called curvature consistency reconstruction algorithm on range image it is shown that optimal estimation theory offer a method for the automatic and adaptive localization of discontinuity while providing a smooth piece wise continuous surface description 
we show that we can effectively fit arbitrarily complex animation model to noisy image data our approach is based on least square adjustment using of a set of progressively finer control triangulation and take advantage of three complementary source of information stereo data silhouette edge and d feature point 
the paper describes a transition logic tl and a deductive formalism for it it show how various important aspect such a ramification qualification specificity simultaneity indeterminism etc involved in planning can be modelled in tl in a rather natural way the deductive formalism for tl extends the linear connection method proposed earlier by the author by embedding the latter into classical logic so that classical and resource sensitive reasoning coexist within tl the attraction of a logical and deductive approach to planning is emphasised and the state of automated deduction briefly described 
we present an incremental system that build accurate cad model of object from multiple range image using a hybrid of surface mesh and volumetric representation the system creates a water tight d model at each step of the modeling process allowing reasonable model to be built from a small number of view we also present a method that can be used to plan the next view and reduce the number of scan needed to recover the object result are presented for the creation of d model of a computer game controller a hip joint prosthesis and a mechanical strut 
we present a framework for characterizing bayesianclassification method this framework can bethought of a a spectrum of allowable dependence in agiven probabilistic model with the naive bayes algorithmat the most restrictive end and the learning offull bayesian network at the most general extreme while much work ha been carried out along the twoends of this spectrum there ha been surprising littledone along the middle we analyze the assumptionsmade a one move along this 
we study the dynamic of supervised learning in layered neural network in the regime where the size p of the training set is proportional to the number n of input here the local field are no longer described by gaussian probability distribution we show how dynamical replica theory can be used to predict the evolution of macroscopic observables including the relevant performance measure incorporating the old formalism in the limit p n a a special case for simplicity we restrict ourselves to single layer network and realizable task 
within valiant s model of learning a formalizedby kearns we show that computable totalpredicates for two formally uncomputableproblems the classical halting problem andthe halting problem relative to a specifiedoracle are formally learnable from example to arbitrarily high accuracy with arbitrarilyhigh confidence under any probability distribution the halting problem relative tothe oracle is learnable in time polynomial inthe measure of accuracy confidence and the 
the determination of the position and the orientation of the camera from the known correspondence of the reference point and the image point is known a the problem of pose estimation in computer vision or space resection in photogrammetry it is well known that using corresponding point ha at most solution le appears to be known about the case of and point in this paper we describe linear solution that always give the unique solution to point and point pose determination for the reference point not lying on the critical configuration the same linear method can also be extended to any n point the robustness and accuracy of the method are experimented both on simulated and real image 
this paper proposes a new connectionist approach to numeric law discovery i e neural network law candidate are trained by using a newly invented second order learning algor ithm based on a quasi newton method called bpq and the minimum description length criterion selects the most suitable from lawcandidates the main advantage of our method over previous work of symbolic or connectionist approach is that it can efficiently discover numeric law whose power value are not restricted to integer experiment showed that the proposed method work well in discovering such law even from data containing irrelevant variable or a small amount of noise 
it is known that human can make finer discrimination betweenfamiliar sound e g syllable than between unfamiliar one e g different noise segment here we show that a corresponding enhancementis present in early auditory processing stage based onprevious work which demonstrated that natural sound had robuststatistical property that could be quantified we hypothesize thatthe auditory system exploit those property to construct efficientneural code to test this 
an algorithm for simultaneous detection segmentation and characterization of spatiotemporal periodicity is presented the use of periodicity template is proposed to localize and characterize temporal activity the template not only indicate the presence and location of a periodic event but also give an accurate quantitative periodicity measure hence they can be used a a new mean of periodicity representation the proposed algorithm can also be considered a a periodicity filter a low level model of periodicity perception the algorithm is computationally simple and shown to be more robust than optical flow based technique in the presence of noise a variety of real world example are used to demonstrate the performance of the algorithm 
feature indexing technique are promising for object recognition since they can quickly reduce the set of possible match for a set of image feature this work exploit another property of such technique they have inherently parallel structure and connectionist network formulation are easy to develop once indexing ha been performed a voting scheme such a geometric hashing can be used to generate object hypothesis in parallel we describe a framework for the connectionist implementation of such indexing and recognition technique with sufficient processing element recognition can be performed in a small number of time step the number of processing element necessary to achieve peak performance and the fan in fan out required for the processing element is examined these technique have been simulated on a conventional architecture with good result 
this paper present a new approach to bitext correspondence problem bcp of noisy bilingual corpus based on image processing ip technique by using one of several way of estimating the lexical translation probability ltp between pair of source and target word we can turn a bitext into a discrete gray level image we contend that the bcp when seen in the light bear a striking resemblance to the line detection problem in ip therefore bcps including sentence and word alignment can benefit from a wealth of effective well established ip technique including convolution based filter texture analysis and hough transform this paper describes a new program plotalign that produce a word level bitext map for noisy or non literal bitext based on these technique 
we propose a novel approach for solving the perceptual grouping problem in vision rather than focusing on local feature and their consistency in the image data our approach aim at extracting the global impression of an image we treat image segmentation a a graph partitioning problem and propose a novel global criterion the normalized cut for segmenting the graph the normalized cut criterion measure both the total dissimilarity between the different group a well a the total similarity within the group we show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion we have applied this approach to segmenting static image a well a motion sequence and found the result to be very encouraging 
we argue that the situation calculus is a natural formalism for representing and reasoning about control and strategic information a a case study in this paper we provide a situation calculus semantics for the prolog cut operator the central search control operator in prolog we show that our semantics is well behaved when the program are properly stastified we also show that according to this semantics the conventional implementation of the negationas failure operator using cut is provably correct with respect to the stable model semantics 
this paper describes a general purpose method we have developed for automatically segmenting object of an unknown number and unknown location in image our method integrates deformable model and statistic of image cue including intensity gradient color and texture by using a combination of image feature rather than a single feature such a gradient our method is more robust to noise and sparse data to allow for the automated segmentation of an unknown number and location of object we simultaneously segment object initialized at uniformly distributed point in the image a method is developed to automatically merge model corresponding to the same object result of the method are presentedfor several example including greyscale color and noisy image 
system for text retrieval routing categorization and other ir task rely heavily on linear classifier we propose that two machine learning algorithm the widrow hoff and eg algorithm be used in training linear text classifier in contrast to most ir method theoretical analysis provides performance guarantee and guidance on parameter setting for these algorithm experimental data is presented showing widrow hoff and eg to be more effective than the widely used rocchio algorithm on several categorization and routing task 
we present an analysis of sfm from the point of view of noise this analysis result in an algorithm that is provably convergent and provably optimal with respect to a chosen norm in particular we cast sfm a a nonlinear optimization problem and define a bilinear projection iteration that converges to fixed point of a certain cost function we then show that such fixed point are fundamental i e intrinsic to the problem of sfm and not an artifact introduced by our algorithm we classify and characterize geometrically local extremum and we argue that they correspond to phenomenon observed in visual psychophysics finally we show under what condition it is possible given convergence to a local extremum to jump to the valley containing the optimum this lead u to suggest a representation of the scene which is invariant with respect to such local extremum 
we discus the uniqueness of d shape recovery of a polyhedron from a single shading image first we analytically show that multiple convex and concave shape solution usually exist for a simple polyhedron if interreflection are not considered then we propose a new approach to uniquely determine the concave shape solution using interreflection a a constraint a numerical example in which two convex shape and two concave shape exist for a trihedral corner ha been shown by horn however it is difficult to prove the uniqueness using constraint equation we analytically show that multiple convex and concave shape solution usually exist for a pyramid using a reflectance map if interreflection distribution is not considered however if interreflection distribution is used a a constraint that limit the shape solution for a concave polyhedron the polyhedral shape can be uniquely determined interreflection are used a a constraint to determine the shape solution in our approach 
this paper present an innovative application of the disciple learning agent shell to the building of an educational agent that generates history test for middle school student to assist in the assessment of their understanding and use of higher order thinking skill disciple ha been taught by an educator to generate and answer basic test question and to explain the answer from it interaction with the educational expert disciple ha learned general rule that allow it to generate a large number of new test question for student together with hint answer and explanation of the answer a a result it can guide the student during their practice of higher order thinking skill a they would be directly guided by the educator it can also be used by the educator to generate a different exam for each student in the class disciple ha been experimentally evaluated by history expert student and tea chers with very promising result the work on developing this educational agent illustrates an integration of machine learning knowledge acquisition problem solving and intelligent tu toring system in the context of computer based assessment involving multimedia document 
historically ssd or correlation based visual tracking algorithm have been sensitive to change in illumination and shading across the target region this paper describes method for implementing ssd tracking that is both insensitive to illumination variation and computationally efficient we first describe a vector space formulation of the tracking problem showing how to recover geometric deformation we then show that the same vector space formulation can be used to account for change in illumination we combine geometry and illumination into an algorithm that track large image region on live video sequence using no more computation than would be required to trade with no accommodation for illumination change we present experimental result which compare the performance of ssd tracking with and without illumination compensation 
foveated vision and two mode tracking a inspired by the human oculomotor system are often used in active vision system the purpose of this paper is to provide answer to the following basic question which arise from implementation first is it beneficial to have foveated vision and what is the optimal size of the foveal window second is there a need for two control mechanism smooth pursuit and saccade for improved performance and how can one efficiently switch between them in order to do so a setup is proposed in which these strategy can be evaluated in a systematic manner it is shown that the fovea appears a a compromise between the tightness of the tracking specification and computational constraint introducing a model for the later and postulating some a priori knowledge of the target behavior it is possible to compute the size of the fovea in an optimal way a a by product smooth pursuit can be defined in a natural way and the use of a two mode tracking scheme is justified the second mode i e saccadic control aim at re centering the target on the fovea so that the smooth pursuit controller can continue to operate it is shown that a control strategy can indeed be defined so that this objective can be met under appropriate operating condition 
this paper present a novel approach to the recovery of generic solid part ofobjects from real d image the part vocabulary chosen is the one of geons whichare qualitative volumetric part primitive that are defined by simple but perceptuallyrelevant property which are viewpoint quasi invariant most previous workson detection and recognition of geons from d image relied on quasi perfect linedrawings the use of aspect ha also been proposed for matching fixed template of 
information extraction ie is the problemof filling out pre defined structured summariesfrom text document we are interestedin performing ie in non traditionaldomains where much of the text is oftenungrammatical such a electronic bulletinboard post and web page we suggest thatthe best approach is one that take into accountmany different kind of information and argue for the suitability of a multistrategyapproach we describe learner for iedrawn from three separate machine 
we present a new machine learning method that given a set of training example induces a denition of the target concept in term of a hierarchy of intermediate concept and their denitions this eectively decomposes the problem into smaller le complex problem the method is inspired by the boolean function decomposition approach to the design of digital circuit to cope with high time complexity of nding an optimal decomposition we propose a suboptimal heuristic algorithm the method implemented in program hint hierarchy induction tool is experimentally evaluated using a set of articial and real world learning problem it is shown that the method performs well both in term of classication accuracy and discovery of meaningful concept hierarchy 
this paper considers the effect of spatial quantization on several moment invariant of particular interest are the affine moment invariant which have emerged in recent year a a useful tool for image reconstruction image registration and recognition of deformed object traditional analysis assumes moment and moment invariant for image that are defined in the continuous domain in practice however the digitization process introduces error that violate the invariance assumption this paper present an analysis of quantization induced error on two dimensional hu moment invariant and affine moment invariant and on invariant derived from one dimensional contour moment error bound are given in several case 
this paper address robust feature tracking we extend the well known shi tomasi kanade tracker by introducing an automatic scheme for rejecting spurious feature we employ a simple and efficient outlier rejection rule called x and prove that it theoretical assumption are satisfied in the feature tracking scenario experiment with real and synthetic image confirm that our algorithm make good feature track better we show a quantitative example of the benefit introduced by the algorithm for the case of fundamental matrix estimation the complete code of the robust tracker is available via ftp 
we study the problem of how to detect interesting object appeared in a given image i our approach is to treat it a a function approximation problem based on an over redundant basis and also account for occlusion where the basis superposition principle is no longer valid since the basis a library of image template is over redundant there are infinitely many way to decompose i we are motivated to select a sparse compact representation of i and to account for occlusion and noise we then study a greedy and iterative weighted lp matching pursuit strategy with 
this paper describes a technique for the reconstruction and segmentation of three dimensional acoustical image using a coupled random field able to actively integrate confidence information associated with acquired data beamforming a method widely applied in acoustic imaging is used to build a three dimensional image associated point by point with another kind of information representing the reliability i e confidence of such an image unfortunately this kind of image is plagued by several problem due to the nature of the signal and to the related sensing system thus heavily affecting data quality specifically speckle noise and the broad directivity characteristic of the sensor lead to very degraded image in the proposed algorithm range and confidence image are modelled a markov random field whose associated probability distribution are specified by a single energy functional a threefold process ha been applied able to reconstruct segment and restore the involved acoustic image exploiting both type of data our approach showed better performance with respect to other mrf based method a well a classical method disregarding reliability information optimal in the maximum a posteriori probability sense estimate of the d and confidence image are obtained by minimizing the energy functional by using simulated annealing 
in this paper we examine mechanism for automatic dialogue initiative setting we show how to incorporate initiative changing in a task oriented human computer dialogue system and we evaluate the effect of initiative both analytically and via computer computer dialogue simulation 
in geometrical camera calibration the objective is to determine a set of camera parameter that describe the mapping between d reference coordinate and d image coordinate various method for camera calibration can be found from the literature however surprisingly little attention ha been paid to the whole calibration procedure i e control point extraction from image model fitting image correction and error originating in these stage the main interest ha been in model fitting although the other stage are also important in this paper we present a four step calibration procedure that is an extension to the two step method there is an additional step to compensate for distortion caused by circular feature and a step for correcting the distorted image coordinate the image correction is performed with an empirical inverse model that accurately compensates for radial and tangential distortion finally a linear method for solving the parameter of the inverse model is presented 
this paper address large scale regression tasksusing a novel combination of greedy input selectionand asymmetric cost our primary goalis learning envelope function suitable for automateddetection of anomaly in future sensordata we argue that this new approachcan be more effective than traditional technique such a static red line limit variance based errorbars and general probability density estimation introduction this paper explores the combination of a 
smoothing regularizers for radial basis function have been studied extensively but no general smoothing regularizers for projective basis function pbfs such a the widely used sigmoidal pbfs have heretofore been proposed we derive new class of algebraically simple m th order smoothing regularizers for network of projective basis function our simple algebraic form enable the direct enforcement of smoothness without the need for e g costly monte carlo integration of the smoothness functional we show that our regularizers are highly correlated with the value of standard smoothness functionals and thus suitable for enforcing smoothness constraint onto pbf network the regularizers are tested on illustrative sample problem and compared to quadratic weight decay the new regularizers are shown to yield better generalization error than weight decay when the implicit assumption in the latter are wrong unlike weight decay the new regularizers distinguish between the role of the input and output weight and capture the interaction between them short version of report long version is cse 
we present method for the global reconstruction of some class of special surface the contour ending cusp on the apparent contour is tracked under a dynamic monocular perspective observer the class of surface considered are surface of revolution sor canal surface and ruled surface this paper present theoretical method for surface reconstruction and error analysis of reconstruction under noise we find the technique used exhibit stability even under large noise this work ha added to the accumulating body of work that ha arisen in the computer vision community concerning the differential geometric aspect of special surface class 
the task in text retrieval is to find the subset of a collection of document relevant to a user s information request usually expressed a a set of word classically document and query are represented a vector of word count in it simplest form relevance is defined to be the dot product between a document and a query vector a measure of the number of common term a central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query linear dimensionality reduction ha been proposed a a technique for extracting underlying structure from the document collection in some domain such a vision dimensionality reduction reduces computational complexity in text retrieval it is more often used to improve retrieval performance we propose an alternative and novel technique that produce em sparse representation constructed from set of highly related word document and query are represented by their distance to these set and relevance is measured by the number of common cluster this technique significantly improves retrieval performance is efficient to compute and share property with the optimal linear projection operator and the independent component of document 
a family of structure from motion algorithm called the factorization method ha been recently developed from the orthographic projection model to the affine camera model all these algorithm are limited to handling only point feature of the image stream we propose in this paper an algorithm for the recovery of shape and motion from line correspondence by the factorization method with the affine camera instead of one step factorization for point a multi step factorization method is developed for line based on the decomposition of the whole shape and motion into three separate substructure each of these substructure can then be linearly solved by factorizing the appropriate measurement matrix it is also established that affine shape and motion with uncalibrated affine camera can be achieved with at least seven line over three view which extends the previous result of koenderink and van doorn for point to line 
we develop a local image correspondence algorithm which performs well near occluding boundary unlike traditional robust method our method can find correspondence when the only contrast present is the occluding boundary itself and when the sign of contrast along the boundary is possibly reversed we define a new image transform which characterizes local image homogeneity defined a an attribute value in a central region and most generally a function describing the surrounding local similarity structure in this paper we use radial similarity function and color attribute within each window we compute the central color and an image with the cumulative probability color is unchanged along a ray from the center to a given point in the window this representation is insensitive to structure outside an occluding boundary but can model the boundary itself we show comparative result tracking finger mouth and eye feature 
this paper present a novel multiscale representation of one dimensional signal based on complex analysis we show that a signal and it derivative at different scale can be represented by one analytic function defined on the unit disc in the complex plane which is called the schwarz representation of a signal this representation is applied to the matching problem using the theory of analytic function we are able to define the inverse of a signal the matching function between two signal can be defined a the composition of one signal s schwarz representation and another signal s inverse the matching function determined by this method ha a group structure and is close formed 
image appearance may change over time due to a variety of cause such a object or camera motion generic photometric event including variation in illumination e g shadow and specular reflection and iconic change which are specific to the object being viewed and include complex occlusion event and change in the material property ofthe object we propose a general framework for representing and recovering these appearance change in an image sequence a a mixture of different cause the approach generalizes previous work on optical flow to provide a richer description of image event and more reliable estimate of image motion 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
huge mass of digital data about product customer and competitor have become available for company in the service sector in order to exploit it inherent and often hidden knowledge for improving business process the application of data mining technology is the only way for reaching good and efficient result a opposed to purely manual and interactive data exploration this paper report on a project initiated at swiss life for mining it data resource from the life insurance business based on the data warehouse masy collecting all relevant data from the oltp system for the processing of private life insurance contract a data mining environment is set up which integrates a palette of tool for automatic data analysis in particular machine learning approach special emphasis lie on establishing comfortable data preprocessing support for normalised relational database and on the management of meta data 
this paper present a new algorithm for detecting object in image one of the fundamental task of computer vision the algorithm extends the representational efficiency of eigenimage method to binary feature which are le sensitive to illumination change than gray level value normally used with eigenimages binary feature square subtemplates are automatically chosen on each training image using feature rather than whole template make the algorithm more robust to background clutter and partial occlusion instead of representing the feature with real valued eigenvector principle component we use binary vector quantization to avoid floating point computation the object is detected in the image using a simple geometric hash table and hough transform on a test of image the algorithm work on we present a theoretical analysis of the algorithm in term of the receiver operating characteristic which consists of the probability of detection and false alarm we verify this analysis with the result of our image test and we use the analysis a a principled way to select some of the algorithm s important operating parameter 
we report on our development of a high performance system for neural network and other signal processing application we have designed and implemented a vector microprocessor and packaged it a an attached processor for a conventional workstation we present performance comparison with workstation on neural network backpropagation training the spert ii system demonstrates roughly time the performance of a mid range workstation and five time the performance of a high end workstation with extensive hand optimization of both workstation version 
we predict stock market using information contained in article published on the web mostly textual article appearing in the leading and the most influential financial newspaper are taken a input from those article the daily closing value of major stock market index in asia europe and america are predicted textual statement contain not only the effect e g stock down but also the possible cause of the event e g stock down because of weakness in the dollar and consequently a weakening of the treasury bond exploiting textual information therefore increase the quality of the input the forecast are available real time via www c ust hk beat predict daily at am hong kong time hence all prediction are available before the major asian market start trading several technique such a rule based k nn algorithm and neural net have been employed to produce the forecast those technique are compared with one another a trading strategy based on the system s forecast is suggested 
traditional subspace method for face recognition compute a measure of similarity between image after projecting them onto a fixed linear subspace that is spanned by some principal component vector a k a eigenfaces of a training set of image by supposing a parametric gaussian distribution over the subspace and a symmetric gaussian noise model for the image given a point in the subspace we can endow this framework with a probabilistic interpretation so that bayes optimal decision can be made however we expect that different image cluster corresponding say to different pose and expression will be best represented by different subspace in this paper we study the recognition performance of a mixture of local linear subspace model that can be fit to training data using the expectation maximization algorithm the mixture model outperforms a nearest neighbor classifier that operates in a pca subspace 
a fundamental problem in depth from defocus is the measurement of relative defocus between image we propose a class of broadband operator that when used together provide invariance to scene texture and produce accurate and dense depth map since the operator are broadband a small number of them are sufficient for depth estimation of scene with complex textural property experiment are conducted on both synthetic and real scene to evaluate the performance of the proposed operator the depth detection gain error is le than irrespective of texture frequency depth accuracy is found to be spl sim of the distance of the object from the imaging optic 
the computational cost of conventional filter method for junction characterization is very high this burden can be attenuated by using steerable filter however in order to achieve a high orientational selectivity to characterize complex junction a large number of basis filter is necessary from this result a yet too high computational effort for steerable filter in this paper we present a new method for characterizing junction which keep the high orientational resolution and is computationally efficient it is based on applying rotated copy of a wedge averaging filter and estimating the derivative with respect to the polar angle the new method is compared with the steerable wedge filter method in experiment with real image we show the superiority of our method a well a it adaptability to scale change and robustness against noise 
we introduce a method for segmenting surface of three dimensional object using two image of the object obtained from the same viewpoint under different illumination condition the method allows surface spectral reflectance to vary from point to point and requires only weak condition on the uncalibrated illumination configuration the algorithm is based on the local recovery of an illumination change matrix that depends on surface geometry but not on the spectral reflectance of the surface we show that for typical sensor noise level this technique can be used for the reliable detection of surface orientation change of a few degree this approach can be generalized using a calibrated setup to recover a dense set of surface orientation estimate from two image we present a set of experiment demonstrating the capability of the algorithm for the segmentation of planar surface in the presence of spatially varying spectral reflectance 
language itself he argues that stochasticgrammars play an important role in the handling of a number of theoretical linguisticissues the principal emphasis of the paper is on syntactic issue with little descriptiongiven of the incorporation of semantic and pragmatic factor into a statistical system or of their interaction with syntax in contrast to abney s general discussion of the contribution of statistic to theoreticallinguistics the work presented by kapur and clark in 
we present an approach to the parsing of dependency structure which brings together the notion of parsing a candidate elimination the use of graded constraint and the parallel disambiguation of related structural representation the approach aim at an increased level of robustness by accepting constraint violation in a controlled way combining redundant and possibly conflicting information on different representational level and facilitating partial parsing a a natural mode of behavior 
gaussian process provide good prior model for spatial data but can be too smooth in many physical situation there are discontinuity along bounding surface for example front in near surface wind field we describe a modelling method for such a constrained discontinuity and demonstrate how to infer the model parameter in wind fiel d with mcmc sampling 
we have recently proposed a scale adaptive algorithm for reliable edge detection and blur estimation the algorithm produce a contour code which consists of estimate of position brightness contrast and blur for each edge point in the image here we address two question can scale adaptation be used to achieve precise localization of blurred edge how much of the perceptual content of an image is carried by the d contour code we report an efficient algorithm for subpixel localization and show that local scale control allows excellent precision even for highly blurred edge we further show how local scale control can quantitatively account for human visual acuity of blurred edge stimulus to address the question of perceptual content we report an algorithm for inverting the contour code to reconstruct an estimate of the original image while reconstruction based on edge brightness and contrast alone introduces significant artifact restitution of the local blur signal is shown to produce perceptually accurate reconstruction 
like model selection in statistic the choice of appropriate data mining algorithm dm algorithm is a very important task in the process of knowledge discovery due to this fact it is necessary to have sophisticated metric that can be used a comparators to evaluate alternative dmalgorithms it ha been shown in literature that data envelopment analysis dea is an appropriate platform to develop multi criterion evaluation metric that can consider in contrary to mono criterion metric all positive and negative property of dm algorithm we discus different extension of dea that enable consideration of qualitative property of dm algorithm and consideration of user preference in development of evaluation metric the result open new discussion in the general debate on model selection in statistic and machine learning 
perceptron decision tree also known a linear machine dts etc are analysed in order that data dependent structural risk minimizationcan be applied data dependent analysis is performed which indicates thatchoosing the maximal margin hyperplanes at the decision node will improvethe generalization the analysis us a novel technique to bound thegeneralization error in term of the margin at individual node experimentsperformed on real data set confirm the validity of the 
in this paper we consider learning first order horn programsfrom entailment in particular we show that any subclass of first orderacyclic horn program with constant arity is exactly learnable fromequivalence and entailment membership query provided it allows apolynomial time subsumption procedure and satisfies some closure condition one consequence of this is that first order acyclic determinatehorn program with constant arity are exactly learnable from equivalenceand 
this paper present a new approach to hierarchical reinforcement learning based on the maxq decomposition of the value function the maxq decomposition ha both a procedural semantics a a subroutine hierarchy and a declarative semantics a a representation of the value function of a hierarchical policy maxq unifies and extends previous work on hierarchical reinforcement learning by singh kaelbling and dayan and hinton condition under which the maxq decomposition can represent the optimal value function are derived the paper defines a hierarchical q learning algorithm prof it convergence and show experimentally that it can learn much faster than ordinary flat q learning finally the paper discus some interesting issue that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non hierarchical execution of the maxq hierarchy 
recent experiment show that the neural code at work in a wide range of creature share some common feature at first sight these observation seem unrelated however we show that these feature arise naturally in a linear filtered threshold crossing model when we set the threshold to maximize the transmitted information this maximization process requires neural adaptation to not only the dc signal level a in conventional light and dark adaptation but also to the statistical structure of the signal and noise distribution we also present a new approach for calculating the mutual information between a neuron s output spike train and any aspect of it input signal which doe not require reconstruction of the input signal this formulation is valid provided the correlation in the spike train are small and we provide a procedure for checking this assumption this paper is based on joint work deweese m optimization principle for the neural code dissertation princeton university preliminary result from the linear filtered threshold crossing model appeared in a previous proceeding deweese m and bialek w information flow in sensory neuron nuovo cimento d and the conclusion we reached at that time have been reaffirmed by further analysis of the model 
we present an extension of the usual projective geometric framework for computer vision which can nicely take into account an information that wa previously not used i e the fact that the pixel in an image correspond to point which lie in front of the camera this framework called the oriented projec tive geometry retains all the advantage of the unoriented projective geometry namel y it simplicity for expressing the viewing geometry of a system of camera while extending it adequation to model realistic situation we discus the mathematical and practical issue raised by this new framework for a number of computer vision algorithm we present different experiment where this new tool clearly help 
naive bayes induction algorithm were previously shown to be surprisingly accurate on many classification task even when the conditional independence assumption on which they are based is violated however most study were done on small database we show that in some larger database the accuracy of naive bayes doe not scale up a well a decision tree we then propose a new algorithm nbtree which induces a hybrid of decision tree classifier and naive bayes classifier the 
in recent year the interest of investor ha shifted to computerized asset allocation portfolio management to exploit the growing dynamic of the capital market in this paper asset allocation is formalized a a markovian decision problem which can be optimized by applying dynamic programming or reinforcement learning based algorithm using an artificial exchange rate the asset allocation strategy optimized with reinforcement learning q learning is shown to be equivalent to a policy 
a data warehouse grow to the point where one hundred gigabyte is considered small the computational efficiency of data mining algorithm on large database becomes increasingly important using a sample from the database can speed up the datamining process but this is only acceptable if it doe not reduce the quality of the mined knowledge to this end we introduce the probably close enough criterion to describe the desired property of a sample sampling usually refers to the use of static statistical test to decide whether a sample is sufficiently similar to the large database in the absence of any knowledge of the tool the data miner intends to use we discus dyrz mic sampling method which take into account the mining tool being used and can thus give better sample we describe dynamic scheme that observe a mining tool s performance on training sample of increasing size and use these result to determine when a sample is sufficiently large we evaluate these sampling method on data from the uc repository and conclude that dynamic sampling is preferable 
previous work ha developed an approach for estimating shape and albedo from multiple image assuming lambertian reflectance with single light source the main contribution of this paper are i to show how the approach can be generalized to include ambient background illumination ii to demonstrate the use of the integrability constraint for solving this problem and iii an iterative algorithm which is able to improve the analysis by finding shadow and rejecting them 
if it is to qualify a knowledge a learner soutput should be accurate stable and comprehensible learning multiple model canimprove significantly on the accuracy andstability of single model but at the costof losing their comprehensibility when theypossess it a do for example simple decisiontrees and rule set this paper proposes andevaluates cmm a meta learner that seek toretain most of the accuracy gain of multiplemodel approach while still producinga single 
we compute the sign of gaussian curvature using a purely geometric definition consider a point p on a smooth surface s and a closed curve g on s which encloses p the image of g on the unit normal gaussian sphere is a new curve b the gaussian curvature at p is defined a the ratio of the area enclosed by g over the area enclosed by b a g contract to p the sign of gaussian curvature at p is determined by the relative orientation of the closed curve g and b we directly compute the relative orientation of two such curve from intensity data we employ three unknown illumination condition to create a photometric scatter plot this plot is in one to one correspondence with the subset of the unit gaussian sphere containing the mutually illuminated surface normal this permit direct computation of the sign of gaussian curvature without the recovery of surface normal our method is albedo invariant we assume diffuse reflectance but the nature of the diffuse reflectance can be general and unknown simulation a well a empirical result demonstrate the accuracy of our technique 
in this paper we describe a method for estimating the internal parameter of the left and right camera associated with a stereo image pair the stereo pair ha known epipolar geometry and therefore d projective reconstruction of pair of matched image point is available the stereo pair is allowed to move and hence there is a collineation relating the two projective reconstruction computed before and after the motion we show that this collineation ha similar but different 
this paper present splice a batch metalearningsystem designed to learn locally stableconcepts in domain with hidden changesin context the majority of machine learningalgorithms assume that target concept remainstable over time in many domain thisassumption is invalid for example financialprediction medical diagnosis and networkperformance are domain in which targetconcepts may not remain stable unstabletarget concept are often due to changesin a hidden context 
optimality theory a constraint based phonology and morphology paradigm ha allowed linguist to make elegant analysis of many phenomenon including infixation and reduplication in this work in progress we build on the work of ellison to investigate the possibility of using ot a a parsing tool that derives underlying form from surface form 
field ha suggested that neuron with line and edge selectivitiesfound in primary visual cortex of cat and monkey form a sparse distributedrepresentation of natural scene and barlow ha reasonedthat such response should emerge from an unsupervised learning algorithmthat attempt to find a factorial code of independent visual feature weshow here that non linear infomax when applied to an ensemble of naturalscenes produce set of visual filter that are localised 
we develop a simple and very fast method for object tracking based exclusively on color information in digitized video image running on a silicon graphic r indy system with an indycam our algorithm is capable of simultaneously tracking object at full frame size time pixel and video frame rate fps robustness with respect to occlusion is achieved via an explicit hypothesis tree model of the occlusion process we demonstrate the efficacy of our technique in the challenging task of tracking people especially tracking human head and hand 
abstract in extended video sequence individual frame are grouped into shot which are defined a a sequence taken by a single camera and related shot are grouped into scene which are defined by a single dramatic event taken by a small number of related camera this hierarchical structure is deliberately constructed dictated by the limitation and preference of the human visual and memory system we present three novel high level segmentation result derived from these consideration some of which are analogous to those involved in the perception of the structure of music first and primarily we derive and demonstrate a method for measuring probable scene boundary by calculating a short term memory based model of shot to shot coherence the detection of local minimum in this continuous measure permit robust and flexible segmentation of the video into scene without the necessity for first aggregating shot into cluster second and independently of the first we then derive and demonstrate a one pas on the fly shot clustering algorithm third we demonstrate partially successful result on the application of these two new method to the next higher theme level of video structure index term digital and video library low level vision color and texture pattern analysis segmentation and grouping vision based computer interface 
robust technique are developed for determining structure from motion in the uncalibrated case the structure recovery is based on previous work in which it wa shown that a camera undergoing unknown motion and having an unknown and possibly varying focal length can be self calibrated via closedform expression in the entry of two matrix derivable from an instantaneous optical flow field critical to the recovery process is the obtaining of accurate numerical estimate up to a scalar factor of these matrix in the presence of noisy optical flow data we present technique for the determination of these matrix via least square method and also a way of enforcing a dependency constraint that is imposed on these matrix a method for eliminating outlying flow vector is also given result of experiment with real image sequence are presented that suggest that the approach hold promise 
estimating motion in scene containing multiple moving object remains a di cult problem in computer vision a promising ap proach to this problem involves using mixture model where the motion of each object is a component in the mixture however ex isting method typically require specifying in advance the number of component in the mixture i e the number of object in the scene 
technique of exploratory data analysis areused to study the weight of evidence that the occurrenceof a query term provides in support of the hypothesisthat a document is relevant to an information need inparticular the relationship between the document frequencyand the weight of evidence is investigated acorrelation between document frequency normalized bycollection size and the mutual information between relevanceand term occurrence is uncovered this correlationis found to be 
we are proposing a new framework of statistical language modeling which integrates lexical association statistic with syntactic preference while maintaining the modularity of those different statistic type facilitating both training of the model and analysis of it behavior in this paper we report the result of an empirical evaluation of our model where the model is applied to disambiguation of dependency structure of japanese sentence we also discussed the room remained for further improvement based on our error analysis 
one issue for autonomous mobile robot operating in unknown or partially known domain is how to handle uncertainty in their sensor observation over time method such a probablistic belief network and survivor function are generally unsatisfactory because they require explicit model of the robot s interaction with it environment including possible contravening event this information is difficult to obtain and is philosophically incompatible with reactive behavior this paper present an approach which eliminates the need for explicit model and reasoning instead it relies solely on directly perceivable attribute of the robot object and environment the attribute qualitatively rate whether the robot s current observation are from an inherently more informed state than previous reading e g from a better viewpoint observation from more informed state have different rate for the accrual and attrition of belief than those taken from le informed state this paper describes the implementation focusing on how the information state is computed using fuzzy logic and how the state dynamically adapts a variation of dempster s rule to generate the total belief data from a mobile robot tracking an unknown object demonstrates that the reactive computation of belief over time performs well for six canonical accrual and attrition case 
in a recent paper friedman geiger and goldszmidt introduced a classifier based on bayesian network called tree augmented naive bayes tan that outperforms naive bayes and performs competitively with c and other state of the art method this classifier ha several advantage including robustness and polynomial computational complexity one limitation of the tan classifier is that it applies only to discrete attribute and thus con tinuous attribute must be prediscretized in this paper we extend tan to deal with continuous attribute directly via parametric e g gaussians and semiparametric e g mixture of gaussians conditional probability the resu lt is a classifier that can represent and combine both discrete and continuous attribute in addition we propose a new method that take advantage of the modeling language of bayesian network in order to represent attribute both in discrete and continuous form simultaneously and use both version in the classification this automates the process of deciding which form of the attribute is most relevant to the classification task it also avoids the commitment to either a discretized or a semi parametric form since different attribute may correlate better with one version or the other our empirical result show that this latter method usually achieves classification performance that is a good a or better than either the purely discrete or the purely continuous tan model 
an approach to clustering is presented that adapts the basic top down induction of decision tree method towards clustering to this aim it employ the principle of instance based learning the resulting methodology is implemented in the tic top down induction of clustering tree system for first order clustering the tic system employ the first order logical decision tree representation of the inductive logic programming system tilde various experiment with tic are presented in both propositional and relational domain 
automatic target recognition atr application require simultaneously a wide field of view fov for better detection and situation awareness high resolution for target recognition and threat assessment and high frame rate for detecting brief event and disambiguating frame to frame correlation uniformly sampling the entire fov at recognition resolution is simply wasteful in atr scenario with localized region of interest roi foveal data acquisition with space variant sampling and contextsensitive sensor articulation is highly optimized for active atr application we propose a multiscale local zernike filter based front end target detection technique for a commercially feasible foveal sensor topology with piecewise constant resolution profile anisotropic heat diffusion is employed for preprocessing of the foveal data expansion template matching is used to derive a detection filter that optimizes the discriminant signal to noise ratio snr result are presented with simulated foveal imagery derived from real uniform acuity flir data 
this paper describes a simple and efficient method to make template based object classification invariant to in plane rotation the task is divided into two part orientation discrimination and classification the key idea is to perform the orientation discrimination before the classification this can be accomplished by hypothesizing in turn that the input image belongs to each class of interest the image can then be rotated to maximize it similarity to the training image in each class these contain the prototype object in an upright orientation this process yield a set of image at least one of which will have the object in an upright position the resulting image can then be classified by model which have been trained with only upright example this approach ha been successfully applied to two real world vision based task rotated handwritten digit recognition and rotated face detection in cluttered scene 
a new variant on key feature object recognition is presented it is applied to optimal matching problem involving d line segment model and data a single criterion function rank both key feature and complete object model match empirical study suggest that the key feature algorithm ha run time which are dramatically le than a more general random start local search algorithm however they also show the key feature algorithm to be brittle failing on some apparently simple problem while local search appears to be robust 
following the success of applying deformable model to feature extraction a natural next step is to apply such model to pattern classification recently we have cast a deformable model under a bayesian framework for classification giving promising result however deformable model method are computationally expensive due to the required iterative optimization process the problem is even more severe when there are a large number of model e g for character recognition because each of them ha to deform and match with the input data before a final classification can be derived in this paper we propose to combine the deformable model into a mixture in which the individual model compete with each other to survive the matching process during classification model that do not compete well are eliminated early thus allowing substantial saving in computation this process of competition elimination ha been applied to handwritten digit recognition in which significant speedup can be achieved without sacrificing recognition accuracy 
an iterative predict and match system render cadmodels to refine prediction about detectable feature inboth optical and range sensor data predicted featuresadapt to a scene specific interpretation which includesa time of day lighting model and reasoning about unmodeledoccluding surface during rendering linksbetween the rendered image and the d cad modelare retained to propagate information between objectmodel scene and sensor coordinate reference frame precise geometric 
we present a vision system for the d model based tracking of unconstrained human movement using image sequence acquired simultaneously from multiple view we recover the d body pose at each time instant without the use of marker the pose recovery problem is formulated a a search problem and entail finding the pose parameter of a graphical human model whose synthesized appearance is most similar to the actual appearance of the real human in the multi view image the model used for this purpose are acquired from the image we use a decomposition approach and a best first technique to search through the high dimensional pose parameter space a robust variant of chamfer matching is used a a fast similarity measure between synthesized and real edge image we present initial tracking result from a large new human in action hia database containing more than frame in each of four orthogonal view they contain subject involved in a variety of activity of various degree of complexity ranging from the more simple one person hand waving to the challenging two person close interaction in the argentine tango 
at the heart of many optimization procedure are powerful pruning and propagation rule this paper present a case study in the construction of such rule we develop a new algorithm complete decreasing best fit that find the optimal packing of object into bin the algorithm use a branching rule based on the well known decreasing best fit approximation algorithm in addition it includes a powerful pruning rule derived from a bound on the solution to the remaining subproblem the bound is constructed by using modular arithmetic to decompose the numerical constraint we show that the pruning rule add essentially a constant factor overhead to runtime whilst reducing search significantly on the hardest problem runtime can be reduced by an order of magnitude finally we demonstrate how propagation rule can be built by adding lookahead to pruning rule this general approach optimization procedure built from branching rule based on good approximation algorithm and pruning and propagation rule derived from bound on the remaining subproblem may be effective on other np complete problem 
this paper describes a new method for face recognition under drastic change of the imaging process through which the facial image are acquired unlike the conventional method that use only the face feature the present method exploit the statistical information of the variation between the face image set being compared in addition to the feature of the face themselves to incorporate both of the face and perturbation feature for recognition we develop a technique called weak orihogonalization of the two subspace that transforms the given two overlapped subspace so that the volume of the intersection of the resulting two subspace is minimized matching operation are performed in the transformed face space that ha thus been weakly orihogonalized against perturbation space experimental result on real picture of the frontal face from driver license show that the new algorithm improves the recognition performance over the conventional method we also demonstrate the effectiveness of our method on image set with change in viewing geometry 
a novel geometric approach for d object segmentation and representation is presented the scheme is based on geometric deformable surface moving towards the object to be detected we show that this model is equivalent to the computation of surface of minimal area better known a minimal surface in a riemannian space this space is defined by a metric induced from the d image volumetric data in which the object are to be detected the model show the relation between classical deformable surface obtained via energy minimization and geometric one derived from curvature based flow the new approach is stable robust and automatically handle change in the surface topology during the deformation based on an efficient numerical algorithm for surface evolution we present example of object detection in real and synthetic image 
we study generalization capability of the mixture of expert learningfrom example generated by another network with the samearchitecture when the number of example is smaller than a criticalvalue the network show a symmetric phase where the expertsdoes not specialize unon crossing the critical point the systemundergoes a continuous phase transition to a symmetry breakingphase where the gating network partition the input space effectivelyand each expert is assigned to an 
we study the integration of background knowledge and concept learning genetic algorithm and show how they have been integrated in the system dogma our emphasis is in speeding up the inductive learning process by using suggestion from the background knowledge to direct genetic search we don t do theory revision by patching the old theory rather we build a new theory by using part of the background knowledge result show that the methodology can lead to better result a well a to clear saving in computational effort compared to learning with purely inductive gas 
an important success factor for the field of kdd lie in the development and integration of method for supporting the construction and execution of kdd process crucial aspect in this context are the incremental development of a precise problem description a decomposition of this top level problem description into manageable and compatible subtasks which can be reused and a selection and combination of adequate algorithm for solving these subtasks in this paper we describe an approach 
this contribution attempt to move beyond the status where single moving object in video image sequence are tracked separately in the scene domain based on individually adapted approach and parameter instead we investigate which performance can be achieved by a combination of approach based on edge element orientation and on optical flow applied to a variety of image sequence and vehicle five different image sequence of traffic scene recorded under different condition have been evaluated quantitative statement are provided about the success rate of the approach after evaluating over full video frame i e more than minute of real world video using one single approach and a single parameter set remaining tracking failure are analyzed and classified 
communication can be more effective when several medium such a text speech or graphic are integrated and coordinated to present information this change the nature of medium specific generation e g language generation which must take into account the multimedia context in which it occurs in this paper will present work on coordinating and integrating speech text static and animated d graphic and stored image a part of several system we have developed at columbia university a particular focus of our work ha been on the generation of presentation that brief a user on information of interest 
a method for calculating optic flow using robust statistic is developed the method generally out performs all competing method in term of accuracy one of the key feature in the success of this method is that we use least median of square which is known to be robust to outlier the computational cost is kept very low by using an approximate solution to the least median of square only in a first stage that detects outlier the essential ingredient of our method should be applicable in a wide range of other computer vision problem 
a robust iterative approach is introduced for finding the dominant plane in a scene using binocular vision neither camera calibration nor stereo correspondence is required recently cohen formalized a framework guaranteeing local convergence of iterative two step method in this paper the framework is adopted with a global step using tentative match to estimate the planar projectivity and a local step attempting to solve the stereo correspondence a detected point in the first image is matched to an auxiliary point in the second image on the line joining the transformed first image point and it closest detected second image point convergence is assured while achieving robustness to both mismatching and non coplanar point 
a method of localising object in image is proposed possible configuration are evaluated using the contour discriminant a likelihood ratio which is derived from a probabilistic model of the feature detection process we treat each step in this process probabilistically including the occurrence of clutter feature and derive the observation density for both correct target configuration and incorrect clutter configuration the contour discriminant distinguishes target object from the background even in heavy clutter making only the most general assumption about the form that clutter might take the method generates sample stochastically to avoid the cost of processing an entire image and promise to be particularly suited to the task of initialising contour tracker based on sampling method 
this paper describes a method to upgrade projective reconstruction to affine and to metric reconstruction using rigid gener almotions of a stereo rig we make clear the algebraic relationship between projective reconstruction the plane at infinity affine reconstruction camera calibration and metric reconstruction we show that all the computation can be carried out using standard linear resolution method and that these method compare favorably with nonlinear optimization method in the presence of gaussian noise we carry out a theoretical error analysis which quantify the relative importance of the accuracy of projective to affine conversion and affine to euclidean conversion experiment with real data are consistent with the theoretical error analysis and with a sensitivity analysis performed with simulated data 
in this paper we propose a machine learningsolution to problem consisting of many similarprediction task each of the individualtasks ha a high risk of overfitting we combinetwo type of knowledge transfer betweentasks to reduce this risk multi task learningand hierarchical bayesian modeling multitasklearning is based on the assumption thatthere exist feature typical to the task athand to find these feature we train a hugetwo layered neural network each task hasits own 
in a new object representation using appearance based part and relation to recognize d object from d image in the presence of occlusion and background clutter wa introduced appearance based part and relation are defined in term of closed region and the union of these region respectively the region are segmented using the mdl principle and their appearance is obtained from collection of image and compactly represented by parametric manifold in the eigenspaces spanned by the part and the relation in this paper we introduce the discriminatory power of the proposed feature and describe how to use it to organize large database of object 
we are interested in description of d data set a obtained from stereo or a d digitizer we therefore consider a input a sparse set of point possibly associated with orientation information in this paper we address the problem of inferring integrated high level description such a surface curve and junction from a sparse point set while the method described previously provides excellent result for smooth structure it only detects discontinuity but doe not localize them for precise localization we propose a non iterative cooperative algorithm in which surface curve and junction work together initial estimate are computed based on previous result where each point in the given sparse and possibly noisy point set is convolved with a predefined vector mask to produce dense saliency map these map serve a input to our novel maximal surface and curve marching algorithm for initial surface and curve extraction refinement of initial estimate is achieved by hybrid voting using excitatory and inhibitory field for inferring reliable and natural extension so that surface curve and curve junction discontinuity are preserved result on several synthetic a well a real data set are presented 
over recent year symmetry research ha shifted from the detection of affinely to perspectively skewed mirror symmetry also link between invariance research and symmetry specific geometric constraint have been established the paper aim to contribute to both strand several set of symmetry specific invariant are derived that can be used in different situation depending on the a priori assumption made it is also argued that all the result directly apply to the case of perspectively skewed point symmetry 
data mining over large data set is important due to it obvious commercial potential however it is also a major challenge due to it computational complexity exploiting the inherent parallelism of data mining algorithm provides a direct solution by utilising the large data retrieval and processing power of parallel architecture in this paper we present some result of our intensive research on paralielising data mining algorithm in particular we also present a methodology for determining the proper parallelisation strategy based on the idea of algorithmic skeleton and performance modelling this research aim to provide a systematic way to develop parallel data mining algorithm and application 
propositional strip planning problem can be viewed a finite state automaton fsas represented in a factored form automaton minimization is a well known technique for reducing the size of an explicit fsa recent work in computer aided verification on model checking ha extended this technique to provide automaton minimization algorithm for factored fsas in this paper we consider the relationship between strip problem solving technique such a regression and the recently developed automaton minimization technique for factored fsas we show that regression computes a partial and approximate minimized form of the fsa corresponding to the strip problem we then define a systematic form of regression which computes a partial but exact minimized form of the associated fsa we also relate minimization to method for performing reachability analysis to detect irrelevant fluents finally we show that exact computation of the minimized automaton is np complete under the assumption that this automaton is polynomial in size 
in this paper we present an automatic system for analyzing and annotating video sequence of technical talk our method us a robust motion estimation technique to detect key frame and segment the video sequence into subsequence containing a single overhead slide the subsequence are stabilized to remove motion that occurs when the speaker adjusts their slide any change remaining between frame in the stabilized sequence may be due to speaker gesture such a pointing or writing and we use active contour to automatically track these potential gesture given the constrained domain we define a simple vocabulary of action which can easily be recognized based on the active contour shape and motion the recognized action provide a rich annotation of the sequence that can be used to access a condensed version of the talk from a web page 
this paper is concerned with the problem of tracking cloud structure like vortex in meteorological image for this purpose we characterize the deformation between two successive occurrence by matching their two boundary curve our approach is based on the computation of the set of path connecting the two curve to be matched it minimizes a cost function which measure the local similarity of the two curve these matching path are obtained a geodesic curve on this cost surface moreover our method allows to consider complex curve of arbitrary topologysince these curve are represented through an implicit function rather than through a parameterization experimental result are given to illustrate the property of themethod in processing synthetic and then meteorologic remotely sensed data 
we investigate the structure of model selection problem via the bias variance decomposition in particular we characterize the essential structure of a model selection task by the bias and variance profile it generates over the sequence of hypothesis class this lead to a new understanding of complexity penalization method first the penalty term in effect postulate a particular profile for the variance a a function of model complexity if the postulated and true profile do not match then systematic under fitting or over fitting result depending on whether the penalty term are too large or too small second it is usually best to penalize according to the true variance of the task and therefore no fixed penalization strategy is optimal across all problem we then use this bias variance characterization to identify the notion of easy and hard model selection problem in particular we show that if the variance profile grows too rapidly in relation to the bias then standard model selection technique become prone to significant error this can happen for example in regression when the independent variable are drawn from wide tailed distribution finally we discus a new model selection strategy that dramatically outperforms standard complexity penalization and hold out method on these hard task 
we discus the temporal difference learning algorithm a applied to approximating the cost to go function of an infinite horizon discounted markov chain the algorithm we analyze update parameter of a linear function approximator on line during a single endless trajectory of an irreducible aperiodic markov chain with a finite or infinite state space we present a proof of convergence with probability a characterization of the limit of convergence and a bound on the resulting 
in this paper we propose an algorithm for doing reconstruction of general sd curve from a number of d image taken by uncalibrated camera no point correspondence between the image are assumed the curve and the view point are uniquely reconstructed module common projective transformation and the point correspondence problem is solved furthermore the algorithm is independent of the choice of coordinate a it is based on orthogonal projection and aligning subspace the algorithm is based on an extension of afine shape of finite point configuration to more general object 
our approach for the robocup competition is to emphasize teamwork among agent by a ugmenting reaction based on awareness of t he c urrent situation with prediction based on predefined multiagent m aneuvers these prediction are accomplished by allowing agent to cooperatively a ccomplish p redefined p lan which are elaborated reactively and hierarchically to ensure re sponsiveness to changing circumstance by supporting the r untime c onstruction of plan our approach simplifies the introduction of new plan strategy and action and produce a framework for dynamic adaptation and plan recognition through automatically generating belief network our implementation is built on top of um pr a procedural reasoning system architecture for real time environment which allows specifying executing and integrating plan based on subgoaling and precondition 
selecting the appropriate spatial scale for local edge analysis is a challenge for natural image where blur scale and contrast may vary over a broad range while previous method for scale adaptation have required the global solution of a non convex optimization problem it is shown that knowledge of sensor property and operator norm can be exploited to define a unique locally computable minimum reliable scale for local estimation the resulting method for local scale control allows edge spanning a broad range of blur scale and contrast to be reliably localized by a single system with no input parameter other than the second moment of the sensor noise local scale control further permit the reliable estimation of local blur scale in complex image where the condition demanded by fourier method for blur estimation break down 
we introduce an extended representation of time series that allows fast accurate classification and clustering in addition to the ability to explore time series data in a relevance feedback framework the representation consists of piecewise linear segment to represent shape and a weight vector that contains the relative importance of each individual linear segment in the classification context the weight are learned automatically a part of the training cycle in the relevance feedback context the weight are determined by an interactive and iterative process in which user rate various choice presented to them our representation allows a user to define a variety of similarity measure that can be tailored to specific domain we demonstrate our approach on space telemetry medical and synthetic data 
using a combination of technique from visual representation view synthesis and visual motor model estimation we present a method for animating movement of an articulated agent e g human or robot arm without the use of any prior model or explicit d information the information needed to generate simulated image can be acquired either on or off line by watching the agent doing an arbitrary possibly unrelated task we present experimental result synthesizing image sequence of the simulated movement of a human arm and a puma robot arm control is in either image camera motor joint or cartesian world coordinate we have created a user interface where a user can input a movement program and then upon execution view movie of the simulated agent executing the program along with the instantaneous value of the dynamic variable 
this paper present probabilistic modeling method to solve the problem of discriminating between five facial orientation with very little labeled data three model are explored the first model maintains no inter pixel dependency the second model is capable of modeling a set of arbitrary pair wise dependency and the last model allows dependency only between neighboring pixel we show that for all three of these model the accuracy of the learned model can be greatly improved by augmenting a small number of labeled training image with a large set of unlabeled image using expectation maximization this is important because it is often difficult to obtain image label while many unlabeled image are readily available through a large set of empirical test we examine the benefit of unlabeled data for each of the model by using only two randomly selected labeled example per class we can discriminate between the five facial orientation with an accuracy of with six labeled example we achieve an accuracy of 
we have developed a video rate stereo machine that ha the capability of generating a dense depth map at the video rate the performance bench mark of the cmu video rate stereo machine are multi image input of up to camera throughput of million point disparity measurement per second frame rate of frame sec a dense depth map of up to x pixel disparity search range of up to pixel high precision of depth output up to bi t with interpolation the capability of passively producing such a dense depth map d representation of a scene at the video rate can open up a new class of application of d vision merging real and virtual world in real time 
no company so far achieved the ultimate goal of zero fault in manufacturing even high quality product occasionally show problem that must be handled a warranty case in this paper we report work done during the development of an early warning system for a large quality information database in the automotive industry we present a multi strategy approach to flexible prediction of upcoming quality problem we used existing technique and combined them in a novel way to solve a concrete application problem the basic idea is to identify sub population that at an early point in time behave like the whole population at a later time such sub population act a early indicator for future development we present our method in the context of a concrete application and present experimental result at the end of the paper we outline how this method can be generalised and transferred to other kdd application problem 
discretization is the process of dividing a continuousvalued base attribute into discrete interval which highlight distinct pattern in the behavior of a related goal attribute in this paper we present an integrated visual framework in which several discretization strategy can be experimented with and which visually assist the user in intuitively determining the appropriate number and location of interval in addition to featuring method based on minimizing classification error or entropy we introduce i an optimal algorithm that minimizes the approximation in 
we have been working on two different kdd system for scientific data one system involves comparative genomics where the database contains more than plant gene and protein sequence plus result extracted from similarity search against public sequence database the second system support a several decade s long longitudinal field study of chimpanzee behavior both system have component for the storing of raw data and for cleaning data before querying begin and for displaying data extraction both system use a relational dbms in this paper we report on a the extension we made to the dbms to support our analysis of the data and b the way that we used those extension a with user we developed a thought from an initial idea to a richer analysis we have found that a a user s initial thought develops he or she make finer distinction and look to explain anomaly seen in coarse calculation in the query to accomplish those exploration we have found it valuable to move piece of sql command into attribute value and to accomplish several smaller query all at once via a command relation thus there is a blurring of the distinction between command and data this blurring allowed u to formulate and accomplish more sophisticated analysis than we had been doing previously background a part of our collaboration with scientist who must explore and analyze large data collection we are developing an environment to support their labor intensive analysis effort our scientist user currently include researcher who are conducting a several decade s long field study of the behavior of chimpanzee and others who are conducting comparative genomics study across several plant specie in both of these project we have gathered data from various source and generated new data then cleaned and stored it using oracle relational dbms we reported on an automated method for this part of the process for the genomics project in we have developed visualization method for these data source and are working on method to closely couple them to the dbms a described in for the genomics project we are incorporating clustering technique for determining pattern of similarity between gene and protein thus our 
the paper introduces the use of the multiple cause mixture model for automatic text category assignment although much research ha been done on text categorization this algorithm is novel in that it is unsupervised i e it doe not require pre labeled training example and it can assign multiple category label to document we present very preliminary result of the application of this model to a standard test collection evaluating it in supervised mode in order to facilitate comparison with other method and showing initial result of it use in unsupervised mode 
the problem of tracking curve in dense visual clutter is a challenging one tracker based on kalman filter are of limited use because they are based on gaussian density which are unimodal they cannot represent simultaneous alternative hypothesis extension to the kalman filter to handle multiple data association work satisfactorily in the simple case of point target but do not extend naturally to continuous curve a new stochastic algorithm is proposed here the condensation algorithm conditional density propagation over time it us factored sampling a method previously applied to interpretation of static image in which the distribution of possible interpretation is represented by a randomly generated set of representative the condensation algorithm combine factored sampling with learned dynamical model to propagate an entire probability distribution for object position and shape over time the result is highly robust tracking of agile motion in clutter markedly superior to what ha previously been attainable from kalman filtering notwithstanding the use of stochastic method the algorithm run in near real time 
the euclidean skeleton is essential for general shape representation this paper provides an efficient method to extract a well connected euclidean skeleton by a neighbor bisector decision ned rule on a vector distance map the shortest vector which generates a pixel s distance is stored when calculating the distance map a skeletal pixel is extracted by checking the vector of the pixel and it neighbor this method succeeds in generating a well connected euclidean skeleton without any linking algorithm a theoretical analysis and many experiment with image of different size also show the ned rule work excellent the average complexity of the method with the ned rule algorithm and the vector distance transform algorithm is linear in the number of the pixel 
we report on the development of the modular neural system quot seeeagle quot for the visual guidance of robot pick and place action several neural network are integrated to a single system that visuallyrecognizes human hand pointing gesture from stereo pairsof color video image the output of the hand recognition stage isprocessed by a set of color sensitive neural network to determinethe cartesian location of the target object that is referenced by thepointing gesture finally this 
an approach to defining actionability a a measure of interestingness of pattern is proposed this approach is based on the concept of an action hierarchy which is defined a a tree of action with pattern and pattern template data mining query assigned to it node a method for discovering actionable pattern is presented and various technique for optimizing the discovery process are proposed 
cortical amplification ha been proposed a a mechanism for enhancing the selectivity of neuron in the primary visual cortex le appreciated is the fact that the same form of amplification can also be used to de tune or broaden selectivity using a network model with recurrent cortical circuitry we propose that the spatial phase invariance of complex cell response arises through recurrent amplification of feedforward input neuron in the network respond like simple cell at low gain and complex cell at high gain similar recurrent mechanism may play a role in generating invariant representation of feedforward input elsewhere in the visual processing pathway 
the value of extracting knowledge from semi structured data is readily apparent with the explosion of the www and the advent of digital library this paper proposes a versatile system architecture for text mining that maintains structured data component in a relational database and unstructured concept in a concept library after a detailed explanation of our system architecture we briefly describe iris our prototype rule generation system 
this paper we present a new method for studying auditory system based on m sequence the methodallows u to perturbatively study the linear response of the system in the presence of various stimulus thisallows one to construct linear kernel at the same time that other stimulus are being presented using themethod we calculate the modulation transfer function of single neuron in the inferior colliculus of the cat 
in this paper we explore the use of machine learning and data mining to improve the prediction of travel time in an automobile we consider two formulation of this problem one that involves predicting speed at different stage along the route and another that relies on direct prediction of transit time we focus on the second formulation which we apply to data collected from the san diego freeway system we report experiment on these data with k nearest neighbout combined with a wrapper to select useful feature and normalization parameter the result suggest that nearest neighbour when using information from freeway sensor substantially outperforms prediction available from existing digital map analysis also reveal some surprise about the usefulness of other feature like the time and day of the trip 
this paper present a novel approach to part based object recognition in the presence of occlusion we focus on the problem of determining the pose of a d object from a single d image when convex part of the object have been matched to corresponding region in the image we consider three type of occlusion self occlusion occlusion whose locus is identified in the image and completely arbitrary occlusion we derive efficient algorithm for the first two case and characterize their performance for the last case we prove that the problem of finding valid pose is computationally hard but provide an efficient approximate algorithm this work generalizes our previous work on region based object recognition which focused on the case of planar model 
this paper present a theoretical framework for the combination of soft decision generated by expert employing mixed some shared and some distinct object representation by taking the confidence of the individual expert into account weighted benevolent fusion strategy are derived this provides a basis for combining classifier and illustrates that a substantial gain in performance can be achieved by fusing the opinion of multiple expert these strategy are experimentally tested and their effectiveness is considered 
we propose an automated approach to modeling drainage channel and more generally linear feature that lie on the terrain from multiple image which result not only in high resolution accurate and consistent model of the feature but also of the surrounding terrain in our specific case we have chosen to exploit the fact that river flow downhill and lie at the bottom of local depression in the terrain valley floor tend to be u shaped and the drainage pattern appears a a network of linear feature that can be visually detected in single gray level image different approach have explored individual facet of this problem ours unifies these element in a common framework we accurately model terrain and feature a dimensional object from several information source that may be in error and inconsistent with one another this approach allows u to generate model that are faithful to sensor data internally consistent and consistent with physical constraint 
tracking research ha diverged into two camp low level approach which are typically fast and robust but provide little fine scale information and high level approach which track complex deformation in high dimensional space but must trade off speed against robustness real time high level system perform poorly in clutter and initialisation for most high level system is either performed manually or by a separate module this paper present a new technique to combine lowand 
analog electronic cochlear model need exponentially scaled filter cmos compatible lateral bipolar transistor clbts can createexponentially scaled current when biased using a resistive line with avoltage difference between both end of the line since these clbtsare independent of the cmos threshold voltage current sourcesimplemented with clbts are much better matched than currentsources created with mo transistor operated in weak inversion measurement from integrated 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
two novel variant of dynamic link architecture that are based on mathematical morphology and incorporate coefficient which weigh the contribution of each node in elastic graph matching according to it discriminatory power are developed they are the so called morphological dynamic link architecture and the morphological signal decomposition dynamic link architecture the proposed variant are tested for face authentication in a cooperative scenario where the candidate claim an identity to be checked their performance is evaluated in term of their receiver operating characteristic and the equal error rate achieved in m vt database an equal error rate in the range is reported 
this paper describes novel and practical japanese parser that us decision tree first we construct a single decision tree to estimate modification probability how one phrase tends to modify another next we introduce a boosting algorithm in which several decision tree are constructed and then combined for probability estimation the two constructed parser are evaluated by using the edr japanese annotated corpus the single tree method outperforms the conventional japanese stochastic method by moreover the boosting version is shown to have significant advantage better parsing accuracy than it single tree counterpart for any amount of training data and no over fitting to data for various iteration 
a multi image focus of attention mechanism ha been developed that can quickly distinguish raised object like building from structured background clutter typical to many aerial image scenario the underlying approach is the space sweep stereo method in which feature from multiple image are backprojected onto a virtual horizontal plane that is methodically swept through the scene backprojected gradient orientation from multiple image are highly correlated when they come from scene location containing structural edge that are roughly horizontal like building roof and terrain otherwise they tend to be uniformly distributed these observation are used to define a structural salience measure that can determine whether a given volume of space contains a statistically signijicant number of structural edge without first pe otrning precise reconstruction of those edge the utility of structural salience for computing focus of attention region is illustrated on sample data from ft hood texas 
a novel deformable template is presented which detects the boundary of an open hand in a grayscale image a dynamic programming algorithm enhanced by pruning technique find the hand contour in the image in a little a second without initialization by the user the template is translationand rotation invariant and accomodates shape deformation significant occlusion and background clutter and the presence of multiple hand 
this paper address the problem of recognizing object in large image database the method is based on local characteristic which are invariant to similarity transformation in the image these characteristic are computed at automatically detected keypoints using the greyvalue signal the method therefore work on image such a painting for which geometry based recognition fails due to the locality of the method image can be recognized being given part of an image and in the presence of occlusion applying a voting algorithm and semi local constraint make the method robust to noise scene clutter and small perspective deformation experiment show an efficient recognition for different type of image the approach ha been validated on an image database containing image some of them being very similar by structure texture or shape 
recent theoretical result for pattern classification with thresholdedreal valued function such a support vector machine sigmoidnetworks and boosting give bound on misclassificationprobability that do not depend on the size of the classifier andhence can be considerably smaller than the bound that follow fromthe vc theory in this paper we show that these technique canbe more widely applied by representing other boolean functionsas two layer neural network thresholded 
in this paper we propose an algorithm for estimating dense shape and motion of dynamic piecewise planar scene from region correspondence using factorization region correspondence are used since they are easier to establish and more reliable than either line or point correspondence the image measurement required are the centroid and area for each region singular value decomposition is employed to find the basis of range space of the motion shape and surface normal matrix by imposing model constraint motion shape and surface normal can be recovered only from region correspondence 
consider a large collection of object each of which ha a large number of attribute of several dierent sort we assume that there are data attribute representing data attribute which are to be statistically estimated from these and attribute which can be controlled or set a motivating example is to assign a credit score to a credit card prospect indicating the likelihood that the prospect will make credit card payment and then to set a credit limit for each prospect in such a way a to maximize the over all expected revenue from the entire collection of prospect in the terminology above the credit score is called a statistical attribute and the credit limit a control attribute the methodology we describe in the paper us data mining to provide more accurate estimate of the statistical attribute and to provide more optimal setting of the control attribute we briefly describe how to parallelize these computation we also briefly comment on some of data management issue which arise for these type of problem in practice we propose using object 
if d rigid motion is estimated with some error a distorted version of the scene structure will in turn be computed of computational interest are these region in space where the distortion are such that the depth become negative because in order to be visible the scene ha to lie in front of the image the stability analysis for the structure from motion problem presented in this paper investigates the optimal relationship between the error in the estimated translational and rotational parameter of a rigid motion that result in the estimation of a minimum number of negative depth value the input used is the value of the flow along some direction which is more general than optic flow or correspondence for a planar retina it is shown that the optimal configuration is achieved when the projection of the translational and rotational error on the image plane are perpendicular furthermore the projection of the actual and the estimated translation lie on a line passing through the image center for a spherical retina given a rotational error the optimal translation is the correct one while given a translational error the optimal rotational error is normal to the translational one at an equal distance from the real and estimated translation the proof besides illuminating the confounding of translation and rotation in structure from motion have an important application to ecological optic explaining difference of planar and spherical eye or camera design in motion and shape estimation 
the proliferation of topic hierarchy for text document ha resulted in a need for tool that automatically classify new document within such hierarchy existing classification scheme which ignore the hierarchical structure and treat the topic a separate class are often inadequate in text classification where the there is a large number of class and a huge number of relevant feature needed to distinguish between them we propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problem one at each node in the classification tree a we show each of these smaller problem can be solved accurately by focusing only on a very small set of feature those relevant to the task at hand this set of relevant feature varies widely throughout the hierarchy so that while the overall relevant feature set may be large each classifier only examines a small subset the use of reduced feature set allows u to utilize more complex probabilistic model without encountering many of the standard computational and robustness difficulty 
in this paper we consider the problem of finding corresponding point from multiple perspective projection image the correspondence problem and estimating the d point from which these point have arisen the triangulation problem we pose the triangulation problem a that of finding the bayesian maximum a posteriori estimate of the d point given it projection in n image assuming a gaussian error model for the image point co ordinate and the camera parameter we solve this by an iterative steepest descent method we then consider the correspondence problem a a statistical hypothesis verification problem given a set of d point under the hypothesis that the point are in correspondence the map estimate of the d point is computed based on the map estimate we derive a statistical test for verifying this hypothesis to find set of corresponding point when multiple point in each of n image are given we propose a method that doe the bayesian triangulation and hypothesis verification on each n tuple of point selecting those that pas the hypothesis test we characterize the performance of the bayesian triangulation in term of the average distance of the triangulated d point from the true d point and of the point correspondence method in term of it misdetection and false alarm rate 
the problem of segmenting a volumetric layer of finite thickness is encountered in several important area within medical image analysis key example include the extraction of the cortical gray matter of the brain and the left ventricle myocardium of the heart the coupling between the two bounding surface of such a layer provides important information that help to solve the segmentation problem here we propose a new approach of coupled surface propagation via level set method which take into account coupling a an important constraint by evolving two embedded surface simultaneously each driven by it own image derived information while maintaining the coupling we capture a representation of the two bounding surface and achieve automatic segmentation on the layer characteristic gray level value instead of image gradient information alone are incorporated in deriving the useful image information to drive the surface propagation which enables our approach to capture the homogeneity inside the layer the level set implementation offer the advantage of easy initialization computational efficiency and the ability to capture deep fold of the sulcus a a test example we apply our approach to unedited d magnetic resonance mr brain image our algorithm automatically isolates the brain from non brain structure and recovers the cortical gray matter 
recently we have proposed a new approach to estimation of the coefficient of eigenimages which is robust against occlusion varying background and other type of non gaussian noise in this paper we show that our method for estimating the coefficient can be applied to convolved and subsampled image yielding the same value of the coefficient this enables an efficient multiresolution approach where the value of the coefficient can directly be propagated through the scale this property is used to extend our robust method to the problem of scaled image we performed extensive experimental evaluation to confirm our theoretical result 
to measure the quality of a set of vector quantization point a mean of measuring the distance between a random point and it quantization is required common metric such a the hamming and euclidean metric while mathematically simple are inappropriate for comparing natural signal such a speech or image in this paper it is shown how an environment of function on an input space induces a canonical distortion measure cdm on x the depiction canonical is justified because it is shown that optimizing the reconstruction error of x with respect to the cdm give rise to optimal piecewise constant approximation of the function in the environment the cdm is calculated in closed form for several different function class an algorithm for training neural network to implement the cdm is presented along with some encouraging experimental result 
current system for content filtering browsing and retrieval rely on low level image descriptor which are un intuitive for most user in this paper we propose an alternative framework that exploit the structured nature of most content source to achieve semantic content characterization and lead to much more meaningful user interaction computationally this framework is based on the principle of bayesian inference and can be implemented efficiently with bayesian network a an illustration of it potential we apply it to the domain of movie database 
in this research we propose a new iterative shape from texture sft algorithm which extract accurate surface depth information of a curved object covered with fairly homogeneous texture directly the shape information can be inferred from the rate of texture distortion depicted in an image and therefore the modeling of the projection and surface geometry a well a the estimation of local texture variation are crucial in obtaining accurate surface shape of an object by introducing semi perspective projection camera model and a parametric surface model we establish a new sft problem formulation called the textural irradiance equation which relates the local texture density called textural intensity to finite surface parameter moreover by adopting an adaptive multiscale filtering scheme for local texture density estimation in which the scale or frequency band of a local edge filter is chosen adaptively according to the local shape information we greatly enhance the accuracy of the estimation of the projected local texture density and the final reconstructed shape we demonstrate the performance of the proposed algorithm by the test with several synthetic and real texture image 
we present and solve a real world problem of learning to drive a bicycle we solve the problem by online reinforcement learning using the sarsa algorithm then we solve the composite problem of learning to balance a bicycle and then drive to a goal in our approach the reinforcement function is independent of the task the agent try to learn to solve 
this paper considers a model based approach to identifying and locating known d object from d image the method is based on geometric feature matching of the model and image data where both are represented in term of local geometric feature this paper extends and refines previous work on feature matching using transformation constraint method by detailing the case of full d object represented a point feature and developing geometric algorithm based on conservative approximation to the previously presented general algorithm which are much more computationally feasible 
temporal segmentation of video is a necessary first step to indexing digital video for browsing and retrieval a number of different video temporal segmentation algorithm have been published in the literature there ha been little effort to evaluate and characterize their performance so a to deliver a single or set of algorithm that may be used by other researcher for indexing video database we present result of evaluating a number of these algorithm and characterizing their performance specifically with respect to robustness to encoder and bitrate change the lesson learnt have relevance to algorithm development and evaluation in general 
heuristic measure for estimating the quality of attribute mostly assume the independence of attribute so in domain with strong dependency between attribute their performance is poor relief and it extension relieff are capable of correctly estimating the quality of attribute in classification problem with strong dependency between attribute by exploiting local information provided by different context they provide a global view we present the analysis of relieff which lead u to it adaptation to regression continuous class problem the experiment on artificial and real world data set show that regressional relieff correctly estimate the quality of attribute in various condition and can be used for non myopic learning of the regression tree regressional relieff and relieff provide a unified view on estimating the attribute quality in regression and classification 
this paper address the problem of computing three dimensionalstructure and motion from an unknown rigid conguration ofpoints and line viewed by an ane projection model an algebraic structure analogous to the trilinear tensor for three perspective camera isdened for congurations of three centered ane camera this centeredane trifocal tensor contains non zero coecients and involves linearrelations between point correspondence and trilinear relation betweenline 
this paper describes a new technique for object recognition based on learning appearance model the image is decomposed into local region which are described by a new texture representation called generali zed second moment that are derived from the output of multiscale multi orientation filter bank class characteristic local texture feature and th eir global composition is learned by a hierarchical mixture of expert architectur e jordan jacob the technique is applied to a vehicle database consisting of general car category sedan van with back door van without back door old sedan and volkswagen bug this is a difficult problem with considerable in class variation the new technique ha a misclassification rate compared to eigen image which give misclassification rate and nearest neighbor which give misclassification rate 
video mosaicing is commonly used to increase the visual field of view by pasting together many video frame existing mosaicing method are effective only in very limited case where the image motion is almost a uniform translation or the camera performs a pure pan forward camera motion or camera zoom are very problematic for traditional mosaicing a mosaicing methodology to allow image mosaicing in the most general case is presented where frame in the video sequence are transformed such that the optical flow becomes parallel this transformation is an oblique projection of the image into a viewing pipe whose central axis is the trajectory of the camera the pipe projection enables to define high quality mosaicing even for the most challenging case of forward motion and of zoom in addition view interpolation generating dense intermediate view is used to overcome parallax effect 
we have already shown that extracting long term dependency from sequentialdata is difficult both for deterministic dynamical system suchas recurrent network and probabilistic model such a hidden markovmodels hmms or input output hidden markov model iohmms inpractice to avoid this problem researcher have used domain specifica priori knowledge to give meaning to the hidden or state variable representingpast context in this paper we propose to use a more general 
abstract the task in the computer security domain of anomaly detection is to characterize the behavior of a computer user the valid or normal user so that unusual occurrence can be detected by comparison of the current input stream to the valid user s profile this task requires an online learning system that can respond to concept drift and handle discrete non metric time sequence data we present an architecture for online learning in the anomaly detection domain and address the issue of incremental updating of system parameter and instance selection we demonstrate a method for measuring direction and magnitude of concept drift in the classification space and present and evaluate approach to the above stated issue which make use of the drift measurement 
the large amount of data collected today is quickly rwrcm mrhnlminn rocolrrhara qhilitinc tn intornrot the l l lx y ucw ii i u ll ly u i y data and discover interesting pattern knowledge discovery and data mining approach hold the potential to automate the interpretation process but these approach frequently utilize computationally expensive algorithm this research outline a general approach for scaling kdd system using parallel and clistribut erl resource and applies the suggested strategy to the subdue knowledge discovery system subdue ha been used to discover interesting and repetitive concept in graph based database from a variety of dom nn h rom wx a nrrhct nt al camnrrnt nf n occ lyclllll vu r yulrs u cj o wi aw c laa ui y i yl a m ing time experiment that demonstrate scalability of parallel version of the subdue system are performed using cad circuit database and artificially generated database and potential achievement and obstacle are discussed 
we propose a new feature distance which is derived from an optimal relational graph matchingcriterion instead of defining an arbitrary similarity measure for grouping we will use the criterionof reducing instability in the relational graph to induce a similarity measure this similarity measurenot only improves the stability of the matching but more importantly also capture the relativeimportance of relational similarity in the feature space for the purpose of grouping we will call 
in high energy physic experiment one ha to sort through a highflux of event at a rate of ten of mhz and select the few that areof interest in making this decision one relies on the location ofthe vertex where the interaction that led to the event took place here we present a solution to this problem based on two feedforwardneural network with fixed architecture whose parametersare chosen so a to obtain a high accuracy the system is testedon many data set and is shown to 
we describe a novel computer vision application vision based human sensing for a smart kiosk interface a smart kiosk is a free standing information dispensing computer appliance capable of engaging in public interaction with multiple people vision sensing is a critical component of the kiosk interface where it is used to determine the context for the interaction we present a taxonomy of vision problem for a kiosk interface and describe a prototype kiosk which us color stereo tracking and graphical output to interact with several user 
two approach for d curved object reconstruction using active sensor and illumination control are proposed and compared to each other in both case the highlight information is fully utilized rather than discarded and knowledge of the object surface is not required the first approach requires camera control only and recovers shape depth from highlight and occluding contour the second approach requires both camera and illumination control and recovers d depth from highlight only 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
we propose a model for view based adaptive affine tracking of moving object we avoid the need for feature based matching in establishing correspondence through learning landmark we use an effective bootstrapping process based on colour segmentation and selective attention we recover affine parameter with dynamic update to the eigenspace using most recent history and perform prediction in parameter space experimental result are given to illustrate our approach 
many application require detecting structural change in a scene over a period of time comparing intensity value of successive image is not effective a such change don t necessarily reflect actual change at a site but might be caused by change in the view point illumination and season we take the approach of comparing a d model of the site prepared from previous image with new image to infer significant change this task is difficult a the image and the model have very different level of abstract representation our approach consists of several step registering a site model to a new image model validation to confirm the presence of model object in the image structural change detection seek to resolve matching problem and indicate possibly changed structure and finally updating model to reflect the change our system is able to detect missing or mi modeled building change in model dimension and new building under some condition 
a common factor in all illusory contour figure is the perception of a surface occluding part of a background in our previous work we have shown that by detecting junction and assigning a proper set of hypothesis at each junction we could diffuse this information and obtain surface reconstruction where the surface boundary represented illusory contour amodal completion emerge at the overlapping surface we here address the problem of selecting the best image organization set of hypothesis we propose two optimization criterion one based on a coherence measure between pair of junction correlation between the diffusion of each pair and another one based on an entropy measure sharpness of the reconstruction we show their similarity and a statistical physic approach to select the best organization the experiment suggest that despite the large number of possible organization our approach may take a few step to select the best organization starting from random organization 
we investigate the relationship between the kinematics infinitesimal motion model of a calibrated stereo rig and point and line image feature measurement seen at two time instance of the rig s motion four image in all in particular we are interested in the byproduct of this analysis providing a direct connection between the spatio temporal derivative of the image at two time instance and kinematics of the d motion of the rig we establish a fundamental result showing that quadruple of point line line line match i e point in the reference image and line coincident with the corresponding point in the remaining three image are sufficient for a unique linear solution for the kinematics of the rig in other word the projected instantaneous motion of one and a half d line is sufficient for recovering the kinematics of the moving rig in particular spatio temporal derivative across point are sufficient for a direct estimation of the rig s motion consequently we describe a new direct estimation method for motion estimation and d reconstruction from stereo image sequence obtained by a stereo rig moving through a rigid world correspondence optic flow are not required a spatio temporal derivative are used instead one can then use the image from both pair combined to compute a dense depth map finally since the basic equation are linear we combine the contribution coming from all pixel in the image using a least square approach 
the demo present a virtual mirror interface which reacts to the viewer using robust real time face tracking the display directly combine a user s face with various graphical distortion performed only on the face region in the image the face detection and tracking is done in real time so the graphical effect stay with the user and continues to adapt al they move within the viewing space increasing in intensity a the user approach the display the tracking system performs well in crowded environment with open and moving background this robust performance is achieved using multi modal integration combining stereo color and grey scale pattern matching module into single realtime system stereo processing is used to isolate the figure of a user from other object and people in the background skinhue classification identifies and track likely body part within the foreground region face pattern detection discriminates and localizes the face within the tracked body part a second set of display located to the side of the main viewing area show intermediate result of each processing module of the system 
this paper is concerned with alleviating thechoice of learning bias via a two step process gamma the set of all hypothesis that are consistentwith the data and cover at least onetraining example is given an implicit characterizationof polynomial complexity theonly bias governing this induction phase isthat of the language of hypothesis gamma classification of further example is donevia interpreting this implicit theory the interpretationmechanism allows one to relaxthe 
in this paper it is shown how to use the generalisedepipolar constraint on apparent contour or silhouette one such constraint is obtained for each frontier pointin each image pair in a sequence of image to estimatethe camera motion introduction n got h r in it wa shown how the viewer motion can becalculated from the contraints on the camera motionand the frontier point however only a pair of imageswas considered at the same time in this situationthe problem is 
this paper present a technique for visual recognition inwhich physical object are represented by family of surface in a localappearance space an orthogonal family of local appearance descriptorsis obtained by applying principal component analysis to image neighborhood the principal component with the largest variance are usedto define a space for describing local appearance the projection of theset of all neighborhood from an image give a discrete sampling of asurface in 
the paper describes visual routine based on model of color and shape a well a crucial issue involving the sche duling of such routine the visual routine are developed in a unique platform the view from a car driving in a simulated world is fed into a datacube pipeline video processor the use of this simulation provides a flexible environment from which to set crucial image processing parameter of the individual routine in addition to the simulation are also tested in similar image generated by driving in the real world to assure the generalizability of the simulation 
fly are capable of rapidly detecting and integrating visual motion information in behaviorly relevant way the first stage of visual mot ion processing in fly is a retinotopic array of functional unit known a el ementary motion detector emds several decade ago reichardt and colleague developed a correlation based model of motion detection that described the behavior of these neural circuit we have implemented a variant of this model in a analog cmos vlsi process the result is a low power continuous time analog circuit with integrated photoreceptors that responds to motion in real time the response of the circuit to drifting sinusoidal grating qualitatively resemble the t emporal frequency response spatial frequency response and direction selectivity of motion sensitive neuron observed in insect in addition to it possible engineering application the circuit could potentially be used a a building block for constructing hardware model of higher level in ect motion integration 
a common factor in all illusory contour figure is the perception of a surface occluding part of a background these surface are not constrained to be at constant depth and they can cross other surface we address the problem of how the image organization that yield illusory contour arise our approach is to iteratively find the most salient surface by i detecting occlusion ii assigning salient surface state a set of hypothesis of the local salient surface configuration iii applying a bayesian model to diffuse these salient surface state and iv efficiently selecting the best image organization set of hypothesis based on the resulting diffused surface 
a serious problem in mining industrial data base is that they are often incomplete and a significant amount of data is missing or erroneously entered this paper explores the use of machine learning based alternative to standard statistical data completion data imputation method for dealing with missing data we have approached the data completion problem using two well known machine learning technique the first is an unsupervised clustering strategy which us a bayesian approach to cluster the data into class the class so obtained are then used to predict multiple choice for the attribute of interest the second technique involves modeling missing variable by supervised induction of a decision tree based classifier this predicts the most likely value for the attribute of interest empirical test using extract from industrial database maintained by tioneywell customer have been done in order to compare the two technique these test show both approach are useful and have advantage and disadvantage we argue that the choice between unsupervised and supervised classification technique should be influenced by the motivation for solving the missing data problem and discus potential application for the procedure we are developing 
traditional light source modelling is concerned with specific type of light source the two most common of which are point source and daylight little attempt ha been made however to relate different type of source to each other for example how may the lighting from an overcast sky be compared to that from a lamp having a theoretical framework to compare different type of light source is important for computer vision in particular for understanding shading and shadow cue a vision system need to take account of the light source in order to interpret these cue in this paper we present a framework for comparing type of light source which is based on a dimensional analysis of the set of light ray in a free space specifically we introduce a d light source hypercube in which the different type of source may be embedded and compared we also present a novel definition for light source which generalizes the standard definition of a source a an emitter 
this paper present a direct reinforcementlearning algorithm called finite elementreinforcement learning in the continuouscase i e continuous state space and time the evaluation of the value function enablesthe generation of an optimal policy for reinforcementcontrol problem such a target orobstacle problem viability problem or optimizationproblems we propose a continuous formalism for thestudying of reinforcement learning using thecontinuous optimal control framework 
although td gammon is one of the major success in machine learning it ha not led to similar impressive breakthrough in temporal difference learning for other application or even other game we were able to replicate some of the success of td gammon developing a competitive evaluation function on a parameter feed forward neural network without using back propagation reinforcement or temporal difference learning method instead we apply simple hill climbing in a relative fitness environment these result and further analysis suggest that the surprising success of tesauro s program had more to do with the co evolutionary structure of the learning task and the dynamic of the backgammon game itself 
a recent neural model of illusory contour formation is based on a distribution of natural shape traced by particle moving with constant speed in direction given by brownian motion the input to that model consists of pair of position and direction constraint and the output consists of the distribution of contour joining all such pair in general these contour will not be closed and their distribution will not be scaleinvariant in this article we show how to compute a scale invariant distribution of closed contour given position constraint alone and use this result to explain a well known illusory contour effect 
in this paper we introduce regression basedpreand postdiction procedure for pmon anonmonotonic logic for action and change withexplicit time we also provide an in depthanalysis of problem with regression that occurwhen nondeterministic action are introduced we do this by employing dijkstra s weakest liberalprecondition operator wlp the presentedwork is related to recent work by lin in situationcalculus and we identify and deal withthree problem with his approach our 
exact interference in densely connected bayesian network is computationally intractable and so there is considerable interest in developing effective approximation scheme one approach which ha been adopted is to bound the log likelihood using a mean field approximating distribution while this lead to a tractable algorithm the mean field distribution is assumed to be factorial and hence unimodal in this paper we demonstrate the feasibility of using a richer class of approximating distribution based on mixture of mean field distribution we derive an efficient algorithm for updating the mixture parameter and apply it to the problem of learning in sigmoid belief network our result demonstrate a systematic improvement over simple mean field theory a the number of mixture component is increased 
we describe a method for interpreting abstract flat syntactic representation lfg f structure a underspecified semantic representation here underspecified discourse representation structure udrss the method establishes a one to one correspondence between subset of the lfg and udrs formalism it provides a model theoretic interpretation and an inferential component which operates directly on underspecified representation for f structure through the translation image of f structure a udrss 
several color object recognition method that are based on image retrieval algorithm attempt to discount change of illumination in order to increase performance when test image illumination condition differ from those that obtained when the image database wa created here we extend the seminal method of swain and ballard to discount changing illumination the new method is based on the first stage of the simplest color indexing method which us angular invariant between color image and edge image channel that method first normalizes image channel and then effectively discard much of the remaining information here we adopt the color normalization stage a an adequate color constancy step further we replace d color histogram by d chromaticity histogram treating these a image we implement the method in a compressed histogram image domain using a combination of wavelet compression and discrete cosine transform dct to fully exploit the technique of low pas filtering for efficiency result are very encouraging with substantially better performance than other method tested the method is also fast in that the indexing process is entirely carried out in the compressed domain and us a feature vector of only or value 
object identification the task of deciding that two observed object are in fact one and the same object is a fundamental requirement for any situated agent that reason about individual object identity a represented by the equality operator between two term in predicate calculus is essentially a first order concept raw sensory observation on the other hand are essentially propositional especially when formulated a evidence in standard probability theory this paper describes pattern of reasoning that allow identity sentence to be grounded in sensory observation thereby bridging the gap we begin by defining a physical event space over which probability are defined we then introduce an identity criterion which selects those event that correspond to identity between observed object from this we are able to compute the probability that any two object are the same given a stream of observation of many object we show that the appearance probability which defines how an object can be expected to appear at subsequent observation given it current appearance is a natural model for this type of reasoning we apply the theory to the task of recognizing car observed by camera at widely separated site in a freeway network with new heuristic to handle the inevitable complexity of matching large number of object and with online learning of appearance probability model despite extremely noisy observation we are able to achieve high level of performance 
one of the key problem in appearance based vision is understanding how to use a set of labeled image to classify new image classification system that can model human performance or that use robust image matching method often make use of similarity judgment that are non metric but when the triangle inequality is not obeyed most existing pattern recognition technique are not applicable we note that exemplar based or nearest neighbor method can be applied naturally when using a wide class of non metric similarity function the key issue however is to find method for choosing good representative of a class that accurately characterize it we show that existing condensing technique for finding class representative are ill suited to deal with non metric dataspaces we then focus on developing technique for solving this problem emphasizing two point first we show that the distance between two image is not a good measure of how well one image can represent another in non metric space instead we use the vector correlation between the distance from each image to other previously seen image second we show that in non metric space boundary point are le significant for capturing the structure of a class than they are in euclidean space we suggest that atypical point may be more important in describing class we demonstrate the importance of these idea to learning thatgeneralizes from experience by improving performance using both synthetic and real image 
in a graph with a small world topology node are highly clustered yet the path length between them is small such a topology can make search problem very difficult since local decision quickly propagate globally we show that graph associated with many different search problem have a small world topology and that the cost of solving such search problem can have a heavy tailed distribution the strategy of randomization and restarts appears to eliminate these heavy tail a novel restart schedule in which the cutoff bound is increased geometrically appears particularly effective 
this paper address the issue of algorithm v representation for case based learning of linguistic knowledge we first present empirical evidence that the success of case based learning method for natural language processing task depends to a large degree on the feature set used to describe the training instance next we present a technique for automating feature set selection for case based learning of linguistic knowledge given a input a baseline case representation the method modifies the representation in response to a number of predefined linguistic bias by adding deleting and weighting feature appropriately we apply the linguistic bias approach to feature set selection to the problem of relative pronoun disambiguation and show that the casebased learning algorithm improves a relevant bias are incorporated into the underlying instance representation finally we argue that the linguistic bias approach to feature set selection offer new possibility for case based learning of natural language it simplifies the process of instance representation design and in theory obviates the need for separate instance representation for each linguistic knowledge acquisition task more importantly the approach offer a mechanism for explicitly combining the frequency information available from corpus based technique with linguistic bias information employed in traditional linguistic and knowledge based approach to natural language processing 
a unique imaging modality based on equal thickness contour etc ha introduced a new opportunity for d shape reconstruction from multiple view we present a computational framework for representing each view of an object in term of it object thickness and then integrating these representation into a d surface by algebraic reconstruction the object thickness is inferred by grouping curve segment that correspond to point of second derivative maximum at each step of the process we use some form of regularization to ensure closeness to the original feature a well a neighborhood continuity we apply our approach to image of a sub micron crystal structure obtained through a holographic process 
when mining large database the data extraction problem and the interface between the database and data mining algorithm become important issue rather than giving a mining algorithm full access to a database by extracting to a flat file or other directlyaccessible data structure we propose the sql interface protocol sip which is a framework for interaction between a mining algorithm and a database the data continues to reside entirely within the database management system dbms but the query interface to the database give the data mining algorithm sufficient information to discover the same pattern it would have found with direct access to the data this model of interaction brine several advantage for ex ample it allows a mining algorithm to be parallelized automatically just by using a parallelized dbms to answer query we show how two family of mining algorithm may be implemented a sipper and we discus related work in database that should further enhance performance in the future 
we employed a white noise velocity signal to study the dynamic of the response of single neuron in the cortical area mt to visual motion response were quantied using reverse correlation optimal linear reconstruction lters and reconstruction signal to noise ratio snr the snr and lower bound estimate of information rate were lower than we expected ninety percent of the information wa transmitted below hz and the highest lower bound on bit rate wa bit s a simulated opponent motion energy subunit with poisson spike statistic wa able to out perform the mt neuron the temporal integration window measured from the reverse correlation half width ranged from m the window wa narrower when a stimulus moved faster but did not change when temporal frequency wa held constant 
this paper present paradise paradigm for dialogue system evaluation a general framework for evaluating spoken dialogue agent the framework decouples task requirement from an agent s dialogue behavior support comparison among dialogue strategy enables the calculation of performance over subdialogues and whole dialogue specifies the relative contribution of various factor to performance and make it possible to compare agent performing different task by normalizing for task complexity 
in this paper we describe a real time computer visionand machine learning system for modeling andrecognizing human behavior in a visual surveillancetask the system is particularly concerned with detectingwhen interaction between people occur andclassifying the type of interaction example of interestinginteraction behavior include following anotherperson altering one s path to meet another and soforth our system combine top down with bottom up informationin a closed feedback 
we study complexity of method using rigid variable like the method of mating or the tableau method on a decidable class of predi cate calculus with equality we show some in trinsic complication introduced by rigid vari ables we also consider strategy for increasing multiplicity in rigid variabl e method and for mally show that the use of intelligent strategy can result in an essential gain in efficiency 
this paper proposes a planning method for a vision guided mobile robot under vision uncertainty and limited computational resource the method considers the following two tradeoff granularity in approximating a probabilistic distribution v plan quality and search depth v plan quality the first tradeoff is managed by predicting the plan quality for a granularity using a learned relationship between them and by adaptively selecting the best granularity the second trade off is managed by formulating the planning process a a search in the space of feasible plan and by appropriately limiting the search considering the merit of each step of the search simulation result and experiment using a real robot show the feasibility of the method 
this paper stem from an ongoing research project on verb phrase ellipsis the project s goal are to implement a verb phrase ellipsis resolution algorithm automatically test the algorithm on corpus data then automatically evaluate the algorithm against human generated answer the paper will establish the current status of the algorithm based on this automatic evaluation categorizing current problem situation an algorithm to handle one of these problem the case of subdeletion will be described and evaluated the algorithm attempt to detect and solve subdeletion by locating adjunct of similar type in a verb phrase ellipsis and corresponding antecedent 
this paper explores unexpected result that lie atthe intersection of two common theme in the kddcommunity large datasets and the goal of buildingcompact model experiment with many differentdatasets and several model construction algorithm including tree learning algorithm such a c withthree different pruning method and rule learning algorithmssuch a c rule and ripper show thatincreasing the amount of data used to build a model oftenresults in a linear increase in 
given a multidimensional data set and a model of it density we considerhow to define the optimal interpolation between two point thisis done by assigning a cost to each path through space based on twocompeting goal one to interpolate through region of high density theother to minimize arc length from this path functional we derive theeuler lagrange equation for extremal motion given two point the desiredinterpolation is found by solving a boundary value problem weshow 
low two different approach to hypothesis production mi and clint for instance identify the target at we study the learn ability of inductive logic t e limit whereas most others use polynomial heurisprogramming ilp concept class with retics for concept induction consequently these sysspect to robust learning we first investigate tems are generally efficient learner but to our knowlthe class of k horn clause and show that it edge none can be formally shown to find the target is not learnable in that model we prove this concept in polynomial time using a reduction on which we impose a few simultaneously theoretical work ha allowed to estabconstraints a possible from this proof we lish learnability result for some subclass of first orthen show how we can also derive negative reder horn clause early study were undertaken in the suit for some pac learnable class finally identification in the limit model gold which we end by discussing the applicational consedescribes learning a converging towards the target quences of our work and it link with other concept in finite time but given an unbounded amount learnability study regarding new learnabilof example schapiro schapiro identified a ity model for ilp most general class learnable in this model by a consistent algorithm mi and other study have since been carried out in this framework banerji 
we describe a method for determining affine and metric calibrationof a camera with unchanging internal parameter undergoingplanar motion it is shown that affine calibration is recovered uniquely and metric calibration up to a two fold ambiguity the novel aspect of this work are first relating the distinguished objectsof d euclidean geometry to fixed entity in the image second showingthat these fixed entity can be computed uniquely via the trifocal tensorbetween image 
to formulate a meaningful query on semistructured data such a on the web that match some of the source s structure we need first to discover something about how the information is represented in the source this is referred to a schema discovery and wa considered for a single object recently in the case of multiple object the task of schema discovery is to identify typical structurine information of those object a a whole we mitivate the schema discovery in this general setting and propose a framework and algorithm for it we apply the framework to a real 
abstract a new algorithm is presented which approximates the perceived visual similarity between image the image are initially transformed into a feature space which capture visual structure texture and color using a tree of filter similarity is the inverse of the distance in this perceptual feature space using this algorithm we have constructed an image database system which can perform example based retrieval on large image database using carefully constructed target set which limit variation to only a single visual characteristic retrieval rate are quantitatively compared to those of standard method accepted advanced in neural information processing 
all illusory surface figure yield a perception of a surface occluding another one or the background occluded surface yield completion a phenomenon known a amodal completion it is intriguing that for some image illusory surface are perceived but not for other image see figure also illusory surface may have portion occluded we aim to understand these phenomenon our approach detects intensity edge and junction from the junction we seek to find an optimal image organization i e multiple ordered surface with the ordering accounting for salience the most salient being the figure while the other surface are classified a background a decision of which surface is the top one is made locally at each pixel allowing the salient surface figure to have portion occluded i e with amodal completion we account for a variety of imagery not explained before 
many presume that parsing the shadow out of an image is a high level task because of the global nature of the shadow formation process but shape from shading algorithm are low level in the sense that they seek solution surface normal or depth value directly from image intensity a dilemma arises since shape from shading involves an illumination term shadow must first be identified we show that a structure intermediate between intensity and surface the shading flow field provides a solution to this dilemma our analysis is based on the observation that the geometric information that can be derived from image support different inference than the photometric information and our specific goal will be to articulate this geometric structure and to show how shading flow field can be reliably computed 
the problem addressed in this paper is related to displayinga real d scene from any viewpoint to display a scene a relativelysparse set of d reference view is stored the image that are inbetweenthe reference view are obtained by interpolation of coordinatesand brightness colour this approach is able to generate the scene representationand render image automatically and efficiently even for complexscenes of d object this is possible since the processing time doesnot 
we describe a new multi phase color based image retrieval system focus fast object color based query system with an online user interface which is capable of identifying multi colored query object in an image in the presence of significant interfering background the query object may occur in arbitrary size orientation and location in the database image the color feature used to describe an image have been developed based on the need for speed in matching and ease of computation on complex image while maintaining the scale and rotation invariance property the first phase match the color content of an image computed a the peak in the color histogram of the image with the query object color using an efficient indexing mechanism the second phase match the spatial relationship between color region in the image with the query using a spatial proximity graph spg structure designed for the purpose the method is fast and ha low storage overhead test result with multi colored query object from artificial and natural domain show that focus is quite effective in handling interfering background and large variation in scale the experimental result on a database of diverse image highlight the capability of the system 
a method is presented for efficient and reliable object recognition within noisy cluttered and occluded range image the method is based on a strategy which hypothesizes the intersection of the object with some selected image point and search for additional surface data at location relative to that point at each increment the image is queried for the existence of surface data at a specific spatial location and the set of possible object pose is further restricted eventually either the object is identified and localized or the initial hypothesis is refuted the strategy is implemented in the discrete domain a a binary decision tree classifier the tree leaf node represent individual voxel template of the model the internal tree node represent the union of the template of their descendant leaf node the union of all leaf node template is the complete template set of the model over it discrete pose space each internal node also reference a single voxel which is the most common element of it child node template traversing the tree is equivalent to efficiently matching the large set of template at a selected image seed location the process is approximately order of magnitude more efficient than brute force template matching experimental result are presented in which object are reliably recognized and localized in dimension in le than second within noisy and significantly occluded range image 
in this paper we investigate the method of stacked generalization in combining model derived from different subset of a training dataset by a single learning algorithm a well a different algorithm the simplest way to combine prediction from competing model is majority vote and the effect of the sampling regime used to generate training subset ha already been studied in this context when bootstrap sample are used the method is called bagging and for disjoint sample we call it 
in this paper we propose a new solution to the stereo correspondence problem by including feature an intensity based matching the feature we use are intensity gradient in both the x and y direction of the left and the deformed right image although a uniform smoothness constraint is still used it is nevertheless applied only to non feature region to avoid local minimum in function minimization we propose to parameterize the disparity function by hierarchical gaussians a simple stochastic gradient method is used to estimate the gaussian weight experiment with various real stereo image show robust performance 
this paper describes an efficient approach to pose invariant object recognition employing pictorial recognition of image patch a complete affine invariance is achieved by a representation which is based on a new sampling configuration in the frequency domain employing singular value decomposition svd the affine transform is decomposed into slant tilt swing scale and d translation from this decomposition we derive an affine invariant representation that allows to recognize image patch that correspond to object surface which are roughly planar invariant to their pose in space the representation is in the form of spectral signature that are derived from a set of cartesian logarithmic logarithmic log log sampling configuration in the frequency domain unlike previous log polar representation which are not invariant to slant i e foreshortening only in one direction our new configuration yield complete affine invariance the proposed log log configuration can be employed both globally or locally by a gabor or fourier transforms local representation enables to recognize separately several object in the same image the actual signature recognition is performed by multi dimensional indexing in a pictorial dataset represented in the frequency domain the recognition also provides d pose information 
the process of grouping and subsequently recognising object in cluttered image is one laden with difficulty however result can be greatly enhanced if the inherent uncertainty of image feature is taken into account this paper show that starting with the individual edgel s uncertainty it is possible to calculate covariance information for all derived quantity this information can be used to choose between competing algorithm selecting the one that produce the more reliable result but also a an aid during the recognition process the consequent application of error propagation lead to a new formulation for the calculation of the cross ratio which is both robust and efficient in dealing with measured line and doe notrequire knowledge about the vanishing point extensive monte carlo simulation a well a the application to image of cluttered street scene demonstrate the robustness and suitability of the approach 
the study of visual navigation problem requires the integration of visual process with motor control most essential in approaching this integration is the study of appropriate spatio temporal representation which the system computes from the imagery and which serve a interface to all motor activity since representation resulting from exact quantitative reconstruction have turned out to be very hard to obtain we argue here for the necessity of representation which can be computed easily reliably and in real time and which recover only the information about the d world which is really needed in order to solve the navigational problem at hand in this paper we introduce a number of such representation capturing aspect of d motion and scene structure which are used for the solution of navigational problem implemented in visual servo system in particular the following three problem are addressed a to change the robot s direction of motion towards a fixed direction b to pursue a moving target while keeping a certain distance from the target and c to follow a wall like perimeter the importance of the introduced representation lie in the following they can be extracted using minimal visual information in particular the sign of flow measurement or the the first order spatiotemporal derivative of the image intensity function in that sense they are direct representation needing no intermediate level of computation such a correspondence they are global in the sense that they represent how three dimensional information is globally encoded in them thus they are robust representation since local error do not affect them usually from sequence of image three dimensional quantity such a motion and shape are computed and used a input to control process the representation discussed here are given directly a input to the control procedure thus resulting in a real time solution 
being able to predict the placement of contrastive accent is essential for the assignment of correct accentuation pattern in spoken language generation i discus two approach to the generation of contrastive accent and propose and alternative method that is feasible and computationally attractive in data to speech system 
the dimensionality of visual motion analysis can be reduced by analyzing projection offlow vector field in contrast to motion vector field these projection exhibit simplegeometric property which are invariant to the scene structure and depend only on thecamera motion using these property structure and motion can be either completely orpartially decoupled we estimate motion parameter from projection of flow field by usingrobust technique implemented in a recursive observer 
this paper present a self calibration and pose estimation method that us two camera which only differ by focal length the estimation of the rotation and focal length is independent of the translation recovery unlike most method we do not initialize our recovery with the projective camera instead we estimate the ego motion and calibration from homographies these homographies can be easily obtained from a fundamental matrix or a trifocal tensor 
query by content image database will be based on similarity rather than on matching where similarity is a measure that is defined and meaningful for every pair of image in the image space since it is the human user that in the end ha to be satisfied with the result of the query it is natural to base the similarity measure that we will use on the characteristic of human similarity assessment in the first part of this paper we review some of these characteristic and define a similarity measure based on them another problem that similarity based database will have to face is how to combine different query into a single complex query we present a solution based on three operator that are the analogous of the and or and not operator one us in traditional database these operator are powerful enough to express query of unlimited complexity yet have a very intuitive behavior making easy for the user to specify a query tailored to a particular need 
abstract 
we propose a new method for view synthesis from real image using stereo vision the method doe not explicitly model scene geometry and enables fast and exact generation of synthetic view we also re evaluate the requirement on stereo algorithm for the application of view synthesis and discus way of dealing with partially occluded region of unknown depth and with completely occluded region of unknown texture our experiment demonstrate that it is possible to efficiently synthesize realistic new view even from inaccurate and incomplete depth information 
a learner s performance doe not rely only onthe representation language and on the algorithminducing a hypothesis in this language also the way the induced hypothesis is interpretedfor the need of concept recognitionis of interest a flexible methodology for hypothesisinterpretion is offered by the philosophyof a learner s second tier a originallysuggested by michalski here thepotential of this general approach is demonstratedin the framework of numeric decisiontrees the 
in general a certain range of sentence in a text is widely assumed to form a coherent unit which is called a discourse segment identifying the segment boundary is a first step to recognize the structure of a text in this paper we describe a method for identifying segment boundary of a japanese text with the aid of multiple surface linguistic cue though our experiment might be small scale we also present a method of training the weight for multiple linguistic cue automatically without the overfitting problem 
a support vector machine svm is a universal learning machine whose decision surfaceis parameterized by a set of support vector and by a set of corresponding weight an svm is also characterized by a kernel function choice of the kernel determines whether the resulting svm is a polynomial classifier a two layer neural network a radialbasis function machine or some otherlearning machine svms are currently considerably slower intest phase than other approach with similar 
in this paper we first explore an intrinsicproblem that exists in the theory inducedby learning algorithm regardless of theselected algorithm search methodology andhypothesis representation by which the theoryis induced one would expect the theoryto make better prediction in some region ofthe description space than others we termthe fact that an induced theory will havesome region of relatively poor performancethe problem of locally low predictive accuracy having 
we propose an em approach combined with the modified separation matrix scheme to perform d motion segmentation of the image sequence which contains multiple moving object we observe that given the detected feature and their d optical flow in most case the object or their flow are separated very well from each other in space the separation matrix method modified by using normalized cut achieves expected grouping result for these case however when the object are overlapped spatially but undergoing independent motion such a ullman s co axial transparent cylinder demonstration there will be no proper affinity to perform segmentation by that way pure underlying d motion becomes the only cite to segment the scene we exploit the em algorithm to deal with such difficult case the scheme is tested on vast number of synthetic image sequence result with real image sequence are also given 
motivated by the need to reason about utility and inspired by the success of bayesian network in representing and reasoning about probability we introduce the notion of utility distribution in which utility have the structure of probability we furthermore define the notion of a bi distribution a structure that includes in a symmetric fashion both a probability distribution and autility distribution we give several example of bi distribution we also show that every state space with standard probability distribution and utility function can be embedded in a bi distribution and provide bound on the size requirement of this bi distribution finally we suggest a reinterpretation of the von neumann and morgenstern theorem in light of this new model 
a logical recasting of binding theory is performed a an enhancing step for the purpose of it full and lean declarative implementation a new insight on sentential anaphoric process is presented which may suggestively be captured by the slogan 
mineset tm silicon graphic interactive system for data mining integrates three powerful technology database access analytical data mining and data visualization it support the knowledge discovery process from data access and preparation through iterative analysis and visualization to deployment mineset is based on a client server architecture that scale to large database the database access component provides a rich set of operator that can be used to preprocess and transform 
because of the difficulty of obtaining ground truth for real image the traditional technique forcomparing low level vision algorithm is to present image result side by side and to let thereader subjectively judge the quality this is not a scientifically satisfactory strategy however human rating experiment can be done in a more rigorous manner to provide useful quantitativeconclusions we present a paradigm based on experimental psychology and statistic inwhich human rate 
of item the intuitive meaning of such a rule is that transaction of the database which contain x tend to contain y an example of an association rule is of transaction that contain beer also contain diaper of all transaction contain both of these item here is called the condence of the rule and the support of the rule the problem is to nd all association rule that satisfy user specied minimum support and minimum con dence constraint application include discovering a nities for market ba ket analysis and cross marketing catalog design loss leader analysis store layout customer segmentation based on buying pattern etc see nearhos roth man viveros for a case study of a successful application in health insurance 
the exact positioning of patient during radiotherapy is essentialfor high precision treatment the registration of portal image sequence can help to control the patient position the particular problem of such megavoltage x ray imagery is it extremely low contrast rendering accurate feature extraction a difficult task to circumvent the step offeature extraction the algorithm presented in this paper relies on anarea bused matching of the image signal using deformable template this strategy contrast with most state of the art registration algorithm for portal imagery the paper includes the mathematical formalism of the least square template matching method a well a the framework for automated quality control together yielding a fast robust and very accurate image matching procedure test on i portal image series with more than image in total have shown very satisfying result artificially rotated and shifted image demonstrate the performance of the method with respect to a ground truth 
closed loop control relies on sensory feedback that is usually assumedto be free but if sensing incurs a cost it may be costeffectiveto take sequence of action in open loop mode we describea reinforcement learning algorithm that learns to combineopen loop and closed loop control when sensing incurs a cost althoughwe assume reliable sensor use of open loop control meansthat action must sometimes be taken when the current state ofthe controlled system is uncertain this is 
we develop a theory for the temporal integration of visual motion motivated by psychophysical experiment the theory proposes that input data are temporally grouped and used to predict and estimate motion flow in the image sequence our theory is expressed in term of the bayesian generalization of standard kalman filtering which allows u to solve temporal grouping in conjunction with prediction and estimation a demonstrated for tracking isolated contour the bayesian formulation is superior to approach which use data association a a first stage followed by conventional kalman filtering our computer simulation demonstrate that our theory qualitatively account for several psychophysical experiment on motion occlusion and motion outlier 
face recognition is a class problem where is the number of known individual and support vector machine svms are a binary classification method by reformulating the face recognition problem and reinterpreting the output of the svm classifier we developed a svm based face recognition algorithm the face recognition problem is formulated a a problem in difference space which model dissimilarity between two facial image in difference space we formulate face recognition a a two class problem the class are dissimilarity between face of the same person and dissimilarity between face of different people by modifying the interpretation of the decision surface generated by svm we generated a similarity metric between face that is learned from example of difference between face the svm based algorithm is compared with a principal component analysis pca based algorithm on a difficult set of image from the feret database performance wa measured for both verification and identification scenario the identification performance for svm is versus for pca for verification the equal error rate is for svm and for pca 
we present a novel data mining approach basedon decomposition in order to analyze a givendataset the method decomposes it to a hierarchyof smaller and le complex datasets that canbe analyzed independently the method is experimentallyevaluated on a real world housingloans allocation dataset showing that the decompositioncan discover meaningful intermediateconcepts decompose a relatively complexdataset to datasets that are easy to analyze andcomprehend and derive a 
in mr brain image segmentation using intensity value is severely limited owing to field inhomogeneity susceptibility artifact and partial volume effect edge based segmentation method suffer from spurious edge and gap in boundary a method is presented which combine the advantage of edge based and region based segmentation first a multiscale image representation is constructed which favor intra tissue diffusion over inter tissue diffusion by exploiting local contrast subsequently a multiscale linking model the hyperstack is used to group voxels into a number of segment this facilitates segmentation of grey matter white matter and cerebrospinal fluid with minimal user interaction using a supervised segmentation technique and mr simulation of a brain phantom a validation it is shown that the error are in the order of or smaller than reported in literature 
this paper address two important issue related to texture pattern retrieval feature extraction and similarity search a gabor feature representation for textured image is proposed and it performance in pattern retrieval is evaluated on a large texture image database these feature compare favorably with other existing texture representation a simple hybrid neural network algorithm is used to learn the similarity by simple clustering in the texture feature space with learning similarity the performance of similar pattern retrieval improves significantly an important aspect of this work is it application to real image data texture feature extraction with similarity learning is used to search through large aerial photograph feature clustering enables efficient search of the database a our experimental result indicate 
this article develops an analogy between object recognition and the transmission of information through a channel based on the statistical representation of the appearance of d object this analogy provides a mean to quantitatively evaluate the contribution of individual receptive field vector and to predict the performance of the object recognition process transinformation also provides a quantitative measure of the discrimination provided by each viewpoint thus permitting the determination of the most discriminant viewpoint a an application the article develops an active object recognition algorithm which is able to resolve ambiguity inherent in a single view recognition algorithm 
we describe a linear algorithm to recover d affine shape motion from line correspondence over three view with uncalibrated affine camera the key idea is the introduction of a one dimensional projective camera this convert the d affine reconstruction of line into d projective reconstruction of point using the full tensorial representation of three uncalibrated d view we prove that the d affine reconstruction of line from minimal data is unique up to a re ordering of the view d affine line reconstruction can be performed by properly rescaling image coordinate instead of using projection matrix the algorithm is validated on both simulated and real image sequence 
the result discussed in this paper are relevant to a large database consisting of consumer profile information together with behavioral transaction pattern the focus of this paper is on the problem of onhne mining of profile association rule in this large database the profile association rule problem is closely related to the quantitative association rule problem we show how to use multidimensional indexing structure in order to perform the mining the use of multidimensional indexing structure to perform profile mining provides considerable advantage in term of the ability to perform very generic range based onhne query 
three factor enter into analysis of performance curve such a learning curve the amount of training the learning algorithm and performance often we want to know whether the algorithm affect performance whether the effect of training on performance depends on the algorithm and whether these effect are localized in region of the curve analysis of variance is adapted to answer these question the carryover effect of learning violate the assumption of parametric analysis of variance but they are rendered harmless by a novel randomized version of the analysis after a brief outline of the statistical preliminary we present the procedure along with some example on real learning curve discus power and type i error and give some example of how our method can be applied to answer more advanced question in comparing performance curve 
in this paper we study a dual version of the ridge regression procedure it allows u to perform non linear regression by constructing a linear regression function in a high dimensional feature space the feature space representation can result in a large increase in the number of parameter used by the algorithm in order to combat this curse of dimensionality the algorithm allows the use of kernel function a used in support vector method we also discus a powerful family of kernel function which is constructed using the anova decomposition method from the kernel corresponding to spline with an infinite number of node this paper introduces a regression estimation algorithm which is a combination of these two element the dual version of ridge regression is applied to the anova enhancement of the infinitenode spline experimental result are then presented based on the boston housing data set which indicate the performance of this algorithm relative to other algorithm 
this paper proposes a novel approach to extract meaningful content information from video by collaborative integration of image understanding and natural language processing a an actual example we developed a system that associate face and name in video called name it which is given news video a a knowledge source then automatically extract face and name association a content information the system can infer the name of a given unknown face image or guess face which are likely to have the name given to the system this paper explains the method with several successful matching result which reveal effectiveness in integrating heterogeneous technique a well a the importance of real content information extraction from video especially face name association 
a method is described for the determination of the viewing parameter of randomly acquired projection of asymmetric object it extends upon the common line algorithm by determining the relative orientation of projection from the location of line of intersection among the fourier transforms of the projection in three dimensional fourier space a new technique for finding the line of intersection in the presence of translational displacement and for subsequently finding the translational displacement is presented a new technique for dealing with noise is also presented the complete algorithm is described and it efficacy is demonstrated using real data this technique may be applied to the three dimensional reconstruction of virus molecule and cell from in vivo image it also ha many other application including the reconstruction of underwater scene radioastronomy geoseismic analysis and portable radiography for medical diagnosis and industrial inspection 
a complete and practical system for object recognition with occlusion ha been developed which is very robust with respect to noise and local deformation of shape a well a scale change and rigid motion of the object the system ha been tested on a wide variety of d object with different shape and surface property no restrictive assumption have been made about the shape of admissible object an industrial application with a controlled environment is envisaged the curvature scale space technique is used to obtain a novel multi scale segmentation of the image contour and the model contour using curvature zero crossing point multi scale segmentation render the system substantially more robust with respect to noise and local shape difference object indexing is used to narrow down the search space and avoid an exhaustive investigation of all model segment a local matching algorithm applies candidate generation selection merging extension and grouping to select the best matching model 
we introduce the problem of view interpolation fordynamic scene our solution to this problem extendsthe concept of view morphing and retains thepractical advantage of that method we are specificallyconcerned with interpolating between two referenceviews captured at different time so that there isa missing interval of time between when the view weretaken the synthetic interpolation produced by our algorithmportray one possible physically valid versionof what transpired in 
we consider the problem of reconstructing thelocation of a moving d point seen from amonocular moving camera since the point ismoving while the camera is moving then even ifthe camera motion is known it is impossible toreconstruct the d location of the point undergeneral circumstance however we show thatif the point is moving along a straight line thenthe parameter of the line and hence the dposition of the point at each time instance canbe uniquely recovered and by 
we present preliminary result concerning robust technique for resolving bridging definite description we report our analysis of a collection of wall street journal article from the penn treebank corpus and our experiment with wordnet to identify relation between bridging description and their antecedent 
this paper report about an application of bayes inferred neuralnetwork classifier to the field of automatic sleep staging up toour current knowledge this is one of the first real world applicationsof bayesian inference we therefore want to share our experienceof this learning paradigm with a wider audience the reason forusing bayesian learning for this task is two fold first bayesianinference is known to embody regularization automatically second a side effect of bayesian 
the choice of an input representation for a neural network can havea profound impact on it accuracy in classifying novel instance however neural network are typically computationally expensiveto train making it difficult to test large number of alternativerepresentations this paper introduces fast quality measure forneural network representation allowing one to quickly and accuratelyestimate which of a collection of possible representationsfor a problem is the best we show 
in this paper we present a neural network based face detection system unlike similar system which are limited to detecting upright frontal face this system detects face at any degree of rotation in the image plane the system employ multiple network a router network first process each input window to determine it orientation and then us this information to prepare the window for one or more detector network we present the training method for both type of network we also perform sensitivity analysis on the network and present empirical result on a large test set finally we present preliminary result for detecting face rotated out of the image plane such a profile and semi profile 
recently there ha been an increased interest in lifelong machine learning method that transfer knowledge across multiple learning task such method have repeatedly been found to outperform conventional single task learning algorithm when the learning task are appropriately related to increase robustness of such approach method are desirable that can reason about the relatedness of individual learning task in order to avoid the danger arising from task that are unrelated and thus potentially misleading this paper describes the task clustering tc algorithm tc cluster learning task into class of mutually related task when facing a new learning task tc first determines the most related task cluster then exploit information selectively from this task cluster only an empirical study carried out in a mobile robot domain show that tc outperforms it non selective counterpart in situation where only a small number of task is relevant 
association rule discovery ha emerged a an important problem in knowledge discovery and data mining the association mining task consists of identifying the frequent itemsets and then forming conditional implication rule among them in this paper we present efficient algorithm for the discovery of frequent itemsets which form the compute intensive phase of the task the algorithm utilize the structural property of frequent itemsets to facilitate fast discovery the related database item are grouped together into cluster representing the potential maximal frequent itemsets in the database each cluster induces a sub lattice of the itemset lattice efficient lattice traversal technique are presented which quickly identify all the true maximal frequent itemsets and all their subset if desired we also present the effect of using different database layout scheme combined with the proposed clustering and traversal technique the proposed algorithm scan a pre processed database only once addressing the open question in association mining whether all the rule can be efficiently extracted in a single database pas we experimentally compare the new algorithm against the previous approach obtaining improvement of more than an order of magnitude for our test database 
an asynchronous pdm pulse density modulating digital neural network system ha been developed in our laboratory it consists of one thousand neuron that are physically interconnected via one million bit synapsis it can solve one thousand simultaneous nonlinear rst order dieren tial equation in a fully parallel and continuous fashion the performance of this system wa measured by a winner take all network with one thousand neuron although the magnitude of the input and network parameter were identical for each competing neuron one of them won in millisecond this processing speed amount to billion connection per second a broad range of neural network including spatiotemporal ltering feedforward and feedback network can be run by loading appropriate network parameter from a host system 
abstract supervised method for ambiguity resolution learn in sterile environment in absence of syntactic noise however in many language engineering application manually tagged corpus are not available nor easily implemented on the other side the exportability of disambiguation cue acquired from a given noise free domain e g the wall street journal to other domain is not obvious unsupervised method of lexical learning have just a well many inherent limitation first the type of syntactic ambiguity phenomenon occurring in real domain are much more complex than the standard v n pp pattern analyzed in literature second especially in sublanguages syntactic noise seems to be a systematic phenomenon because many ambiguity occur within identical phrase in such case there is little hope to acquire a higher statistical evidence of the correct attachment class based model may reduce this problem only to a certain degree depending upon the richness of the sublanguage and upon the size of the application corpus because of these inherent difficulty we believe that syntactic learning should be a gradual process in which the most difficult decision are made a late a possible using increasingly refined level of knowledge 
performing the complex task of knowledge discoveryin database kdd requires a break down ofthe task complexity to enable the possibilityofperformingthe kdd task since even more techniqueswill appear in the future that can solveavarietyofkdd problem a domain expert that want to analysehis domain should have the mean to work withtools that integrate several of these technique a wellas the technique themselves in this paper a frameworkis proposed for a strategy 
in this paper a correlational approach for distinguishing occluding contour from object marking for d object modeling is presented the proposed method is valid under weak perspective projection doe not require to search for correspondence between frame can handle scaling between consecutive image thus can estimate the full euclidean surface structure and doe not require camera calibration or camera motion measurement extensive experimental result show that the method is robust to the occlusion of feature point and image noise unlike previous affine based approach qualitative and quantitative result for the relation between the required minimum viewing angle change for the detection and the surface curvature are also presented 
most technique for attribute selection indecision tree are biased towards attributeswith many value and several ad hoc solutionsto this problem have appeared in themachine learning literature statistical testsfor the existence of an association with aprespecified significance level provide a wellfoundedbasis for addressing the problem however many statistical test are computedfrom a chi squared distribution which is onlya valid approximation to the actual distributionin the 
in this paper two new feature tracking algorithm are proposed in the first algorithm a perspective camera model is used making use of the projective invariant of barrett and assuming the image feature point corresponding to general point in space are tracked by a conventional method in the image sequence the other feature point in the sequence can be tracked using a hough technique correspondence between two reference image a required by the original barrett s invariant is not necessary in the second algorithm an affine camera model is assumed and the image feature point corresponding to non coplanar point in space are assumed tracked in the image sequence using a conventional method these image point form the basis of affine coordinate in each image after the correspondence of a fifth point is established between the first two image the affine coordinate of all image point in the first image can be computed a far a we know this is the only algorithm which can transfer a point knowing only a single image experiment showed that both algorithm gave highly accurate tracking result 
planning for real time application involves decision not only about what action to take in what state to progress toward achieving goal the traditional decision problem faced by ai planning system but also about how to realize those action within hard real time deadline given the inherent limitation of an execution platform determining how to arrange action in a sequence such that timely execution is guaranteed within constraint is a manifestation of the scheduling problem all case of the scheduling problem in any domain of nontrivial complexity are difficult to solve np hard to more efficiently solve the real time plan scheduling problem we propose and analyze an iterative feedback constraint relaxation method in which a scheduler and planner iteratively interact to efficiently develop a well utilized schedule which includes a many planned action a possible this method ha been successfully implemented within the cooperative intelligent real time control architecture circa 
bayesian algorithm for neural network are known toproduce classifier which are very resistent to overfitting it is often claimed that one of the main distinctivefeatures of bayesian learning algorithm is thatthey don t simply output one hypothesis but ratheran entire distribution of probability over an hypothesisset the bayes posterior an alternative perspective isthat they output a linear combination of classifier whose coefficient are given by bayes theorem oneof the 
this paper describes the derivation of probability of correctness from score assigned by most recognizers motivation for this research is three fold i probability value can be used to rerank the output of any recognizer by using a new set of training data if the training data is sufficiently large and representative of the test data the recognition rate are seen to improve significantly ii derivation of probability value put the output of different recognizers on the same scale this make comparison across recognizers trivial and iii word recognition can be readily extended to phrase and sentence recognition because the integration of language model becomes straightforward we have conducted an extensive set of experiment the result show a reranking of recognition choice based on the derived probability value leading to an enhancement in performance 
often when learning from data one attachesa penalty term to a standard error term inan attempt to prefer simple model and preventoverfitting current penalty term forneural network however often do not takeinto account weight interaction this is acritical drawback since the effective numberof parameter in a network usually differsdramatically from the total number of possibleparameters in this paper we presenta penalty term that us principal componentanalysis to help 
several researcher have demonstrated howneural network can be trained to compensatefor nonlinear signal distortion in e g digitalsatellite communication system thesenetworks however require that both theoriginal signal and it distorted version areknown therefore they have to be trainedoff line and they cannot adapt to changingchannel characteristic in this paper a noveldual reinforcement learning approach is proposedthat can adapt on line while the systemis 
this paper discus the problem of selecting appropriate scale for region detection prior to feature localization we argue that an approach in morphological opening closing scale space is better than one in gaussian scale space the proposed operator is based on a new shape decomposition method called morphological band pas filter that decomposes an image into structure of different size and different curvature polarity local appropriate scale is then defined a the scale that maximizes the response of the band pas filter at each point this operator give constant scale value in a region of constant width and it zero crossing coincide with local maximum of the gradient magnitude it usefulness is demonstrated by some example 
this paper present a new scale position and orientation invariant approach to object detection the proposed method first chooses attention region in an image based on the region detection result on the image within the attention region the method then detects target using a novel object detection algorithm that combine template matching method with feature based method via hierarchical mrf and map estimation hierarchical mrf and map estimation provide a flexible framework to incorporate various visual clue the combination of template matching and feature detection help to achieve robustness against complex background and partial occlusion in object detection experimental result are given in the paper 
this paper present a statistical model which train from a corpus annotated with part ofspeech tag and assigns them to previously unseen text with state of the art accuracy the model can be classified a a maximum entropy model and simultaneously us many contextual feature to predict the po tag furthermore this paper demonstrates the use of specialized feature to model difficult tagging decision discus the corpus consistency problem discovered during the implementation of these feature and proposes a training strategy that mitigates these problem 
we introduce a new solid shape model formulation that includes built in offset from a base global component e g an ellipsoid which are function of the global component s parameter the offset provide two feature first they help to form an expected model shape which facilitates appropriate model data correspondence second they scale with the base global model to maintain the expected shape even in the presence of large global deformation we apply this model formulation to the recovery of d cardiac motion from a volunteer dataset of tagged mr image the model instance is a variation of the hybrid volumetric ventriculoid hvv a deformable thick walled ellipsoid model resembling the left ventricle lv of the heart a unique aspect of our implementation is the employment of constant volume constraint when recovering the cardiac motion in addition we present a novel geodesic like prismoidal tessellation of the model which provides for more stable fit 
in this contribution we present an algorithm for tracking non rigid moving object in a sequence of colored image which were recorded by a non stationary camera the application background is vision based driving assistance in the inner city in an initial step object part are determined by a divisive clustering algorithm which is applied to all pixel in the first image of the sequence the feature space is defined by the color and position of a pixel for each new image the cluster of the previous image are adapted iteratively by a parallel k mean clustering algorithm instead of tracking single point edge or area over a sequence of image only the centroid of the cluster are tracked the proposed method remarkably simplifies the correspondence problem and also ensures a robust tracking behaviour 
the appearance of a particular object depends on both the viewpoint from which it is observed and the light source by which it is illuminated if the appearance of two object is never identical for any pose or lighting condition then in theory the object can always be distinguished or recognized the question arises what is the set of image of an object under all lighting condition and pose in this paper we consider only the set of image of an object under variable illumination including multiple extended light source and attached shadow we prove that the set of n pixel image of a convex object with a lambertian reflectance function illuminated by an arbitrary number of point light source at infinity form a convex polyhedral cone in ir sup n and that the dimension of this illumination cone equal the number of distinct surface normal furthermore we show that the cone for a particular object can be constructed from three properly chosen image finally we prove that the set of n pixel image of an object of any shape and with an arbitrary reflectance function seen under all possible illumination condition still form a convex cone in ir sup n these result immediately suggest certain approach to object recognition throughout this paper we offer result demonstrating the empirical validity of the illumination cone representation 
to evolve structured program we introduceh pipe a hierarchical extension ofprobabilistic incremental program evolution pipe structure is induced by quot hierarchicalinstructions quot his limited to top level structuring program part quot skip node quot sn allow for switching program part onand off they facilitate synthesis of certainstructured program in our experiment hpipeoutperforms pipe structural bias canspeed up program synthesis keywords probabilistic incremental program 
discovery of association rule from large database of item set is an important data mining problem association rule are usually stored in relational database for future use in decision support system in this paper the problem of association rule retrieval and item set retrieval is recognized a the subset search problem in relational database the subset search is not well supported by sql query language and traditional database indexing technique we introduce a new index structure called group bitmap index and compare it performance with traditional indexing method b tree and bitmap index we show experimentally that proposed index enables faster subset search and significantly outperforms traditional indexing method set item set satisfies the rule if x y is contained in the set we say that the rule is violated by a given item set item set violates the rule if the set contains x but doe not contain y each rule ha two measure of it statistical importance and strength support and confidence the support of the rule is the number of item set that satisfy the rule divided by the number of all item set the rule confidence is the number of item set that satisfy the rule divided by the number of item set that contain x transaction id item 
a a classifier a set enumeration se tree can beviewed a a generalization of decision tree it canbe shown that at the cost of a higher complexity asingle se tree encapsulates many alternative decisiontree structure an se tree enjoys several advantagesover decision tree it allows for domain based userspecifiedbias it support a flexible tradeoff betweenthe resource allocated to learning and the resultingaccuracy and it can combine knowledge induced fromexamples with other 
image motion induced by camera or object motion can be approximated locally by an affine coordinate transformation we extract d information directly from the affine parameter without camera calibration the derivation relies on the following assumption the object is rigid locally planar and it local d motion is translation these assumption enable complete recovery of d structure whereas it is impossible to compute the direction and magnitude of the motion still it is possible to distinguish between object moving differently explicit expression for the structure and the motion indicator are given in term of the affine parameter computed for each image patch result of experiment on data with known ground truth are described 
we address the problem of finding useful region for two dimensional association rule and decision tree in a previous paper we presented efficient algorithm for computing optimized x monotone region whose intersection with any vertical line are always undivided in practice however the quality of x monotone region is not ideal because the boundary of an xmonotone region tends to be notchy and the region is likely to overfit a training dataset too much to give a good prediction for an unseen test dataset t l r a a t c mn d u a yc pcx yt i ioll z u p p ug cur udrj igllllinear region whose intersection with any vertical line and whose intersection with any horizontal line are both undivided so that the boundary of any rectilinear region is never notchy this property is studied from a theoretical viewpoint experimental test confirm that the rectilinear region le overfits a training database and thefore provides a better prediction for unseen test data we also present a novel efficient algorithm for computing optimized rectilinear region 
undeformed superquadrics are volumetric modeling primitive with an extensive shape vocabulary that are described by only parameter fitting these model viewpoint invariantly to range data enables classification based on the superquadric parameter however current recovery routine show several limitation especially when the algorithm are applied to range image instead of true d image in this paper problem with the common superquadric recovery procedure are identified and solution are presented 
in this paper we consider one aspect of the problem of automatically constructing geometric model of articulated object from multiple range image automatic model construction ha been investigated for rigid object but the technique used do not extend easily to the articulated case the problem arises because of the need to register surface measurement taken from different viewpoint into a common reference frame registration algorithm generally assume that an object doe not change shape from one viewto the next but when automatically building a model of an articulated object it is necessary for the mode of articulation to be present in the example data to avoid this problem we propose that raw surface data of articulated object is first segmented into rigid subset corresponding to rigid subcomponents of the object this allows a model of each subcomponent to be constructed using the conventional approach and a final articulated model to be constructed by assembling each of the subcomponent model we describe an algorithm developed to segment range data into rigid subset based on surface patch correspondence and present some result for the planar patch case 
we propose a solution technique for scheduling and constraint satisfaction problem that combine backtracking free constructive method and local search technique our technique incrementally construct the solution performing a local search on partial solution each time the construction reach a dead end local search on the space of partial solution is guided by a cost function based on three component the distance to feasibility of the partial solution a look ahead factor and for optimization problem a lower bound of the objective function in order to improve search effectiveness we make use of an adaptive relaxation of constraint and an interleaving of different lookahead factor the new technique ha been successfully experimented on two real life problem university course scheduling and sport tournament scheduling 
for a wide variety of classification algorithm scalability to large database can be achieved by observing that most algorithm are driven by a set of sufficient statistic that are significantly smaller than the data by relying on a sql backend to compute the sufficient statistic we leverage the query processing system of sql database and avoid the need for moving data to the client we present a new sql operator unpivot that enables efficient gathering of statistic with minimal change to the sql backend our approach result in significant increase in performance without requiring any change to the physical layout of the data we show analytically how this approach outperforms an alternative that requires changing in the data layout we also compare effect of data representation and show that a dense representation may be preferred to a sparse one even when the data are fairly sparse 
several recent effort in statistical natural language understanding nlu have focused on generating clump of english word from semantic meaning concept miller et al levin and pieracini epstein et al epstein this paper extends the ibm machine translation group s concept of fertility brown et al to the generation of clump for natural language understanding the basic underlying intuition is that a single concept may be expressed in english a many disjoint clump of word we present two fertility model which attempt to capture this phenomenon the first is a poisson model which lead to appealing computational simplicity the second is a general nonparametric fertility model the general model s parameter are boot strapped from the poisson model and updated by the em algorithm these fertility model can be used to impose clump fertility structure on top of preexisting clump generation model here we present result for adding fertility structure to unigram bigram and headword clump generation model on arpa s air travel information service atis domain 
abstract 
in the depth from defocus dfd method two defocused image of a scene are obtained by capturing the scene with different set of camera parameter an arbitrary selection of the camera setting can result in observed image whose relative blurring is insufficient to yield a good estimate of the depth in this paper we study the effect of the degree of relative blurring on the accuracy of the estimate of the depth by addressing the dfd problem in a maximum likelihood based framework we propose a criterion for optimal selection of camera parameter to obtain an improved estimate of the depth the optimality criterion is based on the cramer rao bound of the variance of the error in the estimate of blur simulation a well a experimental result on real image are presented for validation 
local disparity information is often sparse and noisy which createstwo conflicting demand when estimating disparity in an image region the need to spatially average to get an accurate estimate andthe problem of not averaging over discontinuity we have developeda network model of disparity estimation based on disparityselectiveneurons such a those found in the early stage of processingin visual cortex the model can accurately estimate multipledisparities in a region which may 
in this paper we will introduce a common framework for the definition and operation on the different multiple view tensor the novelty of the proposed formulation is to not fix any parameter of the camera matrix but instead letting a group act on them and look at the different orbit in this setting the multiple view geometry can be viewed a a four dimensional linear manifold in ir m where m denotes the number of image the grassman coordinate of this manifold are the epipoles the component of the fundamental matrix the component of the trifocal tensor and the component of the quadfocal tensor all relation between these grassman coordinate can be expressed using the so called quadratic p relation which are quadratic polynomial in the grassman coordinate using this formulation it is evident that the multiple view geometry is described by four different kind of projective invariant the epipoles the fundamental matrix the trifocal tensor and the quadfocal tensor a an application of this formalism it will be shown how the multiple view geometry can be calculated from the fundamental matrix for two view from the trifocal tensor for three view and from the quadfocal tensor for four view a a byproduct we show how to calculate the fundamental matrix from a trifocal tensor a well a how to calculate the trifocal tensor from a quadfocal tensor it is furthermore shown that in general n n n n linearly independent constraint on the quadfocal tensor and that corresponding point can be used to estimate the tensor component linearly finally it is shown that the rank of the trifocal tensor is and that the rank of the quadfocal tensor is 
this paper describes a family of factorization based algorithm that recover d projective structure and motion from multiple uncalibrated perspective image of d point and line they can be viewed a generalization of the tomasi kanade algorithm from affine to fully perspective camera and from point to line they make no restrictive assumption about scene or camera geometry and unlike most existing reconstruction method they do not rely on privileged point or image all of the available image data is used and each feature in each image is treated uniformly the key to projective factorization is the recovery of a consistent set of projective depth scale factor for the image point this is done using fundamental matrix and epipoles estimated from the image data we compare the performance of the new technique with several existing one and also describe an approximate factorization method that give similar result to svd based factorization but run much more quickly for large problem 
the selectional preference of verbal predicate are an important component of lexical information useful for a number of nlp task including disambigliation of word sens approach to selectional preference acquisition without word sense disambiguation are reported to be prone to error arising from erroneous word sens large scale automatic semantic tagging of text in sufficient quantity for preference acquisition ha received little attention a most research in word sense disambiguation ha concentrated on quality word sense disambiguation of a handful of target word the work described here concentrate on adapting semantic tagging method that do not require a massive overhead of manual semantic tagging and that strike a reasonable compromise between accuracy and cost so that large amount of text can be tagged relatively quickly the result of some of these adaptation are described here along with a comparison of the selectional preference acquired with and without one of these method result of a bootstrapping approach are also outlined in which the preference obtained are used for coarse grained sense disambiguation and then the partially disambiguated data is fed back into the preference acquisition system 
we address the detection and tracking of moving object in a video stream obtained from a moving airborne platform the approach is based on the compensation of the image flow induced by the motion of observation platform and the detection and tracking of moving region the use of such an approach lead u to deal with stabilization inaccuracy false alarm and non detection of moving object and tracking difficulty due to partial occlusion or stop and go motion our approach use a hierarchical feature based stabilization scheme and normal component of the residual flow for detecting moving object these object are tracked using a dynamic template for each object and extract trajectory a the result of a graph searching algorithm the proposed framework show that an integration of well known tool and an efficient description of the moving object can give very accurate detection and tracking of moving object 
this paper describes a constrained bin packing problem cbpp and an approximate anytime algorithm for solution a cbpp is a constrained version of the bin packing problem in which a set of item allocated to a bin are ordered in a way to satisfy constraint defined on them and achieve near optimality the algorithm for cbpp us a heuristic search for labeling edge with a binary value together with a beam search and constraint propagation some experimental result are provided this algorithm ha been successfully applied to industrial scale scheduling problem 
document ojlen display a structure determined by the author e g several chapter each with several sub chapter and so on taking into account the structure of a document allows the retrieval process to focus on those part of the document that are most relevant to an information need chiaramella et al advanced a model for indexing and retrieving structured document their aim wa to express the model within a framework based on formal logic with associated theory they developed the logical formalism of the model this paper add to this model a theory of uncertainty the dempster shafer theory of evidence it is shown that the theory provides a rule the dempster s combination rule that a low the expression of the uncertainty with respect to part of a document and that is compatible with the iogica model developed by chiaramella et al 
this paper describes the automatic design of methodsfor detecting fraudulent behavior much of the designis accomplished using a series of machine learningmethods in particular we combine data mining andconstructive induction with more standard machinelearning technique to design method for detectingfraudulent usage of cellular telephone based on profilingcustomer behavior specifically we use a rulelearningprogram to uncover indicator of fraudulentbehavior from a large database 
in previous work we modify the hidden markov model hmm framework to incorporate a global parametric variation in the output probability of the state of the hmm development of the parametric hidden markov model phmm wa motivated by the task of simultaneously recognizing and interpreting gesture that exhibit meaningful variation with standard hmms such global variation confounds the recognition process the original phmm approach assumes a linear dependence of output density mean on the global parameter in this paper we extend the phmm to handle arbitrary smooth nonlinear dependency we show a generalized expectation maximization gem algorithm for training the phmm and a gem algorithm to simultaneously recognize the gesture and estimate the value of the parameter we present result on a pointing gesture where the nonlinear approach permit the natural azimuth elevation parameterization of pointing direction 
different application in the field of vision based navigation of autonomous mobile robot depend on the degree of knowledge of the environment indoor environment application often use landmark or map for navigation others have only knowledge of known and expected object in such application part of the scene are classified in these object e g road junction door wall furniture and a possible path will be estimated in case of a lack of a priori knowledge of the environment we propose an approach for vision based navigation considering any reconstructed d point of the scene a line segment stereo algorithm and a reconstruction procedure lead to uncertain d point of the scene in front of the mobile system all these d point are regarded a obstacle and a following trace estimation will be applied on this d data in order to increase the reliability of reconstructed d point a validation step excludes impossible d point exploiting the stereo geometry of the vision system after validation a two step analysis is applied which contains the minimum distance method and point distribution analysis method this analysis lead to a possible trace for the mobile robot resulting in value for steering angle and velocity given to the mobile system the method ha been implemented on the experimental system movilar mobile vision and laser based robot which is based on a multi processor network it achieves actually process cycle of approximately s and a velocity of about cm s i e a slow walking speed experimental result of the above mentioned method are presented 
we compare the ability of three exemplar based memory model each using three different face stimulus representation to account for the probability a human subject responded old in an old new facial memory experiment the model are the generalized context model simsample a probabilistic sampling model and dbm a novel model related to kernel density estimation that explicitly encodes stimulus distinctiveness the representation are posi tions of stimulus in md face space projection of test face onto the eigenfaces of the study set and a representation based on response to a grid of gabor filter jet of the model representation combinat ion only the distinctiveness model in md space predicts the observed morph familiarity inversion effect in which the subject false alarm rate for morphs between similar face is higher than their hit rate for many of the studied face this evidence is consistent with the hypothesis that human memory for face is a kernel density estimation task with the caveat that distinctive face require larger kernel than d o typical face 
we propose and examine a method of approximate dynamic programming for markov decision process based on structured problem representation we assume an mdp is represented using a dynamic bayesian network and construct value function using decision tree a our function representation the size of the representation is kept within acceptable limit by pruning these value tree so that leaf represent possible range of value thus approximating the value function produced during optimization we propose a method for detecting convergence prove error bound on the resulting approximately optimal value function and policy and describe some preliminary experimental result 
in this paper we present a robust method for creating a triangulated surface mesh from multiple range image our method merges a set of range image into a volumeteric implicit surface representation which is converted to a surface mesh using a variant of the marching cube algorithm unlike previous technique based on implicit surface representation our method estimate the signed distance to the object surface by finding a consensus of locally coherent observation of the surface we call this method the consensus surface algorithm this algorithm effectively eliminates many of the troublesome effect of noise and extraneous surface observation without sacrificing the accuracy of the resulting surface we utilize octrees to represent volumetric implicit surface effectively reducing the computation and memory requirement of the volumetric representation without sacrificing accuracy of the resulting surface we present result which demonstrate that our consensus surface algorithm can construct accurate geometric model from rather noisy input range date 
we introduce the concept of self calibration of a d projectivecamera from point correspondence and describe a method foruniquely determining the two internal parameter of a d camera basedon the trifocal tensor of three d image the method requires the estimationof the trifocal tensor which can be achieved linearly with noapproximation unlike the trifocal tensor of d image and solving forthe root of a cubic polynomial in one variable interestingly enough weprove that a d 
bayesian network are graphical representationsof probability distribution over the lastdecade these representation have become themethod of choice for representation of uncertainlyin artificial intelligence today they playa crucial role in modern expert system diagnosisengines and decision support system in recent year there ha been much interestin learning bayesian network from data learning such model is desirable simply becausethere is a wide array of 
we offer a novel strategy to adapt the perceptual organization process to an object and it context in a scene given a set of training image of an object in context a learning process decides on the relative importance of the basic gestalt relationship such a proximity parallelness similarity symmetry closure and common region towards segregating the object from the background this learning is accomplished using a team of stochastic automaton in a n player cooperative game framework the grouping process which is based on graph partitioning is able to form large group from relationship defined over a small set of primitive and is fast we demonstrate the robust performance of the grouping system on a variety of real image among the interesting conclusion is the significant role of photometric attribute in grouping and the ability to perform figure ground segmentation from a set of local relation each defined over a small number of primitive 
this paper present a novel method for determining the location of the instantaneous epipole in a sequence of image acquired by an uncalibrated camera and containing a single rigid motion e g the camera move in a static environment the method us the full perspective camera model and requires the estimation of the optical flow at a minimum of six image location the key observation is that the optical flow equation can be written in term of the epipole in a strikingly simple form if the translation and rotational flow component are not separated a done usually the epipole location can then be obtained a the minimum of a least square residual function asdsociated to the computed optical flow we report and discus initial experiment on both synthetic and real data and illustrate possible development of this method towards the use of uncalibrated optical flow for d motion and structure reconstruction 
ray based representation of shape have received little attention in computer vision in this paper we show that the problem of recovering shape from silhouette becomes considerably simplified if it is formulated a a reconstruction problem in the space of oriented ray that intersect the object the method can be used with both calibrated and uncalibrated camera doe not rely on point correspondence to compute shape and doe not impose restriction on object topology or smoothness 
existing polarization based image understanding technique use information only from reflected light apart from incandescent body thermally emitted light radiation from element of a scene in the visible spectrum is insignificant however at longer wavelength such a in the infrared thermal emission is typically quite prevalent from a number of scene element of interest flir imagery of both indoor and outdoor scene reveals that many object thermally emit a significant amount of radiation polarization from thermally emitting object ha been observed a long a year ago from incandescent object but since then there have only been a limited number of empirical investigation into this phenomenon this paper present a comprehensive model for explaining polarization of thermal emission from both rough and smooth surface in agreement with empirical data that can significantly enhance the image understanding of flir imagery in particular it is possible to discern metal from dielectric material under certain condition and from an accurate model for thermally emitted polarization it is possible to predictively model polarization signature from cad model of importance to automatic target recognition 
we study the characteristic of learning with ensemble solvingexactly the simple model of an ensemble of linear student wefind surprisingly rich behaviour for learning in large ensemble it is advantageous to use under regularized student which actuallyover fit the training data globally optimal performance canbe obtained by choosing the training set size of the student appropriately for smaller ensemble optimization of the ensembleweights can yield significant improvement 
a model of motion detection is presented the model containsthree stage the first stage is unoriented and is selective for contrastpolarities the next two stage work in parallel a phaseinsensitive stage pool across different contrast polarity througha spatiotemporal filter and thus can detect first and second ordermotion a phase sensitive stage keep contrast polarity separate each of which is filtered through a spatiotemporal filter and thusonly first order motion can be 
this paper address the question of how to integrate local and global information the goal being a stable mechanism to partition parametric data into meaningful class without injecting a priori information about the data to do this we introduce a novel framework to represent both local and global information and their interaction where both type of information are represented together in parameter space and together define a self organisation or warping of the data an unsupervised clustering analysis is then performed to extract from the parametric data class that are stable and meaningful a an example of this paradigm we consider the problem of shape decomposition here we describe how image discontinuity i e curve edge or local curvature can be integrated with global parametric model that represent the image the resulting class cluster are then equivalent to the inferred part decomposition an example of how this process can be used is demonstrated by applying it to the specific problem of determining the part of d object result on real laser rangefinder image of complex object are presented 
several clustering algorithm have been proposed for class identification in spatial database such a earth observation database the effectivity of the well known algorithm such a dbscan however is somewhat limited because they do not fully exploit the richness of the different type of data contained in a spatial database in this paper we introduce the concept of density connected set and present a significantly generalized version of dbscan the major property of this algorithm are a follows any symmetric predicate can be used to define the neighborhood of an object allowing a natural definition in the case of spatially extended object such a polygon and the cardinality function for a set of neighboring object may take into account the non spatial attribute of the object a a mean of assigning application specific weight density connected set can be used a a basis to discover trend in a spatial database we define trend in spatial database and show how to apply the generalized dbscan algorithm for the task of discovering such knowledge to demonstrate the practical impact of our approach we performed experiment on a geographical information system on bavaria which is representative for a broad class of spatial database 
abstract if instead of the full motion field we consider only the direction of the motion field due toa rigid motion what can we say about the three dimensional motion information containedin it this paper provides a geometric analysis of this question based solely on the constraintthat the depth of the surface in view is positive 
we introduce a unified framework for developing matching constraint of multiple affine view and rederive view affine epipolar geometry and view affine image transfer constraint within this framwork we then describe a new linear method for euclidean motion and structure from calibrated affine image based on insight into the particular structure of these multiple view constraint compared with the existing linear method of huang and lee the new method us different and more appropriate constraint it ha no failure mode of the euclidean factorisation method of tomasi and kanade we demonstrate the method on real image sequence 
this paper present a genetic algorithm based approach to the automatic discovery of finite state automaton fsas from positive data fsas are commonly used in computational phonology but given the limited learnability of fsas from arbitrary language subset are usually constructed manually the approach presented here offer a practical automatic method that help reduce the cost of manual fsa construction 
correlation based real time stereo system have been proven to be effective in application such a robot navigation elevation map building etc this paper provides an in depth analysis of the major error source for such a real time stereo system in the context of cross country navigation of an autonomous vehicle three major type of error foreshortening error misalignment error and systematic error are identified the combined disparity error can easily exceed three tenth of a pixel which translates to significant range error upon understanding these error source we demonstrate different approach to either correct them or model their magnitude without excessive additional computation by correcting those error we show that the precision of the stereo algorithm can be improved by 
we propose a model of efficient on line reinforcementlearning based on the expectedmistake bound framework introduced byhaussler littlestone and warmuth the measure of performance we use is theexpected difference between the total rewardreceived by the learning agent and that receivedby an agent behaving optimally fromthe start we call this expected differencethe cumulative mistake of the agent and werequire that it quot level off quot at a reasonably fastrate a the learning 
grouping based on common motion or common fate provides a powerful cue for segmenting image sequence recently a number of algorithm have been developed that successfully perform motion segmentation by assuming that the motion of each group can be described by a low dimensional parametric model e g affine typically the assumption is that motion segment correspond to planar patch in d undergoing rigid motion here we develop an alternative approach where the motion of each group is described by a smooth dense flow field and the stability of the estimation is ensured by mean of a prior distribution on the class of flow field we present a variant of the em algorithm that can segment image sequence by fitting multiple smooth flow field to the spatiotemporal data using the method of green s function we show how the estimation of a single smooth flow field can be performed in closed form thus making the multiple model estimation computationally feasible furthermore the number of model is estimated automatically using similar method to those used in the parametric approach we illustrate the algorithm s performance on synthetic and real image sequence 
this paper introduces a method for regularization of hmm system that avoids parameter overfitting caused by insufficient training data regularization is done by augmenting the em training method by a penalty term that favor simple and smooth hmm system the penalty term is constructed a a mixture model of negative exponential distribution that is assumed to generate the state dependent emission probability of the hmms this new method is the successful transfer of a well known regularization approach in neural network to the hmm domain and can be interpreted a a generalization of traditional state tying for hmm system the effect of regularization is demonstrated for continuous speech recognition task by improving overfitted triphone model and by speaker adaptation with limited training data 
a simple and inexpenssive approach for extracting the three dimentional shape of object is presented it is based on weak structured lighting it differs from other conventional structured lighting approach in that it requires very little hardware besides the camera a desk lamp a pencil and a checkerboard the camera face the object which is illuminated by the desk lamp the user move a pencil in front of the light source casting a moving shadow on the object the d shape of the object is extracted from the spatial and temporal location of the observed shadow experimental result are presented on three different scene demonstrating that the error in reconstructing the surface is le than 
we recently demonstrated a new approach to multiframe structure from motion from point feature which in the appropriate domain provably reconstructs structure and motion correctly the algorithm work for general motion and large perspective effect in this paper we describe how to adapt our approach to translational motion lying along a line or in a plane with arbitrary rotation an analysis of the ba relief effect for multiple motion sequence is also presented 
binocular stereo is the process of obtaining depth information from a pair of left and right view of a scene we present a new approach to compute the disparity map by solving a global optimization problem that model occlusion discontinuity and epipolar line interaction in the model geometric constraint require every disparity discontinuity along the epipolar line in one eye toalways correspond to an occluded region in the other eye while at the same time encouraging smoothness across epipolar line smoothing coefficient are adjusted according to the edge and junction information for some well defined set of optimization function we can map the optimization problem to a maximum flow problem on a directed graph in a novel way which enables u to obtain a global solution in a polynomial time experiment confirm the validity of this approach 
standard recurrent net cannot deal with long minimal time lag between relevant signal several recent nip paper propose alternative method we first show problem used to promote various previous algorithm can be solved more quickly by random weight guessing than by the proposed algorithm we then use lstm our own recent algorithm to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of 
the structure from motion algorithm from two view fails if the object is a planar surface or the camera motion is a pure rotation this paper present a new scheme for automatically detecting these anomaly without using any knowledge about the noise in the image this judgment doe not involve any empirically adjustable threshold either the basic principle of our scheme is to choose a model that ha higher predicting capability measured by the geometric information criterion geometric aic 
this paper proposes a new method for model based tracking of a human body in d motion from multiple view the tracking is performed by estimating the pose increment of the body part from multiple image sequence after establishing of fitting an articulated model to the human body at the initial frame the pose increment can be obtained from solving a system of linear equation this calculation doe not depend on the number of view point experiment verify that the proposed method can avoid occusion and visual degenerate from a particular view point and track a human body in complicated motion 
a principal problem of auditory scene analysis is stream segregation decomposing an input acoustic signal into signal of individual sound source included in the input while existing signal processing algorithm cannot properly solve this inverse problem a multiagentbased architecture ha been considered to be a promising methodology in it modularity and scalability however most attempt made so far depend on subjectively defined rule to deal with variability of sound here we propose a quantitatively principled architecture in agent interaction by formulating the problem a least square optimization in this architecture adaptation of the agent is the essential idea we have developed two kind of processing to realize adaptivity template filtering and phase tracking these mechanism enable each agent to optimally in the least square sense track the individual sound a an example application of the proposed architecture we have built a music recognition system that recognizes instrument name and pitch of the note included in ensemble music performance experimental result show that these adaptive mechanism significantly improve the recognition accuracy 
two key performance characterization of biometric algorithm face recognition in particular are verification performance and and performance a a function of database size and composition this characterization is required for developing robust face recognition algorithm and for successfully transitioning algorithm from the laboratory to real world in this paper we present a general verification protocol and apply it to the result from the sep feret test and discus and present result on the effect of database size and variability on identification and verification performance 
we study the spatiotemporal correlation in natural time varyingimages and explore the hypothesis that the visual system is concernedwith the optimal coding of visual representation throughspatiotemporal decorrelation of the input signal based on themeasured spatiotemporal power spectrum the transform needed todecorrelate input signal is derived analytically and then comparedwith the actual processing observed in psychophysical experiment introductionthe visual system is 
to attain smooth human interaction we propose a system which simultaneously utilizes the stereo disparity and optical flow information of real time stereo gray multiresolution image to recognize object and gesture for real time calculation of the disparity and optical flow information of a stereo image the system first creates pyramid image by utilizing a gaussian filter the system then determines the disparity and optical flow of a low density image and extract region in front of a certain depth the three foremost region are recognized by higher order local auto correlation feature and a linear discriminant analysis with this process the system recognizes the face and hand sign of user which are displayed foremost and roughly recognizes novements within the region in real time with this framework the system can discriminate the face of a user can monitor the basic movement of the user can smoothly learn a presented object by user and can communicate with user from hand sign learned in advance 
a segmentation and velocity estimation technique is presented which treat each object either moving or stationary a a distinct intensity wave profile the fourier component of wave profile and equally of object which move with constant velocity exhibit a regular frequency dependent phase change using a hough transform which embodies the relationship between velocity and phase change moving object are isolated by identifying the subset of the fourier component of the total image intensity wave profile which exhibit this phase relationship velocity is measured by locating local maximum in the hough space and segmentation is effected by re constituting the moving wave profile the object from the fourier component which satisfy the velocity phase change relationship for the detected velocity 
this paper present a novel theory for learning generic prior model from a set of observed natural image based on a minimax entropy theory that the author studied in texture modeling we start by studying the statistic of natural image including the scale invariant property then generic prior model were learnt to duplicate the observed statistic the learned gibbs distribution confirm and improve the form of existing prior model and more interestingly inverted potential are found to be necessary and such potential produce pattern and enhance preferred image feature the learned model is compared with existing prior model in experiment of image restoration 
we present a set of algorithm and a search strategy for the robust contentbased retrieval of multispectral satellite image since the property of interest in these image is usually the physical characteristic of ground cover we use representation and method that are invariant to illumination and atmospheric condition the representation and algorithm are derived for this application from a physical model for the formation of multispectral satellite image the use of several representation and algorithm is necessary to interpret the diversity of physical and geometric structure in these image algorithm are used that exploit multispectral distribution multispectral spatial structure and labeled class the performance of the system is demonstrated on a large set of multispectral satellite image taken over different area of the united state under different illumination and atmospheric condition 
an algorithm to detect depth discontinuity from a stereo pair ofimages is presented the algorithm match individual pixel incorresponding scanline pair while allowing occluded pixel toremain unmatched then propagates the information betweenscanlines by mean of a fast postprocessor the algorithm handleslarge untextured region us a measure of pixel dissimilaritythat is insensitive to image sampling and prune bad search nodesto increase the speed of dynamic programming the computation isrelatively fast taking about nanosecond per pixel perdisparity on a personal computer approximate disparity map andprecise depth discontinuity along both horizontal and verticalboundaries are shown for several stereo image pair containingtextured untextured fronto parallel and slanted object inindoor and outdoor scene 
biological control system routinely guide complex dynamical system through complicatedtasks such a running or diving conventional control technique however stumble with these problem which have complex dynamic many degree of freedom and a task which is often only partially specified e g quot move forward fast quot or quot executea one and one half somersault dive quot to address problem like these we are using abiologically inspired hierarchical control structure in which controller 
we use data from the trec routing experiment to explore how relevance feedback can be applied incrementally using a few judged document each time to achieve result that are a good a if the feedback occurred in one pas we show that relatively few judgment are needed to get highquality result we also demonstrate method that reduce the amount of information archived from past judged document without adversely affecting effectiveness a novel simulation show that such 
a method of registering image at subpixel accuracy ha been proposed which doe not resort to interpolation the method is based on the phase correlation method and is remarkably robust to correlated noise and uniform variation of luminance we have shown that the cross power spectrum of two image containing subpixel shift is a polyphase decomposition of a dirac delta function by estimating the sum of polyphase component one can then determine subpixel shift along each axis 
this paper present technique for constructing full view panoramic mosaic form sequence of image our representation associate a rotation matrix and optionally a focal length with each input image rather than explicitly projecting all of the image onto a common surface e g a cylinder in order to reduce accumulated registration error we apply global alignment block adjustment to whole sequence of image which result in an optimal image mosaic in the least square sense to compensate for small amount of motion parallax introduced by translation of the camera and other unmodeled distortion we develop a local alignment deghosting technique which warp each image based on the result of pairwise local image registration by combining both global and local alignment we significantly improve the quality of our image mosaic thereby enabling the creation of full view panoramic mosaic with hand held camera 
the main problem for building a mosaic is the computation of the warping function homographies in fact two case are to be distinguished the first is when the homography is mainly a translation i e the rotation around the optical axis and the zooming factor are small the second is the general case when the rotation around the optical axis and zooming are arbitrary some efficient method have been developed to solve the first case but the second case is more difficult in particular when the rotation around the optical axis is very large degree or more often in this case human interaction is needed to provide a first approximation of the transformation that will bring u back to the first case in this article we present a method to solve this problem without human interaction for any rotation around the optical axis and fairly large zooming factor 
a simple learning rule is derived the vaps algorithm which canbe instantiated to generate a wide range of new reinforcementlearningalgorithms these algorithm solve a number of openproblems define several new approach to reinforcement learning and unify different approach to reinforcement learning under asingle theory these algorithm all have guaranteed convergence and include modification of several existing algorithm that wereknown to fail to converge on simple mdps these 
one approach to invariant object recognition employ a recurrent neuralnetwork a an associative memory in the standard depiction of thenetwork s state space memory of object are stored a attractive fixedpoints of the dynamic i argue for a modification of this picture if anobject ha a continuous family of instantiation it should be representedby a continuous attractor this idea is illustrated with a network thatlearns to complete pattern to perform the task of filling in 
we describe the generation of a large pose mosaic dataset a collection of several thousand digital image grouped by spatial position into spherical mosaic each annotated with estimate of the acquiring camera s dof pose dof position and dof orientation in an absolute coordinate system the pose mosaic dataset wa generated by acquiring image grouped by spatial position into node essentially spherical mosaic a prototype mechanical pan tilt head wa manually deployed to acquire the data manual surveying provided initial position estimate for each node a back projecting scheme provided initial rotational estimate relative rotation within each node along with internal camera parameter were refined automatically by an optimization correlation scheme relative translation and rotation among node were refined according to point correspondence generated automatically and by a human operator the resulting pose imagery is self consistent under a variety of evaluation metric pose mosaic are useful first class data object for example in automatic reconstruction of textured d cad model which represent urban exterior 
in this paper we consider a multi camera vision system mounted on a moving object in a static three dimensional environment by using the motion flow field seen by all of the camera an algorithm which doe not need to solve the point correspondence problem among the camera is proposed to estimate the d ego motion parameter of the moving object our experiment have shown that using multiple optical flow field obtained from different camera can be very helpful for ego motion estimation 
when faced with inadequate information human often use knowledgegained from previous experienceto help them in making decision even when this knowledgeis spread thinly among manyprevious experience human areable to effectively accumulate andapply it to a current classificationtask of interest inspired byhuman knowledge reuse we havepreviously introduced a generalframework for the use of knowledgeembodied in existing classifiersto aid in a new classification 
there is a wealth of information to be mined from narrative text on the world wide web unfortunately standard natural language processing nlp extraction technique expect full grammatical sentence and perform poorly on the choppy sentence fragment that are often found on web page this paper introduces webfoot a preprocessor that par web page into logically coherent segment based on page layout cue output from webfoot is then passed on to crystal an nlp system that learns 
this paper present a prediction and verification segmentation scheme wing attention image from multiple fixation a major advantage of this scheme is that it can handle a large number of different deformable object presented in complex background the scheme is also relatively efficient since the segmentation is guided by the past knowledge through a prediction and verification scheme the system ha been tested to segment hand in the sequence of intensity image where each sequence represents a hand sign the experimental result showed a correct segmentation rate with a false rejection rate 
the olga project ha developed an animated agent interface for information service the interface combine a graphical interface spoken dialogue and an animated d human like character for multimodal interaction with user the interaction is intelligently managed using technique derived from spoken dialogue but extended for the graphical modality the olga agent is innovative in combining an interactive spoken dialogue system with a d animated character using lip synchronized synthetic speech and gesturing particular attention h a been paid to ensuring that t he behaviour of the a gent i s immediately comprehensible for the user synchronizing speech with mouth movement increase intelligibility while facial expression and gesturing realize the agent s internal state and focus of the dialogue 
computer vision system such a seeing robot aimed at functioning robustly in a natural environment rich on information benefit from relying on multiple cue then the problem of integrating these become central existing approach to cue integration have typically been based on physical and mathematical model for each cue and used estimation and optimization method to fuse the parameterizations of these model in this paper we consider an approach for fusion that doe not rely on the underlying model for each cue it is based on a simple binary voting scheme a particular feature of such a scheme is that also incommensurable cue such a intensity and surface orientation can be fused in a direct way other feature are that uncertainty and the normalization of them is avoided instead consensus of several cue is considered a non accidental and used a support for hypothesis of whatever structure is sought for it is shown that only a small set of cue need to agree to obtain a reliable output we apply the proposed technique to finding instance of planar surface in binocular image without resorting to scene reconstruction or segmentation the result are of course not comparable to the best result that can be obtained by complete scene reconstruction however they provide the most obvious instance of plane also with rather crude assumption and coarse algorithm even though the precise extent of the planar patch is not derived good overall hypothesis are obtained our work applies voting scheme beyond earlier attempt and also approach the cue integration problem in a novel manner although further research is needed to establish the full applicability of our technique our result so far seem quite useful 
the retrieval of image from a large database of image is an important and emerging area of research here a technique to retrieve image based on appearance that work effectively across large change of scale is proposed the database is initially filtered with derivative of a gaussian at several scale a user defined template is then created from an image of an object similar to those being sought the template is also filtered using gaussian derivative the template is then matched with the filter output of the database image and the match ranked according to the match score experiment demonstrate the technique on a number of image in a database no prior segmentation of the image is required and the technique work with viewpoint change up to degree and illumination change 
one of the central problem in stereo matching and other image registration task is the selection of optimal window size for comparing image region this paper address this problem with some novel algorithm based on iteratively diffusing support at different disparity hypothesis and locally controlling the amount of diffusion based on the current quality of the disparity estimate it also develops a novel bayesian estimation technique which significantly outperforms technique based on area based matching ssd and regular diffusion we provide experimental result on both synthetic and real stereo image pair 
we introduced an unsupervised texture segmentation method the selectionist relaxation relying on a markov random field mrf texture description and a genetic algorithm based relaxationscheme it ha been shown elsewhere that this method is convenientfor achieving a parallel and reliable estimation of mrf parameter andconsequently a correct image segmentation nevertheless these resultshave been obtained with an order model on artificial texture the purposeof the present work is 
the class of algorithm for approximating reasoning task presented in this paper is based on approximating the general bucket elimination framework the algorithm have adjustable level of accuracy and efficiency and they can be applied uniformly across many area and problem task we introduce these algorithm in the context of combinatorial optimization and probabilistic inference 
the new classification algorithm clef combine a version of a linear machine known a a phi machine with a non linear function approximator that construct it own feature the algorithm find non linear decision boundary by constructing feature that are needed to learn the necessary discriminant function the clef algorithm is proven to separate all consistently labelled training instance even when they are not linearly separable in the input variable the algorithm is illustrated on a variety of task 
when combining a set of learned model to form an improved estimator the issue of redundancy or multicollinearity in the set ofmodels must be addressed a progression of existing approachesand their limitation with respect to the redundancy is discussed a new approach pcr based on principal component regressionis proposed to address these limitation an evaluation of thenew approach on a collection of domain reveals that pcr wa the most robust combination method a the 
in robotics and other control application it is commonplac e to have a preexisting set of controller for solving subtasks perhaps h and crafted or previously learned or planned and still face a difficult pro blem of how to choose and switch among the controller to solve an overall task a well a possible in this paper we present a framework based on markov decision process and semi markov decision process for phrasing this problem a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controller and example application of the theorem in particular we show how an agent can plan with these high level controller and then use the result of suc h planning to find an even better plan by modifying the existing controller with negligible additional cost and no re planning in one of our example the complexity of the problem is reduced from billion state action pair to le than a million state controller pair 
the need to generate new view of a d object from a single real image arises in several field including graphic and object recognition while the traditional approach relies on the use of d model we exploit d image transformation that are specific to the relevant object class and learnable from example view of other prototypical object of the same class 
tracking with deformable contour in a filtering frame work requir esa dynamical model for prediction for any given application tracking is improved by having an accurate model learned from training data we develop a method for learning dynamical model from training sequence explicitly taking account of the fact that training data are noisy measurement and not true state by introducing an augmented state smoothing filter we show how the technique of expectation maximisation can be applied to this problem and show that the resulting algorithm produce more robust and accurate tracking 
describing a video sequence in term of a small number of coherently moving segment is useful for task ranging from video compression to event perception promising approach is to view the motion segmentation problem in a mixture estimation framework however existing formulation generally use only the motion data and thus fail to make use of static cue when segmenting the sequence furthermore the number of model is either specified in advance or estimated outside the mixture model framework in this work we address both of these issue we show how to add spatial constraint to the mixture formulation and present a variant of the em algorithm that make use of both the form and the motion constraint moreover this algorithm estimate the number of segment given knowledge about the level of model failure expected in the sequence the algorithm s performance is illustrated on synthetic and real image sequence 
stereo sequence promise to be a powerful method for segmenting image for application such a tracking human figure we present a method of statistical background modeling for stereo sequence that improves the reliability and sensitivity of segmentation in the presence of object clutter the dynamic version of the method called gated background adaptation can reliably learn background statistic in the presence of corrupting foreground motion the method ha been used with a simple head discriminator to detect and track people using a stereo head mounted on a pan tilt platform it run at video rate using standard pc hardware 
the problem of determining feature correspondence across multiple view is considered the term true multi image matching is introduced to describe technique that make full and efficient use of the geometric relationship between multiple image and the scene a true multi image technique must generalize to any number of image be of linear algorithmic complexity in the number of image and use all the image in an equal manner a new space sweep approach to true multi image matching is presented that simultaneously determines d feature correspondence and the d position of feature point in the scene the method is illustrated on a seven image matching example from the aerial image domain 
this paper describes a new method for lens distortion calibration using only point correspondence in multiple view without the need to know either the d location of the point or the camera location the standard lens distortion model is a model of the deviation of a real camera from the ideal pinhole or projective camera model given multiple view of a set of corresponding point taken by ideal pinhole camera there exist epipolar and trilinear constraint among pair and triplet of these view in practice due to noise in the feature detection and due to lens distortion these constraint do not hold exactly and we get some error the calibration is a search for the lens distortion parameter that minimize this error using simulation and experimental result with real image we explore the property of this method we describe the use of this method with the standard lens distortion model radial and decentering but it could also be used with any other parametric distortion model finally we demonstrate that lens distortion calibration improves the accuracy of d reconstruction 
it is shown that conventional computer can be exponentially faster than planar hopfield network although there are planar hopfield network that take exponential time to converge a stable state of an arbitrary planar hopfield network can be found by a conventional computer in polynomial time the theory of pls completeness give strong evidence that such a separation is unlikely for nonplanar hopfield network and it is demonstrated that this is also the case for several restricted class of nonplanar hopfield network including those who interconnection graph are the class of bipartite graph graph of degree the dual of the knight s graph the neighbor mesh the hypercube the butterfly the cube connected cycle and the shuffle exchange graph 
we describe a novel approach for image matchingbased on deformable intensity surface in thisapproach the intensity surface of the imageis modeled a a deformable d mesh in the x y i x y space each surface point ha degree of freedom thus capturing fine surfacechanges a set of representative deformationswithin a class of object e g face are statisticallylearned through a principal componentsanalysis thus providing a priori knowledgeabout object specific 
the success of an intelligent robotic system depends on the performance of it vision system which in turn depends to a great extend upon the quality of it calibration during the execution of a task the vision system is subject to external influence such a vibration thermal expansion etc which affect and possibly render invalid the initial calibration moreover it is possible that the parameter of the vision system like e g the zoom or the focus are altered intentionally in order to perform specific vision task this paper describes a technique for automatically maintaining calibration of stereovision system over time without using again any particular calibration apparatus it us all available information i e both spatial and temporal data uncertainty is systematically manipulated and maintained synthetical and real data are used to validate the proposed technique and the result compare very favourably with those given by classical calibration method 
this paper proposes a new algorithm for matching point feature across pair of image despite the well known combinatorial complexity of the problem this work show that an acceptably good solution can be obtained directly by singular value decomposition of an appropriate correspondence strength matrix the approach draw from the method proposed previously but besides suggesting it usefulness for stereo matching in this work a correlation weighted proximity function is used a correspondence strength to specifically cater for real image 
the class of algorithm for approximating rea soning task presented in this paper is based on approximating the general bucket elimination framework the algorithm have adjustable level of accuracy and efficiency and they can be applied uniformly across many area and problem task we introduce these algorithm in the context of combinatorial optimization and probabilistic inference 
crease are a type of ridge valley structure that can be characterized by local condition therefore creaseness refers to local ridgeness and valleyness the curvature of the level curve and the mean curvature m of the level surface are good measure of creaseness for d and d image respectively however the way they are computed give rise to discontinuity reducing their usefulness in many application we propose a new creaseness measure based on these curvature that avoids the discontinuity we demonstrate it usefulness in the registration of ct and mr brain volume from the same patient by searching the maximum in the correlation of their creaseness response ridgeness from the ct and valleyness from the mr due to the high dimensionality of the space of transforms the search is performed by a hierarchical approach combined with an optimization method at each level of the hierarchy 
first order logic is the traditional basis for knowledge representation language however it applicability to many real world task is limited by it inability to represent uncertainty bayesian belief network on the other hand are inadequate for complex kr task due to the limited expressivity of the underlying prepositional language the need to incorporate uncertainty into an expressive language ha led to a resurgence of work on first order probabilistic logic this paper address one of the main objection to the incorporation of probability into the language where do the number come from we present an approach that take a knowledge base in an expressive rule based first order language and leams the probabilistic parameter associated with those rule from data case our approach which is based on algorithm for learning in traditional bayesian network can handle data case where many of the relevant aspect of the situation are unobserved it is also capable of utilizing a rich variety of data case including instance with varying causal structure and even involving a varying number of individual these feature allow the approach to be used for a wide range of task such a learning genetic propagation model or learning first order strip planning operator with uncertain effect 
we consider the problem of aggregation for uncertain and imprecise data for such data we define aggregation operator and use them to provide information on property and pattern of data attribute the aggregate that we define use the kullback leibler information divergence between the aggregated probability distribution and the individual tuple data value we are thus able to provide a probability distribution for the domain value of an attribute or group of attribute using imperfect data information stored in a database is often subject to uncertainty and imprecision an extended relational data model ha previously been proposed for such data which allows u to quantify our uncertainty and imprecision about attribute value by representing them a a probability distribution our aggregation operator are defined on such a data model the provision of such operator is a central requirement in furnishing a database with the capability to perform the operation necessary for knowledge discovery in database 
we present an extensive empirical comparison of several smoothing technique in the domain of language modeling including those described by jelinek and mercer katz and church and gale we investigate for the first time how factor such a training data size corpus gram order bigram versus trigram affect the relative performance of these method which we measure through the cross entropy of test data in addition we introduce two novel smoothing technique one a variation of jelinek mercer smoothing and one a very simple linear interpolation technique both of which outperform existing method 
the purpose of this study is not only to recognize some kind of facial expression which is associated with human emotion but also to estimate it degree our method is based on the idea that facial expression recognition can be achieved by extracting a variation from expressionless face with considering face area a a whole pattern for the purpose of extracting subtle change in the face such a the degree of expression it is necessary to eliminate the individuality appearing in the facial image using a elastic net model a variation of facial expression is represented a motion vector of the deformed net from a facial edge image then applying k l expansion the change of facial expression represented a the motion vector of node is mapped into low dimensional eigen space and estimation is achieved by projecting input image on to the emotion space in this paper we have constructed three kind of expression model happiness anger surprise and experimental result are evaluated 
despite many successful application of robust statistic they have yet to be completely adapted to many computer vision problem range reconstruction particularly in unstructured environment requires a robust estimator that not only tolerates a large outlier percentage but also tolerates several discontinuity extracting multiple surface in an image region observing that random outlier and or point from across discontinuity increase a hypothesized fit s scale estimate standard deviation of the noise our new operator called muse minimum unbiased scale estimator evaluates a hypothesized fit over potential inlier set via an objective function of unbiased scale estimate muse extract the single best fit from the data by minimizing it objective function over a set of hypothesized fit and can sequentially extract multiple surface from an image region we show muse to be effective on synthetic data modelling small scale discontinuity and in preliminary experiment on complicated range data 
abstract data reduction make datasets smaller but preserve classi cation structure of interest it is an important area of research in data mining and database in this paper we present a novel approach to data reduction based on hyper relation our method is a generalization of the data ltering method introduced by for one attribute to many attribute hyper relation are a generalization of conventional database relation in the sense that we allow set of value a tuple entry the advantage of this is that raw data and reduced data can both be represented by hyper relation the collection of hyper relation can be naturally made into a complete boolean algebra and so for any collection of hyper tuples we can nd it unique least upper bound lub a a reduction of it however the lub may not qualify a a reduced version of the given set of tuples then we turn to nd the interior cover the subset of internal element covered by the lub we establish the theoretical result that such an interior cover exists and we present a method by which we can nd it this interior serf a a reduced version of the data the proposed method wa evaluated using real world datasets with respect to it test accuracy the result were quite remarkable in that the cross validated test accuracy were substantially higher than those obtained by c in out of datasets the datasets were reduced with reduction ratio up to 
in recent paper miller goodman smyth provided condition on the cost function used for the training of a neural network in order to ensure that the output of the network approximates the conditional expectation of the desired output given the input however they only considered the single output case in this paper we provide another rather straightforward proof of the same result for the general multi output case all the development being presented in the context of estimation theory more precisely among a class of reasonable performance criterion we provide necessary and sufficient condition on the cost function so that the optimal estimate is the conditional expectation of the desired output whatever the noise characteristic affecting the data we furthermore provide a short overview of related result from estimation theory and verify numerically the development by comparing the optimal estimator of several performance criterion we must stress that while all these result are stated for a neural network they are however true in general for any learning machine that is trained in order to predict an output y in function of an input x keywords performance criterion cost function probabilistic interpretation of output unit 
lexicalized tree adjoining grammar have proved useful for nlp however numerous redundancy problem face ltags developer a highlighted by vijay shanker and schabes we present and a tool that automatically generates the tree family of an ltag it start from a compact hierarchical organization of syntactic description that is linguistically motivated and carry out all the relevant combination of linguistic phenomenon 
a high performance information filtering system ha three mainrequirements it must be effective in supplying user with usefulinformation it must do so in a timely fashion and it must be ableto handle a large throughput of information and a large number ofuser profile efficiently these three requirement pose adifficult problem and to our knowledge no existing system iscapable of meeting all three in this paper we describe a systemwhich combine a number of technique from other informationretrieval and filtering system and is capable of providing highperformance on a typical workstation platform we provide estimatesof computing resource usage and show that our system is alsoscalable 
we present an algorithm for identifying linear pattern on a twodimensionallattice based on the concept of an orientation selectivecell a concept borrowed from neurobiology of vision constructinga multi layered neural network with fixed architecture whichimplements orientation selectivity we define output element correspondingto different orientation which allow u to make a selectiondecision the algorithm take into account the granularityof the lattice a well a the presence of 
it ha been suggested that long range intrinsic connection in striate cortex may play a role in contour extraction gilbert et al a number of recent physiological and psychophysical study have examined the possible role of long range connection in the modulation of contrast detection threshold polat and sagi kapadia et al kov c and julesz and various pre attentive detection task kov c and julesz field et al we have developed a network architecture based on the anatomical connectivity of striate cortex a well a the temporal dynamic of neuronal processing that is able to reproduce the observed experimental result the network ha been tested on real image and ha application in term of identifying salient contour in automatic image processing system 
we present a variational approach to dense stereo reconstruction which combine powerful tool such a regularization and multi scale processing to e stimate directly depth from a number of stereo image while preserving depth discontinuity the problem is set a a regularization and minimization of a nonquadratic functional the tikhonov quadratic regularization term usually used to recover smooth solution is replaced by a function of the gradient depth specifically derived to allow depth discontinuity formation in the solution condition to be fulfilled by this specific regularizing term to preserve discontinuitiesare also presented to solve this problem in the discrete case a pde based expli cit scheme for moving iteratively towards the solution ha been developed this approach present the additional advantage of not introducing any intermediate representation such a disparity or rectified image depth is computed directly from the grey level image and we can also deal with any number greater than two of camera promising experimental result illustrate the capabili tie of this approach 
this paper is a comparative study of feature selection method in statistical learning of text categorization the focus is on aggressive dimensionality reduction five method were evaluated including term selection based on document frequency df information gain ig mutual information mi a test chi and term strength t we found ig and chi most effective in our experiment using ig thresholding with a k nearest neighbor classifier on the reuters corpus removal of up to removal of unique term actually yielded an improved classification accuracy measured by average precision df thresholding performed similarly indeed we found strong correlation between the df ig and chi value of a term this suggests that df thresholding the simplest method with the lowest cost in computation can be reliably used instead of ig or chi when the computation of these measure are too expensive t compare favorably with the other method with up to vocabulary reduction but is not competitive at higher vocabulary reduction level in contrast mi had relatively poor performance due to it bias towards favoring rare term and it sensitivity to probability estimation error 
we present a fast electronic image stabilization system that compensates for d rotation the extended kalman filter framework is employed to estimate the rotation between frame which is represented using unit quaternion a small set of automatically selected and tracked feature point are used a measurement the effectiveness of this technique is also demonstrated by constructing mosaic image from the motion estimate and comparing them to mosaic built from d stabilization algorithm two different stabilization scheme are presented the first implemented in a real time platform based on a datacube mv board estimate the motion between two consecutive frame and is able to process gray level image of resolution x at hz the second scheme estimate the motion between the current frame and an inverse mosaic this allows better estimation without the need for indexing the new image frame experimental result for both scheme using real and synthetic image sequence are presented 
a divide and conquer strategy in shape from shading problem under fully perspective condition is proposed for the information recovery of book surface the whole recovery process is composed of three sequential step preprocessing apparent shape recovery and ortho image generation pure shade image are extracted at preprocessing step by introducing phenomenological model of interreflection and by removing pigment part from observed image using existing invariance equation of slope and that of ortho image being explicit are derived from equation of shading and that of observation being implicit recurrence relation is derived from definition of mean slope in discrete image theoretically it become possible to recover unique shape without iteration using derived equation in case of lambertian cylinder however a feed back shape recovery process is implemented a practical algorithm in order to overcome self shadow result of simulation and real experiment show the properness and acceptability of the proposed strategy and implemented algorithm 
one of the most promising application of d ultrasound lie in the visualisation and volume estimation of internal d structure unfortunately artifact and speckle make automatic analysis of the data difficult in this paper we investigate the use of d spatial compounding to improve data quality and find that accurate registration is the key a correlation based registration technique is applied to d ultrasound data acquired from in vivo examination of a human gall bladder we find that the registration technique performs well and visualisation and segmentation of the compounded data are clearly improved 
there ha been much recent work on measuring image statistic and on learning probability distribution on image we observe that the mapping from image to statistic is many to one and show it can be quantified by a phase space factor this phase space approach throw light on the minimax entropy technique for learning gibbs distribution on image with potential derived from image statistic and elucidates the ambiguity that are inherent to determining the potential in addition it show that if the phase factor can be approximated by an analytic distribution then the computation time for minimax entropy learning can be vastly reduced an illustration of this concept using a gaussian to approximate the phase factor lead to a new algorithm called minutemax which give a good approximation to the result of zhu and mumford in just second of cpu time the phase space approach also give insight into the multi scale potential found by zhu and mumford and suggest that the form of the potential are influenced greatly by phase space consideration finally we prove that probability distribution learned in feature space alone are equivalent to minimax entropy learning with a multinomial approximation of the phase factor 
in this paper we propose a memory based q learning algorithmcalled predictive q routing pq routing for adaptive traffic control we attempt to address two problem encountered in q routing boyan amp littman namely the inability to fine tune routingpolicies under low network load and the inability to learn newoptimal policy under decreasing load condition unlike othermemory based reinforcement learning algorithm in which memoryis used to keep past experience to increase 
in wirth t reinartz we introduced the early indicator method a multi strategy approach for the efficient prediction of various aspect of the fault profile of a set of car in a large automotive database while successful the initial implementation wa limited in various way in this paper we report recent progress focusing on performance gain we achieved through proper process based database support we show how intelligent management of the information collected during a kdd process can both make the task of the user easier and speed up the execution the central idea is to use an object oriented schema a the central information directory to which data knowledge and process can be attached furthermore it enables the automatic exploitation of previously stored result together with the query shipping strategy this achieves efficiency and scalability in order to analyze huge database while we demonstrate the usefulness of our solution in the context of the early indicator method the approach is generally applicable and useful for any integrated comprehensive kdd system 
this paper show that a class of combinatory categorial grammar ccgs augmented with a linguistically motivated form of type raising involving variable is weakly equivalent to the standard ccgs not involving variable the proof is based on the idea that any instance of such a grammar can be simulated by a standard ccg 
the neurothermostat is an adaptive controller that regulates indoorair temperature in a residence by switching a furnace on oroff the task is framed a an optimal control problem in whichboth comfort and energy cost are considered a part of the controlobjective because the consequence of control decision aredelayed in time the neurothermostat must anticipate heating demandswith predictive model of occupancy pattern and the thermalresponse of the house and furnace occupancy 
we introduce a new distance between two distribution that we call the earth mover s distance emd which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving distribution mass around this is a special case of the transportation problem from linear optimization for which efficient algorithm are available the emd also allows for partial matching when used to compare distribution thathave the same overall mass the emd is a true metric and ha easy to compute lower bound in this paper we focus on application to image database especially color and texture we use the emd to exhibit the structure of color distribution and texture space by mean of multi dimensional scaling display we also propose a novel approach to the problem of navigating through a collection of color image which lead to a new paradigm for image database search 
we show that the self calibration of a stereo head from corresponding point in an image pair is in certain circumstance prone to considerable error a novel error analysis reveals that the automated determination of relative orientation and focal length is adversely affected when the camera verge inwards a similar amount and when the principal point location have a horizontal error this analysis is facilitated by the adoption of closed form solution for self calibration from previous work of the author it is also shown that estimation of the fundamental matrix associated with a stereo head image pair is improved when a domain specific parameterization and associated computational technique are adopted experiment conducted with such image pair suggest that given cognisance of sensitive configuration and adoption of the revised method of fundamental matrix estimation robust reconstruction are attainable this is demonstrated on the problem of metrically reconstructing a scene from two pair of image obtained by an uncalibrated stereo head undergoing unknown ground plane motion 
dependence on landmark point or high order derivativeswhen establishing correspondence between geometrical image curve undervarious subclass of projective transformation remains a shortcomingof present method in the proposed framework geometric transformationsare treated a smooth function involving the parameter of thecurves on which the transformation basis point lie by allowing the basispoints to vary along the curve hypothesised correspondence are freedfrom the 
occam s razor ha been the subject of much controversy this paper argues that this is partlybecause it ha been interpreted in two quite differentways the first of which simplicity is a goalin itself is essentially correct while the second simplicity lead to greater accuracy is not thepaper review the large variety of theoretical argumentsand empirical evidence for and against the quot second razor quot and concludes that the balanceis strongly against it in particular it 
this work recovers d graphic model of object with specular surface an object is routed and continuous image of it are taken circular light that generate cone of ray are used to illuminate the rotating object when the light are properly set each point on the object can be highlighted during the rotation the shape for each rotational plane is measured independently using it corresponding epipolar palne image a d graphic model is subsequently reconstructed by combining shape at different rotational plane computing a shape is simple and requires only the motion of the highlight on each rotation plane result not obtained before are given in the d shape recovery experiment on real object 
the genetic algorithm ga is a heuristic search procedure based on mechanism abstracted from population genetics in a previous paper baluja caruana we showed that much simpler algorithm such a hillclimbing and populationbased incremental learning pbil perform comparably to gas on an optimization problem custom designed to benefit from the ga s operator this paper extends these result in two direction first in a large scale empirical comparison of problem that have been reported in ga literature we show that on many problem simpler algorithm can perform significantly better than gas second we describe when crossover is useful and show how it can be incorporated into pbil implicit v explicit search statistic although there ha recently been controversy in the genetic algorithm ga community a to whether gas should be used for static function optimization a large amount of research ha been and continues to be conducted in this direction de jong since much of ga research focus on optimization most often in static environment this study examines the performance of gas in these domain in the standard ga candidate solution are encoded a fixed length binary vector the initial group of potential solution is chosen randomly at each generation the fitness of each solution is calculated this is a measure of how well the solution optimizes the objective function the subsequent generation is created through a process of selection recombination and mutation recombination operator merge the information contained within pair of selected parent by placing random subset of the information from both parent into their respective position in a member of the subsequent generation the fitness proportional selection work a selective pressure higher fitness solution string have a higher probability of being selected for recombination mutation are used to help preserve diversity in the population by introducing random change into the solution string the ga us the population to implicitly maintain statistic about the search space the selection crossover and mutation operator can be viewed a mechanism of extracting the implicit statistic from the population to choose the next set of point to sample detail of gas can be found in goldberg holland population based incremental learning pbil is a combination of genetic algorithm and competitive learning baluja the pbil algorithm attempt to explicitly maintain statistic about the search space to decide where to sample next the object of the algorithm is to create a real valued probability vector which when sampled reveals high quality solution vector with high probability for example if a good solution can be encoded a a string of alternating s and s a suitable final probability vector would be etc the pbil algorithm and parameter are shown in figure initially the value of the probability vector are initialized to sampling from this vector yield random solution vector because the probability of generating a or is equal a search progress the value in the probability vector gradually shift to represent high eval 
until now all super resolution algorithm have presumed that the image were taken under the same illumination condition this paper introduces a new approach to super resolution based on edge model and a local blur estimate which circumvents these difficulty the paper present the theory and the experimental result using the new approach 
the solar chromosphere consists of three class plage network background which contribute differently to ultraviolet radiation reachina thn am th nl r nhv iota cama intmwatd in a y u v y yyj uy a y yivy vu iy relating plage area and intensity to uv irradiante a well a understanding the spatial and temporal evolution of plage shape we describe a data set of solar image mean of segmenting the image into constituent class and a novel high level representation for compact object based on a spatial membership function defmed via a triangulated planar graph segmentation axe found using a discrete markov random field setup and the high level representation are learned by a markov chain monte carlo birth death process on the triangulation 
two stage in measurement of technique for informationretrieval are gathering of document for relevance assessment anduse of the assessment to numerically evaluate effectiveness weconsider both of these stage in the context of the trecexperiments to determine whether they lead to measurement thatare trustworthy and fair our detailed empirical investigation ofthe trec result show that the measured relative performance ofsystems appears to be reliable but that recall is overestimated it is likely that many relevant document have not been found wepropose a new pooling strategy that can significantly increasethe number of relevant document found for given effort withoutcompromising fairness 
this paper describes technique to perform fast and accuratecurve detection using a variant of the hough transform we show thatthe hough transform can be decomposed into small subproblems thatexamine only a subset of the parameter space each subproblem considersonly those curve that pas through some small subset of the data point this property allows the efficient implementation of the hough transformwith respect to both time and space and allows the careful propagationof the 
human being seem to recognize object based on a kind of model matching i e a virtual manipulation on mental image this paper present a d object pose estimation method simulating the human recognition scheme computer synthesizes not only an edge image but also a shading image from an object model then it match the two kind of synthesized image with the inputted image individually by using a non linear least square method and estimate the pose parameter value finally it chooses the better of the individually estimated pose thus the fusion of the shading and the edge information is achieved since the two piece of information complement each other this method ha the advantage of much higher robustness and accuracy of pose estimation than ordinary model matching technique which rely only on geometrical feature such a vertex or edge 
one of the important problem in data mining is the evaluation of subjective interestingness of the discovered rule past research ha found that in many real life application it is easy to generate a large number of rule from the database but most of the rule are not useful or interesting to the user due to the large number of rule it is difficult for the user to analyze them manually in order to identify those interesting one whether a rule is of interest to a user depends on his her existing knowledge of the domain and his her interest in this paper we propose a technique that analyzes the discovered rule against a specific type of existing knowledge which we call general impression to help the user identify interesting rule we first propose a representation language to allow general impression to be specified we then present some algorithm to analyze the discovered classification rule against a set of general impression the result of the analysis tell u which rule conform to the general impression and which rule are unexpected unexpected rule are by definition inte resting 
some of the most successful recent applicationsof reinforcement learning have usedneural network and the td algorithm tolearn evaluation function in this paper we examine the intuition that td operatesby approximating asynchronous valueiteration we note that on the importantsubclass of acyclic task value iteration isinefficient compared with another graph algorithm dag sp which assigns value tostates by working strictly backwards from thegoal we then present rout an 
we present a new algorithm for the automatic recovery of tag grid line intersection in tagged mr image of the left ventricle of the heart our method us an active spring mesh to capture local property of the motion and a global motion model to capture the global coherence of the motion we recover the global component of the motion using robust estimation different motion model have been developed for short and long axis view of the heart the algorithm ha been tested on healthy and pathological data 
in computer vision two complementary approach have been widely used to perform object reconstruction and registration the deformable model framework locally applies internal and external force to fit d data the non rigid registration framework iteratively computes the best global transformation in order to minimize the distance between a template and the data in this paper we first show that applying a global transformation on a surface model is equivalent to applying an external force on a deformable model without any regularizing force second we propose a hybrid framework which combine the registration framework and the deformable model scheme our hybrid deformation approach allows to control the scale at which the model is deformed this is clearly beneficial for performing both reconstruction and registration task we show many example of this approach on active contour and deformable surface furthermore a global transformation based on axial symmetry is introduced 
in this study ultrasound image sequence of fetus head are examined for the bi parietal diameter measurement there are five part of the fetus head that have to be seen clearly before such a measurement can be made our approach is based on model based multi objective analysis for each part an objective function encoding both data and anatomic constraint is formulated the resulting multi objective problem is then transformed into a problem of coupled differential equation and is then solved using numerical integration throughout the process continuity principle is used between the frame so that the result of analysis on one frame can be used on the next frame whenever possible 
jam is a powerful and portable agent based distributed data mining system that employsmeta learning technique to integrate a number of independent classifier concept derivedin parallel from independent and possibly inherently distributed database although metalearningpromotes scalability and accuracy in a simple and straightforward manner brute forcemeta learning technique can result in large inefficient and some time inaccurate meta classifierhierarchies in this paper we 
we consider feature selection in the quot wrapper quot model of feature selection this typicallyinvolves an np hard optimization problemthat is approximated by heuristic searchfor a quot good quot feature subset first consideringthe idealization where this optimization isperformed exactly we give a rigorous boundfor generalization error under feature selection the search heuristic typically used arethen immediately seen a trying to achievethe error given in our bound and succeedingto the 
we propose an approach for boundary finding where the correspondence of a subset of boundary point to a model is simultaneously determined global shape parameter derived from the statistical variation of object boundary point in a training set are used to model the object a bayesian formulation based on this prior knowledge and the edge information of the input image is employed to find the object boundary with it subset point in correspondence with boundary in the training set or the mean boundary we compared the use of a generic smoothness prior and a uniform independent prior with the training set prior in order to demonstrate the power of this statistical information a number of experiment were performed on both synthetic and real medical image of the brain and heart to evaluate the approach including the validation of the dependence of the method on image quality different initialization and prior information 
this paper center on the problem of finding commonality for a set of object belonging to an object oriented database in our approach commonality within a set of object are described by object oriented query that compute this set of object the paper discus the architecture of a knowledge discovery system called masson which employ genetic programming to find such query we also report on an experiment that evaluated the knowledge discovery capability of the masson system 
one person s noise is another person s signal for many application including the detection of credit card fraud and the monitoring of criminal activity in electronic commerce an important knowledge discovery problem is the detection of exceptional outlying event in computational statistic a depth based approach detects outlying data point in a d dataset by based on some definition of depth organizing the data point in layer with the expectation that shallow layer are more likely to contain outlying point than are the deep layer one robust notion of depth called depth contour wa introduced by tukey isodepth developed by rut and rousseeuw is an algorithm that computes d depth contour in this paper we give a fast algorithm fdc which computes the first k d depth contour by restricting the computation to a small selected subset of data point instead of examining all data point consequently fdc scale up much better than isodepth also while isodepth relies on the non existence of collinear point fdc is robust against collinear point 
the paper is the acceptance address for the acm sigir gerard salton award for excellence in research in the preamble the approach of dealing with the broader context of information science when considering information retrieval ir is justified the first part contains personal reflection of the author related to the major event and issue that formed his professional life and research agenda the second and major part considers the broad aspect of information science a a field origin problem addressed area of study structure specialty paradigm split and education problem the third part discus the limit of information science in term of internal limit imposed by the activity in the field and external limit imposed by the very human nature of information processing and use throughout issue related to user and use are transposed a being of primary concern 
this paper discus issue related to bayesian network model learning for unbalanced binary classification task in general the primary focus of current research on bayesian network learning system e g k and it variant is on the creation of the bayesian network structure that fit the database best it turn out that when applied with a specific purpose in mind such a classification the performance of these network model may be very poor we demonstrate that bayesian network model should be created to meet the specific goal or purpose intended for the model 
a new view based approach to the representation and recognition of action is presented the basis of the representation is a temporal template a static vector image where the vector value at each point is a function of the motion property at the corresponding spatial location in an image sequence using aerobics exercise a a test domain we explore the representational power of a simple two component version of the template the first value is a binary value indicating the presence of motion and the second value is a function of the recency of motion in a sequence we then develop a recognition method which match these temporal template against stored instance of view of known action the method automatically performs temporal segmentation is invariant to linear change in speed and run in real time on a standard platform we recently incorporated this technique into the kidsroom an interactive narrative play space for child 
efficient edge detection algorithm such a canny s fail near curve singularity moreover the standard linking algorithm used on top of these detector often fail because of instability in the tracking process due to multiple response to the same edge and interference of nearby edge we propose a hierarchical approach to edge detection based on a graph stabilization method that allows bifurcation resolution in stage curve singularity are recovered at the last stage by using top down feedback to select the best curve connection 
the following investigates the use of single neuron learning algorithmsto improve the performance of text retrieval system thataccept natural language query a retrieval process is explainedthat transforms the natural language query into the query syntaxof a real retrieval system the initial query is expanded using statisticaland learning technique and is then used for document rankingand binary classification the result of experiment suggest thatkivinen and warmuth s 
in many data mining application we are given a set of training example and asked to construct a regression machine or a classifier that ha low prediction error or low error rate on new example respectively an important issue is speed especially when there are large amount of data we show how both classification and prediction error can be reduced by using boosting technique to implement committee machine in our implementation of committee using either classification tree or regression tree we show how we can trade off speed against either error rate or prediction error 
the problem considered in this paper is that of estimating the projective transformation between two image in situation where the image motion is large and featurematching is not aided by a proximity heuristic the overall algorithm designed is based on a multiresolution multihypothesis scheme and similarity between tracking and matching through multiple resolution level are exploited two major tool are developed in this paper i a bayesian framework for incorporating similarity measure of feature correspondence in regression to specify the different level of confidence in the correspondence and ii a bayesian version of ransac which is able to utilise prior estimate and matching probability the algorithm is tested on a number of real image with large image motion and promising result were obtained 
detection of the periodicity of amplitude modulation is a major step inthe determination of the pitch of a sound in this article we willpresent a silicon model that us synchronicity of spiking neuron toextract the fundamental frequency of a sound it is based on theobservation that the so called chopper in the mammalian cochlearnucleus synchronize well for certain rate of amplitude modulation depending on the cell s intrinsic chopping frequency our siliconmodel us three 
it is typical in edge detection application to examine a single scale or to consider some space of scale in the image without knowing which scale is appropriate for each location in the image however many image contain a wide variation in the distance to the scene point and thus object of the same size can appear at greatly differing scale in the image we present a method where the scale of the smoothing and edge detection is varied locally according to the distance to the scene point which we estimate through stereoscopy the edge that are detected are thus at the same scale in the world rather than at the same scale in the image this method ha been implemented efficiently by smoothing the image at a discrete set of scale and performing interpolation to estimate the response at the correct scale for each pixel the application of this technique to an ordnance recognition problem ha resulted in a considerable improvement in performance 
this paper present a multibaseline stereo technique specially suited to detecting obstacle a method is described for weakly calibrating a set of multibaseline stereo camera with high accuracy this method is then used to tailor the stereo search space to the special case where the world in front of the camera consists mainly of nearly horizontal planar surface the ground where we are interested in deviation from those planar surface obstacle the resulting disparity map are presented and compared to the output of a traditional stereo algorithm 
we present algorithm for coupling and training hidden markov model hmms to model interacting process and demonstrate their superiority to conventional hmms in a vision task classifying two handed action hmms are perhaps the most successful framework in perceptual computing for modeling and classifying dynamic behavior popular because they offer dynamic time warping a training algorithm and a clear bayesian semantics however the markovian framework make strong restrictive assumption about the system generating the signal that it is a single process having a small number of state and an extremely limited state memory the single process model is often inappropriate for vision and speech application resulting in low ceiling on model performance coupled hmms provide an efficient way to resolve many of these problem and offer superior training speed model likelihood and robustness to initial condition 
the algorithm described in this article is based on the ob algorithmby hassibi stork and wolff and the main disadvantageof ob is it high complexity ob need to calculate theinverse hessian to delete only one weight thus needing much timeto prune a big net a better algorithm should use this matrix toremove more than only one weight because calculating the inversehessian take the most time in the ob algorithm the algorithm called unit ob described in this 
human and animal study show that mammalian brain undergoesmassive synaptic pruning during childhood removing about half ofthe synapsis until puberty we have previously shown that maintainingnetwork memory performance while synapsis are deleted requires that synapsis are properly modified and pruned removingthe weaker synapsis we now show that neuronal regulation amechanism recently observed to maintain the average neuronal inputfield result in weight dependent synaptic 
this paper describes a data mining approach for extracting enriched data from scientific data archive such a nasa s earth observing system data and information system eosdis that are stored on slow access tertiary storage this enriched data ha significantly smaller volume than the original data yet preserve sufficient property of this data such that over time many different user can repeatedly mine it for different earth science phenomenon this enriched data capture daily trend and significant deviation from trend for each bin of gridded data from an equaldegree grid covering the earth s surface a feature of this 
prior work in automated scientific discovery ha been successful in finding pattern in data given that a reasonably small set of mostly relevant feature is specified the work described in this paper place data in the context of large body of background knowledge specifically data item are connected to multiple database of background knowledge represented a inheritance network the system ha made a practical impact on botanical toxicology research which required linking example of case of plant exposure to database of botanical geographical and climate background knowledge 
this paper pr esents a framework for detecting and tracking moving object in a sequence of image using a statistical approach where the inter frame difference is modeled by a mixture of two laplacian or gaussian distribution and an energy minimization based approach we reformulate the motion detection and tracking problem a a front propagation problem the euler lagrange equation of the designed energy functional is first derived and the flow minimizing the energy is then obtained following the work by caselles et al cks and malladi et al msv msv the contour to be detected and tracked are modeled a geodesic active contour evolving toward the minimum of the designed energy under the influence of internal and external image dependent force using the level set formulation scheme of osher and sethian o complex curve can be detected and tracked and topological change for the evolving curve are naturally managed to reduce the computational cost required by a direct implementation of the formulation scheme of osher and sethian o a new approach exploiting aspect from the classical narrow band a and fast marching set method is proposed and favorably compared to them in order to further reduce the cpu time a multi scale approach ha also been considered very promising experimental result are provided using real video sequence 
a new dynamic subdivision surface model is proposed for shape recovery from d data set the model inherits the attractive property of the catmull clark subdivision scheme and is set in a physic based modeling paradigm unlike other existing method our model doe not require a parameterized input mesh to recover shape of arbitrary topology allows direct manipulation of the limit surface via application of force and provides a fast robust and hierarchical approach to recover complex shape from d data with very few degree of freedom contr ol vertic e we provide an analytic formulation and introduce the physical quantity required to develop the dynamic subdivision surface model which can be deformed by applying force synthesized from the data our experiment demonstrate that this new dynamic model ha a promising future in shape recovery from volume and range data set 
this paper describes a small compact circuit the retino laminar rl circuit that capture the temporal and adaptation property both of the photoreceptor and of the laminar layer of the fly the rl circuit us only six transistor and two capacitor the circuit is operated in the subthreshold domain it ha a low dc gain and a high transient gain the adaptation time constant of the rl circuit can be controlled via an external bias it temporal filtering property change with the background intensity and with the signal to noise ratio the frequency response of the circuit show that in the frequency range of to hz the circuit response go from highpass filtering under high light level to lowpass filtering under low light level i e when the signal to noise ratio is low 
we have developed a computer vision system including both facial feature extraction and recognition that automatically discriminates among subtly different facial expression expression classification is based on facial action coding system facs action unit au and discrimination is performed using hidden markov model hmms three method are developed to extract facial expression information for automatic recognition the first method is facial feature point tracking using a coarse to fine pyramid method this method is sensitive to subtle feature motion and is capable of handling large displacement with sub pixel accuracy the second method is dense flow tracking together with principal component analysis pca where the entire facial motion information per frame is compressed to a low dimensional weight vector the third method is high gradient component i e furrow analysis in the spatiotemporal domain which exploit the transient variation associated with the facial expression upon extraction of the facial information non rigid facial expression is separated from the rigid head motion component and the face image are automatically aligned and normalized using an affine transformation this system also provides expression intensity estimation which ha significant effect on the actual meaning of the expression 
abstract published in neural information processing system nip mit press in press boosting is a general method for improving the performance of any learning algorithm that consistently generates classifier which need to perform only slightly better than random guessing a recently proposed and very promising boosting algorithm is adaboost it ha been applied with great success to several benchmark machine learning problem using rather simple learning algorithm and decision tree in this paper we use adaboost to improve the performance of neural network we compare training method based on sampling the training set and weighting the cost function our system achieves about error on a data base of online handwritten digit from more than writer adaptive boosting of a multi layer network achieved err or on the uci letter and error on the uci satellite data set 
we study the problem of learning to accurately rank a set of object by combining a given collection of ranking or preference function this problem of combining preference arises in several application such a that of combining the result of different search engine or the collaborativefiltering problem of ranking movie for a user based on the movie ranking provided by other user in this work we begin by presenting a formal framework for this general problem we then describe and analyze an efficient algorithm called rankboost for combining preference based on the boosting approach to machine learning we give theoretical result describing the algorithm s behavior both on the training data and on new test data not seen during training we also describe an efficient implementation of the algorithm for a particular restricted but common case we next discus two experiment we carried out to ass the performance of rankboost in the first experiment we used the algorithm to combine different web search strategy each of which is a query expansion for a given domain the second experiment is a collaborative filtering task for making movie recommendation 
a model registration system capable of tracking an object the model of which is known in an image sequence is presented it integrates tracking pose determination and updating of the visible feature the which handle various feature point line and free form curve in a very robust way and is able to give a correct estimate of the pose even when tracking errorsoccur the reliability of the system is shown on an augmented reality project 
a novel method for d head tracking in the presence of large head rotation and facial expression change is described tracking is formulated in term of color image registration in the texture map of a d surface model model appearance is recursively updated via image mosaicking in the texture map a the head orientation varies the resulting dynamic texture map provides a stabilized view of the face that can be used a input to many existing d technique for face recognition facial expression analysis lip reading and eye tracking parameter are estimated via a robust minimization procedure this provides robustness to occlusion wrinkle shadow and specular highlight the system wa tested on a variety of sequence taken with low quality uncalibrated video camera experimental result are reported 
this paper introduces an object based approach for temporal video partitioning and content based indexing where the basic indexing unit is lifespan of a video object rather than a camera shot or story unit we propose a system to extract content based feature of video object vos based on a compact d triangular mesh representation of them an adaptive mesh based video object tracking scheme is then employed to compute the motion trajectori of all node point a set of key snapshot which constitute a visual summary of the lifespan of the object are automatically selected using motion and shape information the system provides direct access to the vos and give the functionality such a object based search manipulation animation and tracking 
perspective camera calibration ha been in the last decade a research subject for a large group of researcher and a a result several camera calibration methodology can be foundin the literature however only a small number of those method base their approach on the use of monoplane calibration point to realize an explicit d camera calibration to avoid the singularity obtained with the calibration equation when monoplane calibration point are used this method computes the calibration parameter in a multi step procedure and requires a first guess solution for the intrinsic parameter these parameter are updated and their accuracy increased through an iterative procedure a stability analysis a a function of the pose of the camera is presented camera pose view strategy for accurate camera orientation computation can be extracted from the pose view stability analysis 
we describe an approach to the classification of d object using a multi scale representation this approach start with a smoothing algorithm for representing object at different scale smoothing is applied in curvature space directly thus avoiding the usual shrinkage problem and allowing for efficient implementation a d similarity measure that integrates the representation of the object at multiple scale is introduced given a library of model object that are similar based on this multi scale measure are grouped together into class the object that are in the same class are combined into a single prototype object finally the prototype are used for hierarchical recognition by first comparing the scene representation to the prototype and then matching it only to the object in the most likely class rather than to the entire library of model beyond it application to object recognition this approach provides an attractive implementation of the intuitive notion of scale and approximate similarity for d shape 
conventional video camera have limited field of view which make them restrictive for certain application in computational vision a catadioptric sensor us a combination of lens and mirror placed in a carefully arranged configuration to capture a much wider field of view when designing a catadioptric sensor the shape of the mirror s should ideally be selected to ensure that the complete catadioptric system ha a single effective viewpoint in this paper we derive the complete class of single lens single mirror catadioptric sensor which have a single viewpoint and an expression for the spatial resolution of a catadioptric sensor in term of the resolution of the camera used to construct it we also include a preliminary analysis of thedefocus blur caused by the use of a curved mirror 
i present a theory of mean field approximation based on information geometry this theory includes in a consistent way the naive mean field approximation a well a the tap approach and the linear response theorem in statistical physic giving clear information theoretic interpretation to them 
many data mining task e g association rule sequential pattern use complex pointer based data structure e g hash tree that typically suffer from sub optimal data locality in the multiprocessor case shared access to these data structure may also result in false sharing for these task it is commonly observed that the recursive data structure is built once and accessed multiple time during each iteration furthermore the access pattern after the build phase are highly ordered in such case locality and false sharing sensitive memory placement of these structure can enhance performance significantly we evaluate a set of placement policy for parallel association discovery and show that simple placement scheme can improve execution time by more than a factor of two more complex scheme yield additional gain our experiment show that simple placement scheme can be quite effective and for the datasets we looked at improve the execution time by a factor of two more complex scheme yield additional gain these result are directly applicable to other mining task like quantitative association srikant multi level taxonomy association srikant and sequential pattern agrawal which also use hash tree based structure most of the current work in parallel association mining ha only focused on distributed memory machine park agrawal cheung shintani han zaki where false sharing doesn t arise however our locality optimization are equally applicable to these method a detailed version of this paper appears in parthasarathy 
human speech is inherently multi modal consisting of both audio and visual component recently researcher have shown that the incorporation of information about the position of the lip into acoustic speech recognisers enables robust recognition of noisy speech in the case of hidden markov model recognition we show that his happens because the visual signal stabilises the alignment of state it is also shown that unadorned lip both the inner and outer contour can be robustly tracked in real time on general purpose workstation to accomplish this efficient algorithm are employed which contain three key component shape model motion model and focused colour feature detector all of which are learnt from example 
nonmonotonic reasoning is virtually absent from industry and ha been so since it incep tion the result is that the field is becoming marginalized within ai i argue that this is be cause researcher in the area focus exclusively on commonsense problem which are irrelevant to industry and because few efficient algorithm and or tool have been developed a sensible strategy is thus to focus on industry problem and to develop solution within tractable subtheories of nonmonotonic logic i examine one of the few example of nonmonotonic reason ing in industry inheritance of business rule in the medical insurance domain and show how the paradigm of inheritance with excep tions can be extended to a broader and more powerful kind of nonmonotonic reasoning fi nally i discus the underlying lesson that can be generalized to other industry problem 
i present an expectation maximization em algorithm for principal component analysis pca the algorithm allows a few eigenvectors and eigenvalue to be extracted from large collection of high dimensional data it is computationally ecient in space and time and doe not require computing the sample covariance of the data it also naturally accommodates missing information i introduce a new variation of pca known a sensible principal component analysis spca which defines a proper density 
iterative refinement clustering algorithm e g k mean em converge to one of numerous local minimum it is known that they are especially sensitive to initial condition we present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the mode of a distribution the refined initial starting condition lead to convergence to better local minimum the procedure is applicable to a wide class of clustering algorithm for both discrete and continuous data we demonstrate the application of this method to the expectation maximization em clustering algorithm and show that refined initial point indeed lead to improved solution refinement run time is considerably lower than the time required to cluster the full database the method is scalable and can be coupled with a scalable clustering algorithm to address the large scale clustering in data mining 
previous research on nonlinear shape restoration are based on the assumption that the original shape and distortion of the image have known formulation under certain condition the main contribution of this research is the development of a new restoration algorithm called multi step restoration the algorithm is based on a liner interpolation theory that is able to detect and restore nonlinear shape distortion in any irregular quadrilateral shaped pattern the main idea of the algorithm is the use of two dimensional spline function in bicubic biquadratic and or bilinear model to approximate the three dimensional nonlinear distortion curve the performance of the approach shown by experiment is promising 
the rocchio relevance feedback algorithm is one of the most popular and widely applied learning method from information retrieval here a probabilistic analysis of this algorithm is presented in a text categorization framework the analysis give theoretical insight into the heuristic used in the rocchio algorithm particularly the word weighting scheme and the similarity metric it also suggests improvement which lead to a probabilistic variant of the rocchio classifier the rocchio classifier it probabilistic variant and a naive bayes classifier are compared on six text categorization task the result show that the probabilistic algorithm are preferable to the heuristic rocchio classifier not only because they are more well founded but also because they achieve better performance 
we prove that the canonical distortion measure cdm is the optimal distance measure to use for nearest neighbour nn classification and show that it reduces to squared euclidean distance in feature space for function class that can be expressed a linear co mbinations of a fixed set of feature pac like bound are given on the sam plecomplexity required to learn the cdm an experiment is presented in which a neural network cdm wa learnt for a japanese ocr environment and then used to do nn classification 
an important issue in data mining is the recognition of complex dependency between attribute past technique for identifying attribute dependence include correlation coefficient scatterplots and equiwidth histogram these technique are sensitive to outlier and often are not sufficiently informative to identify the kind of attribute dependence present we propose a new approach which we call independence diagram we divide each attribute into range for each pair of attribute the combination of these range defines a two dimensional grid for each cell of this grid we store the number of data item in it we display the grid scaling each attribute axis so that the displayed width of a range is proportional to the total number of data item within that range the brightness of a cell is proportional to the density of data item in it a a result both attribute are independently normalized by frequency ensuring insensitivity to outlier and skew and allowing specific focus on attribute dependency furthermore independence diagram provide quantitative measure of the interaction between two attribute and allow formal reasoning about issue such a statistical significance 
recently there have been increasing interest in using nonlinear pdes for application in computer vision and image processing in this paper we propose a general statistical framework for designing a new class of pdes for a given application a markov random field model p i is learned according to the minimax entropy principle studied in so that p i should characterize the ensemble of image in our application p i is a gibbs distribution whose energy term can be divided into two category subsequently the partial differential equation given by gradient descent on the gibbs potential are essentially reaction diffusion equation where the energy term in one category produce anisotropic diffusion while the inverted energy term in the second category produce reaction associated with pattern formation we call this new class of pdes the gibbs reaction and diffusion equation grade and we demonstrate experiment where grade are used for texture pattern formation denoising image enhancement and clutter removal 
a recent trend in motion based segmentation ha been to rely on statistical procedure derived from expectation maximization em principle em based approach have various attractives for segmentation such a proceeding by taking non greedy soft decision with regard to the assignment of pixel to region or allowing the use of sophisticated prior capable of imposing spatial coherence on the segmentation a practical difficulty with such prior is however the determination of appropriate value for their parameter in this work we exploit the fact that the em framework is itself suited for empirical bayesian data analysis to develop an algorithm that find the estimate of the prior parameter which best explain the observed data such an approach maintains the bayesian appeal of incorporating prior belief but requires only a qualitative description of the prior avoiding the requirement of a quantitative specification of it parameter this eliminates the need for trial and error strategy for parameter determination and lead to better segmentation in le iteration 
recent work by kautz et al provides tantalizing evidence that large classical planning problem may be efficiently solved by translating them into propositional satisfiability problem using stochastic search technique and translating the resulting truth assignment back into plan for the original problem we explore the space of such transformation providing a simple framework that generates eight major encoding generated by selecting one of four action representation and one of two frame axiom and a number of subsidiary one we describe a fully implemented compiler that can generate each of these encoding and we test the compiler on a suite of strip planning problem in order to determine which encoding have the best property 
the increasing stream of electronic informationavailable make it ever more time consuming to findinteresting information information filtering systemsthat use machine learning technique hold thepromise of relieving user of this burden by learninga model of the user s interest and using this modelto find interesting document most of these system however suffer from the problem that usersmust use the system for a long time before a usefulmodel of their interest can be learned 
here we derive measure quantifying the information loss of a synaptic signal due to the presence of neuronalnoise source a it electrotonically propagates along a weakly active dendrite we model the dendrite a an infinite linear cable with noise source distributed along it length the noise source we consider are thermal noise channel noise arising from the stochastic nature of voltage dependent ionic channel k and na andsynapticnoise duetospontaneousbackgroundactivity we ass the efficacy of information transfer using a signal detection paradigm where the objectiveis to detectthe presence absenceof a presynapticspike from thepost synapticmembranevoltage thisallows u to analyticallyassess the role of each of these noise source in information transfer for our choice of parameter we find that the synaptic noise is the dominant noise source which limit the maximum length over which information be reliably transmitted 
abstract initial experiment described here are directed toward using reinforcement learning rl to develop an automatic recovery system ar for high agility aircraft an ar is an outer loop ight control system designed to bring an aircraft from a range of initial state to straight level and non inverted ight in minimum time and while satisfying given constraint here we report on result for a simple version of of the problem involving only single axis pitch simulated recovery through simulated control experience using a medium delity aircraft simulation the rl system approximated an optimal policy for longitudinal stick input to produce minimum time transition to straight and level ight in unconstrained case a well a while meeting a pilot station acceleration constraint 
the development of natural language processing nlp system that perform machine translation mt and information retrieval ir ha highlighted the need for the automatic recognition of proper name while various name recognizers have been developed they suffer from being too limited some only recognize one name class and all are language specific this work develops an approach to multilingual name recognition that us machine learning and a portable framework to simplify the porting task by maximizing reuse and automation 
we have designed and implemented a system for real time detection of d feature on a reconfigurable computer based on field programmable gate array fpga s we envision this device a the front end of a system able to track image feature in real time control application like autonomous vehicle navigation the algorithm employed to select good feature is inspired by tomasi and kanade s method compared to the original method the algorithm that we have devised doe not require any floating point or transcendental operation and can be implemented either in hardware or in software moreover it map efficiently into a highly pipelined architecture well suited to implementation in fpga technology we have implemented the algorithm on a low cost reconfigurable computer and have observed reliable operation on an image stream generated by a standard ntsc video camera at hz 
reinforcement learning is the process by which an autonomous agent us it experience interacting with an environment to improve it behavior the markov decision process mdp model is a popular way of formalizing the reinforcement learning problem but it is by no mean the only way in this paper we show how many of the important theoretical result concerning reinforcement learning in mdps extend to a generalized mdp model that includes mdps two player game and mdps under a worst case optimality criterion a special case the basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous convergence keywords reinforcement learning q learning convergence markov game 
visually guided arm reaching movement are produced by distributed neural network within parietal and frontal region of the cerebral cortex experimental data indicate that single neuron in these region are broadly tuned to parameter of movement appropriate command are elaborated by population of neuron the coordinated action of neuron can be visualized using a neuronal population vector npv however the npv provides only a rough estimate of movement parameter direction velocity and may even fail to reflect the parameter of movement when arm posture is changed we designed a model of the cortical motor command to investigate the relation between the desired direction of the movement the actual direction of movement and the direction of the npv in motor cortex the model is a two layer self organizing neural network which combine broadly tuned muscular proprioceptive and cartesian visual information to calculate angular motor command for the initial part of the movement of a two link arm the network wa trained by motor babbling in position simulation showed that the network produced appropriate movement direction over a large part of the workspace small deviation of the actual trajectory from the desired trajectory existed at the extremity of the workspace these deviation were accompanied by large deviation of the npv from both trajectory these result suggest the npv doe not give a faithful image of cortical processing during arm reaching movement 
this paper describes a vectorial representation that can be used to ass the symmetry of object in d image the method exploit a magneto static analogy commencing from the gradient field extracted from filtered grey scale image we construct a vector potential our magneto static analogy is that tangential gradient vector represent the element of a current distribution on the image plane by embedding the image plane in an augmented dimensional space we compute the vector potential by performing volume integration over the current distribution the associated magnetic field is computed by taking the curl of the vector potential the auxiliary spatial dimension provides a natural scale space sampling of the generating current distribution a the height above the image plane is increased so the volume over which averaging is effected also increase we extract edge and symmetry line through a topographic analysis of the vector field at various height above the image plane symmetry ax are line of where the curl of the vector potential vanishes at edge the divergence of the vector potential vanishes 
planning and learning at multiple level of temporal abstra ction is a key problem for artificial intelligence in this paper we summar ize an approach to this problem based on the mathematical framework of markov decision process and reinforcement learning current mo del based reinforcement learning is based on one step model that cannot represent common sense higher level action such a going to lunch grasping an object or flying to denver this paper generalizes prior wor k on temporally abstract model sutton and extends it from t he prediction setting to include action control and planning we intro duce a more general form of temporally abstract model the multi time model and establish it suitability for planning and learning by virtue of it relationship to the bellman equation this paper summarizes the theoretical framework of multi time model and illustrates their potential a dvantages in a gridworld planning task a new approach to modeling at multiple time scale wa introduced by sutton based on prior work by singh dayan and sutton and pinette this approach enables model of the environment at different temporal scale to be interm ixed producing temporally abstract model however that work wa concerned only with predicting the environment this paper summarizes an extension of the approach including action and control of the environment precup sutton in particular we generalize the usual notion of a 
a one dimensional model of primate smooth pursuit mechanism hasbeen implemented in m cmos vlsi the scheme consolidatesrobinson s negative feedback model with wyatt and pola s positivefeedback scheme to produce a smooth pursuit system which zero thevelocity of a target on the retina furthermore the system us thecurrent eye motion a a predictor for future target motion analysis stability and biological correspondence of the system are discussed forimplementation at the focal 
dimension reducing feature extraction neural network technique which also preserve neighbourhood relationship in data have traditionally been the exclusive domain of kohonen self organising map recently we introduced a novel dimension reducing feature extraction process which is also topographic based upon a radial basis function architecture it ha been observed that the generalisation performance of the system is broadly insensitive to model order complexity and other smoothing 
entity identification ei is the identification and integration of all record which represent the same realworld entity and is an important task in database integration process when a common identification mechanism for similar record across heterogeneous database is not readily available ei is performed by examining the relationship between various attribute value among the record we propose the use of distance between attribute value a a measure of similarity between the record they represent recordmatching condition for ei can then be expressed a constraint on the attribute distance we show how knowledge discovery technique can be used to automatically derive these condition expressed a decision tree directly from the data using a distancebased framework 
we apply information maximization maximum likelihood blindsource separation to complex valued signal mixed with complexvalued nonstationary matrix this case arises in radio communicationswith baseband signal we incorporate known sourcesignal distribution in the adaptation thus making the algorithmsless quot blind quot this result in drastic reduction of the amount of dataneeded for successful convergence adaptation to rapidly changingsignal mixing condition such a to 
we present a bias variance decompositionof expected misclassification rate the mostcommonly used loss function in supervisedclassification learning the bias variancedecomposition for quadratic loss functionsis well known and serf a an importanttool for analyzing learning algorithm yetno decomposition wa offered for the morecommonly used zero one misclassification loss function until the recent work of kong amp dietterich and breiman their decomposition suffers 
distributed data mining system aim to discover and combine usefull information that is distributed across multiple database the jam system for example applies machine learning algorithm to compute model over distributed data set and employ meta learning technique to combine the multiple model occasionally however these model or classiers are induced from database that have moderately dieren t schema and hence are incompatible in this paper we investigate the problem of combining multiple model computed over distributed data set with dieren t schema through experiment performed on actual credit card data provided by two dieren t nancial institution we evaluate the eectiv ene of the proposed approach and demonstrate their potential utility 
we present the notion of ranking for evaluation of two class classifier ranking is based on using the ordering information contained in the output of a scoring model rather than just setting a classification threshold using this ordering information we can evaluate the model s performance with regard to complex goal function such a the cor rect identification of the k most likely and or least likely to be responder out of a group of potential customer using ranking we can also obtain increased efficiency in comparing classifier and selecting the better one even for the standard goal of achieving a minimal misclassification rate this feature of ranking is illustrated by simulation result we also discus it theoretically showing the similarity in structure between the reducible model dependent part of the linear ranking score and the standard misclassification rate score and characterizing the situation when we expect linear ranking to outperform misclassification rate a a method for model discrimination 
lateral competition within a layer of neuron sharpens and l ocalizes the response to an input stimulus here we investigate a model for the activity dependent development of ocular dominance map which allows to vary the degree of lateral competition for weak competit ion it resembles a correlation based learning model and for strong competition it becomes a self organizing map thus in the regime of weak competition the receptive field are shaped by the second order sta tistics of the input pattern whereas in the regime of strong competition the higher moment and feature of the individual pattern become important when correlated localized stimulus from two eye drive the cortical development we find that a topographic map and binocular localized receptive field emerge when the degree of competition excee d a critical value and that receptive field exhibit eye dominance beyond a second critical value for anti correlated activity between t he eye the second order statistic drive the system to develop ocular domi nance even for weak competition but no topography emerges topography is established only beyond a critical degree of competition 
a machine learning ha graduated fromtoy problem to quot real world quot application user are finding that quot real world quot problemsrequire them to perform aspect of problemsolving that are not currently addressedby much of the machine learning literature specifically user are finding that the task ofselecting a set of feature to define a problemand obtaining a set of example of the problemare often more important for a successfulmachine learning application than the 
in the developing nervous system gradient of target derived diffusible factor play an important role in guiding axon to appropriate target in this paper the shape that such a gradient might have is calculated a a function of distance from the target and the time since the start of factor production using estimate of the relevant parameter value from the experimental literature the spatiotemporal domain in which a growth cone could detect such a gradient is derived for large time a value for the maximum guidance range of about mm is obtained this value t well with experimental data for smaller time the analysis predicts that guidance over longer range may be possible this prediction remains to be tested 
a method for the temporal classification of natural gesture from video imagery is presented the work is motivated by recent development in the theory of natural gesture which have identified several key temporal aspect of gesture important to communication in particular gesticulation during conversation can be coarsely characterized a period of bi phasic or tri phasic gesture separated by a rest state we first present an automatic procedure for hypothesizing plausible rest state configuration of a speaker second we develop a state based parsing algorithm used to both select among candidate rest state and to parse an incoming video stream into bi phasic and tri phasic gesture finally we demonstrate the use of the bi phasic tri phasic labeling to select semantically significant static image for low bandwidth coding of video of story telling speaker 
gamut mapping colour constancy attempt to determine the set of diagonal matrix taking the gamut of image colour under an unknown illuminantion to the gamut of colour observed under a standard illuminant forsyth developed such an algorithm in rgb sensor space which finlayson later modified to work in a d chromaticity space in this paper we prove that forsyth s d solution gamut is when projected to d identical to the gamut recovered by the d algorithm whilst this implies that there is no inherent disadvantage in working in chromaticity space this algorithm ha a number of problem the d solution set is distorted and contains practically non feasible illuminant these problem have been addressed separately in previous work we address them together in this paper non feasible illuminant are discarded by intersecting the solution gamut with a non convex gamut of common illuminant in d this intersection is relatively simple but to remove the distortion both these set should be represented a d cone of mapping and the intersection is more difficult we present an algorithm which avoids performing this intersection explicitly and which is simple to implement test of this algorithm on both real and synthetic image show that it performs significantly better than the best current algorithm 
in this paper we describe a method for automatically retrieving collocation from large text corpus this method retrieve collocation in the following stage extracting string of character a unit of collocation extracting recurrent combination of string in accordance with their word order in a corpus a collocation through the method various range of collocation especially domain specific collocation are retrieved the method is practical because it us plain text without any information dependent on a language such a lexical knowledge and part of speech 
in data mining similarity or distance between attribute is one of the central notion such a notion can be used to build attribute hierarchy etc similarity metric can be user defined but an important problem is defining similarity on the basis of data several method based on statistical technique exist for defining the similarity between two attribute a and b they typically consider only the value of a and b not the other attribute we describe how a similarity notion between attribute can be defined by considering the value of other attribute the basic idea is that in a relation r two attribute a and b are similar if the subrelations aa l r and ab r are similar similarity between the two relation is defined by considering the marginal frequency of a selected subset of other attribute we show that the framework produce natural notion of similarity empirical result on the reuters document dataset show for example how natural classification for country can be discovered from keyword distribution in document the similarity notion is easily computable with scalable algorithm 
dual route and connectionist single route model of reading havebeen at odds over claim a to the correct explanation of the readingprocess recent dual route model predict that subject shouldshow an increased naming latency for irregular word when the irregularityis earlier in the word e g chef is slower than glow aprediction that ha been confirmed in human experiment sincethis would appear to be an effect of the left to right reading process coltheart amp rastle claim 
this paper proposes an expansion of set of primitiveconstraints available within the primitiveoptimality theory framework eisner a this expansion consists of the addition of anew family of constraint existential implicationalconstraints which allow the specificationof faithfulness constraint that can be satisfiedat a distance and the definition of twoways to combine simple constraint into complexconstraints that is constraint disjunction crowhurst and hewitt 
we represent local spatial structure in a color image using feature matrix that are computed from an image region feature matrix contain significantly more information about local image structure than previous representation although feature matrix are useful for surface recognition this representation depends on the spectral property of the scene illumination using a finite dimensional linear model for surface spectral reflectance with the same number of parameter a the number of color band we show that illumination change correspond to linear transformation of the feature matrix and that surface rotation correspond to circular shift of the matrix from these relationship we derive an algorithm for illumination and geometry invariant recognition of local surface structure we demonstrate the algorithm with a series of experiment on image of real object 
this paper present a negative result current machine colour constancy algorithm are not good enough for colour based object recognition this result ha surprised u since we have previously used the better of these algorithm successfully to correct the colour balance of image for display colour balancing ha been the typical application of colour constancy rarely ha it been actually put to use in a computer vision system so our goal wa to show how well the various method would do on an obvious machine colour vision task namely object recognition although all the colour constancy method we tested proved insufficient for the task we consider this an important finding in itself in addition we present result showing the correlation between colour constancy performance and object recognition performance and a one might expect the better the colour constancy the better the recognition rate 
this paper introduces the rl top architecturefor robot learning a hybrid systemcombining teleo reactive planning and reinforcementlearning technique the aim ofthis system is to speed up learning by decomposingcomplex task into hierarchy ofsimple behaviour which can be learnt moreeasily behaviour learnt in this way cansubsequently be re used to solve a variety ofproblems reducing the need to learn everynew task from scratch it is even possibleto learn multiple 
in this paper a method to extract curvilinear structure from digital image is presented the approach is based on differential geometric property of the image function for each pixel the second order taylor polynomial is computed by convolving the image with the derivative of a gaussian smoothing kernel line point are required to have a vanishing gradient and a high curvature in the direction perpendicular to the line the use of the taylor polynomial and the gaussian kernel lead to a single response of the filter to each line furthermore the line position can be determined with sub pixel accuracy finally the algorithm scale to line of arbitrary width an analysis about the scale space behaviour of two typical line type parabolic and bar shaped is given from this analysis requirement and useful value for the parameter of the filter can be derived additionally an algorithm to link the individual line point into line and junction that preserve the maximum number of line point is given example on aerial image of different resolution illustrate the versatility of the presented approach 
we propose a motion segmentation algorithm that aim to break a scene into it most prominent moving group instead of identifying point correspondence between the image frame the idea to find what group of pixel are transformed from one image frame to another to do this we treat the image sequence a a three dimensional spatiotemporal data set and construct a weighted graph by taking each pixel a a node and connecting pixel that are in the spatiotemporal neighborhood of each other we define a motion profile vector at each image pixel which capture the probability distribution of the image velocity at that point by defining a distance between motion profile at two pixel we can assign a weight on the graph edge connecting them using normalized cut we find the most salient partition of the spatiotemporal volume formed by the image sequence each partition which is in the form of a spatiotemporal volume corresponds to a group of pixel moving coherently in space and time normalized cut can be computed efficiently by solving a generalized eignevalue problem 
an adaptive on line algorithm extending the learning of learningidea is proposed and theoretically motivated relying only on gradientflow information it can be applied to learning continuousfunctions or distribution even when no explicit loss function is givenand the hessian is not available it efficiency is demonstratedfor a non stationary blind separation task of acoustic signal introductionneural network provide powerful tool to capture the structure in data by 
verbal and compositional lexical aspect provide the underlying temporal structure of event knowledge of lexical aspect e g a telicity is therefore required for interpreting event sequence in discourse dowty moens and steedman passoneau interfacing to temporal database androutsopoulos processing temporal modifier antonisse describing allowable alternation and their semantic effect resnik tenny and selecting tense and lexical item for natural language generation dorr and olsen klavans and chodorow cf slobin and bocaz we show that it is possible to represent lexical aspect both verbal and compositional on a large scale using lexical conceptual structure lcs representation of verb in the class cataloged by levin we show how proper consideration of these universal piece of verb meaning may be used to refine lexical representation and derive a range of meaning from combination of lcs representation a single algorithm may therefore be used to determine lexical aspect class and feature at both verbal and sentence level finally we illustrate how knowledge of lexical aspect facilitates the interpretation of event in nlp application 
structured attribute have domain value set that are partially ordered set typically hierarchy such attribute allow knowledge discovery program t o incorporate background knowledge about hierarchical relationship among attribute value inductive generalization rule for structured attribute have been developed that take into consideration the type of node in the domain hierarchy anchor or non anchor and the type of decision rule to be generated characteristic discriminant or minimum complexity these generalization rule enhance the ability of knowledge discovery system inlen to exploit the semantic content of the domain knowledge in the process of generating hypothesis if the dependent attribute e g a decision attribute is structured the system generates a system of hierarchically organized rule representing relationship between the value of this attribute and independent attribute such a situation often occurs i n practice when the decision to be assigned to a situation can be at different level of abstraction e g this is a liver disease or this is a liver cancer continuous attribute e g physical measurement are quantized into a hierarchy of value range of value arranged into different level these method are illustrated by an example concerning the discovery of pattern in world economics and demographic knowledge that relates the numerical age with the higher level concept in general the structure of the domain doe not have to be fixed it may be changing with the context of the problem at hand structuring attribute can prove advantageous for a knowledge discovery system it allows fact trend and regularity to be revealed both at high and low level of abstraction and for background knowledge to be stored and generalization to be made at the appropriate level 
a real time system is described for automatically detecting modeling and tracking face in d a closed loop approach is proposed which utilizes structure from motion to generate a d model of a face and then feed back the estimated structure to constrain feature tracking in the next frame the system initializes by using skin classification symmetry operation d warping and eigenfaces to find a face feature trajectory are then computed by ssd or correlation based tracking the trajectory are simultaneously processed by an extended kalman filter to stably recover d structure camera geometry and facial pose adaptively weighted estimation is used in this filter by modeling the noise characteristic of the d image patch tracking technique in addition the structural estimate is constrained by using parametrized model of facial structure eigen head the kalman filter s estimate of the d state and motion of the face predicts the trajectory of the feature which constrains the search space for the next frame in the video sequence the feature tracking and kalman filtering closed loop system operates at hz 
a real time system is described for automatically detecting modeling and tracking face in d a closed loop approach is proposed which utilizes structure from motion to generate a d model of a face and then feed back the estimated structure to constrain feature track ing in the next frame the system initializes by u ing skin classi cation symmetry operation d warp ing and eigenfaces to nd a face feature trajectory are then computed by ssd or correlation based track ing the trajectory are simultaneously processed by an extended kalman lter to stably recover d struc ture camera geometry and facial pose adaptively weighted estimation is used in this lter by modeling the noise characteristic of the d image patch tracking technique in addition the structural estimate is con strained by using parametrized model of facial struc ture eigen head the kalman lter s estimate of the d state and motion of the face predicts the trajectory of the feature which constrains the search space for the next frame in the video sequence the feature track ing and kalman ltering closed loop system operates at hz 
in the poisson neuron model the output is a rate modulated poissonprocess snyder and miller the time varying rate parameter r t is an instantaneous function g of the stimulus r t g s t in a poisson neuron then r t give the instantaneousfiring rate the instantaneous probability of firing at anyinstant t and the output is a stochastic function of the input inpart because of it great simplicity this model is widely used usuallywith the addition of a 
a said in signal processing one person s noise is another person s signal for many application such a the exploration of satellite or medical image and the monitoring of criminal activity in electronic commerce identifying exception can often lead to the discovery of truly unexpected knowledge in this paper we study an intuitive notion of outlier a key contribution of this paper is to show how the proposed notion of outlier unifies or generalizes many ex 
abstract given a set of object in the visual field how doe the the visual system learn to attend to a particular object of interest while ignoring the rest how are occlusion and background clutter so effortlessly discounted for when recognizing a familiar object in this paper we attempt to answer these question in the context of a kalman filter based model of visual recognition that ha previously been useful in explaining certain neurophysio logical phenomenon such a endstopping and related extra classical receptive field effect in the visual cortex by using result from the field of rob ust statistic we describe an extension of the kalman filter model that can handle multiple object in the visual field the resulting robust kalman filter model demonstrates how certain form of attention can be viewed a an emergent property of the interaction between top down expectation and bottom up signal the model also suggests functional interpretatio n of certain attention related effect that have been observed in visual cortical neuron experimental result are provided to help demonstrate the abil ity of the model to perform robust segmentation and recognition of object and image sequence in the presence of varying degree of occlusion and clutter 
we report here that change in the normalized electroencephalographic eeg cross spectrum can be used in conjunction withfeedforward neural network to monitor change in alertness of operatorscontinuously and in near real time previously we haveshown that eeg spectral amplitude covary with change in alertnessas indexed by change in behavioral error rate on an auditorydetection task here we report for the first time that increasesin the frequency of detection error 
combining multiple classi er is an e ective technique for improving accuracy there are many general combining algorithm such a bagging or error correcting output coding that signi cantly improve classi er like deci sion tree rule learner or neural network unfortunately many combining method do not improve the nearest neighbor classi er in this paper we present mf a combining algorithm designed to improve the accuracy of the nearest neighbor nn classi er mf combine multiple nn classi er each using only a random subset of feature the ex perimental result are encouraging on datasets from the uci repository mf sig ni cantly improved upon the nn k near est neighbor knn and nn classi er with forward and backward selection of feature mf wa also robust to corruption by irrele vant feature compared to the knn classi er finally we show that mf is able to reduce both bias and variance component of error 
reinforcement learning method for discrete and semi markov decisionproblems such a real time dynamic programming canbe generalized for controlled diffusion process the optimalcontrol problem reduces to a boundary value problem for a fullynonlinear second order elliptic differential equation of hamiltonjacobi bellman hjb type numerical analysis provides multigridmethods for this kind of equation in the case of learning control however the system of equation on the various 
we present a novel method for the shape and motion estimation of a deformable model using error residual from model based motion analysis the motion of the model is first estimated using a model based least square method using the residual from the least square solution the non rigid structure of the model can be better estimated by computing how change in the shape of the model affect it motion parameterization this method is implemented a a component in a deformable model based framework that us optical flow information and edge this general model based framework is applied to human face shape and motion estimation we present experiment that demonstrate that this framework is a considerable improvement over a framework that us only optical flow information and edge 
the recognition of human gesture and facial expression in imagesequences isan important and challenging problem that enables a hostof human computer interaction application thispaperdescribes a frameworkforincrementalrecognitionofhumanmotionthatextendsthe condensation algorithm proposed by isard and blake eccv human motion are modeled astemporal trajectoriesof some estimated parameter over time the condensationalgorithm us random sampling technique to incrementally match the trajectory model to the multi variate input data the recognition framework is demonstrated withtwoexamples therstexampleinvolvesanaugmentedocewhiteboardwithwhichausercanmakesimplehandgesturestograbregionsof theboard printthem savethem etc thesecondexampleillustratesthe recognition of human facial expression using the estimated parameter of a learned model of mouth motion 
a map mrf based scheme is proposed for recovering the depth and the focused image of a scene from two defocused image the space variant blur parameter and the focused image of the scene are both modeled a mrfs and their map estimate are obtained using simulated annealing the performance of the proposed scheme is tested on both synthetic a well a real data and the estimate of the depth are found to be better than that of the existing window based technique 
in this paper an adaptive split and merge segmentation method is proposed the splitting phase of the algorithm employ the incremental delaunay triangulation competent of forming grid edge of arbitrary orientation and position the tessellation grid defined by the delaunay triangulation is adjusted to the semantics of the image data by combining similarity and difference information among pixel experimental result on synthetic image show that the method is robust to different object edge orientation partially weak object edge and very noisy homogeneous region experiment on a real image indicate that the method yield good segmentation result even when there is a quadratic sloping of intensity particularly suited for segmenting natural scene of man made object 
the unsupervised detection of hierarchical structuresis a major topic in unsupervised learning and one ofthe key question in data analysis and representation we propose a novel algorithm for the problem of learningdecision tree for data clustering and related problem in contrast to many other method based onsuccessive tree growing and pruning we propose anobjective function for tree evaluation and we derive anon greedy technique for tree growing applying theprinciples of maximum 
there is considerable interest in the computer vision community in representing and modelling motion motion model are use d a predictor to increase the robustness and accuracy of visual tracker and a classifier for gesture recognition this paper present a significant development of random sampling method to allow automatic switching between multiple motion model a a natural extension of the tracking process the bayesian mixed state framework is described in it generality and the example of a bouncing ball is used to demonstrate that a mixed state model can significantly improve tracking performance in heavy clutter the relevance of the approach to the problem of gesture recognition is then investigated using a tracker which is able to follow the natural drawing action of a hand holding a pen and switch state according to the hand s motion 
three different statistical model of colour data for use in segmentation or tracking algorithm are proposed result of a performance comparison of a tracking algorithm applied to two separate application using each of the three different type of underlying model of the data are presented from these a comparison of the performance of the statistical colour model themselves is obtained 
we study model feed forward network a time series predictorsin the stationary limit the focus is on complex yet non chaotic behavior the main question we address is whether the asymptoticbehavior is governed by the architecture regardless the detail ofthe weight we find hierarchy among class of architectureswith respect to the attractor dimension of the long term sequencethey are capable of generating larger number of hidden unit cangenerate higher dimensional attractor 
previous resolution based approach totheory guided induction of logic programsproduce hypothesis in the form of a set of resolventsof a theory where the resolvent representallowed sequence of resolution stepsfor the initial theory there are however many characterization of allowed sequencesof resolution step that cannot be expressedby a set of resolvent one approach tothis problem is presented the system merlin which is based on an earlier techniquefor learning 
let s be a set of six point in space let psi be any hyperboloid of one sheet containing s and let i be a sequence of image of s taken by an uncalibrated camera moving over psi then reconstruction from i is subject to a three way ambiguity which is unbroken a long a the optical centre of the camera remains on psi let p be an image of s taken from a point on psi the image near p define a tangent space which split into a direct sum w p oplus n p oplus f p where wp corresponds to image near p for which the ambiguity is maintained np corresponds to image for which the ambiguity is broken and fp corresponds to image which are physically impossible 
we present a new method for synthesizing novel view of a d scene from few model image in full correspondence the core of this work is the derivation of a tensorial operator that describes the transformation from a given tensor of three view to a novel tensor of a new configuration of three view by repeated application of the operator on a seed tensor with a sequence of desired virtual camera position we obtain a chain of warping function tensor from the set of model image to create the desired virtual view 
in this paper we first introduce the multi focus camera a new image sensor used for depth from defocus dfd range measurement it can capture three image with different focus value simultaneously we then propose two different depth measurement method using the camera the first method an augmented version of the one proposed in employ a noniterative optimization process to compute depth value on edge point the second one incorporates a coded aperture with the camera and applies model based pattern matching to estimate depth value of textured surface here we propose two type of coded aperture and corresponding analysis algorithm d fourier analysis to acquire a depth map and a blur free image from three defocused image taken with a pair of pinhole and d convolution based model matching for the fast and precise depth measurement using a coded aperture with four pinhole experimental result showed that the multi focus camera work well a a practical dfd range sensor and that the coded aperture much improve it range estimation capability for real world scene 
in general procedure for determining bayes optimal adaptivecontrols for markov decision process mdp s require a prohibitiveamount of computation the optimal learning problemis intractable this paper proposes an approximate approach inwhich bandit process are used to model in a certain quot local quot sense a given mdp bandit process constitute an important subclass ofmdp s and have optimal learning strategy defined in term ofgittins index that can be computed relatively 
we present a method for the integration of illumination constraint within a deformable model framework these constraint are incorporated a nonlinear holonomic constraint in the lagrange equation of motion governing the deformation of the model for improved numerical performance we employ the baumgarte stabilization method our methodology is general and can be used for a broad range of illumination constraint this approach avoids commonly used approximation in shape from shading such a linearization and the use of partial differential equation which require initial boundary condition furthermore global and local parameterizations of the deformable model allow an improved estimation of shape from shading we demonstrate this improvement over previously used approach through a series of experiment on standardized set of real and synthetic data 
eric b baumnec research institute independence wayprinceton nj eric research nj nec comabstract i argue that the mind should be viewed a an economy and describe an algorithm that autonomously apportionscomplex task to multiple cooperating agent insuch a way that the incentive of each agent is exactly tomaximize my reward a owner of the system a specificmodel called quot the hayek machine quot is proposedand tested on a simulated block world bw planningproblem hayek 
this paper discus the main issue in evaluating beat tracking system and proposes a method of evaluating the accuracy of these system output although there have been a few attempt to evaluate beat tracking system they have not sufficiently addressed quantitative evaluation of tracking accuracy in particular for audio signal our method compare subjective hand labeled beat position with computer parsed beat position and enables u to quantitatively analyze the deviation error of the tracked rhythm beat not only at the quarter note level but also at the half note and measure level it also considers several typical error such a half tempo and doubletempo error a well a correct parsing in testing our beat tracking system for audio signal without drum sound on popular song sampled from compact disc we used the proposed method to evaluate overall recognition rate and tracking quickness and accuracy 
this paper proposes a method that can spot and recognize each facial expression from time sequential image that contain multiple facial expression that could abruptly change from one expression to another expression previously the author have proposed an hmm hidden markov model based method for recognizing a spotted facial expression in this paper to hmm we add state corresponding to the simultaneous motion of two different facial expression i e a muscle relaxation for one expression and a muscle contraction for another expression then the added state are each linked from the hmm apex state of one expression and are linked to that of another expression experimental result showed that for most pair of expression the change in expression can be recognized accurately in addition recognition rate for very fast change of expression improved significantly the proposed method wa applied to regenerate facial expression on a synthesized character to show the method s effectiveness in obtaining facial motion information 
a new cascade basis reduction method of computing the optimal least square set of basis function steering a given function is presented the method combine the lie group theoretic and the singular value decomposition approach in such a way that their respective strength complement each other since the lie group theoretic approach is used the set of basis and steering function computed can be expressed analytically because the singular value decomposition method is used this set of basis and steering function is optimal in the least square sense furthermore the computational complexity in designing basis function for transformation group with large number of parameter is significantly reduced the efficiency of the cascade basis reduction method is demonstrated by designing a set of basis function that steer a gabor function under the four parameter linear transformation group 
a central theme of computational vision research ha been the realization that reliable estimation of local scene property requires propagating measurement across the image many author have therefore suggested solving vision problem using architecture of locally connected unit updating their activity in parallel unfortunately the convergence of traditional relaxation method on such architecture ha proven to be excruciatingly slow and in general they do not guarantee that the stable 
we propose a novel solution to the problem of motion compensation of coronary angiographs a the heart is beating it is difficult for the physician to observe closely a particular point e g stenosis on the artery tree we propose to rigidly compensate the sequence so that the area around the point of interest appears stable this is a difficult problem because the artery deform in a nonrigid manner and only their d x ray projection is observed also the lack of feature around the selected point make the matching subject to the aperture problem the algorithm automatically extract a section of the artery of interest model it a a polyline andtracks it the problem is formulated a an energy minimization problem which is solved using a shortest path in a graph algorithm the motion compensated sequence can be obtained by translating every pixel so that the point of interest remains stable we have applied this algorithm to many example in two set of angiography data and have obtained excellent result 
in macaque inferotemporal cortex it neuron have been found to respond selectively to complex shape while showing broad tuning invariance with respect to stimulus transformation such a translation and scale change and a limited tuning to rotation in depth training monkey with novel paperclip like object logothetis et al could investigate whether these invariance property are due to experience with exhaustivelymany transformedinstancesofanobjectorifthereare mechanism that allow the cell to show response invariance also to previously unseen instance of that object they found object selective cell in anterior it which exhibited limited invariance to various transformation after trainingwithsingle object view while previous model accounted for the tuning of the cell for rotation in depth and for their selectivity to a specific object relative to a population of distractor object the model described here attempt to explain in a biologically plausible way the additional property of translation and size invariance using the same stimulus a in the experiment we find that model it neuron exhibit invariance property which closely parallel those of real neuron simulation show that the model is capable of unsupervised learning of view tuned neuron 
in this paper we propose a general framework to build a task oriented d object recognition system for cad based vision cbv feature from d space curve representing the object s rim provide sufficient information to allow identification and pose estimation of industrial cad model however feature relying on differential surface property tend to be very vulnerable with respect to noise to model the statistical behavior of the data we introduce bayesian netswhich model the relationship between object and observable feature furthermore task oriented selection of the optimal action to reduce the uncertainty of recognition result is incorporated into the bayesian net this enables the integration of intelligent recognition strategy depending on the already acquired evidence into a robust and effcient d cad based recognition system 
computing camera rotation from image sequence can be used for image stabilization and when the camera rotation is known the computation of translation and scene structure are much simplified a well a robust approach for recovering camera rotation is presented which doe not assume any specific scene structure e g no planar surface is required and which avoids prior computation of the epipole given two image taken from two different viewing position the rotation matrix between the image can be computed from any three homgraphy matrix the homographies are computed using the trilinear tensor which describes the relation between the projection of a d point into three image the entire computation is linear for small angle and is therefore fast and stable iterating the linear computation can then be used to recover larger rotation a well 
abstract this article present a statistical theory for texture modeling this theory combine filtering theory and markov random field modeling through the maximum entropy principle and interprets and clarifies many previous concept and method for texture analysis and synthesis from a unified point of view our theory characterizes the ensemble of image i with the same texture appearance by a probability distribution f i on a random field and the objective of texture modeling is to make inference about f i given a set of observed texture example in our theory texture modeling consists of two step a set of filter is selected from a general filter bank to capture feature of the texture these filter are applied to observed texture image and the histogram of the filtered image are extracted these histogram are estimate of the marginal distribution of f i this step is called feature extraction the maximum entropy principle is employed to derive a distribution p i which is restricted to have the same marginal distribution a those in this p i is considered a an estimate of f i this step is called feature fusion a stepwise algorithm is proposed to choose filter from a general filter bank the resulting model called frame filter random field and maximum entropy is a markov random field mrf model but with a much enriched vocabulary and hence much stronger descriptive ability than the previous mrf model used for texture modeling gibbs sampler is adopted to synthesize texture image by drawing typical sample from p i thus the model is verified by seeing whether the synthesized texture image have similar visual appearance to the texture image being modeled experiment on a variety of d and d texture are described to illustrate our theory and to show the performance of our algorithm these experiment demonstrate that many texture which are previously considered a from different category can be modeled and synthesized in a common framework keywords texture modeling texture analysis and synthesis minimax entropy maximum entropy markov 
the requirement of a strict and fixed distinction between dependent variable and independent variable together with the presence of missing data typically imposes considerable problem for most standard statistical prediction procedure this paper describes a solution of these problem through the weighted effect approach in which recursive neural net are used to learn how to compensate for any main and interaction effect attributable to missing data through the use of an effect set in addition to the data of actual cxqes fxtensive simulation of the approach based on an existing psychological data base showed high predictive validity and a graceful degradation in performance with an increase in the number of unknown predictor variable moreover the method proved amenable to the use of twoparameter logistic curve to arrive at a three way low high and undecided decision scheme with a priori known error rate 
new set of color model are proposed for object recognition invariant to a change in view point object geometry and illumination further computational method are presented to combine color and shape invariant to produce a high dimensional invariant feature set for discriminatory object recognition experiment on a database of image show that object recognition based on composite color and shape invariant feature provides excellent recognition accuracy furthermore object recognition based on color invariant provides very high recognition accuracy whereas object recognition based entirely on shape invariant yield very poor discriminative power the image database and the performance of the recognition scheme can be experienced within pictoseek on line a part of the zomax system at http www win uva nl research isi zomax 
sigmoid type belief network a class of probabilistic neural network provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problem often the parameter used in these network need to be learned from example unfortunately estimating the parameter via exact probabilistic calculation i e the em algorithm is intractable even for network with fairly small number of hidden unit we propose to avoid the infeasibility of the e step by bounding likelihood instead of computing them exactly we introduce extended and complementary representation for these network and show that the estimation of the network parameter can be made fast reduced to quadratic optimization by performing the estimation in either of the alternative domain the complementary network can be used for continuous density estimation a well 
in this paper we study the infinitesimal time case of the so called multilinear constraint that exist for each subsequence in a sequence of image these constraint link the infinitesimal motion of the image point with the infinitesimal viewer motion the analysis is done both for calibrated and uncalibrated camera two simplification are also presented for the uncalibrated camera case one simplification is made using affine reduction and kinetic depth the second simplification is based upon a projective reduction with respect to the image of a planar patch 
classification for very large datasets ha many practical application in data mining technique such a discretization and dataset sampling can be used to scale up decision tree classifier to large datasets unfortunately both of these technique can cause a significant loss in accuracy we present a novel decision tree classifier called cloud which sample the splitting point for numeric attribute followed by an estimation step to narrow the search space of the best split cloud reduces computation and i o complexity substantially compared to state of the art classifters while maintaining the quality of the generated tree in term of accuracy and tree size we provide experimental result with a number of real and synthetic data ets 
this paper present a general framework for the computation of projective invariant of arbitrary degree of freedom dof trihedral polyhedron we show that high dof figure can be broken down into set of connected four dof polyhedron for which known invariant exist although the more general shape do not posse projective property a a whole when viewed by a single camera each subpart doe yield a projective description which is based on the butterfly invariant furthermore planar projective invariant can be measured which link together the subpart and so we can develop a local global description for general trihedral polyhedron we demonstrate the recovery of polyhedral shape description from image by exploiting the local global nature of the invariant introduction in this article we introduce a general scheme for understand ing the shape property of trihedral polyhedron trihedral polyhedron are solid polyhe dra made up of plane in arbitrary position and a such no special constraint exist b etween the plane the nomenclature trihedral derives from the fact that the vertex of the polyhedron are only ever defined by triple of plane point in space need at least three plane to assert their location but any more would provide excess constraint and hence would not be generic and stably realisable the result in this paper are a summ ary of those given in in all we generalise the result in which showed how a projectively invariant description can be computed for four degree of freedom dof polyhedron from a single view in turn wa a extension of the work of sugihara the latter dealt with scaled orthographic projection and the calibrated perspec tive case whereas the former demonstrated the projective equivalence of all member of the family of four dof polyhedron generating a set of scene measurement using an uncalibrated camera we show in this paper that the approach of can be extended to inclu de all trihedral polyhedron we also build on some recent work for computing the invariant of minimal point configuration in three dimensional space being able to compute measure for small local feature group provides robustness to occlusion more global description can be built up using the local global nature of many shape description we derive a part whole decomposition by drawing the invariant description of and the invariant based on the butterfly configurationof mundy together the butterfly invariant is a geometric description of a special six point configuration our interest in the butterfly invariant wa promoted by the re cent paper of sugimoto this paper discus an invariant very similar to the original butterfly invariant but suggests an algebraic rather than a geometric f ormulation however sugimoto suggested that the invariant in in some way replace the invariant described by in fact these two type of invariant can be taken hand in hand and are exactly complementary this we show partly in this paper and in more detail in the contribution of this paper are three fold in section we discus how the original invariant description of can be decomposed into a set of three independent butterfly invariant then we show in section how to reduce a five dof figure into set of 
computational comparison is made betweentwo feature selection approach for finding aseparating plane that discriminates betweentwo point set in an n dimensional featurespace that utilizes a few of the n feature dimension a possible in the concave minimizationapproach a separating planeis generated by minimizing a weighted sum ofdistances of misclassified point to two parallelplanes that bound the set and whichdetermine the separating plane midway betweenthem 
in this paper we discus new result on the shape from darkness problem using the motion of cast shadow to recover scene structure our approach is based on collecting a set of image from a fixed viewpoint a a known light source move across the sky previously published solution to this problem have performed the reconstruction only for cross section of the scene in this paper we present a reconstruction algorithm and discus the reconstruction of an entire d scene under various light source trajectory we also consider the constraint on reconstruction we conclude with experimental result that illustrate the convergence property of the solution process and it robustness property 
the full version of this paper appeared aticml many problem correspond to theclassical control task of determining the appropriatecontrol action to take given some sequence of observation one standard approachto learning these control rule calledbehavior cloning involves watching a perfectoperator operate a plant and then tryingto emulate it behavior in the experimentallearning approach by contrast thelearner first guess an initial operation toaction 
an image clad surface representation based on regularization theory is introduced in this paper this representation is based on a hybrid model derived from the physical membrane and plate model the representation called the spl lambda spl tau representation ha two dimension one dimension represents smoothness or scale while the other represents the continuity of the image or surface it contains image surface sampled both in scale space and the weighted sobolev space of continuous function thus this new representation can be viewed a an extension of the well known scale space representation we have experimentally shown that the proposed hybrid model result in improved result compared to the two extreme constituent model i e the membrane and the plate model based on this hybrid model a generalized edge detector ged which encompasses most of the well known edge detector under a common framework is developed the existing edge detector can be obtained from the generalized edge detector by simply specifying the valve of two parameter one of which control the shape of the filter spl tau and the other control the scale of the filter spl lambda by sweeping the valve of these two parameter continuously one can generate an edge representation in the spl lambda spl tau space which is very useful for developing a goal directed edge detection scheme for a specific task the proposed representation and the edge detector have been evaluated qualitatively and quantitatively on several different type of image data such a intensity range and stereo image 
in the past the evaluation of machine translation system ha focused on single system evaluation because there were only few system available but now there are several commercial system for the same language pair this requires new method of comparative evaluation in the paper we propose a black box method for comparing the lexical coverage of mt system the method is based on list of word from different frequency class it is shown how these word list can be compiled and used for testing we also present the result of using our method on mt system that translate between english and german 
this paper proposes a novel pattern classificationapproach called the nearest linear combination nlc approach for eigenface based face recognition assumethat multiple prototypical vector are availableper class each vector being a point in an eigenfacespace a linear combination of prototypical vectorsbelonging to a face class is used to define a measureof distance from the query vector to the class the measure being defined a the euclidean distancefrom the query to the linear 
telegraphic message with numerous instance of omission pose a new challenge to parsing in that a sentence with omission cause a higher degree of ambiguity than a sentence without omission misparsing induced by omission ha a far reaching consequence in machine translation namely a misparse of the input often lead to a translation into the target language which ha incoherent meaning in the given context this is more frequently the case if the structure of the source and target language are quite different a in english and korean thus the question of how we parse telegraphic message accurately and efficiently becomes a critical issue in machine translation in this paper we describe a technical solution for the issue and present the performance evaluation of a machine translation system on telegraphic message before and after adopting the proposed solution the solution lie in a grammar design in which lexicalized grammar rule defined in term of semantic category and syntactic rule defined in term of part of speech are utilized together the proposed grammar achieves a higher parsing coverage without increasing the amount of ambiguity misparsing when compared with a purely lexicalized semantic grammar and achieves a lower degree of ambiguity misparses without decreasing the parsing coverage when compared with a purely syntactic grammar 
a a benchmark task the spiral problem is well known in neural network unlike previous work that emphasizes learning we approachthe problem from a generic perspective that doe not involve learning we point out that the spiral problem is intrinsically connected to the inside outside problem a generic solution to both problem is proposedbased on oscillatory correlation using a time delay network our simulationresults are qualitatively consistent with human performance andwe 
time series prediction is one of the major application of neural network after a short introduction into the basic theoretical foundat ion we argue that the iterated prediction of a dynamical system may be interpreted a a model of the system dynamic by mean of rbf neural network we describe a modeling approach and extend it to be able to model instationary system a a practical test for the capability of the method we investigate the modeling of musical and speech signal and demonstrate that the model may be used for synthesis of musical and speech signal 
two important problem in camera control are how to keep a moving camera fixated on a target point and how to precisely aim a camera whose approximate pose is known towards a given d position this paper describes how electronic image alignment technique can be used to solve these problem a well a provide other benefit such a stabilized video hence stabilized fixated imagery is obtained despite large latency in the control loop even for simple control strategy these technique have been tested using an airborne camera and real time affine image alignment 
clustering algorithm are attractive for the task of class identification in spatial database however the application to large spatial database rise the following requirement for clustering algorithm minimal requirement of domain knowledge to determine the input parameter discovery of cluster with arbitrary shape and good efficiency on large database the well known clustering algorithm offer no solution to the combination of these requirement in this paper we present the new clustering algorithm dbscan relying on a density based notion of cluster which is designed to discover cluster of arbitrary shape dbscan requires only one input parameter and support the user in determining an appropriate value for it we performed an experimental evaluation of the effectiveness and efficiency of dbscan using synthetic data and real data of the sequoia benchmark the result of our experiment demonstrate that dbscan is significantly more effective in discovering cluster of arbitrary shape than the well known algorithm clarans and that dbscan outperforms clarans by a factor of more than in term of efficiency 
we describe the implementation of a hidden markov model state decoding system a component for a wordspotting speech recognition system the key specification for this state decoder design is microwatt power dissipation this requirement led to a continuoustime analog circuit implementation we describe the tradeo s inherent in the choice of an analog design and explain the mapping of the discrete time state decoding algorithm into the continuous domain we characterize the operation of a word state state decoder test chip 
we present a method for the analysis of nonstationary time serieswith multiple operating mode in particular it is possible todetect and to model both a switching of the dynamic and a lessabrupt time consuming drift from one mode to another this isachieved in two step first an unsupervised training method providesprediction expert for the inherent dynamical mode then the trained expert are used in a hidden markov model that allowsto model drift an application to 
we present here an approach and algorithm for mining generalized term association the problem is to find co occurrence frequency of term given a collection of document each with relevant term and a taxonomy of term we have developed an efficient count propagation algorithm cpa targeted for library application such a medline the basis of our approach is that set of term termsets can be put into a taxonomy by exploring this taxonomy cpa propagates the count of termsets to their ancestor in the taxonomy instead of separately counting individual termset we found that cpa is more efficient than other algorithm particularly for counting large termsets a benchmark on data set extracted from a medline database showed that cpa outperforms other known algorithm by up to around half the computing time at the cost of le than of additional memory to keep the taxonomy of termsets we have used discovered knowledge of term association for the purpose of improving search capability of medline 
content based indexing method are of great interest for image and video retrievial in audio visual archive such a in the divan project that we are currently developping detecting and recognizing human face automatically in video data provide user with powerful tool for performing query in this article a new scheme for face recognition using a wavelet packet decomposition is presented each face is described by a subset of band filtered image containing wavelet coefficient these coefficient characterize the face texture and a set of simple statistical measure allows u to form compact and meaningful feature vector then an efficient and reliable probalistic metric derived from the bhattacharrya distance is used in order to classify the face feature vector into person class 
we use the constant statistic constraint to calibrate an array ofsensors that contains gain and offset variation this algorithm hasbeen mapped to analog hardware and designed and fabricated witha um cmos technology measured result from the chip show thatthe system achieves invariance to gain and offset variation of theinput signal introductiontransistor mismatch and parameter variation cause unavoidable nonuniformitiesfrom sensor to sensor a one time calibration procedure 
facial expression provides sensitive cue about emotion and play a major role in interpersonal and humancomputer interaction most facial expression recognition system have focused on only six basic emotion and their concomitant prototypic expression posed by a small set of subject in reality human are capable of producing thousand of expression that vary in complexity intensity and meaning to represent the full range of facial expression we developed a computer vision system that automatically recognizes individual action unit au or au combination using hidden markov model and estimate expression intensity three module are used to extract facial expression information facial feature point tracking dense flow tracking with principal component analysis pca and high gradient component detection i e furrow detection the average recognition rate of upper and lower face expression is and respectively using feature point tracking upper face using dense flow tracking with pca and and upper and lower face respectively using high gradient component detection 
this paper present a technique to determine the identity of object in a scene using histogram of the response of a vector of local linear neighborhood operator receptive field this technique can be used to determine the most probable object in a scene independent of the object s position image plane orientation and scale in this paper we describe the mathematical foundation of the technique and present the result of experiment which compare robustness and recognition rate for different local neighborhood operator and histogram similarity measurement 
in this work we develop a new criterion to performpessimistic decision tree pruning ourmethod is theoretically sound and is based ontheoretical concept such a uniform convergenceand the vapnik chervonenkis dimension we show that our criterion is very wellmotivated from the theory side and performsvery well in practice the accuracy ofthe new criterion is comparable to that of thecurrent method used in c introductionthe phenomenon of overfitting the data is well knownin 
in order to rank the performance of machine learning algorithm many researcher conduct experiment on benchmark data set since most learning algorithm have domain specific parameter it is a popular custom to adapt these parameter to obtain a 
we rst describe a hierarchical generative model that can be viewed a a non linear generalisation of factor analysis and can be implemented in a neural network the model performs per ceptual inference in a probabilistically consistent manner by using top down bottom up and lateral connection these connection can be learned using simple rule that require only locally avail able information we then show how to incorporate lateral con nections into the generative model the model extract a sparse distributed hierarchical representation of depth from simpli ed random dot stereograms and the localised disparity detector in the rst hidden layer form a topographic map when presented with image patch from natural scene the model develops topo graphically organised local feature detector 
w s is a real time visual surveillance system for detecting and tracking people and monitoring their activity in an outdoor environment by integrating realtime stereo computation into an intensitybased detection and tracking system unlike many system for tracking people w s make no use of color cue instead w s employ a combination of stereo shape analysis and tracking to locate people and their part head hand foot torso and create model of people s appearance so that they can be tracked through interaction such a occlusion w s is capable of simultaneously tracking multiple people even with occlusion it run at hz for resolution image on a dual pentium pc 
this paper establishes common ground for researcher addressing the challenge of scaling up inductive data mining algorithm to very large database and for practitioner who want to understand the state of the art we begin with a discussion of important but often tacit issue related to scaling up we then overview existing method categorizing them into three main approach finally we use the overview to recommend how to proceed when dealing with a large problem and where future 
this article raise the problem of error caused by the metrology of a calibration pattern to the accurate estimation of the intrinsic and extrinsic calibration parameter modeling the video system in order to take into account these error a new approach is proposed that enables u to compute in the same time the traditional calibration parameter and the d geometry of the calibration pattern using a multi image calibration algorithm experimental result show that the proposed algorithm lead to reliable calibration result and prof that calibration error no longer depend on the accuracy of calibration point measurement but on the accuracy of calibration point detection in the image plane 
shape indexing is a way of making rapid association between feature detected in an image and object model that could have produced them when model database are large the use of high dimensional feature is critical due to the improved level of discrimination they can provide unfortunately finding the nearest neighbour to a query point rapidly becomes inefficient a the dimensionality of the feature space increase past indexing method have used hash table for hypothesis recovery but only in low dimensional situation in this paper we show that a new variant of the k d tree search algorithm make indexing in higher dimensional space practical this best bin first or bbf search is an approximate algorithm which find the nearest neighbour for a large fraction of the query and a very close neighbour in the remaining case the technique ha been integrated into a fully developed recognition system which is able to detect complex object in real cluttered scene in just a few second 
one approach to recognizing object seen from arbitrary viewpoint isby extracting invariant property of the object from single image such property are found in image of d object only when theobjects are constrained to belong to certain class e g bilaterally symmetric object existing study that follow thisapproach propose how to compute invariant representation for ahandful of class of object a fundamental question regarding theinvariance approach is whether it can be applied to a wide range ofclasses to answer this question it is essential to study the set ofclasses for which invariance exists this paper introduces a newmethod for determining the existence of invariant function forclasses of object together with the set of image from which theseinvariants can be computed we develop algebraic test that determinewhether the object in a given class can be identified from singleimages these test apply to class of object undergoing affineprojection in addition these test allow u to determine the set ofviews of the object which are degenerate we apply these test toseveral class of object and determine which of them is identifiableand which of their view are degenerate 
we consider the microscopic equation for learning problem inneural network the aligning field of an example are obtainedfrom the cavity field which are the field if that example wereabsent in the learning process in a rough energy landscape we assumethat the density of the metastable state obey an exponentialdistribution yielding macroscopic property agreeing with the firststep replica symmetry breaking solution iterating the microscopicequations provide a learning 
many knowledge discovery kdd system need to spend substantial amount of effort to search for rulesand pattern within large amount of data after some natural evolution a a consequence of updatesapplied to their database these system must update their previously discovered knowledge to reflect thecurrent state of their database the straight forward approach of re running the discovery process onthe whole updated database to re discover the rule and pattern is not 
in this paper we propose a framework structured semantic space a a foundation for word sense disarnbiguation task and present a strategy to identify the correct sense of a word in some context based on the space the semantic space is a set of multidimensional real valued vector which formally describe the context of word instead of locating all word sens in the space we only make use of mono sense word to outline it we design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the node sense cluster corresponding with set of similar sens in the dendrogram given a word in a particular context the context would activate some cluster in the dendrogram based on it similarity with the context of the word in the cluster then the correct sense of the word could be determined by comparing it definition with those of the word in the cluster 
abstract here we analyze synaptic transmission from an information theoretic perspective we derive closed form expression for the lower bound on the capacity of a simple model of a cortical synapse under two explicit coding paradigm under the signal estimation paradigm we assume the signal to be encoded in the mean firing rate of a poisson neuron the performance of an optimal linear estimator of the signal then provides a lower bound on the capacity for signal estimation under the signal detection paradigm the presence or absence of the signal ha to be detected performance of the optimal spike detector allows u to compute a lower bound on the capacity for signal detection we find that single synapsis for empirically measured parameter value transmit information poorly but significant improvement can be achieved with a small amount of redundancy 
computing a consistent interpretation of the variable involved in a set of temporal constraint is an important task for many area of ai requiring temporal reasoning we focus on the important class of the qualitative relation in nebel and biirckert s ord horn algebra and of the metric constraint forming a stp possibly augmented w i th inequations for these tractable class we present three new algorithm for solving the problem of finding a solution and an efficient algorithm for determining the consistency of a stp augmented with in equation 
we train recurrent network to control chemotaxis in a computermodel of the nematode c elegans the model presented is basedclosely on the body mechanic behavioral analysis neuroanatomyand neurophysiology of c elegans each imposing constraint relevantfor information processing simulated worm moving autonomouslyin simulated chemical environment display a varietyof chemotaxis strategy similar to those of biological worm introductionthe nematode c elegans provides a unique 
this article present a theory for multi scale representation of temporaldata assuming that a real time vision system should represent the incomingdata at different time scale an additional causality constraintarises compared to traditional scale space theory we can only use whathas occurred in the past for computing representation at coarser timescales based on a previously developed scale space theory in term of noncreationof local maximum with increasing scale a complete 
in poggio and edelman proposed a view based model of objectrecognition that account for several psychophysical propertiesof certain recognition task the model predicted the existence ofview tuned and view invariant unit that were later found by logothetiset al logothetis et al in it cortex of monkeystrained with view of specific paperclip object the model however doe not specify the input to the view tuned unit and theirinternal organization in this paper we 
abstract we present the cem conditional expectation maximization al gorithm a an extension of the em expectation maximization algorithm to conditional density estimation under missing data a bounding and maximization process is given to speci cally optimize conditional likelihood instead of the usual joint likelihood we ap ply the method to conditioned mixture model and use bounding technique to derive the model s update rule monotonic conver gence computational e ciency and regression result superior to em are demonstrated 
research emanating from artificial intelligencehas throughout it history contributedto technique and idea in software engineering we describe in this paper a case studyshowing the use of theory revision to the refinementof a formally specified requirementsmodel in a previous project we were contractedto create a precise model of the complexcriteria governing the separation of aircraftprofiles in atlantic airspace duringthat work it became clear that the automated 
a neural network approach to stereovision is presented based onaliasing effect of simple disparity estimator and a fast coherencedetectionscheme within a single network structure a dense disparitymap with an associated validation map and additionally the fused cyclopean view of the scene are available the networkoperations are based on simple biological plausible circuitry thealgorithm is fully parallel and non iterative introductionhumans experience the three dimensional 
in order to reduce false alarm and to improve the target detection performance of an automatic target detection and recognition system operating in a cluttered environment it is important to develop the model not only for man made target but also of natural background clutter because of the high complexity of natural clutter this clutter model can only be reliably built through learning from real example if available contextual information that characterizes each training example can be used to further improve the learned clutter model in this paper we present such a clutter model aided target detection system emphasis are placed on two topic learning the background clutter model from sensory data through a self organizing process reinforcing the learned clutter model using contextual information 
abstract this paper present an original method and it implementation to extract terminology from corpus by combining linguistic filter and statistical method starting from a linguistic study of the term of telecommunication domain we designed a number of filter which enable u to obtain a first selection of sequence that may be considered a term various statistical score are applied to this selection and result are evaluated this method ha been applied to french and to english but this paper deal only with french 
support vector machine work by mappingtraining data for classification task into ahigh dimensional feature space in the featurespace they then find a maximal marginhyperplane which separate the data thishyperplane is usually found using a quadraticprogramming routine which is computationallyintensive and is non trivial to implement in this paper we propose an adaptationof the adatron algorithm for classificationwith kernel in high dimensionalspaces the algorithm is 
the handling of anomalous or outlying observation in a data set is one of the most important task in data pre processing it is important for three reason first outlying observation can have a considerable influence on the result of an analysis second although outlier are often measurement or recording error some of them can represent phenomenon of interest something significant from the viewpoint of the application domain third for many application exception identified can often lead to the discovery of unexpected knowledge 
learning difficulty can vary considerably from one algorithm to another because different approach may be biased or tuned towards a certain initial problem description reason for poor performance include noise class distribution number of attribute or example in the sample however when intrinsic accuracy is high and performance is poor the problem can be caused by feature interaction pattern are more difficult to identify because they are conditional system that attempt to learn in domain such a this can perform constructive induction to change the initial representation to one which make classification information more visible however system that attempt to reformulate example description often do so regardless of the initial representation the author present a data based detection measure that estimate concept difficulty several measure including ness variation blurring and j are compared they argue that a measure based only on the a posteriori probability of the class variable ha limited use and that disparity between concept difficulty and blurring result on some data set can be explained by employing a simple technique that average the blurring measure over subset generated by splitting on the best attribute 
the article describes research currently being carried out by the control of power system group at the queen s university of belfast into the application of data mining to the performance monitoring and optimisation of the steam generation system in thermal power plant this work is being carried out in conjunction with premier power plc which owns and operates ballylumford power station near larne in northern ireland this station consists of mw and mw gas oil fired generating unit plus mw gas oil turbine the main component of a steam generation system consist of an oil gas fired boiler a turbine and a condenser although the operation of these is conceptually simple the component are extremely complicated and due to the nature of the process involved in steam generation they are prone to degradation and failure this can lead to a reduction in the thermal efficiency of the plant increase in plant emission and the possibility of unscheduled power outage the aim of the research is twofold to develop model of the plant over the full range of operating condition and to develop and implement a system which will use the model to determine the condition of the plant accurately and which will be able to make operational suggestion to engineer operator to rectify any deviation detected the model are to be created by data mining on the large database of archived plant data which premier power ha made available to queen s university 
the author examine the total daily load data for a large region of the uk over an eight year period the objective is to examine the data and determine what factor influence the load level the approach is to assume little knowledge of the system starting with a minimal number of input and a network with few hidden neuron this way the network will formulate a relationship between the given input and the load by examining the peculiarity of those day which do not fit into the model it is possible to discover why they do not and to create extra input that convey the information required 
we outline an approach which aim to link data mining technique within an architecture to assist in understanding natural language text it is obvious that understanding language is a kind of knowledge problem and it is generally acknowledged that knowledge acquisition is costly and time consuming we suggest that rule induction and related approach can help make this particular problem more tractable paving the way for various useful and usable product it is taken a axiomatic that the information and especially textual information which is available to individual and organisation will continue to grow unfortunately the capacity of people to deal with information unaided is going to remain static therefore there is a need for tool which can summarise categorise and contextualise information 
the aim of the investigation is to develop a system which will give product designer access to data and information from a range of corporate database deemed essential to their function in particular customer complaint product material feature r d testing access to these data may point to fundamental design anomaly or inefficiency which would not have been otherwise apparent the goal of the investigation therefore are to providing a mechanism to enable information to be used from later life cycle stage by earlier one to provide this information in a format which would be understandable and useable to another product life cycle function 
pharmaceutical company are continually striving to determine the common key characteristic of compound that determine their functionality e g relief of asthma so that they may continue to provide safe and effective medicine historically this ha been carried out by visually comparing graphical representation of the structure of compound which posse the same functionality so that key substructure pharmacophores may be determined however with the advent of high throughput screening technique providing data on enormous number of compound this ha become inappropriate a human can only compare a certain number of pattern accurately in a day potential solution to this problem may appear to come from a knowledge based system approach based upon pattern matching however we suggest this solution doe lie with a knowledge based system approach but one which relies on data mining a the underpinning technology we discus our initial work which show that organic compound possessing the same functionality may be mined for common substructure using data mining technique we also discus how our prototype tool in which a selection of data mining algorithm may be chosen ha been developed in a functional programming language the functional language gofer which wa used to rapidly prototype the tool readily lends itself to the task due to it polymorphism and lazy evaluation the lazy evaluation of gofer is a particularly useful feature of the language which readily enables the common characteristic to be determined no matter how large the compound 
