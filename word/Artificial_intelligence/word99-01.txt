we introduce a new notion related to auction and mechanism design called fair imposition in this setting a center wish to fairly and efficiently allocate task among a set of agent not knowing their cost structure a in the study of auction the main obstacle to overcome is the self interest of the agent which will in general cause them to hide their true cost unlike the auction setting however here the center ha the power to impose arbitrary behavior on the agent and furthermore wish to distribute the cost a fairly a possible among them we define the problem precisely present solution criterion for these problem the central of which is called k efficiency and present both positive result in the form of concrete protocol and negative result in the form of impossibility theorem concerning these criterion 
diagrammatic reasoning comprises phenomenon that range from the so called free ride e g almost immediate understanding of visually perceived relationship to convention about token such reasoning must involve cognitive process that are highly perceptual in content in the domain of mathematical proof where diagram have had a long history we have an opportunity to investigate in detail and in a controlled setting the various perceptual device and cognitive process that facilitate diagrammatically based argument this paper continues recent work by examining two kind of diagrammatic proof called category and by jamnik et al the first being one in which generalization of a diagram instance is implied and the second being one in which an infinite completion is represented by an ellipsis we provide explanation of why these proof work a semantics for ellipsis and conjecture about the underlying cognitive process that seem to resonate with such proof 
in this paper we consider the problem of cyclic schedule such a arise in manufacturing we introduce a new formulation of this problem that is a very simple modification of a standard job shop scheduling formulation and which enables u to use existing constraint reasoning technique to generate cyclic schedule we present evidence for the effectiveness of this formulation and describe extension for handling multiple capacity resource and for recovering from break in cyclic schedule 
market mechanism play a central role in ai a a coordination tool in multiagent system and a an application area for algorithm design mechanism where buyer are directly cleared with seller and thus do not require an external liquidity provider are highly desirable for electronic marketplace for several reason in this paper we study the inherent complexity of and design algorithm for clearing auction and reverse auction with multiple indistinguishable unit for sale we consider setting where bidder express their preference via price quantity curve and setting where the bid are price quantity pair we show that market with piecewise linear supply demand curve and non discriminatory pricing can always be cleared in polynomial time surprisingly if discriminatory pricing is used to clear the market the problem becomes complete even for step function curve if the price quantity curve are all linear then in most variant the problem admits a poly time solution even for discriminatory pricing when bidder express their preference with price quantity pair the problem is complete but solvable in pseudo polynomial time with free disposal the problem admits a poly time approximation scheme but no such approximation scheme is possible without free disposal we also present pseudo polynomial algorithm for xor bid and ofs bid and analyze the approximability 
we propose a method to gradually expand the move to consider at the node of game search tree the algorithm begin with an iterative deepening search using the minimal set of move and if the search doe not succeed iteratively widens the set of possible move performing a complete iterative deepening search after each widening when d esigning carefully the different set of possible move the a lgorithm can save some time in the game of go tree search a shown by experimental result 
a model for the formation of situation concept is described a characteristic of this form of concept formation is that it doe not require instructive feedback this render it suitable for concept formation by autonomous agent it is experimentally demonstrated that situation concept constructed independently by several agent can convey useful information between agent through a learned system of communication a relation wa found between the development of the learned system of communication and the duration of the situation 
many imaging system seek a good interpretation of the scene presented i e a plausible perhaps optimal mapping from aspect of the scene to real world object this paper address the issue of nding such likely mapping efficiently in general an interpretation policy specie when to apply which imaging operator which can range from low level edge detector and region grower through highlevel token combination rule and expectation driven objectdetectors given the cost of these operator and the distribution of possible image we can determine both the expected cost and expected accuracy of any such policy our task is to nd a maximally effective policy typically one with sufcient accuracy whose cost is minimal we explore this framework in several context including the eigenface approach to face recognition our result show in particular that policy which select the operator that maximize information gain per unit cost work more effectively than other policy including one that at each stage simply try to establish the putative most likely interpretation 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
for many application it is important that agent be not only correct but also comprehensible to human user typically people have tried to make agent behavior and reasoning understandable by adding post hoc special purpose explanation system with often disappointing result here i instead take the comprehensibility of agent behavior a a central agent design consideration from the start i describe an agent architecture the expressivator that support comprehensibility on top of a behavior based framework using four technical innovation structuring the agent s behavior according to the sign and signifier it is intended to communicate allowing the agent to keep track of it impression on the user with sign management using behavioral transition to explain the reason for agent behavior and expressing behavioral interrelationship directly using meta level control 
we present a formal characterization and semantic representation for a number of credulous inference relation based on the notion of an epistemic state it is shown in particular that credulous inference can be naturally represented in term of expectation see gardenfors and makinson we describe also the relationship between credulous and usual skeptical nonmonotonic inference and show how they can facilitate each other 
we present an extensive experimental study of consequence finding algorithm based on kernel resolution using both a trie based and a novel zbdd based implementation which us zerosuppressed binary decision diagram to concisely store and process very large clause set our study considers both the full prime implicate task and application of consequence finding for restricted target language in abduction model based and faulttree diagnosis and polynomially bounded knowledge compilation we show that the zbdd implementation can push consequence finding to a new limit solving problem which generate over clause 
in this paper we use a logic based system description for a simple non logic functional language to examine the way in which a diagnosis system can use it system description to improve debugging performance the key concept is that the notion of expression replacement which is the basis for repairing a program can also serve a a fundamental heuristic for searching the source of an error we formally define replacement in term of fault mode explicitly define a replacement order and use the replacement heuristic for finding diagnosis finally we incorporate the use of multiple test case and discus their use in discriminating between diagnosis 
much work in computer science ha adopted competitive analysis a a tool for decision making under uncertainty in this work we extend competitive analysis to the context of multi agent system unlike classical competitive analysis where the behavior of an agent s environment is taken to be arbitrary we consider the case where an agent s environment consists of other agent these agent will usually obey some minimal rationality constraint this lead to the definition of rational competitive analysis we introduce the concept of rational competitive analysis and initiate the study of competitive analysis for multi agent system we also discus the application of rational competitive analysis to the context of bidding game a well a to the classical one way trading problem 
shopbots are agent that automatically search the internet to obtain information about price and other attribute of good and service they herald a future in which autonomous agent profoundly influence electronic market in this study a simple economic model is proposed and analyzed which is intended to quan tify some of the likely impact of a proliferation of shopbots and other economically motivated software agent in addition this paper report on simulation of pricebots adaptive pricesetting agent which firm may well implement to combat or even take advantage of the grow ing community of shopbots this study form part of a larger research program that aim to provide insight into the impact of agent tech nology on the nascent information economy 
we introduce two mechanism for scaling computation in the framework of temporal reasoning the first one address abstraction at the methodological level operator are defined that engender flexible switching between different granularity of temporal representation structure the second one account for abstraction at the interface level of a temporal reasoning engine various generalization of temporal relation are introduced that approximate more fine grained representation by abstracting away irrelevant detail 
integrating the splitting rule into a saturation based theorem prover may be highly beneficial for solving certain class of fist order problem the use of splitting in the context of saturation based theorem proving based on explicit case analysis a implemented in spa employ backtracking which is difficult to implement a it affect design of the whole system here we present a quot cheap quot and efficient technique for implementing splitting that doe not use backtracking 
a theory of shape is important for ai both for recognition and description of object and for reasoning about the possible behaviour of ob jects theory of shape may be loosely clas sified a either volume based or outline based we present a theory of the latter type ini tially confined to two dimensional outline we represent outline by mean of string over an alphabet of seven qualitative curvature type and give a regular grammar which generates the string corresponding to possible outline we use subset of the curvature type alphabet to characterise cognitively salient subclass of outline with corresponding regular subgrammars and use decusping smoothing and merg ing operator to simplify outline for represen tation at coarser granularity we give an algo rithm for deriving the curvature sequence of an outline using only local information obtained a the outline is traversed finally we indicate how more detailed including quantitative in formation can be incorporated into the theory 
story generation is experiencing a revival despite disappointing preliminary result from the preceding three decade one of the principle reason for previous inadequacy wa the low level of writing quality which resulted from the excessive focus of story grammar on plot design although these system leveraged narrative theory via corpus analysis they failed to thoroughly extend those analysis to all relevant linguistic level the end result wa narrative that were recognizable a story but whose prose quality wa unsatisfactory however the blame for poor writing quality cannot be laid squarely at the foot of story grammar a natural language generation ha to date not fielded system capable of faithfully reproducing either the variety or complexity of naturally occurring story this paper present the author architecture for accomplishing precisely that task the storybook implementation of a narrative prose generator and a brief description of a formal evaluation of the story it produce 
we introduce two abstraction mechanism for streamlining the process of semantic interpretation configurational description of dependency graph increase the linguistic generality of interpretation schema while interfacing them to lexical and conceptual inheritance hierarchy reduces the amount and complexity of semantic specification 
metaquery also known a metapattem is a datamining tool useful for learning rule in volving more than one relation in the database a metaquery is a template or a second order proposition in a language l that describes the type of pattern to be discovered this tool ha already been successfully applied to several real world application in this paper we advance the state of the art in metaqueries research in several way first we analyze the related computational problem and classify it a np hard with a tractable sub set that is quite immediately evident second we argue that the notion of support for metaqueries where support is intuitively some indi cation to the relevance of the rule to be discovered is not adequately defined in the liter ature and propose our own definition third we propose some efficient algorithm for com puting support and present preliminary experi mental result that indicate that our algorithm are indeed quite useful 
we present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave one out error jaakkola and haussler proved for support vector machine svms vapnik the new approach directly minimizes the expression given by the bound in an attempt to minimize leave one out error this give a convex optimization problem which construct a sparse linear classifier in feature space using the kernel technique a such the algorithm posse many of the same property a svms the main novelty of the algorithm is that apart from the choice of kernel it is parameterless the selection of the number of training error is inherent in the algorithm and not chosen by an extra free parameter a in svms first experiment using the method on benchmark datasets from the uci repository show result similar to svms which have been tuned to have the best choice of parameter 
compilation to boolean satisfiability ha become a powerful paradigm for solving ai problem however domain that require metric reasoning cannot be compiled efficiently to sat even if they would otherwise benefit from compilation we address this problem by introducing the lcnf representation which combine propositional logic with metric constraint we present lpsat an engine which solves lcnf problem by interleaving call to an incremental simplex algorithm with systematic satisfaction method we describe a compiler which convert metric resource planning problem into lcnf for processing by lpsat the experimental section of the paper explores several optimization to lpsat including learning from constraint failure and randomized cutoff 
we introduce a framework for finding preference information to derive desired conclusion in nonmonotonic reasoning a new abductive framework called preference abduction enables u to infer an appropriate set of priority to explain the given observation skeptically thereby resolving the multiple extension problem in the answer set semantics for extended logic program preference abduction is also combined with a usual form of abduction in abductive logic programming and ha application such a specification of rule preference in legal reasoning and preference view update the issue of learning abducibles and priority is also discussed in which abduction to a particular cause is equivalent to abduction to preference 
abstract locating expertise source in a community of interest or practice is a critical need for distributed organization operating in knowledge intensive business sector this is specially true in those one that deal with innovation activites which have to manage knowledge about the creation of new knowledge finding fastly a suitable expert and knowing how to reach him or her can be seen a a way to gain advantage and speed up organizational knowledge creation and learning usually expertise location is done through the use of personal social or knowledge network and involves aspect such a trust and reputation however and specially in distributed organization relying on communication technology for cooperation each member of a community is just aware of it own personal social or knowledge network this make difficult to get to know other potential expert in the community which may pertain to other member network netexpert is an agent based expertise location system that replicates the process of social and knowledge network building at a community or organization level in so doing it is able to connect several network and put into contact expertise that otherwise would remain hidden keywords software agent multiagent system assistant agent expertise location localize expertise social 
certain planning system that deal with quantitative time constraint have used an underlying simple temporal problem solver to ensure temporal consistency of plan however many application involve process of uncertain duration whose timing cannot be controlled by the execution agent these case require more complex notion of temporal feasibility in previous work various controllability property such a weak strong and dynamic controllability have been defined the most interesting and useful controllability property the dynamic one ha ironically proved to be the most difficult to analyze in this paper we resolve the complexity issue for dynamic controllability unexpectedly the problem turn out to be tractable we also show how to efficiently execute network whose status ha been verified 
we propose a casa labelling method of the tf representation which is based on the periodicity of the speech related to the voicing a local voicing index is estimated in four subbands after demodulation of the signal this index is used a a reliability measure for both pitch identification and speech recognition first this model allows robust f identification thanks to the voicing index which is a consistent reliability measure associated to the f measure since the task is to recognise speech corrupted with additive noise the periodicity is specific to the signal in our model the evaluation of speech reliability is not direct it also depends on a priori knowledge about the relationship between snr and the voicing index the goal is to assign to each tf region a probability clean enough to feed a multistream recogniser only adapted to clean data the model is able to localise region where the target speech dominates over the background noise this probability is evaluated according to a function established a priori the snr feature mapping and the choice of a snr decision threshold this is adapted to a new multistream recognition approach since the previous probability serve to weight the stream posterior 
we describe an interactive system which support the exploration of argument generated from bayesian network in particular we consider key feature which support interactive behaviour an attentional mechanism which update the activation of concept a the interaction progress a set of exploratory response and a set of probabilistic pattern and an argument grammar which support the generation of natural language argument from bayesian network a preliminary evaluation ass the effect of our exploratory response on user s belief 
this paper describes a new layered brain architecture for simulated autonomous and semi autonomous creature that inhabit graphical world the main feature of the brain is it division into distinct system which communicate through common access to an internal mental blackboard the brain wa designed to encourage experimentation with various system and architecture it ha so far proven flexible enough to accommodate research advancing in a number of different direction by a small team of researcher 
we present a tool for the annotation of xmlencoded multi modal language corpus nonhierarchical data is supported by mean of standoff annotation we define base level and suprabase level element and theory independent markables for multi modal annotation and apply them to a cospecification annotation scheme we also describe how arbitrary annotation scheme can be represented in term of these element apart from theoretical consideration however the development of a fast robust and highly usable annotation tool wa a major objective of the work presented 
propagating constraint is the main feature of anyconstraint solver this is thus of prime importanceto manage constraint propagation a efficiently aspossible justifying the use of the best algorithm but the ease of integration is also one of the concernswhen implementing an algorithm in a constraintsolver this paper focus on ac whichis the simplest arc consistency algorithm knownso far we propose two refinement that preserveas much a possible the ease of 
new method to generate hard random problem instance have driven progress on algorithm for deduction and constraint satisfaction recently achlioptas et al aaai introduced a new generator based on latin square that creates only satisfiable problem and so can be used to accurately test incomplete one sided solver we investigate how this and other generator are biased away from the uniform distribution of satisfiable problem and show how they can be improved by imposing a balance condition more generally we show that the generator is one member of a family of related model that generate distribution ranging from one that are everywhere tractable to one that exhibit a sharp hardness threshold we also discus the critical role of the problem encoding in the performance of both systematic and local search solver 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we present a formal framework for treating both incomplete information in the initial database and possible failure during an agent s execution of a course of action these two aspect of uncertainty are formalized by two different notion of probability we introduce also a concept of expected probability which is obtained by combining the two previous notion expected probability account for the probability of a sentence on the hypothesis that the sequence of action needed to make it true might have failed expected probability lead to the possibility of comparing course of action and verifying which is more safe 
the majority of existing language generation system have a pipeline architecture which offer efficient sequential execution of module but doe not allow decision about text content to be revised in later stage however a exemplified in this paper in some case choosing appropriate content can depend on text length and formatting which in a pipeline architecture are determined after content planning is completed unlike pipeline interleaved and revision based architecture can deal with such dependency but tend to be more expensive computationally since our system need to generate acceptable hypertext explanation reliably and quickly the pipeline architecture wa modified instead to allow additional content to be requested in later stage of the generation process if necessary 
the paper describes a branch and bound scheme that us heuristic generated mechanically by the mini bucket approximation this scheme is presented and evaluated for optimization task such a finding the most probable explanation mpe in bayesian network the mini bucket scheme yield monotonic heuristic of varying strength which cause different amount of pruning allowing a controlled tradeoff between preprocessing and search the resulting branch and bound with mini bucket heuristic bbmb is evaluated using random network probabilistic decoding and medical diagnosis network result show that the bbmb scheme overcomes the memory explosion of bucket elimination allowing a gradual tradeoff of space for time and of time for accuracy 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we introduce a new notion related to auction and mechanism design called fair imposition in this setting a center wish to fairly and efficiently allocate task among a set of agent not knowing their cost structure a in the study of auction the main obstacle to overcome is the self interest of the agent which will in general cause them to hide their true cost unlike the auction setting however here the center ha the power to impose arbitrary behavior on the agent and furthermore wish to distribute the cost a fairly a possible among them we define the problem precisely present solution criterion for these problem the central of which is called k efficiency and present both positive result in the form of concrete protocol and negative result in the form of impossibility theorem concerning these criterion 
diagrammatic reasoning comprises phenomenon that range from the so called free ride e g almost immediate understanding of visually perceived relationship to convention about token such reasoning must involve cognitive process that are highly perceptual in content in the domain of mathematical proof where diagram have had a long history we have an opportunity to investigate in detail and in a controlled setting the various perceptual device and cognitive process that facilitate diagrammatically based argument this paper continues recent work by examining two kind of diagrammatic proof called category and by jamnik et al the first being one in which generalization of a diagram instance is implied and the second being one in which an infinite completion is represented by an ellipsis we provide explanation of why these proof work a semantics for ellipsis and conjecture about the underlying cognitive process that seem to resonate with such proof 
in this paper we consider the problem of cyclic schedule such a arise in manufacturing we introduce a new formulation of this problem that is a very simple modification of a standard job shop scheduling formulation and which enables u to use existing constraint reasoning technique to generate cyclic schedule we present evidence for the effectiveness of this formulation and describe extension for handling multiple capacity resource and for recovering from break in cyclic schedule 
market mechanism play a central role in ai a a coordination tool in multiagent system and a an application area for algorithm design mechanism where buyer are directly cleared with seller and thus do not require an external liquidity provider are highly desirable for electronic marketplace for several reason in this paper we study the inherent complexity of and design algorithm for clearing auction and reverse auction with multiple indistinguishable unit for sale we consider setting where bidder express their preference via price quantity curve and setting where the bid are price quantity pair we show that market with piecewise linear supply demand curve and non discriminatory pricing can always be cleared in polynomial time surprisingly if discriminatory pricing is used to clear the market the problem becomes complete even for step function curve if the price quantity curve are all linear then in most variant the problem admits a poly time solution even for discriminatory pricing when bidder express their preference with price quantity pair the problem is complete but solvable in pseudo polynomial time with free disposal the problem admits a poly time approximation scheme but no such approximation scheme is possible without free disposal we also present pseudo polynomial algorithm for xor bid and ofs bid and analyze the approximability 
we propose a method to gradually expand the move to consider at the node of game search tree the algorithm begin with an iterative deepening search using the minimal set of move and if the search doe not succeed iteratively widens the set of possible move performing a complete iterative deepening search after each widening when d esigning carefully the different set of possible move the a lgorithm can save some time in the game of go tree search a shown by experimental result 
a model for the formation of situation concept is described a characteristic of this form of concept formation is that it doe not require instructive feedback this render it suitable for concept formation by autonomous agent it is experimentally demonstrated that situation concept constructed independently by several agent can convey useful information between agent through a learned system of communication a relation wa found between the development of the learned system of communication and the duration of the situation 
many imaging system seek a good interpretation of the scene presented i e a plausible perhaps optimal mapping from aspect of the scene to real world object this paper address the issue of nding such likely mapping efficiently in general an interpretation policy specie when to apply which imaging operator which can range from low level edge detector and region grower through highlevel token combination rule and expectation driven objectdetectors given the cost of these operator and the distribution of possible image we can determine both the expected cost and expected accuracy of any such policy our task is to nd a maximally effective policy typically one with sufcient accuracy whose cost is minimal we explore this framework in several context including the eigenface approach to face recognition our result show in particular that policy which select the operator that maximize information gain per unit cost work more effectively than other policy including one that at each stage simply try to establish the putative most likely interpretation 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
for many application it is important that agent be not only correct but also comprehensible to human user typically people have tried to make agent behavior and reasoning understandable by adding post hoc special purpose explanation system with often disappointing result here i instead take the comprehensibility of agent behavior a a central agent design consideration from the start i describe an agent architecture the expressivator that support comprehensibility on top of a behavior based framework using four technical innovation structuring the agent s behavior according to the sign and signifier it is intended to communicate allowing the agent to keep track of it impression on the user with sign management using behavioral transition to explain the reason for agent behavior and expressing behavioral interrelationship directly using meta level control 
we present a formal characterization and semantic representation for a number of credulous inference relation based on the notion of an epistemic state it is shown in particular that credulous inference can be naturally represented in term of expectation see gardenfors and makinson we describe also the relationship between credulous and usual skeptical nonmonotonic inference and show how they can facilitate each other 
we present an extensive experimental study of consequence finding algorithm based on kernel resolution using both a trie based and a novel zbdd based implementation which us zerosuppressed binary decision diagram to concisely store and process very large clause set our study considers both the full prime implicate task and application of consequence finding for restricted target language in abduction model based and faulttree diagnosis and polynomially bounded knowledge compilation we show that the zbdd implementation can push consequence finding to a new limit solving problem which generate over clause 
in this paper we use a logic based system description for a simple non logic functional language to examine the way in which a diagnosis system can use it system description to improve debugging performance the key concept is that the notion of expression replacement which is the basis for repairing a program can also serve a a fundamental heuristic for searching the source of an error we formally define replacement in term of fault mode explicitly define a replacement order and use the replacement heuristic for finding diagnosis finally we incorporate the use of multiple test case and discus their use in discriminating between diagnosis 
much work in computer science ha adopted competitive analysis a a tool for decision making under uncertainty in this work we extend competitive analysis to the context of multi agent system unlike classical competitive analysis where the behavior of an agent s environment is taken to be arbitrary we consider the case where an agent s environment consists of other agent these agent will usually obey some minimal rationality constraint this lead to the definition of rational competitive analysis we introduce the concept of rational competitive analysis and initiate the study of competitive analysis for multi agent system we also discus the application of rational competitive analysis to the context of bidding game a well a to the classical one way trading problem 
shopbots are agent that automatically search the internet to obtain information about price and other attribute of good and service they herald a future in which autonomous agent profoundly influence electronic market in this study a simple economic model is proposed and analyzed which is intended to quan tify some of the likely impact of a proliferation of shopbots and other economically motivated software agent in addition this paper report on simulation of pricebots adaptive pricesetting agent which firm may well implement to combat or even take advantage of the grow ing community of shopbots this study form part of a larger research program that aim to provide insight into the impact of agent tech nology on the nascent information economy 
we introduce two mechanism for scaling computation in the framework of temporal reasoning the first one address abstraction at the methodological level operator are defined that engender flexible switching between different granularity of temporal representation structure the second one account for abstraction at the interface level of a temporal reasoning engine various generalization of temporal relation are introduced that approximate more fine grained representation by abstracting away irrelevant detail 
integrating the splitting rule into a saturation based theorem prover may be highly beneficial for solving certain class of fist order problem the use of splitting in the context of saturation based theorem proving based on explicit case analysis a implemented in spa employ backtracking which is difficult to implement a it affect design of the whole system here we present a quot cheap quot and efficient technique for implementing splitting that doe not use backtracking 
a theory of shape is important for ai both for recognition and description of object and for reasoning about the possible behaviour of ob jects theory of shape may be loosely clas sified a either volume based or outline based we present a theory of the latter type ini tially confined to two dimensional outline we represent outline by mean of string over an alphabet of seven qualitative curvature type and give a regular grammar which generates the string corresponding to possible outline we use subset of the curvature type alphabet to characterise cognitively salient subclass of outline with corresponding regular subgrammars and use decusping smoothing and merg ing operator to simplify outline for represen tation at coarser granularity we give an algo rithm for deriving the curvature sequence of an outline using only local information obtained a the outline is traversed finally we indicate how more detailed including quantitative in formation can be incorporated into the theory 
story generation is experiencing a revival despite disappointing preliminary result from the preceding three decade one of the principle reason for previous inadequacy wa the low level of writing quality which resulted from the excessive focus of story grammar on plot design although these system leveraged narrative theory via corpus analysis they failed to thoroughly extend those analysis to all relevant linguistic level the end result wa narrative that were recognizable a story but whose prose quality wa unsatisfactory however the blame for poor writing quality cannot be laid squarely at the foot of story grammar a natural language generation ha to date not fielded system capable of faithfully reproducing either the variety or complexity of naturally occurring story this paper present the author architecture for accomplishing precisely that task the storybook implementation of a narrative prose generator and a brief description of a formal evaluation of the story it produce 
we introduce two abstraction mechanism for streamlining the process of semantic interpretation configurational description of dependency graph increase the linguistic generality of interpretation schema while interfacing them to lexical and conceptual inheritance hierarchy reduces the amount and complexity of semantic specification 
metaquery also known a metapattem is a datamining tool useful for learning rule in volving more than one relation in the database a metaquery is a template or a second order proposition in a language l that describes the type of pattern to be discovered this tool ha already been successfully applied to several real world application in this paper we advance the state of the art in metaqueries research in several way first we analyze the related computational problem and classify it a np hard with a tractable sub set that is quite immediately evident second we argue that the notion of support for metaqueries where support is intuitively some indi cation to the relevance of the rule to be discovered is not adequately defined in the liter ature and propose our own definition third we propose some efficient algorithm for com puting support and present preliminary experi mental result that indicate that our algorithm are indeed quite useful 
we present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave one out error jaakkola and haussler proved for support vector machine svms vapnik the new approach directly minimizes the expression given by the bound in an attempt to minimize leave one out error this give a convex optimization problem which construct a sparse linear classifier in feature space using the kernel technique a such the algorithm posse many of the same property a svms the main novelty of the algorithm is that apart from the choice of kernel it is parameterless the selection of the number of training error is inherent in the algorithm and not chosen by an extra free parameter a in svms first experiment using the method on benchmark datasets from the uci repository show result similar to svms which have been tuned to have the best choice of parameter 
compilation to boolean satisfiability ha become a powerful paradigm for solving ai problem however domain that require metric reasoning cannot be compiled efficiently to sat even if they would otherwise benefit from compilation we address this problem by introducing the lcnf representation which combine propositional logic with metric constraint we present lpsat an engine which solves lcnf problem by interleaving call to an incremental simplex algorithm with systematic satisfaction method we describe a compiler which convert metric resource planning problem into lcnf for processing by lpsat the experimental section of the paper explores several optimization to lpsat including learning from constraint failure and randomized cutoff 
we introduce a framework for finding preference information to derive desired conclusion in nonmonotonic reasoning a new abductive framework called preference abduction enables u to infer an appropriate set of priority to explain the given observation skeptically thereby resolving the multiple extension problem in the answer set semantics for extended logic program preference abduction is also combined with a usual form of abduction in abductive logic programming and ha application such a specification of rule preference in legal reasoning and preference view update the issue of learning abducibles and priority is also discussed in which abduction to a particular cause is equivalent to abduction to preference 
abstract locating expertise source in a community of interest or practice is a critical need for distributed organization operating in knowledge intensive business sector this is specially true in those one that deal with innovation activites which have to manage knowledge about the creation of new knowledge finding fastly a suitable expert and knowing how to reach him or her can be seen a a way to gain advantage and speed up organizational knowledge creation and learning usually expertise location is done through the use of personal social or knowledge network and involves aspect such a trust and reputation however and specially in distributed organization relying on communication technology for cooperation each member of a community is just aware of it own personal social or knowledge network this make difficult to get to know other potential expert in the community which may pertain to other member network netexpert is an agent based expertise location system that replicates the process of social and knowledge network building at a community or organization level in so doing it is able to connect several network and put into contact expertise that otherwise would remain hidden keywords software agent multiagent system assistant agent expertise location localize expertise social 
certain planning system that deal with quantitative time constraint have used an underlying simple temporal problem solver to ensure temporal consistency of plan however many application involve process of uncertain duration whose timing cannot be controlled by the execution agent these case require more complex notion of temporal feasibility in previous work various controllability property such a weak strong and dynamic controllability have been defined the most interesting and useful controllability property the dynamic one ha ironically proved to be the most difficult to analyze in this paper we resolve the complexity issue for dynamic controllability unexpectedly the problem turn out to be tractable we also show how to efficiently execute network whose status ha been verified 
we propose a casa labelling method of the tf representation which is based on the periodicity of the speech related to the voicing a local voicing index is estimated in four subbands after demodulation of the signal this index is used a a reliability measure for both pitch identification and speech recognition first this model allows robust f identification thanks to the voicing index which is a consistent reliability measure associated to the f measure since the task is to recognise speech corrupted with additive noise the periodicity is specific to the signal in our model the evaluation of speech reliability is not direct it also depends on a priori knowledge about the relationship between snr and the voicing index the goal is to assign to each tf region a probability clean enough to feed a multistream recogniser only adapted to clean data the model is able to localise region where the target speech dominates over the background noise this probability is evaluated according to a function established a priori the snr feature mapping and the choice of a snr decision threshold this is adapted to a new multistream recognition approach since the previous probability serve to weight the stream posterior 
we describe an interactive system which support the exploration of argument generated from bayesian network in particular we consider key feature which support interactive behaviour an attentional mechanism which update the activation of concept a the interaction progress a set of exploratory response and a set of probabilistic pattern and an argument grammar which support the generation of natural language argument from bayesian network a preliminary evaluation ass the effect of our exploratory response on user s belief 
this paper describes a new layered brain architecture for simulated autonomous and semi autonomous creature that inhabit graphical world the main feature of the brain is it division into distinct system which communicate through common access to an internal mental blackboard the brain wa designed to encourage experimentation with various system and architecture it ha so far proven flexible enough to accommodate research advancing in a number of different direction by a small team of researcher 
we present a tool for the annotation of xmlencoded multi modal language corpus nonhierarchical data is supported by mean of standoff annotation we define base level and suprabase level element and theory independent markables for multi modal annotation and apply them to a cospecification annotation scheme we also describe how arbitrary annotation scheme can be represented in term of these element apart from theoretical consideration however the development of a fast robust and highly usable annotation tool wa a major objective of the work presented 
propagating constraint is the main feature of anyconstraint solver this is thus of prime importanceto manage constraint propagation a efficiently aspossible justifying the use of the best algorithm but the ease of integration is also one of the concernswhen implementing an algorithm in a constraintsolver this paper focus on ac whichis the simplest arc consistency algorithm knownso far we propose two refinement that preserveas much a possible the ease of 
new method to generate hard random problem instance have driven progress on algorithm for deduction and constraint satisfaction recently achlioptas et al aaai introduced a new generator based on latin square that creates only satisfiable problem and so can be used to accurately test incomplete one sided solver we investigate how this and other generator are biased away from the uniform distribution of satisfiable problem and show how they can be improved by imposing a balance condition more generally we show that the generator is one member of a family of related model that generate distribution ranging from one that are everywhere tractable to one that exhibit a sharp hardness threshold we also discus the critical role of the problem encoding in the performance of both systematic and local search solver 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we present a formal framework for treating both incomplete information in the initial database and possible failure during an agent s execution of a course of action these two aspect of uncertainty are formalized by two different notion of probability we introduce also a concept of expected probability which is obtained by combining the two previous notion expected probability account for the probability of a sentence on the hypothesis that the sequence of action needed to make it true might have failed expected probability lead to the possibility of comparing course of action and verifying which is more safe 
the majority of existing language generation system have a pipeline architecture which offer efficient sequential execution of module but doe not allow decision about text content to be revised in later stage however a exemplified in this paper in some case choosing appropriate content can depend on text length and formatting which in a pipeline architecture are determined after content planning is completed unlike pipeline interleaved and revision based architecture can deal with such dependency but tend to be more expensive computationally since our system need to generate acceptable hypertext explanation reliably and quickly the pipeline architecture wa modified instead to allow additional content to be requested in later stage of the generation process if necessary 
the paper describes a branch and bound scheme that us heuristic generated mechanically by the mini bucket approximation this scheme is presented and evaluated for optimization task such a finding the most probable explanation mpe in bayesian network the mini bucket scheme yield monotonic heuristic of varying strength which cause different amount of pruning allowing a controlled tradeoff between preprocessing and search the resulting branch and bound with mini bucket heuristic bbmb is evaluated using random network probabilistic decoding and medical diagnosis network result show that the bbmb scheme overcomes the memory explosion of bucket elimination allowing a gradual tradeoff of space for time and of time for accuracy 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
this paper study the role of two mechanism memory and balance to exploit the arm race resulting from predator prey interaction when solving a given problem memory ensures that individual are not only well adapted to the current member of the opposite population but also to earlier generation of opponent a balanced co evolution on the other hand adapts the speed of evolution i e the reproduction rate to the performance of a population it lead to a steady progress in both population indirectly a balanced co evolution avoids a premature loss of genetic diversity this in turn diminishes the need for a long memory span the current paper show how both mechanism can be incorporated in coevolutionary genetic algorithm cgas empirical result support the importance of and interaction between both mechanism 
we present a methodology for achieving cooperation between already existing theorem provers employing different proof paradigm and or different search control and using dif ferent but related logic cooperation between the provers is achieved by periodically inter changing clause which are selected by so called referee by employing referee both on the side of a sending prover and a receiving prover the communication is both successand demanddriven which result in a rather small commu nication overhead and synergetical effect we report on experiment regarding the coop eration of the provers spa setheo and discount in domain of the tptp library and with problem stemming from an application in software component retrieval the experiment show significant improvement in the number of problem solved a well a in the solution time 
when several agent learn concurrently the payoff received by an agent is dependent on the behavior of the other agent a the other agent learn the reward of one agent becomes non stationary this make learning in multiagent systemsmore difficult than single agent learning a few method how ever are known to guarantee convergence to equilibrium in the limit in such system in this paper we experimentally study one such technique the minimax q in a competitive domain and prove it equivalence with another well known method for competitive domain we study the rate of convergence of minimax q and investigate possible way for increasing the same we also present a variant of the algorithm minimax sarsa and prove it convergence to minimax q value under appropriate condition finally we show that this new algorithm performs better than simple minimax q in a general sum domain a well 
keyphrases are an important mean of document summarization clustering and topic search only a small minority of document have author assigned keyphrases and manually assigning keyphrases to existing document is very laborious therefore it is highly desirable to automate the keyphrase extraction process this paper show that a simple procedure for keyphrase extraction based on the naive bayes learning scheme performs comparably to the state of the art it go on to explain how this procedure s performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand result on a large collection of technical report in computer science show that the quality of the extracted keyphrases improves significantly when domain specific information is exploited 
in the field of spatial reasoning point to point relation have been thoroughly examined but only little attention ha been payed to the modeling of path relation we propose a computational model that extends the existing referential semantics for point to point relation to path relation on the linguistic side we present some research on german path preposition a well a result on their english counterpart this analysis of path preposition is used to extract a semantic model for path relation on the geometric side we examine the characteristic of trajectory and propose a computational method to find an appropriate path relation for a given situation finally we show how our finding on the linguistic and the geometric side can be brought together to form a consistent model 
there is a lot of research on formalization of intention the common idea of these theory is to interprete intention a an unary modal operator in kripkean semantics these theory suffer from the side effect problem seriously we introduce an alternative approach by establishing a nonclassical logic of intention this logic is based on a novel non kripkean semantics which embodies some cognitive feature we show that this logic doe provide a formal specification and a decidable inference mechanism of intention consequence all and only the instance of sideeffects except one in absorbent form are forbidden in the logic 
a lot of recent research ha focused on method of modeling web user and on efficient way to initialize and manage user model in this paper we present a new user modeling technique relying on a temporal graph based data model for semistructured information our technique can be used to filter www information and to make the web experience personalized for the individual user moreover such graph based data model also offer appropriate query language which allow user defined query either over the whole web site or over the personalized user site view 
in this paper we construct a new concept description language intended for representing dynamic and intensional knowledge the most important feature distinguishing this language from it predecessor in the literature is that it allows application of modal operator to all kind of syntactic term concept role and formula moreover the language may contain both local i e state dependent and global i e state independent concept role and object all this provides u with the most complete and natural mean for reflecting the dynamic and intensional behaviour of application domain we construct a satisfiability checking mosaic type algorithm for this language based on alc in i arbitrary multimodal frame ii frame with universal accessibility relation for knowledge and iii frame with transitive symmetrical and euclidean relation for belief on the other hand it is shown that the satisfaction problem becomes undecidable if the underlying frame are arbitrary linear order or the language contains the common knowledge operator for n agent 
localization is one of the most important capability for autonomous mobile agent markov localization ml applied to dense range image ha proven to be an effective technique but it computational and storage requirement put a large burden on robot system and make it difficult to update the map dynamically in this paper we introduce a new technique based on correlation of a sensor scan with the map that is several order of magnitude more efficient than ml cbml correlation based ml permit video rate localization using dense range scan dynamic map update and a more precise error model than ml in this paper we present the basic method of cbml and validate it efficiency and correctness in a series of experiment on an implemented mobile robot base 
information retrieval query often result in a large number of document found to be relevant these document are usually sorted by relevance not by an analysis of what the user meant if the document collection contains many document on one of those meaning it is hard to find other document we present a technique called conceptual grouping that automatically distinguishes between different meaning of a user query given a document collection by analysing a word co occurrence network of a text database we are able to form group of word related to the query grouped by semantic coherence these group are used to reorganise the result according to what the user ha meant by his query testing show that this automated technique can improve precision help user find what they need more easily and give them a semantic overview of the document collection 
a large portion of real world data is stored in commercial relational database system in contrast most statistical learning method work only with flat data representation thus to apply these method we are forced to convert our data into a flat form thereby losing much of the relational structure present in our database this paper build on the recent work on probabilistic relational model prms and describes how to learn them from database prms allow the property of an object to depend probabilistically both on other property of that object and on property of related object although prms are significantly more expressive than standard model such a bayesian network we show how to extend well known statistical method for learning bayesian network to learn these model we describe both parameter estimation and structure learning the automatic induction of the dependency structure in a model moreover we show how the learning procedure can exploit standard database retrieval technique for efficient learning from large datasets we present experimental result on both real and synthetic relational database 
in this paper we describe a logic based ai architecturebased on brook subsumption architecture we axiomatizeeach of the layer of control in his system separatelyand use independent theorem provers to deriveeach layer s output given it input we implementthe subsumption of lower layer by higher layer usingcircumscription to make assumption in lower levelsand nonmonotonically retract them when higher levelscome up with some new conclusion we give formalsemantics to our 
statistic based classifier in natural language are developed typically by assuming a generative model for the data estimating it parameter from training data and then using bayes rule to obtain a classifier for many problem the assumption made by the generative model are evidently wrong leaving open the question of why these approach work this paper present a learning theory account of the major statistical approach to learning in natural language a class of linear statistical query lsq hypothesis is defined and learning with it is shown to exhibit some robustness property many statistical learner used in natural language including naive bayes markov model and maximum entropy model are shown to be lsq hypothesis explaining the robustness of these predictor even when the underlying probabilistic assumption do not hold this coherent view of when and why learning approach work in this context may help to develop better learning method and an understanding of the role of learning in natural language inference 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
a number of experimental study e g geldard and sher rick kolers and von gr unau have suggested that interpretation of event can override direct sensory evi dence for example for some sequence of perceptual event of short duration the interpretation of individual event in the sequence depends on the characteristic of the sequence a a whole this backwards referral in time in which later event influence the perception of earlier event is difficult to account for within a serial model of cognition without incor porating implausible delay basically delaying sensory expe rience until all the data is in dennett and kinsbourne have proposed the multiple draft theory a a way of modelling such cognitive process the multiple draft theory is based on a paral lel distributed view of cognition in which large number of process work independently on multiple interpretation of data simultaneously these are the multiple draft eventu ally a single draft may become dominant but no draft is ever entirely safe from revision 
this paper present a generalized associative memory model which store a collection of tuples whose component are set rather than scalar it is shown that all library pattern are stored stably on the other hand spurious memory may develop application of this model to storage and retrieval of naturally arising generalized sequence in bioinformatics are presented the model is shown to work well for detection of novel generalized sequence against a large database of stored sequence and for removal of noisy black pixel in a probe image against a very large set of stored image 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
a fundamental difficulty faced by group of agent that work together is how to efficiently coordinate their effort this paper present technique that allow heterogeneous agent to more efficiently solve coordination problem by acquiring procedural knowledge in particular each agent autonomously learns coordinated procedure that reflect her contribution towards successful past joint behavior empirical result validate the significant benefit of coordinated procedure 
the symmetric alldiff constraint is a particular case of the all diff constraint a case in which variable and value are defined from the same set that is every variable represents an element c of s and it value represent the element of s that are compatible with c this constraint requires that all the value taken by the variable are different similar to the classical all diff constraint and that if the variable representing the element i is assigned to the value representing the element j then the variable representing the element j is assigned to the value representing the element this constraint is present in many real world problem such sport scheduling where it express match between team in this paper we show how to compute the arc consistency of this constraint in o n m m i d i where n is the number of involved variable and d i the domain of the variable i we also propose a filtering algorithm of le complexity o m 
the self organizing map is a very popular unsupervisedneural network model for the analysisof high dimensional input data a it istypically found in information retrieval application however the interpretation of themap requires much manual effort especially asfar a the analysis of the learned feature andthe characteristic of identified cluster is concerned in this paper we present our novel labelsom method which based on the featureslearned by the map automatically selects 
a general framework for update based planning is presented we first give a new family of dependence based update operator that are wellsuited to the representation of simple action and we identify the complexity of query entailment from an updated belief base then we introduce conditional nondeterministic and concurrent update so a to encode the corresponding type of action effect plan verification and existence are expressed in this update based framework 
description logic are formalism for the representation of and reasoning about conceptual knowledge on an abstract level concrete domain allow the integration of description logic reasoning with reasoning about concrete object such a number time interval or spatial region the importance of this combined approach especially for building real world application is widely accepted however the complexity of reasoning with concrete domain ha never been formally analyzed and efficient algorithm have not been developed this paper close the gap by providing a tight bound for the complexity of reasoning with concrete domain and presenting optimal algorithm 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
sqca is an implemented technique for the semi quantitative comparative analysis of dynamical system it is both able to deal with incompletely specified model and make precise prediction by exploiting semiquantitative information in the form of numerical bound on the variable and function occuring in the model the technique ha a solid mathematical foundation which facilitates proof of correctness and convergence property 
the paper report on experiment in which autonomous visually grounded agent bootstrap an ontology and a shared lexicon without prior design nor other form of human intervention the agent do so while playing a particular languagegame called the guessing game we show that synonymy and polysemy arise a emergent property in the language but also that there are tendency to dampen it so a to make the language more coherent and thus more optimal from the viewpoint of communicative success cognitive complexity and learnability 
we study the problem of automatically generating an integrated schema for xml dtds introducing a novel view inference approach we show that the set of view and source description can be automatically derived introduction the problem of information integration ha become significant a the growing number of information source on the internet information integration system provide user with an integrated schema of underlying source the integrated schema is designed by hand and a mapping between the integrated schema and the source schema is needed for the system to answer query a xml bray paoli sperberg mcqueen ha become a new standard for representation and exchange of data on the internet in this article we consider the problem of automatically generating an integrated schema for different xml dtds with similar document type architecture xml with a dtd is self descriptive and provides a semistructured data model these property render that dtds defining similar document type have structural and naming similarity given a collection of source dtds we propose a view inference approach which automatically derives the set of integrated view and source description 
there are now numerous agent application that track interest of thousand of user in situation where change occur continuously shim et al suggested that such agent can be made efficient by merging commonality in their activity however past algorithm cannot merge more than or concurrent activity we develop technique so that a large number of concurrent activity typically over can be partitioned into component group of activity of small size e g to so that each component s activity can be merged using previously developed algorithm e g shim et al we first formalize the problem and show that finding optimal partition is nphard we then develop three algorithm greedy based and bab branch and bound based and bab are both guaranteed to compute optimal solution greedy on the other hand us heuristic and typically find suboptimal solution we implemented all three algorithm we experimentally show that the greedy algorithm find partition whose cost are at most worse than that found by based and bab however greedy is able to handle over thousand concurrent request very fast while the other two method are much slower and able to handle only request hence greedy appears to be the best 
case based reasoning cbr is concerned with solving new problem by adapting solution that worked for similar problem in the past year of experience in building and fielding cbr system have shown that the rase approach is not free from problem it ha been realized that the knowledge engineering effort required for designing many real world easebases can be prohibitively high based on the wide spread use of database and powerful machine learning method some cbr researcher have been investigating the possibility of designing casebases automatically this paper proposes a flexible model for the automatic discovery of abstract case from data base based on the lattice machine it also proposes an efficient and effective algorithm for retrieving such case besides the known benefit associated with abstract case the main advantage of this approach are that the discovery process is fully automated no knowledge engineering cost 
we analyze economic efficiency and equilibrium property in decentralized task allocation problem involving hierarchical dependency and resource contention we bound the inefficiency of a type of approximate equilibrium in proportion to the number of agent and the bidding parameter in a particular market protocol this protocol converges to an approximate equilibrium with respect to all agent except those which may acquire unneeded input we introduce a decommitment phase to allow such agent to decommit from their input contract experiment indicate that the augmented market protocol produce highly efficient allocation on average 
this paper present the permitted relation between two rectangle whose side are parallel to the ax of some orthogonal basis in a dimensional euclidean space elaborating rectangle algebra just like interval algebra it defines the concept of convexity a well a the one of weak preconvexity and strong preconvexity it introduces afterwards the fundamental operation of intersection composition and inversion and demonstrates that the concept of weak preconvexity is preserved by the operation of composition whereas the concept of strong preconvexity is preserved by the operation of intersection finally fitting the propagation technique conceived to solve interval network it show that the polynomial path consistency algorithm is a decision method for the problem of proving the consistency of strongly preconvex rectangle network 
partially observable markov decision process pomdps provide a coherent mathematical framework for planning under uncertainty when the state of the system cannot be fully observed however the problem of finding an exact pomdp solution is intractable computing such solution requires the manipulation of a piecewise linear convex value function which specifies a value for each possible belief state this value function can be represented by a set of vector each one with dimension equal to the size of the state space in nontrivial problem however these vector are too large for such a representation to be feasible preventing the use of exact pomdp algorithm we propose an approximation scheme where each vector is represented a a linear combination of basis function to provide a compact approximation to the value function we also show that this representation can be exploited to allow for efficient computation in approximate value and policy iteration algorithm in the context of factored pomdps where the transition model is specified using a dynamic bayesian network 
this paper describes a method for structuring a robot motor learning task by designing a suitably parameterized policy we show that a simple search algorithm along with biologically motivated constraint offer an effective mean for motor skill acquisition the framework make use of the robot counterpart to several element found in human motor learning imitation equilibrium point control motor program and synergy we demonstrate that through learning coordinated behavior emerges from initial crude knowledge about a difficult robot weightlifting task 
a component based generic agent architecture for multi attribute integrative negotiation is introduced and it application is described in a prototype system for negotiation about car developed in co operation with among others dutch telecom kpn the approach can be characterised a co operative one to one multi criterion negotiation in which the privacy of both party is protected a much a possible 
the task of causal structure discovery from empirical data is a fundamental problem in many area experimental data is crucial for accomplishing this task however experiment are typically expensive and must be selected with great care this paper us active learning to determine the experiment that are most informative towards uncovering the underlying structure we formalize the causal learning task a that of learning the structure of a causal bayesian network we consider an active learner that is allowed to conduct experiment where it intervenes in the domain by setting the value of certain variable we provide a theoretical framework for the active learning problem and an algorithm that actively chooses the experiment to perform based on the model learned so far experimental result show that active learning can substantially reduce the number of observation required to determine the structure of a domain 
much excitement ha been generated by the recent success of stochastic local search procedure at finding satisfying assignment to large formula many of the problem on which these method have been effective are non boolean in that they are most naturally formulated in term of variable with domain size greater than two to tackle such a problem with a boolean procedure the problem is first reformulated a an equivalent boolean problem this paper introduces and study the alternative of extending a boolean stochastic local search procedure to operate directly on non boolean problem it then compare the non boolean representation to three boolean representation and present experimental evidence that the non boolean method is often superior for problem with large domain size 
this paper present part of an on going project to integrate perception attention drive emotion behavior arbitration and expressive act for a robot designed to interact socially with human we present the design of a visual attention system based on a model of human visual search behavior from wolfe the attention system integrates perception motion detection color saliency and face popouts with habituation effect and influence from the robot s motivational and behavioral state to create a context dependent attention activation map this activation map is used to direct eye movement and to satiate the drive of the motivational system 
constructing good model for chemical carcinogenesis wa identified in ijcai a providing a substantial challenge to knowledge discovery program attention wa drawn to a comparative exercise which called for prediction on the outcome of rodent carcinogenicity bioassay this the predictive toxicology evaluation or pte challenge wa seen to provide ai program with an opportunity to participate in an enterprise of scientific merit and a yardstick for comparison against strong competition here we provide an assessment of the machine learning ml submission made model submitted are assessed on their accuracy in comparison to model developed with expert collaboration and their explanatory value for toxicology the principal finding were a using structural information available from a standard modelling package layman devised feature and outcome of established biological test result from mlderived model were at least a good a those with expert derived technique this wa surprising b the combined use of structural and biological feature by ml derived model wa unusual and suggested new avenue for toxicology modelling this wa also unexpected and c significant effort wa required to interpret the output of even the most symbolic of ml derived model much of this could have been alleviated with measure for converting the result into a more toxicology friendly form a it stand their absence is sufficient to prevent a whole hearted acceptance of these promising method by toxicologist this suggests that ml technique have been able to respond not fully but nevertheless substantially to the pte challenge 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this paper we take the spatial semantic hierarchy a the agent s target spatial representation and use a circumscriptive theory to specify the minimal model associated with this representation we provide a logic program to calculate the model of the proposed theory we also illustrate how the different level of the representation assume different spatial property about both the environment and the action performed by the agent these spatial property play the role of filter the agent applies in order to distinguish the different environment state it ha visited 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
decision making in multiagent setting requires significant computational resource agent need to model each other to decide how to coordinate this sometimes may require solving nested model of many other agent and may be impractical to perform in an acceptable time in this paper we investigate way in which the agent can be equipped with flexible decision making procedure to allow multiagent decision making under time pressure one of the technique we implemented us iterative deepening algorithm guided by performance profile generated offline when the interaction involves many agent the algorithm iteratively enhances the quality of coordinated decision making by incrementally adding the level of nesting considered but with the additional penalty of increased running time to identify the appropriate scope of modeling online we use the concept of urgency which represents cost of delaying decision we validate our framework with experiment in a simulated anti air defense domain the contribution of our framework is that it endows our autonomous agent with flexibility to cope with time pressure in complex multiagent setting 
this paper present a real time auditory and visual tracking of multiple object for humanoid under real world environment real time processing is crucial for sensorimotor task in tracking and multiple object tracking is crucial for real world application multiple sound source tracking need perception of a mixture of sound and cancellation of motor noise caused by body movement however it real time processing ha not been reported yet real time tracking is attained by fusing information obtained by sound source localization multiple face recognition speaker tracking focus of attention control and motor control auditory stream with sound source direction are extracted by active audition system with motor noise cancellation capability from khz sampling sound visual stream with face id and d position are extracted by combining skincolor extraction correlation based matching and multiple scale image generation from a single camera these auditory and visual stream are associated by comparing the spatial location and associated stream are used to control focus of attention auditory visual and association processing are performed asynchronously on different pc s connected by tcp ip network the resulting system implemented on an upper torso humanoid can track multiple object with the delay of msec which is forced by visual tracking and network latency 
decision tree induced from stored case are increasingly used to guide case retrieval in case based reasoning cbr system for fault diagnosis and product recommendation in this paper we refer to such a decision tree a an identification tree when a often in practice each of the fault to be identified or available product is represented by a single case in the case library we evaluate common splitting criterion for decision tree in the special case of identification tree we present simplified version of those that are most effective in reducing the average path length of an identification tree or equivalently the average number of question asked when the tree is used for problem solving we also identify condition in which no such reduction is possible with any splitting criterion 
we present an empirical analysis of optimization technique devised to speed up the so called tbox classification supported by description logic system which have to deal with very large knowledge base e g containing more than concept introduction axiom these technique are integrated into the race architecture which implement a tbox and abox reasoner for the description logic alcnhr the described technique consist of adaption of previously known a well a new optimization technique for efficiently coping with these kind of very large knowledge base the empirical result presented in this paper are based on experience with an ontology for the unified medical language system and demonstrate a considerable runtime improvement they also indicate that appropriate description logic system based on sound and complete algorithm can be particularly useful for very large knowledge base 
the approach of multi perspective enterprise modelling is now more commonly accepted and used in practice a a way to manage knowledge than ever before however the concept of applying multiple modelling language to describe the same domain may still sound frightening to many in addition to the cost time and complexity involved problem such a knowledge sharing between multiple model and achieving and maintaining integrity between them are also important we argue that multi perspective enterprise modelling is helpful and in some situation necessary this paper give example of how formal method such a logical language can provide assistance in making such an approach more appealing and transparent we suggest that the mpm approach is valuable in representing understanding and analysing a complex domain but that much automated support is needed key word knowledge management knowledge sharing multi perspective modelling business modelling bsdm enterprise modelling process modelling knowledge based support tool business process re engineering role activity and communication diagram 
this paper proposes new result in the fieldof software assistant helping user of interactivetools in the task of automatically performingrepetitive task we propose an innovativeintegration of such an assistant intoan interactive programming environment inthis context learning to recognize situation inwhich repetitive task occur is difficult becauselanguages describing user action are complexand because fast learning is mandatory toachieve this goal we propose an 
boolean linear program blps are ubiquitous in ai satisfiability testing planning with resource constraint and winner determination in combinatorial auction are all example of this type of problem although increasingly well informed by work in or current ai research ha tended to focus on specialized algorithm for each type of blp task and ha only loosely patterned new algorithm on effective method from other task in this paper we introduce a single general purpose local search procedure that can be simultaneously applied to the entire range of blp problem without modification although one might suspect that a general purpose algorithm might not perform a well a specialized algorithm we find that this is not the case here our experiment show that our generic algorithm simultaneously achieves performance comparable with the state of the art in satisfiability search and winner determination in combinatorial auction two very different blp problem our algorithm is simple and combine an old idea from or with recent idea from ai 
a family of resource bounded paraconsistent inference relation is introduced these relation are based on s entailment an inference relation logically weaker than classical entailment and parametrized by a set s of variable their property are investigated especially from the computational complexity point of view among the strong feature of our framework is the fact that tractability is ensured each time s is bounded and that binary connective behave in a classical manner moreover our family is large enough to include both s inference the standard inference relation based on the selection of consistent subbase and some additional form of paraconsistent reasoning a specific case 
we propose a new model for representing and revising belief structure which relies on a notion of partial language splitting and tolerates some amount of inconsistency while retaining classical logic the model preserve an agent s ability to answer query in a coherent way using belnap s four valued logic axiom analogous to the agm axiom hold for this new model the distinction between implicit and explicit belief is represented and psychologically plausible computationally tractable procedure for query answering and belief base revision are obtained 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
many large markov decision process mdps can be represented compactly using a structured representation such a a dynamic bayesian network unfortunately the compact representation doe not help standard mdp algorithm because the value function for the mdp doe not retain the structure of the process description we argue that in many such mdps structure is approximately retained that is the value function are nearly additive closely approximated by a linear function over factor associated with small subset of problem feature based on this idea we present a convergent approximate value determination algorithm for structured mdps the algorithm maintains an additive value function alternating dynamic programming step with step that project the result back into the restricted space of additive function we show that both the dynamic programming and the projection step can be computed efficiently despite the fact that the number of state is exponential in the number of state variable 
a decentralized multiagent system comprises agent who act autonomously based on local knowledge achieving coordination in such a system is nontrivial hut is essential in most application where disjointed or incoherent behavior would be undesirable coordination in decentralized system is a richer phenomenon than previously believed in particular five major attribute are crucial the extent of the local knowledge and choice of the member agent the extent of their shared knowledge the level of their inertia and the level of precision of the required coordination interestingly precision and inertia turn out to control the coordination process they define different region within each of which the other attribute relate nicely with coordination but among which their relationship are altered or even reversed based on our study we propose simple design rule to obtain coordinated behavior in decentralized multiagent system 
user waiting time for information on the www may be reduced by pre sending document they are likely to request albeit at a possible expense of additional transmission cost in this paper we describe a prediction model which anticipates the document a user is likely to request next and present a decision theoretic approach for pre sending document based on the prediction made by this model we introduce two evaluation method which measure the immediate and the eventual benefit of pre sending a document we use these evaluation method to compare the performance of our decision theoretic policy to that of a naive pre sending policy and to identify the domain parameter configuration for which each of these policy provides a clear overall benefit to the user 
qualitative and quantitative representation of space in general and motion in particular have their typical field of application which are unified in an autonomously moving robot interacting with human being therefore it is necessary to make some consideration on both approach when dealing with such a robot this paper present quantitative and qualitative representation of locomotion and algorithm to deal with them this work wa applied to the navigation of a semi autonomous wheelchair along route in network of corridor 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we expect a variety of autonomous system from rover to life support system to play a critical role in the success of future space mission the crew and ground support personnel will want to control and be informed by these system at varying level of detail depending on the situation moreover these system will need to operate safely in the presence of people and cooperate with them effectively we call such autonomous system human centered in contrast with traditional black box autonomous system our goal is to design a framework for human centered autonomous system that enables user to interact with these system at whatever level of control is most appropriate whenever they so choose but minimize the necessity for such interaction this paper discus on going research at the nasa ames research center and the johnson space center in developing human centered autonomous system that can be used for space mission 
we demonstrate multiagent control of modular self reconfigurable msr robot for object manipulation task and show how it provides a useful programming abstraction such robot consist of many module that can move relative to each other and change their connectivity thereby changing the robot s overall shape to suit different task we illustrate this approach through simulation experiment of the telecube msr robot system 
finding a solution to the frame problem that is robust in the presence of action with indirect effect ha proven to be a difficult task example that feature the instantaneous propagation of interacting indirect effect are particularly taxing this article show that an already widely known predicate calculus formalism namely the event calculus can handle such example with only minor enhancement 
three method for improving the generalization performance of linear support vector machine are proposed and this in the case that some dimension in the data can be considered irrelevant for the pattern recognition task at hand in contrast to other method the generalization improvement is not obtained by modifying the system but by modifying the training set itself the first proposed method guarantee faultless performance and can be applied in the case of a small number of irrelevant dimension the two other proposed method are approximation of this method for the case of larger number of irrelevant dimension in order to test these approximation the method are applied to a real world application d object recognition without segmentation in this case the image of the object contain a background that should be made irrelevant for the recognition task the approximative method that us a pair of black white background a training set give rise to excellent result we are able to report on correct recognition rate of unseen view of d object placed on a wide variety of cluttered background that never are worse than for an set size of object this is compared to the performance of svms without this training method in this case the performance can drop below on specific background 
this paper we look at the relationship between thedesign of agent and the design of aa for an intelligent system we present guideline to help agent designer build agent thatare easier to use in system with aa intelligent agentsmany of the recent exciting development in artificial intelligence ai have centered around the concept of an agent anagent is an autonomous entity that sens it environment andacts intelligently and pro actively towards it goal an 
in text classification most technique use bag of word to represent document the main problem is to identify what word are best suited to classify the document in such a way a to discriminate between them feature selection technique are then needed to identify these word the feature selection method presented in this paper is rather simple and computationally efficient it combine a well known feature selection criterion the information gain and a new algorithm that selects and add a feature to a bag of word if it doe not occur too often with the feature already in a small set composed of the best feature selected so far for their high information gain in brief it try to avoid considering feature whose discrimination capability is sufficiently covered by already selected feature reducing in size the set of the feature used to characterize the document set this paper present this feature selection method and it result and how we have predetermined some of it parameter through experimentation 
computing the least common subsumer lcs in description logic is an inference task first introduced for sublanguages of classic roughly speaking the lcs of a set of concept description is the most specific concept description that subsumes all of the input description a such the lcs allows to extract the commonality from given concept description a task essential for several application like e g inductive learning information retrieval or the bottom up construction of kr knowledge base previous work on the lcs ha concentrated on description logic that either allow for number restriction or for existential restriction many application however require to combine these constructor in this work we present an lcs algorithm for the description logic alen which allows for both constructor a well a concept conjunction primitive negation and value restriction the proof of correctness of our lcs algorithm is based on an appropriate structural characterization of subsumption in alen also introduced in this paper 
problem solving method provide reusable architecture and component for implementing the reasoning part of knowledge based system the unified problem solving method development language upml ha been developed to describe and implement such architecture and component and to facilitate their semiautomatic reuse and adaptation in a nutshell upml is a framework for developing knowledge intensive reasoning system based on library of generic problem solving component the paper describes the component architectural constraint development guideline and tool provided by upml our focus is hereby on the meta ontology that ha been developed to formalize the architectural structure and element of upml 
information sharing is important for different goal such a sharing reputation of seller among potential buyer load balancing solving technical problem etc in the short run providing information a a response to query is often unbeneficial in the long run mechanism that enable beneficial stable strategy for information exchange can be found this paper present such mechanism and specifies under which condition it is beneficial to the agent to answer query we analyze a model of repeated encounter in which two agent ask each other queriesover time we present different strategy that enable information exchange and compare them according to the expected utility for the agent and the condition required for the cooperative equilibrium to exist 
method to avoid overfitting fall into two broad category data oriented using separate data for validation and representation oriented penalizing complexity in the model both have limitation that are hard to overcome we argue that fully adequate model evaluation is only possible if the search process by which model are obtained is also taken into account to this end we recently proposed a method for process oriented evaluation p e and successfully applied it to rule induction domingo b however for the sake of simplicity this treatment made a number of rather artificial assumption in this paper the assumption are removed and a simple formula for error estimation is obtained empirical trial show the new better founded form of poe to be a accurate a the previous one while further reducing theory size 
in automated negotiation system consisting of self interested agent contract have traditionally been binding leveled commitment contract i e contract where each party can decommit by paying a predetermined penalty were recently shown to improve pareto efficiency even if agent rationally decommit in nash equilibrium using inflated threshold on how good their outside offer must be before they decommit this paper operationalizes the four leveled commitment contracting protocol by presenting algorithm for using them algorithm are presented for computing the nash equilibrium decommitting threshold and decommitting probability given the contract price and the penalty existence and uniqueness of the equilibrium are analyzed algorithm are also presented for optimizing the contract itself price and penalty existence and uniqueness of the optimum are analyzed using the algorithm we offer a contract optimization service on the web a part of mediator our next generation electronic commerce server finally the algorithm are generalized to contract involving more than two agent 
combinatorial auction i e auction where bidder can bid on combination of item tend to lead to more efficient allocation than traditional auction in multi item auction where the agent valuation of the item are not additive however determining the winner so a to maximize revenue is np complete we present a search algorithm for optimal winner determination experiment are shown on several bid distribution the algorithm allows combinatorial auction to scale up to significantly larger number of item and bid than prior approach to optimal winner determination by capitalizing on the fact that the space of bid is necessarily sparsely populated in practice we do this via provably sufficient selective generation of child in the search and by using a method for fast child generation heuristic that are accurate and optimized for speed and four method for preprocessing the search space 
lp is a covering algorithm for adaptive information extraction from text ie it induces symbolic rule that insert sgml tag into text by learning from example found in a user defined tagged corpus training is performed in two step initially a set of tagging rule is learned then additional rule are induced to correct mistake and imprecision in tagging induction is performed by bottom up generalization of example in the training corpus shallow knowledge about natural language processing nlp is used in the generalization process the algorithm ha a considerable success story from a scientific point of view experiment report excellent result with respect to the current state of the art on two publicly available corpus from an application point of view a successful industrial ie tool ha been based on lp real world application have been developed and license have been released to external company for building other application this paper present lp experimental result and application and discus the role of shallow nlp in rule induction 
the paper describes the application of a probabilisticlocalization technique for indoor mobilerobot navigation based on a qualitative topological representation of the environment from the characterization of the uncertaintiesrelated to motor behavior and landmark recognition a markov based localization system hasbeen developed which maintains and update aprobability distribution for all landmark containedin a topological map a reliable measureof successful localization 
qualitative probabilistic network represent probabilistic influence between variable due to the level of representation detail provided knowledge about influence that hold only in specific context cannot be expressed the result computed from a qualitative network a a consequence can be quite weak and uninformative we extend the basic formalism of qualitative probabilistic network by providing for the inclusion of context specific information about influence and show that exploiting this information upon inference ha the ability to forestall unnecessarily weak result 
we analyze the computational complexity of causal relationship in pearl s structural model where we focus on causality between variable event causality and probabilistic causality in particular we analyze the complexity of the sophisticated notion of weak and actual causality by halpern and pearl in the course of this we also prove an open conjecture by halpern and pearl and establish other semantic result to our knowledge no complexity aspect of causal relationship have been considered so far and our result shed light on this issue 
in repeated general sum game an agent using a quot best response quot strategy maximizes it own payoff assuming it behavior ha no effect on it opponent this notion of best response requires some degree of learning to determine the fixed opponent behavior 
the hr program form concept and make conjecture in domain of pure mathematics and us theorem prover otter and model generator mace to prove or disprove the conjecture hr measure property of concept and ass the theorem and proof involving them to estimate the interestingness of each concept and employ a best first search this approach ha led hr to the discovery of interesting new mathematics and enables it to build theory from just the axiom of finite algebra 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
role limiting approach using explicit theory of problem solving have been successful for acquiring knowledge from domain expert however most system using this approach do not support acquiring procedural knowledge only instance and type information approach using interdependency among different piece of knowledge have been successful for acquiring procedural knowledge but these approach usually do not provide all the support that domain expert require we show how the two approach can be combined in such a way that each benefit from information provided by the other we extend the role limiting approach with a knowledge acquisition tool that dynamically generates question for the user based on the problem solving method this allows a more flexible interaction pattern when user add knowledge this tool generates expectation for the procedural knowledge that is to be added when these procedure are refined new expectation are created from interdependencymodels that in turn refine the information used by the system the implemented ka tool provides broader support than previously implemented system preliminary evaluation in a travel planning domain show that user who are not programmer can with little training specify executable procedural knowledge to customize an intelligent system 
combinatorial auction where bidder can bid on bundle of item can lead to more economical allocation but determining the winner is np complete and inapproximable we present cabob a sophisticated search algorithm for the problem it us decomposition technique upper and lower bounding also across component elaborate and dynamically chosen bid ordering heuristic and a host of structural observation experiment against cplex show that cabob is usually faster never drastically slower and in many case drastically faster we also uncover interesting aspect of the problem itself first the problem with short bid that were hard for the first generation of specialized algorithm are easy second almost all of the cat distribution are easy and become easier with more bid third we test a number of random restart strategy and show that they do not help on this problem because the run time distribution doe not have a heavy tail at least not for cabob 
word sense disambiguation wsd is the process of distinguishing between different sens of a word in general the disambiguation rule differ for different word for this reason the automatic construction of disambiguation rule is highly desirable one way to achieve this aim is by applying machine learning technique to training data containing the various sens of the ambiguous word in the work presented here the decision tree learning algorithm c is applied on a corpus of financial news article instead of concentrating on a small set of ambiguous word a done in most of the related previous work all content word of the examined corpus are disambiguated furthermore the effectiveness of word sense disambiguation for different part of speech noun and verb is examined empirically 
finding optimal solution for job shop scheduling problem requires high computational effort especially under consideration of uncertainty and frequent replanning in contrast to computational solution domain expert are often able to derive good local dispatching heuristic by looking at typical problem instance they can be efficiently applied by looking at few relevant feature however these rule are usually not optimal especially in complex decision situation here we describe an approach that try to combine both world a neural network based agent autonomously optimizes it local dispatching policy with respect to a global optimization goal defined for the overall plant on two benchmark scheduling problem we show both learning and generalization ability of the proposed approach 
this paper present new complexity result for propositional closed world reasoning cwr from tractable knowledge base kb both basic cwr generalized cwr extended generalized cwr careful cwr and extended cwr equivalent to circumscription are considered the focus is laid on tractable kb belonging to target class for exact compilation function blake formula dnfs disjunction of horn formula and disjunction of renamable horn formula the complexity of inference is identified for all the form of cwr listed above for each of them new tractable fragment are exhibited our result suggest knowledge compilation a a valuable approach to deal with the complexity of cwr in some situation 
if you have planned to achieve one particular goal in a stochastic delayed reward problem and then someone asks about a different goal what should you do what if you need to be ready to quickly supply an answer for any possible goal this paper show that by using a new kind of automaton caily generated abstract action hierarchy that with n state preparing for all of n possible goal can be much much cheaper than n time the work of preparing for one goal in goal based markov decision problem it is usual to generate a policy x mapping state to action and a value function j x mapping state to an estimate of minimum expected cost to goal starting at x in this paper we will use the terminology that a multipolicy x y for all state pair x y map a state x to the first action it should take in order to reach y with expected minimum cost and a multi valuefunction j x y is a definition of this minimum cost building these object quickly and with little memory is the main purpose of this paper but a secondary result is a natural automatic way to create a set of parsimonious yet powerful abstractactions for mdps the paper concludes with a set of empirical result on increasingly large mdps 
in literature the mediator architecture ha been proposed for taking information from distributed heterogeneous and often dynamic source and making them work together a a whole in this paper we propose a distributed case based approach for the main problem of a mediator i e rewriting query according to mediator s schema according to this approach we use a case memory a mediator s schema therefore such a schema is not static a in other system but is dynamically updated through the cooperation with information source and other mediator strongly influenced by the query submitted by a consumer from the analysis of different cooperation strategy arises that it is more efficient and effective for a mediator to directly cooperate with information source when the source are few otherwise it is more efficient to cooperate with other mediator 
in this paper the hopfield neural network with delay hnnd is studied from the standpoint of regarding it a an optimized computational model two general updating rule for network with delay gurd are given based on hopfield type neural network with delay for optimization problem and characterized dynamic threshold it is proved that in any sequence of updating rule mode the gurd monotonously converges to a stable state of the network the diagonal element of the connection matrix are shown to have an important influence on the convergence process and they represent the relationship of the local maximum value of the energy function with the stable state of the network all ordinary dhnn algorithm are instance of gurd it can be shown that the convergence condition of gurd may be relaxed in the context of application for instance the condition of nonnegative diagonal element of the connection matrix can be removed from the original convergence theorem new updating rule mode and restrictive condition can guarantee the network to achieve a local maximum of the energy function 
we study fragment of allen s algebra that contain a basic relation distinct from the equality relation we prove that such a fragment is either npcomplete or else contained in some already known tractable subalgebra we obtain this result by giving a new uniform description of known maximal tractable subalgebras and then systematically using an algebraic technique for description of maximal subalgebras with a given property this approach avoids the need for extensive computerassisted search 
agent based computing represents an exciting new synthesis both for artificial intelligence ai and more generally computer science it ha the potential to significantly improve the theory and the practice of modelling designing and implementing complex system yet to date there ha been little systematic analysis of what make an agent such an appealing and powerful conceptual model moreover even le effort ha been devoted to exploring the inherent disadvantage that stem from adopting an agent oriented view here both set of issue are explored the standpoint of this analysis is the role of agent based software in solving complex real world problem in particular it will be argued that the development of robust and scalable software system requires autonomous agent that can complete their objective while situated in a dynamic and uncertain environment that can engage in rich high level social interaction and that can operate within flexible organisational structure 
most formulation of reinforcement learning depend on a single reinforcement reward value to guide the search for the optimal policy solution if observation of this reward is rare or expensive converging to a solution can be impractically slow one way to exploit additional domain knowledge is to use more readily available but related quantity a secondary reinforcer to guide the search through the space of all policy we propose a method to augment policy gradient reinforcement learning algorithm by using prior domain knowledge to estimate desired relative level of a set of secondary reinforcement quantity rl can then be applied to determine a policy which will establish these level the primary reinforcement reward is then sampled to calculate a gradient for each secondary reinforcer in the direction of increased primary reward these gradient are used to improve the estimate of relative secondary value and the process iterates until reward is maximized we prove that the algorithm converges to a local optimum in secondary reward space and that the rate of convergence of the performance gradient estimate in secondary reward space is independent of the size of the state space experimental result demonstrate that the algorithm can converge many order of magnitude faster than standard policy gradient formulation 
many planning domain require a richer notion of time in which action can overlap and have different duration the key to fast performance in classical planner e g graphplan ipp and blackbox ha been the use of a disjunctive representation with powerful mutual exclusion reasoning this paper present tgp a new algorithm for temporal planning tgp operates by incrementally expanding a compact planning graph representation that handle action of differing duration the key to tgp performance is tight mutual exclusion reasoning which is based on an expressive language for bounding mutexes and includes mutexes between action and proposition our experiment demonstrate that mutual exclusion reasoning remains valuable in a rich temporal setting 
state abstraction is of central importance in reinforcementlearning and markov decision process this paper study the case of variable resolutionstate abstraction for continuous state deterministicdynamic control problem in which near optimalpolicies are required we describe variable resolutionpolicy and value function representationsbased on kuhn triangulation embedded in a kdtree we then consider top down approach tochoosing which cell to split in order to generateimproved 
software design is the hardest part of creating intelligent agent therefore agent architecture should be optimized a design tool this paper present an architectural synthesis between the three layer architecture which dominate autonomous robotics and virtual reality and a more agent oriented approach to viewing behavior module we provide an approach behavior oriented design bod for rapid maintainable development we demonstrate our approach by modeling primate learning 
abstract the discovery of association rule is one of the classic problem of data mining typically it is done over well structured data such a dat aba in this paper we present a method of discovery of association rule in semi structured data namely in a set of conceptual graph the method is based on conceptual clustering of the data and constructing of a conceptual hierarchy a feature of the method is the possibility of using different le vels of generalization 
we are beginning to make use of technology that intervenes in the content of the communication language processing ha indeed a large practical potential if we take into account multiple modality of communication multimodality refers to the perception of different co ordinated medium used in delivering a message but also to the combination of various attitude in relation to communication and information access e g goal oriented and exploration oriented in the paper reference is made to some prototype developed at irst conceived for cultural tourism in a recent one the specificity is the combination of two form of navigation taking place at the same time one in information space the other in the physical space some challenge for the future are discussed toward the end 
this paper conduct an experiment to investigate the effect of a physical constraint on a subject s viewpoint when using spoken language to navigate a robot in addition a robot navigation environment named spondia ii ha been developed for the experiment with an actual autonomous mobile robot it is well known that the meaning of an utterance such a a demonstrative pronoun depends on the viewpoint of the speaker or the hearer in a conversation between people the primary factor in determining viewpoint is the physical constraint that are mediated by their body movement this paper note that these physical constraint also have an effect on viewpoint even when people instruct a robot furthermore it is argued that the utterance process also would greatly improve if the robot were able to comprehend the constraint 
among the local consistency technique used in the resolution of constraint satisfaction problem csps path consistency pc ha received a great deal of attention a constraint graph g is pc if for any valuation of a pair of variable that satisfy the constraint in g between them one can find value for the intermediate variable on any other path in g between those variable so that all the constraint along that path are satisfied on complete graph montanari showed that pc hold if and only if each path of length two is pc by convention it is therefore said that a csp is pc if the completion of it constraint graph is pc in this paper we show that montanari s theorem extends to triangulated graph one can therefore enforce pc on sparse graph by triangulating instead of completing them the advantage is that with triangulation much le universal constraint need to be added we then compare the pruning capacity of the two approach we show that when the constraint are convex the pruning capacity of pc on triangulated graph and their completion are identical on the common edge furthermore our experiment show that there is little difference for general nonconvex problem 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
reasoning about action ha been a focus of interest in ai from the beginning and continues to receive attention rut the range of situation considered ha been rather narrow and fall well short of what is needed for understanding natural language language understanding requires sophisticated reasoning about action and event and the world s language employ a variety of grammatical and lexical device to construe direct attention and focus on and control inference about action and event we implemented a neurally inspired computational model that is able to reason about linguistic action and event description such a those found in news story the system us an active event representation that also seems to provide natural and cognitiveiy motivated solution to classical problem in logical theory of reasoning about action for logical approach to reasoning about action we suggest that looking at story understanding set up fairly strong desideratum both in term of the fine grained event and action distinction and the kind of real time inference required 
this research seek to quantify the impact of thechoice of reward function on behavioral diversity inlearning robot team the methodology developedfor this work ha been applied to multirobot foraging soccer and cooperative movement this paperfocuses specifically on result in multirobot foraging in these experiment three type of reward areused with q learning to train a multirobot team toforage a local performance based reward a globalperformance based reward and a 
recently more and more researcher have been supporting the view that learning is a goaldriven process one of the key property of a goal driven learner is introspectiveness the ability to notice the gap in it knowledge and to reason about the information required to fill in those gap in this paper we introduce a quantitative introspective learning paradigm into case based reasoning cbr the result is an integrated problem solving model which will learn introspectively feature weight in a case base in order to be responsive dynamically to it user in contrast to the existing qualitative method for introspective learning our model ha the advantage of being able to capture accurate learning information in the interaction with it user a cbr system equipped with quantitative introspective learning ability can allow the feature weight to be captured automatically and to track it user changing preference continuously in such a system while the reasoning part is still case based the learning part is shouldered by a quantitative introspective learning model weight learning and evolution are accomplished in the background the effectiveness of this integration will be demonstrated through a series of empirical experiment 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
neural network ensemble is a learning paradigm where several neural network are jointly used to solve a problem in this paper the relationship between the generalization ability of the neural network ensemble and the correlation of the individual neural network is analyzed which reveals that ensembling a selective subset of individual network is superior to ensembling all the individual network in some case therefore an approach named gasen is proposed which train several individual neural network and then employ genetic algorithm to select an optimum subset of individual network to constitute an ensemble experimental result show that comparing with a popular ensemble approach i e averaging all and a theoretically optimum selective ensemble approach i e enumerating gasen ha preferable performance in generating ensemble with strong generalization ability in relatively small computational cost 
robot are gradually entering into diverse application domain such a home office and playing field this article present advanced research activity related to these domain first is robocup which is an attempt to promote ai and robotics research by providing a common task for evaluation of various performance theory algorithm and robot architecture in order for robot both physical robot and soft agent to play a soccer game reasonably well a wide range of technology need to be integrated and a number of technical breakthrough must be accomplished the recent result from the last two robocups are reviewed and future league are introduced second the richer domain of service robotics ha also received significant interest recently the task here is to serve a a human assistant in an office or domestic environment for task like cleaning and delivery the human robot interaction is a key issue to success which pose new challenge in term of integration of spoken dialogue gesture body language etc in addition mobile manipulation and safe navigation around human is essential to success these two area integrates many different discipline including control perception natural language processing hybrid system and handling of uncertainty and applied to tour guiding mail delivery domestic service and rescue activity 
for many application it is important to accuratelydistinguish false negative result fromfalse positive this is particularly importantfor medical diagnosis where the correct balancebetween sensitivity and specificity play an importantrole in evaluating the performance of aclassifier in this paper we discus two schemesfor adjusting the sensitivity and specificity ofsupport vector machine and the descriptionof their performance using receiver operatingcharacteristic roc curve 
real time monitoring call for decision making capability in reaction to observed event associative model provide efficiency by matching the observed situation to a recorded pattern equipped with an accurate decision we rely on a decision tree accounting for the context and temporal chronicle expressing dynamic pattern in highly reactive domain i e when action get a frequent a observation the decision must anticipate the complete recognition of a pattern comparing possible evolution this paper focus on the on line decision process a game against nature in the general case a timed game automaton gather the possible next step with associated goodness value and us an opportunistic algorithm to compute a temporally expressive decision maximizing it utility i e the chance of winning 
simple temporal network have proved useful in application that involve metric time however many application involve event whose timing is not controlled by the execution agent a number of property relating to overall controllability in such case have been introduced in vidal and ghallab and vidal and fargier including weak and strong controllability we derive some new result concerning these property in particular we prove the negation of weak controllability is np hard confirming a conjecture in vidal and fargier we also introduce a more general controllability property of which weak and strong controllability are special case a propagation algorithm is provided for determining whether the property hold and we identify tractable case where the algorithm run in polynomial time 
simple recurrent network srns have been widely used in natural language task sardsrn extends the srn by explicitly representing the input sequence in a sardnet self organizing map the distributed srn component lead to good generalization and robust cognitive property whereas the sardnet map provides exact representation of the sentence constituent this combination allows sardsrn to learn to parse sentence with more complicated structure than can the srn alone and suggests that the approach could scale up to realistic natural language 
in this paper we introduce a multi agent deontic update semantics that build on a logic of prescriptive obligation norm and a logic of descriptive obligation normative proposition in this preference based logic we formalize right a a new type of strong prescriptive permission and duty and commitment a prescriptive obligation between agent 
a foundational approach to modelling belief contraction and revision is presented based on a notion of similarity between belief set in contracting from a belief set the result is the belief set s most similar to the original in which is not believed similar consideration apply to belief revision the modelling of belief change generalises the grove modelling based on a system of sphere where instead of having a total order on set of possible world we have a total order on set of belief set given this modelling set of postulate are determined for contraction and revision the resulting postulate set subsume those in the agm approach the approach shed light on the foundation of belief revision in that first it provides a more general framework than the agm approach second it illustrates assumption under lying the agm approach and third it allows a fine grained investigation of proposed principle underlying belief change lastly it demonstrates that at their most basic revision and contraction of belief are not interdefinable notion but rather distinct concept 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
the fastest growing community of web user is that of mobile visitor who browse with wireless pda cell phone and pager unfortunately most web site today are optimized exclusively for desktop broadband client and deliver content poorly suited for mobile device device that can display only a few line of text using slow wireless network to best serve the need of this growing community we propose building web site personalizersthat observe the behavior of web visitor and automatically customize and adapt site for each individual mobile visitor in this paper we give an overview of our approach to web site personalization a utilitymaximizing search through the space of personalized web site following this framework we have implemented two personalizers proteus and minpath proteus allows change to site navigation adding or removing link a well a content manipulation rearranging or eliding content and evaluates the result with a learned model of the current visitor m inpath concentrate exclusively on adding shortcut link but us a model learned by clustering visitor based on their sequence of page request we introduce proteus and minpath and outline our current and future direction for these personalizers 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
in the last several year the computational complexity of classical planning and htn planning have been studied but in both case it is assumed that the planner ha complete knowledge about the initial state recently there ha been proposal to use sensing action to plan in presence of incompleteness in this paper we study the complexity of planning in such case in our study we use the action description language a proposed in by m gelfond and v lifschitz and it extension the language a allows planning in the situation with complete information it is known that if we consider only plan of feasible polynomial length the planning problem for such situation is np complete even checking whether a given objective is attainable from a given initial state is np complete in this paper we show that the planning problem in presence of incompleteness is indeed harder it belongs to the next level of complexity hierarchy in precise term it is p complete to overcome the complexity of this problem c baral and t son have proposed several approximation we show that under certain condition one of these approximation o approximation make the problem np complete thus indeed reducing it complexity 
a coordination mechanism for a system of sparsely communicating agent is described the mechanism is based on a stochastic version of cellular automaton a parameter similar to a temperature can be tuned to change the behaviour of the system it is found that the best coordination occurs near a phase transition between order and chaos coordination doe not rely on any particular structure of the connection between agent thus it may be applicable to a large array of sparsely communicating mobile robot 
the ability to generate effective evaluative argument is critical for system intended to advise and persuade their user we have developed a system that generates evaluative argument that are tailored to the user properly arranged and concise we have also devised an evaluation framework in which the effectiveness of evaluative argument can be measured with real user this paper present the result of a formal experiment we performed in our framework to verify the influence of user tailoring on argument effectiveness 
progressive processing is a model of computation that allows a system to tradeoff computational resource against the quality of result this paper generalizes the existing model to maice it suitable for dynamic composition of information retrieval technique the new framework address effectively the uncertainty associated with the duration and output quality of each component we show how to construct an optimal meta level controller for a single task based on solving a corresponding markov decision problem and how to extend the solution to the case of multiple and dynamic task using the notion of an opportunity cost 
a new algorithm for computing the maximum entropy ranking over model is presented the algorithm handle arbitrary set of propositional default with associated strength assignment and succeeds whenever the set satisfies a robustness condition failure of this condition implies the problem may not be sufficiently specified for a unique solution to exist this work extends the applicability of the maximum entropy approach detailed in goldszmidt et al and clarifies the assumption on which the method is based 
one advantage of case based reasoning cbr is the relative ease of constructing and maintaining cbr system especially a a number of commercial cbr tool are available however there are area of cbr that current tool have not yet addressed one of these is easing or automating the acquisition of adaptation knowledge since task like design or planning typically require a significant amount of adaptation cbr system for these task still do not fully benefit from cbr s promise of reducing the development effort to address this we have developed several knowledge light method for learning adaptation knowledge from the case in the case base these method perform substitutional adaptation for both nominal and numerical value and are suitable for decomposable design problem in particular formulation and configuration test performed on a tablet formulation domain show promising result the automatic adaptation method we present can easily be incorporated in general purpose cbr tool thus further contributing to reducing the cost of cbr system 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we present a new knowledge representation and reasoning framework for modeling nonlinear dynamical system the goal of this framework are to smoothly incorporate varying level of domain knowledge and to tailor the reasoning method and hence the search space accordingly our solution exploit generalized physical network gpn a rneta level representation of idealized two terminal element together with a hierarchy of qualitative and quantitative analysis tool to produce a dynamic modeling domain whose complexity naturally adapts to the amount of available information about the target system 
we present efficient technique for computing near optimal strategy for a class of stochastic commodity trading problem modeled a markov decision process mdps the process ha a continuous state space and a large action space and cannot be solved efficiently by standard dynamic programming method we exploit structural property of the process and combine it with monte carlo estimation technique to obtain novel and efficient algorithm that closely approximate the optimal strategy 
nowadays firm formerly considering the human operator a the main error source in process control bend their effort towards anthropocentric approach to re integrate the human factor especially the knowledge he she ha been developping a the essential resource for a high quality decision process a the expert operator remains a rare resource and in order to capitalize his her knowledge and know how the development of tool integrating this new dimension ha become an important challenge this paper deal with a tool for knowledge acquisition under cognitive constraint assuming that cognitive principle could be sometimes useful to improve machine learning tool result additionally we have to cope with the difficulty linked to the fact that the acquired strategy have to be adapted on line after describing the underlying cognitive principle we will introduce the decision representation space and it related notation we will then show the difficulty linked to the search of an optimal representation of the expert strategy set and how the heuristic used by the algorithm studied avoid these np complete problem finally the current result and our work perspective are stated 
this paper present a novel statistical latent class model for text mining and interactive information access the described learning architecture called cluster abstraction model cam is purely data driven and utilizes contact specific word occurrence statistic in an intertwined fashion the cam extract hierarchical relation between group of document a well a an abstractive organization of keywords an annealed version of the expectation maximization em algorithm for maximum likelihood estimation of the model parameter is derived the benefit of the cam for interactive retrieval and automated cluster summarization are investigated experimentally 
the inductive learning is effective for a variety of natural language processing problem however it need expensive training data the quality of learned rule often depends on the quality of training data used in this paper we propose a method to detect error in training data automatically to improve the quality of the training data we consider a japanese word segmentation problem in which training data mean the corpus including word boundary sign we use two clue to detect error one is a property of a decision list and the other is the nature of adaboost in experiment we provided doubtful instance from the kyoto univeristy corpus which is the most trustworthy japanse tagged corpus most of them had tag which we could not easily determine to be correct or incorrect moreover instance were actually error 
based on a set of design principle automated visual presentation system promise to simplify an application programmer s design task by automatically constructing appropriate visual explanation for different information however these automated presentation system must be equipped with a powerful inference approach to suit practical application here we present a planning based practical inference approach that can design a series of connected visual presentation in interactive environment our emphasis here is on a set of important visual planning feature and how they facilitate visual design this set of feature includes a knowledge rich representation of visual planning variable and constraint a novel object decomposition model that can be used with action decomposition to simplify the visual synthesis process and practical temporal and spatial reasoning capability to facilitate coherent visual design and presentation in addition we have implemented our visual planning approach in a visual planner called previse a part of our automated presentation testbed system a set of example is also given to illustrate the necessity and utility of our visual planning approach 
q is a reinforcement learning algorithm that combine q learning and td online implementation of q that use eligibility trace have been shown to speed basic q learning in this paper we present an asymptotic analysis of watkins q with accumulative eligibility trace we first introduce an asymptotic approximation of q that appears to be a gain matrix variant of basic qlearning using the ode method we then determine an optimal gain matrix for q learning that maximizes it rate of convergence toward the optimal value function the similarity between this optimal gain and the asymptotic gain of q explains the relative efficiency of the latter for furthermore by minimizing the difference between these two gain optimal value for the parameter and the decreasing learning rate can be determined this optimal strongly depends on the exploration policy during learning a robust approximation of these learning parameter lead to the definition of a new efficient algorithm called aq learning average q learning that show a close resemblance to schwartz r learning our result have been demonstrated through numerical simulation 
for an effective teacher student interaction the teacher ha to maintain a constant understanding of what is going on in the student s mind when coming to physic the teacher s ability to propose and to relate explanation at different level of abstraction a a chain of causal interaction deep or a a set of observable phenomenon shallow may determine a successful and lasting learning in the student here we describe a knowledge representation to be used by the teacher to depict to herself the student s mental model and to tune her future lesson according to the current student comprehension supported by a cognitive theory of child physic learning we used the system why for modeling the evolution of a student s learning a it appeared at the teacher s eye two of why s feature turned out to be essential a to deal with explanation having different level of abstraction and b the possibility to continuously evaluate the coherence of the hypothesized learner s model with respect to her explanation in the long term the work s outcome might contribute to the development of teaching assistant system that support the teacher in identifying what ha to be explained next 
deciding whether a propositional formula in conjunctive normal form is satisfiable sat is an npcomplete problem the problem becomes linear when the formula contains binary clause only interestingly the reduction to sat of a number of well known and important problem such a classical ai planning and automatic test pattern generation for circuit yield formula containing many binary clause in this paper we introduce and experiment with simplify a formula simplifier targeted at such problem simplify construct the implication graph corresponding to the binary clause in the formula and us this graph to deduce new unit literal the deduced literal are used to simplify the formula and update the graph and so on until stabilization finally we use the graph to construct an equivalent simpler set of binary clause experimental evaluation of this simplifier on a number of bench mark formula produced by encoding ai planning problem prove simplify to be fast and effective 
a central problem in case based reasoning cbr is how to store and retrieve case one approach to this problem is to use exemplar based model where only the prototypical case are stored however the development of an exemplar based model ebm requires the solution of several problem i how can a ebm be represented ii given a new case how can a suitable exemplar be retrieved iii what make a good exemplar iv how can an ebm be learned incrementally this paper develops a new model called a probabilistic exemplar based model that address these question the model utilizes bayesian network to develop a suitable representation and us probabilistic propagation for assessing and retrieving exemplar when a new case is presented the model learns incrementally by revising the exemplar retained and by updating the conditional probability required by the bayesian network the paper also present the result of evaluating the model on three datasets 
anchoring is the process of creating and maintaining the correspondence between symbol and percept that refer to the same physical object although this process must necessarily be present in any symbolic reasoning system embedded in a physical environment e g an autonomous robot the systematic study of anchoring a a clearly separated problem is just in it initial phase in this paper we focus on the use of symbol in action and plan and the consequence this ha for anchoring in particular we introduce action property and partial matching of object description we also consider the use of indefinite reference in the context of action the use of our formalism is exemplified in a mobile robotic domain 
there are fundamental limitation on using mental attitude to formalise the semantics of an agent communication language acl instead we define a general semantic framework for an acl in term of protocol we then argue that the proper role of mental attitude is to link what an agent think about the content of a message to what it doe in response to receiving that message we formalise this connection through normative and informative specification and demonstrate it use in communication between two bdi style agent 
this paper describes and evaluates the confidence based dual reinforcement q routing algorithm cdrq routing for adaptive packet routing in communication network cdrq routing is based on the q learning framework of q routing the main contribution of this work is the increased quantity and improved quality of exploration in cdrq routing which lead to faster adaptation and better routing policy learned a compared to q routing the state of the art adaptive bellman ford routing and the non adaptive shortest path routing experiment over several network topology have shown that at different load cdrq routing learns superior policy significantly faster than q routing moreover cdrq routing learns policy that sustain higher load level than q routing analysis show that overhead due to exploration is insignificant a eqmpared to the improvement in cdrq routing 
the problem of action selection by autonomous agent becomes increasingly difficult when acting in continuous non deterministic and dynamic environment pursuing multiple and possibly conflicting goal we propose a method that exploit additional information gained from continuous state is able to deal with unexpected situation and take multiple and conflicting goal into account including additional motivational aspect such a dynamic goal which allow for situation dependent motivational influence on the agent further we show some domain independent property of this algorithm along with empirical result gained using the robocup simulated soccer environment 
keeping track of multiple object over time is a problem that arises in many real world domain the problem is often complicated by noisy sensor and unpredictable dynamic previous work by huang and russell drawing on the data association literature provided a probabilistic analysis and a threshold based approximation algorithm for the case of multiple object detected by two spatially separated sensor this paper analysis the case in which large number of sensor are involved we show that the approach taken by huang and russell who used pairwise sensor based appearance probability a the elementary probabilistic model doe not scale when more than two observation are made the object intrinsic property must be estimated these provide the necessary conditional independency to allow a spatial decomposition of the global probability model we also replace huang and russell s threshold algorithm for object identification with a polynomial time approximation scheme based on markov chain monte carlo simulation using sensor data from a freeway traffic simulation we show that this allows accurate estimation of long range origin destination information even when the individual link in the sensor chain are highly unreliable 
dlr is an expressive description logic dl with n ary relation particularly suited for modeling database schema although dlr ha constituted one of the crucial step for applying dl technology to data management there is one important aspect of database schema that dl including dlr do not capture yet namely the notion of identification constraint and functional dependency in this paper we introduce a dl which extends dlr and fully capture the semantics of such constraint and we address the problem of reasoning in such a logic we show that verifying knowledge base satisfiability and logical implication in the presence of identification constraint and nonunary functional dependency can be done in exptime thus with the same worst case computational complexity a for plain dlr we also show that adding just unary functional dependency to dlr lead to undecidability 
we present a new algorithm for polynomial time learning of near optimal behavior in stochastic game this algorithm incorporates and integrates important recent result of kearns and singh in reinforcement learning and of monderer and tennenholtz in repeated game in stochas tic game we face an exploration v exploitation dilemma more complex than in markov decision process namely given information about partic ular part of a game matrix how much effort should the agent invest in learning it unknown part we explain and address these issue within the class of single controller stochastic game this solution can be extended to stochastic game in general 
this paper present a methodology for assessing the degree of diagnosability of a system i e given a set of sensor which fault can be discriminated and characterising and determining the minimal additional sensor which guarantee a specified degree of diagnosability this method ha been applied to several subsystem of a ge neral electric frame gas turbine owned by a major uk utility 
i highlight some inefficiency of graphplan s backward search algorithm and describe how these can be eliminated by adding explanation based learning and dependency directed backtracking capability to graphplan i will then demonstrate the effectiveness of these augmentation by describing result of empirical study that show dramatic improvement in run time w speedup a well a solvability horizon on benchmark problem across seven different domain 
given a set of m observation on n variable an o mn algorithm is proposed to find a basis of all affine relation between these variable satisfied by the observation on a variable example this new algorithm is time faster than the all subset option for linear regression of the sa package which is a non polynomial alternative extension to the case where square ratio product of pair of variable or logarithm of such term appear in the relation is straightforward and remains polynomial the method is first tested with data for several classical discovery studied previously by the bacon program then it is added to the autographix system for computer aided graph theory thus making it entirely automated to demonstrate the power of the resulting system five novel relation or conjecture in graph theory are found two of which pertain to mathematical chemistry three conjecture involve five invariant which is more than in most proposition of graph theory proof of two conjecture are also given 
latent semantic analysis lsa is a statistical corpus based text comparison mechanism that wa originally developed for the task of information retrieval but in recent year ha produced remarkably human like ability in a variety of language task lsa ha taken the test of english a a foreign language and performed a well a non native english speaker who were successful college applicant it ha shown an ability to learn word at a rate similar to human it ha even graded paper a reliably a human grader we have used lsa a a mechanism for evaluating the quality of student response in an intelligent tutoring system and it performance equal that of human raters with intermediate domain knowledge it ha been claimed that lsa s text comparison ability stem primarily from it use of a statistical technique called singular value decomposition svd which compress a large amount of term and document co occurrence information into a smaller space this compression is said to capture the semantic information that is latent in the corpus itself we test this claim by comparing lsa to a version of lsa without svd a well a a simple keyword matching model 
a number of important scientific and engineering application such a fluid dynamic simulation and aircraft design require analysis of spatially distributed data from expensive experiment and complex simulation in such data scarce application it is advantageous to use model of given sparse data to identify promising region for additional data collection this paper present a principled mechanism for applying domain specific knowledge to design focused sampling strategy in particular our approach us ambiguity identified in a multi level qualitative analysis of sparse data to guide iterative data collection two case study demonstrate that this approach lead to highly effective sampling decision that are also explainable in term of problem structure and domain knowledge 
lp is an algorithm for adaptive information extraction from web related text that induces symbolic rule by learning from a corpus tagged with sgml tag induction is performed by bottom up generalisation of example in a training corpus training is performed in two step initially a set of tagging rule is learned then additional rule are induced to correct mistake and imprecision in tagging shallow nlp is used to generalise rule beyond the flat word structure generalization allows a better coverage on unseen text a it limit data sparseness and overfitting in the training phase in experiment on publicly available corpus the algorithm outperforms any other algorithm presented in literature and tested on the same corpus experiment also show a significant gain in using nlp in term of effectiveness reduction of training time and training corpus size in this paper we present the machine learning algorithm for rule induction in particular we focus on the nlp based generalisation and the strategy for pruning both the search space and the final rule set 
the prisoner s dilemma is a useful model for studying the balance between self interest and group interest in multi agent system although many stratergies have been developed that perform well most of these stratergies make strong assumption about the information available to the agent it is in this context that we describe a satisficing learning stratergy for the prisoner s dilemma and present evidance that stable outcome other than the nash equilibrium are possible in addition we offer emperical evidence that under typical circumstance mutual cooperation is the most likely outcome and identify condition under which two satisficing agent will learn to cooperate 
many mobile robot task can be most efficiently solved when a group of robot is utilized the type of organization and the level of coordination and communication within a team of robot affect the type of task that can be solved this paper examines the tradeoff of homogeneity versus heterogeneity in the control system by allowing a team of robot to coevolve their high level controller given different level of difficulty of the task our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robot to create specialist the key factor is not difficulty per se but the number of skill set necessary to successfully solve the task a the number of skill needed increase the more beneficial and necessary heterogeneity becomes we demonstrate this in the task domain of herding where one or more robot must herd another robot into a confined space 
we present a new framework for reasoning about point interval and duration point interval duration network pidn the pidn adequately handle both qualitative and quantitaive temporal information we show that interval algebra point algebra tcsp pdn and apdn become special case of pidn the underlying algebraic structure of pidn is closed under composition and intersection determinig consistency of p i dn is np ilard however we identify some tractable subclass of pidn we show that path consistency is not sufficient to ensure global consistency of the tractable subclass of pidn we identify a subclass for which enforcing consistency suffices to ensure the global consistency and prove that this subclass is maximal for qualitative constraint our approach is based on the geometric interpretation of the domain of temporal object interestingly the classical helly s theorem of is used to prove the complexity for the tractable subclass 
the multi stream dependency detection algorithm find rule that capture statistical dependency between pattern in multivariate time series of categorical data oates and cohen c rule strength is measured by the g statistic wickens and an upper bound on the value of g for the descendant of a node allows msdd s search space to be pruned however in the worst case the algorithm will explore exponentially many rule this paper present and empirically evaluates two way of addressing this problem the first is a set of three method for reducing the size of msdd s search space based on information collected during the search process second we discus an implementation of msdd that distributes it computation over multiple machine on a network 
the blackbox planning system unifies the planning a satisfiability framework kautz and selman with the plan graph approach to strip planning blum and furst we show that strip problem can be directly translated into sat and efficiently solved using new randomized systematic solver for certain computationally challenging benchmark problem this unified approach outperforms both satplan and graphplan alone we also demonstrate that polynomialtime sat simplification algorithm applied to the encoded problem instance are a powerful complement to the mutex propagation algorithm that work directly on the plan graph 
in order to generate high quality explanation in technical or mathematical domain the presentation must be adapted to the knowledge of the intended audience current proof presentation system only communicate proof on a fixed degree of abstraction independently of the addressee s knowledge in this paper we propose an architecture for an interactive proof explanation system called prex based on the theory of human cognition act r it dialog planner exploit a cognitive model in which both the user s knowledge and his cognitive process are modeled by this mean his cognitive state are traced during the explanation the explicit representation of the user s cognitive state in act r allows the dialog planner to choose a degree of abstraction tailored to the user for each proof step to be explained moreover the system can revise it assumption about the user s knowledge and react to his interaction 
the task of information filtering is to classifydocuments from a stream into either relevantor irrelevant according to a particular user interestwith the objective to reduce informationload when using an information filter in anenvironment that is changing a time proceeds method for adapting the filter should be consideredin order to retain the desired accuracyin classification we favor a methodology thatattempts to detect change and adapts the informationfilter only if 
information extraction and text classication are usually seen a complementary form of shallow text processing in that they are aimed at very dierent task in this paper we describe two simple but real world domain in which text classication technique can be used directly for information extraction specically we describe system for extracting information from business card and for automatically processing change of address email message that are based primarily on text classication technique our main technical contribution is a novel integration of hidden markov model and text classiers 
we give a graph theoretical characterization of answer set of normal logic program we show that there is a one to one correspondence between answer set and a special non standard graph coloring of so called block graph of logic program this lead u to an alternative implementation paradigm to compute answer set by computing non standard graph coloring our approach is rule based and not atom based like most of the currently known method we present an implementation for computing answer set which work on polynomial space 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
most ie system process text in sequential step or phase ranging from lexical and morpho logical processing recognition and typing of proper name parsing of larger syntactic constituent resolution of anaphora and coreference and the ultimate extraction of domain relevent event and relationship from the text we discus each of these system component and various approach to their design 
generating expression which communicate information already known to the hearer building enthymematic argument and characterising refutation all pose significant problem to traditional natural language generation technique after exploring these problem an approach is proposed which through it employment of a notion of saliency handle them cleanly and offer support for further feature including clue word generation it is argued that propositional salience and it interaction with intentional attentional epistemic and structural component of a text generation system have a key role to play in the design and realisation of persuasive text 
ai research ha often been driven by popular vision hal asimov s robot star trek and by critical application area medical expert system spoken dialogue system etc these vision and application serve to inspire and guide researcher posing challenge illustrating technical weakness and generally channeling creative energy without doubt the widely held vision of the autonomous robot ha exerted a substantial integrative force such that numerous discipline ranging from mechanical engineering to cognitive science can see how their intellectual endeavor can contribute to the overall endeavor in this brief position paper and in the accompanying talk i would like to propose that the next generation of intelligent multimodal user interface can offer a similar intellectual focus for ai researcher after providing a brief overview of our work in this area and two example i would like to suggest the potential impact that such interface could have in the relatively near term 
this paper describes improvement to the temporal difference td learning method the standard form of the td method ha the problem that two control parameter learning rate and temporal discount need to be chosen appropriately these parameter can have a major effect on performance particularly the learning rate parameter which affect the stability of the process a well a the number of observation required our extension to the td algorithm automatically set and subsequently adjusts these parameter the learning rate adjustment is based on a new concept we call temporal coherence tc the experiment reported here compare the extended td algorithm performance with human chosen parameter and with an earlier method for learning rate adjustment in a complex game domain the learning task wa that of learning the relative value of piece without any initial domain specific knowledge and from self play only the result show that the improved method lead to better learning i e faster and le subject to the effect of noise than the selection of human chosen value for the control parameter and a comparison method 
we show how event extraction can be used for handling delayed response task with arbitrary delay period between the stimulus and the cue for response our approach is based on a number of information processing level where the lowest level work on raw time stepped based sensory data this data is classified using an unsupervised clustering mechanism the second level work on this classified data but still on the individual time step basis an event extraction mechanism detects and signal transition between class this form the basis for the third level a this level only is updated when event occur it is independent of the time scale of the lower level interaction we also sketch how an event filtering mechanism could be constructed which discard irrelevant data from the event stream such a mechanism would output a fourth level representation which could be used for delayed response task where irrelevant or distracting event could occur during the delay 
recent research ha shown the promise of using propositional reasoning and search to solve ai planning problem in this paper we further explore this area by applying integer programming to solve ai planning problem the application of integer programming to ai planning ha a potentially significant advantage a it allows quite naturally for the incorporation of numerical constraint and objective into the planning domain moreover the application of integer programming to ai planning address one of the challenge in propositional reasoning posed by kautz and selman who conjectured that the principal technique used to solve integer program the linear programming lp relaxation is not useful when applied to propositional search we discus various ip formulation for the class of planning problem based on strip style planning operator our main objective is to show that a carefully chosen ip formulation significantly improves the strength of the lp relaxation and that the resultant lp are useful in solving the ip and the associated planning problem our result clearly show the importance of choosing the right representation and more generally the promise of using integer programming technique in the ai planning domain 
we propose a technique to improve the performance of hierarchical model based diagnosis based on structural abstraction given a hierarchical representation and the set of currently available observation the technique is able to dynamically derive a tailored hierarchical representation to diagnose the current situation we implement our strategy a an extension to the well known mozetic s approach mozetic and illustrate the obtained performance improvement our approach is more efficient than mozetic s one when due to abstraction fewer observation are available at the coarsest hierarchical level 
state abstraction is of central importance in remforcement learning and markov decision process this paper study the case of variable resolution state abstraction for continuous state deterministic dynamic control problem in which near optimal policy are required we describe variable resolution policy and value function representation based on kuhn triangulation embedded in a kd tree we then consider top down approach to choosing which cell to split in order to generate improved policy we begin with local approach based on value function property and policy property that use only feature of individual cell in making splitting choice later by introducing two new non local measure influence and variance we derive a splitting criterion that allows one cell to efficiently take into account it impact on other cell when deciding whether to split we evaluate the performance of a variety of splitting criterion on many benchmark problem published on the web paying careful attention to their number of cell versus closeness to optimality tradeoff curve 
we study the impact of backbone in optimization and approximation problem we show that some optimization problem like graph coloring resemble decision problem with problem hardness positively correlated with backbone size for other optimization problem like block world planning and traveling salesperson problem problem hardness is weakly and negatively correlated with backbone size while the cost of finding optimal and approximate solution is positively correlated with backbone size a third class of optimization problem like number partitioning have region of both type of behavior we find that to observe the impact of backbone size on problem hardness it is necessary to eliminate some symmetry perform trivial reduction and factor out the effective problem size 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
many organization employ lesson learned ll process to collect analyze store and distribute validated experiential knowledge lesson of their member that when reused can substantially improve organizational decision process unfortunately deployed ll system do not facilitate lesson reuse and fail to bring lesson to the attention of the user when and where they are needed and applicable i e they fail to bridge the lesson distribution gap our approach for solving this problem named monitored distribution tightly integrates lesson distribution with these decision process we describe a case based implementation of monitored distribution alds in a plan authoring tool suite hicap we evaluate it utility in a simulated military planning domain our result show that monitored distribution can significantly improve plan evaluation measure for this domain 
in combinatorial auction multiple good are sold simultaneously and bidder may bid for arbitrary combination of good determining the outcome of such an auction is an optimization problem that is np complete in the general case we propose two method of overcoming this apparent intractability the first method which is guaranteed to be optimal reduces running time by structuring the search space so that a modified depth first search usually avoids even considering allocation that contain conflicting bid caching and pruning are also used to speed searching our second method is a heuristic market based approach it set up a virtual multi round auction in which a virtual agent represents each original bid bundle and place bid according to a fixed strategy for each good in that bundle we show through experiment on synthetic data that a our first method find optimal allocation quickly and offer good anytime performance and b in many case our second method despite lacking guarantee regarding optimality or running time quickly reach solution that are nearly optimal 
markov decision process mdps provide a coherent mathematical framework for planning under uncertainty however exact mdp solution algorithm require the manipulation of a value function which specifies a value for each state in the system most real world mdps are too large for such a representation to be feasible preventing the use of exact mdp algorithm various approximate solution algorithm have been proposed many of which use a linear combination of basis function a a compact approximation to the value function almost all of these algorithm use an approximation based on the weighted l norm euclidean distance this approach prevents the application of standard convergence result for mdp algorithm all of which are based on max norm this paper make two contribution first it present the first approximate mdp solution algorithm both value and policy iteration that use max norm projection thereby directly optimizing the quantity required to obtain the best error bound second it show how these algorithm can be applied efficiently in the context of factored mdps where the transition model is specified using a dynamic bayesian network 
there are obvious reason for trying to automate the production of multilingual document among them are the rapidly growing need for such document the high cost and low availability of good translator and the fact that translator often need more time than is available to produce good multilingual version these problem are compounded when equivalent version of a document are needed in not just two or three but many language a is often the case in europe where there are now eleven official language in the european community this talk present some recent development in multilingual natural language generation mnlg these allow the automatic production of high quality multilingual document while avoiding many of the well known pitfall of the more familiar alternative of machine translation mt for example the difficulty of information extraction from a source document and the danger of source language bias 
several algorithm have already been implemented which combine association rule with first order logic formula although this resulted in several usable algorithm little attention wa payed until recently to the efficiency of these algorithm in this paper we present some new idea to turn one important intermediate step in the process of discovering such rule i e the discovery of frequent item set more efficient using an implementation that we coined farmer we show that indeed a speed up is obtained and that using these idea the performance is much more comparable to original association rule algorithm 
existing conflict detection method for csp s such a de kleer ginsberg cannot make use of powerful propagation which make them unusable for complex real world problem on the other hand powerful constraint propagation method lack the ability to extract dependency or conflict which make them unusable for many advanced ai reasoning method that require conflict a well a for interactive application that require explanation in this paper we present a non intrusive conflict detection algorithm called quickxplain that tackle those problem it can be applied to any propagation or inference algorithm a powerful a it may be our algorithm improves the efficiency of direct non intrusive conflict detector by recursively partitioning the problem into subproblems of half the size and by immediately skipping those subproblems that do not contain an element of the conflict q uickxplain is used a explanation component of an advanced industrial constraint based configuration tool 
introduction we describe the gpt system and it utilization over a number of example gpt general planning tool is an integrated software tool for modeling analyzing and solving a wide range of planning problem dealing with uncertainty and partial information that ha been used for u and others for research and teaching our approach is based on different state model that can handle various type of action dynamic deterministic and probabilistic and sensor feedback null partial 
combinatorial auction ca where bidder can bid on bundle of item can be very desirable market mechanism when the item sold exhibit complementarity and or substitutability so the bidder s valuation for bundle are not additive however in a basic ca the bidder may need to bid on exponentially many bundle leading to difficulty in determining those valuation undesirable information revelation and unnecessary communication in this paper we present a design of an auctioneer agent that us topological structure inherent in the problem to reduce the amount of information that it need from the bidder an analysis tool is presented a well a data structure for storing and optimally assimilating the information received from the bidder using this information the agent then narrow down the set of desirable welfare maximizing or pareto efficient allocation and decides which question to ask next several algorithm are presented that ask the bidder for value order and rank information 
although a partially observable markov decision process pomdp provides an appealing model for problem of planning under uncertainty exact algorithm for pomdps are intractable this motivates work on approximation algorithm and grid based approximation is a widely used approach we describe a novel approach to grid based approximation that us a variable resolution regular grid and show that it outperforms previous grid based approach to approximation 
singapore ha one of the busiest port in the world ship berthing is one of the problem faced by the planner at the port in this paper we study the ship berthing problem we first provide the problem formulation and study the complexity of the problem with different restriction in general the ship berthing problem is np complete although some of it variant may be solved quickly while a geometrical model is intuitive the model cannot be easily extended to handle clearance constraint and berth restriction rather than solving the problem geometrically we transform the problem into the problem of fixing direction of edge in graph to form directed acyclic graph with minimal lonqest path since the problem is np complete solving the problem exactly in polynomial time is highly unlikely a a result we devise a fast and effective greedy algorithm to can generate good solution the greedy method together with a tabu search like post optimization algorithm is able to return optimal or near optimal solution 
a technique is proposed for computing the weakest sufficient wsc and strongest necessary snc condition for formula in an expressive fragment of first order logic using quantifier elimination technique the efficacy of the approach is demonstrated by using the technique to compute snc s and wsc s for use in agent communication application theory approximation and generation of abductive hypothesis additionally we generalize recent result involving the generation of successor state axiom in the propositional situation calculus via snc s to the first order case subsumption result for existing approach to this problem and a re interpretation of the concept of forgetting a a process of quantifier elimination are also provided 
we provide a perspective on knowledge compilation which call for analyzing different compilation approach according to two key dimension the succinctness of the target compilation language and the class of query and transformation that the language support in polytime we argue that such analysis is necessary for placing new compilation approach within the context of existing one we also go beyond classical flat target compilation language based on cnf and dnf and consider a richer nested class based on directed acyclic graph which we show to include a relatively large number of target compilation language 
many planning problem exhibit a high degree of symmetry that cannot yet be exploited successfully by modern planning technology for example problem in the gripper domain in which a robot with two grippers must transfer ball from one room to another are trivial to the human problem solver because the high degree of symmetry in the domain mean that the order in which pair of ball are transported is irrelevant to the length of the shortest transportation plan however planner typically search all possible ordering giving rise to an exponential explosion of the search space this paper describes a way of detecting and exploiting symmetry in the solution of problem that demonstrate these characteristic we have implemented our technique in stan a graphplan based planner that us state analysis technique in a number of way to exploit the underlying structure of domain we have achieved a dramatic improvement in performance in solving problem exhibiting symmetry we present a range of result and indicate the further development we are now pursuing 
text mining concern the discovery of knowledge from unstructured textual data one important task is the discovery of rule that relate specific word and phrase although existing method for this task learn traditional logical rule soft matching method that utilize word frequency information generally work better for textual data this paper present a rule induction system textrise that allows for partial matching of text valued feature by combining rule based and instance based learning we present initial experiment applying textrise to corpus of book description and patent document retrieved from the web and compare it result to those of traditional rule and instance based method 
we propose a formal framework for modelling case based inference cbi which is a crucial part of the case based reasoning cbr methodology a a representation of the similarity structure of a system the concept of a similarity profile is introduced this concept make it possible to formalize the cbr hypothesis that similar problem have similar solution and to realize cbi in the form of constraint based inference in order to exploit the similarity structure more efficiently a probabilistic generalization of the constraintbased view is developed this formalization allows for realizing cbi in the context of probabilistic reasoning and statistical inference and hence make a powerful methodological framework accessible to cbr within the generalized setting a formalized cbr hypothesis corresponds to the assumption of a certain stochastic model and a memory of case can be seen a statistical data underlying the inference process a a particular result we establish an approximate probabilistic reasoning scheme which generalizes the constraint based approach 
whereas matching in description logic is now relatively well investigated there are only very few formal result on matching under additional side condition though these side condition were already present in the original paper by borgida and mcguinness introducing matching in dl the present paper close this gap for the dl and it sublanguages 
finding model of a predicate logic formula is a well known hard problem whose complexity is exponential in the number of variable however even though this number is kept constant substantial difference in complexity arise when searching for solution in different problem instance such a behavior appears to be quite general according to recent result reported in the literature in fact several class of hard problem exhibit a narrow phase transition with respect to some order parameter in correspondence of which the complexity dramatically rise up still remaining tractable elsewhere in this paper we provide an extensive experimental study on the emergence of a phase transition in the problem of matching a horn clause to a universe searching for a model of the clause or for a proof that no such model exists a it turn out phase transition in the matching problem depends in an essential way on two order parameter one capturing syntactic aspect of the clause structure intensional aspect while the other related to the structure of the universe extensional aspect 
the ac algorithm is a basic and widely used arc consistency enforcing algorithm in constraint satisfaction problem csp it strength lie in that it is simple empirically efficient and extensible however it worst case time complexity wa not considered optimal since the first complexity result for ac mackworth and freuder with the bound o ed where e is the number of constraint and d the size of the largest domain in this paper we show suprisingly that ac achieves the optimal worst case time complexity with o ed the result is applied to obtain a path consistency algorithm which ha the same time and space complexity a the best known theoretical result our experimental result show that the new approach to ac is comparable to the traditional ac implementation for simpler problem where ac is more efficient than other algorithm and significantly faster on hard instance 
a new general approach is described for approximate inference in first order probabilistic language using markov chain monte carlo mcmc technique in the space of concrete possible world underlying any given knowledge base the simplicity of the approach and it lazy construction of possible world make it possible to consider quite expressive language in particular we consider two extension to the basic relational probability model rpm defined by koller and pfeffer both of which have caused difficulty for exact algorithm the first extension deal with uncertainty about relation among object where mcmc sample over relational structure the second extension deal with uncertainty about the identity of individual where mcmc sample over set of equivalence class of object in both case we identify type of probability distribution that allow local decomposition of inference while encoding possible domain in a plausible way we apply our algorithm to simple example and show that the mcmc approach scale well 
the principle of minimal change is prevalent in various guise throughout the development of area such a reasoning about action belief change and nonmonotonic reasoning recent literature ha witnessed the proposal of several theory of action that adopt an explicit representation of causality it is claimed that an explicit notion of causality is able to deal with the frame problem in a manner not possible with traditional approach based on minimal change however such claim remain untested by all but representative example it is our purpose here to objectively test these claim in an abstract sense to determine whether an explicit representation of causality is capable of providing something that the principle of minimal change is unable to capture working towards this end we provide a precise characterisation of the limit of applicability of minimal change 
in the present paper we introduce the notion of variable assignment problem vap a an abstract framework for characterizing diagnosis component of the system to be diagnosed are put in correspondence with variable behavioral mode of the component are the value of the variable and a diagnosis is a variable assignment which explains the observation of the diagnostic problem by considering the constraint put by the domain theory in order to have a concise representation of diagnosis and to reduce the search space we introduce the notion of scenario for representing a set of diagnosis the paper discus the definition of preference criterion for ranking solution and their use for guiding the heuristic search for diagnosis experimental data are reported for the evaluation of such a heuristic search on a real world diagnostic problem concerning the identification of fault in a space robot arm in this domain where a high number of diagnosis may be possible our approach allows one to get a concise representation of the large number of solution and to define effective diagnostic strategy able to provide relevant information about fault localization and identification 
in this article we describe a possibilistic probabilistic conditional planner called ptlplan being inspired by bacchus and kabanza s tlplan ptlplan is a progressive planner that us strategic knowledge encoded in a temporal logic to reduce it search space action effect and sensing can be context dependent and uncertain and the information the planning agent ha at each point in time is represented a a set of situation with associated possibility or probability besides presenting the planner itself it representation of action and plan and it algorithm we also provide some promising data from performance test 
within the framework of constraint satisfaction problem we propose a new scheme of cooperative parallel search the cooperation is realized by exchanging nogoods instantiation which can t be extended to a solution we associate a process with each solver and we introduce a manager of no good in order to regulate exchange of nogoods each solver run the algorithm forward checking with nogood recording we add to algorithm a phase of interpretation which limit the size of the search tree according to the received nogoods solver differ from each other in ordering variable and or value by using different heuristic the interest of our approach is shown experimentally in particular we obtain linear or superlinear speed up for consistent problem like for inconsistent one up to about ten solver 
deductive mode estimation ha become an essential component of robotic space system like nasa s deep space probe future robot will serve a component of large robotic network monitoring these network will require modeling language and estimator that handle the sophisticated behavior of robotic component this paper introduces rmpl a rich modeling language that combine reactive programming construct with probabilistic constraint based modeling and that offer a simple semantics in term of hidden markov model hmms to support efficient realtime deduction we translate rmpl model into a compact encoding of hmms called probabilistic hierarchical constraint automaton phca finally we use these model to track a system s most likely state by extending traditional hmm belief update 
in this paper a simple classification modelbased on a linguistic processing that producessyntactic information i e grammatical categoriesof word in document is described moreover an experimental set up able to dynamicallysupport several test of different approacheshas been realized in order to get largescale empirical evidence the evidence forthis are discussed with comparative evaluationagainst other alternative more complex model definition of the problem 
a few research g roups are now proposing a series of step and methodology for developing ontology however mainly due to the fact t hat ontological engineering is still a relatively immature discipline each work group employ it own methodology our goal is to present the most representative methodology used in ontology development and to perform an analysis of such methodology against the same framework of reference so the goal of this paper is not to provide new insight about methodology but to put it all i n one place and help people to select which methodology to use 
in the present work we examine the causal theory of action put forward by mccain and turner mccain and turner for determining ramification our principal aim is to provide a characterisation of this causal theory of action in term of a shoham like preferential semantics shoham this would have a twofold advantage it would place mccain and turner s theory in perspective allowing a comparison with other logic of action and it would allow u to glean further insight into the nature of causality underlying their work we begin by showing that our aim is not attainable by a preferential mechanism alone at this point we do not abandon preferential semantics altogether but augment it in order to arrive at the desired result we draw the following moral which is at the heart of our paper two component minimal change under a preferential structure and causality are required to provide a concise solution to the frame and ramification problem 
in this paper we consider the projection task determining what doe or doe not hold after performing a sequence of action in a general setting where a solution to the frame problem may or may not be available and where online information from sensor may or may not be applicable we formally characterize the projection task for action theory of this sort and show how a generalized form of regression produce correct answer whenever it can be used we characterize condition on action theory sequence of action and sensing information that are sufficient to guarantee that regression can be used and present a provably correct regressionbased procedure in prolog for performing the task under these condition 
news report are an important source of information about society their analysis allows to understand it current interest and to measure the social importance and influence of different event in this paper we focus on the study of a very common phenomenon of news the influence of the peak news topic on other current news topic we propose a text mining method to analyze such influence we differentiate between the observable association those discovered from the newspaper and the real world association and propose a technique in which the real one can be i nferred from the observable one we argue that the discovery of the ephemeral association can be translated into knowledge about interest of society and social behavior 
a very promising approach for integrating top down and bottom up proof search is the use of bottom up generated lemma in top down provers when generating lemma however the currently used lemma generation procedure suffer from the well known problem of forward reasoning method e g the proof goal is ignored in order to overcome these problem we propose two relevancy based lemma generation method for top down provers the first approach employ a bottom up level saturation procedure controlled by top down generated pattern which represent promising subgoals the second approach us evolutionary search and provides a self adaptive control of lemma generation and goal decomposition 
allen s well known interval algebra ha been developed for temporal representation and reasoning but there are also interesting spatial application where interval can be used a prototypical example are traffic scenario where car and their region of influence can be represented a interval on a road a the underlying line there are several difference of temporal and spatial interval which have to be considered when developing a spatial interval algebra in this paper we analyze the first important difference a opposed to temporal interval spatial interval can have an intrinsic direction with respect to the underlying line we develop an algebra for qualitative spatial representation and reasoning about directed interval identify tractable subset and show that path consistency is sufficient for deciding consistency for a particular subset which contains all base relation 
this article is an edited transcript of a lecture given at ijcai stockholm sweden on august the article summarizes concept principle and tool that were found useful in application involving causal modeling the principle are based on structural model semantics in which functional or counterfactua relationship representing autonomous physical process are the fundamental building block the article present the conceptual basis of this semantics illustrates it application in simple problem and discus it ramification to computational and cognitive problem concerning causation 
this paper discus some trend in model based diagnosis we consider some recent application and discus why they were possible the lesson we learned from them the new impulse that they gave to research in the field and the new challenge that emerged from them 
neural logic network or neulonet is a hybrid of neural network expert system it strength lie in it ability to learn and to represent human logic in decision making using component net rule the technique originally employed in neulonet learning is backpropagation however the resulting weight adjustment will lead to a loss in the logic of the net rule a new technique is now developed that allows the neulonet to learn by composing net rule using genetic programming this paper present experimental result to demonstrate this new and exciting capability in capturing human decision logic from example comparison will also be made between the use of net rule and the use of standard boolean logic of negation disjunction and conjunction in evolutionary computation 
the aim of our research is to build a reflexive agent that is able to either manifest an emotion it is feeling or to hide it if the agent decides to manifest it emotion it can establish what verbal or nonverbal signal to employ in it communication and how to combine and synchronize them in the decision of whether to express an emotion in a given context a number of factor are considered such a the agent s own personality and goal the interlocutor s characteristic and the context in planning how to communicate an emotion various factor are considered a well the available modality face gaze voice etc the cognitive ease in producing and processing the various signal the expressiveness of every signal in communicating specific meaning and finally the appropriateness of signal to social situation 
we introduce a language independent strategy for inducing part of speech tag from corpus unlike ot her technique that use language specific lexicon ru lesets and so forth to tag our algorithm bootstrap on ly from cluster property and language universal we describe the theory and illustrate an introductory exp eriment which verifies the feasibility of this nearknowledge free tag induction strategy we induce ta g for perfect syntactic c luster generated from the brown corpus getting of ou r hypothesis correct 
we propose to extend the temporal causal graph formalism used in model based diagnosis in order to deal with non trivial interaction like partial cancellation of fault effect a high level causal language is defined in which property such a the persistence of effect and the triggering or sustaining property of cause can be expressed various interaction phenomenon are associated with these feature instead of proposing an ad hoc reasoning mechanism to process this extended formalism the specification in this language are automatically translated into an event calculus based language having well established semantics our approach improves the way fault interaction and intermittent fault are coped with in temporal abductive diagnosis 
we address the conflict between identification and control or alternatively the conflict between exploration and exploitation within the framework of reinforcement learning q learning ha recently become a popular off policy reinforcement learning method the conflict between exploration and exploitation slows down q learning algorithm their performance doe not scale up and degrades rapidly a the number of state and action increase one reason for this slowness is that exploration lack the ability to extrapolate and interpolate from learning and to a large extent ha to reinvent the wheel moreover not all reinforcement problem one encounter are finite state and action system our approach to solving continuous state and action problem is to approximate the continuous state and action space with finite set of state and action and then to apply a finite state and action learning method this approach provides the mean for solving continuous state and action problem but doe not yet address the performance problem associated with scaling up state and action we address the scaling problem using functional approximation method towards that end this paper introduces two new reinforcement algorithm qlvq and quad q learning respectively and show their successful application for cart centering and fractal compression 
modeling an experimental system often result in a number of alternative model that are justified equally well by the experimental data in order to discriminate between these model additional experiment are needed we present a method for the discrimination of model in the form of semiquantitative differential equation the method is a generalization of previous work in model discrimination it is based on an entropy criterion for the selection of the most informative experiment which can handle case where the model predict multiple qualitative behavior the applicability of the method is demonstrated on a real life example the discrimination of a set of competing model of the growth of phytoplankton in a bioreactor 
in this paper we propose a potts spin mean field annealed network to address the open independent and incompatibility class of causal reasoning also said abduction abductive diagnosis the strong feature of the current work is it characterization of the reasoning task in these class by an energy target function computation of a scenario also said explanation is done by mean of mean field equation the application of the model to small and large scale causal problem reveals it efficacy and robustness in handling varied and multiple causal interaction 
computing least common subsumers lcs is an inference task that can be used to support the bottom up construction of knowledge base for kr system based on description logic previous work on how to compute the lcs ha concentrated on description logic that allow for universal value restriction but not for existential restriction the main new contribution of this paper is the treatment of description logic with existential restriction more precisely we show that for the description logic ale which allows for conjunction universal value restriction existential restriction negation of atomic concept a well a the top and the bottom concept the lcs always exists and can effectively be computed our approach for computing the lcs is based on an appropriate representation of concept description by certain tree and a characterization of subsumption by homomorphism between these tree the lcs operation then corresponds to the product operation on tree 
we present a theorem prover for quantified boolean formula and evaluate it on random quantified formula and formula that represent problem from automated planning even though the notion of quantified boolean formula is theoretically important automated reasoning with qbf ha not been thoroughly investigated universal quantifier are needed in representing many computational problem that cannot be easily translated to the propositional logic and solved by satisfiability algorithm therefore efficient reasoning with qbf is important the davis putnam procedure can be extended to evaluate quantified boolean formula a straightforward algorithm of this kind is not very efficient we identify universal quantifier a the main area where improvement to the basic algorithm can be made we present a number of technique for reducing the amount of search that is needed and evaluate their effectiveness by running the algorithm on a collection of formula obtained from planning and generated randomly for the structured problem we consider the technique lead to a dramatic speed up 
in this paper we consider the projection task determining what doe or doe not hold after performing a sequence of action in a general setting where a solution to the frame problem may or may not be available and where online information from sensor may or may not be applicable we formally characterize the projection task for action theory of this sort and show how a generalized form of regression produce correct answer whenever it can be used we characterize condition on action theory sequence of action and sensing information that are sufficient to guarantee that regression can be used and present a provably correct regressionbased procedure in prolog for performing the task under these condition 
in this paper we present a scheme of postulate for revising epistemic state by conditional belief these postulate are supported mainly by following the specific non classical nature of conditionals and the aim of preserving conditional belief is achieved by studying specific interaction between conditionals represented properly by two relation because one of the postulate claim propositional belief revision to be a special case of conditional belief revision our framework also cover the work of darwiche and pearl darwiche and pearl and we show that all postulate presented there may be derived from our postulate we state representation theorem for the principal postulate and finally we present a conditional belief operator obeying all of the postulate by using ordinal conditional function a representation of epistemic state 
ai research ha developed an extensive collection of method to solve state space problem using the challenging domain of sokoban this paper study the effect of search enhancement on program performance we show that the current state of the art in at generally requires a large programming and research effort into domain dependent method to solve even moderately complex problem in such difficult domain the application of domain specific knowledge to exploit property of the search space can result in large reduction in the size of the search tree often several order of magnitude per search enhancement understanding the effect of these enhancement on the search lead to a new taxonomy of search enhancement and a new framework for developing single agent search application this is used to illustrate the large gap between what is portrayed in the literature versus what is needed in practice 
data intensive web site have created a new form of knowledge base a richly structured body of data several novel system for creating data intensive web site support declarative specification of a site s structure and content i e the page the data available in each page and the link between page declarative system provide a platform on which a technique can be developed that further simplify the task of constructing and maintaining web site this paper address the problem of specifying and verifying integrity constraint on a web site s structure we describe a language that can capture many practical constraint and an accompanying sound and complete verification algorithm the algorithm ha the important property that if the constraint are violated it proposes fix to either the constraint or to the site definition finally we establish tight bound on the complexity of the verification problem we consider 
this paper is concerned with providing a common framework for both the logical specification and execution of agent while numerous high level agent theory have been proposed in order to model agent such a theory of intention these often have little formal connection to practical agentbased system on the other hand many of the agent based programming language used for implementing real agent lack firm logical semantics our approach is to define a logical framework in which agent can be specified and then show how such specification can be directly executed in order to implement the agent s behaviour we here extend this approach to capture an important aspect of practical agent namely their resource bounded nature we present a logic in which resource boundedness can be specified and then consider how specification within this logic can be directly executed the mechanism we use to capture finite resource is to replace the standard modal logic previously used to represent an agent s belief with a multi context representation of belief thus providing tight control over the agent s reasoning capability where necessary this logical framework provides the basis for the specification and execution of agent comprising dynamic temporal activity deliberation concerning goal and resource bounded reasoning 
planning under partial observability is one of the most significant and challenging planning problem it ha been shown to be hard both theoretically and experimentally in this paper we present a novel approach to the problem of planning under partial observability in non deterministic domain we propose an algorithm that search through a possibly cyclic and or graph induced by the domain the algorithm generates conditional plan that are guaranteed to achieve the goal despite of the uncertainty in the initial condition the uncertain effect of action and the partial observability of the domain we implement the algorithm by mean of bdd based symbolic model checking technique in order to tackle in practice the exponential blow up of the search space we show experimentally that our approach is practical by evaluating the planner with a set of problem taken from the literature and comparing it with other state of the art planner for partially observable domain 
a case based approach to adaptation for estimation task is presented in which there is no requirement for explicit adaptation knowledge instead a target case is estimated from the value of three existing case one retrieved for it similarity to the target case and the others to provide the knowledge required to adapt the similar case with recursive application of the adaptation process any problem space can be fully covered by fewer than nk selected case where n is the number of case attribute and k is the number of value of each attribute moreover a k k problem space is fully covered by any set of k known case provided there is no redundancy in the case library circumstance in which the approach is appropriate are identified by theoretical analysis and confirmed by experimental result 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
model of computer user that are learned on the basis of data can make use of two type of information data about user in general and data about the current individual user focusing on user model that take the form of bayesian network we compare four type of model that represent different way of combining these two type of data model of the four type are applied to the data of an experiment and they are evaluated according to theoretical empirical and practical criterion one of the model type is a new variant of the ahugin method for adapting the probability of a bayesian network while it is being used differential adaptation is a principled way of determining the speed with which each aspect of a network is adapted to an individual user 
this paper present a system to support writinga survey of a specific domain the systemutilizes reference information that consistsof reference relationship between paper andthe information derived from the descriptionaround citation we think the following areinevitable for writing a survey collecting papersof the specific domain and understandingtheir essence and difference among them therefore we firstly extract fragment of paperswhere the author describes the 
from a early a month of age human child distinguish between motion pattern generated by animate object from pattern generated by moving inanimate object even when the only stimulus that the child observes is a single point of light moving against a blank background the mechanism by which the animate inanimate distinction are made are unknown but have been shown to rely only upon the spatial and temporal property of the movement in this paper i present both a multiagent architecture that performs this classification a well a detailed comparison of the individual agent contribution against human baseline 
we propose a new definition of abduction in logic programming and contrast it with that of kakas and mancarella s we then introduce a rewriting system for answering query and generating explanation and show that it is both sound and complete under the partial stable model semantics and sound and complete under the answer set semantics when the underlying program is so called odd loop free we discus an application of the work to a problem in reasoning about action and provide some experimental result 
r max is a very simple model based reinforcement learning algorithm which can attain near optimal average reward in polynomial time in r max the agent always maintains a complete but possibly inaccurate model of it environment and act based on the optimal policy derived from this model the model is initialized in an optimistic fashion all action in all state return the maximal possible reward hence the name during execution it is updated based on the agent s observation r max improves upon several previous algorithm it is simpler and more general than kearns and singh s e algorithm covering zero sum stochastic game it ha a built in mechanism for resolving the exploration v exploitation dilemma it formally justifies the optimism under uncertainty bias used in many rl algorithm it is simpler more general and more efficient than brafman and tennenholtz s lsg algorithm for learning in single controller stochastic game it generalizes the algorithm by monderer and tennenholtz for learning in repeated game it is the only algorithm for learning in repeated game to date which is provably efficient considerably improving and simplifying previous algorithm by banos and by megiddo 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
a challenge of future knowledge management and decision support system is to combine the storage and effective reuse of data systematically captured a process or system information with user experience in dealing with problem and non trivial situation in cbr situation specific user experience are typically captured in case in our approach case are linked within a semantic network of more general domain knowledge in this paper we present a way to automate the construction and dynamical refinement of such a model of case specific and general knowledge on the basis of external process data continuously being generated a data mining method based on a bayesian network approach is used we are also looking into how the notion of causality being a central issue in both bns and model based ai can be compared and better understood by relating it to such a combined model 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
the paper present a structured modeling language sml and a relational database framework for specification and automated generation of causal model the framework describes a relational database scheme for encoding a library of causal network template modeling the basic component in a modeling domain sml provides a formal language for specifying model a structured component that can be composed from the basic component the language enables specification of model a parameterized relational query that can be instantiated for specific model instance the paper describes an algorithm that given a library and a specification computes a causal model in time and space linear in the number of basic component the algorithm enables model reuse by combining model fragment from the template library to compose new model the present automated modeling approach ha been implemented using the structured query language sql and a relational database environment the approach ha been successfully used for modeling an automated work cell in a real life digital manufacturing application 
this paper present a geometric based approach for multiple mobile robot motion coordination all the robot path being computed independently we address the problem of coordinating the motion of the robot along their own path in such a way they do not collide each other the proposed algorithm is based on a bounding box representation of the obstacle in the so called coordination diagram the algorithm is resolution complete it efficiency is illustrated by example involving more than robot 
to perform rational decision making autonomous agent need considerable computational resource in multi agent setting when other agent are present in the environment these demand are even more severe we investigate way in which the agent s knowledge and the result of deliberative decision making can be compiled to reduce the complexity of decision making procedure and to save time in urgent situation we use machine learning algorithm to compile decision theoretic deliberation into condition action rule on how to coordinate in a multi agent environment using different learning algorithm we endow a resource bounded agent with a tapestry of decision making tool ranging from purely reactive to fully deliberative one the agent can then select a method depending on the time constraint of the particular situation we also propose combining the decision making tool so that for example more reactive method serve a a pre processing stage to the more accurate but slower deliberative decision making one we validate our framework with experimental result in simulated coordinated defense the experiment show that compiling the result of decision making save deliberation time while offering good performance in our multi agent domain 
context specific independence csi refers to conditional independency that are true only in specific context it ha been found useful in various inference algorithm for bayesian network this paper study the role of csi in general we provide a characterization of the computational leverage offered by csi without referring to particular inference algorithm we identify the issue that need to be addressed in order to exploit the leverage and show how those issue can be addressed we also provide empirical evidence that demonstrates the usefulness of csi 
the golomb ruler problem ha been proposed a a challengingconstraint satisfaction problem we consider a large number ofdifferent model of this problem both binary and non binary the problemcan be modelled using quaternary constraint but in practice usinga set of auxiliary variable and ternary constraint give better result a binary encoding of the problem give a smaller search tree but isimpractical because it take far longer to run we compare variable orderingheuristics 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we investigate the ability of two central encoding method to propagate reachability and relevance information using resolution step more specifically we compare the ability of unit propagation and higher order resolution step to propagate reachability and relevance information in the context of the linear and graphplan encoding scheme to the ability of a natural class of reachability and relevance algorithm that operate at the plan level a a result of our observation and additional consideration we experiment with a preprocessing step based on limited binary resolution that show nice result 
we describe a framework that help student learn from example by generating example problem solution whose level of detail is tailored to the student domain knowledge the framework us natural language generation technique and a probabilistic student model to selectively introduce gap in the example solution so that the student can practice applying rule learned from previous example in problem solving episode of difficulty adequate to her knowledge filling in solution gap is part of the meta cognitive skill known a self explanation generate explanation to oneself to clarify an example solution which is crucial to effectively learn from example in this paper we describe how example with tailored solution gap are generated and how they are used to support student in learning through gap filling self explanation 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we are interested in the problem of determining a course of action to achieve a desired objective in a non deterministic environment markov decision process mdps provide a framework for representing this action selection problem and there are a number of algorithm that learn optimal policy within this formulation this framework ha also been used to study state space abstraction problem decomposition and policy reuse these technique sacrifice optimality of their solution for improved learning speed in this paper we examine the suboptimality of reusing policy that are solution to subproblems this is done within a restricted class of mdps namely those where non zero reward is received only upon reaching a goal state we introduce the definition of a subproblem within this class and provide motivation for how reuse of subproblem solution can speed up learning the contribution of this paper is the derivation of a tight bound on the loss in optimality from this reuse we examine a bound that is based on bellman error which applies to all mdps but is not tight enough to be useful we contribute our own theoretical result that give an empirically tight bound on this suboptimality 
decision tree grafting add node to an existing decision tree with the objective of reducing prediction error a new grafting algorithm is presented that considers one set of training data only for each leaf of the initial decision tree the set of case that fail at most one test on the path to the leaf this new technique is demonstrated to retain the error reduction power of the original grafting algorithm while dramatically reducing compute time and the complexity of the inferred tree bias variance analysis reveal that the original grafting technique operated primarily by variance reduction while the new technique reduces both bias and variance 
requirement engineering is a complex task which benefit from computer support despite the progress made in automatic reasoning on requirement the tool supporting requirement elicitation remain difficult to use in this paper we propose a novel approach where a tool s reasoning is intimately linked to the dialogue it ha with it user because the dialogue is guided by rule ensuring coherence the interaction with the tool is more natural we discus in detail the rule we use to organise the dialogue and how we apply them to the requirement elicitation tool we present an evaluation of this approach demonstrating improvement in usability during the elicitation process 
this paper we propose a set of techniquesto largely automate the process of ka by usingtechnologies based on information extraction ie information retrieval and natural language processing weaim to reduce all the impeding factor mention above andthereby contribute to the wider utility of the knowledgemanagement tool in particular we intend to reduce theintrospection of knowledge engineer or the extendedelicitations of knowledge from expert by extensive textualanalysis using 
the use of linear data fusion is a fast developing area in the field of military information and combat system however the use of data fusion in conventional application area is not a wide spread to date linear data fusion ha been used only in application in which substantial knowledge of both the problem domain and the sensor device in use are available however in application such a condition monitoring the problem domain can be very complex with little or no knowledge about the interaction between measured parameter this paper describes the use of non linear self learning or self organising system a a tool for data fusion since these system can learn complex interrelationship between a number of parameter and use this information a a tool for improved classification 
previously the plan language cc golog wa introduced for the purpose of specifying event driven behavior typically found in robot controller so far however cc golog is usable only for projecting the outcome of a plan and it is unclear how to actually execute plan on line on a robot in this paper we provide such an execution model for cc golog and in addition show how to interleave execution with a new kind of time bounded projection along the way we also demonstrate how a typical robot control architecture where a high level controller communicates with low level process via message can be directly modelled in cc golog 
an important problem in clustering is how to decide what is the best set of cluster for a given data set in term of both the number of cluster and the membership of those cluster in this paper we develop four criterion for measuring the quality of different set of cluster these criterion are designed so that different criterion prefer cluster set that generalise at different level of granularity we evaluate the suitability of these criterion for non hierarchical clustering of the result returned by a search engine we also compare the number of cluster chosen by these criterion with the number of cluster chosen by a group of human subject our result demonstrate that our criterion match the variability exhibited by human subject indicating there is no single perfect criterion instead it is necessary to select the correct criterion to match a human subject s generalisation need 
financial prediction is so far the most important application in contemporary scientific study in this paper we present a fully integrated stock prediction system norn finance forecaster a neural oscillatory based recurrent network for finance prediction system to provide both a long term trend prediction and b short term stock price prediction one of the major characteristic of the proposed system is the automation of the conventional financial technical analysis technique such a market pattern analysis via noegm neural oscillatory based elastic graph matching model and it integration with the time difference recurrent neural network model this will provide a fully integrated and automated tool for analytic and investigation of stock investment from the implementation point of view the stock pricing information of major hong kong stock in the period of to are being adopted for system training and evaluation a compared with contemporary neural prediction model the proposed system ha achieved challenging result in term of efficiency and accuracy 
the hit and the pagerank algorithm are eigenvector method for identifying authoritative or influential article given hyperlink or citation information that such algorithm should give consistent answer is surely a desideratum and in this paper we address the question of when they can be expected to give stable ranking under small perturbation to the hyperlink pattern using tool from matrix perturbation theory and markov chain theory we provide condition under which these method are stable and give specific example of instability when these condition are violated we also briefly describe a modification to hit that improves it stability 
xspirit is a professional expert system shell for knowledge acquisition inference and response using conditional logic and probability composed conditionals on propositional variable with finite domain are the communication tool between the user and the knowledge base making the process of acquisition inference and query comfortable and intelligible xspirit allows partial rather than complete information about the knowledge domain and supplement missing part by the principle of information fidelity by virtue of evident temporary information knowledge undergoes a well defined adaptation process respecting this principle again the construction and transformation of probability distribution a developed here allow acquired knowledge remaining uncertainty and strength of inference to be measured in the information unit bit xspirit allows large scale application with hundred of composed conditionals and umpteen variable 
this paper present exclaim a hybrid language for knowledge representation and reasoning originally developed a an operationalization language for the kads knowledge based system kb development methodology exclaim ha a meta level architecture it structure the knowledge on three level namely the domain inference and task level an extension of a description logic is used for implementing the domain level the inference and task level are general logic program integrated with the domain level by mean of upward and downward reflection rule which describe the automatic domain operation performed whenever argument of inference or task are accessed inference and task support non deterministic reasoning which in turn requires a non monotonic domain level description logic offer a set of inference service some not available in other knowledge representation language which are extremely useful in knowledge modeling such inference service include domain level deduction semantic consistency verification and automatic classification of concept we argue that such validation and verification facility are important in assisting a knowledge engineer in developing model these model are reusable due to the layered architecture a well a to the possibility of writing generic inference using a reified membership relation 
helping end user build and check process model is a challenge for many science and engineering field many ai researcher have investigated useful way of verifying and validating knowledge base for ontology and rule but it is not easy to directly apply them to checking process model other technique developed for checking and refining planning knowledge tend to focus on automated plan generation rather than helping user author process information in this paper we propose a complementary approach which help user author and check process model our system called kanal relates piece of information in process model among themselves and to the existing kb analyzing how different piece of input are put together to achieve some effect it build interdependency model from this analysis and us them to find error and propose fix our initial evaluation show that kanal wa able to find most of the error in the process model and suggest useful fix including the fix that directly point to the source of the error 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
until now interval based temporal description logic dl did if at all only admit tboxes of a very restricted form namely acyclic macro definition in this paper we present a temporal dl that overcomes this deficieny and combine interval based temporal reasoning with general tboxes we argue that this combination is very interesting for many application domain an automaton based decision procedure is devised and a tight exptime complexity bound is obtained since the presented logic can be viewed a being equipped with a concrete domain our result can be seen from a different perspective we show that there exist interesting concrete domain for which reasoning with general tboxes in decidable 
time series prediction by artificial neural network anns are traditionally formulated a unconstrained optimization problem a an unconstrained formulation provides little guidance on search direction when a search get stuck in a poor local minimum we have proposed recently to use a constrained formulation in order to use constraint violation to provide additional guidance in this paper we formulate ann learning with cross validation for time series prediction a a non differentiable nonlinear constrained optimization problem based on our theory of lagrange multiplier for discrete constrained optimization we propose an efficient learning algorithm called violation guided back propagation vgbp that computes an approximate gradient using back propagation bp that introduces annealing to avoid blind acceptance of trial point and that applies a relax and tighten r t strategy to achieve faster convergence extensive experimental result on well known benchmark when compared to previous work show one to two order of magnitude improvement in prediction quality while using le weight 
market based mechanism such a auction are being studied a an appropriate mean for resource allocation in distributed and inultiagenl decision problem when agent value resource in combination rather than in isolation one generally relies on combinatorial auction where agent bid tor resource bundle or simultaneous auction for all resource we develop a different model where agent bid for required reources sequentially this model ha the advantage that it can be applied in setting where combinatorial and simultaneous model are infeasible e g when resource are made available at different point in time by different party a well a certain benefit in setting where combinatorial model are applicable we develop a dynamic programming model tor agent to compute bidding policy based on estimated distribution over price we also describe how these distribution are updated to provide a learning model for bidding behavior 
a tight integration of mitchell s version space algorithm with agrawal et al s apriori algorithm is presented the algorithm can be used to generate pattern that satisfy a variety of constraint on data constraint that can be imposed on pattern include the generality relation among pattern and imposing a minimum or a maximum frequency on data set of interest the theoretical framework is applied to an important application in chemo informatics i e that of finding fragment of interest within a given set of compound fragment are linearly connected substructure of compound an implementation a well a preliminary experiment within the application are presented 
the representational issue of preference in the framework of a possibilistic qualitative ordinal decision model under uncertainty were originally introduced few year ago by dubois and prade and more recently linked to case based decision problem by dubois et al in this approach the uncertainty is assumed to be of possibilistic nature uncertainty or similarity and preference on consequence are both measured on commensurate ordinal scale however in case based decision problem similarity or preference on consequence may sometimes take value that are incomparable in order to cope with some of these situation we propose an extension of the model where both preference and uncertainty arc graded on distributive lattice providing axiomatic setting for characterising a pessimistic and an optimistic qualitative utility finally we extend our proposal to also include belief state that may be partially inconsistent supplying element for a qualitative case based decision methodology 
this paper introduces grounded model and compare them to axiomatic model of mathematics grounded model differ from axiomatic theory in establishing explicit connection between language and reality that are learnt through language game they are constructed and updated by autonomous agent connected to their environment through sensor and actuator using some conceptualization mechanism and language game described in steel they are based on conceptualization and support a form of intuitive reasoning which can be done sometimes by constraint satisfaction and it is argued to be the basis of some axiomatizations this is illustrated with a simple example of spatial reasoning 
a new algorithm for solving the three dimensional container packing problem is proposed in this paper this new algorithm deviate from the traditional approach of wall building and layering it us the concept of building growing from multiple side of the container we tested our method using all test case from the or library experimental result indicate that the new algorithm is able to achieve an average packing utilization of more than this is better than the result reported in the literature 
in robot navigation one of the important and fundamental issue is to reconstruct position of landmark or vision sensor locating around the robot this paper proposes a method for reconstructing qualitative position of multiple vision sensor from qualitative information observed by the vision sensor i e motion direction of moving object the process iterates the following step observing motion direction of moving object from the vision sensor classifying the vision sensor into spatially classified pair acquiring three point constraint and propagating the constraint the method have been evaluated with simulation 
stochastic local search sl technique are very effective in solving hard propositional satisfiability problem this ha lead to the popularity of the encode solve paradigm in which different problem are encoded a propositional satisfiability problem to which sl technique are applied in ai planning is the main area in which this methodology is used yet it seems plausible that sl method should perform better when applied to the original problem space whose structure they can exploit a part of our attempt to validate this thesis we experimented with lpsp a planner that applies sl technique to the space of linear plan lpsp outperforms sl applied to encoded planning problem that enforce a similar linearity assumption because of it ability to exploit the special structure of planning problem additional experiment reported in a longer version of this paper conducted on the hamiltonian circuit problem lend farther support to our thesis 
deciding whether a propositional formula in conjunctive normal form is satisfiable sat is an np complete problem the problem becomes linear when the formula contains binary clause only interestingly the reduction to sat of a number of well known and important problem such a classical ai planning and automatic test pattern generation for circuit yield formula containing many binary clause in this paper we introduce and experiment with simplify a formula simplifier targeted at such problem simplify construct the implication graph corresponding to the binary clause in the formula and us this graph to deduce new unit literal the deduced literal are used to simplify the formula and update the graph and so on until stabilization finally we use the graph to construct an equivalent simpler set of binary clause experimental evaluation of this simplifier on a number of bench mark formula produced by encoding ai planning problem prove simplify to be fast and effective 
in the last year the investigation on description logic dl ha been driven by the goal of applying them in several area such a software engineering information system database information integration and intelligent access to the web the modeling requirement arising in the above area have stimulated the need for very rich language including fixpoint construct to represent recursive structure we study a dl comprising the most general form of fixpoint construct on concept all classical concept forming construct plus inverse role n ary relation qualified number restriction and inclusion assertion we establish the exptime decidability of such logic by presenting a decision procedure based on a reduction to nonemptiness of alternating automaton on infinite tree we observe that this is the first decidability result for a logic combining inverse role number restriction and general fixpoints 
one of the more controversial recent planning algorithm is the shop algorithm an htn planning algorithm that plan for task in the same order that they are to be executed shop can use domaindependent knowledge to generate plan very quickly but it can be difficult to write good knowledge base for shop our hypothesis is that this difficulty is because shop s total ordering requirement for the subtasks of it method is more restrictive than it need to be to examine this hypothesis we have developed a new htn planning algorithm called shop like shop shop is sound and complete and it construct plan in the same order that they will later be executed but unlike shop shop allows the subtasks of each method to be partially ordered our experimental result suggest that in some problem domain the difficulty of writing shop knowledge base derives from shop s total ordering requirement and that in such case shop can plan a efficiently a shop using knowledge base simpler than those needed by shop 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
the temporal difference td learning algorithm offer the hope that the arduous task of manually tuning the evaluation function weight of game playing program can be automated with one exception td gammon td learning ha not been demonstrated to be effictive in a high performance world class game palying program further there ha been doubt expressed by game program developer that learned weight could compete with the best hand tuned weight chinook is the world man machine tuned over year this paper show that td learinng is capable of competing with the best human effort 
we study the complexity of model checking in proposit ional nonmonotonic logic specifically we first define the problem of model checking in such formalism based on the fact that several nonmonotonic logic make use of interpretation structure i e default extension stable expansion universal kripke model which are more complex than standard interpretation of propositional logic then we analyze the complexity of checking whether a given interpretation structure satisfies a nonmonotonic theory in particular we characterize the complexity of model checking for reiter s default logie and it restriction moore s autoepistemie logic and several nonmonotonic modal logic the result obtained show that in all such formalism model checking is computationally easier than logical inference 
decision making is particularly important for emergency manager a they often need to make quick and high quality decision under stress based on scratch and inadequate information and to follow expert knowledge or past experience the potential release of radioactive material from the guangdong nuclear power station gnp at daya bay though is highly unlikely could perhaps be the most dreaded disaster which would cause drastic damage to life and property the government of the hong kong special administrative region government hksar ha therefore in completed the daya bay contingency plan dbcp to prepare for such disaster to supplement the expert in assisting disaster manager with a useful tool to make better quality decision based on well structured accurate sufficient expert knowledge a prototype expert system ha been developed to cover two major area of the plan namely a determination of activation level of the dbcp and provision of an action checklist and b recommendation on counter measure 
the success of evolutionary method on standard control learning task ha created a need for new benchmark the classic pole balancing problem is no longer difficult enough to serve a a viable yard stick for measuring the learning efficiency of these system the double pole case where two pole connected to the cart must be balanced simultaneously is much more difficult especially when ve locity information is not available in this article we demonstrate a neuroevolution system enforced sub population esp that is used to evolve a con troller for the standard double pole task and a much harder non markovian version in both case our result show that esp is faster than other neuroevolution method in addition we introduce an in cremental method that evolves on a sequence of task and utilizes a local search technique deltacoding to sustain diversity this method enables the system to solve even more difficult version of the task where direct evolution cannot 
abstract global localization is the problem of determining the position of a robot under global uncertainty this problem can be divided in two phase from the sensor data or sensor view determine the set of location where the robot can be and devise a strategy by which the robot can correctly eliminate all but the right location the approach proposed in this paper is based on markov localization it applies the principal component method to get rotation invariant feature for each location of the map a bayesian classification system to cluster the feature and polar correlation between the sensor view and the local map view to determine the location where the robot can be in order to solve efficiently the localization problem a well a to consider the perceptual limitation of the sensor the possible location of the robot are restricted to be in a roadmap that keep the robot close to obstacle and correlation between the possible local map view are pre computed the hypothesis are clustered and a greedy search determines the robot movement to reduce the number of cluster of hypothesis this approach is tested using a simulated and a real mobile robot with promising result 
stochastic local search sl algorithm for prepositional satisfiability testing sat have become popular and powerful tool for solving suitably encoded hard combinatorial from different domain like e g planning consequently there is a considerable interest in finding sat encoding which facilitate the efficient application of sl algorithm in this work we study how two encoding scheme for combinatorial problem like the well known constraint satisfaction or hamilton circuit problem affect sl performance on the sat encoded instance to explain the observed performance difference we identify feature of the induces search space which affect sl performance we furthermore present initial result of a comparitive analysis of the performance of the sat encoding and solving approach versus that of native sl algorithm directly applied to the unencoded problem instance 
computational model of analogical problem solving have traditionally described source and target domain in term of their causal structure but psychological research show that visual reasoning play a part for many kind of analogy this paper describes a model that transfer a solution from a source analog to a new target problem using only visual knowledge represented symbolically the knowledge representation is based on a language of primitive visual element and transformation we found that visual knowledge is sufficient for transfer but that causal knowledge is needed to determine if the transferred solution is appropriate 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
weakening implication by assuming the object identity bias allows for both a model theoretical and a proof theoretical definition of a novel and more manageable ordering relationship over clausal space in this paper we give two important result namely the soundness and the refutation completeness through a subsumption theorem of the underlying derivation procedure that make this relationship particularly appealing for inducing a generalization model for clausal search space 
this paper investigates the problem of policy learning in multiagentenvironments using the stochastic game framework whichwe briefly overview we introduce two property a desirable forum learning agent when in the presence of other learning agent namely rationality and convergence we examine existing reinforcementlearning algorithm according to these two propertiesand notice that they fail to meet both criterion we then contributea new learning algorithm adjusted policy 
plan library are the most important knowledge source of many plan recognition system the plan decomposition they contain provide information about how a plan ha to be executed to actually achieve it associated goal and be recognized by the system this paper present an approach to the automatic acquisition of plan decomposition from sample action sequence in particular a clustering algorithm is introduced that allows group of similar sequence to be discovered and used for the generation of plan library empirical test indicate that these library can indeed be successfully used for plan recognition purpose 
most conventional law equation discovery system such a bacon require experimental environment to acquire their necessary data the mathematical technique such a linear system identification and neural network fitting presume the class of equation to model given observed data set the study reported in this paper proposes a novel method to discover an admissible model equation from a given set of observed data while the equation is ensured to reflect first principle governing the objective system the power of the proposed method come from the use of the scale type of the observed quantity a mathematical property of identity and quasi bi variate fitting to the given data set it principle and algorithm are described with moderately complex example and it practicality is demonstrated through a real application to psychological and sociological law equation discovery 
disjunctive logic programming dlp with sta ble model semantics is a powerful nonmono tonic formalism for knowledge representation and reasoning reasoning with dlp is harder than with normal v free logic program liecause stable model checking deciding whether a given model is a stable model of a propositional dlp program is co np complet e while it is polynomial for normal logic program this paper proposes a new transformation 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute to any output class to address this we have developed an algorithm called lrex for local rule extraction which tackle these issue by extracting rule at two level hrex extract rule by examining the hidden unit to class assignment while mrex extract rule based on the input space to output space mapping the rule extracted by our algorithm are compared and contrasted against a competing local rule extraction system the central claim of this paper is that local function network such a radial basis function rbf network have a suitable architecture based on gaussian function that is amenable to rule extraction 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
sensitivity analysis of neural network is useful for network design pich used a stochastic model to describe the multilayer perceptron mlp but it doesn t match the true mlp closely and too severe limitation are imposed on both input and weight perturbation this paper attempt to generalize pich s stochastic model of mlp and derive an universal expression of mlp s sensitivity for all sigmoidal activation function without any restriction on input and output perturbation the effect of network design parameter such a the number of layer the number of neuron per layer and the chosen activation function are analyzed and they provide useful information for network design decision making furthermore we use our sensitivity expression to design mlp for a given application it can help to design the network structure a well a the training of mlp 
we analyse the complexity of standard and weak model checking for propositional default logic in particular we solve the open problem of complexity in case of normal default theory and introduce a new ample class of default theory with a tractable model checking problem 
in this paper we present and compare automatically generated title for machine translated document using several different statistic based method a na ve bayesian a k nearest neighbour a tf idf and an iterative expectation maximization method for title generation were applied to original english news document and again to the same document translated from english into portuguese french or german and back to english using systran the autosummarization function of microsoft word wa used a a base line result on several metric show that the statistic based method of title generation for machine translated document are fairly language independent and title generation is possible at a level approaching the accuracy of title generated for the original english document 
this paper describes the region occlusion calculus roc that can be used to model spatial occlusion and the effect of motion parallax of arbitrary shaped object roc assumes the region based ontology of rcc and extends galton s line of sight calculus by allowing concave shaped object into the modelled domain this extension is used to describe the effect of mutually occluding body the inclusion of van benthem s axiomatisation of comparative nearness facilitates reasoning about relative distance between occluding body further an envisionment table is developed to model sequence of occlusion event enabling reasoning about object and their image formed in a changing visual field 
we show that node of high degree tend to occur infrequently in random graph but frequently in a wide variety of graph associated with real world search problem we then study some alternative model for randomly generating graph which have been proposed to give more realistic topology for example we show that watt and strogatz s small world model ha a narrow distribution of node degree on the other hand barab si and albert s power law model give graph with both node of high degree and a small world topology these graph may therefore be useful for benchmarking we then measure the impact of node of high degree and a small world topology on the cost of coloring graph the long tail in search cost observed with small world graph disappears when these graph are also constructed to contain node of high degree we conjecture that this is a result of the small size of their backbone pair of edge that are frozen to be the same color 
in recent year auction have grown in interest within the ai community a innovative mechanism for resource allocation the primary contribution of this paper is to identify a family of hybrid auction called survival auction which combine the benefit of both sealed bid auction namely quick and predictable termination time and ascending bid auction namely more information revelation often leading among other thing to better allocation and greater expected revenue survival auction are multi round sealed bid auction with an information revelation component in which some bidder are eliminated from the auction from one round to the next these auction are intuitive easy to implement and most importantly provably optimal more precisely we show that a the survival auction in which all but the lowest bidder make it into the next round the auction last for n round when there are n bidder is strategically equivalent to the japanese ascending bid auction which itself ha been proven to be optimal in many setting and that b under certain symmetry condition even a survival auction in which only the two highest bidder make it into the next round the auction last only two round is nash outcome equivalent to the japanese auction 
most study concerning constraint satisfaction problem csps involve variable that take value from small domain this paper deal with an alternative form of temporal csps the number of variable is relatively small and the domain are large collection of interval such situation may arise in temporal database where several type of query can be modeled and processed a csps for these problem systematic csp algorithm can take advantage of temporal indexing to accelerate search directed search version of chronological backtracking and forward checking are presented and tested our result show that indexing can drastically improve search performance 
we propose a method for compiling propositional theory into a new tractable form that we refer to a decomposable negation normal form dnnf we show a number of result about our compilation approach first we show that every propositional theory can be compiled into dnnf and present an algorithm to this effect second we show that if a clausal form ha a bounded treewidth then it dnnf compilation ha a linear size and can be computed in linear time treewidth is a graphtheoretic parameter which measure the connectivity of the clausal form third we show that once a propositional theory is compiled into dnnf a number of reasoning task such a satisfiability and forgetting can be performed in linear time finally we propose two technique for approximating the dnnf compilation of a theory when the size of such compilation is too large to be practical one of the technique generates a sound but incomplete compilation while the other generates a complete but unsound compilation together these approximation bound the exact compilation from below and above in term for their ability to answer query 
shop simple hierarchical ordered planner is a domain independent htn planning system with the following characteristic shop plan for task in the same order that they will later be executed this avoids some goal interaction issue that arise in other htn planner so that the planning algorithm is relatively simple since shop know the complete world state at each step of the planning process it can use highly expressive domain representation for example it can do planning problem that require complex numeric computation in our test shop wa several order of magnitude faster man blackbox and several time faster than tlpian even though shop is coded in lisp and the other planner are coded in c 
ontology are used in agent oriented software development information system and expert system in order to support interoperability declarativity and intelligent service in the frodo project we design a scalable agent based middleware for distributed organizational memory om in this paper we investigate which ontology related service should be provided a middleware component to this end we discus three basic dimension to characterize stored information that determine the concrete specification of ontology based system formality stability and sharing scope a short discussion of technique which are suited to find a balance on these dimension lead to a characterization of role of ontology related actor in the om scenario which are described with respect to their goal knowledge competency right and obligation these actor class and the related competency are candidate to define agent type speech act and standard service in the envisioned om middleware 
motivated by the problem of query answering over multiple structured commonsense theory we exploit graph based technique to improve the efficiency of theorem proving for structured theory theory are organized into subtheories that are minimally connected by the literal they share we presentmessage passing algorithm that reason over these theory using consequence finding specializing our algorithm for the case of first order resolution and for batch and concurrent theorem proving we provide an algorithm that restricts the interaction between subtheories by exploiting the polarity of literal we attempt to minimize the reasoning within each individual partition by exploiting existing algorithm for focused incremental and general consequence finding finally we propose an algorithm that compiles each subtheory into one in a reduced sublanguage we have proven the soundness and completeness of all of these algorithm 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we perform a comprehensive theoretical and experimental analysis of the use of all different constraint we prove that generalized arc consistency on such constraint lie between neighborhood inverse consistency and under a simple restriction path inverse consistency on the binary representation of the problem by generalizing the argument of kondrak and van beek we prove that a search algorithm that maintains generalized arc consistency on all different constraint dominates a search algorithm that maintains arc consistency on the binary representation our experiment show the practical value of achieving these high level of consistency for example we can solve almost all benchmark quasigroup completion problem up to order with just a few branch of search these result demonstrate the benefit of using non binary constraint like all different to identify structure in problem 
recent research ha addressed the problem of planning in non deterministic domain classical planning ha also been extended to the case of goal that can express temporal property however the combination of these two aspect is not trivial in non deterministic domain goal should take into account the fact that a plan may result in many possible different execution and that some requirement can be enforced on all the possible execution while others may be enforced only on some execution in this paper we address this problem we define a planning algorithm that generates automatically plan for extended goal in nondeterministic domain we also provide preliminary experimental result based on an implementation of the planning algorithm that us symbolic model checking technique 
perceptual consistency is important in many computer vision application unfortunately except for color computational feature and similarity measurement for other visual feature are not necessarily consistent with human s perception this paper address three critical issue regarding perceptually consistent texture analysis development of perceptual texture space assessment of how consistent computational feature are to human perception and mapping computational feature to perceptual space it demonstrates the construction of a reliable perceptual texture space which can be used a a yardstick for assessing the perceptual consistency of computational feature and similarity measurement moreover it is found that commonly used computational texture feature are not very consistent with human perception and mapping them to the perceptual space improves their perceptual consistency 
in this paper we follow the same general ideology a in gammerman et al and describe a new transductive learning algorithm using support vector machine the algorithm presented provides confidence value for it predicted classification of new example we also obtain a measure of credibility which serf a an indicator of the reliability of the data upon which we make our prediction experiment compare the new algorithm to a standard support vector machine and other transductive method which use support vector machine such a vapnik s margin transduction empirical result show that the new algorithm not only produce confidence and credibility measure but is comparable to and sometimes exceeds the performance of the other algorithm 
we present a sample re weighting scheme inspired by recent result in margin theory the basic idea is to add to the training set replica of sample which are not classified with a sufficient margin we prove the convergence of the input distribution obtained in this way a study case we consider an instance of the scheme involving a nn classifier implementing a vector quantization algorithm that accommodates tangent distance model the tangent distance model created in this way have shown a significant improvement in generalization power with respect to the standard tangent model more over the obtained model were able to outperform state of the art algorithm such a svm 
generalized vickrey mechanism have received wide attention in the literature because they are efficient and strategy proof i e truthful bidding is optimal whatever the bid of other agent however it is well known that it is impossible for an exchange with multiple buyer and seller to be efficient and budget balanced even putting strategy proofness to one side a market maker in an efficient exchange must make more payment than it collect we enforce budget balance a a hard constraint and explore payment rule to distribute surplus after an exchange clear to minimize distance to vickrey payment different rule lead to different level of truth revelation and efficiency experimental and theoretical analysis suggest a simple threshold scheme which give surplus to agent with payment further than a certain threshold value from their vickrey payment the scheme appears able to exploit agent uncertainty about bid from other agent to reduce manipulation and boost allocative efficiency in comparison with other simple rule 
let u consider the following problem given a probably huge set of set s and a query set q is there some set s in s such that s subseteq q this problem occurs in at least three application area the matching of a large number usually several s of production rule the processing of query in data base supporting set valued attribute and the identification of inconsistent subgoals during artificial intelligence planning in this paper we introduce a data structure and algorithm that allow a compact representation of such a huge set of set and an efficient answering of em subset and em superset query 
in order to generate high quality explanation in mathematical domain the presentation must be adapted to the knowledge of the intended audience most proof presentation system only communicate proof on a fixed degree of abstraction independently of the addressee s knowledge in this paper we shall present the proof explanation system p rex based on assumption about the addressee s knowledge it dialog planner chooses a degree of abstraction for each proof step to be explained in reaction to the user s interaction which are allowed at any time it enters clarification dialog to revise it user model and to adapt the explanation 
a new reinforcement learning rl methodology is proposed to design multi agent system in the realistic setting of situated agent with local perception the task of automatically building a coordinated system is of crucial importance we use simple reactive agent which learn their own behavior in a decentralized way to cope with the difficulty inherent to rl used in that framework we have developed an incremental learning algorithm where agent face more and more complex task we illustrate this general framework on a computer experiment where agent have to coordinate to reach a global goal 
the majority of naturally sounding musical performance ha musical expression fluctuation in tempo volume etc musical expression is affected by various factor such a the performer performative style mood and so forth however in past research on the computerized generation of musical expression these factor are treated a being le significant or almost ignored hence the majority of past approach find it relatively hard to generate multiple performance for a given piece of music with varying musical expression in this paper we propose a case based approach to the generation of expressively modulated performance this method enables the generation of varying musical expression for a single piece of music we have implemented the proposed case based method in a musical performance system and we also describe the system architecture and experiment performed on the system 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
a key open problem in reinforcement learning is to assure convergence when using a compact hypothesis class to approximate the value function although the standard temporal difference learning algorithm ha been shown to converge when the hypothesis class is a linear combination of fixed basis function it may diverge with a general non linear hypothesis class this paper describes the bridge algorithm a new method for reinforcement learning and show that it converges to an approximate global optimum for any agnostically learnable hypothesis class convergence is demonstrated on a simple example for which temporal difference learning fails weak condition are identified under which the bridge algorithm converges for any hypothesis class finally connection are made between the complexity of reinforcement learning and the pac learnability of the hypothesis class 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
we present a new algorithm to reduce the space complexity of heuristic search it is most effec tive for problem space that grow polynomially with problem size but contain large number of short cycle for example the problem of finding a lowest cost corner to corner path in a d dimensional grid ha application to gene sequence alignment in computational biology the main idea is to perform a bidirectional search but saving only the open list and not the closed list once the search completes we have one node on an optimal path but don t have the solution path itself the path is then reconstructed by recursively applying the same algorithm between the initial node and the in termediate node and also between the inter mediate node and the goal node if n is the length of the grid in each dimension and d is the number of dimension this algorithm re duce the memory requirement from to the time complexity only increase by a constant factor of in two dimension and in three dimension 
ontology and problem solving method are promising candidate for reuse in knowledge engineering ontology define domain knowledge at a generic level while problem solving method specify generic reasoning knowledge both type of component can be viewed a complementary entity that can be used to configure new knowledge system from existing reusable component in this paper we give an overview of approach for ontology and problem solving method 
the qualification problem refers to the difficulty that arises in formalizing action because it is difficult or impossible to specify in advance all the precondition that should hold before an action can be executed we study the qualification problem in the setting of the situation calculus and give a simple formalization using nested abnormality theory a formalism based on circumscription the formalization that we present allows u to combine a solution to the frame problem with a solution to the qualification problem 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
this paper present a multi domain information extraction system in order to decrease the time spent on the elaboration of resource for the ie system and guide the end user in a new domain we suggest to use a machine learning system that help defining new template and associated resource this knowledge is automatically derived from the text collection in interaction with the end user to rapidly develop a local ontology giving an accurate image of the content of the text the system is finally evaluated using classical indicator 
when not enough time is available to fully explore a search tree different algorithm will visit different leaf depth first search and depth bounded discrepancy search for example make opposite assumption about the distribution of good leaf unfortunately it is rarely clear a priori which algorithm will be most appropriate for a particular problem rather than fixing strong assumption in advance we propose an approach in which an algorithm attempt to adjust to the distribution of leaf cost in the tree while exploring it by sacrificing completeness such flexible algorithm can exploit information gathered during the search using only weak assumption a an example we show how a simple depth based additive cost model of the tree can be learned on line empirical analysis using a generic tree search problem show that adaptive probing is competitive with systematic algorithm on a variety of hard tree and outperforms them when the node ordering heuristic make many mistake result on boolean satisfiability and two different representation of number partitioning confirm these observation adaptive probing combine the flexibility and robustness of local search with the ability to take advantage of constructive heuristic 
the algorithm presented here bcc is an enhancement of the well known backtrack used to solve constraint satisfaction problem though most backtrack improvement rely on propagation of local information bcc us global knowledge of the constraint graph structure and in particular it biconnected component to reduce search space permanently removing value and compiling partial solution during exploration this algorithm performs well by itself without any filtering when the biconnected component are small achieving optimal time complexity in case of a tree otherwise it remains compatible with most existing technique adding only a negligible overhead cost 
a view of plan recognition shaped by bothoperational and computational requirement ispresented operational requirement governingthe level of fidelity and nature of the reasoningprocess combine with computational requirementsincluding performance speed and softwareengineering effort to constrain the typesof solution available to the software developer by adopting machine learning to providespatio temporal recognition of environmentalevents and relationship an agent can 
case base maintenance is gaining increasing recognition in research and the practical application of case based reasoning cbr this intense interest is highlighted by smyth and keane s research on case deletion policy in their work smyth and keane advocated a case deletion policy whereby the case in a case base are classified and deleted based on their coverage potential and adaptation power the algorithm wa empirically shown to improve the competence of a cbr system and outperform a number of previous deletion based strategy in this paper we present a different case base maintenance policy that is based on case addition rather than deletion the advantage of our algorithm is that we can place a lower bound on the competence of the resulting case base we demonstrate that the coverage of the computed case base cannot be worse than the optimal case base in coverage by a fixed lower bound and the coverage is often much closer to optimum we also show that the smyth and keane s deletion based policy cannot guarantee any such lower bound our result highlight the importance of finding the right case base maintenance algorithm in order to guarantee the best case base coverage we demonstrate the effectiveness of our algorithm through an experiment in case based planning 
the robocup robot world cup soccer effort initiated to stimulate research in multi agent and robotics ha blossomed into a significant effort of international proportion robocup is simultaneously a fundamental research effort and a set of competition for testing research idea at ijcai a broad research challenge wa issued for the robocup synthetic agent covering area of multi agent learning teamwork and agent modeling this paper outline our attack on the entire breadth of the robocup research challenge on all of it category in the form of two fielded contrasting robocup team and two off line soccer analysis agent we compare the team and the agent to generalize the lesson learned in learning teamwork and agent modeling 
a general and expressive model of sequential decision making under uncertainty is provided by the markov decision process mdps framework complex application with very large state space are best modelled implicitly instead of explicitly by enumerating the state space for example a precondition effect operator the representation used in ai planning this kind of representation are very powerful and they make the construction of policy plan computationally very complex in many application average reward over unit time is the relevant rationality criterion a opposed to the more widely used discounted reward criterion and for providing a solid basis for the development of efficient planning algorithm the computational complexity of the decision problem related to average reward ha to be analyzed we investigate the complexity of the policy plan existence problem for mdps under the average reward criterion with mdps represented in term of conditional probabilistic precondition effect operator we consider policy with and without memory and with different degree of sensing observability the unrestricted policy existence problem for the partially observable case wa earlier known to be undecidable the result place the remaining computational problem to the complexity class exp and nexp deterministic and nondeterministic exponential time 
recently we showed that for traditional bidirectional search with front to end evaluation it is not the meeting of search front but the cost of proving the optimality of a solution that is problematic using our improved understanding of the problem we developed a new approach to improving this kind of search switching to unidirectional search after the search frontier meet for the first time with the first solution found this new approach show improvement over previous bidirectional search approach and partly also over the corresponding unidirectional search approach in different domain together with a special purpose improvement for the tsp this approach showed better result than the standard search algorithm using the same knowledge 
semantic interoperability is the faculty of interpretingknowledge imported from other language atthe semantic level i e to ascribe to each importedpiece of knowledge the correct interpretation or setof model it is a very important requirement fordelivering a worldwide semantic web this paperpresents preliminary investigation towards developinga unified view of the problem it proposesa definition of semantic interoperability based onmodel theory and show how it applies 
computer security depends heavily on the strength of cryptographic algorithm thus cryptographic key search is often the search problem for many government and corporation in the recent year ai search technique have achieved notable success in solving real world problem following a recent result which showed that the property of the u s data encryption standard can be encoded in propositional logic this paper advocate the use of cryptographic key search a a benchmark for propositional reasoning and search benchmark based on the encoding of cryptographic algorithm optimally share the feature of real world and random problem in this paper two state of the art ai search algorithm walk sat by kautz selman and rel sat by bayardo schrag have been tested on the encoding of the data encryption standard to see whether they are up the task and we discus what lesson can be learned from the analysis on this benchmark to improve sat solver new challenge in this field conclude the paper 
we address the problem of knowledge acquisition for alarm correlation in a complex dynamic system like a telecommunication network to reduce the amount of information coming from telecommunication equipment one need to preprocess the alarm stream and we propose here a way to acquire some knowledge to do that the key idea is that only the frequent alarm set are relevant for reducing the information stream we aggregate frequent relevant information and suppress frequent noisy information we propose algorithm for analysing alarm log first stage is to discover frequently occurring temporally constrained alarm set called chronicle and second stage is to filter them according to their interdependency level we also show experimental result with an actual telecommunication atm network 
coordination of agent activity is a key problem in multiagent system set in a larger decision theoretic context the existence of coordination problem lead to difficulty in evaluating the utility of a situation this in turn make defining optimal policy for sequential decision process problematic we propose a method for solving sequential multi agent decision problem by allowing agent to reason explicitly about specific coordination mechanism we define an extension of value iteration in which the system s state space is augmented with the state of the coordination mechanism adopted allowing agent to reason about the short and long term prospect for coordination the long term consequence of mi coordination and make decision to engage or avoid coordination problem based on expected value we also illustrate the benefit of mechanism generalization 
by automatically reformulating the problem domain constructive induction ideally overcomes the defect of the initial description the reformulation presented here us the version space primitive d e f defined for any pair of example e and f a the set of hypothesis covering e and discriminating f from these primitive we derive a polynomial number of m of n concept experimentally many of these concept turn out to be significant and consistent a simple learning strategy thus consists of exhaustively exploring these concept and retaining those with sufficient quality tunable complexity is achieved in the monkei algorithm by considering a user supplied number of primitive d ei fi where ei and fi are stochastically sampled in the training set monkei demonstrates good performance on some benchmark problem and obtains outstanding result on the predictive toxicology evaluation challenge 
this paper present an incremental learning algorithm within the framework of a fuzzy intelligent system the incremental learning algorithm is based on priority value attached to fuzzy rule the priority value of a fuzzy rule is generated based on the fuzzy belief value of the fuzzy rule derived from the training data the fuzzy incremental algorithm ha three important property it can detect and recover from incorrect knowledge once new knowledge is available it will not lose the useful knowledge generated from the old data while it attempt to learn from new data and it provides a mechanism allowing to emphasize on knowledge learnt from the new data the incremental fuzzy learning algorithm ha been implemented in a fuzzy intelligent system for automotive engineering diagnosis it performance is presented in the paper 
this paper present a novel approach to non linear black box system identification which combine qualitative reasoning qr method with fuzzy logic system such a method aim at building a good initialization of a fuzzy identifier so that it will converge to the input output relation which capture the nonlinear dynamic of the system fuzzy inference procedure should be initialized with a rule base predefined by the human expert when such a base is not available or poorly defined the inference procedure becomes extremely inefficient our method aim at solving the problem of the construction of a meaningful rule base fuzzy rule are automatically generated by encoding the knowledge of the system dynamic described by the outcome of it qualitative simulation both efficiency and robustness of the method are demonstrated by it application to the identification of the kinetics of thiamine vitamin b and it phosphoesters in the cell of the intestine tissue 
in this paper we describe a multiagent system in which agent negotiate to allocate resource and satisfy constraint in a real time environment of multisensor target tracking the agent attempt to optimize the use of their own consumable resource while adhering to the global goal i e accurate and effective multisensor target tracking agent negotiate based on different strategy which are selected and instantiated using case based reasoning cbr agent are also fully reflective in that they are aware of all their resource including system level one such a cpu allocation and this allows them to achieve real time behavior we focus our discussion on multisensor target racking case based negotiation and real time behavior and present experimental result comparing our methodology to one using either no negotiation or using a static negotiation protocol 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
we describe a coherent view of learning and reasoning with relational representation in the context of natural language processing in particular we discus the neuroidal architecture inductive logic programming and the snow system explaining the relationship among these and thereby offer an explanation of the theoretical basis for the snow system we suggest that extension of this system along the line suggested by the theory may provide new level of scalability and functionality 
cf loadingtexthtml cf contextpath cf ajaxscriptsrc cfide script ajax cf jsonprefix cf clientid bf ab d afd f a bbe using text classifier for numerical classification function settab var mytabs coldfusion layout gettablayout citationdetails mytabs on tabchange function tabpanel activetab document cookie picked activetab id function letemknow coldfusion window show letemknow function testthis alert test function loadalert alert i am in the load alert function loadalert alert i am in the load alert google load visualization package orgchart google setonloadcallback drawchart function drawchart var data new google visualization datatable data addcolumn string name data addcolumn string manager data addcolumn string tooltip data addrows v f cc for this article 
ontology and information sharing have a major role to play in the development of knowledge based agent and the overcome of the knowledge acquisition bottleneck this paper support this claim by presenting an approach to ontology specification import and development that is part of disciple rkf disciple rkf is a theory methodology and learning agent shell for the rapid development of knowledge based agent by subject matter expert with limited assistance from knowledge engineer the disciple approach ha been subject of intensive evaluation a part of darpa s high performance knowledge base and rapid knowledge formation program demonstrating very good result 
in the past decade a large number of robotshave been built that explicitly implement biologicalnavigation behaviour we review thesebiomimetic approach using a framework thatallows for a common description of biologicaland technical navigation behaviour the reviewshows that biomimetic system make significantcontributions to two field of research first they provide a real world test of modelsof biological navigation behaviour second they make new navigation 
we describe a new practical domain independent task planner called shaper specially designed to deal efficiently with large problem shaper performs in two step in the first step executed off line for a given domain subclass shaper explores and build a compact representation of the state space called the shape graph the main contribution of shaper is it ability to resist to combinatorial explosion thanks to the manipulation of set of similar state description called shape the shape graph is then used by shaper to answer very efficiently to planning request a first version of the planner ha been implemented it ha been tested on several well known benchmark domain the result are very promising when compared with themost efficient planner from aips competition 
this paper emphasizes the interest of xml meta language for corporate knowledge management taking into account the advantage of the world wide web and of ontology for knowledge management we present osirix a tool enabling enterprise ontologyguided search in xml document that may consitute a part of a corporate memory 
we propose lcirc an extension of circumscription by allowing propositional combination and nesting of circumscriptive theory a shown lifschitzs nested abnormality theory nats introduced in aij are naturally embedded into this language we analyze the complexity of lcirc and nats and in particular the effect of nesting the latter is found a source of complexity a both formalism are proved to be pspace complete we identify case of lower complexity including a tractable case our result give insight into the cost of using lcirc resp nats a a host language for expressing other formalism such a narrative 
in this paper we extend previous work on the boundary based approach to describing shape by deriving an unbounded hierarchy of atomic shape descriptor called token based on tangent bearing and it successive derivative and incorporating angle and cusp curve feature both open and closed curve have token string description at all level in the hierarchy we provide a pair of compatibility matrix for generating transition table for any level from which level specific token ordering graph that encode basic string syntax can be systematically constructed 
this paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional mean this paradigm suggests different tradeoff than those in the traditional approach to this problem the ilp approach and a a result it enjoys several significant advantage over it in particular the new paradigm is more flexible and allows the use of any propositional algorithm including probabilistic algorithm within it we evaluate the new approach on an important and relation intensive task information extraction and show that it outperforms existing method while being order of magnitude more efficient 
this paper present a system that simulates the emergence of realistic vowel system in a population of agent that try to imitate each other a well a possible the agent start with no knowledge of the sound system at all although none of the agent ha a global view of the language and none of the agent doe explicit optimization a coherent vowel system emerges that happens to be optimal for acoustic distinctiveness the result presented here fit in and confirm the theory of luc steel steel that view language a a complex dynamic system and the origin of language a the result of self organization and cultural evolution 
this paper present a statistical approach to collaborative filtering and investigates the use of latent class model for predicting individual choice and preference based on observed preference behavior two model are discussed and compared the aspect model a probabilistic latent space model which model individual preference a a convex combination of preference factor and the two sided clustering model which simultaneously partition person and object into cluster we present em algorithm for different variant of the aspect model and derive an approximate em algorithm based on a variational principle for the two sided clustering model the benefit of the different model are experimentally investigated on a large movie data set 
ontology are set to play a key r le in the semantic web by providing a source of shared and precisely defined term that can be used in description of web resource reasoning over such description will be essential if web resource are to be more accessible to automated process shoq d is an expressive description logic equipped with named individual and concrete datatypes which ha almost exactly the same expressive power a the latest web ontology language e g oil and daml we present sound and complete reasoning service for this logic 
in this paper we extend and integrate pre viously reported technique for resource con strained scheduling to develop a csp procedure for solving rcpsp max the resource con strained project scheduling problem with time window generalized precedence relation be tween start time of activity rcpsp max is a well studied problem within the opera tions research community and the presence of a large set of benchmark problem provides a good opportunity for comparative performance analysis our base csp scheduling model gen eralizes previous profile based approach to cumulative scheduling by focusing on global analysis of minimal conflicting set rather than pairwise conflict analysis this generalization increase the tendency for more effective conflict resolution since rcpsp max is an opti mization problem other idea from prior work are adapted to embed this base csp model within a multi pas iterative sampling procedure the overall procedure called i it erative sampling earliest solution is applied to the above mentioned set of benchmark problem i is shown to perform quite well in comparison to current state of the art procedure for rcpsp max particularly a search space size becomes limiting for systematic procedure 
this paper describes multidimensional neural preference class and preference moore machine a a principle for integrating different neural and or symbolic knowledge source we relate neural preference to multidimensional fuzzy set representation furthermore we introduce neural preference moore machine and relate traditional symbolic transducer with simple recurrent network by using neural preference moore machine finally we demonstrate how the concept of preference class and preference moore machine can be used to integrate knowledge from different neural and or symbolic machine we argue that our new concept for preference moore machine contribute a new potential approach towards general principle of neural symbolic integration 
we are interested in semantical underpinnings for existing approach to preference handling in extended logic programming within the framework of answer set programming a a starting point we explore three different approach that have been recently proposed in the literature because these approach use rather different formal mean we furnish a series of uniform characterization that allow u to gain insight into the relationship among these approach to be more precise we provide different characterization in term of i fixpoints ii order preservation and iii translation into standard logic program while the two former provide semantics for logic programming with preference information the latter furnishes implementation technique for these approach 
this paper describes a method to automate the diagnosis of student programming error in programming learning environment in order to recognize correct student program a well a to identify error in incorrect student program program are represented using an improved dependence graph representation the student program is compared with a specimen program also called a model program at the semantic level after both are standardized by program transformation the method is implemented using smalltalk in siples ii an automatic program diagnosis system for samlltalk programming learning environment the system ha been tested on approximately student program for various task experimental result show that using the method semantic error in a student program can be identified rigorously and safely semantics preserving variation in a student program can be eliminated or accommodated the test also show that the system can identify a wide range of error a well a produce indication of the correction needed this method is essential for the development of programming learning environment the technique of the improved program dependence graph representation program standardization by transformation and semantic level program comparison are also useful in other research field including program understanding and software maintenance 
this paper argues that the reuse of domain knowledge must be complemented by the reuse of problem solving method problem solving method psms provide a mean to structure search and can provide tractable solution to reasoning with a very large knowledge base we show that psms can be used in a way which complement large scale representation technique and optimisation such a those for taxonornie reasoning found in cyc our approach illustrates the advantage of task oriented knowledge modelling and we demonstrate that the resulting ontology have both task dependent and task independent element further we show how the task ontology can be organised into conceptual level to reflect knowledge typing principle 
recently several approach for updating knowledge base represented a logic program have been proposed in this paper we present a generic framework for declarative specification of update policy which is built upon such approach it extends the lups language for update specification and incorporates the notion of event into the framework an update policy allows an agent to flexibly react upon new information arriving a an event and perform suitable change of it knowledge base the framework compiles update policy to logic program by mean of generic translation and can be instantiated in term of different concrete update approach it thus provides a flexible tool for designing adaptive reasoning agent 
resource co funded by several agent must be exploited in such a way that three kind of constraint are met physical problem hard constraint efficiency constraint aiming at maximizing the satisfaction of each agent a fairness constraint which is ideally satisfied when each agent receives an amount of the resource exactly proportional to it financial contribution this paper investigates a decision problem for which the common property resource is an earth observation satellite the problem is to decide on the daily selection of a subset of picture among a set of candidate picture which could be taken the next day considering the satellite trajectory this subset must satisfy the three kind of constraint stated above although fair division problem have received considerable attention for a long time especially from microeconomist this specific problem doe not fall entirely within a classical approach this is because the candidate picture may be incompatible and because a picture is only of value to the agent requesting it a in the general case efficiency and fairness constraint are antagonistic we propose three way for solving this share problem the first one give priority to fairness the second one to efficiency and the third one computes a set of compromise 
calibration is the degree to which an agent s probability estimate subjective probability correspond to actual frequency or the objective probability underlyingthem cognitive psychologist have studied human calibration a a part of theirprogram to investigate how human cognition deviate from the ideal in gambling one attempt to estimate the odds of an event so a to maximize return there aretwo major factor involved knowledge of the domain and the meta knowledge of the 
in this paper we investigate new approachesto dynamic programming based optimal controlof continuous time and space system weuse neural network to approximate the solutionto the hamilton jacobi bellman hjb equation which is in the deterministic casestudied here a first order non linear partialdifferential equation we derive the gradientdescent rule for integrating this equation insidethe domain given the condition on theboundary we apply this approach to the quot caron 
we present a hybrid approach for the multidimensional knapsack problem the proposed approach combine linear programming and tabu search the resulting algorithm improves significantly on the best known result of a set of more than benchmark instance 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
conceptual graph are very useful for representing structured knowledge however existing formulation of fuzzy conceptual graph are not suitable for matching image of natural scene this paper present a new variation of fuzzy conceptual graph that is more suited to image matching this variant differentiates between a model graph that describes a known scene and an image graph which describes an input image a new measurement is defined to measure how well a model graph match an image graph a fuzzy graph matching algorithm is developed based on error tolerant subgraph isomorphism test result show that the matching algorithm give very good result for matching image to predefined scene model 
an issue that is critical for the application of markov decision process mdps to realistic problem is how the complexity of planning scale with the size of the mdp in stochastic environment with very large or even infinite state space traditional planning and reinforcement learning algorithm are often inapplicable since their running time typically scale linearly with the state space size in this paper we present a new algorithm that given only a generative model simulator for an arbitrary mdp performs near optimal planning with a running time that ha no dependence on the number of state although the running time is exponential in the horizon time which depends only on the discount factor and the desired degree of approximation to the optimal policy our result establish for the first time that there are no theoretical barrier to computing near optimal policy in arbitrarily large unstructured mdps our algorithm is based on the idea of sparse sampling we prove that a randomly sampled look ahead tree that cover only a vanishing fraction of the full look ahead tree nevertheless suffices to compute near optimal action from any state of an mdp practical implementation of the algorithm are discussed and we draw tie to our related recent result on finding a near best strategy from a given class of strategy in very large partially observable mdps kmn 
this paper proposes a new model for gesture recognition the model called view and motion based aspect model vambam is an omnidirectional view based aspect model based on motion based segmentation this model realizes location free and rotation free gesture recognition with a distributed omnidirectional vision system dovs the distributed vision system consisting of multiple omnidirectional camera is a prototype of a perceptual information infrastructure for monitoring and recognizing the real world in addition to the concept of vabam this paper show how the model realizes robust and real time visual recognition of the dovs 
recently lakemeyer and levesque proposed the logic aoc which amalgamates both the situation calculus and levesques logic of only knowing while very expressive the practical relevance of the formalism is unclear because it heavily relies on second order logic in this paper we demonstrate that the picture is not a bleak a it may seem in particular we show that for large class of aol knowledge base and query including epistemic one query evaluation requires first order reasoning only we also provide a simple semantic definition of progressing a knowledge base for a particular class of knowledge base adapted from earlier result by lin and reiter we show that progression is first order representable and easy to compute 
abstract we discus the relationship between probabilis tic logic and cm given a set of logical sen tences and their probability of being true the outcome of a probabilistic logic system con sists of lower and upper bound on the prob ability of an additional sentence to be true these bound are computed using a linear pro gramming formulation in cm system the outcome is defined by the probability of the support and the plausibility with the assump tions being independent after a first phase which consists of computing the prime implicants depending only on the variable of the assumption we propose to reformulate a cm system without independence condition on the assumption using the linear program ming framework of probabilisticlogic and show how to exploit it particular structure to solve it efficiently when an independence condition is imposed on the assumption the two system give different result comparison are made on small problem using the assumption basedev idential language program abel of anrig et a and the psat program of jaumard eta 
in constraint satisfaction problem csps value belonging to variable domain should be completely known before the constraint propagation process start in many application however the acquisition of domain value is a computational expensive process or some domain value could not be available at the beginning of the computation for this purpose we introduce an interactive constraint satisfaction problem icsp model a extension of the widely used csp model the variable domain value can be acquired when needed during the resolution process by mean of interactive constraint which retrieve possibly consistent information experimental result on randomly generated csps and for d object recognition show the effectiveness of the proposed approach 
in this paper we show decidability of a rather expressive fragment of the situation calculus we allow second order quantification over finite and infinite set of situation we do not impose a domain closure assumption on action therefore infinite and even uncountable domain are allowed the decision procedure is based on automaton accepting infinite tree 
this paper introduces a new framework for extending consistent domain of numeric csp the aim is to offer the greatest possible freedom of choice for one variable to the designer of a cad application thus we provide here an efficient and incremental algorithm which computes the maximal extension of the domain of one variable the key point of this framework is the definition for each inequality of an univariate extremum function which computes the left most and right most solution of a selected variable in a space delimited by the domain of the other variable we show how these univariate extremum function can be implemented efficiently the capability of this approach are illustrated on a ballistic example 
this article explores the advantage and one potential implementation of a new style of computation in which multiple line of symbolic processing are pursued at different speed within a hybrid multi agent system the cognitive architecture dual consists of small hybrid computational entity called dual agent each agent ha a symbolic processor capable of simple symbol manipulation there is also an activation level associated with each agent activation spread according to connectionist rule the speed of each symbolic processor is proportional to the activation level of the corresponding dual agent and varies dynamically thus multiple candidate solution to a given problem can be explored in parallel more computational resource are dedicated to the more promising candidate and the degree of promise is reevaluated dynamically this allows for flexible and efficient behavior of the system a a whole the exact relationship between symbolic speed and connectionist activation is based on an energetic analogy the symbolic processor is conceptualized a a machine converting connectionist activation into symbolic work a language for implementing variable speed symbol manipulation using delayed evaluation is introduced s llsp a small example from a dual based cognitive model illustrates variable speed marker passing in a semantic network 
previous study considered quality optimization of anytime algorithm by taking into account the quality of the final result the problem we are interested in is the maximization of the average quality of a contract algorithm over a time interval we first informally illustrate and motivate this problem with few concrete situation then we prove that the problem is nphard but quadratic if the time interval is large enough eventually we give empirical result 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
many state of the art heuristic planner derive their heuristic function by relaxing the planning task at hand where the relaxation is to assume that all delete list are empty looking at a collection of planning benchmark we measure topological property of state space with respect to that relaxation the result suggest that given the heuristic based on the relaxation many planning benchmark are simple in structure this shed light on the recent success of heuristic planner employing local search 
the traditional representation of game using the extensive form or the strategic normal form obscure much of the structure that is present in real world game in this paper we propose a new representation language for general multiplayer game multi agent influence diagram maid this representation extends graphical model for probability distribution to a multi agent decision making context maid explicitly encode structure involving the dependence relationship among variable a a consequence we can define a notion of strategic relevance of one decision variable to another is strategically relevant to if to optimize the decision rule at the decision maker need to take into consideration the decision rule at we provide a sound and complete graphical criterion for determining strategic relevance we then show how strategic relevance can be used to detect structure in game allowing a large game to be broken up into a set of interacting smaller game which can be solved in sequence we show that this decomposition can lead to substantial saving in the computational cost of finding nash equilibrium in these game 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
this paper present an extended system epdl of propositional dynamic logic by allowing a proposition a a modality for representing and specifying direct and indirect effect of action in a unified logical structure a set of causal logic based on the framework are proposed to model causal propagation through logical relevancy and iterated effect of causation it is shown that these logic capture the basic property of causal reasoning 
this paper deal with the way dual genetic algorithm dga an extension of the standard one explore the search space after a brief introduction presenting genetic algorithm and dualism the fitness distance correlation is discussed in the context of dualism from this discussion a conjecture is made about the genetic heuristic used by dual genetic algorithm to explore the search space this conjecture is reinforced by the visualization of the population centroid trajectory in the plane fitness distance these trajectory help to point out leg up behavior which allow the dual genetic algorithm to reach the global optimum from walk on deceptive path 
recently tremendous advance have been made in the performance of ai planning system however increased performance is only one of the prerequisite for bringing planning into the realm of real application advance in the scope of problem that can be represented and solved must also be made in this paper we address two important representational feature concurrently executable action with varying duration and metric quantity like resource both essential for modeling real application we show how the forward chaining approach to planning can be extended to allow it to solve planning problem with these two feature forward chaining using heuristic or domain specific information to guide search ha shown itself to be a very promising approach to planning and it is sensible to try to build on this success in our experiment we utilize the tlplan approach to planning in which declaratively represented control knowledge is used to guide search we show that this extra knowledge can be intuitive and easy to obtain and that with it impressive planning performance can be achieved 
bayesian belief network bbns have become accepted and used widely to model uncertain reasoning and causal relationship we have developed an interactive visualization tool visnet that allows student and or teacher to inspect bbns using visnet it is possible to experiment with concept such a marginal probability change in probability probability propagation and cause effect relationship in bbns using visualization technique vismod visualization of bayesian student model an extended version of visnet open the internal representation of the student s knowledge to teacher and or student interested in knowing more about the knowledge about them represented in the system both visnet and vismod aim to support reflection process in learning environment that rely on the use of bayesian model 
this paper present a new system called the asystem performing abductive reasoning within the framework of abductive logic programming it is based on a hybrid computational model that implement the abductive search in term of two tightly coupled process a reduction process of the highlevel logical representation to a lower level constraint store and a lower level constraint solving process a set of initial proof of principle experiment demonstrate the versatility of the approach stemming from it declarative representation of problem and the good underlying computational behaviour of the system the approach offer a general methodology of declarative problem solving in ai where an incremental and modular refinement of the high level representation with extra domain knowledge can improve and scale the computational performance of the framework 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this paper proposes the use of machine learning technique to greatly automate the creation and maintenance of domain specific search engine we describe new research in reinforcement learning text classification and information extraction that enables efficient spidering populates topic hierarchy and identifies informative text segment using these technique we have built a demonstration system a search engine for computer science research paper available at www cora justrcsettrch com 
based on an abstract framework for nonmonotonic reasoning bondarenko et at have extended the logic programming semantics of admissible and preferred argument to other nonmonotonic formalism such a circumscription autoepisternic logic and default logic although the new semantics have been tacitly assumed to mitigate the computational problem of nonmonotonic reasoning under the standard semantics of stable extension it seems questionable whether they improve the worst case behaviour a a matter of fact we show that credulous reasoning under the new semantics in propositional logic programming and prepositional default logic ha the same computational complexity a under the standard semantics furthermore sceptical reasoning under the admissibility semantics is easier since it is trivialised to monotonic reasoning finally sceptical reasoning under the preferability semantics is harder than under the standard semantics 
in linguistics the semantic relation between word in a sentence are accounted for inter alia a the assignment of thematic role e g agent instrument etc a in predicate logic simple linguistic expression are decomposed into one predicate often the verb and it argument the predicate assigns thematic role to the argument so that each sentence ha a thematic grid a structure with all thematic role assigned by the predicate in order to reveal the thematic grid of a sentence a system called htrp hybrid thematic role processor is proposed in which the connectionist architecture ha a input a featural representation of the word of a sentence and a output it thematic grid both a random initial weight version riw and a biased initial weight version biw are proposed to account for system without and with initial knowledge respectively in biw initial connection weight reflect symbolic rule for thematic role for both version after supervised training a set of final symbolic rule is extracted which is consistently correlated to linguistic symbolic knowledge in the case of biw this amount to a revision of the initial rule in riw symbolic rule seem to be induced from the connectionist architecture and training 
parametric ordinary differential equation arise in many area of science and engineering since some of the data is uncertain and given by interval traditional numerical method do not apply interval method provide a way to approach these problem but they often suffer from a loss in precision and high computation cost this paper present a constraint satisfaction approach that enhances interval method with a pruning step based on a global relaxation of the problem theoretical and experimental evaluation show that the approach produce significant improvement in accurracy and or efficiency over the best interval method 
domain specific search engine are becoming increasingly popular because they offer increased accuracy and extra feature not possible with general web wide search engine unfortunately they are also difficult and time consuming to maintain this 
this paper describes a new method for visualizing complex information space a painted image scientific visualization convert data into picture that allow viewer to see trend relationship and pattern we introduce a formal definition of the correspondence between traditional visualization technique and painterly style from the impressionist art movement this correspondence allows u to apply perceptual guideline from visualization to control the presentation of information in a computer generated painting the result is an image that is visually engaging but that also allows viewer to rapidly and accurately explore and analyze the underlying data value we conclude by applying our technique to a collection of environmental and weather reading to demonstrate it viability in a practical real world visualization environment 
current computer based design tool for mechanical engineer are not tailored to the early stage of design most design start a pencil and paper sketch and are entered into cad system only when nearly complete our goal is to create a kind of magic paper capable of bridging the gap between these two stage we want to create a computer based sketching environment that feel a natural a sketching on paper but unlike paper understands a mechanical engineer s sketch a it is drawn one important step toward realizing this goal is resolving ambiguity in the sketch determining for example whether a circle is intended to indicate a wheel or a pin joint and doing this a the user draw so that it doesn t interfere with the design process we present a method and an implemented program that doe this for freehand sketch of simple d mechanical device 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
extracting rule from rbfs is not a trivial task because of nonlinear function or high input dimensionality in such case some of the hidden unit of the rbf network have a tendency to be shared across several output class or even may not contribute 
kb consistency form the class of strong consistency used in interval constraint programming we survey prove and give theoretical motivation to some technical improvement to a naive kbconsistency algorithm our contribution is twofold on the one hand we introduce an optimal bconsistency algorithm whose time complexity of o md n improves the known bound by a factor n m is the number of constraint n is the number of variable and d is the maximal size of the interval of the box on the other hand we prove that improved bound on time complexity can effectively be reached for higher value of k these result are obtained with very affordable overhead in term of space complexity 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
this paper present a method for designing bundle in a combinatorial auction protocol that is robust against false name bid internet auction have become an integral part of electronic commerce and a promising field for applying ai technology however the possibility of a new type of cheating called a false name bid i e a bid submitted under a fictitious name ha been pointed out a protocol called leveled division set lds protocol that is robust against false name bid ha been developed however this protocol requires the auctioneer to define a leveled division set a leveled division set is a series of division set where a division set is a set of division and a division is a combination of bundle of good we need to solve a very complicated optimization problem to construct a leveled division set in order to obtain a good social surplus we have developed a heuristic method for overcoming this problem in this method we first find a good division with a winner determination algorithm and then construct a leveled division set by using this division a a seed through a simulation we showthat our method can obtain a social surplus that is very close to optimal 
although several attempt have been made to introduce natural language processing nlp technique in information retrieval most one failed to prove their effectiveness in increasing performance in this paper text classification tc ha been taken a the ir task and the effect of linguistic capability of the underlying system have been studied a novel model for tc extending a well know statistical model i e rocchio s formula ittner et al and applied to linguistic feature ha been defined and experimented the proposed model represents an effective feature selection methodology all the experiment result in a significant improvement with respect to other purely statistical method e g yang thus stressing the relevance of the available linguistic information moreover the derived classifier reach the performance about of the best known model i e support vector machine svm and k nearest neighbour knn characterized by an higher computational complexity for training and processing 
we present a model based monitoring method for dynamic system that exhibit both discrete and continuous behavior mimic dvorak and kuiper us qualitative and semiquantitative model to monitor dynamic system even with incomplete knowledge recent advance have improved the quality of semi quantitative behavior prediction used observation to refine static envelope around monotonic function and provided a semiquantitative system identification method using these we reformulate and extend mimic to handle discontinuous change between model each hypothesis being monitored is embodied a a tracker which us the observation stream to refine it behavioral prediction it underlying model and the time uncertainty of any discontinuous transition 
this paper investigates the relationship between resolution and tableau proof system for the satisfiability of general knowledge base in the description logic alc we show that resolution proof system can polynornially simulate their tableau counterpart our resolution proof system is based on a selection refinement and utilises standard redundancy elimination criterion to ensure termination 
we present a general method for proving tractability of reasoning over disjunction of jointly exhaustive and pairwise disjoint relation example of these kind of relation are allen s temporal interval relation and their spatial counterpart the r cc relation by randell cui and colin applying this method doe not require detailed knowledge about the considered relation instead it is rather sufficient to have a subset of the considered set of relation for which path consistency is known to decide consistency using this method we give a complete classification of tractability of reasoning over rcc by identifying two large new maximal tractable subset and show that these two subset together with h the already known maximal tractable subset are the only such set for rcc that contain all base relation we also apply our method to allen s interval algebra and derive the known maximal tractable subset 
we present a dynamic programming approach for the solution of first order markov decision process this technique us an mdp whose dynamic is represented in a variant of the situation calculus allowing for stochastic action it produce a logical description of the optimal value function and policy by constructing a set of first order formula that minimally partition state space according to distinction made by the value function and policy this is achieved through the use of an operation known a decision theoretic regression in effect our algorithm performs value iteration without explicit enumeration of either the state or action space of the mdp this allows problem involving relational fluents and quantification to be solved without requiring explicit state space enumeration or conversion to propositional form 
computer modeling and simulation are indispensable for understanding the functioning of an organism on a molecular level we present an implemented method for the qualitative simulation of large and complex genetic regulatory network the method allows a broad range of regulatory interaction between gene to be represented and ha been applied to the analysis of a real network of biological interest the network controlling the inititation of sporulation in the bacterium b subtilis 
this paper address the problem of building an interruptible real time system using contract algorithm contract algorithm offer a trade off between computation time and quality of result but their run time must be determined when they are activated many ai technique provide useful contract algorithm that are not interruptible we show how to optimally sequence contract algorithm to create the best interruptible system with or without stochastic information about the deadline these result extend the foundation of real time problem solving and provide useful guidance for embedding contract algorithm in application 
ontology have been established for knowledge sharing and are widely used a a mean for conceptually structuring domain of interest with the growing usage of ontology the problem of overlapping knowledge in a common domain becomes critical we propose the new method fca merge for merging ontology following a bottom up approach which offer a structural description of the merging process the method is guided by application specific instance of the given source ontology that are to be merged we apply technique from natural language processing and formal concept analysis to derive a lattice of concept a a structural result of fca merge the generated result is then explored and transformed into the merged ontology with human interaction 
successfully managing information mean being able to find relevant new information and to correctly integrate it with pre existing knowledge much information is nowadays stored a multilingual textual data therefore advanced classification system are currently considered a strategic component for effective knowledge management we describe an experience integrating different innovative ai technology such a hierarchical pattern matching and information extraction to provide flexible multilingual classification adaptable to user need pattern matching produce fairly accurate and fast categorisation over a large number of class while information extraction provides fine grained classification for a reduced number of class the resulting system wa adopted by the main italian financial news agency providing a pay to view service 
visitor who browse the web from wireless pda cell phone and pager are frequently stymied by web interface optimized for desktop pc simply replacing graphic with text and reformatting table doe not solve the problem because deep link structure can still require minute to traverse in this paper we develop an algorithm minpath that automatically improves wireless web navigation by suggesting useful shortcut link in real time minpath find shortcut by using a learned model of web visitor behavior to estimate the saving of shortcut link and suggests only the few best link we explore a variety of predictive model including na ve bayes mixture model and mixture of markov model and report empirical evidence that minpath find useful shortcut that save substantial navigational effort 
this paper present a new method for building domain specific web search engine previous method eliminate irrelevant document from the page accessed using heuristic based on human knowledge about the domain in question accordingly they are hard to build and can not be applied to other domain the keyword spice method in contrast improves search performance by adding domain specific keywords called keyword spice to the user s input query the modified query is then forwarded to a general purpose search engine keyword spice can be effectively discovered automatically from web document allowing u to build high quality domain specific search engine in various domain without requiring the collection of heuristic knowledge we describe a machine learning algorithm which is a type of decision tree learning algorithm that can extract keyword spice to demonstrate the value of the proposed approach we conduct experiment in the domain of cooking the result confirm the excellent performance of our method in term of both precision and recall 
the creation of a complex web site is a thorny problem in user interface design in ijcai we challenged the ai community to address this problem by creating adaptive web site in response we investigate the problem of index page synthesis the automatic creation of page that facilitate a visitor s navigation of a web site previous work ha employed statistical method to generate candidate index page that are of limited value because they do not correspond to concept or topic that are intuitive to people in this paper we formalize index page synthesis a a conceptual clustering problem and introduce a novel approach which we call conceptual cluster mining we search for a small number of cohesive cluster that correspond to concept in a given concept description language l next we present sgml an algorithm schema that combine a statistical clustering algorithm with a concept learning algorithm the clustering algorithm is used to generate seed cluster and the concept learning algorithm to describe these seed cluster using expression in l finally we offer preliminary experimental evidence that instantiation of sgml outperform existing algorithm e g cobweb in this domain 
this paper present a new multi unit auction protocol ir protocol that is robust against false name bid internet auction have become an integral part of electronic commerce and a promising field for applying agent and artificial intelligence technology although the internet provides an excellent infrastructure for executing auction the possibility of a new type of cheating called false name bid ha been pointed out a false name bid is a bid submitted under a fictitious name a protocol called lds ha been developed for combinatorial auction of multiple different item and ha proven to be robust against false name bid although we can modify the lds protocol to handle multi unit auction in which multiple unit of an identical item are auctioned the protocol is complicated and requires the auctioneer to carefully predetermine the combination of bundle to obtain a high social surplus or revenue for the auctioneer our newly developed ir protocol is easier to use than the lds since the combination of bundle is automatically determined in a flexible manner according to the declared evaluation value of agent the evaluation result show that the ir protocol can obtain a better social surplus than that obtained by the lds protocol 
we consider the problem of how an agent creates a discrete spatial representation from it continuous interaction with the environment such representation will be the minimal one that explains the experience of the agent in the environment in this 
this paper investigates the formation of color category and color naming in a population of agent the agent perceive and categorize color stimulus and try to communicate about these perceived stimulus while doing so they adapt their internal representation to be more successful at conveying color meaning in future interaction the agent have no access to global information or to the representation of other agent they only exchange word form the factor driving the population coherence are the shared environment and the interaction the experiment show how agent can form a coherent lexicon of color term and particularly how a coherent color categorization emerges through these linguistic interaction the result are interpreted in the light of theory describing and explaining universal tendency in human color categorization and color naming at the same time the experiment confirm the view that certain aspect of language act a a complex dynamic system arising from self organization and cultural interaction 
temporal aliasing artifact are common in both computer generated and natural motion sequence one of the most striking manifestation of temporal aliasing is the apparent reversal of motion commonly referred to a the wagon wheel effect in this paper we examine temporal aliasing from the standpoint of joint spatiotemporal spatiotemporal frequency representation we show that apparent motion reversal can be explained using these representation and demonstrate that a motion estimation algorithm based on such a representation the d gabor transform can accurately predict this illusion 
this paper describes the mashgreen dm prototype following three goal the identification and implementation of the task related to urban traffic control that use deep reasoning mechanism a main tool for their resolution the chosen model for explaining urban traffic behaviour is a qualitative model which includes among it main feature their low temporal and spatial computational cost the definition of a functional architecture with soft real time constraint that integrates the developed task a specilized component named agent composed by four functional module executes every task these module are the following one communication protocol method agent specilization data space and control this architecture verifies that the execution performance of every agent are not compromised by the inclusion of new agent in the system or by the interaction due to the active system agent the implementation of a prototype in a high performance computational architecture a beowulf computer system 
symmetry often appears in real world constraint satisfaction problem but strategy for exploiting it are only beginning to be developed here a rationale for exploiting symmetry within depth first search is proposed leading to an heuristic for variable selection and a domain pruning procedure these strategy are then applied to a highly symmetric combinatorial problem namely the generation of balanced incomplete block design experimental result show that these strategy achieve a reduction of up to two order of magnitude in computational effort interestingly two previously developed strategy are shown to be particular instance of this approach 
in order to understand cognitive aspect of autonomous robot it is fruitful to develop a mechanism by which the robot autonomously analyzes physical sensor data and construct a state space this paper proposes a coherent approach to constructing such a robot oriented state space by statistically analyzing sensor pattern and reward given a the result of task execution in the state space construction the robot creates sensor pattern classifier called empirically obtained perceiver eops which when combined represent internal state of the robot a novel feature of this method is that the eop directs attention to select necessary information and the state space is obtained with the attention control mechanism using eops we have confirmed that the robot can effectively construct state space through it vision sensor and execute a navigation task with the obtained state space in a complicated simulated world 
this paper introduces how to eliminate redundant search space for forward chaining theorem proving a much a possible we consider how to keep on minimal useful consequent atom set for necessary branch in a proof tree in the most case an unnecessary non horn clause used for forward chaining will be split only once the increase of the search space by invoking unnecessary forward chaining clause will be nearly linear not exponential anymore in a certain sense we unsearch more than necessary we explain the principle of our method and provide an example to show that our approach is powerful for forward chaining theorem proving 
for many supervised learning task it is very costly to produce training data with class label active learning acquires data incrementally at each stage using the model learned so far to help identify especially useful additional data for labeling existing empirical active learning approach have focused on learning classifier however many application require estimation of the probability of class membership or score that can be used to rank new case we present a new active learning method for class probability estimation cpe and ranking bootstrap lv selects new data for labeling based on the variance in probability estimate a determined by learning multiple model from bootstrap sample of the existing labeled data we show empirically that the method reduces the number of data item that must be labeled across a wide variety of data set we also compare bootstrap lv with uncertainty sampling an existing active learning method designed to maximize classification accuracy the result show that bootstrap lv dominates for cpe surprisingly it also often is preferable for accelerating simple accuracy maximization 
in the future web of unmanned air and space vehicle will act together to robustly perform elaborate mission in uncertain environment we coordinate these system by introducing a reactive model based programming language rmpl that combine within a single unified representation the flexibility of embedded programming and reactive execution language and the deliberative reasoning power of temporal planner the kirk planning system take a input a problem expressed a a rmpl program and compiles it into a temporal plan network tpn similar to those used by temporal planner but extended for symbolic constraint and decision this intermediate representation clarifies the relation between temporal planning and causal link planning and permit a single task model to be used for planning and execution such a unified model ha been described a a holy grail for autonomous agent by the designer of the remote agent muscettola et al b 
design pattern describe micro architecture that solve recurrent architectural problem in objectoriented programming language it is important to identify these micro architecture during the maintenance of object oriented program but these micro architecture often appear distorted in the source code we present an application of explanation based constraint programming for identifying these distorted micro architecture 
