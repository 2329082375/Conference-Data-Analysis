we study the problem of learning personalized user model from rich user interaction in particular we focus on learning from clustering feedback i e grouping recommended item into cluster which enables user to express similarity or redundancy between different item we propose and study a new machine learning problem for personalization which we call collaborative clustering analogous to collaborative filtering in collaborative clustering the goal is to leverage how existing user cluster or group item in order to predict similarity model for other user clustering task we propose a simple yet effective latent factor model to learn the variability of similarity function across a user population we empirically evaluate our approach using data collected from a clustering interface we developed for a goal oriented data exploration or sensemaking task asking user to explore and organize attraction in paris we evaluate using several realistic use case and show that our approach learns more effective user model than conventional clustering and metric learning approach 
personalized recommendation system are used in a wide variety of application such a electronic commerce social network web search and more collaborative filtering approach to recommendation system typically assume that the rating matrix e g movie rating by viewer is low rank in this paper we examine an alternative approach in which the rating matrix is locally low rank concretely we assume that the rating matrix is low rank within certain neighborhood of the metric space defined by user item pair we combine a recent approach for local low rank approximation based on the frobenius norm with a general empirical risk minimization for ranking loss our experiment indicate that the combination of a mixture of local low rank matrix each of which wa trained to minimize a ranking loss outperforms many of the currently used state of the art recommendation system moreover our method is easy to parallelize making it a viable approach for large scale real world rank based recommendation system 
the world wide web www ha become an indispensable part of the modern life providing many benefit in diverse way for instance the huge amount of information from the web offer people unprecedented level of opportunity for education entertainment social activity productivity improvement and business the web however ha also become perilous with many danger such a privacy violation and security breach and therein exist many villain who would like to turn into victim scrupulous a well a casual user in this regard www ha become almost like wild wild west where wonderful opportunity and great peril co existed tizen www tizen org is a web centric open source standard based software platform for smart device such a smartphones smart tv ivi in vehicle infotainment and other consumer device like camera printer and more tizen is web centric in that it directly support web apps application apps written in html and javascript even outside the web browser and provides seamless support for the web a such tizen not only share the benefit and peril of the web with other platform but also ha the additional burden to meet the performance of non web platform platform that directly support only conventional programming language in this talk we present tizen s approach to taming the web to maximize it benefit while minimizing the risk of it peril we also describe various optimization of tizen that enable delivering web app performance on par with that of non web platform 
when a website hosting user generated content asks user a straightforward question wa this content helpful with one yes and one no button a the two possible answer one might expect to get a straightforward answer in this paper we explore how user respond to this question and find that their response are not quite straightforward after all using data from amazon product review we present evidence that user do not make absolute independent voting decision based on individual review quality alone rather whether user vote at all a well a the polarity of their vote for any given review depends on the context in which they view it review receive a larger overall number of vote when they are misranked and the polarity of vote becomes more positive negative when the review is ranked lower higher than it deserves we distill these empirical finding into a new probabilistic model of rating behavior that includes the dependence of rating decision on context understanding and formally modeling voting behavior is crucial for designing learning mechanism and algorithm for review ranking and we conjecture that many of our finding also apply to user behavior in other online content rating setting 
we study the problem of learning personalized user model from rich user interaction in particular we focus on learning from clustering feedback i e grouping recommended item into cluster which enables user to express similarity or redundancy between different item we propose and study a new machine learning problem for personalization which we call collaborative clustering analogous to collaborative filtering in collaborative clustering the goal is to leverage how existing user cluster or group item in order to predict similarity model for other user clustering task we propose a simple yet effective latent factor model to learn the variability of similarity function across a user population we empirically evaluate our approach using data collected from a clustering interface we developed for a goal oriented data exploration or sensemaking task asking user to explore and organize attraction in paris we evaluate using several realistic use case and show that our approach learns more effective user model than conventional clustering and metric learning approach 
personalized recommendation system are used in a wide variety of application such a electronic commerce social network web search and more collaborative filtering approach to recommendation system typically assume that the rating matrix e g movie rating by viewer is low rank in this paper we examine an alternative approach in which the rating matrix is locally low rank concretely we assume that the rating matrix is low rank within certain neighborhood of the metric space defined by user item pair we combine a recent approach for local low rank approximation based on the frobenius norm with a general empirical risk minimization for ranking loss our experiment indicate that the combination of a mixture of local low rank matrix each of which wa trained to minimize a ranking loss outperforms many of the currently used state of the art recommendation system moreover our method is easy to parallelize making it a viable approach for large scale real world rank based recommendation system 
the world wide web www ha become an indispensable part of the modern life providing many benefit in diverse way for instance the huge amount of information from the web offer people unprecedented level of opportunity for education entertainment social activity productivity improvement and business the web however ha also become perilous with many danger such a privacy violation and security breach and therein exist many villain who would like to turn into victim scrupulous a well a casual user in this regard www ha become almost like wild wild west where wonderful opportunity and great peril co existed tizen www tizen org is a web centric open source standard based software platform for smart device such a smartphones smart tv ivi in vehicle infotainment and other consumer device like camera printer and more tizen is web centric in that it directly support web apps application apps written in html and javascript even outside the web browser and provides seamless support for the web a such tizen not only share the benefit and peril of the web with other platform but also ha the additional burden to meet the performance of non web platform platform that directly support only conventional programming language in this talk we present tizen s approach to taming the web to maximize it benefit while minimizing the risk of it peril we also describe various optimization of tizen that enable delivering web app performance on par with that of non web platform 
when a website hosting user generated content asks user a straightforward question wa this content helpful with one yes and one no button a the two possible answer one might expect to get a straightforward answer in this paper we explore how user respond to this question and find that their response are not quite straightforward after all using data from amazon product review we present evidence that user do not make absolute independent voting decision based on individual review quality alone rather whether user vote at all a well a the polarity of their vote for any given review depends on the context in which they view it review receive a larger overall number of vote when they are misranked and the polarity of vote becomes more positive negative when the review is ranked lower higher than it deserves we distill these empirical finding into a new probabilistic model of rating behavior that includes the dependence of rating decision on context understanding and formally modeling voting behavior is crucial for designing learning mechanism and algorithm for review ranking and we conjecture that many of our finding also apply to user behavior in other online content rating setting 
on many social networking web site such a facebook and twitter resharing or reposting functionality allows user to share others content with their own friend or follower a content is reshared from user to user large cascade of reshares can form while a growing body of research ha focused on analyzing and characterizing such cascade a recent parallel line of work ha argued that the future trajectory of a cascade may be inherently unpredictable in this work we develop a framework for addressing cascade prediction problem on a large sample of photo reshare cascade on facebook we find strong performance in predicting whether a cascade will continue to grow in the future we find that the relative growth of a cascade becomes more predictable a we observe more of it reshares that temporal and structural feature are key predictor of cascade size and that initially breadth rather than depth in a cascade is a better indicator of larger cascade this prediction performance is robust in the sense that multiple distinct class of feature all achieve similar performance we also discover that temporal feature are predictive of a cascade s eventual shape observing independent cascade of the same content we find that while these cascade differ greatly in size we are still able to predict which end up the largest 
given a large dataset of user rating of movie what is the best model to accurately predict which movie a person will like and how can we prevent spammer from tricking our algorithm into suggesting a bad movie is it possible to infer structure between movie simultaneously in this paper we describe a unified bayesian approach to collaborative filtering that accomplishes all of these goal it model the discrete structure of rating and is flexible to the often non gaussian shape of the distribution additionally our method find a co clustering of the user and item which improves the model s accuracy and make the model robust to fraud we offer three main contribution we provide a novel model and gibbs sampling algorithm that accurately model the quirk of real world rating such a convex rating distribution we provide proof of our model s robustness to spam and anomalous behavior we use several real world datasets to demonstrate the model s effectiveness in accurately predicting user s rating avoiding prediction skew in the face of injected spam and finding interesting pattern in real world rating data 
we consider the problem of resolving duplicate in a database of place where a place is defined a any entity that ha a name and a physical location when other auxiliary attribute like phone and full address are not available deduplication based solely on name and approximate location becomes an exceptionally challenging problem that requires both domain knowledge a well an local geographical knowledge for example the pair newpark mall gap outlet and newpark mall sears outlet have a high string similarity but determining that they are different requires the domain knowledge that they represent two different store name in the same mall similarly in most part of the world a local business called central park cafe might simply be referred to by central park except in new york where the keyword cafe in the name becomes important to differentiate it from the famous park in the city in this paper we present a language model that can encapsulate both domain knowledge a well a local geographical knowledge we also present unsupervised technique that can learn such a model from a database of place finally we present deduplication technique based on such a model and we demonstrate using real datasets that our technique are much more effective than simple tf idf based model in resolving duplicate our technique are used in production at facebook for deduplicating the place database 
one of the most important innovation of social networking website is the notion of a feed a sequence of news item presented to the user a a stream that expands a the user scroll down the common method for monetizing such stream is to insert ad in between news item in this paper we model this setting and observe that allocation and pricing of ad insertion in a stream pose interesting algorithmic and mechanism design challenge in particular we formulate an optimization problem that capture a typical stream ad placement setting we give an approximation algorithm for this problem that provably achieves a value close to the optimal and show how this algorithm can be turned into an incentive compatible mechanism finally we conclude with a simple practical algorithm that make the allocation decision in an online fashion we prove this algorithm to be approximately welfare maximizing and show that it also ha good incentive property 
given a large graph like who call whom or who like whom what behavior is normal and what should be surprising possibly due to fraudulent activity how do graph evolve over time how doe influence news virus propagate over time we focus on three topic a anomaly detection in large static graph b pattern and anomaly in large time evolving graph and c cascade and immunization for the first we present a list of static and temporal law including advance pattern like eigenspokes we show how to use them to spot suspicious activity in on line buyer and seller setting in facebook in twitter like network for the second we show how to handle time evolving graph a tensor how to handle large tensor in map reduce environment a well a some discovery such setting for the third we show that for virus propagation a single number is enough to characterize the connectivity of graph and thus we show how to do efficient immunization for almost any type of virus si no immunity sir lifetime immunity etc we conclude with some open research question for graph mining 
online social network have become ubiquitous to today s society and the study of data from these network ha improved our understanding of the process by which relationship form research in statistical relational learning focus on method to exploit correlation among the attribute of linked node to predict user characteristic with greater accuracy concurrently research on generative graph model ha primarily focused on modeling network structure without attribute producing several model that are able to replicate structural characteristic of network such a power law degree distribution or community structure however there ha been little work on how to generate network with real world structural property and correlated attribute in this work we present the attributed graph model agm framework to jointly model network structure and vertex attribute our framework learns the attribute correlation in the observed network and exploit a generative graph model such a the kronecker product graph model kpgm and chung lu graph model cl to compute structural edge probability agm then combine the attribute correlation with the structural probability to sample network conditioned on attribute value while keeping the expected edge probability and degree of the input graph model we outline an efficient method for estimating the parameter of agm a well a a sampling method based on accept reject sampling to generate edge with correlated attribute we demonstrate the efficiency and accuracy of our agm framework on two large real world network showing that agm scale to network with hundred of thousand of vertex a well a having high attribute correlation 
an incisive understanding of personal psychological trait is not only essential to many scientific discipline but also ha a profound business impact on online recommendation recent study in psychology suggest that novelty seeking trait is highly related to consumer behavior in this paper we focus on understanding individual novelty seeking trait embodied at different level and across heterogeneous domain unlike the questionnaire based method widely adopted in the past we first present a computational framework novel seeking model nsm for exploring the novelty seeking trait implied by observable activity then we explore the novelty seeking trait in two heterogeneous domain check in behavior in location based social network which reflects mobility pattern in the physical world and online shopping behavior on e commerce site which reflects consumption concept in economic activity to demonstrate the effectiveness of nsm we conducted extensive experiment with a large dataset covering the two domain activity for hundred of thousand of individual our result suggest that nsm offer a powerful paradigm for presenting an effective measurement of a personality trait that can explicitly explain the deviation of individual from the habit of individual and crowd uncovering the correlation of novelty seeking trait at different level and across heterogeneous domain the proposed method provides emerging implication for personalized cross domain recommendation and targeted advertising 
e commerce web site such a ebay a well a advertising exchange adx such a doubleclick s rightmedia or adecn work a intermediary who sell item e g page view on behalf of a seller e g a publisher to buyer on the opposite side of the market e g advertiser these platform often use fixed percentage sharing scheme according to which i the platform run an auction amongst buyer and ii give the seller a constant fraction e g of the auction proceeds in these setting the platform face asymmetric information regarding both the valuation of buyer for the item a in a standard auction environment a well a about the seller s opportunity cost of selling the item moreover platform often face intense competition from similar market place and such competition is likely to favor auction rule that secure high payoff to seller in such an environment what selling mechanism should platform employ our goal in this paper is to study optimal mechanism design in setting plagued by competition and two sided asymmetric information and identify condition under which the current practice of employing constant cut is indeed optimal in particular we first show that for a large class of competition game platform behave in equilibrium a if they maximize a a convex combination of seller s payoff and platform s revenue with weight on the seller s payoff which is proxy for the intensity of competition in the market we generalize the analysis of myerson and satterthwaite and derive the optimal direct revelation mechanism for each a expected the optimal mechanism applies a reserve price which is decreasing in next we present an indirect implementation based on sharing scheme we show that constant cut are optimal if and only if the opportunity cost of the seller ha a power form distribution and derive a simple formula for computing the optimal constant cut a a function of the seller distribution of opportunity cost and the market competition proxy finally for completeness we study the case of a seller s optimal auction with a fixed profit for the platform and derive the optimal direct and indirect implementation in this setting 
it ha been suggested that online search and retrieval contributes to the intellectual isolation of user within their preexisting ideology where people s prior view are strengthened and alternative viewpoint are infrequently encountered this so called filter bubble phenomenon ha been called out a especially detrimental when it come to dialog among people on controversial emotionally charged topic such a the labeling of genetically modified food the right to bear arm the death penalty and online privacy we seek to identify and study information seeking behavior and access to alternative versus reinforcing viewpoint following shocking emotional and large scale news event we choose for a case study to analyze search and browsing on gun control right a strongly polarizing topic for both citizen and leader of the united state we study the period of time preceding and following a mass shooting to understand how it occurrence follow on discussion and debate may have been linked to change in the pattern of searching and browsing we employ information theoretic measure to quantify the diversity of web domain of interest to user and understand the browsing pattern of user we use these measure to characterize the influence of news event on these web search and browsing pattern 
the web is rapidly evolving into the web of the world where people place thing and their relationship are all digitally represented this evolution open up unparalleled opportunity to organize this vast digital universe for even greater human purpose in this talk dr lu will share an outline of microsoft s quest and aspiration to organize the digital universe with a pervasive computational fabric of digital information digital service and digital experience that empower every human being on the planet to accomplish more and enrich their life dr lu will discus high level computational structure and present specific example across bing window and other product and service to illustrate microsoft s approach to delivering end user value and accelerating the pace of innovation for the industry a a whole 
discovering and tracking topic shift in news constitutes a new challenge for application nowadays topic evolve emerge and fade making it more difficult for the journalist or the press consumerto decrypt the news for instance the current syrian chemical crisis ha been the starting point of the un russian initiative and also the revival of the u france alliance a topical mapping representing how the topic evolve in time would be helpful to contextualize information a far a we know few topic tracking system can provide such temporal topic connection in this paper we introduce a novel framework inspired from collective factorization for online topic discovery able to connect topic between different time slot the framework learns jointly the topic evolution and their time dependency it offer the user the ability to control through one unique hyper parameter the tradeoff between the past accumulated knowledge and the current observed data we show on semi synthetic datasets and on yahoo news article that our method is competitive with state of the art technique while providing a simple way to monitor topic evolution including emerging and disappearing topic 
local search user today decide what business to visit solely based on distance information and business rating that can be sparse or stale we believe that when user search for local business such a bar or restaurant they need to know more about the ambience of each business such a how crowded it is how loud and of what type the music it play is a well a how loud the human chatter in the business is unfortunately this information doesn t exist today in this paper we propose to automatically crowdsource such rich local business ambience metadata through real user check in event every time a user check into a business the phone is in user s hand and the phone s sensor can sense the business environment we leverage the phone s microphone during this time to infer the occupancy and human chatter level the music type a well a the music and noise level in the business a people check in to business throughout the day business metadata can be automatically updated over time enabling a new generation of local search experience using approximately audio trace collected from real business of various type over a period of month we show that by properly extracting the temporal and frequency signature of the audio signal it is feasible to train model that can simultaneously infer occupancy human chatter music and noise level in a business with higher than accuracy 
traditional search system generally present a ranked list of document a answer to user query in aggregated search system result from different and increasingly diverse vertical image video news etc are returned to user for instance many such search engine return to user both image and web document a answer to the query flower aggregated search ha become a very popular paradigm in this paper we go one step further and study a different search paradigm composite retrieval rather than returning and merging result from different vertical a is the case with aggregated search we propose to return to user a set of bundle where a bundle is composed of cohesive result from several vertical for example for the query london olympic one bundle per sport could be returned each containing result extracted from news video image or wikipedia composite retrieval can promote exploratory search in a way that help user understand the diversity of result available for a specific query and decide what to explore in more detail in this paper we propose and evaluate a variety of approach to construct bundle that are relevant cohesive and diverse compared with three baseline traditional general web only ranking federated search ranking and aggregated search our evaluation result demonstrate significant performance improvement for a highly heterogeneous web collection 
web application security is an important problem in today s internet a major cause of this status is that many programmer do not have adequate knowledge about secure coding so they leave application with vulnerability an approach to solve this problem is to use source code static analysis to find these bug but these tool are known to report many false positive that make hard the task of correcting the application this paper explores the use of a hybrid of method to detect vulnerability with le false positive after an initial step that us taint analysis to flag candidate vulnerability our approach us data mining to predict the existence of false positive this approach reach a trade off between two apparently opposite approach human coding the knowledge about vulnerability for taint analysis versus automatically obtaining that knowledge with machine learning for data mining given this more precise form of detection we do automatic code correction by inserting fix in the source code the approach wa implemented in the wap tool and an experimental evaluation wa performed with a large set of open source php application 
massive open online course moocs one of the latest internet revolution have engendered hope that constant iterative improvement and economy of scale may cure the cost disease of higher education while scalable in many way providing feedback for homework submission particularly open ended one remains a challenge in the online classroom in course where the student teacher ratio can be ten thousand to one or worse it is impossible for instructor to personally give feedback to student or to understand the multitude of student approach and pitfall organizing and making sense of massive collection of homework solution is thus a critical web problem despite the challenge the dense solution space sampling in highly structured homework for some moocs suggests an elegant solution to providing quality feedback to student on a massive scale we outline a method for decomposing online homework submission into a vocabulary of code phrase and based on this vocabulary we architect a queryable index that allows for fast search into the massive dataset of student homework submission to demonstrate the utility of our homework search engine we index over a million code submission from user worldwide in stanford s machine learning mooc and a semi automatically learn shared structure amongst homework submission and b generate specific feedback for student mistake codewebs is a tool that leverage the redundancy of densely sampled highly structured homework in order to force multiply teacher effort giving articulate instant feedback is a crucial component of the online learning process and thus by building a homework search engine we hope to take a step towards higher quality free education 
the past year have seen a great improvement in the rigor of information retrieval experimentation due primarily to two factor high quality public portable test collection such a those produced by trec the text retrieval conference and the increased practice of statistical hypothesis testing to determine whether measured improvement can be ascribed to something other than random chance together these create a very useful standard for reviewer program committee and journal editor work in information retrieval ir increasingly cannot be published unless it ha been evaluated using a well constructed test collection and shown to produce a statistically significant improvement over a good baseline but a the saying go any tool sharp enough to be useful is also sharp enough to be dangerous statistical test of significance are widely misunderstood most researcher treat them a a black box evaluation result go in and a p value come out because significance is such an important factor in determining what research direction to explore and what is published using p value obtained without thought can have consequence for everyone doing research in ir ioannidis ha argued that the main consequence in the biomedical science is that most published research finding are false could that be the case in ir a well 
recommendation system have been widely used in e commerce site social network etc one of the core task in recommendation system is to predict the user rating on item although many model and algorithm have been proposed how to make accurate prediction for new user with extremely few rating record still remains a big challenge which is called the cold start problem many existing method utilize additional information such a social graph to cope with the cold start problem however the side information may not always be available in contrast to such method we propose a more general solution to address the cold start problem based on the observed user rating record only specifically we define a random walk on a bipartite graph of user and item to simulate the preference propagation among user in order to alleviate the data sparsity problem for cold start user then we propose a monte carlo algorithm to estimate the similarity between different user this algorithm take a precomputation approach and thus can efficiently compute the user similarity given any new user for rating prediction in addition our algorithm can easily handle dynamic update and can be parallelized naturally which are crucial for large recommendation system theoretical analysis is presented to demonstrate the efficiency and effectiveness of our algorithm and extensive experiment also confirm our theoretical finding 
query auto completion qac is a common interactive feature that assist user in formulating query by providing completion suggestion a they type in order for qac to minimise the user s cognitive and physical effort it must i suggest the user s intended query after minimal input keystroke and ii rank the user s intended query highly in completion suggestion typically qac approach rank completion suggestion by their past popularity accordingly qac is usually very effective for previously seen and consistently popular query user are increasingly turning to search engine to find out about unpredictable emerging and ongoing event and phenomenon often using previously unseen or unpopular query consequently qac must be both robust and time sensitive that is able to sufficiently rank both consistently and recently popular query in completion suggestion to address this trade off we propose several practical completion suggestion ranking approach including i a sliding window of query popularity evidence from the past day ii the query popularity distribution in the last n query observed with a given prefix and iii short range query popularity prediction based on recently observed trend using real time simulation experiment we extensively investigated the parameter necessary to maximise qac effectiveness for three openly available query log datasets with prefix of character msn and aol both english and sogou chinese optimal parameter vary for each query log capturing the differing temporal dynamic and querying distribution result demonstrate consistent and language independent improvement of up to over a non temporal qac baseline for all query log with prefix length of character this work is an important step towards more effective qac approach 
we consider a single buyer with a combinatorial preference that would like to purchase related product and service from different vendor where each vendor supply exactly one product we study the general case where subset of product can be substitute a well a complementary and analyze the game that is induced on the vendor where a vendor s strategy is the price that he asks for his product this model generalizes both bertrand competition where vendor are perfect substitute and nash bargaining where they are perfect complement and capture a wide variety of scenario that can appear in complex crowd sourcing or in automatic pricing of related product we study the equilibrium of such game and show that a pure efficient equilibrium always exists in the case of submodular buyer preference we fully characterize the set of pure nash equilibrium essentially showing uniqueness for the even more restricted substitute buyer preference we also prove uniqueness over mixed equilibrium finally we begin the exploration of natural generalization of our setting such a when service have cost when there are multiple buyer or uncertainty about the the buyer s valuation and when a single vendor supply multiple product 
the development of semantic web rdf brings new requirement for data analytics tool and method going beyond querying to semantics rich analytics through warehouse style tool in this work we fully redesign from the bottom up core data analytics concept and tool in the context of rdf data leading to the first complete formal framework for warehouse style rdf analytics notably we define i analytical schema tailored to heterogeneous semantics rich rdf graph ii analytical query which beyond relational cube allow flexible querying of the data and the schema a well a powerful aggregation and iii olap style operation experiment on a fully implemented platform demonstrate the practical interest of our approach 
browser based defense have recently been advocated a an effective mechanism to protect web application against the threat of session hijacking fixation and related attack in existing approach all such defense ultimately rely on client side heuristic to automatically detect cooky containing session information to then protect them against theft or otherwise unintended use while clearly crucial to the effectiveness of the resulting defense mechanism these heuristic have not a yet undergone any rigorous assessment of their adequacy in this paper we conduct the first such formal assessment based on a gold set of cooky we collect from popular website of the alexa ranking to obtain the gold set we devise a semi automatic procedure that draw on a novel notion of authentication token which we introduce to capture multiple web authentication scheme we test existing browser based defense in the literature against our gold set unveiling several pitfall both in the heuristic adopted and in the method used to ass them we then propose a new detection method based on supervised learning where our gold set is used to train a binary classifier and report on experimental evidence that our method outperforms existing proposal interestingly the resulting classification together with our hand on experience in the construction of the gold set provides new insight on how web authentication is implemented in practice 
driven by outstanding success story of internet startup such a facebook and the huffington post recent study have thoroughly described their growth these highly visible online success story however overshadow an untold number of similar venture that fail the study of website popularity is ultimately incomplete without general mechanism that can describe both success and failure in this work we present six year of the daily number of user dau of twenty two membership based website encompassing online social network grassroots movement online forum and membership only internet store well balanced between success and failure we then propose a combination of reaction diffusion decay process whose resulting equation seem not only to describe well the observed dau time series but also provide mean to roughly predict their evolution this model allows an approximate automatic dau based classification of website into self sustainable v s unsustainable and whether the startup growth is mostly driven by marketing medium campaign or word of mouth adoption 
online controlled experiment also called a b testing have been established a the mantra for data driven decision making in many web facing company a b testing support decision making by directly comparing two variant at a time it can be used for comparison between two candidate treatment and a candidate treatment and an established control in practice one typically run an experiment with multiple treatment together with a control to make decision for both purpose simultaneously this is known to have two issue first having multiple treatment increase false positive due to multiple comparison second the selection process cause an upward bias in estimated effect size of the best observed treatment to overcome these two issue a two stage process is recommended in which we select the best treatment from the first screening stage and then run the same experiment with only the selected best treatment and the control in the validation stage traditional application of this two stage design often focus only on result from the second stage in this paper we propose a general methodology for combining the first screening stage data together with validation stage data for more sensitive hypothesis testing and more accurate point estimation of the treatment effect our method is widely applicable to existing online controlled experimentation system 
given the heterogeneity of the data one can find on the linked data cloud being able to trace back the provenance of query result is rapidly becoming a must have feature of rdf system while provenance model have been extensively discussed in recent year little attention ha been given to the efficient implementation of provenance enabled query inside data store this paper introduces tripleprov a new system extending a native rdf store to efficiently handle such query tripleprov implement two different storage model to physically co locate lineage and instance data and for each of them implement algorithm for tracing provenance at two granularity level in the following we present the overall architecture of our system it different lineage storage model and the various query execution strategy we have implemented to efficiently answer provenance enabled query in addition we present the result of a comprehensive empirical evaluation of our system over two different datasets and workload 
user provided rating data about product and service is one key feature of website such a amazon tripadvisor or yelp since these rating are rather static but might change over time a temporal analysis of rating distribution provides deeper insight into the evolution of a product quality given a time series of rating distribution in this work we answer the following question how to detect the base behavior of user regarding a product s evaluation over time how to detect point in time where the rating distribution differs from this base behavior e g due to attack or spontaneous change in the product s quality to achieve these goal we model the base behavior of user regarding a product a a latent multivariate autoregressive process this latent behavior is mixed with a sparse anomaly signal finally leading to the observed data we propose an efficient algorithm solving our objective and we present interesting finding on various real world datasets 
micro finance organization provide non profit lending opportunity to mitigate poverty by financially supporting impoverished yet skilled entrepreneur who are in desperate need of an institution that lends to them in kiva org a widely used crowd funded micro financial service a vast amount of micro financial activity are done by lending team and thus understanding their diverse characteristic is crucial in maintaining a healthy micro finance ecosystem a the first step for this goal we model different lending team by using a maximum entropy distribution approach based on a wealthy set of heterogeneous information regarding micro financial transaction available at kiva based on this approach we achieved a competitive performance in predicting the lending activity for the top team furthermore we provide deep insight about the characteristic of lending team by analyzing the resulting team specific lending model we found that lending team are generally more careful in selecting loan by a loan s geo location a borrower s gender a field partner s reliability etc when compared to lender without team affiliation in addition we identified interesting lending behavior of different lending team based on lender background and interest such a their ethnic religious linguistic educational regional and occupational aspect finally using our proposed model we tackled a novel problem of lending team recommendation and showed it promising performance result 
service composition us existing service based application a component to achieve a business goal the composite service operates in a highly dynamic environment hence it can fail at any time due to the failure of component service service composition language such a bpel provide a compensation mechanism to rollback the error but such a compensation mechanism ha several issue for instance it cannot guarantee the functional property of the composite service after compensation in this work we propose an automated approach based on a genetic algorithm to calculate the recovery plan that could guarantee the satisfaction of functional property of the composite service after recovery given a composite service with large state space the proposed method doe not require exploring the full state space of the composite service therefore it allows efficient selection of recovery plan in addition the selection of recovery plan is based on their quality of service qos a qos optimal recovery plan allows effective recovery from the state of failure our approach ha been evaluated on real world case study and ha shown promising result 
we study the power of fractional allocation of resource to maximize our influence in a network this work extends in a natural way the well studied model by kleinberg kempe and tardos where a designer selects a small seed set of node in a social network to influence directly this influence cascade when other node reach certain threshold of neighbor influence and the goal is to maximize the final number of influenced node despite extensive study from both practical and theoretical viewpoint this model limit the designer to a binary choice for each node with no chance to apply intermediate level of influence this model capture some setting precisely such a exposure to an idea or pathogen but it fails to capture very relevant concern in others for example a manufacturer promoting a new product by distributing five off coupon instead of giving away a single free product while fractional version of problem tend to be easier to solve than integral version for influence maximization we show that the two version have essentially the same computational complexity on the other hand the two version can have vastly different solution the added flexibility of fractional allocation can lead to significantly improved influence our main theoretical contribution is to show how to adapt the major positive result from the integral case to the fractional case specifically mossel and roch used the submodularity of influence to obtain their integral result we introduce a new notion of continuous submodularity and use this to obtain matching fractional result we conclude that we can achieve the same greedy e approximation for the fractional case a the integral case and that other heuristic are likely to carry over a well in practice we find that the fractional model performs substantially better than the integral model according to simulation on real world social network data 
homophily is a phenomenon observed very frequently in social network and is related with the inclination of people to be involved with others that exhibit similar characteristic the root of homophily can be subtle and are mainly traced back to two mechanism i social selection and ii peer influence decomposing the effect of each of these mechanism requires analysis of longitudinal data this ha been a burden to similar study in traditional social science due to the hardness of collecting such information however the proliferation of online social medium ha enabled the collection of massive amount of information related with human activity in this work we are interested in examining the force of the above mechanism in the context of the location visited by people for our study we use a longitudinal dataset collected from gowalla a location based social network lbsn lbsns unlike other online social medium bond user online interaction with their activity in real world physical location prior work on lbsns ha focused on the influence of geographical constraint on the formation of social tie on the contrary in this paper we perform a microscopic study of the peer influence and social selection mechanism in lbsns our analysis indicates that while the similarity of friend spatial trail at a geographically global scale cannot be attributed to peer influence the latter can explain up to of the geographically localized similarity between friend moreover this percentage depends on the type of location we examine and it can be even higher for specific category e g nightlife spot finally we find that the social selection mechanism is only triggered by place that exhibit specific network characteristic we believe that our work can have significant implication on obtaining a deeper understanding of the way that people create friendship act and move in real space which can further facilitate and enhance application such a recommender system trip planning and marketing 
the web ha enabled one of the most visible recent development in education the deployment of massive open online course with their global reach and often staggering enrollment moocs have the potential to become a major new mechanism for learning despite this early promise however moocs are still relatively unexplored and poorly understood in a mooc each student s complete interaction with the course material take place on the web thus providing a record of learner activity of unprecedented scale and resolution in this work we use such trace data to develop a conceptual framework for understanding how user currently engage with moocs we develop a taxonomy of individual behavior examine the different behavioral pattern of highand low achieving student and investigate how forum participation relates to other part of the course we also report on a large scale deployment of badge a incentive for engagement in a mooc including randomized experiment in which the presentation of badge wa varied across sub population we find that making badge more salient produced increase in forum engagement 
online advertising is an essential part of the internet and the main source of revenue for many web centric firm such a search engine social network and online publisher a key component of online advertising is the auction mechanism which selects and price the set of winning ad this work is inspired by one of the biggest practical drawback of the widely popular vickrey clarke grove vcg mechanism which is the unique incentive compatible mechanism that maximizes social welfare it is known that vcg lack a desired property of revenue monotonicity a natural notion which state that the revenue of a mechanism shouldn t go down a the number of bidder increase or if the bidder increase their bid most firm which depend on online advertising revenue have a large sale team to attract more bidder on their inventory a the general belief is that more bidder will increase competition and hence revenue however the lack of revenue monotonicity of vcg conflict with this general belief and can be strategically confusing for the firm s business in this work we seek incentive compatible mechanism that are revenue monotone this natural property come at the expense of social welfare one can show that it is not possible to get incentive compatibility revenue monotonicity and optimal social welfare simultaneously in light of this we introduce the notion of price of revenue monotonicity porm to capture the loss in social welfare of a revenue monotone mechanism we further study revenue monotonicity for two important online advertising scenario first one is the text v image ad auction where in an ad slot one can either show a single image ad or a few text ad second one is the video pod auction where we have a video advertising slot of k second which can be filled with multiple video ad for the image text auction we give a mechanism that satisfy both rm and ic and achieve porm of i k i ln k we also show that the porm of our mechanism is the best possible by proving a matching lower bound of i k i on the porm of any deterministic mechanism under some mild assumption for the video pod auction we give a mechanism that achieves a porm of log k ln k 
we consider a model of repeated online auction in which an ad with an uncertain click through rate face a random distribution of competing bid in each auction and there is discounting of payoff we formulate the optimal solution to this explore exploit problem a a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser s expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impression the advertiser ha received thus far we then use this result to illustrate that the value of incorporating active exploration into a machine learning system in an auction environment is exceedingly small 
r rml is used to specify transformation of data available in relational database into materialised or virtual rdf datasets sparql query evaluated against virtual datasets are translated into sql query according to the r rml mapping so that they can be evaluated over the underlying relational database engine in this paper we describe an extension of a well known algorithm for sparql to sql translation originally formalised for rdbms backed triple store that take into account r rml mapping we present the result of our implementation using query from a synthetic benchmark and from three real use case and show that sparql query can be in general evaluated a fast a the sql query that would have been generated by sql expert if no r rml mapping had been used 
understanding the group characteristic in mmorpgs is important in user behavior study since people tend to gather together and form group due to their inherent nature in this paper we analyze the group activity of user in aion one of the largest mmorpgs based on the record of the activity of user in particular we focus on i how social interaction within a group differ from the one across group ii what make a group rise sustain or fall iii how group member join and leave a group and iv what make a group end we first find that structural pattern of social interaction within a group are more likely to be close knit and reciprocative than the one across group we also observe that member in a rising group i e the number of member increase are more cohesive and communicate with more evenly within the group than the one in other group our analysis further reveals that if a group is not cohesive not actively communicating or not evenly communicating among member member of the group tend to leave 
with the rapid growth of web service in the past decade the issue of qos aware web service recommendation is becoming more and more critical since the web service qos information collection work requires much time and effort and is sometimes even impractical the service qos value is usually missing there are some work to predict the missing qos value using traditional collaborative filtering method based on user service static model however the qos value is highly related to the invocation context e g qos value are various at different time by considering the third dynamic context information a temporal qos aware web service recommendation framework is presented to predict missing qos value under various temporal context further we formalize this problem a a generalized tensor factorization model and propose a non negative tensor factorization ntf algorithm which is able to deal with the triadic relation of user service time model extensive experiment are conducted based on our real world web service qos dataset collected on planet lab which is comprised of service invocation response time and throughput value from user on web service at time period the comprehensive experimental analysis show that our approach achieves better prediction accuracy than other approach 
knowledge base kb s contain data about a large number of people organization and other entity however this knowledge can never be complete due to the dynamic of the ever changing world new company are formed every day new song are composed every minute and become of interest for addition to a kb to keep up with the real world s entity the kb maintenance process need to continuously discover newly emerging entity in news and other web stream in this paper we focus on the most difficult case where the name of new entity are ambiguous this raise the technical problem to decide whether an observed name refers to a known entity or represents a new entity this paper present a method to solve this problem with high accuracy it is based on a new model of measuring the confidence of mapping an ambiguous mention to an existing entity and a new model of representing a new entity with the same ambiguous name a a set of weighted keyphrases the method can handle both wikipedia derived entity that typically constitute the bulk of large kb s a well a entity that exist only in other web source such a online community about music or movie experiment show that our entity discovery method outperforms previous method for coping with out of kb entity called unlinkable in entity linking 
network are characterized by node and edge while there ha been a spate of recent work on estimating the number of node in a network the edge estimation question appears to be largely unaddressed in this work we consider the problem of estimating the average degree of a large network using efficient random sampling where the number of node is not known to the algorithm we propose a new estimator for this problem that relies on access to node sample under a prescribed distribution next we show how to efficiently realize this ideal estimator in a random walk setting our estimator ha a natural and simple implementation using random walk we bound it performance in term of the mixing time of the underlying graph we then show that our estimator are both provably and practically better than many natural estimator for the problem our work contrast with existing theoretical work on estimating average degree which assume that a uniform random sample of node is available and the number of node is known 
community detection ha arisen a one of the most relevant topic in the field of graph mining principally for it application in domain such a social or biological network analysis different community detection algorithm have been proposed during the last decade approaching the problem from different perspective however existing algorithm are in general based on complex and expensive computation making them unsuitable for large graph with million of vertex and edge such a those usually found in the real world in this paper we propose a novel disjoint community detection algorithm called scalable community detection scd by combining different strategy scd partition the graph by maximizing the weighted community clustering wcc a recently proposed community detection metric based on triangle analysis using real graph with ground truth overlapped community we show that scd outperforms the current state of the art proposal even those aimed at finding overlapping community in term of quality and performance scd provides the speed of the fastest algorithm and the quality in term of nmi and f score of the most accurate state of the art proposal we show that scd is able to run up to two order of magnitude faster than practical existing solution by exploiting the parallelism of current multi core processor enabling u to process graph of unprecedented size in short execution time 
personalization or customizing the experience of each individual user is seen a a useful way to navigate the huge variety of choice on the web today a key tenet of personalization is the capacity to model user preference the paradigm ha shifted from that of individual preference whereby we look at a user s past activity alone to that of shared preference whereby we model the similarity in preference between pair of user e g friend people with similar interest however shared preference are still too granular because it assumes that a pair of user would share preference across all item we therefore postulate the need to pay attention to context which refers to the specific item on which the preference between two user are to be estimated in this paper we propose a generative model for contextual agreement in preference for every triplet consisting of two user and an item the model estimate both the prior probability of agreement between the two user a well a the posterior probability of agreement with respect to the item at hand the model parameter are estimated from rating data to extend the model to unseen rating we further propose several matrix factorization technique focused on predicting agreement rather than rating experiment on real life data show that our model yield context specific similarity value that perform better on a prediction task than model relying on shared preference 
influence maximization fundamental for word of mouth marketing and viral marketing aim to find a set of seed node maximizing influence spread on social network early method mainly fall into two paradigm with certain benefit and drawback greedy algorithm selecting seed node one by one give a guaranteed accuracy relying on the accurate approximation of influence spread with high computational cost heuristic algorithm estimating influence spread using efficient heuristic have low computational cost but unstable accuracy we first point out that greedy algorithm are essentially finding a self consistent ranking where node rank are consistent with their ranking based marginal influence spread this insight motivates u to develop an iterative ranking framework i e imrank to efficiently solve influence maximization problem under independent cascade model starting from an initial ranking e g one obtained from efficient heuristic algorithm imrank find a self consistent ranking by reordering node iteratively in term of their ranking based marginal influence spread computed according to current ranking we also prove that imrank definitely converges to a self consistent ranking starting from any initial ranking furthermore within this framework a last to first allocating strategy and a generalization of this strategy are proposed to improve the efficiency of estimating ranking based marginal influence spread for a given ranking in this way imrank achieves both remarkable efficiency and high accuracy by leveraging simultaneously the benefit of greedy algorithm and heuristic algorithm a demonstrated by extensive experiment on large scale real world social network imrank always achieves high accuracy comparable to greedy algorithm while the computational cost is reduced dramatically about time faster than other scalable heuristic 
revisioned text content is present in numerous collaboration platform on the web most notably wikis to track authorship of text token in such system ha many potential application the identification of main author for licensing reason or tracing collaborative writing pattern over time to name some in this context two main challenge arise first it is critical for such an authorship tracking system to be precise in it attribution to be reliable for further processing second it ha to run efficiently even on very large datasets such a wikipedia a a solution we propose a graph based model to represent revisioned content and an algorithm over this model that tackle both issue effectively we describe the optimal implementation and design choice when tuning it to a wiki environment we further present a gold standard of token from english wikipedia article annotated with their origin this gold standard wa created manually and confirmed by multiple independent user of a crowdsourcing platform it is the first gold standard of this kind and quality and our solution achieves an average of precision on this data set we also perform a first ever precision evaluation of the state of the art algorithm for the task exceeding it by over on average our approach outperforms the execution time of the state of the art by one order of magnitude a we demonstrate on a sample of over english wikipedia article we argue that the increased size of an optional materialization of our result by about compared to the baseline is a favorable trade off given the large advantage in runtime performance 
emerging trend and product pose a challenge to modern search engine since they must adapt to the constantly changing need and interest of user for example vertical search engine such a amazon ebay walmart yelp and yahoo local provide business category hierarchy for people to navigate through million of business listing the category information also provides important ranking feature that can be used to improve search experience however category hierarchy are often manually crafted by some human expert and they are far from complete manually constructed category hierarchy cannot handle the ever changing and sometimes long tail user information need in this paper we study the problem of how to expand an existing category hierarchy for a search navigation system to accommodate the information need of user more comprehensively we propose a general framework for this task which ha three step detecting meaningful missing category modeling the category hierarchy using a hierarchical dirichlet model and predicting the optimal tree structure according to the model reorganizing the corpus using the complete category structure i e associating each webpage with the relevant category from the complete category hierarchy experimental result demonstrate that our proposed framework generates a high quality category hierarchy and significantly boost the retrieval performance 
online recommendation site are valuable information source that people contribute to and often use to choose restaurant however little is known about the dynamic behind participation in these online community and how the recommendation in these community are formed in this work we take a first look at online restaurant recommendation community to study what endogenous i e related to entity being reviewed and exogenous factor influence people s participation in the community and to what extent we analyze an online community corpus of k restaurant and their m associated review from to spread across every u s state we construct model for number of review and rating by community member based on several dimension of endogenous and exogenous factor we find that while endogenous factor such a restaurant attribute e g meal price service affect recommendation surprisingly exogenous factor such a demographic e g neighborhood diversity education and weather e g temperature rain snow season also exert a significant effect on review we find that many of the effect in online community can be explained using offline theory from experimental psychology our study is the first to look at exogenous factor and how it related to online online restaurant review it ha implication for designing online recommendation site and in general social medium and online community 
document level relevance judgment are a major component in the calculation of effectiveness metric collecting high quality judgment is therefore a critical step in information retrieval evaluation however the nature of and the assumption underlying relevance judgment collection have not received much attention in particular relevance judgment are typically collected for each document in isolation although user read each document in the context of other document in this work we aim to investigate the nature of relevance judgment collection we collect relevance label in both isolated and conditional setting and ask for judgment in various dimension of relevance a well a overall relevance then we compare the relevance metric based on various type of judgment with other metric of quality such a user preference our analysis illuminate how these setting for judgment collection affect the quality and the characteristic of the judgment we also find that the metric based on conditional judgment show higher correlation with user preference than isolated judgment 
we performed controlled experiment of human participant in a continuous sequence of ad auction similar to those used by internet company the goal of the research wa to understand user strategy in making bid we studied the behavior under two auction type the generalized second price gsp auction and the vickrey clarke grove vcg payment rule and manipulated also the participant knowledge condition explicitly given valuation and payoff information from which valuation could be deduced we found several interesting behavior among them are no convergence to equilibrium wa detected moreover the frequency with which participant modified their bid increased with time we can detect explicit better response behavior rather than just mixed bidding while bidder in gsp auction do strategically shade their bid they tend to bid higher than theoretically predicted by the standard vcg like equilibrium of gsp bidder who are not explicitly given their valuation but can only deduce them from their gain behave a little le precisely than those with such explicit knowledge but mostly during an initial learning phase vcg and gsp yield approximately the same high social welfare but gsp tends to give higher revenue 
we study the problem of computing similarity ranking in large scale multi categorical bipartite graph where the two side of the graph represent actor and item and the item are partitioned into an arbitrary set of category the problem ha several real world application including identifying competing advertiser and suggesting related query in an online advertising system or finding user with similar interest and suggesting content to them in these setting we are interested in computing on the fly ranking of similar actor given an actor and an arbitrary subset of category of interest two main challenge arise first the bipartite graph are huge and often lopsided e g the system might receive billion of query while presenting only million of advertiser second the sheer number of possible combination of category prevents the pre computation of the result for all of them we present a novel algorithmic framework that address both issue for the computation of several graph theoretical similarity measure including common neighbor and personalized pagerank we show how to tackle the imbalance in the graph to speed up the computation and provide efficient real time algorithm for computing ranking for an arbitrary subset of category finally we show experimentally the accuracy of our approach with real world data using both public graph and a very large dataset from google adwords 
statement about rdf statement or meta triple provide additional information about individual triple such a the source the occurring time or place or the certainty integrating such meta triple into semantic knowledge base would enable the querying and reasoning mechanism to be aware of provenance time location or certainty of triple however an efficient rdf representation for such meta knowledge of triple remains challenging the existing standard reification approach allows such meta knowledge of rdf triple to be expressed using rdf by two step the first step is representing the triple by a statement instance which ha subject predicate and object indicated separately in three different triple the second step is creating assertion about that instance a if it is a statement while reification is simple and intuitive this approach doe not have formal semantics and is not commonly used in practice a described in the rdf primer in this paper we propose a novel approach called singleton property for representing statement about statement and provide a formal semantics for it we explain how this singleton property approach fit well with the existing syntax and formal semantics of rdf and the syntax of sparql query language we also demonstrate the use of singleton property in the representation and querying of meta knowledge in two example of semantic web knowledge base yago and bkr our experiment on the bkr show that the singleton property approach give a decent performance in term of number of triple query length and query execution time compared to existing approach this approach which is also simple and intuitive can be easily adopted for representing and querying statement about statement in other knowledge base 
hundred of thousand of photograph are uploaded to the internet every minute through various social networking and photo sharing platform while some image get million of view others are completely ignored even from the same user different photograph receive different number of view this begs the question what make a photograph popular can we predict the number of view a photograph will receive even before it is uploaded these are some of the question we address in this work we investigate two key component of an image that affect it popularity namely the image content and social context using a dataset of about million image from flickr we demonstrate that we can reliably predict the normalized view count of image with a rank correlation of using both image content and social cue in this paper we show the importance of image cue such a color gradient deep learning feature and the set of object present a well a the importance of various social cue such a number of friend or number of photo uploaded that lead to high or low popularity of image 
after a decade long approval process multiple rejection and an independent review icann approved the xxx tld for inclusion in the domain name system to begin general availability on december it sponsoring registry proposed it a an expansion of the name space a well a a way to separate adult from child appropriate content many independent group including trademark holder political group and the adult entertainment industry itself were concerned that it would primarily generate value through defensive and speculative registration without actually serving a real need this paper measure the validity of these concern using data gathered from icann whois and web request we use this information to characterize each xxx domain and infer the registrant s most likely intent we find that at most of xxx domain host or redirect to potentially legitimate web content with the rest generally serving either defensive or speculative purpose indeed registrant spent roughly m up front to defend existing brand and trademark within the xxx tld and an additional m over the course of the first year additional evidence suggests that over of annual domain registration are for purely defensive purpose and do not even resolve 
the performance of web browser ha become a major bottleneck when dealing with complex webpage many calculation redundancy exist when processing similar webpage thus it is possible to cache and reuse previously calculated intermediate result to improve web browser performance significantly in this paper we propose a similarity based optimization approach to improve webpage processing performance of web browser through caching and reusing of style property calculated previously we are able to eliminate the redundancy caused by processing similar webpage from the same website we propose a tree structured architecture to store style property to facilitate efficient caching and reuse experiment on webpage of various website show that the proposed technique can speed up the webpage loading process by up to and reduce the redundant style calculation by up to for the first visit to a webpage with almost negligible overhead 
with more than million article the largest collaborative knowledge resource never sleep experiencing several article edits every second over one fifth of these article describes individual people the majority of which are still alive such article are by their nature prone to corruption and vandalism manual quality assurance by expert can barely cope with this massive amount of data can it be effectively replaced by feedback from the crowd can we provide meaningful support for quality assurance with automated text processing technique which property of the article should then play a key role in the machine learning algorithm and why in this paper we study the user perceived quality of wikipedia article based on a novel wikipedia user feedback dataset in contrast to previous work on quality assessment which mostly relied on judgement of active wikipedia author we analyze rating of ordinary wikipedia user along four quality dimension complete well written trustworthy and objective we first present an empirical analysis of the novel dataset with over million wikipedia article rating we then select a subset of biographical article and perform classification experiment to predict their quality rating along each of the dimension exploring multiple linguistic surface and network property of the rated article additionally we study the classification performance and difference for the biography of living and dead people a well a those for men and woman we demonstrate the effectiveness of our approach by the f score of and for the dimension complete well written trustworthy and objective based on the result we believe that the quality assessment of big textual data can be effectively supported by current text classification and language processing tool 
worker reliability is a longstanding issue in crowdsourcing and the automatic discovery of high quality worker is an important practical problem most previous work on this problem mainly focus on estimating the quality of each individual worker jointly with the true answer of each task however in practice for some task worker quality could be associated with some explicit characteristic of the worker such a education level major and age so the following question arises how do we automatically discover related worker attribute for a given task and further utilize the finding to improve data quality in this paper we propose a general crowd targeting framework that can automatically discover for a given task if any group of worker based on their attribute have higher quality on average and target such group if they exist for future work on the same task our crowd targeting framework is complementary to traditional worker quality estimation approach furthermore an advantage of our framework is that it is more budget efficient because we are able to target potentially good worker before they actually do the task experiment on real datasets show that the accuracy of final prediction can be improved significantly for the same budget or even le budget in some case our framework can be applied to many real word task and can be easily integrated in current crowdsourcing platform 
we study the pattern by which a user consumes the same item repeatedly over time in a wide variety domain ranging from check in at the same business location to re watch of the same video we find that recency of consumption is the strongest predictor of repeat consumption based on this we develop a model by which the item from t timesteps ago is reconsumed with a probability proportional to a function of t we study theoretical property of this model develop algorithm to learn reconsumption likelihood a a function of t and show a strong fit of the resulting inferred function via a power law with exponential cutoff we then introduce a notion of item quality show that it alone underperforms our recency based model and develop a hybrid model that predicts user choice based on a combination of recency and quality we show how the parameter of this model may be jointly estimated and show that the resulting scheme outperforms other alternative 
online service rely on unique identifier of machine to tailor offering to their user an implicit assumption is made that each machine identifier map to an individual however shared ma chine are common leading to interwoven search history and noisy signal for application such a personalized search and ad vertising we present method for attributing search activity to individual searcher using ground truth data for a sample of almost four million u s web searcher containing both machine identifier and person identifier we show that over half of the machine identifier comprise the query of multiple people we characterize variation in feature of topic time and other aspect such a the complexity of the information sought per the number of searcher on a machine and show significant difference in all measure based on these insight we develop model to accurately estimate when multiple people contribute to the log ascribed to a single machine identifier we also develop model to cluster search behavior on a machine allowing u to attribute historical data accurately and automatically assign new search activity to the correct searcher the finding have implication for the design of application such a personalized search and advertising that rely heavily on machine identifier to custom tailor their service 
in online social medium system user are not only posting consuming and resharing content but also creating new and destroying existing connection in the underlying social network while each of these two type of dynamic ha individually been studied in the past much le is known about the connection between the two how doe user information posting and seeking behavior interact with the evolution of the underlying social network structure here we study way in which network structure reacts to user posting and sharing content we examine the complete dynamic of the twitter information network where user post and reshare information while they also create and destroy connection we find that the dynamic of network structure can be characterized by steady rate of change interrupted by sudden burst information diffusion in the form of cascade of post re sharing often creates such sudden burst of new connection which significantly change user local network structure we also explore the effect of the information content on the dynamic of the network and find evidence that the appearance of new topic and real world event can lead to significant change in edge creation and deletion lastly we develop a model that quantifies the dynamic of the network and the occurrence of these burst a a function of the information spreading through the network the model can successfully predict which information diffusion event will lead to burst in network dynamic 
url shortening service facilitate the need of exchanging long url using limited space by creating compact url alias that redirect user to the original url when followed some of these service show advertisement ad to link clicking user and pay a commission of their advertising earnings to link shortening user in this paper we investigate the ecosystem of these increasingly popular ad based url shortening service even though traditional url shortening service have been thoroughly investigated in previous research we argue that due to the monetary incentive and the presence of third party advertising network ad based url shortening service and their user are exposed to more hazard than traditional shortening service by analyzing the service themselves the advertiser involved and their user we uncover a series of issue that are actively exploited by malicious advertiser and endanger the user moreover next to documenting the ongoing abuse we suggest a series of defense mechanism that service and user can adopt to protect themselves 
web search involves voluminous data stream that record million of user interaction with the search engine recently latent topic in web search data have been found to be critical for a wide range of search engine application such a search personalization and search history warehousing however the existing method usually discover latent topic from web search data in an offline and retrospective fashion hence they are increasingly ineffective in the face of the ever increasing web search data that accumulate in the format of online stream in this paper we propose a novel probabilistic topic model the web search stream model wssm which is delicately calibrated for handling two salient feature of the web search data it is in the format of stream and in massive volume we further propose an efficient parameter inference method the stream parameter inference spi to efficiently train wssm with massive web search stream based on a large scale search engine query log we conduct extensive experiment to verify the effectiveness and efficiency of wssm and spi we observe that wssm together with spi discovers latent topic from web search stream faster than the state of the art method while retaining a comparable topic modeling accuracy 
event sequence such a patient medical history or user sequence of product review trace how individual progress over time identifying common pattern or progression stage in such event sequence is a challenging task because not every individual follows the same evolutionary pattern stage may have very different length and individual may progress at different rate in this paper we develop a model based method for discovering common progression stage in general event sequence we develop a generative model in which each sequence belongs to a class and sequence from a given class pas through a common set of stage where each sequence evolves at it own rate we then develop a scalable algorithm to infer class of sequence while also segmenting each sequence into a set of stage we evaluate our method on event sequence ranging from patient medical history to online news and navigational trace from the web the evaluation show that our methodology can predict future event in a sequence while also accurately inferring meaningful progression stage and effectively grouping sequence based on common progression pattern more generally our methodology allows u to reason about how event sequence progress over time by discovering pattern and category of temporal evolution in large scale datasets of event 
stack overflow is the most popular community based question answering cqa website for programmer on the web with m user m question and m answer stack overflow ha explicit detailed guideline on how to post question and an ebullient moderation community despite these precise communication and safeguard question posted on stack overflow can be extremely off topic or very poor in quality such question can be deleted from stack overflow at the discretion of experienced community member and moderator we present the first study of deleted question on stack overflow we divide our study into two part i characterization of deleted question over year of data ii prediction of deletion at the time of question creation our characterization study reveals multiple insight on question deletion phenomenon we find that it take substantial time to vote a question to be deleted but once voted the community take swift action we also see that question author delete their question to salvage reputation point we notice some instance of accidental deletion of good quality question but such question are voted back to be undeleted quickly we discover a pyramidal structure of question quality on stack overflow and find that deleted question lie at the bottom lowest quality of the pyramid we also build a predictive model to detect the deletion of question at the creation time we experiment with feature based on user profile community generated question content and syntactic style and report an accuracy of our finding reveal important suggestion for content quality maintenance on community based question answering website to the best of our knowledge this is the first large scale study on poor quality deleted question on stack overflow 
web search are increasingly formulated a natural language question rather than keyword query retrieving answer to such question requires a degree of understanding of user expectation an important step in this direction is to automatically infer the type of answer implied by the question e g factoid statement on a topic instruction review etc answer type taxonomy currently exist for factoid style question but not for open domain question building taxonomy for non factoid question is a harder problem since these question can come from a very broad semantic space a few attempt have been made to develop taxonomy for non factoid question but these tend to be too narrow or domain specific in this paper we address this problem by modeling the answer type a a latent variable that is learned in a data driven fashion allowing the model to be more adaptive to new domain and data set we propose approach that detect the relevance of candidate answer to a user question by jointly clustering question according to the hidden variable and modeling relevance conditioned on this hidden variable in this paper we propose new model a logistic regression mixture lrm b glocal logistic regression mixture g lrm and c mixture glocal logistic regression mixture mg lrm that automatically learn question cluster and cluster specific relevance model all three model perform better than a baseline relevance model that us explicit answer type category predicted by a supervised answer type classifier on a newsgroups dataset our model also perform better than a baseline relevance model that doe not use any answer type information on a blog dataset 
we describe quizz a gamified crowdsourcing system that simultaneously ass the knowledge of user and acquires new knowledge from them quizz operates by asking user to complete short quiz on specific topic a a user answer the quiz question quizz estimate the user s competence to acquire new knowledge quizz also incorporates question for which we do not have a known answer the answer given by competent user provide useful signal for selecting the correct answer for these question quizz actively try to identify knowledgeable user on the internet by running advertising campaign effectively leveraging the targeting capability of existing publicly available ad placement service quizz quantifies the contribution of the user using information theory and sends feedback to the advertisingsystem about each user the feedback allows the ad targeting mechanism to further optimize ad placement our experiment which involve over ten thousand user confirm that we can crowdsource knowledge curation for niche and specialized topic a the advertising network can automatically identify user with the desired expertise and interest in the given topic we present controlled experiment that examine the effect of various incentive mechanism highlighting the need for having short term reward a goal which incentivize the user to contribute finally our cost quality analysis indicates that the cost of our approach is below that of hiring worker through paid crowdsourcing platform while offering the additional advantage of giving access to billion of potential user all over the planet and being able to reach user with specialized expertise that is not typically available through existing labor marketplace 
estimating set similarity is a central problem in many computer application in this paper we introduce the odd sketch a compact binary sketch for estimating the jaccard similarity of two set the exclusive or of two sketch equal the sketch of the symmetric difference of the two set this mean that odd sketch provide a highly space efficient estimator for set of high similarity which is relevant in application such a web duplicate detection collaborative filtering and association rule learning the method extends to weighted jaccard similarity relevant e g for tf idf vector comparison we present a theoretical analysis of the quality of estimation to guarantee the reliability of odd sketch based estimator our experiment confirm this efficiency and demonstrate the efficiency of odd sketch in comparison with b bit minwise hashing scheme on association rule learning and web duplicate detection task 
topic modeling ha been proved to be an effective method for exploratory text mining it is a common assumption of most topic model that a document is generated from a mixture of topic in real world scenario individual document usually concentrate on several salient topic instead of covering a wide variety of topic a real topic also adopts a narrow range of term instead of a wide coverage of the vocabulary understanding this sparsity of information is especially important for analyzing user generated web content and social medium which are featured a extremely short post and condensed discussion in this paper we propose a dual sparse topic model that address the sparsity in both the topic mixture and the word usage by applying a spike and slab prior to decouple the sparsity and smoothness of the document topic and topic word distribution we allow individual document to select a few focused topic and a topic to select focused term respectively experiment on different genre of large corpus demonstrate that the dual sparse topic model outperforms both classical topic model and existing sparsity enhanced topic model this improvement is especially notable on collection of short document 
monitoring web browsing behavior ha benefited many data mining application such a top k discovery and anomaly detection however releasing private user data to the greater public would concern web user about their privacy especially after the incident of aol search log release where anonymization wa not correctly done in this paper we adopt differential privacy a strong provable privacy definition and show that differentially private aggregate of web browsing activity can be released in real time while preserving the utility of shared data our proposed algorithm utilize the rich correlation of the time series of aggregated data and adopt a state space approach to estimate the underlying true aggregate from the perturbed value by the differential privacy mechanism we evaluate our algorithm with real world web browsing data utility evaluation with three metric demonstrate that the quality of the private released data by our solution closely resembles that of the original unperturbed aggregate 
linked open data lod comprises an unprecedented volume of structured data on the web however these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality we present a methodology for test driven quality assessment of linked data which is inspired by test driven software development we argue that vocabulary ontology and knowledge base should be accompanied by a number of test case which help to ensure a basic level of quality we present a methodology for assessing the quality of linked data resource based on a formalization of bad smell and data quality problem our formalization employ sparql query template which are instantiated into concrete quality test case query based on an extensive survey we compile a comprehensive library of data quality test case pattern we perform automatic test case instantiation based on schema constraint or semi automatically enriched schema and allow the user to generate specific test case instantiation that are applicable to a schema or dataset we provide an extensive evaluation of five lod datasets manual test case instantiation for five schema and automatic test case instantiation for all available schema registered with linked open vocabulary lov one of the main advantage of our approach is that domain specific semantics can be encoded in the data quality test case thus being able to discover data quality problem beyond conventional quality heuristic 
in recent year location aware music recommendation is increasing in popularity a more and more user consume music on the move in this demonstration we present an intelligent system called just for me to facilitate accurate music recommendation based on where user present our system is developed based on a novel probabilistic generative model which can effectively integrate the location context and global music popularity trend this approach allows u to gain more comprehensive modeling on user preference and thus significantly enhances the music recommendation performance 
we propose two dynamic indexing scheme for shortest path and distance query on large time evolving graph which are useful in a wide range of important application such a real time network aware search and network evolution analysis to the best of our knowledge these method are the first practical exact indexing method to efficiently process distance query and dynamic graph update we first propose a dynamic indexing scheme for query on the last snapshot the scalability and efficiency of it offline indexing algorithm and query algorithm are competitive even with previous static method meanwhile the method is dynamic that is it can incrementally update index a the graph change over time then we further design another dynamic indexing scheme that can also answer two kind of historical query with regard to not only the latest snapshot but also previous snapshot through extensive experiment on real and synthetic evolving network we show the scalability and efficiency of our method specifically they can construct index from large graph with million of vertex answer query in microsecond and update index in millisecond 
this paper address the problem of extracting accurate label from crowdsourced datasets a key challenge in crowdsourcing prior work ha focused on modeling the reliability of individual worker for instance by way of confusion matrix and using these latent trait to estimate the true label more accurately however this strategy becomes ineffective when there are too few label per worker to reliably estimate their quality to mitigate this issue we propose a novel community based bayesian label aggregation model communitybcc which assumes that crowd worker conform to a few different type where each type represents a group of worker with similar confusion matrix we assume that each worker belongs to a certain community where the worker s confusion matrix is similar to a perturbation of the community s confusion matrix our model can then learn a set of key latent feature i the confusion matrix of each community ii the community membership of each user and iii the aggregated label of each item we compare the performance of our model against established aggregation method on a number of large scale real world crowdsourcing datasets our experimental result show that our communitybcc model consistently outperforms state of the art label aggregation method requiring on average le data to pas the accuracy mark 
a they compete for developer mobile app ecosystem have been exposing a growing number of apis through their software development kit many of these apis involve accessing sensitive functionality and or user data and require approval by user android for instance allows developer to select from over possible permission expecting user to review and possibly adjust setting related to these permission ha proven unrealistic in this paper we report on the result of a study analyzing people s privacy preference when it come to granting permission to different mobile apps our result suggest that while people s mobile app privacy preference are diverse a relatively small number of profile can be identified that offer the promise of significantly simplifying the decision mobile user have to make specifically our result are based on the analysis of setting of million smartphone user of a mobile security and privacy platform the platform relies on a rooted version of android where user are allowed to choose between granting denying or requesting to be dynamically prompted when it come to granting different android permission to mobile apps they have downloaded 
how doe one develop a new online community that is highly engaging to each user and promotes social interaction a number of website offer friend finding feature that help user bootstrap social network on the website by copying link from an established network like facebook or twitter this paper quantifies the extent to which such social bootstrapping is effective in enhancing a social experience of the website first we develop a stylised analytical model that suggests that copying tends to produce a giant connected component i e a connected community quickly and preserve property such a reciprocity and clustering up to a linear multiplicative factor second we use data from two website pinterest and last fm to empirically compare the subgraph of link copied from facebook to link created natively we find that the copied subgraph ha a giant component higher reciprocity and clustering and confirm that the copied connection see higher social interaction however the need for copying diminishes a user become more active and influential such user tend to create link natively on the website to user who are more similar to them than their facebook friend our finding give new insight into understanding how bootstrapping from established social network can help engage new user by enhancing social interactivity 
clustering web item i e web resource like video image into semantic group benefit many application such a organizing item generating meaningful tag and improving web search in this paper we systematically investigate how user generated comment can be used to improve the clustering of web item in our preliminary study of last fm we find that the two data source extracted from user comment the textual comment and the commenting user provide complementary evidence to the item intrinsic feature these source have varying level of quality but we importantly we find that incorporating all three source improves clustering to accommodate such quality imbalance we invoke multi view clustering in which each data source represents a view aiming to best leverage the utility of different view to combine multiple view under a principled framework we propose conmf co regularized non negative matrix factorization which extends nmf for multi view clustering by jointly factorizing the multiple matrix through co regularization under our conmf framework we devise two paradigm pair wise conmf and cluster wise conmf and propose iterative algorithm for their joint factorization experimental result on last fm and yelp datasets demonstrate the effectiveness of our solution in last fm conmf better k mean with a statistically significant f increase of while achieving comparable performance with the state of the art multi view clustering method cosc co regularized spectral clustering on a yelp dataset conmf outperforms the best baseline cosc with a statistically significant performance gain of 
although criticized for some of it limitation modularity remains a standard measure for analyzing social network quantifying the statistical surprise in the arrangement of the edge of the network ha led to simple and powerful algorithm however relying solely on the distribution of edge instead of more complex structure such a path limit the extent of modularity indeed recent study have shown restriction of optimizing modularity for instance it resolution limit we introduce here a novel formal and well defined modularity measure based on random walk we show how this modularity can be computed from path induced by the graph instead of the traditionally used edge we argue that by computing modularity on path instead of edge more informative feature can be extracted from the network we verify this hypothesis on a semi supervised classification procedure of the node in the network where we show that under the same setting the feature of the random walk modularity help to classify better than the feature of the usual modularity additionally the proposed approach outperforms the classical label propagation procedure on two data set of labeled social network 
one problem facing player of competitive game is negative or toxic behavior league of legend the largest esport game us a crowdsourcing platform called the tribunal to judge whether a reported toxic player should be punished or not the tribunal is a two stage system requiring report from those player that directly observe toxic behavior and human expert that review aggregated report while this system ha successfully dealt with the vague nature of toxic behavior by majority rule based on many vote it naturally requires tremendous cost time and human effort in this paper we propose a supervised learning approach for predicting crowdsourced decision on toxic behavior with large scale labeled data collection over million user report involved in million toxic player and corresponding crowdsourced decision our result show good performance in detecting overwhelmingly majority case and predicting crowdsourced decision on them we demonstrate good portability of our classifier across region finally we estimate the practical implication of our approach potential cost saving and victim protection 
a weakly supervised method acquires fine grained class label that do not occur verbatim in the input data or underlying text collection the method generates more specific class label gold mining company listed on the toronto stock exchange that capture the semantics of the underlying class out of pair of input class label company listed on the toronto stock exchange gold mining company available for an instance golden star resource when applied to wikipedia article and their category the method generates new category for existing article and expands existing category with additional article 
user attribute such a occupation education and location are important for many application in this paper we study the problem of profiling user attribute in social network to capture the correlation between attribute and social connection we present a new insight that social connection are discriminatively correlated with attribute via a hidden factor relationship type for example a user s colleague are more likely to share the same employer with him than other friend based on the insight we propose to co profile user attribute and relationship type of their connection to achieve co profiling we develop an efficient algorithm based on an optimization framework our algorithm capture our insight effectively it iteratively profile attribute by propagation via certain type of connection and profile type of connection based on attribute and the network structure we conduct extensive experiment to evaluate our algorithm the result show that our algorithm profile various attribute accurately which improves the state of the art method by 
named entity recognition ner play an important role in a variety of online information management task including text categorization document clustering and faceted search while recent ner system can achieve near human performance on certain document like news article they still remain highly domain specific and thus cannot effectively identify entity such a original technical concept in scientific document in this work we propose novel approach for ner on distinctive document collection such a scientific article based on n gram inspection and classification we design and evaluate several entity recognition feature ranging from well known part of speech tag to n gram co location statistic and decision tree to classify candidate in addition we show how the use of external knowledge base either specific like dblp or generic like dbpedia can be leveraged to improve the effectiveness of ner for idiosyncratic collection we evaluate our system on two test collection created from a set of computer science and physic paper and compare it against state of the art supervised method experimental result show that a careful combination of the feature we propose yield up to ner accuracy over scientific collection and substantially outperforms state of the art approach such a those based on maximum entropy 
eli pariser coined the term filter bubble to describe the potential for online personalization to effectively isolate people from a diversity of viewpoint or content online recommender system built on algorithm that attempt to predict which item user will most enjoy consuming are one family of technology that potentially suffers from this effect because recommender system have become so prevalent it is important to investigate their impact on user in these term this paper examines the longitudinal impact of a collaborative filtering based recommender system on user to the best of our knowledge it is the first paper to measure the filter bubble effect in term of content diversity at the individual level we contribute a novel metric to measure content diversity based on information encoded in user generated tag and we present a new set of method to examine the temporal effect of recommender system on the user experience we do find that recommender system expose user to a slightly narrowing set of item over time however we also see evidence that user who actually consume the item recommended to them experience lessened narrowing effect and rate item more positively 
over the past few year massive amount of world knowledge have been accumulated in publicly available knowledge base such a freebase nell and yago yet despite their seemingly huge size these knowledge base are greatly incomplete for example over of people included in freebase have no known place of birth and have no known ethnicity in this paper we propose a way to leverage existing web search based question answering technology to fill in the gap in knowledge base in a targeted way in particular for each entity attribute we learn the best set of query to ask such that the answer snippet returned by the search engine are most likely to contain the correct value for that attribute for example if we want to find frank zappa s mother we could ask the query who is the mother of frank zappa however this is likely to return the mother of invention which wa the name of his band our system learns that it should in this case add disambiguating term such a zappa s place of birth in order to make it more likely that the search result contain snippet mentioning his mother our system also learns how many different query to ask for each attribute since in some case asking too many can hurt accuracy by introducing false positive we discus how to aggregate candidate answer across multiple query ultimately returning probabilistic prediction for possible value for each attribute finally we evaluate our system and show that it is able to extract a large number of fact with high confidence 
together with the sign positive or negative and strength strong or weak the directionality is also an important property of social tie though usually ignored in undirected social network for it invisibility however we believe most social tie are natively directed and the awareness of directionality can improve our understanding about the network structure and further benefit social network analysis and mining task thus it s appealing to study whether there exist interesting pattern about directionality in social network and whether we can learn the direction for undirected network based on these pattern in this study we engage in the investigation of directionality pattern on real world directed social network and summarize our finding using four consistency hypothesis based on these hypothesis we propose redirect an optimization framework which make it possible to infer the hidden direction of undirected social tie based on the network topology only this general framework can incorporate various predictive model under specific scenario furthermore we show how to improve redirect by introducing semi self supervision in the framework and how to construct the self labeled training data using simple but effective heuristic experimental result show that even without external information our approach can recover the direction of network effectively moreover we re quite surprising to find that redirect can benefit predictive task remarkably with a case study of link prediction in experiment the redirected network inferred using redirect are proven much more informative than original undirected one and can improve the prediction performance significantly it convinces u that redirect can be a beneficial general data preprocess tool for various network analysis and mining task by uncovering the hidden direction of undirected social network 
collaborative filtering with implicit feedback ha been steadily receiving more attention since the abundant implicit feedback are more easily collected while explicit feedback are not necessarily always available several recent work address this problem well utilizing pairwise ranking method with a fundamental assumption that a user prefers item with positive feedback to the item without observed feedback which also implies that the item without observed feedback are treated equally without distinction however user have their own preference on different item with different degree which can be modeled into a ranking relationship in this paper we exploit this prior information of a user s preference from the nearest neighbor set by the neighbor implicit feedback which can split item into different item group with specific ranking relation we propose a novel prigp personalized ranking with item group based pairwise preference learning algorithm to integrate item based pairwise preference and item group based pairwise preference into the same framework experimental result on three real world datasets demonstrate the proposed method outperforms the competitive baseline on several ranking oriented evaluation metric 
for many language that use non roman based indigenous script e g arabic greek and indic language one can often find a large amount of user generated transliterated content on the web in the roman script such content creates a monolingual or multi lingual space with more than one script which we refer to a the mixed script space ir in the mixed script space is challenging because query written in either the native or the roman script need to be matched to the document written in both the script moreover transliterated content feature extensive spelling variation in this paper we formally introduce the concept of mixed script ir and through analysis of the query log of bing search engine estimate the prevalence and thereby establish the importance of this problem we also give a principled solution to handle the mixed script term matching and spelling variation where the term across the script are modelled jointly in a deep learning architecture and can be compared in a low dimensional abstract space we present an extensive empirical analysis of the proposed method along with the evaluation result in an ad hoc retrieval setting of mixed script ir where the proposed method achieves significantly better result increase in mrr and increase in map compared to other state of the art baseline 
cross medium hashing which conduct cross medium retrieval by embedding data from different modality into a common low dimensional hamming space ha attracted intensive attention in recent year the existing cross medium hashing approach only aim at learning hash function to preserve the intra modality and inter modality correlation but do not directly capture the underlying semantic information of the multi modal data we propose a discriminative coupled dictionary hashing dcdh method in this paper in dcdh the coupled dictionary for each modality is learned with side information e g category a a result the coupled dictionary not only preserve the intra similarity and inter correlation among multi modal data but also contain dictionary atom that are semantically discriminative i e the data from the same category is reconstructed by the similar dictionary atom to perform fast cross medium retrieval we learn hash function which map data from the dictionary space to a low dimensional hamming space besides we conjecture that a balanced representation is crucial in cross medium retrieval we introduce multi view feature on the relatively weak modality into dcdh and extend it to multi view dcdh mv dcdh in order to enhance their representation capability the experiment on two real world data set show that our dcdh and mv dcdh outperform the state of the art method significantly on cross medium retrieval 
passage based retrieval model have been studied for some time and have been shown to have some benefit for document ranking finding passage that are not only topically relevant but are also answer to the user question would have a significant impact in application such a mobile search to develop model for answer passage retrieval we need to have appropriate test collection and evaluation measure making annotation at the passage level is however expensive and can have poor coverage in this paper we describe the advantage of document summarization measure for evaluating answer passage retrieval and show that these measure have high correlation with existing measure and human judgment 
we study time critical search where user have urgent information need in the context of an acute problem a example user may need to know how to stem a severe bleed help a baby who is choking on a foreign object or respond to an epileptic seizure while time critical situation and action have been studied in the realm of decision support system little ha been done with time critical search and retrieval and little direct support is offered by search system critical challenge with time critical search include accurately inferring when user have urgent need and providing relevant information that can be understood and acted upon quickly we leverage survey and search log data from a large mobile search provider to a characterize the use of search engine for time critical situation and b develop predictive model to accurately predict urgent information need given a query and a diverse set of feature spanning topical temporal behavioral and geospatial attribute the method and finding highlight opportunity for extending search and retrieval to consider the urgency of query 
text based social medium channel such a twitter produce torrent of opinionated data about the most diverse topic and entity the analysis of such data aka sentiment analysis is quickly becoming a key feature in recommender system and search engine a prominent approach to sentiment analysis is based on the application of classification technique that is content is classified according to the attitude of the writer a major challenge however is that twitter follows the data stream model and thus classifier must operate with limited resource including labeled data and time for building classification model also challenging is the fact that sentiment distribution may change a the stream evolves in this paper we address these challenge by proposing algorithm that select relevant training instance at each time step so that training set are kept small while providing to the classifier the capability to suit itself to and to recover itself from different type of sentiment drift simultaneously providing capability to the classifier however is a conflicting objective problem and our proposed algorithm employ basic notion of economics in order to balance both capability we performed the analysis of event that reverberated on twitter and the comparison against the state of the art reveals improvement both in term of error reduction up to and reduction of training resource by order of magnitude 
multi tree ensemble model have been proven to be effective for document ranking using a large number of tree can improve accuracy but it take time to calculate ranking score of matched document this paper investigates data traversal method for fast score calculation with a large ensemble we propose a d blocking scheme for better cache utilization with simpler code structure compared to previous work the experiment with several benchmark show significant acceleration in score calculation without loss of ranking accuracy 
we propose to enhance proximity based probabilistic retrieval model with more contextual information a term pair with higher contextual relevance of term proximity is assigned a higher weight several measure are proposed to estimate the contextual relevance of term proximity we assume the top ranked document from a basic weighting model are more relevant to the query and calculate the contextual relevance of term proximity using the top ranked document we propose a context sensitive proximity model and the experimental result on standard trec data set show the effectiveness of our proposed model 
one of the fundamental problem in image search is to rank image document according to a given textual query existing search engine highly depend on surrounding text for ranking image or leverage the query image pair annotated by human labelers to train a series of ranking function however there are two major limitation the surrounding text are often noisy or too few to accurately describe the image content and the human annotation are resourcefully expensive and thus cannot be scaled up we demonstrate in this paper that the above two fundamental challenge can be mitigated by jointly exploring the cross view learning and the use of click through data the former aim to create a latent subspace with the ability in comparing information from the original incomparable view i e textual and visual view while the latter explores the largely available and freely accessible click through data i e crowdsourced human intelligence for understanding query specifically we propose a novel cross view learning method for image search named click through based cross view learning ccl by jointly minimizing the distance between the mapping of query and image in the latent subspace and preserving the inherent structure in each original space on a large scale click based image dataset ccl achieves the improvement over support vector machine based method by in term of relevance while reducing the feature dimension by several order of magnitude e g from thousand to ten moreover the experiment also demonstrate the superior performance of ccl to several state of the art subspace learning technique 
the potential impact of a scientific article ha a significant correlation with it ability to establish novel connection between pre existing knowledge discovering hidden connection between the existing scientific literature is an interesting yet highly challenging information retrieval problem literature based discovery lbd us computational algorithm to discover potential hidden connection between previously disconnected set of literature most of the current lbd method focus on analyzing latent semantic feature in text but are usually computationally demanding in particular they do not aim at predicting novel discovery link between cluster of literature combining latent semantic and structural feature of literature is a promising yet unexplored lbd approach this approach is potentially scalable and effective for example incorporating structural feature of web page ha increased the effectiveness of many large scale ir system the bibliographic structure of scientific paper make it possible to view a corpus of literature a a complex network of node article and link citation relationship in which recognizable community or cluster can be observed each representing a distinct research field consequently potential hidden connection between disparate field might be found from among non overlapping cluster that do not have any existing link between their member yet exhibit a high propensity to converge in the future this work approach lbd a a cluster link prediction problem we view disjoint literature set a disjoint cluster in citation network our method search for hidden connection between disjoint cluster whose member node show high probability in forming future link to this end we address two research problem the first problem is to group paper into cluster of distinct research area we compare the accuracy of well known community detection algorithm such a louvain and infomap in detecting research field cluster from citation network of physic literature we evaluate the quality of these cluster using purity rand index f measure and normalized mutual information since ground truth community are usually unknown we also propose using alternative textual coherence measure such a jensen shannon divergence the second problem is to predict the future formation of link between the node in previously disconnected cluster we introduce a novel algorithm latent domain similarity lds which us combination of semantic feature e g distribution of technical term in title and abstract and structural feature e g cited reference citing article of two or more article in order to infer shared latent domain between them we assume that while two set of literature could have been published separately in two seemingly unrelated field it is possible that they share many similar domain previously unknown to researcher in each field the goal is to explore whether these shared latent domain correlate with the probability of previously disconnected cluster to form future citation link with each other 
all research project begin with a goal for instance to describe search behavior to predict when a person will enter a second query or to discover which ir system performs the best different research goal suggest different research approach ranging from field study to lab study to online experimentation this tutorial will provide an overview of the different type of research goal common evaluation approach used to address each type and the constraint each approach entail participant will come away with a broad perspective of research goal and approach in ir and an understanding of the benefit and limitation of these research approach the tutorial will take place in two independent but interrelated part each focusing on a unique set of research approach but with the same intended tutorial outcome these outcome will be accomplished by deconstructing and analyzing our own published research paper with further illustration of each technique using the broader literature by using our own research a anchor we will provide insight about the research process revealing the difficult choice and trade offs researcher make when designing and conducting ir study 
microblogs such a twitter are important source for spreading vital information at high speed they also reflect the general people s reaction and opinion towards major event or story with information traveling so quickly it is helpful to be able to apply unsupervised learning technique to discover topic for information extraction and analysis although graphical model have been traditionally used for topic discovery in microblogs and text stream previous work may not be a efficient because of the diverse and noisy nature of microblogs in this paper we demonstrate the application of the author topic and the author recipient topic model to microblogs we extensively compare these model under different setting to an lda baseline our result show that the author recipient topic model extract the most coherent topic establishing that joint modeling on author recipient pair and on the content of tweet lead to quantitatively better topic discovery this paper also address the problem of topic modeling on short text by using clustering technique this technique help in boosting the performance of our model our study reveals interesting trait about twitter message user and their interaction 
we propose a two stage lossless compression approach on large scale rdf data our approach exploit both representation compression and component compression technique to support query and dynamic operation directly on the compressed data 
religious belief play an important role in how people behave influencing how they form preference interpret event around them and develop relationship with others traditionally the religion label of user population are obtained by conducting a large scale census study such an approach is both high cost and time consuming in this paper we study the problem of predicting user religion label using their microblogging data we formulate religion label prediction a a classification task and identify content structure and aggregate feature considering their self and social variant for representing a user we introduce the notion of representative user to identify user who are important in the religious user community we further define feature using representative user we show that svm classifier using our proposed feature can accurately assign christian and muslim label to a set of twitter user with known religion label 
a large number of web query are related to product entity studying evolution of product entity can help analyst understand the change in particular attribute value for these product however studying the evolution of a product requires u to be able to link various version of a product together in a temporal order while it is easy to temporally link recent version of product in a few domain manually solving the problem in general is challenging the ability to temporally order and link various version of a single product can also improve product search engine in this paper we tackle the problem of finding the previous version predecessor of a product entity given a repository of product entity we first parse the product name using a crf model after identifying entity corresponding to a single product we solve the problem of finding the previous version of any given particular version of the product for the second task we leverage innovative feature with a na ve bayes classifier our method achieve a precision of in identifying the product version from product entity name and a precision of in identifying the predecessor 
we examine the effect of expanding a judged set of sentence with their duplicate from a corpus including new sentence that are exact duplicate of the previously judged sentence may allow for better estimation of performance metric and enhance the reusability of a test collection we perform experiment in context of the temporal summarization track at trec we find that adding duplicate sentence to the judged set doe not significantly affect relative system performance however we do find statistically significant change in the performance of nearly half the system that participated in the track we recommend adding exact duplicate sentence to the set of relevance judgement in order to obtain a more accurate estimate of system performance 
this research fall in the area of enhancing the quality of tag based item recommendation system it aim to achieve this by employing a multi dimensional user profile approach and by analyzing the semantic aspect of tag tag based recommender system have two characteristic that need to be carefully studied in order to build a reliable system firstly the multi dimensional correlation called a tag assignment user item tag should be appropriately modelled in order to create the user profile secondly the semantics behind the tag should be considered properly a the flexibility with their design can cause semantic problem such a synonymy and polysemy this research proposes to address these two challenge for building a tag based item recommendation system by employing tensor modeling a the multi dimensional user profile approach and the topic model a the semantic analysis approach the first objective is to optimize the tensor model reconstruction and to improve the model performance in generating quality recommendation a novel tensor based recommendation using probabilistic ranking trpr method ha been developed result show this method to be scalable for large datasets and outperforming the benchmarking method in term of accuracy the memory efficient loop implement the n mode block striped matrix product for tensor reconstruction a an approximation of the initial tensor the probabilistic ranking calculates the probability of user to select candidate item using their tag preference list based on the entry generated from the reconstructed tensor the second objective is to analyse the tag semantics and utilize the outcome in building the tensor model this research proposes to investigate the problem using topic model approach to keep the tag nature a the social vocabulary for the tag assignment data topic can be generated from the occurrence of tag given for an item however there is only limited amount of tag available to represent item a collection of topic since an item might have only been tagged by using several tag consequently the generated topic might not able to represent the item appropriately furthermore given that each tag can belong to any topic with various probability score the occurrence of tag cannot simply be mapped by the topic to build the tensor model a standard weighting technique will not appropriately calculate the value of tagging activity since it will define the context of an item using a tag instead of a topic 
in this work we investigate approach to engineer better topic set in information retrieval test collection by recasting the trec evaluation exercise from one of building more effective system to an exercise in building better topic we present two possible approach to quantify topic goodness topic ease and topic set predictivity a novel interpretation of a well known result and a twofold analysis of data from several trec edition lead to a result that ha been neglected so far both topic ease and topic set predictivity have changed significantly across the year sometimes in a perhaps undesirable way 
the task of author verification is concerned with the question whether or not someone is the author of a given piece of text algorithm that extract writing style feature from text are used to determine how close in style different document are currently evaluation of author verification algorithm are restricted to small scale corpus with usually le than one hundred test case in this work we present a methodology to derive a large scale author verification corpus based on wikipedia talkpages we create a corpus based on english wikipedia which is significantly larger than existing corpus we investigate two dimension on this corpus which so far have not received sufficient attention the influence of topic and the influence of time on author verification accuracy 
there are many existing study of user behavior in simple task e g navigational and informational search within a short duration of query however we know relatively little about user behavior especially browsing and clicking behavior for longer search session solving complex search task in this paper we characterize and compare user behavior in relatively long search session minute about query for search task of four different type the task differ in two dimension the user is locating fact or is pursuing intellectual understanding of a topic the user ha a specific task goal or ha an ill defined and undeveloped goal we analyze how search behavior a well a browsing and clicking pattern change during a search session in these different task our result indicate that user behavior in the four type of task differ in various aspect including search activeness browsing style clicking strategy and query reformulation a a search session progress we note that user shift their interest to focus le on the top result but more on result ranked at lower position in browsing we also found that result eventually become le and le attractive for the user the reason vary and include downgraded search performance of query decreased novelty of search result and decaying persistence of user in browsing our study highlight the lack of long session support in existing search engine and suggests different strategy of supporting longer session according to different task type 
term relevance prediction from brain signal trpb is proposed to automatically detect relevance of text information directly from brain signal an experiment with forty participant wa conducted to record neural activity of participant while providing relevance judgment to text stimulus for a given topic high precision scientific equipment wa used to quantify neural activity across electroencephalography eeg channel a classifier based on a multi view eeg feature representation showed improvement up to in relevance prediction based on brain signal alone relevance wa also associated with brain activity with significant change in certain brain area consequently trpb is based on change identified in specific brain area and doe not require user specific training or calibration hence relevance prediction can be conducted for unseen content and unseen participant a an application of trpb we demonstrate a high precision variant of the classifier that construct set of relevant term for a given unknown topic of interest our research show that detecting relevance from brain signal is possible and allows the acquisition of relevance judgment without a need to observe any other user interaction this suggests that trpb could be used in combination or a an alternative for conventional implicit feedback signal such a dwell time or click through activity 
state of the art question answering qa system employ passage retrieval based on bag of word similarity model with respect to a query and a passage we propose a combination of a traditional bag of word similarity model and an annotation similarity model to improve passage ranking the proposed annotation similarity model is generic enough to process annotation of arbitrary type historical fact validation is a subtask to determine whether a given sentence tell u historically correct information which is important for a qa task on world history experimental result show that the combined model gain up to and improvement in historical fact validation in term of precision at rank and mean reciprocal rank respectively 
we present a novel unsupervised approach to re ranking an initially retrieved list the approach is based on the cross entropy method applied to permutation of the list and relies on performance prediction using pseudo predictor we establish a lower bound on the prediction quality that is required so a to have our approach significantly outperform the original retrieval our experiment serve a a proof of concept demonstrating the considerable potential of the proposed approach a case in point only a tiny fraction of the huge space of permutation need to be explored to attain significant improvement over the original retrieval 
caching query result is an efficient technique for web search engine admission policy can prevent infrequent query from taking space of more frequent query in the cache in this paper we present two novel admission policy tailored for query result cache these policy are based on query result prefetching information we also propose a demote operation for the query result cache to improve the cache hit ratio we then use a trace of over million query to evaluate our admission policy a well a traditional policy experimental result show that our prefetch aware admission policy can achieve hit ratio better than state of the art admission policy 
all pair similarity search used in many data mining and information retrieval application is a time consuming process although a partition based approach accelerates this process by simplifying parallelism management and avoiding unnecessary i o and comparison it is still challenging to balance the computation load among parallel machine with a distributed architecture this is mainly due to the variation in partition size and irregular dissimilarity relationship in large datasets this paper present a two stage heuristic algorithm to improve the load balance and shorten the overall processing time we analyze the optimality and competitiveness of the proposed algorithm and demonstrates it effectiveness using several datasets we also describe a static partitioning algorithm to even out the partition size while detecting more dissimilar pair the evaluation result show that the proposed scheme outperforms a previously developed solution by up to in the tested case 
prior art stay at the foundation for future work in academic research however the increasingly large amount of publication make it difficult for researcher to effectively discover the most important previous work to the topic of their research in this paper we study the automatic discovery of the core paper for a research area we propose a collective topic model on three type of object paper author and published venue we model any of these object a bag of citation based on probabilistic latent semantic analysis plsa authorship published venue and citation relation are used for quantifying paper importance our method discus milestone paper discovery in different case of input object experiment on the acl anthology network ann indicate that our model is superior in milestone paper discovery when compared to a previous model which considers only paper 
existing work on collaborative filtering cf is often based on the overall rating the item have received however in many case understanding how a user rate each aspect of an item may reveal more detailed information about her preference and thus may lead to more effective cf prior work ha studied extracting quantizing sentiment on different aspect from the review based on which the unknown overall rating are inferred however in that work all the aspect are treated equally while in reality different user tend to place emphasis on difference aspect when reaching the overall rating for example user may give a high rating to a movie just for it plot despite it mediocre performance this emphasis on aspect varies for different user and different item in this paper we propose a method that us tensor factorization to automatically infer the weight of different aspect in forming the overall rating the main idea is to learn through constrained optimization a compact representation of a weight tensor indexed by three dimension for user item and aspect respectively overall rating can then be predicted using the obtained weight experiment on a movie dataset show that our method compare favorably with three baseline method 
for large scale category system such a directory mozilla which consist of ten of thousand category it ha been empirically verified in earlier study that the distribution of document among category can be modeled a a power law distribution it implies that a significant fraction of category referred to a rare category have very few document assigned to them this characteristic of the data make it harder for learning algorithm to learn effective decision boundary which can correctly detect such category in the test set in this work we exploit the distribution of document among category to i derive an upper bound on the accuracy of any classifier and ii propose a ranking based algorithm which aim to maximize this upper bound the empirical evaluation on publicly available large scale datasets demonstrate that the proposed method not only achieves higher accuracy but also much higher coverage of rare category a compared to state of the art method 
to use math expression in search current search engine require knowing expression name or using a structure editor or string encoding e g latex for mathematical non expert this can lead to an intention gap between the query they wish to express and what the interface will allow them to express min is a search interface that support drawing expression on a canvas using mouse touch keyboard and image we present a user study examining whether min change search behavior for mathematical non expert and to identify real world usage scenario for multimodal math search interface participant found query by expression using hand drawn input useful and identified scenario in which they would like to use system like min such a for locating editing and sharing complex expression e g with many greek letter and working on complex math problem 
score safe index processing ha received a great deal of attention over the last two decade by pre calculating maximum term impact during indexing the number of scoring operation can be minimized and the top k document for a query can be located efficiently however these method often ignore the importance of the effectiveness gain possible when using sequential dependency model we present a hybrid approach which leverage score safe processing and suffix based self indexing structure in order to provide efficient and effective top k document retrieval 
a the scale of available on line data grows ever larger individual and business must cope with increasing complexity in decision making process which utilize large volume of unstructured semi structured and or structured data to satisfy multiple interrelated information need which contribute to an overall decision traditional decision support system ds have been developed to address this need but such system are typically expensive to build and are purpose built for a particular decision making scenario making them difficult to extend or adapt to new decision scenario in this paper we propose a novel decision representation which allows decision maker to formulate and organize natural language question or assertion into an analytic hierarchy which can be evaluated a part of an ad hoc decision process or a a documented repeatable analytic process we then introduce a new decision support framework quad which take advantage of automatic question answering qa technology to automatically understand and process a decision representation producing a final decision by gathering and weighting answer to individual question using a bayesian learning and inference process an open source framework implementation is presented and applied to two real world application target validation a fundamental decision making task for the pharmaceutical industry and product recommendation from review text an everyday decision making situation faced by on line consumer in both application we implemented and compared a number of decision synthesis algorithm and present experimental result which demonstrate the performance of the quad approach versus other baseline approach 
search satisfaction is a property of a user s search process understanding it is critical for search provider to evaluate the performance and improve the effectiveness of search engine existing method model search satisfaction holistically at the search task level ignoring important dependency between action level satisfaction and overall task satisfaction we hypothesize that searcher latent action level satisfaction i e whether they believe they were satisfied with the result of a query or click influence their observed search behavior and contributes to overall search satisfaction we conjecture that by modeling search satisfaction at the action level we can build more complete and more accurate predictor of search task satisfaction to do this we develop a latent structural learning method whereby rich structured feature and dependency relation unique to search satisfaction prediction are explored using in situ search satisfaction judgment provided by searcher we show that there is significant value in modeling action level satisfaction in search task satisfaction prediction in addition experimental result on large scale log from bing com demonstrate clear benefit from using inferred action satisfaction label for other application such a document relevance estimation and query suggestion 
the statistical machine translation smt component of cross lingual information retrieval clir system is often regarded a black box that is optimized for translation quality independent from the retrieval task in recent work smt ha been tuned for retrieval by training a reranker on k best translation ordered according to their retrieval performance in this paper we propose a decomposable proxy for retrieval quality that obviates the need for costly intermediate retrieval furthermore we explore the full search space of the smt decoder by directly optimizing decoder parameter under a retrieval based objective experimental result for patent retrieval show our approach to be a promising alternative to the standard pipeline approach 
a query log is a key asset in a commercial search engine everyday million of user rely on search engine to find information on the web by entering a few keywords on a simple search interface those query represent a subset of user behavioral data which is used to mine and discover search pattern for improving the overall end user experience while query are very useful it is not always possible to capture precisely what the user wa looking for when the intent is not that clear we explore a different alternative based on human computation to gather a bit more information from user and show the type of query log that would be possible to construct 
hashtags have been widely used to annotate topic in tweet short post on twitter com in this paper we study the problem of real time prediction of bursting hashtags will a hashtag burst in the near future if it will how early can we predict it and how popular will it become based on empirical analysis of data collected from twitter we propose solution to these challenging problem the performance of different feature and possible solution are evaluated 
we address the problem of retrieving chess game position similar to a given query position from a collection of archived chess game we investigate this problem from an information retrieval ir perspective the advantage of our proposed ir based approach is that it allows using the standard inverted organization of stored chess position leading to an efficient retrieval moreover in contrast to retrieving exactly identical board position the ir based approach is able to provide approximate search functionality in order to define the similarity between two chess board position we encode each game state with a textual representation this textual encoding is designed to represent the position reachability and the connectivity between chess piece due to the absence of a standard ir dataset that can be used for this search task a new evaluation benchmark dataset wa constructed comprising of document chess position from a freely available chess game archive experiment conducted on this dataset demonstrate that our proposed method of similarity computation which take into account a combination of the mobility and the connectivity between the chess piece performs well on the search task achieving map and ndcg value of and respectively 
many domain specific search task are initiated by document length query e g patent invalidity search aim to find prior art related to a new query patent we call this type of search query document search in this type of search the initial query document is typically long and contains diverse aspect or sub topic user tend to issue many query based on the initial document to retrieve relevant document to help user in this situation we propose a method to suggest diverse query that can cover multiple aspect of the query document we first identify multiple query aspect and then provide diverse query suggestion that are effective for retrieving relevant document a well being related to more query aspect in the experiment we demonstrate that our approach is effective in comparison to previous query suggestion method 
the elia fano representation of monotone sequence ha been recently applied to the compression of inverted index showing excellent query performance thanks to it efficient random access and search operation while it space occupancy is competitive with some state of the art method such a gamma delta golomb code and pfordelta it fails to exploit the local clustering that inverted list usually exhibit namely the presence of long subsequence of close identifier in this paper we describe a new representation based on partitioning the list into chunk and encoding both the chunk and their endpoint with elia fano hence forming a two level data structure this partitioning enables the encoding to better adapt to the local statistic of the chunk thus exploiting clustering and improving compression we present two partition strategy respectively with fixed and variable length chunk for the latter case we introduce a linear time optimization algorithm which identifies the minimum space partition up to an arbitrarily small approximation factor we show that our partitioned elia fano index offer significantly better compression than plain elia fano while preserving their query time efficiency furthermore compared with other state of the art compressed encoding our index exhibit the best compression ratio query time trade off 
the development and evaluation of information retrieval and recommender system ha traditionally focused on the relevance and accuracy of retrieved document and recommendation respectively however there is an increasing realization that accuracy alone might be a sub optimal strategy for a successful user experience property such a novelty and diversity have been explored in both field for assessing and enhancing the usefulness of search result and recommendation in this doctoral research we study the assessment and enhancement of both property in the confluence of information retrieval and recommender system 
we address the task of recipient recommendation for emailing in enterprise we propose an intuitive and elegant way of modeling the task of recipient recommendation which us both the communication graph i e who are most closely connected to the sender and the content of the email additionally the model can incorporate evidence a prior probability experiment on two enterprise email collection show that our model achieves very high score and that it outperforms two variant that use either the communication graph or the content in isolation 
the semantics of mathematical formula depend on their spatial structure and they usually exist in layout presentation such a pdf latex and presentation mathml which challenge previous text index and retrieval method this paper proposes an innovative mathematics retrieval system along with the novel algorithm which enables efficient formula index and retrieval from both webpage and pdf document unlike prior study which require user to manually input formula markup language a query the new system enables user to copy formula query directly from pdf document furthermore by using a novel indexing and matching model the system is aimed at searching for similar mathematical formula based on both textual and spatial similarity a hierarchical generalization technique is proposed to generate sub tree from the semi operator tree of formula and support substructure match and fuzzy match experiment based on massive wikipedia and citeseer repository show that the new system along with novel algorithm comparing with two representative mathematics retrieval system provides more efficient mathematical formula index and retrieval while simplifying user query input for pdf document 
rating prediction is to predict the preference rating of a user to an item that she ha not rated before using the business review data from yelp in this paper we study business rating prediction a business here can be a restaurant a shopping mall or other kind of business different from most other type of item that have been studied in various recommender system e g movie song book a business physically exists at a geographical location and most business have geographical neighbor within walking distance when a user visit a business there is a good chance that she walk by it neighbor through data analysis we observe that there exists weak positive correlation between a business s rating and it neighbor rating regardless of the category of business based on this observation we assume that a user s rating to a business is determined by both the intrinsic characteristic of the business and the extrinsic characteristic of it geographical neighbor using the widely adopted latent factor model for rating prediction in our proposed solution we use two kind of latent factor to model a business one for it intrinsic characteristic and the other for it extrinsic characteristic the latter encodes the neighborhood influence of this business to it geographical neighbor in our experiment we show that by incorporating geographical neighborhood influence much lower prediction error is achieved than the state of the art model including biased mf svd and social mf the prediction error is further reduced by incorporating influence from business category and review content 
finding way to help user ass relevance when they search using math expression is critical for making mathematical information retrieval mir system easier to use we designed a study where participant completed search task involving mathematical expression using two different summary style and measured response time and relevance assessment accuracy the control summary style used google s regular hit formatting where expression are presented a text e g in latex while the second summary style render the math expression participant were undergraduate and graduate student participant in the rendered summary style n had on average a higher assessment accuracy than those in the non rendered summary style n with no significant difference in response time participant in the rendered condition reported having fewer problem reading hit than participant in the control condition this suggests that user will benefit from search engine that properly render math expression in their hit summary 
social tag are known to be a valuable source of information for image retrieval and organization however contrary to the conventional document retrieval rich tag frequency information in social sharing system such a flickr is not available thus we cannot directly use the tag frequency analogous to the term frequency in a document to represent the relevance of tag many heuristic approach have been proposed to address this problem among which the well known neighbor voting based approach are the most effective method the basic assumption of these method is that a tag is considered a relevant to the visual content of a target image if this tag is also used to annotate the visual neighbor image of the target image by lot of different user the main limitation of these approach is that they treat the voting power of each neighbor image either equally or simply based on it visual similarity in this paper we cast the social tag relevance learning problem a an adaptive teleportation random walk process on the voting graph in particular we model the relationship among image by constructing a voting graph and then propose an adaptive teleportation random walk in which a confidence factor is introduced to control the teleportation probability on the voting graph through this process direct and indirect relationship among image can be explored to cooperatively estimate the tag relevance to quantify the performance of our approach we compare it with state of the art method on two publicly available datasets nu wide and mir flickr the result indicate that our method achieves substantial performance gain on these datasets 
query auto completion qac facilitates faster user query input by predicting user intended query most qac algorithm take a learning based approach to incorporate various signal for query relevance prediction however such model are trained on simulated user input from query log data the lack of real user interaction data in the qac process prevents them from further improving the qac performance in this work for the first time we collect a high resolution qac query log that record every keystroke in a qac session based on this data we discover two user behavior namely the horizontal skipping bias and vertical position bias which are crucial for relevance prediction in qac in order to better explain them we propose a novel two dimensional click model for modeling the qac process with emphasis on these behavior extensive experiment on our qac data set from both pc and mobile device demonstrate that our proposed model can accurately explain the user behavior in interacting with a qac system and the resulting relevance model significant improves the qac performance over existing click model furthermore the learned knowledge about the skipping behavior can be effectively incorporated into existing learning based model to further improve their performance 
due to it low storage cost and fast query speed hashing ha been widely adopted for approximate nearest neighbor search in large scale datasets traditional hashing method try to learn the hash code in an unsupervised way where the metric euclidean structure of the training data is preserved very recently supervised hashing method which try to preserve the semantic structure constructed from the semantic label of the training point have exhibited higher accuracy than unsupervised method in this paper we propose a novel supervised hashing method called latent factor hashing lfh to learn similarity preserving binary code based on latent factor model an algorithm with convergence guarantee is proposed to learn the parameter of lfh furthermore a linear time variant with stochastic learning is proposed for training lfh on large scale datasets experimental result on two large datasets with semantic label show that lfh can achieve superior accuracy than state of the art method with comparable training time 
the fast growth of technology ha driven the advancement of our society it is often necessary to quickly grab the evolution of technology in order to better understand the technology trend the availability of huge volume of granted patent document provides a reasonable basis for analyzing technology evolution in this paper we propose a unified framework named patentline to generate a technology evolution tree for a given topic or a classification code related to granted patent the framework integrates different type of patent information including patent content citation of patent temporal relation etc and provides a concise yet comprehensive evolution summary the generated summary enables a variety of patent related analysis such a identifying relevant prior art and detecting technology gap a case study on a collection of u patent demonstrates the efficacy of our proposed framework 
hashing technique have been extensively investigated to boost similarity search for large scale high dimensional data most of the existing approach formulate the their objective a a pair wise similarity preserving problem in this paper we consider the hashing problem from the perspective of optimizing a list wise learning to rank problem and propose an approach called list wise supervised hashing lwh in lwh the hash function are optimized by employing structural svm in order to explicitly minimize the ranking loss of the whole list wise permutation instead of merely the point wise or pair wise supervision we evaluate the performance of lwh on two real world data set experimental result demonstrate that our method obtains a significant improvement over the state of the art hashing approach due to both structural large margin and list wise ranking pursuing in a supervised manner 
the purpose of this study is to investigate the extent to which two theory information scent and need for cognition explain people s search behavior when interacting with search engine result page serps information scent the perception of the value of information source wa manipulated by varying the number and distribution of relevant result on the first serp need for cognition nfc a personality trait that measure the extent to which a person enjoys cognitively effortful activity wa measured by a standardized scale a laboratory experiment wa conducted with forty eight participant who completed six open ended search task result showed that while interacting with serps containing more relevant document participant examined more document and clicked deeper in the search result list when interacting with serps that contained the same number of relevant result distributed across different rank participant were more likely to abandon their query when relevant document appeared later on the serp with respect to nfc participant with higher nfc paginated le frequently and paid le attention to result at lower rank than those with lower nfc the interaction between nfc and the number of relevant result on the serp affected the time spent on searching and a participant s likelihood to reformulate paginate and stop our finding suggest evaluating system effectiveness based on the first page of result even for task that require the user to view multiple document and varying interface feature based on nfc 
medical information is accessible from diverse source including the general web social medium journal article and hospital record information searcher can be patient and their family researcher practitioner and clinician challenge in medical information retrieval include diversity of user and user knowledge and expertise variation in the format reliability and quality of biomedical and medical information the multi modal nature of much of the data and the need for accuracy and reliability of medical information the aim of the workshop is to bring together researcher interested in medical information search with the goal of identifying specific challenge that need to be addressed to advance the state of the art 
million of people search the web each day a a consequence the ranking algorithm employed by web search engine have a profound influence on which page user visit characterizing this influence and informing user when different engine favor certain site or point of view enables more transparent access to the web s information we present paw a platform for analyzing difference among web search engine paw measure content emphasis the degree to which difference across search engine ranking correlate with feature of the ranked content including point of view e g positive or negative orientation toward their company s product and advertisement we propose an approach for identifying the orientation in search result at scale through a novel technique that minimizes the expected number of human judgment required we apply paw to news search on google and bing and find no evidence that the engine emphasize result that express positive orientation toward the engine company s product we do find that the engine emphasize particular news site and that they also favor page containing their company s advertisement a opposed to competitor advertisement 
entity are centric to a large number of real world application wikipedia show entity infoboxes for a large number of entity however not much structured information is available about character entity in book automatic discovery of character from book can help in effective summarization such a structured summary which not just introduces character in the book but also provides a high level relationship between them can be of critical importance for buyer this task involves the following challenging novel problem automatic discovery of important character given a book automatic social graph construction relating the discovered character automatic summarization of text most related to each of the character and automatic infobox extraction from such summarized text for each character a part of this demo we design mechanism to address these challenge and experiment with publicly available book 
abstract using a novel evaluation toolkit that simulates a human reviewer in the loop we compare the effectiveness of three machine learning protocol for technology assisted review a used in document review for discovery in legal proceeding our comparison address a central question in the deployment of technology assisted review should training document be selected at random or should they be selected using one or more non random method such a keyword search or active learning on eight review task four derived from the trec legal track and four derived from actual legal matter recall wa measured a a function of human review effort the result show that entirely non random training method in which the initial training document are selected using a simple keyword search and subsequent training document are selected by active learning require substantially and significantly le human review effort p 
one of the main target of any search engine is to make every user fully satisfied with her search result for this reason lot of effort are being paid to improving ranking model in order to show the best result to user however there is a class of document on the web which can spoil all effort being shown to the user when user receive result which are not only irrelevant but also completely out of the picture of their expectation they can get really frustrated so we attempted to find a method to determine such document and reduce their negative impact upon user and a a consequence on search engine in general 
finding relevant web service and composing them into value added application is becoming increasingly important in cloud and service based marketplace the key problem with current approach to finding relevant web service is that most of them only provide search over a discrete set of feature using exact keyword matching we demonstrate in this paper that by utilizing well known indexing scheme such a inverted file and r tree index over web service attribute the earth mover s distance emd algorithm can be used efficiently to find partial match between a query and a database of web service 
supervised text classifier require extensive human expertise and labeling effort in this paper we propose a weakly supervised text classification algorithm based on the labeling of latent dirichlet allocation lda topic our algorithm is based on the generative property of lda in our algorithm we ask an annotator to assign one or more class label to each topic based on it most probable word we classify a document based on it posterior topic proportion and the class label of the topic we also enhance our approach by incorporating domain knowledge in the form of labeled word we evaluate our approach on four real world text classification datasets the result show that our approach is more accurate in comparison to semi supervised technique from previous work a central contribution of this work is an approach that delivers effectiveness comparable to the state of the art supervised technique in hard to classify domain with very low overhead in term of manual knowledge engineering 
collaborative filtering cf based recommendation algorithm such a latent factor model lfm work well in term of prediction accuracy however the latent feature make it difficulty to explain the recommendation result to the user fortunately with the continuous growth of online user review the information available for training a recommender system is no longer limited to just numerical star rating or user item feature by extracting explicit user opinion about various aspect of a product from the review it is possible to learn more detail about what aspect a user care which further shed light on the possibility to make explainable recommendation in this work we propose the explicit factor model efm to generate explainable recommendation meanwhile keep a high prediction accuracy we first extract explicit product feature i e aspect and user opinion by phrase level sentiment analysis on user review then generate both recommendation and disrecommendations according to the specific product feature to the user s interest and the hidden feature learned besides intuitional feature level explanation about why an item is or is not recommended are generated from the model offline experimental result on several real world datasets demonstrate the advantage of our framework over competitive baseline algorithm on both rating prediction and top k recommendation task online experiment show that the detailed explanation make the recommendation and disrecommendations more influential on user s purchasing behavior 
measurement are fundamental to any empirical science and similarly search evaluation is a vital part of information retrieval ir evaluation ensures the progressive development of approach tool and method studied in this field apart from the scientific perspective the evaluation approach are also important from the practical perspective indeed the evaluation experiment enable commercial search engine to make data driven decision while developing new feature and working on the quality of the user experience thus it is not surprising that evaluation ha gained a huge attention from the research community and such an interest span almost fifty year of research the cranfield experiment evolved into the widely used offline system evaluation approach despite it convenience and popularity the offline evaluation approach ha several limitation these limitation resulted in the development and recent growth in popularity of the online user based evaluation approach such a interleaving and a b testing 
with recent advance in radio frequency identification rfid wireless sensor network and web based service physical thing are becoming an integral part of the emerging ubiquitous web in this paper we focus on the thing recommendation problem in internet of thing iot in particular we propose a unified probabilistic based framework by fusing information across relationship between user i e user social network and thing i e thing correlation to make more accurate recommendation the proposed approach not only inherits the advantage of the matrix factorization but also exploit the merit of social relationship and thing thing correlation we validate our approach based on an internet of thing platform and the experimental result demonstrate it feasibility and effectiveness 
many technique in information retrieval produce count from a sample and it is common to analyse these count a proportion of the whole term frequency are a familiar example proportion carry only relative information and are not free to vary independently of one another for the proportion of one term to increase one or more others must decrease these constraint are hallmark of compositional data while there ha long been discussion in other field of how such data should be analysed to our knowledge compositional data analysis coda ha not been considered in ir in this work we explore compositional data in ir through the lens of distance measure and demonstrate that common measure naive to composition have some undesirable property which can be avoided with composition aware measure a a practical example these measure are shown to improve clustering 
this paper describes an advanced search engine that support user in querying document by mean of keywords entity and category user simply type word which are automatically mapped onto appropriate suggestion for entity and category based on named entity disambiguation the search engine return document containing the query s entity and prominent entity from the query s category 
similarity search method based on hashing for effective and efficient cross modal retrieval on large scale multimedia database with massive text and image have attracted considerable attention the core problem of cross modal hashing is how to effectively construct correlation between multi modal representation which are heterogeneous intrinsically in the process of hash function learning analogous to canonical correlation analysis cca most existing cross modal hash method embed the heterogeneous data into a joint abstraction space by linear projection however these method fail to bridge the semantic gap more effectively and capture high level latent semantic information which ha been proved that it can lead to better performance for image retrieval to address these challenge in this paper we propose a novel latent semantic sparse hashing lssh to perform cross modal similarity search by employing sparse coding and matrix factorization in particular lssh us sparse coding to capture the salient structure of image and matrix factorization to learn the latent concept from text then the learned latent semantic feature are mapped to a joint abstraction space moreover an iterative strategy is applied to derive optimal solution efficiently and it help lssh to explore the correlation between multi modal representation efficiently and automatically finally the unified hashcodes are generated through the high level abstraction space by quantization extensive experiment on three different datasets highlight the advantage of our method under cross modal scenario and show that lssh significantly outperforms several state of the art method 
a common problem in unstructured peer to peer p p information retrieval is the need to compute global statistic of the full collection when only a small subset of the collection is visible to a peer without accurate estimate of these statistic the effectiveness of modern retrieval model can be reduced we show that for the case of a probably approximately correct p p architecture and using either the bm retrieval model or a language model with dirichlet smoothing very close approximation of the required global statistic can be estimated with very little overhead and a small extension to the protocol however through theoretical modeling and simulation we demonstrate this technique also greatly increase the ability for adversarial peer to manipulate search result we show an adversary controlling fewer than of peer can censor or increase the rank of document or disrupt overall search result a a defense we propose a simple modification to the extension and show global statistic estimation is viable even when up to of peer are adversarial 
cripts e g arabic greek and indic language one can often find a large amount of user generated transliterated content on the web in the roman script such content creates a monolingual or cross lingual space with more than one script which is referred a mixed script space and information retrieval in this space is referred a mixed script information retrieval msir in mixed script space the document and query may either be in the native script and or the roman transliterated script for a language mono lingual scenario there can be further extension of msir such a multi lingual msir in which term can be in multiple script in multiple language since there are no standard way of spelling a word in a non native script transliteration content almost always feature extensive spelling variation this phenomenon present a non trivial term matching problem for search engine to match the native script or roman transliterated query with the document in multiple script taking into account the spelling variation this problem although prevalent inweb search for user of many language around the world ha received very little attention till date very recently we have formally defined the problem of msir and presented the quantitative study on it through bing query log analysis 
recommender system usually need to compare a large number of item before user most preferred one can be found this process can be very costly if recommendation are frequently made on large scale datasets in this paper a novel hashing algorithm named preference preserving hashing pph is proposed to speed up recommendation hashing ha been widely utilized in large scale similarity search e g similar image search and the search speed with binary hashing code is significantly faster than that with real valued feature however one challenge of applying hashing to recommendation is that recommendation concern user preference over item rather than their similarity to address this challenge pph contains two novel component that work with the popular matrix factorization mf algorithm in mf user preference over item are calculated a the inner product between the learned real valued user item feature the first component of pph constrains the learning process so that user preference can be well approximated by user item similarity the second component which is a novel quantization algorithm generates the binary hashing code from the learned real valued user item feature finally recommendation can be achieved efficiently via fast hashing code search experiment on three real world datasets show that the recommendation speed of the proposed pph algorithm can be hundred of time faster than original mf with real valued feature and the recommendation accuracy is significantly better than previous work of hashing for recommendation 
cold start is one of the most challenging problem in recommender system in this paper we tackle the cold start problem by proposing a context aware semi supervised co training method named csel specifically we use a factorization model to capture fine grained user item context then in order to build a model that is able to boost the recommendation performance by leveraging the context we propose a semi supervised ensemble learning algorithm the algorithm construct different weak prediction model using example with different context and then employ the co training strategy to allow each weak prediction model to learn from the other prediction model the method ha several distinguished advantage over the standard recommendation method for addressing the cold start problem first it defines a fine grained context that is more accurate for modeling the user item preference second the method can naturally support supervised learning and semi supervised learning which provides a flexible way to incorporate the unlabeled data the proposed algorithm are evaluated on two real world datasets the experimental result show that with our method the recommendation accuracy is significantly improved compared to the standard algorithm and the cold start problem is largely alleviated 
traditionally the efficiency and effectiveness of search system have both been of great interest to the information retrieval community however an in depth analysis on the interplay between the response latency of web search system and user search experience ha been missing so far in order to fill this gap we conduct two separate study aiming to reveal how response latency affect the user behavior in web search first we conduct a controlled user study trying to understand how user perceive the response latency of a search system and how sensitive they are to increasing delay in response this study reveals that when artificial delay are introduced into the response the user of a fast search system are more likely to notice these delay than the user of a slow search system the introduced delay become noticeable by the user once they exceed a certain threshold value second we perform an analysis using a large scale query log obtained from yahoo web search to observe the potential impact of increasing response latency on the click behavior of user this analysis demonstrates that latency ha an impact on the click behavior of user to some extent in particular given two content wise identical search result page we show that the user are more likely to perform click on the result page that is served with lower latency 
previous study of online user attention during information seeking task have mainly focused on analyzing searcher behavior in the web search setting while these study enabled better understanding of search result examination their finding might not generalize for the task and search interface in other domain such a shopping or social medium in this paper we present to best of our knowledge the first cross domain comparison of search examination behavior and pattern of aggregated attention across web search news shopping and social network domain we investigate how domain of the search and the scope of the information need affect search examination and find significant difference beyond those arising from natural disparity between individual for example we find that the mean fixation duration a common indicator of cognitive load varies significantly across domain e g mean fixation duration in the social network domain exceeds that of general web search by over we also find large difference in the aggregate pattern of user attention on the screen especially in the shopping and social network domain compared to the web search domain emphasizing the need for domain specific user model and evaluation metric 
we examine the spatial keyword search problem to retrieve object of interest that are ranked based on both their spatial proximity to the query location a well a the textual relevance of the object s keywords existing solution for the problem are based on either using a combination of textual and spatial index or using specialized hybrid index that integrate the indexing of both textual and spatial attribute value in this paper we propose a new approach that is based on modeling the problem a a top k aggregation problem which enables the design of a scalable and efficient solution that is based on the ubiquitous inverted list index our performance study demonstrates that our approach outperforms the state of the art hybrid method by a wide margin 
it is rare for a new user interface to break through and become successful especially in information intensive task like search coming to consensus or building up knowledge most complex interface end up going unused often the successful solution lie in a previously unexplored part of the interface design space that is simple in a new way that work just right in this talk i will give example of such success in the information intensive interface design space and attempt to provide stimulating idea for future research direction 
a large number of mainstream application like temporal search event detection and trend identification assume knowledge of the timestamp of every document in a given textual collection in many case however the required timestamps are either unavailable or ambiguous a characteristic instance of this problem emerges in the context of large repository of old digitized document for such document the timestamp may be corrupted during the digitization process or may simply be unavailable in this paper we study the task of approximating the timestamp of a document so called document dating we propose a contentbased method and use recent advance in the domain of term burstiness which allow it to overcome the drawback of previous document dating method e g the fix time partition strategy we use an extensive experimental evaluation on different datasets to validate the efficacy and advantage of our methodology showing that our method outperforms the state of the art method on document dating 
recommender system ha become an important component in modern ecommerce recent research on recommender system ha been mainly concentrating on improving the relevance or profitability of individual recommended item but in reality user are usually exposed to a set of item and they may buy multiple item in one single order thus the relevance or profitability of one item may actually depend on the other item in the set in other word the set of recommendation is a bundle with item interacting with each other in this paper we introduce a novel problem called the bundle recommendation problem brp by solving the brp we are able to find the optimal bundle of item to recommend with respect to preferred business objective however brp is a large scale np hard problem we then show that it may be sufficient to solve a significantly smaller version of brp depending on property of input data this allows u to solve brp in real world application with million of user and item both offline and online experimental result on a walmart com demonstrate the incremental value of solving brp across multiple baseline model 
we present a generic method for augmenting unsupervised query segmentation by incorporating part of speech po sequence information to detect meaningful but rare n gram our initial experiment with an existing english po tagger employing two different po tagsets and an unsupervised po induction technique specifically adapted for query show that po information can significantly improve query segmentation performance in all these case 
we propose a family of new evaluation measure called markov precision mp which exploit continuous time and discrete time markov chain in order to inject user model into precision continuous time mp behaves like time calibrated measure bringing the time spent by the user into the evaluation of a system discrete time mp behaves like traditional evaluation measure being part of the same markovian framework the time based and rank based version of mp produce value that are directly comparable we show that it is possible to re create average precision using specific user model and this help in providing an explanation of average precision ap in term of user model more realistic than the one currently used to justify it we also propose several alternative model that take into account different possible behavior in scanning a ranked result list finally we conduct a thorough experimental evaluation of mp on standard trec collection in order to show that mp is a reliable a other measure and we provide an example of calibration of it time parameter based on click log from yandex 
adequacy of citation is very important for a scientific paper however it is not an easy job to find appropriate citation for a given context especially for citation in different language in this paper we define a novel task of cross language context aware citation recommendation which aim at recommending english citation for a given context of the place where a citation is made in a chinese paper this task is very challenging because the context and citation are written in different language and there exists a language gap when matching them to tackle this problem we propose the bilingual context citation embedding algorithm i e blsrec i which can learn a low dimensional joint embedding space for both context and citation moreover two advanced algorithm named blsrec ii and blsrec iii are proposed by enhancing blsrec i with translation result and abstract information respectively we evaluate the proposed method based on a real dataset that contains chinese context and english citation the result demonstrate that our proposed algorithm can outperform a few baseline and the blsrec ii and blsrec iii method can outperform the blsrec i method 
we present a study of the correlation between the extent to which the cluster hypothesis hold a measured by various test and the relative effectiveness of cluster based retrieval with respect to document based retrieval we show that the correlation can be affected by several factor such a the size of the result list of the most highly ranked document that is analyzed we further show that some cluster hypothesis test are often negatively correlated with one another moreover in several setting some of the test are also negatively correlated with the relative effectiveness of cluster based retrieval 
a person often us a single search engine for very different task for example an author editing a manuscript may use the same academic search engine to find the latest work on a particular topic or to find the correct citation for a familiar article the author s tolerance for latency and accuracy may vary according to task however search engine typically employ a consistent approach for processing all query in this paper we explore how a range of search need and expectation can be supported within a single search system using differential search we introduce citesight a system that provides personalized citation recommendation to author group that vary based on task citesight present cached recommendation instantaneously for online task e g active paper writing and refines these recommendation in the background for offline task e g future literature review we develop an active cache warming process to enhance the system a the author work and context coupling a technique for augment sparse citation network by evaluating the quality of the recommendation and collecting user feedback we show that differential search can provide a high level of accuracy for different task on different time scale we believe that differential search can be used in many situation where the user s tolerance for latency and desired response vary dramatically based on use 
in recent year researcher have investigated search result diversification through a variety of approach in such situation information retrieval system need to consider both aspect of relevance and diversity for those retrieved document on the other hand previous research ha demonstrated that data fusion is useful for improving performance when we are only concerned with relevance however it is not clear if it help when both relevance and diversity are both taken into consideration in this short paper we propose a few data fusion method to try to improve performance when both relevance and diversity are concerned experiment are carried out with group of top ranked result submitted to the trec web diversity task we find that data fusion is still a useful approach to performance improvement for diversity a for relevance previously 
all research project begin with a goal for instance to describe search behavior to predict when a person will enter a second query or to discover which ir system performs the best different research goal suggest different research approach ranging from field study to lab study to online experimentation this tutorial will provide an overview of the different type of research goal common evaluation approach used to address each type and the constraint each approach entail participant will come away with a broad perspective of research goal and approach in ir and an understanding of the benefit and limitation of these research approach the tutorial will take place in two independent but interrelated part each focusing on a unique set of research approach but with the same intended tutorial outcome these outcome will be accomplished by deconstructing and analyzing our own published research paper with further illustration of each technique using the broader literature by using our own research a anchor we will provide insight about the research process revealing the difficult choice and trade offs researcher make when designing and conducting ir study 
current approach for contextual sentiment lexicon construction in phrase level sentiment analysis assume that the numerical star rating of a review represents the overall sentiment orientation of the review text although widely adopted we find through user rating analysis that this is not necessarily true in this paper we attempt to bridge the gap between phrase level and review document level sentiment analysis by leveraging the result given by review level sentiment classification to boost phrase level sentiment polarity labeling in contextual sentiment lexicon construction task using a novel constrained convex optimization framework experimental result on both english and chinese review show that our framework improves the precision of sentiment polarity labeling by up to which is a significant improvement from current approach 
social medium such a twitter ha come to reflect the reaction of the general public to major event since post are short and noisy it is hard to extract reliable event based on word frequency even though an event term appears in a particularly low frequency a long a at least one reliable user mention the term it should be extracted this paper proposes an event extraction method which combine user reliability and timeline analysis the latent dirichlet allocation lda topic model is adapted with the weight of event term on timeline and reliable user to extract social event the reliable user are detected on twitter according to their tweeting behavior socially well known user and active user reliable and low frequency event can be detected based on reliable user in order to see the effectiveness of the proposed method experiment are conducted on a korean tweet collection the proposed model achieved in precision this show that the lda with timeline and reliable user is effective for extracting event on the twitter test collection 
web search engine are optimized to reduce the high percentile response time to consistently provide fast response to almost all user query this is a challenging task because the query workload exhibit large variability consisting of many short running query and a few long running query that significantly impact the high percentile response time with modern multicore server parallelizing the processing of an individual query is a promising solution to reduce query execution time but it give limited benefit compared to sequential execution since most query see little or no speedup when parallelized the root of this problem is that short running query which dominate the workload do not benefit from parallelization they incur a large parallelization overhead taking scarce resource from long running query on the other hand parallelization substantially reduces the execution time of long running query with low overhead and high parallelization efficiency motivated by these observation we propose a predictive parallelization framework with two part predicting long running query and selectively parallelizing them for the first part prediction should be accurate and efficient for accuracy we study a comprehensive feature set covering both term feature reflecting dynamic pruning efficiency and query feature reflecting query complexity for efficiency to keep overhead low we avoid expensive feature that have excessive requirement such a large memory footprint for the second part we use the predicted query execution time to parallelize long running query and process short running query sequentially we implement and evaluate the predictive parallelization framework in microsoft bing search our measurement show that under moderate to heavy load the predictive strategy reduces the th percentile response time by from m to m compared with prior approach that parallelize all query 
presence of hyperlink in a tweet is a strong indication of tweet being more informative in this paper we study the problem of hashtag recommendation for hyperlinked tweet i e tweet containing link to web page by recommending hashtags to hyperlinked tweet we argue that the function of hashtags such a providing the right context to interpret the tweet tweet categorization and tweet promotion can be extended to the linked document the proposed solution for hashtag recommendation consists of two phase in the first phase we select candidate hashtags through five scheme by considering the similar tweet the similar document the named entity contained in the document and the domain of the link in the second phase we formulate the hashtag recommendation problem a a learning to rank problem and adopt ranksvm to aggregate and rank the candidate hashtags our experiment on a collection of million tweet show that the proposed solution achieves promising result 
personal expertise or interest often evolve over time despite much work on expertise retrieval in the recent year very little work ha studied the dynamic of personal expertise in this paper we propose a probabilistic model to characterize how people change or stick with their expertise specifically three factor are taken into consideration in whether an expert will choose a new expertise area the personality of the expert in exploring new area the similarity between the new area and the expert s current area the popularity of the new area these three factor are integrated into a unified generative process a predictive language model is derived to estimate the distribution of the expert s word in her future publication in addition kl divergence is defined on the predictive language model to quantify and forecast the change of expertise we conduct the experiment on a testbed of academic publication and the initial result demonstrate the effectiveness of the proposed approach 
authorship attribution aa aim to identify the author of a set of document traditional study in this area often assume that there are a large set of labeled document available for training however in the real life it is hard or expensive to collect a large set of labeled data for example in the online review domain most reviewer author only write a few review which are not enough to serve a the training data for accurate classification in this paper we present a novel two view co training framework to iteratively identify the author of a few unlabeled data to augment the training set the key idea is to first represent each document a several distinct view and then a co training technique is adopted to exploit the large amount of unlabeled document starting from training text per author we systematically evaluate the effectiveness of co training for authorship attribution with limited labeled data two method and three view are investigated logistic regression lr and support vector machine svm method and character lexical and syntactic view the experimental result show that lr is particularly effective for improving co training in aa and the lexical view performs the best among three view when combined with a lr classifier furthermore the co training framework doe not make much difference between one classifier from two view and two classifier from one view instead it is the learning approach and the view that play a critical role 
query performance prediction qpp is the estimation of the retrieval success for a query without explicit knowledge about relevant document qpp is especially interesting in the context of automatic query expansion aqe based on pseudo relevance feedback prf prf based aqe is known to produce unreliable result when the initial set of retrieved document is poor theoretically a good predictor would allow to selectively apply prf based aqe when performance of the initial result set is good enough thus enhancing the overall robustness of the system qpp would be of great benefit in the context of microblog retrieval a aqe wa the most widely deployed technique for enhancing retrieval performance at trec in this work we study the performance of the state of the art predictor under microblog retrieval condition a well a introducing our own predictor our result show how our proposed predictor outperform the baseline significantly 
a popular strategy for search result diversification is to first retrieve a set of document utilizing a standard retrieval method and then rerank the result we adopt a different perspective on the problem based on data fusion starting from the hypothesis that data fusion can improve performance in term of diversity metric we examine the impact of standard data fusion method on result diversification we take the output of a set of ranker optimized for diversity or not and find that data fusion can significantly improve state of the art diversification method we also introduce a new data fusion method called diversified data fusion which infers latent topic of a query using topic modeling without leveraging outside information our experiment show that data fusion method can enhance the performance of diversification and ddf significantly outperforms existing data fusion method in term of diversity metric 
user frequently interact with web search system on their mobile device via multiple modality including touch and speech these interaction mode are substantially different from the user experience on desktop search a a result system designer have new challenge and question around understanding the intent on these platform in this paper we study the query reformulation pattern in mobile log we group query reformulations based on their input method into four category text text text voice voice text and voice voice we discus the unique characteristic of each of these group by comparing them against each other and desktop log we also compare the distribution of reformulation type e g adding dropping word against desktop log and show that there are new class of reformulations that are caused by error in speech recognition our result suggest that user do not tend to switch between different input type e g voice and text voice to text switch are largely caused by speech recognition error and text to voice switch are unlikely to be about the same intent 
relevation is a system for performing relevance judgement for information retrieval evaluation relevation is web based fully configurable and expandable it allows researcher to effectively collect assessment and additional qualitative data the system is easily deployed allowing assessor to smoothly perform their relevance judging task even remotely relevation is available a an open source project at http ielab github io relevation 
entity centric document filtering is the task of analyzing a time ordered stream of document and emitting those that are relevant to a specified set of entity e g people place organization this task is exemplified by the trec knowledge base acceleration kba track and ha broad applicability in other modern ir setting in this paper we present a simple yet effective approach based on learning high quality boolean query that can be applied deterministically during filtering we call these boolean statement sufficient query we argue that using deterministic query for entity centric filtering can reduce confounding factor seen in more familiar score then threshold filtering method experiment on two standard datasets show significant improvement over state of the art baseline model 
with the rise in popularity of smart phone there ha been a recent increase in the number of image taken at large social e g festival and world e g natural disaster event which are uploaded to image sharing website such a flickr a with all online image they are often poorly annotated resulting in a difficult retrieval scenario to overcome this problem many photo tag recommendation method have been introduced however these method all rely on historical flickr data which is often problematic for a number of reason including the time lag problem i e in our collection user upload image on average day after taking them meaning training data is often out of date in this paper we develop an image annotation model which exploit textual content from related twitter and wikipedia data which aim to overcome the discussed problem the result of our experiment show and highlight the merit of exploiting social medium data for annotating event image where we are able to achieve recommendation accuracy comparable with a state of the art model 
this study investigated query formulation by user with it cognitive search intent csis which are user need for the cognitive characteristic of document to be retrieved em e g comprehensibility subjectivity and concreteness our four main contribution are summarized a follows i we proposed an example based method of specifying search intent to observe query formulation by user without biasing them by presenting a verbalized task description ii we conducted a questionnaire based user study and found that about half our subject did not input any keywords representing csis even though they were conscious of csis iii our user study also revealed that over of subject occasionally had experience with search with csis while our evaluation demonstrated that the performance of a current web search engine wa much lower when we not only considered user topical search intent but also csis and iv we demonstrated that a machine learning based query expansion could improve the performance for some type of csis our finding suggest user over adapt to current web search engine and create opportunity to estimate csis with non verbal user input 
online review are immensely valuable for customer to make informed purchase decision and for business to improve the quality of their product and service however customer review grow exponentially while varying greatly in quality it is generally very tedious and difficult if not impossible for user to read though the huge amount of review data fortunately review quality evaluation enables a system to select the most helpful review for user decision making previous study predict only the overall review utility about a product and often focus on developing different data feature to learn a quality function for addressing the problem in this paper we aim to select the most helpful review not only at the product level but also at a fine grained product aspect level we propose a novel supervised joint aspect and sentiment model sjasm which is a probabilistic topic modeling framework that jointly discovers aspect and sentiment guided by a review helpfulness metric one key advantage of sjasm is it ability to infer the underlying aspect and sentiment which are indicative of the helpfulness of a review we validate sjasm using publicly available review data and our experimental result demonstrate the superiority of sjasm over several competing model 
evaluation is a fundamental part of information retrieval and in the conventional cranfield evaluation paradigm set of relevance assessment are a fundamental part of test collection this workshop revisits how relevance assessment can be efficiently created seeking to provide a forum for discussion and exploration of the topic 
a large number of image are continuously uploaded to popular photo sharing website and online social community in this demonstration we show a novel application which automatically classifies image in a live photo stream according to their attractiveness for the community based on a number of visual and textual feature the system effectively introduces an additional facet to browse and explore photo collection by highlighting the most attractive photograph and demoting the least attractive 
people often use more than one query when searching for information they revisit search result to re find information and build an understanding of their search need through iterative exploration of query formulation these task are not well supported by search interface and web browser we designed and built searchpanel a chrome browser extension that support people in their ongoing information seeking this extension combine document and process metadata into an interactive representation of the retrieved document that can be used for sense making navigation and re finding document in a real world deployment spanning over two month result show that searchpanel appears to have been primarily used for complex information need in search session with long duration and high number of query when process metadata wa present in the ui searcher in explorative search session submitted more and longer query and interacted more with the serp these result indicate that the process metadata feature in searchpanel seem to be of particular importance for exploratory search 
with the rapid expansion of online social network social network based recommendation ha become a meaningful and effective way of suggesting new item or activity to user in this paper we propose two method to improve the performance of the state of art social network based recommender system snrs which is based on a probabilistic model our first method classifies the correlation between pair of user rating the other is making the system robust to sparse data i e few immediate friend having few common rating with the target user our experimental study demonstrates that our technique significantly improve the accuracy of snrs 
compression of collection such a text database can both reduce space consumption and increase retrieval efficiency through better caching and better exploitation of the memory hierarchy a promising technique is relative lempel ziv coding in which a sample of material from the collection serf a a static dictionary in previous work this method demonstrated extremely fast decoding and good compression ratio while allowing random access to individual item however there is a trade off between dictionary size and compression ratio motivating the search for a compact yet similarly effective dictionary in previous work it wa observed that since the dictionary is generated by sampling some of it selected substring may be discarded with little loss in compression unfortunately simple dictionary pruning approach are ineffective we develop a formal model of our approach based on generating an optimal dictionary for a given collection within a memory bound we generate measure for identification of low value substring in the dictionary and show on a variety of size of text collection that halving the dictionary size lead to only marginal loss in compression ratio this is a dramatic improvement on previous approach 
the query performance prediction task is to estimate retrieval effectiveness with no relevance judgment pre retrieval prediction method operate prior to retrieval time hence these predictor are often based on analyzing the query and the corpus upon which retrieval is performed we propose a em corpus independent approach to pre retrieval prediction which relies on information extracted from wikipedia specifically we present wikipedia based feature that can attest to the effectiveness of retrieval performed in response to a query em regardless of the corpus upon which search is performed empirical evaluation demonstrates the merit of our approach a a case in point integrating the wikipedia based feature with state of the art pre retrieval predictor that analyze the corpus yield prediction quality that is consistently better than that of using the latter alone 
trending search suggestion is leading a new paradigm of image search where user s exploratory search experience is facilitated with the automatic suggestion of trending query existing image search engine however only provide general suggestion and hence cannot capture user s personal interest in this paper we move one step forward to investigate personalized suggestion of trending image search according to user search behavior to this end we propose a learning based framework including two novel component the first component i e trending aware weight regularized matrix factorization ta wrmf is able to suggest personalized trending search query by learning user preference from many user a well a auxiliary common search the second component associate the most representative and trending image with each suggested query the personalized suggestion of image search consists of a trending textual query and it associated trending image the combined textual visual query not only are trending bursty and personalized to user s search preference but also provide the compelling visual aspect of these query we evaluate our proposed learning based framework on a large scale search log with million user and million query in two week from a commercial image search engine the evaluation demonstrate that our system achieve about gain compared with state of the art in term of query prediction accuracy 
recently some recommendation method try to relieve the data sparsity problem of collaborative filtering by exploiting data from user multiple type of behavior however most of the exist method mainly consider to model the correlation between different behavior and ignore the heterogeneity of them which may make improper information transferred and harm the recommendation result to address this problem we propose a novel recommendation model named group latent factor model glfm which attempt to learn a factorization of latent factor space into subspace that are shared across multiple behavior and subspace that are specific to each type of behavior thus the correlation and heterogeneity of multiple behavior can be modeled by these shared and specific latent factor experiment on the real world dataset demonstrate that our model can integrate user multiple type of behavior into recommendation better 
in this study we analyze an educational search engine log for shedding light on k student search behavior in a learning environment we specially focus on query session user and click characteristic and compare the trend to the finding in the literature for general web search engine our analysis help understanding how student search with the purpose of learning in an educational vertical and reveals new direction to improve the search performance in the education domain 
click dwell time is the amount of time that a user spends on a clicked search result many previous study have shown that click dwell time is strongly correlated with result level satisfaction and document relevance accurate estimate of dwell time are therefore important for application such a search satisfaction prediction and result ranking however dwell time can be estimated in different way according to the information available about the search process for example a result reached for the query garfield may involve s of server side dwell time observable to the search engine and s of client side dwell time observable from the browser since search engine can only observe server side action i e activity on the search engine result page server side dwell time are estimated by measuring the time between a search result click and the next search event click or query conversely more detailed information about page dwell time can be obtained via client side method such a web browser toolbars the client side information enables the estimation of more accurate dwell time by measuring the amount of time that a user spends on page of interest either the landing page or page on the full navigation trail in this paper we define three different dwell time i e server side client side and trail dwell time and examine their effectiveness for predicting click satisfaction for this we collect toolbar and search engine log from real user and provide an analysis of dwell time for improving prediction performance moreover we show further improvement in predicting click level satisfaction by combining dwell time with other query feature e g query clarity 
this paper examines the space time performance of in memory conjunctive list intersection algorithm a used in search engine where integer represent document identifier we demonstrate that the combination of bitvectors large skip delta compressed list and url ordering produce superior result to using skip or bitvectors alone we define semi bitvectors a new partial bitvector data structure that store the front of the list using a bitvector and the remainder using skip and delta compression to make it particularly effective we propose that document be ordered so a to skew the posting list to have dense region at the front this can be accomplished by grouping document by their size in a descending manner and then reordering within each group using url ordering in each list the division point between bitvector and delta compression can occur at any group boundary we explore the performance of semi bitvectors using the gov dataset for various number of group resulting in significant space time improvement over existing approach semi bitvectors do not directly support ranking indeed bitvectors are not believed to be useful for ranking based search system because frequency and offset cannot be included in their structure to refute this belief we propose several approach to improve the performance of ranking based search system using bitvectors and leave their verification for future work these proposal suggest that bitvectors and more particularly semi bitvectors warrant closer examination by the research community 
web search engine utilize behavioral signal to develop search experience tailored to individual user to be effective such personalization relies on access to sufficient information about each user s interest and intention for new user or new query profile information may be sparse or non existent to handle these case and perhaps also improve personalization for those with profile search engine can employ signal from user who are similar along one or more dimension i e those in the same cohort in this paper we describe a characterization and evaluation of the use of such cohort modeling to enhance search personalization we experiment with three pre defined cohort topic location and top level domain preference independently and in combination and also evaluate method to learn cohort dynamically we show via extensive experimentation with large scale log from a commercial search engine that leveraging cohort behavior can yield significant relevance gain when combined with a production search engine ranking algorithm that us similar class of personalization signal but at the individual searcher level additional experiment show that our gain can be extended when we dynamically learn cohort and target easily identifiable class of ambiguous or unseen query 
unlike in general recommendation scenario where a user ha only a single role user in trust rating network e g epinions are associated with two different role simultaneously a a truster and a a trustee with different role user can show distinct preference for rating item which the previous approach do not involve moreover based on explicit single link between two user existing method can not capture the implicit correlation between two user who are similar but not socially connected in this paper we propose to learn dual role preference truster trustee specific preference for trust aware recommendation by modeling explicit interaction e g rating and trust and implicit interaction in particular local link structure of trust network are exploited a two regularization term to capture the implicit user correlation in term of truster trustee specific preference using a real world and open dataset we conduct a comprehensive experimental study to investigate the performance of the proposed model rorec the result show that rorec outperforms other trust aware recommendation approach in term of prediction accuracy 
this paper present expertime a web based system for tracking expertise over time we visualize a person s expertise profile on a timeline where we detect and characterize change in the focus or topic of expertise it is possible to zoom in on a given time period in order to examine the underlying data that is used a supporting evidence it is also possible to perform visual and quantitative comparison of two arbitrarily selected time period in a highly interactive environment we invite profile owner to evaluate and fine tune their profile and to leave feedback 
recommender system using collaborative filtering technique are capable of make personalized prediction however these system are highly vulnerable to profile injection attack group attack are attack that target a group of item instead of one and there are common attribute among these item such profile will have a good probability of being similar to a large number of user profile making them hard to detect we propose a novel technique for identifying group attack profile which us an improved metric based on degree of similarity with top neighbor degsim and rating deviation from mean agreement rdma we also extend our work with a detailed analysis of target item rating pattern experiment show that the combined method can improve detection rate in user based recommender system 
hierarchical multi label classification assigns a document to multiple hierarchical class in this paper we focus on hierarchical multi label classification of social text stream concept drift complicated relation among class and the limited length of document in social text stream make this a challenging problem our approach includes three core ingredient short document expansion time aware topic tracking and chunk based structural learning we extend each short document in social text stream to a more comprehensive representation via state of the art entity linking and sentence ranking strategy from document extended in this manner we infer dynamic probabilistic distribution over topic by dividing topic into dynamic global topic and local topic for the third and final phase we propose a chunk based structural optimization strategy to classify each document into multiple class extensive experiment conducted on a large real world dataset show the effectiveness of our proposed method for hierarchical multi label classification of social text stream 
with the rise in popularity of smart phone taking and sharing photograph ha never been more openly accessible further photo sharing website such a flickr have made the distribution of photograph easy resulting in an increase of visual content uploaded online due to the laborious nature of annotating image however a large percentage of these image are unannotated making their organisation and retrieval difficult therefore there ha been a recent research focus on the automatic and semi automatic process of annotating these image despite the progress made in this field however annotating image automatically based on their visual appearance often result in unsatisfactory suggestion and a a result these model have not been adopted in photo sharing website many method have therefore looked to exploit new source of evidence for annotation purpose such a image context for example in this demonstration we instead explore the scenario of annotating image taken at a large scale event where evidence can be extracted from a wealth of online textual resource specifically we present a novel tag recommendation system for image taken at a popular music festival which allows the user to select relevant tag from related tweet and wikipedia content thus reducing the workload involved in the annotation process 
the query performance prediction task ha been described a estimating retrieval effectiveness in the absence of relevance judgment the expectation throughout the year were that improved prediction technique would translate to improved retrieval approach however this ha not yet happened herein we provide an in depth analysis of why this is the case to this end we formalize the prediction task in the most general probabilistic term using this formalism we draw novel connection between task and method used to address these task in federated search fusion based retrieval and query performance prediction furthermore using formal argument we show that the ability to estimate the probability of effective retrieval with no relevance judgment i e to predict performance implies knowledge of how to perform effective retrieval we also explain why the expectation that using previously proposed query performance predictor would help to improve retrieval effectiveness wa not realized this is due to a misalignment with the actual goal for which these predictor were devised ranking query based on the presumed effectiveness of using them for retrieval over a corpus with a specific retrieval method focusing on this specific prediction task namely query ranking by presumed effectiveness we present a novel learning to rank based approach that us markov random field the resultant prediction quality substantially transcends that of state of the art predictor 
a query considered in isolation provides limited information about the searcher s interest previous work ha considered various type of user behavior e g click and dwell time to obtain a better understanding of the user s intent we consider the searcher s search and page view history using search log from a commercial search engine we i investigate the impact of feature derived from user behavior on reranking a generic ranked list ii optimally integrate the contribution of user behavior and candidate document by learning their relative importance per query based on similar user we use dwell time on clicked url when estimating the relevance of document for a query and perform bayesian probabilistic matrix factorization a smoothing to predict the relevance considering user behavior achieves better ranking than non personalized ranking aggregation of user behavior and query document feature with a user dependent adaptive weight outperforms combination with a fixed uniform value 
in the current web era the popularity of web resource fluctuates ephemerally based on trend and social interest a a result content based relevance signal are insufficient to meet user constantly evolving information need in searching for web item incorporating future popularity into ranking is one way to counter this however predicting popularity a a third party a in the case of general search engine is difficult in practice due to their limited access to item view history to enable popularity prediction externally without excessive crawling we propose an alternative solution by leveraging user comment which are more accessible than view count due to the sparsity of comment traditional solution that are solely based on view history do not perform well to deal with this sparsity we mine comment to recover additional signal such a social influence by modeling comment a a time aware bipartite graph we propose a regularization based ranking algorithm that account for temporal social influence and current popularity factor to predict the future popularity of item experimental result on three real world datasets crawled from youtube flickr and last fm show that our method consistently outperforms competitive baseline in several evaluation task 
topic model have been widely used for text analysis previous topic model have enjoyed great success in mining the latent topic structure of text document with many effort made on endowing the resulting document topic distribution with different motivation however none of these model have paid any attention on the resulting topic word distribution since topic word distribution also play an important role in the modeling performance topic model which emphasize only the resulting document topic representation but pay le attention to the topic term distribution are limited in this paper we propose the orthogonalized topic model otm which imposes an orthogonality constraint on the topic term distribution we also propose a novel model fitting algorithm based on the generalized expectation maximization algorithm and the newthon raphson method quantitative evaluation of text classification demonstrates that otm outperforms other baseline model and indicates the important role played by topic orthogonalizing 
it is crucial for query auto completion to accurately predict what a user is typing given a query prefix and it context e g previous query conventional context aware approach often produce relevant query to the context the purpose of this paper is to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance we first conduct an in depth analysis of how the user reformulate their query based on the analysis we propose a supervised approach to query auto completion where three kind of reformulation related feature are considered including term level query level and session level feature these feature carefully capture how the user change preceding query along the query session extensive experiment have been conducted on the large scale query log of a commercial search engine the experimental result demonstrate a significant improvement over competitive baseline 
we investigate the application of a light weight approach to result list clustering for the purpose of diversifying search result we introduce a novel post retrieval approach which is independent of external information or even the full text content of retrieved document only the retrieval score of a document is used our experiment show that this novel approach is beneficial to effectiveness albeit only on certain baseline system the fact that the method work indicates that the retrieval score is potentially exploitable in diversity 
axiomatic approach provides a systematic way to think about heuristic identify the weakness of existing method and optimize the existing method accordingly this tutorial aim to promote axiomatic thinking that can benefit not only the study of ir model but also the method for many ir application 
retrievability is an important and interesting indicator that can be used in a number of way to analyse information retrieval system and document collection rather than focusing totally on relevance retrievability examines what is retrieved how often it is retrieved and whether a user is likely to retrieve a document or not this is important because a document need to be retrieved before it can be judged for relevance in this tutorial we shall explain the concept of retrievability along with a number of retrievability measure how it can be estimated and how it can be used for analysis since retrieval precedes relevance we shall also provide an overview of how retrievability relates to effectiveness describing some of the insight that researcher have discovered so far we shall also show how retrievability relates to efficiency and how the theory of retrievability can be used to improve both effectiveness and efficiency then we shall provide an overview of the different application of retrievability such a search engine bias corpus profiling etc before wrapping up with challenge and opportunity the final session of the day will look at example problem and way to analyse and apply retrievability to other problem and domain 
information retrieval test collection traditionally use a combination of automatic and manual run to create a pool of document to be judged the quality of the final judgment produced for a collection is a product of the variety across each of the run submitted and the pool depth in this work we explore fully automated approach to generating a pool by combining a simple voting approach with machine learning from document retrieved by automatic run we are able to identify a large portion of relevant document that would normally only be found through manual run our initial result are promising and can be extended in future study to help test collection curator ensure proper judgment coverage is maintained across complete document collection 
with the development of microblog service ten of thousand of message are produced every day and recommending useful message according to user interest is recognized a an effective way to overcome the information overload problem collaborative filtering which rooted from recommender system ha been utilized for microblog recommendation where social relationship information can help improve the recommendation performance however most of existing method only consider the static relationship i e the following relationship which totally ignores the relationship conveyed by user repost behavior to explore the effect of behavior based relationship on recommendation we propose an interaction based collaborative filtering ibcf approach specifically we first use topic model to analyze user interactive behavior and measure the topic specific relationship strength then we incorporate the relationship factor into the matrix factorization framework experimental result show that compared to the current popular social recommendation method ibcf can achieve better performance on the map and ndcg evaluation measure and have better interpretability for the recommended result 
online search evaluation metric are typically derived based on implicit feedback from the user for instance computing the number of page click number of query or dwell time on a search result in a recent paper dupret and lalmas introduced a new metric called absence time which us the time interval between successive session of user to measure their satisfaction with the system they evaluated this metric on a version of yahoo answer in this paper we investigate the effectiveness of absence time in evaluating new feature in a web search engine such a new ranking algorithm or a new user interface we measured the variation of absence time to the effect of experiment performed on a search engine our finding show that the outcome of absence time agreed with the judgement of human expert performing a thorough analysis of a wide range of online and offline metric in out of these case we also investigated the relationship between absence time and a set of commonly used covariates feature such a the number of query and click in the session our result suggest that user are likely to return to the search engine sooner when their previous session ha more query and more click 
this paper investigates the temporal cluster hypothesis in search task where time play an important role do relevant document tend to cluster together in time we explore this question in the context of tweet search and temporal feedback starting with an initial set of result from a baseline retrieval model we estimate the temporal density of relevant document which is then used for result reranking our contribution lie in a method to characterize this temporal density function using kernel density estimation with and without human relevance judgment and an approach to integrating this information into a standard retrieval model experiment on trec datasets confirm that our temporal feedback formulation improves search effectiveness thus providing support for our hypothesis our approach out performs both a standard baseline and previous temporal retrieval model temporal feedback improves over standard lexical feedback with and without human judgment illustrating that temporal relevance signal exist independently of document content 
in many online news service user often write comment towards news in subjective emotion such a sadness happiness or anger knowing such emotion can help understand the preference and perspective of individual user and therefore may facilitate online publisher to provide more relevant service to user although building emotion classifier is a practical task it highly depends on sufficient training data that is not easy to be collected directly and the manually labeling work of comment can be quite labor intensive also online news ha different domain which make the problem even harder a different word distribution of the domain require different classifier with corresponding distinct training data this paper address the task of emotion tagging for comment of cross domain online news the cross domain task is formulated a a transfer learning problem which utilizes a small amount of labeled data from a target news domain and abundant labeled data from a different source domain this paper proposes a novel framework to transfer knowledge across different news domain more specifically different approach have been proposed when the two domain share the same set of emotion category or use different category an extensive set of experimental result on four datasets from popular online news service demonstrates the effectiveness of our proposed model in cross domain emotion tagging for comment of online news in both the scenario of sharing the same emotion category or having different category in the source and target domain 
session search is a complex search task that involves multiple search iteration triggered by query reformulations we observe a markov chain in session search user s judgment of retrieved document in the previous search iteration affect user s action in the next iteration we thus propose to model session search a a dual agent stochastic game the user agent and the search engine agent work together to jointly maximize their long term reward the framework which we term win win search is based on partially observable markov decision process we mathematically model dynamic in session search including decision state query change click and reward a a cooperative game between the user and the search engine the experiment on trec and session datasets show a statistically significant improvement over the state of the art interactive search and session search algorithm 
while many multidimensional model of relevance have been posited prior study have been largely exploratory rather than confirmatory lacking a methodological framework to quantify the relationship among factor or measure model fit to observed data many past model could not be empirically tested or falsified to enable more positivist experimentation xu and chen proposed a psychometric framework for multidimensional relevance modeling however we show their framework exhibit several methodological limitation which could call into question the validity of finding drawn from it in this work we identify and address these limitation scale their methodology via crowdsourcing and describe quality control method from psychometrics which stand to benefit crowdsourcing ir study in general methodology we describe for relevance judging is expected to benefit both human centered and system centered ir 
in this tutorial we will present review and compare the most popular evaluation metric for some of the most salient information related task covering i information retrieval ii clustering and iii filtering the tutorial will make a special emphasis on the specification of constraint for suitable metric in each of the three task and on the systematic comparison of metric according to such constraint the last part of the tutorial will investigate the challenge of combining and weighting metric 
click log provide a unique and highly valuable source of human judgment on ad relevance however click are heavily biased by lot of factor two main factor that are widely acknowledged to be the most influential one are neighboring ad and presentation order the latter is referred to a positional effect a popular practice to recover the ad quality cleaned from positional bias is to adopt click model based on examination or cascade hypothesis originally developed for organic search in this paper we show the strong evidence that this practice is far from perfection when considering the top ad block on a search engine result page serp we show that cascade hypothesis is the most questionable one because of important difference between organic and sponsored search result that may encourage user to analyze the whole ad block before clicking additionally we design a testing setup for an unbiased evaluation of click model prediction accuracy 
modeling user behavior on a search engine result page is important for understanding the user and supporting simulation experiment a result page become more complex click model evolve a well in order to capture additional aspect of user behavior in response to new form of result presentation we propose a method for evaluating the intuitiveness of vertical aware click model namely the ability of a click model to capture key aspect of aggregated result page such a vertical selection item selection result presentation and vertical diversity this method allows u to isolate model component and therefore give a multi faceted view on a model s performance we argue that our method can be used in conjunction with traditional click model evaluation metric such a log likelihood or perplexity in order to demonstrate the power of our method in situation where result page can contain more than one type of vertical e g image and news we extend the previously studied federated click model such that it model user click on such page our evaluation method yield non trivial yet interpretable conclusion about the intuitiveness of click model highlighting their strength and weakness 
work on using relevance feedback for retrieval ha focused on the single retrieved list setting that is an initial document list is retrieved in response to the query and feedback for the most highly ranked document is used to perform a second search we address a setting wherein the list for which feedback is provided result from fusing several intermediate retrieved list accordingly we devise method that utilize the feedback while exploiting the special characteristic of the fusion setting specifically the feedback serf two different yet complementary purpose the first is to directly rank the pool of document in the intermediate list the second is to estimate the effectiveness of the intermediate list for improved re fusion in addition we present a meta fusion method that us the feedback for these two purpose simultaneously empirical evaluation demonstrates the merit of our approach a a case in point the retrieval performance is substantially better than that of using the relevance feedback a in the single list setting the performance also substantially transcends that of a previously proposed approach to utilizing relevance feedback in fusion based retrieval 
understanding how people interact when searching is central to the study of interactive information retrieval iir most of the prior work ha either been conceptual observational or empirical while this ha led to numerous insight and finding regarding the interaction between user and system the theory ha lagged behind in this paper we extend the recently proposed search economic theory to make the model more realistic we then derive eight interaction based hypothesis regarding search behaviour to validate the model we explore whether the search behaviour of thirty six participant from a lab based study is consistent with the theory our analysis show that observed search behaviour are in line with predicted search behaviour and that it is possible to provide credible explanation for such behaviour this work describes a concise and compact representation of search behaviour providing a strong theoretical basis for future iir research 
circumlocution is when many word are used to describe what could be said with fewer e g a machine that take moisture out of the air instead of dehumidifier web search is a perfect backdrop for circumlocution where people struggle to name what they seek in some domain not knowing the correct term can have a significant impact on the search result that are retrieved we study the medical domain where professional medical term are not commonly known and where the consequence of not knowing the correct term can impact the accuracy of surfaced information a well a escalation of anxiety and ultimately the medical care sought given a free form colloquial health search query our objective is to find the underlying professional medical term the problem is complicated by the fact that people issue quite varied query to describe what they have machine learning algorithm can be brought to bear on the problem but there are two key complexity creating high quality training data and identifying predictive feature to our knowledge no prior work ha been able to crack this important problem due to the lack of training data we give novel solution and demonstrate their efficacy via extensive experiment greatly improving over the prior art 
we present a novel approach to the cluster labeling task using fusion method the core idea of our approach is to weigh label suggested by any labeler according to the estimated labeler s decisiveness with respect to each of it suggested label we hypothesize that a cluster labeler s labeling choice for a given cluster should remain stable even in the presence of a slightly incomplete cluster data using state of the art cluster labeling and data fusion method evaluated over a large data collection of cluster we demonstrate that overall the cluster labeling fusion method that further consider the labeler s decisiveness provide the best labeling performance 
in the age of big data automatic method for creating summary of document become increasingly important in this paper we propose a novel unsupervised method for multi document summarization in an unsupervised and language independent fashion this approach relies on the strength of word association in the set of document to be summarized the summary are generated by picking sentence which cover the most specific word association of the document s we measure the performance on the duc dataset our experiment indicate that the proposed method is the best performing unsupervised summarization method in the state of the art that make no use of human curated knowledge base 
web search ha seen two big change recently rapid growth in mobile search traffic and an increasing trend towards providing answer like result for relatively simple information need e g weather today such result display the answer or relevant information on the search page itself without requiring a user to click while click on organic search result have been used extensively to infer result relevance and search satisfaction click on answer like result are often rare or meaningless making it challenging to evaluate answer quality together these call for better measurement and understanding of search satisfaction on mobile device in this paper we studied whether tracking the browser viewport visible portion of a web page on mobile phone could enable accurate measurement of user attention at scale and provide good measurement of search satisfaction in the absence of click focusing on answer like result in web search we designed a lab study to systematically vary answer presence and relevance to the user s information need obtained satisfaction rating from user and simultaneously recorded eye gaze and viewport data a user performed search task using this ground truth we identified increased scrolling past answer and increased time below answer a clear measurable signal of user dissatisfaction with answer while the viewport may contain three to four result at any given time we found strong correlation between gaze duration and viewport duration on a per result basis and that the average user attention is focused on the top half of the phone screen suggesting that we may be able to scalably and reliably identify which specific result the user is looking at from viewport data alone 
we study the problem of linking information between different idiomatic usage of the same language for example colloquial and formal language we propose a novel probabilistic topic model called multi idiomatic lda milda it modeling principle follow the intuition that certain word are shared between two idiom of the same language while other word are non shared that is idiom specific we demonstrate the ability of our model to learn relation between cross idiomatic topic in a dataset containing product description and review we intrinsically evaluate our model by the perplexity measure following that a an extrinsic evaluation we present the utility of the new milda topic model in a recently proposed ir task of linking pinterest pin given in colloquial english on the user side to online webshops given in formal english on the retailer side we show that our multi idiomatic model outperforms the standard monolingual lda model and the pure bilingual lda model both in term of perplexity and map score in the ir task 
reputation management expert have to monitor among others twitter constantly and decide at any given time what is being said about the entity of interest a company organization personality solving this reputation monitoring problem automatically a a topic detection task is both essential manual processing of data is either costly or prohibitive and challenging topic of interest for reputation monitoring are usually fine grained and suffer from data sparsity we focus on a solution for the problem that i learns a pairwise tweet similarity function from previously annotated data using all kind of content based and twitter based feature ii applies a clustering algorithm on the previously learned similarity function our experiment indicate that i twitter signal can be used to improve the topic detection process with respect to using content signal only ii learning a similarity function is a flexible and efficient way of introducing supervision in the topic detection clustering process the performance of our best system is substantially better than state of the art approach and get close to the inter annotator agreement rate a detailed qualitative inspection of the data further reveals two type of topic detected by reputation expert reputation alert issue which usually spike in time and organizational topic which are usually stable across time 
most existing tag based social image search engine present search result a a ranked list of image which cannot be consumed by user in a natural and intuitive manner in this paper we present a novel concept preserving image search result summarization algorithm named prism prism exploit both visual feature and tag of the search result to generate high quality summary which not only break the result into visually and semantically coherent cluster but it also maximizes the coverage of the summary w r t the original search result it first construct a visual similarity graph where the node are image in the search result and the edge represent visual similarity between pair of image this graph is optimally decomposed and compressed into a set of concept preserving subgraphs based on a set of summarization objective image in a concept preserving subgraph are visually and semantically cohesive and are described by a minimal set of tag or concept lastly one or more exemplar image from each subgraph is selected to form the exemplar summary of the result set through empirical study we demonstrate the effectiveness of prism against state of the art image summarization and clustering algorithm 
the aim of risk sensitive evaluation is to measure when a given information retrieval ir system doe not perform worse than a corresponding baseline system for any topic this paper argues that risk sensitive evaluation is akin to the underlying methodology of the student s t test for matched pair hence we introduce a risk reward tradeoff measure trisk that generalises the existing urisk measure a used in the trec web track s risk sensitive task while being theoretically grounded in statistical hypothesis testing and easily interpretable in particular we show that trisk is a linear transformation of the t statistic which is the test statistic used in the student s t test this inherent relationship between trisk and the t statistic turn risk sensitive evaluation from a descriptive analysis to a fully fledged inferential analysis specifically we demonstrate using past trec data that by using the inferential analysis technique introduced in this paper we can decide whether an observed level of risk for an ir system is statistically significant and thereby infer whether the system exhibit a real risk and determine the topic that individually lead to a significant level of risk indeed we show that the latter permit a state of the art learning to rank algorithm lambdamart to focus on those topic in order to learn effective yet risk averse ranking system 
online service rely on machine identifier to tailor service such a personalized search and advertising to individual user the assumption made is that each identifier comprises the behavior of a single person however shared machine usage is common and in these case the activity of multiple user may be generated under a single identifier creating a potentially noisy signal for application such a search personalization we propose enhancing web search personalization with method that can disambiguate among different user of a machine thus connecting the current query with the appropriate search history using log containing both person and machine identifier and log from a popular commercial search engine we learn model that accurately assign observed search behavior to each of different user this information is then used to augment existing personalization method that are currently based only on machine identifier we show that this new capability to infer user can be used to improve the performance of existing personalization method the early finding of our research are promising and have implication for search personalization 
when applying learning to rank algorithm in real search application noise in human labeled training data becomes an inevitable problem which will affect the performance of the algorithm previous work mainly focused on studying how noise affect ranking algorithm and how to design robust ranking algorithm in our work we investigate what inherent characteristic make training data robust to label noise the motivation of our work come from an interesting observation that a same ranking algorithm may show very different sensitivity to label noise over different data set we thus investigate the underlying reason for this observation based on two typical kind of learning to rank algorithm i e pairwise and listwise method and three different public data set i e ohsumed td and mslr web k we find that when label noise increase in training data it is the emph document pair noise ratio i e emph pnoise rather than emph document noise ratio i e emph dnoise that can well explain the performance degradation of a ranking algorithm 
with the prevalence of the geo position enabled device and service a rapidly growing amount of tweet are associated with geo tag consequently the real time search on geo tagged twitter stream ha attracted great attention in this paper we advocate the significance of the co occurrence of keywords for the geo tagged tweet data analytics which is overlooked by existing study particularly we formally introduce the problem of identifying local frequent keyword co occurrence pattern over the geo tagged twitter stream namely lfp xspace query to accommodate the high volume and the rapid update of the twitter stream we develop an inverted kmv sketch ik xspace sketch for short structure to capture the co occurrence of keywords in limited space then efficient algorithm are developed based on ik xspace sketch to support lfp xspace query a well a it variant the extensive empirical study on real twitter dataset confirms the effectiveness and efficiency of our approach 
twitter a one of the most popular social medium platform provides a convenient way for people to communicate and interact with each other it ha been well recognized that influence exists during user interaction some pioneer study on finding influential user have been reported in the literature but they do not distinguish different influence role which are of great value for various marketing purpose in this paper we move a step forward trying to further distinguish influence role of twitter user in a certain topic by defining three view of feature relating to topic sentiment and popularity respectively we propose a multi view influence role clustering mirc algorithm to group twitter user into five category experimental result show the effectiveness of the proposed approach in inferring influence role 
twitter is a popular platform for sharing activity plan and opinion through tweet user often reveal their location information and short term visiting plan in this paper we are interested in extracting fine grained location mentioned in tweet with temporal awareness more specifically we like to extract each point of interest poi mention in a tweet and predict whether the user ha visited is currently at or will soon visit this poi our proposed solution named petar consists of two main component a poi inventory and a time aware poi tagger the poi inventory is built by exploiting the crowd wisdom of foursquare community it contains not only the formal name of poi but also the informal abbreviation the poi tagger based on conditional random field crf model is designed to simultaneously identify the poi and resolve their associated temporal awareness in our experiment we investigated four type of feature i e lexical grammatical geographical and bilou schema feature for time aware poi extraction with the four type of feature petar achieves promising extraction accuracy and outperforms all baseline method 
using the inferred measure framework is a popular choice for constructing test collection when the target document set is too large for pooling to be a viable option within the framework different amount of assessing effort is placed on different region of the ranked list a defined by a sampling strategy the sampling strategy is critically important to the quality of the resultant collection but there is little published guidance a to the important factor this paper address this gap by examining the effect on collection quality of different sampling strategy within the inferred measure framework the quality of a collection is measured by how accurately it distinguishes the set of significantly different system pair top k pooling is competitive though not the best strategy because it cannot distinguish topic with large relevant set size incorporating a deep very sparsely sampled stratum is a poor choice strategy that include a top pool create better collection than those that do not a well a allow precision score to be directly computed 
pseudo relevance feedback is an effective technique to improve the performance of ad hoc information retrieval traditionally the expansion term are extracted either according to the term distribution in the feedback document or according to both the term distribution in the feedback document and in the whole document collection however most of the existing model employ a single term frequency normalization mechanism or criterion that cannot take into account various aspect of a term s saliency in the feedback document in this paper we propose a simple and heuristic but effective model in which three term frequency transformation technique are integrated to capture the saliency of a candidate term associated with the original query term in the feedback document through evaluation and comparison on six trec collection we show that our proposed model is effective and generally superior to the recent progress of relevance feedback model 
music information retrieval and music recommendation are seeing a paradigm shift towards method that incorporate user context aspect however structured experiment on a standardized music dataset to investigate the effect of doing so are scarce in this paper we compare performance of various combination of collaborative filtering and geospatial a well a cultural user model for the task of music recommendation to this end we propose a geospatial model that us gps coordinate and a cultural model that us semantic location continent country and state of the user we conduct experiment on a novel standardized music collection the million musical tweet dataset of listening event extracted from microblogs overall we find that modeling listener location via gaussian mixture model and computing similarity from these outperforms both cultural user model and collaborative filtering 
vocabulary mismatch ha long been recognized a one of the major issue affecting search effectiveness ineffective query usually fail to incorporate important term and or incorrectly include inappropriate keywords however in this paper we show another cause of reduced search performance sometimes user issue reasonable query term but system cannot identify the correct property of those term and take advantage of the property specifically we study two distinct type of term that exist in all search query necessary term for which term occurrence alone is indicative of document relevance and frequent term for which the relative term frequency is indicative of document relevance within the set of document where the term appears we evaluate these two property of query term in a dataset result show that only of the term are both necessary and frequent while another only hold one of the property and the final third do not hold any of the property however existing retrieval model do not clearly distinguish term with the two property and consider them differently we further show the great potential of improving retrieval model by treating term with distinct property differently 
evaluation a a service eaas is a new methodology that enables community wide evaluation and the construction of test collection on document that cannot be distributed the basic idea is that evaluation organizer provide a service api through which the evaluation task can be completed however this concept violates some of the premise of traditional pool based collection building and thus call into question the quality of the resulting test collection in particular the service api might restrict the diversity of run that contribute to the pool this might hamper innovation by researcher and lead to incomplete judgment pool that affect the reusability of the collection this paper show that the distinctiveness of the retrieval run used to construct the first test collection built using eaas the trec microblog collection is not substantially different from that of the trec ad hoc collection a high quality collection built using traditional pooling further analysis using the leave out uniques test suggests that pool from the microblog collection are le complete than those from trec although both collection benefit from the presence of distinctive and effective manual run although we cannot yet generalize to all eaas implementation our analysis reveal no obvious flaw in the test collection built using the methodology in the trec microblog track 
extensive previous research ha shown that searcher often require assistance with query formulation and refinement yet it is not clear what kind of assistance is most useful and how effective it is both objectively e g in term of task success and subjectively e g in term of searcher perception of the search difficulty this work describes the result of a controlled user study comparing the effect of providing specific v generic search hint on search success and satisfaction our result indicate that specific search hint tend to effectively improve searcher success rate and reduce perceived effort while generic one can be detrimental in both search effectiveness and user satisfaction the result of this study are an important step towards the design of future search system that could effectively assist and guide the user in accomplishing complex search task 
linkedin is the world s largest professional network with over million member one of the primary activity on the site is people search for which linkedin member are both the user and the corpus this paper present insight about people search behavior on linkedin based on a log analysis and a user study in particular it examines the role that network distance play in name search and non name search for name search user primarily click on only one of the result and closer network distance lead to higher click through rate in contrast for non name search user are more likely to click on multiple result that are not in their existing connection but with whom they have shared connection the result show that while network distance contributes significantly to linkedin search engagement in general it role varies dramatically depending on the type of search query 
people often read summary of news article in order to get reliable information about an event or a topic however the information expressed in news article is not always certain and some sentence contain uncertain information about the event existing summarization system do not consider whether a sentence in news article is certain or not in this paper we propose a novel system called ctsum to incorporate the new factor of information certainty into the summarization task we first analyze the sentence in news article and automatically predict the certainty level of sentence by using the support vector regression method with a few useful feature the predicted certainty score are then incorporated into a summarization system with a graph based ranking algorithm experimental result on a manually labeled dataset verify the effectiveness of the sentence certainty prediction technique and experimental result on the duc dataset show that our new summarization system cannot only produce summary with better content quality but also produce summary with higher certainty 
obesity and it associated health consequence such a high blood pressure and cardiac disease affect a significant proportion of the world s population at the same time the popularity of location based service lb and recommender system is continually increasing with improvement in mobile technology we observe that the health domain lack a suggestion system that focus on healthy lifestyle choice we introduce the mobile application fityou which dynamically generates recommendation according to the user s current location and health condition a a real time lb it utilizes preference determined from user history and health information from a biometric profile the system wa developed upon a top performing contextual suggestion system in both trec and contextual suggestion track 
influence maximization is the problem of finding a set of seed node in social network for maximizing the spread of influence traditionally researcher view influence propagation a a stochastic process and formulate the influence maximization problem a a discrete optimization problem thus most previous work focus on finding efficient and effective heuristic algorithm within the greedy framework in this paper we view the influence maximization problem from the perspective of data reconstruction and propose a novel framework named textsl data reconstruction for influence maximization drim in our framework we first construct an influence matrix each row of which is the influence of a node to other node then we select k most informative row to reconstruct the matrix and the corresponding node are the seed node which could maximize the influence spread finally we evaluate our framework on two real world data set and the result show that drim is at least a effective a the traditional greedy algorithm 
while microblogging ha emerged a an important information sharing and communication platform it ha also become a convenient venue for spammer to overwhelm other user with unwanted content currently spammer detection in microblogging focus on using social networking information but little on content analysis due to the distinct nature of microblogging message first label information is hard to obtain second the text in microblogging are short and noisy a we know spammer detection ha been extensively studied for year in various medium e g email sm and the web motivated by abundant resource available in the other medium we investigate whether we can take advantage of the existing resource for spammer detection in microblogging while people accept that text in microblogging are different from those in other medium there is no quantitative analysis to show how different they are in this paper we first perform a comprehensive linguistic study to compare spam across different medium inspired by the finding we present an optimization formulation that enables the design of spammer detection in microblogging using knowledge from external medium we conduct experiment on real world twitter datasets to verify whether email sm and web spam resource help and how different medium help for spammer detection in microblogging 
can the activity pattern of page use during information search session discriminate between different type of information seeking task we model sequence of interaction with search result and content page during information search session two representation are created the sequence of page use and a cognitive representation of page interaction the cognitive representation is based on logged eye movement pattern of textual information acquisition via the reading process page sequence action from task session n in a user study are analyzed the study task differed from one another in basic dimension of complexity specificity level and the type of information product intellectual or factual the result show that difference in task type can be measured at both the level of observation of page type sequence and at the level of cognitive activity on the page we discus the implication for personalization of search system measurement of task similarity and the development of user centered information system that can support the user s current and expected search intention 
existing recommender system usually model item a static unchanging in attribute description and feature however in domain such a mobile apps a version update may provide substantial change to an app a update reflected by an increment in it version number may attract a consumer s interest for a previously unappealing version version description constitute an important recommendation evidence source a well a a basis for understanding the rationale for a recommendation we present a novel framework that incorporates feature distilled from version description into app recommendation we use a semi supervised topic model to construct a representation of an app s version a a set of latent topic from version metadata and textual description we then discriminate the topic based on genre information and weight them on a per user basis to generate a version sensitive ranked list of apps for a target user incorporating our version feature with state of the art individual and hybrid recommendation technique significantly improves recommendation quality an important advantage of our method is that it target particular version of apps allowing previously disfavored apps to be recommended when user relevant feature are added 
information retrieval ir and information privacy security are two fast growing computer science discipline there are many synergy and connection between these two discipline however there have been very limited effort to connect the two important discipline on the other hand due to lack of mature technique in privacy preserving ir concern about information privacy and security have become serious obstacle that prevent valuable user data to be used in ir research such a study on query log social medium tweet session and medical record retrieval this privacy preserving ir workshop aim to spur research that brings together the research field of ir and privacy security and research that mitigates privacy threat in information retrieval by constructing novel algorithm and tool that enable web user to better understand associated privacy risk 
microblogging service have emerged a an essential way to strengthen the communication among individual one of the most important feature of microblog over traditional social network is the extensive proliferation in information diffusion a the outbreak of information diffusion often brings in valuable opportunity or devastating effect it will be beneficial if a mechanism can be provided to predict whether a piece of information will become viral and which part of the network will participate in propagating this information in this work we define three type of influence namely interest oriented influence social oriented influence and epidemic oriented influence that will affect a user s decision on whether to perform a diffusion action we propose a diffusion targeted influence model to differentiate and quantify various type of influence further we model the problem of diffusion prediction by factorizing a user s intention to transmit a microblog into these influence the learned prediction model is then used to predict the future diffusion state of any new microblog we conduct experiment on a real world microblogging dataset to evaluate our method and the result demonstrate the superiority of the proposed framework a compared to the state of the art approach 
context aware recommendation car can lead to significant improvement in the relevance of the recommended item by modeling the nuanced way in which context influence preference the dominant approach in context aware recommendation ha been the multidimensional latent factor approach in which user item and context variable are represented a latent feature in low dimensional space an interaction between a user item and a context variable is typically modeled a some linear combination of their latent feature however given the many possible type of interaction between user item and contextual variable it may seem unrealistic to restrict the interaction among them to linearity to address this limitation we develop a novel and powerful non linear probabilistic algorithm for context aware recommendation using gaussian process the method which we call gaussian process factorization machine gpfm is applicable to both the explicit feedback setting e g numerical rating a in the netflix dataset and the implicit feedback setting i e purchase click we derive stochastic gradient descent optimization to allow scalability of the model we test gpfm on five different benchmark contextual datasets experimental result demonstrate that gpfm outperforms state of the art context aware recommendation method 
spelling correction in the s wa all about algorithm and small dictionary this century it is about mining vast data set of past user behavior simple algorithm and using those to correct mistake the large internet giant are data driven enterprise that use data to transform and continually improve user experience in this talk hugh williams share story about data and how it is used to build internet product and explains why he belief data will transform business a we know them every major company is becoming a data driven company and hugh share example of transformation occurring in health aviation farming and telecommunication he recently joined pivotal a company that is assembling the toolkit that exists in only a few consumer internet company and making that toolkit open and available to every industry including big data platform development framework and an open cloud independent platform a a service he will conclude by sharing detail about pivotal the pivotal vision and roadmap hugh e williams ha been senior vice president of research development at pivotal since january his team build big data technology and development framework and service including pivotal s hadoop spring java framework and greenplum database offering most recently he spent four and a half year a an executive with ebay where he wa responsible for the team that conceived designed and built ebay s user experience search engine big data technology and platform prior to joining ebay he managed an r d team at microsoft s bing for four and a half year spent over ten year researching and developing search technology and ran his own startup and consultancy for several year he ha published over work mostly in the field of information retrieval including two book for o reilly medium inc he hold u s patent with many more pending he ha a phd from rmit university in australia 
how assessor and end user judge the relevance of image ha been studied in information science and information retrieval for a considerable time the criterion by which assessor judge relevance ha been intensively studied and there ha been a large amount of work which ha investigated how relevance judgment for test collection can be more cheaply generated such a through crowd sourcing relatively little work ha investigated the process individual assessor go through to judge the relevance of an image in this paper we focus on the process by which relevance is judged for image and in particular the degree of effort a user must expend to judge relevance for different topic result suggest that topic difficulty and how semantic visual a topic is impact user performance and perceived effort 
many query are submitted to search engine by right clicking the marked text i e the query in web browser because the document being read by the searcher often provides sufficient contextual information for the query search engine could provide much more relevant search result if the query is augmented by the contextual information captured from the source document how to extract the right contextual information from the source document is the main focus of this study to this end we evaluate text component extraction scheme and feature extraction scheme the former determines from which text component e g title meta data or paragraph containing the selected query to extract contextual information the latter determines which word or phrase to extract in total combination are evaluated and our evaluation result show that noun phrase extracted from all paragraph that contain the query word is the best option 
this paper address the problem of identifying local expert in social medium system like twitter local expert in contrast to general topic expert have specialized knowledge focused around a particular location and are important for many application including answering local information need and interacting with community expert and yet identifying these expert is difficult hence in this paper we propose a geo spatial driven approach for identifying local expert that leverage the fine grained gps coordinate of million of twitter user we propose a local expertise framework that integrates both user topical expertise and their local authority concretely we estimate a user s local authority via a novel spatial proximity expertise approach that leverage over million geo tagged twitter list we estimate a user s topical expertise based on expertise propagation over million geo tagged social connection on twitter we evaluate the proposed approach across query coupled with over individual judgment from amazon mechanical turk we find significant improvement over both general non local expert approach and comparable local expert finding approach 
recent research ha shown that the performance of search engine can be improved by enriching a user s personal profile with information about other user with shared interest in the existing approach group of similar user are often statically determined e g based on the common document that user clicked however these static grouping method are query independent and neglect the fact that user in a group may have different interest with respect to different topic in this paper we argue that common interest group should be dynamically constructed in response to the user s input query we propose a personalisation framework in which a user profile is enriched using information from other user dynamically grouped with respect to an input query the experimental result on query log from a major commercial web search engine demonstrate that our framework improves the performance of the web search engine and also achieves better performance than the static grouping method 
we consider a scenario where a searcher requires both high precision and high recall from an interactive retrieval process such scenario are very common in real life exemplified by medical search legal search market research and literature review when access to the entire data set is available an active learning loop could be used to ask for additional relevance feedback label in order to refine a classifier when data is accessed via search service however only limited subset of the corpus can be considered subset defined by query in that setting relevance feedback ha been used in a query enhancement loop that update a query we describe and demonstrate the effectiveness of req rec requery reclassify a double loop retrieval system that combine iterative expansion of a query set with iterative refinement of a classifier this permit a separation of concern where the query selector s job is to enhance recall while the classifier s job is to maximize precision on the item that have been retrieved by any of the query so far the overall process alternate between the query enhancement loop to increase recall and the classifier refinement loop to increase precision the separation allows the query enhancement process to explore larger part of the query space our experiment show that this distribution of work significantly outperforms previous relevance feedback method that rely on a single ranking function to balance precision and recall 
majority of the study on modeling the evolution of a social network using spectral graph kernel do not consider temporal effect while estimating the kernel parameter a a result such kernel fail to capture structural property of the evolution over the time in this paper we propose temporal spectral graph kernel of four popular graph kernel namely path counting triangle closing exponential and neumann their response in predicting future growth of the network have been investigated in detail using two large datasets namely facebook and dblp it is evident from various experimental setup that the proposed temporal spectral graph kernel outperform all of their non temporal counterpart in predicting future growth of the network 
state of the art method for product recommendation encounter significant performance drop in category where a user ha no purchase history this problem need to be addressed since current online retailer are moving beyond single category and attempting to be diversified in this paper we investigate the challenge problem of product recommendation in unexplored category and discover that the price a factor transferrable across category can improve the recommendation performance significantly through our investigation we address four research question progressively what is the impact of unexplored category on recommendation performance how to represent the price factor from the recommendation point of view what doe price factor across category mean to recommendation how to utilize price factor across category for recommendation in unexplored category based on a series of experiment and analysis conducted on a dataset collected from a leading e commerce website we discover valuable finding for the above four question first unexplored category cause performance drop by relatively for current recommendation system second the price factor can be represented a either a quantity for a product or a distribution for a user to improve performance third consumer behavior with respect to price factor across category is complicated and need to be carefully modeled finally and most importantly we propose a new method which encodes the two perspective of the price factor the proposed method significantly improves the recommendation performance in unexplored category over the state of the art baseline system and shortens the performance gap by relatively 
in this paper we describe virlab a novel web based virtual laboratory for information retrieval ir unlike existing command line based ir toolkits the virlab system provides a more interactive tool that enables easy implementation of retrieval function with only a few line of code simplified evaluation process over multiple data set and parameter setting and straightforward result analysis interface through operational search engine and pair wise comparison these feature make virlab a unique and novel tool that can help teaching ir model improving the productivity for doing ir model research a well a promoting controlled experimental study of ir model 
recent work introduced a probabilistic framework that measure search engine performance information theoretically this allows for novel meta evaluation measure such a information difference which measure the magnitude of the difference between search engine in their ranking of document for which we have relevance information using information difference we can compare the behavior of search engine which document the search engine prefers a well a search engine performance how likely the search engine is to satisfy a hypothetical user in this work we a extend this probabilistic framework to precision oriented context b show that information difference can be used to detect similar search engine at shallow rank and c demonstrate the utility of the information difference methodology by showing that well tuned search engine employing different retrieval model are more similar than a well tuned and a poorly tuned implementation of the same retrieval model 
social recommender system ha become an emerging research topic due to the prevalence of online social networking service during the past few year in this paper aiming at providing fundamental support to the research of social recommendation problem we conduct an in depth analysis on the correlation between social friend relation and user interest similarity when evaluating interest similarity without distinguishing different friend a user ha we surprisingly observe that social friend relation generally cannot represent user interest similarity a user s average similarity on all his her friend is even correlated with the average similarity on some other randomly selected user however when measuring interest similarity using a finer granularity we find that the similarity between a user and his her friend are actually controlled by the network structure in the friend network factor that affect the interest similarity include subgraph topology connected component number of co friend etc we believe our analysis provides substantial impact for social recommendation research and will benefit ongoing research in both recommender system and other social application 
recently significant progress ha been made in research on what we call semantic matching sm in web search question answering online advertisement cross language information retrieval and other task advanced technology based on machine learning have been developed let u take web search a example of the problem that also pervades the other task when comparing the textual content of query and document web search still heavily relies on the term based approach where the relevance score between query and document are calculated on the basis of the degree of matching between query term and document term this simple approach work rather well in practice partly because there are many other signal in web search hypertext user log etc that complement it however when considering the long tail of web search it can suffer from data sparseness e g trenton doe not match new jersey capital query document mismatch occur when searcher and author use different term representation and this phenomenon is prevalent due to the nature of human language 
how can a search engine with a relatively weak relevance ranking function compete with a search engine that ha a much stronger ranking function this dual challenge which to the best of our knowledge ha not been addressed in previous work entail an interesting bi modal utility function for the weak search engine that is the goal is to produce in response to a query a document result list whose effectiveness doe not fall much behind that of the strong search engine and which is quite different than that of the strong engine we present a per query algorithmic approach that leverage fundamental retrieval principle such a pseudo feedback based relevance modeling we demonstrate the merit of our approach using trec data 
log based document re ranking is a special form of session search the task re rank document from search engine result page serp according to the search log in which both the search activity from other user and personalized query log for a user are available the purpose of re ranking is to provide the user with a new and better ordering of the initial retrieved document we test the system on the wscd dataset in which the actual content of the query and document are not available due to privacy concern the challenge is to perform effective re ranking purely based on user behavior such a click and query reformulations rather than document content in this paper we propose to model log based document re ranking a a partially observable markov decision process pomdp experiment on the document re ranking task show that our approach is effective and outperforms the baseline ranking provided by a commercial search engine 
many national and international heritage institute realize the importance of archiving the web for future culture heritage web archiving is currently performed either by harvesting a national domain or by crawling a pre defined list of website selected by the archiving institution in either method crawling result in more information being harvested than just the website intended for preservation which could be used to reconstruct impression of page that existed on the live web of the crawl date but would have been lost forever we present a method to create representation of what we will refer to a a web collection s aura the web document that were not included in the archived collection but are known to have existed due to their mention on page that were included in the archived web collection to create representation of these unarchived page we exploit the information about the unarchived url that can be derived from the crawl by combining crawl date distribution anchor text and link structure we illustrate empirically that the size of the aura can be substantial in the dutch web archive contained m unique page while we uncover reference to m additional unarchived page 
enterprise computer network are filled with user performing a variety of task ranging from business critical task to personal interest browsing due to this multi modal distribution of behavior it is non trivial to automatically discern which behavior are business relevant and which are not additionally it is difficult to infer community of interest within the enterprise even given an organizational mapping in this work we present a two step framework for classifying user behavior within an enterprise in a data driven way a a first step we use a latent topic model on active search query to identify type of behavior and topic of interest associated with a given user we then leverage the information about user s assigned role within the organization to extract relevant topic which are most reflective of self organizing community of interest we demonstrate that our framework is able to identify rich community of interest that are better representation of how user interact and assemble in an enterprise setting 
usage of mobile device for web search grows rapidly in recent year the common tendency is that user want to receive information immediately result in incorporating rich snippet and vertical result into search engine result page serps and in increasing of good abandonment this article provides an offline metric for quality evaluation of mobile web search which take good abandonment rate into consideration the metric is the dbn click model that allows the probability to be satisfied directly on the serp the model parameter are estimated from the mobile search log of a controlled experiment the new metric outperforms traditional err metric in term of the validation dataset built using a serp degradation technique 
query auto completion qac is a popular feature of web search engine that aim to assist user to formulate query faster and avoid spelling mistake by presenting them with possible completion a soon a they start typing however despite the wide adoption of auto completion in search system there is little published on how user interact with such service in this paper we present the first large scale study of user interaction with auto completion based on query log of bing a commercial search engine our result confirm that lower ranked auto completion suggestion receive substantially lower engagement than those ranked higher we also observe that user are most likely to engage with auto completion after typing about half of the query and in particular at word boundary interestingly we also noticed that the likelihood of using auto completion varies with the distance of query character on the keyboard overall we believe that the result reported in our study provide valuable insight for understanding user engagement with auto completion and are likely to inform the design of more effective qac system 
text reuse is a common phenomenon in a variety of user generated content along with the quick expansion of social medium reuses of local text are occurring much more frequently than ever before the task of detecting these local reuses serf a an essential step for many application it ha attracted extensive attention in recent year however semantic level similarity have not received consideration in most previous work in this paper we introduce a novel method to efficiently detect local reuses at the semantic level for large scale problem we propose to use continuous vector representation of word to capture the semantic level similarity between short text segment in order to handle ten of billion of document method based on information geometry and hashing method are introduced to aggregate and map text segment presented by word embeddings to binary hash code experimental result demonstrate that the proposed method achieve significantly better performance than state of the art approach in all six document collection belonging to four different category at some recall level the precision of the proposed method are even time higher than previous method moreover the efficiency of the proposed method is comparable to or better than that of some other hashing method 
it is becoming increasingly difficult to stay aware of the state of the art in any research field due to the exponential increase in the number of academic publication this problem effect author and reviewer of submission to academic journal and conference who must be able to identify which portion of an article are novel and which are not therefore having a process to automatically judge the flow of novelty though a document would assist academic in their quest for truth in this article we propose the concept of within document novelty location a method of identifying location of novelty and non novelty within a given document in this preliminary investigation we examine if a second order statistical model ha any benefit in term of accuracy and confidence over a simpler first order model experiment on text sequence taken from three academic article showed that the second order model provided a significant increase in novelty location accuracy for two of the three document there wa no significant difference in accuracy for the remaining document which is likely to be due to the absence of context analysis 
we present a study to understand the effect that negated term e g no fever and family history e g family history of diabetes have on searching clinical record our analysis is aimed at devising the most effective mean of handling negation and family history in doing so we explicitly represent a clinical record according to it different content type negated family history and normal content the retrieval model weight each of these separately empirical evaluation show that overall the presence of negation harm retrieval effectiveness while family history ha little effect we show negation is best handled by weighting negated content rather than the common practise of removing or replacing it however we also show that many query benefit from the inclusion of negated content and that negation is optimally handled on a per query basis additional evaluation show that adaptive handing of negated and family history content can have significant benefit 
online community within the enterprise offer their leader an easy and accessible way to attract engage and influence others our research study the recommendation of social medium content to leader owner of online community within the enterprise we developed a system that suggests to owner new content from outside the community which might interest the community member a online community are taking a central role in the pervasion of social medium to the enterprise sharing such recommendation can help owner create a more lively and engaging community we compared seven different method for generating recommendation including content based member based and hybridization of the two for member based recommendation we experimented with three group owner active member and regular member our evaluation is based on a survey in which community owner rated a total of recommended content item we analyzed the quality of the different recommendation method and examined the effect of different community characteristic such a type and size 
we tackle the problem of improving microblog retrieval algorithm by proposing a robust structural representation of query tweet pair we employ these structure in a principled kernel learning framework that automatically extract and learns highly discriminative feature we test the generalization power of our approach on the trec microblog and task we find that relational syntactic feature generated by structural kernel are effective for learning to rank l r and can easily be combined with those of other existing system to boost their accuracy in particular the result show that our l r approach improves on almost all the participating system at trec only using their raw score a a single feature our method yield an average increase of in retrieval effectiveness and position in system rank 
similarity search is an important problem in many large scale application such a image and text retrieval hashing method ha become popular for similarity search due to it fast search speed and low storage cost recent research ha shown that hashing quality can be dramatically improved by incorporating supervised information e g semantic tag label into hashing function learning however most existing supervised hashing method can be regarded a passive method which assume that the labeled data are provided in advance but in many real world application such supervised information may not be available this paper proposes a novel active hashing approach active hashing with joint data example and tag selection ah jdets which actively selects the most informative data example and tag in a joint manner for hashing function learning in particular it first identifies a set of informative data example and tag for user to label based on the selection criterion that both the data example and tag should be most uncertain and dissimilar with each other then this labeled information is combined with the unlabeled data to generate an effective hashing function an iterative procedure is proposed for learning the optimal hashing function and selecting the most informative data example and tag extensive experiment on four different datasets demonstrate that ah jdets achieves good performance compared with state of the art supervised hashing method but requires much le labeling cost which overcomes the limitation of passive hashing method furthermore experimental result also indicate that the joint active selection approach outperforms a random non active selection method and active selection method only focusing on either data example or tag 
online health seeking ha transformed the way of health knowledge exchange and reusability the existing general and vertical health search engine however just routinely return list of matched document or question answer qa pair which may overwhelm the seeker or not sufficiently meet the seeker expectation instead our multilingual system is able to return one multi faceted answer that is well structured and precisely extracted from multiple heterogeneous healthcare source further should the seeker not be satisfied with the returned search result our system can automatically route the unsolved question to the professional with relevant expertise 
due to the fast query speed and low storage cost hashing based approximate nearest neighbor search method have attracted much attention recently many state of the art method are based on eigenvalue decomposition in these approach the information caught in different dimension is unbalanced and generally most of the information is contained in the top eigenvectors we demonstrate that this lead to an unexpected phenomenon that longer hashing code doe not necessarily yield better performance in this work we introduce a random subspace strategy to address this limitation at first a small fraction of the whole feature space is randomly sampled to train the hashing algorithm each time and only the top eigenvectors are kept to generate one piece of short code this process will be repeated several time and then the obtained many piece of short code are concatenated into one piece of long code theoretical analysis and experiment on two benchmark confirm the effectiveness of the proposed strategy for hashing 
in some jurisdiction party to a lawsuit can request document from each other but document subject to a claim of privilege may be withheld the trec legal track developed what is presently the only public test collection for evaluating privilege classification this paper examines the reliability and reusability of that collection for reliability the key question is the extent to which privilege judgment correctly reflect the opinion of the senior litigator whose judgment is authoritative for reusability the key question is the degree to which system whose result contributed to creation of the test collection can be fairly compared with other system that use those privilege judgment in the future these correspond to measurement error and sampling error respectively the result indicate that measurement error is the larger problem 
data retrieved from community question answering cqa site such a content and user assessment of content is commonly used for expertise estimation related task one such task in which the received vote are directly used a graded relevance assessment value is ranking reply of a question even though these available assessment value are very practical for evaluation purpose they may not always reflect the correct assessment value of the content due to the possible temporal or presentation bias introduced by the cqa system during voting process this paper analyzes a very commonly used cqa data collection in term of these introduced bias and their effect on the experimental evaluation of approach a more bias free test set construction approach which ha correlated result with the manual assessment is also proposed in this paper 
web search query without hyperlink click are often referred to a abandoned query understanding the reason for abandonment is crucial for search engine in evaluating their performance abandonment can be categorized a good or bad depending on whether user information need are satisfied by result page content previous research ha sought to understand abandonment rationale via user survey or ha developed model to predict those rationale using behavioral pattern however these model ignore important contextual factor such a the relationship between the abandoned query and prior abandonment instance we propose more advanced method for modeling and predicting abandonment rationale using contextual information from user search session by analyzing search engine log and discover dependency between abandoned query and user behavior we leverage these dependency signal to build a sequential classifier using a structured learning framework designed to handle such signal our experimental result show that our approach is more accurate than the state of the art abandonment rationale classifier going beyond prediction we leverage the prediction result to significantly improve relevance using instance of predicted good and bad abandonment 
several researcher have found that a user s mouse position give an indication of the user s gaze during web search and other task a part of a user study that involved relevance judging of document summary and full document we recorded user mouse movement we found that in a large number of case the user did nothing more with their mouse than move it to the button used for recording the relevance decision in addition we found that different search topic can result in large difference in the amount of mouse movement that is indicative of user attention for simple reading task such a short document summary mouse tracking doe not appear to be an effective mean of discerning user attention while more complex task may allow mouse movement to provide information regarding user attention on average indication of user attention existed in only of the relevance judgment made for full document 
web archive already hold together more than billion file and this number continues to grow a new initiative arise searching on all version of these file acquired throughout time is challenging since user expect a fast and precise answer from web archive a the one provided by current web search engine this work study for the first time how to improve the search effectiveness of web archive including the creation of novel temporal feature that explore the correlation found between web document persistence and relevance the persistence wa analyzed over year of web snapshot additionally we propose a temporal dependent ranking framework that exploit the variance of web characteristic over time influencing ranking model based on the assumption that closer period are more likely to hold similar web characteristic our framework learns multiple model simultaneously each tuned for a specific period experimental result show significant improvement over the search effectiveness of single model that learn from all data independently of it time thus our approach represents an important step forward on the state of the art ir technology usually employed in web archive 
although searcher often click on more than one result following a query little is known about how they interact with search result after their first click using large scale query log analysis we characterize what people do when they return to a result page after having visited an initial result we find that the initial click provides insight into the searcher s subsequent behavior with short initial dwell time suggesting more future interaction and later click occurring close in rank to the first although user think of a search result list a static when people return to a result list following a click there is the opportunity for the list to change potentially providing additional relevant content such change however can be confusing leading to increased abandonment and slower subsequent click we explore the risk and opportunity of changing search result during use observing for example that when result change above a user s initial click that user is le likely to find new content whereas change below correlate with increased subsequent interaction our result can be used to improve people s search experience during the course of a single query by seamlessly providing new more relevant content a the user interacts with a search result page helping them find what they are looking for without having to issue a new query 
most of the previous approach surrounding collaborative information retrieval cir provide either a user based mediation in which the system only support user collaborative activity or a system based mediation in which the system play an active part in balancing user role re ranking result and distributing them to optimize overall retrieval performance in this paper we propose to combine both of these approach by a role mining methodology that learns from user action about the retrieval strategy they adapt this hybrid method aim at showing how user are different and how to use these difference for suggesting role the core of the method is expressed a an algorithm that monitor user action in a cir setting discovers difference among the collaborator along certain dimension and suggests appropriate role to make the most out of individual skill and optimize ir performance our approach is empirically evaluated and relies on two different laboratory study involving pair of user our experiment show promising result that highlight how role mining could optimize the collaboration within a search session the contribution of this work include a new algorithm for mining user role in collaborative ir an evaluation methodology and a new approach to improve ir performance with the operationalization of user driven system mediated collaboration 
relevance feedback ha been shown to improve retrieval for a broad range of retrieval model it is the most common way of adapting a retrieval model for a specific query in this work we expand this common way by focusing on an approach that enables u to do query specific modification of a retrieval model for learning to rank problem our approach is based on using feedback document in two way to improve the retrieval model directly and to identify a subset of training query that are more predictive than others experiment with the gov collection show that this approach can obtain statistically significant improvement over two baseline learning to rank svm rank with no feedback and learning to rank with standard relevance feedback 
volunteer are extremely crucial to nonprofit organization npos to sustain their continuing operation on the other hand many talent are looking for appropriate volunteer opportunity to realize their dream of making an impact on the world with their expertise this is a typical supply and demand matching issue fortunately user profiling and the discovery of user volunteering tendency can benefit from user continuous enthusiasm and active participation in diverse online social network osns and the huge amount of publicly available user generated content ugcs in this work we aim to bridge the gap between the supply of talent with volunteering tendency and the demand of social enterprise and enhance the social welfare this is done by incorporating volunteering tendency into user profiling across multiple osns consequently this interdisciplinary research open a new window for both computer science and social science to the best of our knowledge this is the first attempt to tackle the problem of volunteer matching for social enterprise based on publicly available ugcs first we explain the definition of the main concept with example second we propose a system architecture for addressing the problem of volunteerism matching that includes three component profile collection profile enrichment and profile matching finally we identify the major challenge encountered in our current research work this paper discus our design and progress in this research 
identifying and targeting visitor on an e commerce website with personalized content in real time is extremely important to marketer although such targeting exists today it is based on demographic attribute of the visitor we show that dynamic visitor attribute extracted from their click stream provide much better predictive capability of visitor intent in this demonstration we showcase an interactive real time user interface for marketer to visualize and target visitor segment our dashboard not only provides the marketer understanding of their visitor click pattern but also let them target individual or group of visitor with offer and promotion 
the use of sampling randomized algorithm or training based on the unpredictable input of user in information retrieval often lead to non deterministic output evaluating the effectiveness of system incorporating these method can be challenging since each run may produce different effectiveness score current ir evaluation technique do not address this problem using the context of distributed information retrieval a a case study for our investigation we propose a solution based on multivariate linear modeling we show that the approach provides a consistent and reliable method to compare the effectiveness of non deterministic ir algorithm and explain how statistic can safely be used to show that two ir algorithm have equivalent effectiveness 
we present a post hoc analysis of a benchmarking activity for information retrieval ir in the medical domain to determine if performance for query with different level of complexity can be associated with different ir method or technique our analysis is based on data and run for task of the clef ehealth lab which provided patient query and a large medical document collection for patient centred medical information retrieval technique development we categorise the query based on their complexity which is defined a the number of medical concept they contain we then show how query complexity affect performance of run submitted to the lab and provide suggestion for improving retrieval quality for this complex retrieval task and similar ir evaluation task 
this paper present a project called knowing camera for real time recognizing and annotating place of interest poi in smartphone photo with the availability of online geotagged image of such place we propose a spatial visual s v framework which consists of a probabilistic field of view model in the spatial phase and sparse coding similarity metric in the visual phase to recognize phone captured poi moreover we put forward an offline collaborative salient area costar mining algorithm to detect common visual feature called costars among the noisy photo geotagged on each poi thus to clean the geotagged image database the mining result can be utilized to annotate the region of interest on the query image during the online query processing besides this mining procedure further improves the efficiency and accuracy of the s v framework our experiment in the real world and oxford k datasets show promising recognition and annotation performance of the proposed approach and that the proposed costar mining technique outperforms state of the art approach 
we combine search in triple store with full text search into what we call emph semantic full text search we provide a fully functional web application that allows the incremental construction of complex query on the english wikipedia combined with the fact from freebase the user is guided by context sensitive suggestion of matching word instance class and relation after each keystroke we also provide a powerful api which may be used for research task or a a back end e g for a question answering system our web application and public api are available under url http broccoli c uni freiburg de 
location model built on social medium have been shown to be an important step toward understanding place in query current search technology focus on predicting broad region such a city hyperlocal scenario are important because of the increasing prevalence of smartphones and mobile search and recommendation user expect the system to recognize their location and provide information about their immediate surroundings in this work we propose an algorithm for constructing hyperlocal model of place that are a small a half a city block we show that dynamic location model dlms are computationally efficient and provide better estimate of the language model of hyperlocal place than the standard method of segmenting the globe into approximately equal grid square we evaluate the model using a repository of million geotagged public image from flickr we show that the index produced by dlms have a larger vocabulary and smaller average document length than their fixed grid counterpart for index with an equivalent number of location this produce location model that are more robust to retrieval parameter and more accurate in predicting location in text 
word segmentation is a challenging issue and the corresponding algorithm can be used in many application of natural language processing this paper address the problem of vietnamese word segmentation proposes a probabilistic ensemble learning pel framework and design a novel pel based word segmentation pelws algorithm supported by the data structure of syllable syllable frequency index the pelws algorithm combine multiple weak segmenters to form a strong segmenter within the pel framework the experimental result show that the pelws algorithm can achieve the state of the art performance in the vietnamese word segmentation task 
the vast amount of real time and social content in microblogs result in an information overload for user when searching microblog data given the user s search query delivering content that is relevant to her interest is a challenging problem traditional method for personalized web search are insufficient in the microblog domain because of the diversity of topic sparseness of user data and the highly social nature in particular social interaction between user need to be considered in order to accurately model user s interest alleviate data sparseness and tackle the cold start problem in this paper we therefore propose a novel framework for collaborative personalized twitter search at it core we develop a collaborative user model which exploit the user s social connection in order to obtain a comprehensive account of her preference we then propose a novel user model structure to manage the topical diversity in twitter and to enable semantic aware query disambiguation our framework integrates a variety of information about the user s preference in a principled manner a thorough evaluation is conducted using two personalized twitter search query log demonstrating a superior ranking performance of our framework compared with state of the art baseline 
reputation analysis is naturally linked to a sentiment analysis task of the targeted entity this analysis leverage on a sentiment lexicon that includes general sentiment word and domain specific jargon however in most case target entity are themselves part of the sentiment lexicon creating a loop from which it is difficult to infer an entity reputation sometimes the entity became a reference in the domain and is vastly cited a an example of a highly reputable entity for example in the movie domain it is not uncommon to see review citing batman or anthony hopkins a esteemed reference in this paper we describe an unsupervised method for performing a simultaneous analysis of the reputation of multiple named entity our method jointly extract named entity reputation and a domain specific sentiment lexicon the objective is two fold named entity are naturally ranked by our method and we can build a reputation graph of the domain s named entity this framework ha immediate application in term of visualization or search by reputation 
the web is an important resource for understanding and diagnosing medical condition based on exposure to online content people may develop undue health concern believing that common and benign symptom are explained by serious illness in this paper we investigate potential strategy to mine query and searcher history for clue that could help search engine choose the most appropriate information to present in response to exploratory medical query to do this we performed a longitudinal study of health search behavior using the log of a popular web search engine we found that query variation which might appear innocuous e g bad headache v severe headache may hold valuable information about the searcher which could be used by search engine to improve performance furthermore we investigated how medically concerned user respond differently to search engine result page serps and find that their disposition for clicking on concerning page is pronounced potentially leading to a self reinforcement of concern finally we studied to which degree variation in the serp impact future search and real world healthseeking behavior and obtained some surprising result e g viewing concerning page may lead to a short term reduction of in world healthcare utilization 
in recent year social medium ha become one of the most popular tool for discovering and following breaking news and ongoing event however tool and interface have lagged behind user expectation with current tool making it difficult to discover new event and failing to provide a solution to the problem of information overload we have developed an interactive interface for visualizing event backed by a state of the art event detection approach which is able to detect track and summarize event in real time our interface provides up to the second information about ongoing event in an easy to understand manner including category information temporal distribution and location information all of which wa previously unobtainable in real time 
we propose a framework which can perform web page segmentation with a structured prediction approach it formulates the segmentation task a a structured labeling problem on a transformed web page segmentation graph wps graph wps graph model the candidate segmentation boundary of a page and the dependency relation among the adjacent segmentation boundary each labeling scheme on the wps graph corresponds to a possible segmentation of the page the task of finding the optimal labeling of the wps graph is transformed into a binary integer linear programming problem which considers the entire wps graph a a whole to conduct structured prediction a learning algorithm based on the structured output support vector machine framework is developed to determine the feature weight which is capable to consider the inter dependency among candidate segmentation boundary furthermore we investigate it efficacy in supporting the development of automatic web page classification 
simrank is an attractive structural context measure of similarity between two object in a graph it recursively follows the intuition that two object are similar if they are referenced by similar object the best known matrix based method for calculating simrank however implies an assumption that the graph is non singular it adjacency matrix is invertible in reality non singular graph are very rare such an assumption in is too restrictive in practice in this paper we provide a treatment of by supporting similarity assessment on non invertible adjacency matrix assume that a singular graph g ha n node with r kr time for k iteration in contrast the only known matrix based algorithm that support singular graph need o r n time the experimental result on real and synthetic datasets demonstrate the superiority of invsr on singular graph against it baseline 
different important study in web search result clustering have recently shown increasing performance motivated by the use of external resource following this trend we present a new algorithm called dual c mean which provides a theoretical background for clustering in different representation space it originality relies on the fact that external resource can drive the clustering process a well a the labeling task in a single step to validate our hypothesis a series of experiment are conducted over different standard datasets and in particular over a new dataset built from the trec web track to take into account query log information the comprehensive empirical evaluation of the proposed approach demonstrates it significant advantage over traditional clustering and labeling technique 
we address the core challenge of the entity retrieval task ranking entity in response to a query by their presumed relevance to the information need that the query represents a an initial research direction we explored two model for entity ranking that were evaluated using the inex entity ranking dataset and which posted promising performance a natural future direction to explore is how to generalize these model to address various type of information need that are associated with entity 
we address the query performance prediction task for entity retrieval that is retrieval effectiveness is estimated with no relevance judgement first we show how to adapt state of the art query performance predictor proposed for document retrieval to the entity retrieval domain we then present a novel predictor that is based on the cluster hypothesis evaluation performed with the inex entity ranking track collection show that our predictor can often outperform the most effective predictor we experimented with 
understanding a text which wa written some time ago can be compared to translating a text from another language complete interpretation requires a mapping in this case a kind of time travel translation between present context knowledge and context knowledge at time of text creation in this paper we study time aware re contextualization the challenging problem of retrieving concise and complementing information in order to bridge this temporal context gap we propose an approach based on learning to rank technique using sentence level context information extracted from wikipedia the employed ranking combine relevance complimentarity and time awareness the effectiveness of the approach is evaluated by contextualizing article from a news archive collection using more than manually judged relevance pair to this end we show that our approach is able to retrieve a significant number of relevant context information for a given news article 
conventional approach to road hazard detection involve manual inspection of road by government transportation agency these approach are usually expensive to execute and sometimes are not able to capture the most recent hazard moreover they often only focus on major highway due to a lack of sufficient manpower consequently many hazard on minor road get ignored which may pose serious danger to driver in this paper we demonstrate an application of twitter to atomically determining road hazard by building language model based on twitter user online communication our system aim at pinpointing potential road hazard that pose driving risk the likelihood of poor driving condition can then be exposed via map overlay to warn driver about potentially dangerous driving condition in their locale or on current route thereby significantly reducing the chance of an accident occurring to the best of our knowledge this is the first work demonstrating the utility of social medium to automatically detect road hazard we conduct experiment on a testbed of tweet discussing road condition and the initial result demonstrate the effectiveness of our approach 
the somera workshop target cutting edge research from all field of retrieval recommendation and browsing in social medium a well a the analysis of user s multifaceted trace therein submission to the workshop cover a broad range of topic including multimedia retrieval and exploration user aware recommender system network analysis event detection and computational linguistics 
search result diversification ha gained attention a a way to tackle the ambiguous or multi faceted information need of user most existing method on this problem utilize a heuristic predefined ranking function where limited feature can be incorporated and extensive tuning is required for different setting in this paper we address search result diversification a a learning problem and introduce a novel relational learning to rank approach to formulate the task however the definition of ranking function and loss function for the diversification problem are challenging in our work we firstly show that diverse ranking is in general a sequential selection process from both empirical and theoretical aspect on this basis we define ranking function a the combination of relevance score and diversity score between the current document and those previously selected and loss function a the likelihood loss of ground truth based on plackett luce model which can naturally model the sequential generation of a diverse ranking list stochastic gradient descent is then employed to conduct the unconstrained optimization and the prediction of a diverse ranking list is provided by a sequential selection process based on the learned ranking function the experimental result on the public trec datasets demonstrate the effectiveness and robustness of our approach 
over the last two decade the information retrieval landscape ha changed dramatically twenty year ago there were fewer than k web site and the earliest web search engine indexed approximately k page today search engine index billion of web page image video news music social medium book etc and have become the main entry point for a wide range of information service communication and entertainment despite these tremendous accomplishment we still have a long way to go many search are unsuccessful and even those that succeed are often harder than they should be to address these challenge we need to extend our evaluation method to handle the diversity of searcher task and interactivity that characterize information system today i will discus recent work on user modeling and temporal dynamic of information system to illustrate the power of utilizing converging line of evidence from laboratory panel and large scale log technique to understand and support searcher 
we make the suggestion that instead of implementing custom index structure and query evaluation algorithm ir researcher should simply store document representation in a column oriented relational database and implement ranking model using sql for rapid prototyping this is particularly advantageous since researcher can explore new scoring function and feature by simply issuing sql query without needing to write imperative code we demonstrate the feasibility of this approach by an implementation of conjunctive bm using two modern column store experiment on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom built retrieval engine but provides many additional advantage including cleaner query semantics a simpler architecture built in support for error analysis and the ability to exploit advance in database technology for free 
user often reformulate or modify their query when they engage in searching information particularly when the search task is complex and exploratory this paper investigates query reformulation behavior in collaborative tourism information searching on the web a user study wa conducted with pair of participant and each pair worked a a team collaboratively on an exploratory travel search task in two scenario we analyzed user collaborative query cq reformulation behavior in two dimension firstly cq reformulation strategy and secondly the effect of individual query and chat log on cq reformulation the finding show that individual query and chat log were two major source of query term in cq reformulation the statistical result demonstrate the significant effect of individual query on cq reformulation we also found that five operation were performed to reformulate the cqs namely addition modification reordering addition and modification and addition and reordering these finding have implication for the design of query suggestion that could be offered to user during search using collaborative search tool 
searching for scene in team sport video is a task that recurs very often in game analysis and other related activity performed by coach in most case query are formulated on the basis of specific motion characteristic the user remembers from the video providing sketching interface for graphically specifying query input is thus a very natural user interaction for a retrieval application however the quality of the query the sketch heavily depends on the memory of the user and her ability to accurately formulate the intended search query by transforming this d memory of the known item s into a d sketch query in this paper we present an auto suggest search feature that harness spatiotemporal data of team sport video to suggest potential direction containing relevant data during the formulation of a sketch based motion query user can intuitively select the direction of the desired motion query on the fly using the displayed visual clue thus relaxing the need for relying heavily on memory to formulate the query at the same time this significantly enhances the accuracy of the result and the speed at which they appear a first evaluation ha shown the effectiveness and efficiency of our approach 
the tremendous increase of multimedia data in recent year ha heightened the need for system that not only allow to search with keywords but that also support content based retrieval in order to effectively and efficiently query large collection in this paper we introduce adam a system that is able to store and retrieve multimedia object by seamlessly combining aspect from database and information retrieval adam is able to work with both structured and unstructured data and to jointly provide boolean retrieval and similarity search to efficiently handle large volume of data it make use of a signature based indexing and the distribution of the collection to multiple shard that are queried in a mapreduce style we present adam in the setting of a sketch based image retrieval application using the imagenet collection containing million image 
we propose an approach for semantifying web extracted fact in particular we map subject and object term of these fact to instance and relational phrase to object property defined in a target knowledge base by doing this we resolve the ambiguity inherent in the web extracted fact while simultaneously enriching the target knowledge base with a significant number of new assertion in this paper we focus on the mapping of the relational phrase in the context of the overall work ow furthermore in an open extraction setting identical semantic relationship can be represented by different surface form making it necessary to group these surface form together to solve this problem we propose the use of markov clustering in this work we present a complete ontology independent generalized workflow which we evaluate on fact extracted by nell and reverb our target knowledge base is dbpedia our evaluation show promising result in term of producing highly precise fact moreover the result indicate that the clustering of relational phrase pay of in term of an improved instance and property mapping 
augmented reality ar browser are an emerging category of mobile application that add interactive virtual object to the user s view of the physical world this paper give the first system level evaluation of their security and privacy property we start by analyzing the functional requirement that ar browser must support in order to present ar content we then investigate the security architecture of junaio layar and wikitude browser which are running today on over million mobile device and identify new category of security and privacy vulnerability unique to ar browser finally we provide the first engineering guideline for securely implementing ar functionality 
contest are widely used a a mean for effort elicitation in setting ranging from government r d contest to online crowdsourcing contest on platform such a kaggle innocentive or topcoder such rank order mechanism where agent reward depend only on the relative ranking of their submission quality are natural mechanism for incentivizing effort when it is easier to obtain ordinal rather than cardinal information about agent output or where absolute measure of quality are unverifiable an increasing number of online contest however rank entry according to some numerical evaluation of their absolute quality for instance the performance of an algorithm on a test dataset or the performance of an intervention in a randomized trial can the contest designer incentivize higher effort by making the reward in an ordinal rank order mechanism contingent on such cardinal information we model and analyze cardinal contest where a principal running a rank order tournament ha access to an absolute measure of the quality of agent submission in addition to their relative ranking and ask how modifying the rank order tournament to incorporate cardinal information can improve incentive for effort our main result is that a simple threshold mechanism a mechanism that award the prize for a rank if and only if the absolute quality of the agent at that rank exceeds a certain threshold is optimal amongst all mixed cardinal ordinal mechanism where the fraction of the jth prize awarded to the jth ranked agent is any arbitrary non decreasing function of her submission s quality further the optimal threshold mechanism us exactly the same threshold for each rank we study what contest parameter determine the extent of the benefit from incorporating such cardinal information into an ordinal rank order contest and investigate the extent of improvement in equilibrium effort via numerical simulation 
we investigate the practice of website selling counterfeit good we inspect web search result for query across brand we devise a binary classifier that predicts whether a given website is selling counterfeit by examining automatically extracted feature such a whois information pricing and website content we then apply the classifier to result collected between january and august we find that overall of search result point to website selling fake for complicit search term such a replica rolex of the search result point to fake compared to for innocent term such a hermes buy online using a linear regression we find that brand with a higher street price for fake have higher incidence of counterfeit in search result but that brand who take active countermeasure such a filing dmca request experience lower incidence of counterfeit in search result finally we study how the incidence of counterfeit evolves over time finding that the fraction of search result pointing to fake remains remarkably stable 
voice activated intelligent assistant such a siri google now and cortana are prevalent on mobile device however it is challenging to evaluate them due to the varied and evolving number of task supported e g voice command web search and chat since each task may have it own procedure and a unique form of correct answer it is expensive to evaluate each task individually this paper is the first attempt to solve this challenge we develop consistent and automatic approach that can evaluate different task in voice activated intelligent assistant we use implicit feedback from user to predict whether user are satisfied with the intelligent assistant a well a it component i e speech recognition and intent classification using this approach we can potentially evaluate and compare different task within and across intelligent assistant ac cording to the predicted user satisfaction rate our approach is characterized by an automatic scheme of categorizing user system interaction into task independent dialog action e g the user is commanding selecting or confirming an action we use the action sequence in a session to predict user satisfaction and the quality of speech recognition and intent classification we also incorporate other feature to further improve our approach including feature derived from previous work on web search satisfaction prediction and those utilizing acoustic characteristic of voice request we evaluate our approach using data collected from a user study result show our approach can accurately identify satisfactory and unsatisfactory session 
we examine the first large real world data set on personal knowledge question s security and memorability from their deployment at google our analysis confirms that secret question generally offer a security level that is far lower than user chosen password it turn out to be even lower than proxy such a the real distribution of surname in the population would indicate surprisingly we found that a significant cause of this insecurity is that user often don t answer truthfully a user survey we conducted revealed that a significant fraction of user who admitted to providing fake answer did so in an attempt to make them harder to guess although on aggregate this behavior had the opposite effect a people harden their answer in the same and predictable way on the usability side we show that secret answer have surprisingly poor memorability despite the assumption that their reliability motivates their continued deployment from million of account recovery attempt we observed a significant fraction of user e g of our english speaking u user were unable to recall their answer when needed this is lower than the success rate of alternative recovery mechanism such a sm reset code over comparing question strength and memorability reveals that the question that are potentially the most secure e g what is your first phone number are also the one with the worst memorability we conclude that it appears next to impossible to find secret question that are both secure and memorable secret question continue have some use when combined with other signal but they should not be used alone and best practice should favor more reliable alternative 
the successful development and deployment of large scale internet service depends critically on performance even small regression in processing time can translate directly into significant energy and user experience cost despite the widespread use of distributed server infrastructure e g in cloud computing and web service there is little research on how to benchmark such system to obtain valid and precise inference with minimal data collection cost correctly a b testing distributed internet service can be surprisingly difficult because interdependency between user request e g for search result social medium stream photo and host server violate assumption required by standard statistical test we develop statistical model of distributed internet service performance based on data from perflab a production system used at facebook which vet thousand of change to the company s codebase each day we show how these model can be used to understand the tradeoff between different benchmarking routine and what factor must be taken into account when performing statistical test using simulation and empirical data from perflab we validate our theoretical result and provide easy to implement guideline for designing and analyzing such benchmark 
consider a user who submits a search query shakira having a specific search goal in mind such a her age but at the same time willing to explore information for other entity related to her such a comparable singer in previous work a system called spark wa developed to provide such search experience given a query submitted to the yahoo search engine spark provides related entity suggestion for the query exploiting among else public knowledge base from the semantic web we refer to this search scenario a explorative entity search the effectiveness and efficiency of the approach ha been demonstrated in previous work the way user interact with these related entity suggestion and whether this interaction can be predicted have however not been studied in this paper we perform a large scale analysis into how user interact with the entity result returned by spark we characterize the user query and session that appear to promote an explorative behavior based on this analysis we develop a set of query and user based feature that reflect the click behavior of user and explore their effectiveness in the context of a prediction task 
it is well known that the performance of web browsing a well a mobile application or apps suffers on today s cellular network in this work we perform a systematic measurement study of more than popular apps and cellular network and discover that while cellular network have predictable latency it is the path between exit point of cellular network e g ggsn and cloud server that degrades apps performance high latency and unpredictability over this path affect browsing and activity completion time of apps worsening the performance by several magnitude furthermore we find that a the number of apps on mobile device increase cellular network in turn suffer due to large number of active connection primarily used for push notification experiencing heavy signaling overhead in the network towards accelerating the performance of apps and improving their operational efficiency we envision an easy to deploy operator managed platform and study two architectural optimization that sit at vantage point inside cellular network virtual app server vapp and network assisted virtual push notification server vpns vapps improve apps browsing experience while vpnss take the burden of carrying periodic message off cellular network using trace driven simulation we find that vapps can improve activity completion time by more than fold whereas vpns can reduce the signaling load by a factor of in cellular network and reduce energy consumption by a factor of on mobile device 
the heterogeneous information network hin is a graph data model in which node and edge are annotated with class and relationship label large and complex datasets such a yago or dblp can be modeled a hin recent work ha studied how to make use of these rich information source in particular meta path which represent sequence of node class and edge type between two node in a hin have been proposed for such task a information retrieval decision making and product recommendation current method assume meta path are found by domain expert however in a large and complex hin retrieving meta path manually can be tedious and difficult we thus study how to discover meta path automatically specifically user are asked to provide example pair of node that exhibit high proximity we then investigate how to generate meta path that can best explain the relationship between these node pair since this problem is computationally intractable we propose a greedy algorithm to select the most relevant meta path we also present a data structure to enable efficient execution of this algorithm we further incorporate hierarchical relationship among node class in our solution extensive experiment on real world hin show that our approach capture important meta path in an efficient and scalable manner 
given a repeatedly issued query and a document with a not yet confirmed potential to satisfy the user need a search system should place this document on a high position in order to gather user feedback and obtain a more confident estimate of the document utility on the other hand the main objective of the search system is to maximize expected user satisfaction over a rather long period what requires showing more relevant document on average the state of the art approach to solving this exploration exploitation dilemma rely on strongly simplified setting making these approach infeasible in practice we improve the most flexible and pragmatic of them to handle some actual practical issue the first one is utilizing prior information about query and document the second is combining bandit based learning approach with a default production ranking algorithm we show experimentally that our framework enables to significantly improve the ranking of a leading commercial search engine 
many recommenders aim to provide relevant recommendation to user by building personal topic interest profile and then using these profile to find interesting content for the user in social medium recommender system build user profile by directly combining user topic interest signal from a wide variety of consumption and publishing behavior such a social medium post they authored commented on d or liked here we propose to separately model user topical interest that come from these various behavioral signal in order to construct better user profile intuitively since publishing a post requires more effort the topic interest coming from publishing signal should be more accurate of a user s central interest than say a simple gesture such a a by separating a single user s interest profile into several behavioral profile we obtain better and cleaner topic interest signal a well a enabling topic prediction for different type of behavior such a topic that the user might or comment on but might never write a post on that topic to do this at large scale in google we employed matrix factorization technique to model each user s behavior a a separate example entry in the input user by topic matrix using this technique which we call behavioral factorization we implemented and built a topic recommender predicting user s topical interest using their action within google we experimentally showed that we obtained better and cleaner signal than baseline method and are able to more accurately predict topic interest a well a achieve better coverage 
micro task crowdsourcing is rapidly gaining popularity among research community and business a a mean to leverage human computation in their daily operation unlike any other service a crowdsourcing platform is in fact a marketplace subject to human factor that affect it performance both in term of speed and quality indeed such factor shape the dynamic of the crowdsourcing market for example a known behavior of such market is that increasing the reward of a set of task would lead to faster result however it is still unclear how different dimension interact with each other reward task type market competition requester reputation etc in this paper we adopt a data driven approach to a perform a long term analysis of a popular micro task crowdsourcing platform and understand the evolution of it main actor worker requester and platform b we leverage the main finding of our five year log analysis to propose feature used in a predictive model aiming at determining the expected performance of any batch at a specific point in time we show that the number of task left in a batch and how recent the batch is are two key feature of the prediction c finally we conduct an analysis of the demand new task posted by the requester and supply number of task completed by the workforce and show how they affect task price on the marketplace 
researcher have shown that in recent year unwanted web tracking is on the rise with browser based fingerprinting being adopted by more and more website a a viable alternative to third party cooky in this paper we propose privaricator a solution to the problem of browser based fingerprinting a key insight is that when it come to web tracking the real problem with fingerprinting is not uniqueness of a fingerprint it is linkability i e the ability to connect the same fingerprint across multiple visit thus making fingerprint non deterministic also make them hard to link across browsing session in privaricator we use the power of randomization to break linkability by exploring a space of parameterized randomization policy we evaluate our technique in term of being able to prevent fingerprinting and not breaking existing benign site the best of our randomization policy render all the fingerprinters we tested ineffective while causing minimal damage on a set of alexa site on which we tested with no noticeable performance overhead 
complex network phenomenon such a information cascade in online social network are hard to fully observe model and forecast in forecasting a recent trend ha been to forgo the use of parsimonious model in favor of model with increasingly large degree of freedom that are trained to learn the behavior of a process from historical data extrapolating this trend into the future eventually we would renounce model all together but is it possible to forecast the evolution of a complex stochastic process directly from the data without a model in this work we show that model free forecasting is possible we present sed an algorithm that forecast process statistic based on relationship of statistical equivalence using two general axiom and historical data to the best of our knowledge sed is the first method that can perform axiomatic model free forecast of complex stochastic process our simulation using simple and complex evolving process and test performed on a large real world dataset show promising result 
densest subgraph computation ha emerged a an important primitive in a wide range of data analysis task such a community and event detection social medium such a facebook and twitter are highly dynamic with new friendship link and tweet being generated incessantly calling for efficient algorithm that can handle very large and highly dynamic input data while either scalable or dynamic algorithm for finding densest subgraphs have been proposed a viable and satisfactory solution for addressing both the dynamic aspect of the input data and it large size is still missing we study the densest subgraph problem in the the dynamic graph model for which we present the first scalable algorithm with provable guarantee in our model edge are added adversarially while they are removed uniformly at random from the current graph we show that at any point in time we are able to maintain a approximation of a current densest subgraph while requiring o polylog n r amortized cost per update with high probability where r is the total number of update operation executed and n is the maximum number of node in the graph in contrast a naive algorithm that recomputes a dense subgraph every time the graph change requires omega m work per update where m is the number of edge in the current graph our theoretical analysis is complemented with an extensive experimental evaluation on large real world graph showing that approximate densest subgraphs can be maintained efficiently within hundred of microsecond per update 
the proliferation of heterogeneous linked data on the web pose new challenge to database system in particular because of this heterogeneity the capacity to store track and query provenance data is becoming a pivotal feature of modern triple store in this paper we tackle the problem of efficiently executing provenance enabled query over rdf data we propose implement and empirically evaluate five different query execution strategy for rdf query that incorporate knowledge of provenance the evaluation is conducted on web data obtained from two different web crawl the billion triple challenge and the web data common our evaluation show that using an adaptive query materialization execution strategy performs best in our context interestingly we find that because provenance is prevalent within web data and is highly selective it can be used to improve query processing performance this is a counterintuitive result a provenance is often associated with additional overhead 
trending search topic cause unpredictable query load spike that hurt the end user search experience particularly the mobile one by introducing longer delay to understand how trending search topic are formed and evolve over time we analyze million query submitted during period where popular event caused search query volume spike based on our finding we design and evaluate pockettrend a system that automatically detects trending topic in real time identifies the search content associated to the topic and then intelligently push this content to user in a timely manner in that way pockettrend enables a client side search engine that can instantly answer user query related to trending event while at the same time reducing the impact of these trend on the datacenter workload our result using real mobile search log show that in the presence of a trending event up to of the overall search traffic can be eliminated from the datacenter with a many a of all user benefiting from pockettrend 
the frequently changing user preference and or item profile have put essential importance on the dynamic modeling of user and item in personalized recommender system however due to the insufficiency of per user item record when splitting the already sparse data across time dimension previous method have to restrict the drifting purchasing pattern to pre assumed distribution and were hardly able to model them rather directly with for example time series analysis integrating content information help to alleviate the problem in practical system but the domain dependent content knowledge is expensive to obtain due to the large amount of manual effort in this paper we make use of the large volume of textual review for the automatic extraction of domain knowledge namely the explicit feature aspect in a specific product domain we thus degrade the product level modeling of user preference which suffers from the lack of data to the feature level modeling which not only grant u the ability to predict user preference through direct time series analysis but also allows u to know the essence under the surface of product level change in purchasing pattern besides the expanded feature space also help to make cold start recommendation for user with few purchasing record technically we develop the fourier assisted auto regressive integrated moving average farima process to tackle with the year long seasonal period of purchasing data to achieve daily aware preference prediction and we leverage the conditional opportunity model for daily aware personalized recommendation extensive experimental result on real world cosmetic purchasing data from a major e commerce website jd com in china verified both the effectiveness and efficiency of our approach 
downside management is an important topic in the field of recommender system user satisfaction increase when good item are recommended but satisfaction drop significantly when bad recommendation are pushed to them for example a parent would be disappointed if violent movie are recommended to their kid and may stop using the recommendation system entirely a vegetarian would feel steak house recommendation useless a ceo in a mid sized company would feel offended by receiving intern level job recommendation under circumstance where there is penalty for a bad recommendation a bad recommendation is worse than no recommendation at all while most existing work focus on upside management recommending the best item to user this paper emphasizes on achieving better downside management reducing the recommendation of irrelevant or offensive item to user the approach we propose is general and can be applied to any scenario or domain where downside management is key to the system to tackle the problem we design a user latent preference model to predict the user preference in a specific dimension say the dietary restriction of the user the acceptable level of adult content in a movie or the geographical preference of a job seeker we propose to use multinomial regression a the core model and extend it with a hierarchical bayesian framework to address the problem of data sparsity after the user latent preference is predicted we leverage it to filter out downside item we validate the soundness of our approach by evaluating it with an anonymous job application dataset on linkedin the effectiveness of the latent preference model wa demonstrated in both offline experiment and online a b testing the user latent preference model help to improve the vpi view per impression and api application per impression significantly which in turn achieves a higher user satisfaction 
we present a novel method for open domain named entity extraction by exploiting the collective hidden structure in webpage title our method uncovers the hidden textual structure shared by set of webpage title based on generalized url pattern and a multiple sequence alignment technique the highlight of our method include the boundary of entity can be identified automatically in a collective way without any manually designed pattern seed or class name the connection between entity are also discovered naturally based on the hidden structure which make it easy to incorporate distant or weak supervision the experiment show that our method can harvest large scale of open domain entity with high precision a large ratio of the extracted entity are long tailed and complex and cover diverse topic given the extracted entity and their connection we further show the effectiveness of our method in a weakly supervised setting our method can produce better domain specific entity in both precision and recall compared with the state of the art approach 
better access to on line information graphic is a pressing need for people who are blind or have severe vision impairment we present a new model for accessible presentation of on line information graphic and demonstrate it use for presenting floor plan while floor plan are increasingly provided on line people who are blind are at best provided with only a high level textual description this make it difficult for them to understand the spatial arrangement of the object on the floor plan our new approach provides user with significantly better access to such plan the user can automatically generate an accessible version of a floor plan from an on line floor plan image quickly and independently by using a web service this generates a simplified graphic showing the room wall door and window in the original floor plan a well a a textual overview the accessible floor plan is presented on an ipad using audio feedback a the user touch graphic element on the screen the element they are touching is described by speech and non speech audio in order to help them navigate the graphic 
many previous technique identify trending topic in social medium even topic that are not pre defined we present a technique to identify trending rumor which we define a topic that include disputed factual claim putting aside any attempt to ass whether the rumor are true or false it is valuable to identify trending rumor a early a possible it is extremely difficult to accurately classify whether every individual post is or is not making a disputed factual claim we are able to identify trending rumor by recasting the problem a finding entire cluster of post whose topic is a disputed factual claim the key insight is that when there is a rumor even though most post do not raise question about it there may be a few that do if we can find signature text phrase that are used by a few people to express skepticism about factual claim and are rarely used to express anything else we can use those a detector for rumor cluster indeed we have found a few phrase that seem to be used exactly that way including is this true really and what relatively few post related to any particular rumor use any of these enquiry phrase but lot of rumor diffusion process have some post that do and have them quite early in the diffusion we have developed a technique based on searching for the enquiry phrase clustering similar post together and then collecting related post that do not contain these simple phrase we then rank the cluster by their likelihood of really containing a disputed factual claim the detector which search for the very rare but very informative phrase combined with clustering and a classifier on the cluster yield surprisingly good performance on a typical day of twitter about a third of the top cluster were judged to be rumor a high enough precision that human analyst might be willing to sift through them 
we consider the problem of learning distributed representation for document in data stream the document are represented a low dimensional vector and are jointly learned with distributed vector representation of word token using a hierarchical framework with two embedded neural language model in particular we exploit the context of document in stream and use one of the language model to model the document sequence and the other to model word sequence within them the model learn continuous vector representation for both word token and document such that semantically similar document and word are close in a common vector space we discus extension to our model which can be applied to personalized recommendation and social relationship mining by adding further user layer to the hierarchy thus learning user specific vector to represent individual preference we validated the learned representation on a public movie rating data set from movielens a well a on a large scale yahoo news data comprising three month of user activity log collected on yahoo server the result indicate that the proposed model can learn useful representation of both document and word token outperforming the current state of the art by a large margin 
knowledge on the web relies heavily on multi relational representation such a rdf and schema org automatically extracting knowledge from document and linking existing database are common approach to construct multi relational data complementary to such approach there is still a strong demand for manually encoding human expert knowledge for example human annotation is necessary for constructing a common sense knowledge base which store fact implicitly shared in a community because such knowledge rarely appears in document a human annotation is both tedious and costly an important research challenge is how to best use limited human resource while maximizing the quality of the resulting dataset in this paper we formalize the problem of dataset construction a active learning problem and present the active multi relational data construction amdc method amdc repeatedly interleaf multi relational learning and expert input acquisition allowing u to acquire helpful label for data construction experiment on real datasets demonstrate that our solution increase the number of positive triple by a factor of to and that the predictive performance of the multi relational model in amdc achieves the highest or comparable to the best performance throughout the data construction process 
in second price auction with symmetric bidder we find that improved targeting via enhanced information disclosure decrease revenue when there are two bidder and increase revenue if there are at least four bidder with asymmetry improved targeting increase revenue if the most frequent winner win le than of the time but can decrease revenue otherwise we derive analogous result for position auction finally we show that revenue can vary non monotonically with the number of bidder who are able to take advantage of improved targeting 
most semantic web application rely on querying graph typically by using sparql with a triple store increasingly application also analyze property of the graph structure to compute statistical inference the current semantic web infrastructure however doe not efficiently support such operation this force developer to extract the relevant data for external statistical post processing in this paper we propose to rethink query execution in a triple store a a highly parallelized asynchronous graph exploration on an active index data structure this approach also allows to integrate sparql querying with the sampling of graph property to evaluate this architecture we implemented random walk triplerush which is built on a distributed graph processing system our evaluation show that this architecture enables both competitive graph querying a well a the ability to execute various type of random walk with restarts that sample interesting graph property thanks to the asynchronous architecture first result are sometimes returned in a fraction of the full execution time we also evaluate the scalability and show that the architecture support fast query time on a dataset with more than a billion triple 
recognizing entity instance in document according to a knowledge base is a fundamental problem in many data mining application the problem is extremely challenging for short document in complex domain such a social medium and biomedical domain large concept space and instance ambiguity are key issue that need to be addressed most of the document are created in a social context by common author via social interaction such a reply and citation such social context are largely ignored in the instance recognition literature how can user interaction help entity instance recognition how can the social context be modeled so a to resolve the ambiguity of different instance in this paper we propose the socinst model to formalize the problem into a probabilistic model given a set of short document e g tweet or paper abstract posted by user who may connect with each other socinst can automatically construct a context of subtopics for each instance with each subtopic representing one possible meaning of the instance the model is also able to incorporate social relationship between user to help build social context we further incorporate domain knowledge into the model using a dirichlet tree distribution we evaluate the proposed model on three different genre of datasets icdm contest weibo and i b in icdm contest the proposed model clearly outperforms p l e with t test all the top contestant in weibo and i b our result also show that the recognition accuracy of socinst is up to better than those of several alternative method 
the linked data principle provide a decentral approach for publishing structured data in the rdf format on the web in contrast to structured data published in relational database where a key is often provided explicitly finding a set of property that allows identifying a resource uniquely is a non trivial task still finding key is of central importance for manifold application such a resource deduplication link discovery logical data compression and data integration in this paper we address this research gap by specifying a refinement operator dubbed rocker which we prove to be finite proper and non redundant we combine the theoretical characteristic of this operator with two monotonicities of key to obtain a time efficient approach for detecting key i e set of property that describe resource uniquely we then utilize a hash index to compute the discriminability score efficiently therewith we ensure that our approach can scale to very large knowledge base result show that rocker yield more accurate result ha a comparable runtime and consumes le memory w r t existing state of the art technique 
given the abundance of online information available to mobile user particularly tourist and weekend traveler recommender system that effectively filter this information and suggest interesting participatory opportunity will become increasingly important previous work ha explored recommending interesting location however user would also benefit from recommendation for activity in which to participate at those location along with suitable time and day thus system that provide collaborative recommendation involving multiple dimension such a location activity and time would enhance the overall experience of user the relationship among these dimension can be modeled by higher order matrix called tensor which are then solved by tensor factorization however these tensor can be extremely sparse in this paper we present a system and an approach for performing multi dimensional collaborative recommendation for who user what activity when time and where location using tensor factorization on sparse user generated data we formulate an objective function which simultaneously factorizes coupled tensor and matrix constructed from heterogeneous data source we evaluate our system and approach on large scale real world data set consisting of flickr photo collected from three major metro region in usa we compare our approach with several state of the art baseline and demonstrate that it outperforms all of them 
child s online privacy ha garnered much attention in medium legislation and industry adult are concerned that child may not adequately protect themselves online however relatively little discussion ha focused on the privacy breach that may occur to child at the hand of others namely their parent and relative when adult post information online they may reveal personal information about their child to other people online service data broker or surveillant authority this information can be gathered in an automated fashion and then linked with other online and offline source creating detailed profile which can be continually enhanced throughout the child s life in this paper we conduct a study to see how widespread these behavior are among adult on facebook and instagram we use a number of method firstly we automate a process to examine adult user on facebook for evidence of child in their public photo album using the associated comment in combination with publicly available voter registration record we are able to infer child s name face birth date and address secondly in order to understand what additional information is available to facebook and the user friend we survey adult facebook user about their behavior and attitude with regard to posting their child s information online thirdly we analyze user on instagram to infer fact about their child finally we make recommendation for privacy conscious parent and suggest an interface change through which facebook can nudge parent towards better stewardship of their child s privacy 
microblogging platform such a twitter have recently received much attention a great source for live web sensing real time event detection and opinion analysis previous work usually assumed that tweet mainly describe what s happening now however a large portion of tweet contains time expression that refer to time frame within the past or the future such message often reflect expectation or memory of social medium user in this work we investigate how microblogging user collectively refer to time in particular we analyze half a year long portion of japanese and four month long collection of u tweet and we quantify collective temporal attention of user a well a other related temporal characteristic this kind of knowledge is helpful in the context of growing interest for detection and prediction of important event within social medium the exploratory analysis we perform is possible thanks to the development of visual analytics framework for robust overview and easy detection of various regularity in the past and future oriented thinking of twitter user we believe that the visualization we provide and the finding we outline can be also valuable for sociologist and computer scientist to test and refine their model about time in natural language 
recent online service rely heavily on automatic personalization to recommend relevant content to a large number of user this requires system to scale promptly to accommodate the stream of new user visiting the online service for the first time in this work we propose a content based recommendation system to address both the recommendation quality and the system scalability we propose to use a rich feature set to represent user according to their web browsing history and search query we use a deep learning approach to map user and item to a latent space where the similarity between user and their preferred item is maximized we extend the model to jointly learn from feature of item from different domain and user feature by introducing a multi view deep learning model we show how to make this rich feature based user representation scalable by reducing the dimension of the input and the amount of training data the rich user feature representation allows the model to learn relevant user behavior pattern and give useful recommendation for user who do not have any interaction with the service given that they have adequate search and browsing history the combination of different domain into a single model for learning help improve the recommendation quality across all the domain a well a having a more compact and a semantically richer user latent feature vector we experiment with our approach on three real world recommendation system acquired from different source of microsoft product window apps recommendation news recommendation and movie tv recommendation result indicate that our approach is significantly better than the state of the art algorithm up to enhancement on existing user and enhancement on new user in addition experiment on a publicly open data set also indicate the superiority of our method in comparison with transitional generative topic model for modeling cross domain recommender system scalability analysis show that our multi view dnn model can easily scale to encompass million of user and billion of item entry experimental result also confirm that combining feature from all domain produce much better performance than building separate model for each domain 
the proliferation of the web present an unsolved problem of automatically analyzing billion of page of natural language we introduce a scalable algorithm that cluster hundred of million of web page into hundred of thousand of cluster it doe this on a single mid range machine using efficient algorithm and compressed document representation it is applied to two web scale crawl covering ten of terabyte clueweb and clueweb contain and million web page and were clustered into to cluster to the best of our knowledge such fine grained clustering ha not been previously demonstrated previous approach clustered a sample that limit the maximum number of discoverable cluster the proposed em tree algorithm us the entire collection in clustering and produce several order of magnitude more cluster than the existing algorithm fine grained clustering is necessary for meaningful clustering in massive collection where the number of distinct topic grows linearly with collection size these fine grained cluster show an improved cluster quality when assessed with two novel evaluation using ad hoc search relevance judgment and spam classification for external validation these evaluation solve the problem of assessing the quality of cluster where categorical labeling is unavailable and unfeasible 
with broadband penetration rate of le than per caput tribal area in the u s represent some of the most underserved community in term of internet access although numerous source have identified this digital divide there have been no empirical measurement of the performance and usage of service that do exist in these area in this paper we present the characterization of the tribal digital village tdv network a multi hop wireless network currently connecting reservation in san diego county this work represents the first traffic analysis of broadband usage in tribal land after identifying some of the unique purpose of broadband connectivity in indigenous community such a language revitalization and cultural development we focus on the performance of popular application that enable such activity including youtube and instagram though only a fraction of the bandwidth capacity is actually used of youtube uploads and of instagram uploads fail due to packet loss on the relay and access link that connect the reservation to the tdv backbone although failure rate are prohibitive to the contribution of locally generated medium particularly video our analysis of instagram medium interaction and engagement in the tdv network reveals a high locality of interest resident engage with locally created medium time more than medium created by outside source furthermore locally created medium circulates through the network two day longer than non local medium the result of our analysis point to new direction for increasing content availability on reservation 
web scale information network containing billion of entity are common nowadays querying these network can be modeled a a subgraph matching problem since information network are incomplete and noisy in nature it is important to discover answer that match exactly a well a answer that are similar to query existing graph matching algorithm usually use graph index to improve the efficiency of query processing for web scale information network it may not be feasible to build the graph index due to the amount of work and the memory storage required in this paper we propose an efficient algorithm for finding the best k answer for a given query without precomputing graph index the quality of an answer is measured by a matching score that is computed online to speed up query processing we propose a novel technique for bounding the matching score during the computation by using bound we can efficiently prune the answer that have low quality without having to evaluate all possible answer the bounding technique can be implemented in a distributed environment allowing our approach to efficiently answer the query on web scale information network we demonstrate the effectiveness and the efficiency of our approach through a series of experiment on real world information network the result show that our bounding technique can reduce the running time up to two order of magnitude comparing to an approach that doe not use bound 
in this paper we detail our effort at creating and running a controlled study designed to examine how student in a mooc might be motivated to do a better job during peer grading this study involves more than one thousand student of a popular mooc we ask two specific question when a student know that his or her own peer grading effort are being examined by peer doe this knowledge alone tend to motivate the student to do a better job when grading assignment and when a student not only know that his or her own peer grading effort are being examined by peer but he or she is also given a number of other peer grading effort to evaluate so the peer grader see how other peer grader evaluate assignment do both of these together tend to motivate the student to do a better job when grading assignment we find strong statistical evidence that grading the grader doe in fact tend to increase the quality of peer grading 
a common complaint about online auction for consumer good is the presence of sniper who place bid in the final second of sequential ascending auction with predetermined ending time the literature conjecture that sniper are best responding to the existence of incremental bidder that bid up to their valuation only a they are outbid sniper aim to catch these incremental bidder at a price below their reserve with no time to respond a a consequence these incremental bidder may experience regret when they are outbid at the last moment at a price below their reservation value we measure the effect of this experience on a new buyer s propensity to participate in future auction we show the effect to be causal using a carefully selected subset of auction from ebay com and instrumental variable estimation strategy bidder respond to sniping quite strongly and are between and percent le likely to return to the platform 
entity linking connects the web of document with knowledge base it is the task of linking an entity mention in text to it corresponding entity in a knowledge base whereas a large body of work ha been devoted to automatically generating candidate entity or ranking and choosing from them manual effort are still needed e g for defining gold standard link for evaluating automatic approach and for improving the quality of link in crowdsourcing approach however structured description of entity in knowledge base are sometimes very long to avoid overloading human user with too much information and help them more efficiently choose an entity from candidate we aim to substitute entire entity description with compact equally effective structured summary that are automatically generated to achieve it our approach analyzes entity description in the knowledge base and the context of entity mention from multiple perspective including characterizing and differentiating power information overlap and relevance to context extrinsic evaluation where human user carry out entity linking task and intrinsic evaluation where human user rate summary demonstrate that summary generated by our approach help human user carry out entity linking task more efficiently faster without significantly affecting the quality of link obtained and our approach outperforms existing approach to summarizing entity description 
many data processing task such a semantic annotation of image translation of text in foreign language and labeling of training data for machine learning model require human input and on a large scale can only be accurately solved using crowd based online work recent work show that framework where crowd worker compete against each other can drastically reduce crowdsourcing cost and outperform conventional reward scheme where the payment of online worker is proportional to the number of accomplished task pay per task in this paper we investigate how team mechanism can be leveraged to further improve the cost efficiency of crowdsourcing competition to this end we introduce strategy for team based crowdsourcing ranging from team formation process where worker are randomly assigned to competing team over strategy involving self organization where worker actively participate in team building to combination of team and individual competition our large scale experimental evaluation with more than participant and overall hour of work spent by crowd worker demonstrates that our team based crowdsourcing mechanism are well accepted by online worker and lead to substantial performance boost 
many modern desktop and mobile platform including ubuntu google chrome window and firefox o support so called web based system application that run outside the web browser and enjoy direct access to native object such a file camera and geolocation we show that the access control model of these platform are a incompatible and b prone to unintended delegation of native access right when application request native access for their own code they unintentionally enable it for untrusted third party code too this enables malicious ad and other third party content to steal user oauth authentication credential access camera on their device etc we then design implement and evaluate powergate a new access control mechanism for web based system application it solves two key problem plaguing all existing platform security and consistency first unlike the existing platform powergate correctly protects native object from unauthorized access second powergate provides uniform access control semantics across all platform and is backward compatible powergate enables application developer to write well defined native object access policy with explicit principal such a application s own local code and third party web code is easy to configure and incurs negligible performance overhead 
ad hoc keyword search engine built using modern information retrieval method do a good job of handling fine grained query however they perform poorly at facilitating spatial and spatially embedded thematic exploration of the result despite the fact that many query e g civil war refer to different document and topic in different place this is not for lack of data geographic information such a place name event and coordinate are common in unstructured document collection on the web the association between geographic and thematic content in these document can provide a rich groundwork to organize information for exploratory research in this paper we describe the architecture of an interactive thematic map search engine frankenplace designed to facilitate document exploration at the intersection of theme and place the map interface enables a user to zoom the geographic context of their query in and out and quickly explore through thousand of search result in a meaningful way and by combining topic model with geographically contextualized search result user can discover related topic based on geographic context frankenplace utilizes a novel indexing method called geoboost for boosting term associated with cell on a discrete global grid the resulting index factor in the geographic scale of the place or feature mentioned in related text the relative textual scope of the place reference and the overall importance of the containing document in the document network the system is currently indexed with over million document from the web including the english wikipedia and online travel blog entry we demonstrate that frankenplace can support four distinct type of exploratory search task while being adaptive to scale and location of interest 
many of the world s most popular website catalyze their growth through invitation from existing member new member can then in turn issue invitation and so on creating cascade of member signups that can spread on a global scale although these diffu sive invitation process are critical to the popularity and growth of many website they have rarely been studied and their property remain elusive for instance it is not known how viral these cascade structure are how cascade grow over time or how diffusive growth affect the resulting distribution of member characteristic present on the site in this paper we study the diffusion of linkedin an online professional network comprising over million member a large fraction of whom joined the site a part of a signup cascade first we analyze the structural pattern of these signup cascade and find them to be qualitatively different from previously studied information diffusion cascade we also examine how signup cascade grow over time and observe that diffusion via invitation on linkedin occurs over much longer timescales than are typically associated with other type of online diffusion finally we connect the cascade structure with rich individual level attribute data to investigate the interplay between the two using novel technique to study the role of homophily in diffusion we find striking difference between the local edge wise homophily and the global cascade level homophily we observe in our data suggesting that signup cascade form surprisingly coherent group of member 
advertising is a significant source of revenue for most online social network conventional online advertising method need to be customized for online social network in order to address their distinct characteristic recent experimental study have shown that providing social cue along with ad e g information about friend liking the ad or clicking on an ad lead to higher click rate in other word the probability of a user clicking an ad is a function of the set of friend that have clicked the ad in this work we propose formal probabilistic model to capture this phenomenon and study the algorithmic problem that then arises our work is in the context of display advertising where a contract is signed to show an ad to a pre determined number of user the problem we study is the following given a certain number of impression what is the optimal display strategy i e the optimal order and the subset of user to show the ad to so a to maximize the expected number of click unlike previous model of influence maximization we show that this optimization problem is hard to approximate in general and that it is related to finding dense subgraphs of a given size in light of the hardness result we propose several heuristic algorithm including a two stage algorithm inspired by influence and exploit strategy in viral marketing we evaluate the performance of these heuristic on real data set and observe that our two stage heuristic significantly outperforms the natural baseline 
the advent of mobile device and medium cloud service ha led to the unprecedented growing of personal photo collection one of the fundamental problem in managing the increasing number of photo is automatic image tagging existing research ha predominantly focused on tagging general web image with a well labelled image database e g imagenet however they can only achieve limited success on personal photo due to the domain gap between personal photo and web image these gap originate from the difference in semantic distribution and visual appearance to deal with these challenge in this paper we present a novel transfer deep learning approach to tag personal photo specifically to solve the semantic distribution gap we have designed an ontology consisting of a hierarchical vocabulary tailored for personal photo this ontology is mined from active user in flickr with million photo and million unique tag to deal with the visual appearance gap we discover the intermediate image representation and ontology prior by deep learning with bottom up and top down transfer across two domain where web image are the source domain and personal photo are the target moreover we present two mode single and batch mode in tagging and find that the batch mode is highly effective to tag photo collection we conducted personal photo tagging on real personal photo and personal photo search on the mit adobe fivek photo dataset the proposed tagging approach is able to achieve a performance gain of and in term of ndcg against the state of the art hand crafted feature based and deep learning based method respectively 
the web browser is a killer app on mobile device such a smartphones however the user experience of mobile web browsing is undesirable because of the slow resource loading to improve the performance of web resource loading caching ha been adopted a a key mechanism however the existing passive measurement study cannot comprehensively characterize the performance of mobile web caching for example most of these study mainly focus on client side implementation but not server side configuration suffer from biased user behavior and fail to study miscached resource to address these issue in this paper we present a proactive approach for a comprehensive measurement study on mobile web cache performance the key idea of our approach is to proactively crawl resource from hundred of website periodically with a fine grained time interval thus we are able to uncover the resource update history and cache configuration at the server side and analyze the cache performance in various time granularity based on our collected data we build a new cache analysis model and study the upper bound of how high percentage of resource could potentially be cached and how effective the caching work in practice we report detailed analysis result of different website and various type of web resource and identify the problem caused by unsatisfactory cache performance in particular we identify two major problem redundant transfer and miscached resource which lead to unsatisfactory cache performance we investigate three main root cause same content heuristic expiration and conservative expiration time and discus what mobile web developer can do to mitigate those problem 
opinion spamming refers to the illegal marketing practice which involves delivering commercially advantageous opinion a regular user in this paper we conduct a real case study based on a set of internal record of opinion spam leaked from a shady marketing campaign we explore the characteristic of opinion spam and spammer in a web forum to obtain some insight including subtlety property of opinion spam spam post ratio spammer account first post and reply submission time of post activeness of thread and collusion among spammer then we present feature that could be potentially helpful in detecting spam opinion in thread the result of spam detection on first post show spam first post put more focus on certain topic such a the user experience on the promoted item spam first post generally use more word and picture to showcase the promoted item in an attempt to impress people spam first post tend to be submitted during work time and the thread that spam first post initiate are more active to be placed at striking position the spam detection on reply is more challenging besides lower spam ratio and le content reply even do not mention the promoted item their major intention is to keep the discussion in a thread alive to attract more attention on it submission time of reply thread activeness position of reply and spamicity of first post are more useful than content based feature in spam detection on reply 
implicit feedback from user of a web search engine is an essential source providing consistent personal relevance label from the actual population of user however previous study on personalized search employ this source in a rather straightforward manner basically document that were clicked on get maximal gain and the rest of the document are assigned the zero gain a we demonstrate in our paper a ranking algorithm trained using these gain directly a the ground truth relevance label lead to a suboptimal personalized ranking in this paper we develop a framework for automatic reweighting of these label our approach is based on more subtle aspect of user interaction with the result page we propose an efficient methodology for deriving confidence level for relevance label that relies directly on the objective ranking measure all our algorithm are evaluated on a large scale query log provided by a major commercial search engine the result of the experiment prove that the current state of the art personalization approach could be significantly improved by enriching relevance grade with weight extracted from post impression user behavior 
we study a natural generalization of the correlation clustering problem to graph in which the pairwise relation between object are categorical instead of binary this problem wa recently introduced by bonchi et al under the name of chromatic correlation clustering and is motivated by many real world application in data mining and social network including community detection link classification and entity de duplication our main contribution is a fast and easy to implement constant approximation framework for the problem which build on a novel reduction of the problem to that of correlation clustering this result significantly progress the current state of knowledge for the problem improving on a previous result that only guaranteed linear approximation in the input size we complement the above result by developing a linear programming based algorithm that achieves an improved approximation ratio of although this algorithm cannot be considered to be practical it further extends our theoretical understanding of chromatic correlation clustering we also present a fast heuristic algorithm that is motivated by real life scenario in which there is a ground truth clustering that is obscured by noisy observation we test our algorithm on both synthetic and real datasets like social network data our experiment reinforce the theoretical finding by demonstrating that our algorithm generally outperform previous approach both in term of solution cost and reconstruction of an underlying ground truth clustering 
numerous graph mining application rely on detecting subgraphs which are large near clique since formulation that are geared towards finding large near clique are hard and frequently inapproximable due to connection with the maximum clique problem the poly time solvable densest subgraph problem which maximizes the average degree over all possible subgraphs lie at the core of large scale data mining however frequently the densest subgraph problem fails in detecting large near clique in network in this work we introduce the k clique densest subgraph problem k this generalizes the well studied densest subgraph problem which is obtained a a special case for k for k we obtain a novel formulation which we refer to a the triangle densest subgraph problem given a graph g v e find a subset of vertex s such that s max limit v t s s where t s is the number of triangle induced by the set s on the theory side we prove that for any k constant there exist an exact polynomial time algorithm for the k clique densest subgraph problem furthermore we propose an efficient k approximation algorithm which generalizes the greedy peeling algorithm of asahiro and charikar for k finally we show how to implement efficiently this peeling framework on mapreduce for any k generalizing the work of bahmani kumar and vassilvitskii for the case k on the empirical side our two main finding are that i the triangle densest subgraph is consistently closer to being a large near clique compared to the densest subgraph and ii the peeling approximation algorithm for both k and k achieve on real world network approximation ratio closer to rather than the pessimistic k guarantee an interesting consequence of our work is that triangle counting a well studied computational problem in the context of social network analysis can be used to detect large near clique finally we evaluate our proposed method on a popular graph mining application 
we motivate and describe technique that allow to detect an emergent relational schema from rdf data we show that on a wide variety of datasets the found structure explains well over of the rdf triple further we also describe technical solution to the semantic challenge to give short name that human find logical to these emergent table column and relationship between table our technique can be exploited in many way e g to improve the efficiency of sparql system or to use existing sql based application on top of any rdf dataset using a rdbms 
storing and searching large labeled graph is indeed becoming a key issue in the design of space time efficient online platform indexing modern social network or knowledge graph but a far a we know all these result are limited to design compressed graph index which support basic access operation onto the link structure of the input graph such a given a node u return the adjacency list of u this paper take inspiration from the facebook unicorn s platform and proposes some compressed indexing scheme for large graph whose node are labeled with string of variable length i e node s attribute such a user s nick name that support sophisticated search operation which involve both the linked structure of the graph and the string content of it node an extensive experimental evaluation over real social network will show the time and space efficiency of the proposed indexing scheme and their query processing algorithm 
we study the ability of a passive eavesdropper to leverage third party http tracking cooky for mass surveillance if two web page embed the same tracker which tag the browser with a unique cookie then the adversary can link visit to those page from the same user i e browser instance even if the user s ip address varies further many popular website leak a logged in user s identity to an eavesdropper in unencrypted traffic to evaluate the effectiveness of our attack we introduce a methodology that combine web measurement and network measurement using openwpm our web privacy measurement platform we simulate user browsing the web and find that the adversary can reconstruct of a typical user s browsing history we then analyze the effect of the physical location of the wiretap a well a legal restriction such a the nsa s one end foreign rule using measurement unit in various location asia europe and the united state we show that foreign user are highly vulnerable to the nsa s dragnet surveillance due to the concentration of third party tracker in the u s finally we find that some browser based privacy tool mitigate the attack while others are largely ineffective 
we investigate current deployment practice for virtual hosting a widely used method for serving multiple http and http origin from the same server in popular content delivery network cloud hosting infrastructure and web server our study uncovers a new class of http origin confusion attack when two virtual host use the same tl certificate or share a tl session cache or ticket encryption key a network attacker may cause a page from one of them to be loaded under the other s origin in a client browser these attack appear when http server are configured to allow virtual host fallback from a client requested secure origin to some other unexpected le secure origin we present evidence that such vulnerable virtual host configuration are widespread even on the most popular and security scrutinized website thus allowing a network adversary to hijack page or steal secure cooky and single sign on token to prevent our virtual host confusion attack and recover the isolation guarantee that are commonly assumed in shared hosting environment we propose fix to web server software and advocate conservative configuration guideline for the composition of http with tl 
wifi based indoor positioning ha recently gained more attention due to the advent of the ieee v standard requirement by the fcc for e call and increased interest in location based service while there exist several indoor localization technique we find that these technique tradeoff either accuracy scalability pervasiveness or cost all of which are important requirement for a truly deployable positioning solution wireless signal strength based approach suffer from location error whereas time of flight tof based solution provide good accuracy but are not scalable recent solution address these issue by augmenting wifi with either smartphone sensing or mobile crowdsourcing however they require tight coupling between wifi infrastructure and a client device or they can determine the client s location only if it is mobile in this paper we present cupid which improved our previously proposed cupid indoor positioning system to overcome these limitation we achieve this by addressing the fundamental limitation in time of flight based localization and combining tof with signal strength to address scalability experiment from city using different mobile device comprising of more than million location fix demonstrate feasibility cupid is currently under production and we expect cupid to ignite the wide adoption of wlan based positioning system and their service 
most recent question answering qa system query large scale knowledge base kb to answer a question after parsing and transforming natural language question to kb executable form e g logical form a a well known fact kb are far from complete so that information required to answer question may not always exist in kb in this paper we develop a new qa system that mine answer directly from the web and meanwhile employ kb a a significant auxiliary to further boost the qa performance specifically to the best of our knowledge we make the first attempt to link answer candidate to entity in freebase during answer candidate generation several remarkable advantage follow redundancy among answer candidate is automatically reduced the type of an answer candidate can be effortlessly determined by those of it corresponding entity in freebase capitalizing on the rich information about entity in freebase we can develop semantic feature for each answer candidate after linking them to freebase particularly we construct answer type related feature with two novel probabilistic model which directly evaluate the appropriateness of an answer candidate s type under a given question overall such semantic feature turn out to play significant role in determining the true answer from the large answer candidate pool the experimental result show that across two testing datasets our qa system achieves an improvement under f metric compared with various existing qa system 
modern internet company improve their service by mean of data driven decision that are based on online controlled experiment also known a a b test to run more online controlled experiment and to get statistically significant result faster are the emerging need for these company the main way to achieve these goal is to improve the sensitivity of a b experiment we propose a novel approach to improve the sensitivity of user engagement metric that are widely used in a b test by utilizing prediction of the future behavior of an individual user this problem of prediction of the exact value of a user engagement metric is also novel and is studied in our work we demonstrate the effectiveness of our sensitivity improvement approach on several real online experiment run at yandex especially we show how it can be used to detect the treatment effect of an a b test faster with the same level of statistical significance 
query issued to a search engine are often under specified or ambiguous the user s search context or background may provide information that disambiguates their information need in order to automatically predict and issue a more effective query the disambiguation can take place at different stage of the retrieval process for instance contextual query suggestion may be computed and recommended to user on the result page when appropriate an approach that doe not require modifying the original query s result alternatively the search engine can attempt to provide efficient access to new relevant document by injecting these document directly into search result based on the user s context in this paper we explore these complementary approach and how they might be combined we first develop a general framework for mining context sensitive query reformulations for query suggestion we evaluate our context sensitive suggestion against a state of the art baseline using a click based metric the resulting query suggestion generated by our approach outperform the baseline by overall and by on an ambiguous query subset while the query suggestion generated by our approach have higher quality than the existing baseline we demonstrate that using them naively for injecting new document into search result can lead to inferior ranking to remedy this issue we develop a classifier that decides when to inject new search result using feature based on suggestion quality and user context we show that our context sensitive result fusion approach corfu improves retrieval quality for ambiguous query by up to our approach can efficiently scale to massive search log enabling a data driven strategy that benefit from observing how user issue and reformulate query in different context 
the spanning centrality of an edge e in an undirected graph g is the fraction of the spanning tree of g that contain e despite it appealing definition and apparent value in certain application in computational biology spanning centrality hasn t so far received a wider attention a a measure of edge centrality we may partially attribute this to the perceived complexity of computing it which appears to be prohibitive for very large network contrary to this intuition spanning centrality can in fact be approximated arbitrary well by very efficient near linear time algorithm due to spielman and srivastava combined with progress in linear system solver in this article we bring theory into practice with careful and optimized implementation that allow the fast computation of spanning centrality in very large graph with million of node with this computational tool in our disposition we demonstrate experimentally that spanning centrality is in fact a useful tool for the analysis of large network specifically we show that relative to common centrality measure spanning centrality is more effective in identifying edge whose removal cause a higher disruption in an information propagation procedure while being very resilient to noise in term of both the edge score and the resulting edge ranking 
given a large collection of co evolving online activity such a search for the keywords xbox playstation and wii how can we find pattern and rule are these keywords related if so are they competing against each other can we forecast the volume of user activity for the coming month we conjecture that online activity compete for user attention in the same way that specie in an ecosystem compete for food we present ecoweb i e ecosystem on the web which is an intuitive model designed a a non linear dynamical system for mining large scale co evolving online activity our second contribution is a novel parameter free and scalable fitting algorithm ecoweb fit that estimate the parameter of ecoweb extensive experiment on real data show that ecoweb is effective in that it can capture long range dynamic and meaningful pattern such a seasonalities and practical in that it can provide accurate long range forecast ecoweb consistently outperforms existing method in term of both accuracy and execution speed 
semantic tagging of mathematical expression stme give semantic meaning to token in mathematical expression in this work we propose a novel stme approach that relies on neither text along with expression nor labelled training data instead our method only requires a mathematical grammar set we point out that besides the grammar of mathematics the special property of variable and user habit of writing expression help u understand the implicit intent of the user we build a system that considers both restriction from the grammar and variable property and then apply an unsupervised method to our probabilistic model to learn the user habit to evaluate our system we build large scale training and test datasets automatically from a public math forum the result demonstrate the significant improvement of our method compared to the maximum frequency baseline we also create statistic to reveal the property of mathematics language 
the goal of collaborative filtering is to get accurate recommendation at the top of the list for a set of user from such a perspective collaborative ranking based formulation with suitable ranking loss function are natural while recent literature ha explored the idea based on objective function such a ndcg or average precision such objective are difficult to optimize directly in this paper building on recent advance from the learning to rank literature we introduce a novel family of collaborative ranking algorithm which focus on accuracy at the top of the list for each user while learning the ranking function collaboratively we consider three specific formulation based on collaborative p norm push infinite push and reverse height push and propose efficient optimization method for learning these model experimental result illustrate the value of collaborative ranking and show that the proposed method are competitive usually better than existing popular approach to personalized recommendation 
twitter contains a wealth of timely information however staying on top of breaking event requires that an information analyst constantly scan many source leading to information overload for example a user might wish to be made aware whenever an infectious disease outbreak take place when a new smartphone is announced or when a distributed denial of service do attack might affect an organization s network connectivity there are many possible event category an analyst may wish to track making it impossible to anticipate all those of interest in advance we therefore propose a weakly supervised approach in which extractor for new category of event are easy to define and train by specifying a small number of seed example we cast seed based event extraction a a learning problem where only positive and unlabeled data is available rather than assuming unlabeled instance are negative a is common in previous work we propose a learning objective which regularizes the label distribution towards a user provided expectation our approach greatly outperforms heuristic negative used in most previous work in experiment on real world data significant performance gain are also demonstrated over two novel and competitive baseline semi supervised em and one class support vector machine we investigate three security related event breaking on twitter do attack data breach and account hijacking a demonstration of security event extracted by our system is available at http kb cse ohio state edu event hacked 
minwise hashing minhash is a widely popular indexing scheme in practice minhash is designed for estimating set resemblance and is known to be suboptimal in many application where the desired measure is set overlap i e inner product between binary vector or set containment minhash ha inherent bias towards smaller set which adversely affect it performance in application where such a penalization is not desirable in this paper we propose asymmetric minwise hashing em mh alsh to provide a solution to this well known problem the new scheme utilizes asymmetric transformation to cancel the bias of traditional minhash towards smaller set making the final collision probability monotonic in the inner product our theoretical comparison show that for the task of retrieving with binary inner product asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithm for general inner product thus we obtain an algorithmic improvement over existing approach in the literature experimental evaluation on four publicly available high dimensional datasets validate our claim the proposed scheme outperforms often significantly other hashing algorithm on the task of near neighbor retrieval with set containment our proposal is simple and easy to implement in practice 
online review on product and service can be very useful for customer but they need to be protected from manipulation so far most study have focused on analyzing online review from a single hosting site how could one leverage information from multiple review hosting site this is the key question in our work in response we develop a systematic methodology to merge compare and evaluate review from multiple hosting site we focus on hotel review and use more than million review from more than million user spanning three prominent travel site our work consists of three thrust a we develop novel feature capable of identifying cross site discrepancy effectively b we conduct arguably the first extensive study of cross site variation using real data and develop a hotel identity matching method with accuracy c we introduce the trueview score a a proof of concept that cross site analysis can better inform the end user our result show that we detect time more suspicious hotel by using multiple site compared to using the three site in isolation and we find that of all hotel appearing in all three site seem to have low trustworthiness score our work is an early effort that explores the advantage and the challenge in using multiple reviewing site towards more informed decision making 
the rise of crowdsourcing brings new type of malpractice in internet advertising one can easily hire web worker through malicious crowdsourcing platform to attack other advertiser such human generated crowd fraud are hard to detect by conventional fraud detection method in this paper we carefully examine the characteristic of the group behavior of crowd fraud and identify three persistent pattern which are moderateness synchronicity and dispersivity then we propose an effective crowd fraud detection method for search engine advertising based on these pattern which consists of a constructing stage a clustering stage and a filtering stage at the constructing stage we remove irrelevant data and reorganize the click log into a surfer advertiser inverted list at the clustering stage we define the sync similarity between surfer click history and transform the coalition detection to a clustering problem solved by a nonparametric algorithm and finally we build a dispersity filter to remove false alarm cluster the nonparametric nature of our method ensures that we can find an unbounded number of coalition with nearly no human interaction we also provide a parallel solution to make the method scalable to web data and conduct extensive experiment the empirical result demonstrate that our method is accurate and scalable 
most of today s mobile device come equipped with both cellular lte and wifi wireless radio making radio bundling simultaneous data transfer over multiple interface both appealing and practical despite recent study documenting the benefit of radio bundling with mptcp many fundamental question remain about potential gain from radio bundling or the relationship between performance and energy consumption in these scenario in this study we seek to answer these question using extensive measurement to empirically characterize both energy and performance for radio bundling approach in doing so we quantify potential gain of bundling using mptcp versus an ideal protocol we study the link between traffic partitioning and bundling performance and use a novel componentized energy model to quantify the energy consumed by cpu and radio during traffic management our result show that mptcp achieves only a fraction of the total performance gain possible and that it energy agnostic design lead to considerable power consumption by the cpu we conclude that not only there is room for improved bundling performance but an energy aware bundling protocol is likely to achieve a much better tradeoff between performance and power consumption 
crowdsourcing via paid microtasks ha been successfully applied in a plethora of domain and task previous effort for making such crowdsourcing more effective have considered aspect a diverse a task and workflow design spam detection quality control and pricing model our work expands upon such effort by examining the potential of adding gamification to microtask interface a a mean of improving both worker engagement and effectiveness we run a series of experiment in image labeling one of the most common use case for microtask crowdsourcing and analyse worker behavior in term of number of image completed quality of annotation compared against a gold standard and response to financial and game specific reward each experiment study these parameter in two setting one based on a state of the art non gamified task on crowdflower and another one using an alternative interface incorporating several game element our finding show that gamification lead to better accuracy and lower cost than conventional approach that use only monetary incentive in addition it seems to make paid microtask work more rewarding and engaging especially when sociality feature are introduced following these initial insight we define a predictive model for estimating the most appropriate incentive for individual worker based on their previous contribution this allows u to build a personalised game experience with gain seen on the volume and quality of work completed 
a large number of extension exist in browser vendor online store for million of user to download and use many of those extension process sensitive information from user input and webpage however it remains a big question whether those extension may accidentally leak such sensitive information out of the browser without protection in this paper we present a framework lvdetector that combine static and dynamic program analysis technique for automatic detection of information leakage vulnerability in legitimate browser extension extension developer can use lvdetector to locate and fix the vulnerability in their code browser vendor can use lvdetector to decide whether the corresponding extension can be hosted in their online store advanced user can also use lvdetector to determine if certain extension are safe to use the design of lvdetector is not bound to specific browser or javascript engine and can adopt other program analysis technique we implemented lvdetector and evaluated it on popular firefox and google chrome extension lvdetector identified previously unknown information leakage vulnerability in extension with a accuracy rate the evaluation result and the feedback to our responsible disclosure demonstrate that lvdetector is useful and effective 
increased popularity of smartphones ha attracted a large number of developer to various smartphone platform a a result app market are also populated with spam apps which reduce the user quality of experience and increase the workload of app market operator apps can be spammy in multiple way including not having a specific functionality unrelated app description or unrelated keywords and publishing similar apps several time and across diverse category market operator maintain anti spam policy and apps are removed through continuous human intervention through a systematic crawl of a popular app market and by identifying a set of removed apps we propose a method to detect spam apps solely using app metadata available at the time of publication we first propose a methodology to manually label a sample of removed apps according to a set of checkpoint heuristic that reveal the reason behind removal this analysis suggests that approximately of the apps being removed are very likely to be spam apps we then map the identified heuristic to several quantifiable feature and show how distinguishing these feature are for spam apps finally we build an adaptive boost classifier for early identification of spam apps using only the metadata of the apps our classifier achieves an accuracy over with precision varying between and recall varying between by applying the classifier on a set of apps present at the app market during our crawl we estimate that at least of them are spam apps 
a well known phenomenon in social network is homophily the tendency of agent to connect with similar agent a derivative of this phenomenon is the emergence of community another phenomenon observed in numerous network is the existence of certain agent that belong simultaneously to multiple community an understanding of these phenomenon constitutes a central research topic of network science in this work we focus on a fundamental theoretical question related to the above phenomenon with various application given an undirected graph g can we infer efficiently the latent vertex feature which explain the observed network structure under the assumption of a generative model that exhibit homophily we propose a probabilistic generative model with the property that the probability of an edge among two vertex is a non decreasing function of the common feature they posse this property is true for many real world network and surprisingly is ignored by many popular overlapping community detection method a it wa shown recently by the empirical work of yang and leskovec our main theoretical contribution is the first provably rapidly mixing markov chain for inferring latent feature on the experimental side we verify the efficiency of our method in term of run time where we observe that it significantly outperforms state of the art method our method is more than time faster than a state of the art machine learning method and typically provides non trivial speedup compared to bigclam furthermore we verify on real data with ground truth available that our method learns efficiently high quality labelings we use our method to learn social circle from twitter ego network and perform multilabel classification 
we are living in a world of big sensor data due to the widespread prevalence of visual sensor e g surveillance camera and social sensor e g twitter feed many event are implicitly captured in real time by such heterogeneous sensor combining these two complementary sensor stream can significantly improve the task of event detection and aid in comprehending evolving situation however the different characteristic of these social and sensor data make such information fusion for event detection a challenging problem to tackle this problem we propose an innovative multi layer tweeting camera framework integrating both physical sensor and social sensor to detect various concept of real world event in this framework visual concept detector are applied on camera video frame and these concept can be construed a camera tweet posted regularly these tweet are represented by a unified probabilistic spatio temporal pst data structure which is then aggregated to a concept based image cmage a the common representation for visualization to facilitate event analysis we define a set of operator and analytic function that can be applied on the pst data by the user to discover occurrence of event and to analyse evolving situation we further leverage on geo located social medium data by mining current topic discussed on twitter to obtain the high level semantic meaning of detected event in image we quantitatively evaluate our framework with a large scale dataset containing image from new york real time traffic cctv camera university foodcourt camera feed and twitter data which demonstrates the feasibility and effectiveness of the proposed framework result of combining camera tweet and social tweet are shown to be promising for detecting real world event 
in this paper we address the problem of estimating the index size needed by web search engine to answer a many query a possible by exploiting the marked difference between query and click frequency we provide a possible formal definition for the notion of essential web page a those that cover a large fraction of distinct query i e we look at the problem a a version of maxcover although in general maxcover is approximable to within a factor of e from the optimum we provide a condition under which the greedy algorithm doe find the actual best cover or remains at a known bounded factor from it the extra check for optimality or for bounding the ratio from the optimum come at a negligible algorithmic cost moreover in most practical instance of this problem the algorithm is able to provide solution that are provably optimal or close to optimal we relate this observed phenomenon to some property of the query click graph our experimental result confirm that a small number of web page can respond to a large fraction of the query e g of the page answer of the query our approach can be used in several related search application and ha in fact an even more general appeal a a first example our preliminary experimental study confirms that our algorithm ha extremely good performance on other social network based maxcover instance 
understanding the correlation between two different score for the same set of item is a common problem in graph analysis and information retrieval the most commonly used statistic that quantifies this correlation is kendall s tau however the standard definition fails to capture that discordance between item with high rank are more important than those between item with low rank recently a new measure of correlation based on average precision ha been proposed to solve this problem but like many alternative proposal in the literature it assumes that there are no tie in the score this is a major deficiency in a number of context and in particular when comparing centrality score on large graph a the obvious baseline indegree ha a very large number of tie in social network and web graph we propose to extend kendall s definition in a natural way to take into account weight in the presence of tie we prove a number of interesting mathematical property of our generalization and describe an o n log n algorithm for it computation we also validate the usefulness of our weighted measure of correlation using experimental data on social network and web graph 
we study the following problem given the name of an ad hoc concept a well a a few seed entity belonging to the concept output all entity belonging to it since producing the exact set of entity is hard we focus on returning a ranked list of entity previous approach either use seed entity a the only input or inherently require negative example they suffer from input ambiguity and semantic drift or are not viable option for ad hoc tail concept in this paper we propose to leverage the million of table on the web for this problem the core technical challenge is to identify the exclusive table for a concept to prevent semantic drift existing holistic ranking technique like personalized pagerank are inadequate for this purpose we develop novel probabilistic ranking method that can model a new type of table entity relationship experiment with real life concept show that our proposed solution is significantly more effective than applying state of the art set expansion or holistic ranking technique 
malvertising is a malicious activity that leverage advertising to distribute various form of malware because advertising is the key revenue generator for numerous internet company large ad network such a google yahoo and microsoft invest a lot of effort to mitigate malicious ad from their ad network this drive adversary to look for alternative method to deploy malvertising in this paper we show that browser extension that use ad a their monetization strategy often facilitate the deployment of malvertising moreover while some extension simply serve ad from ad network that support malvertising other extension maliciously alter the content of visited webpage to force user into installing malware to measure the extent of these behavior we developed expector a system that automatically inspects and identifies browser extension that inject ad and then classifies these ad a malicious or benign based on their landing page using expector we automatically inspected over chrome browser extension we found extension that inject ad and detected extension that participate in malvertising using different ad network and with a total user base of 
in this paper we propose and evaluate a scheme to produce canonical label for blank node in rdf graph these label can be used a the basis for a skolemisation scheme that get rid of the blank node in an rdf graph by mapping them to globally canonical iris assuming no hash collision the scheme guarantee that two skolemised graph will be equal if and only if the two input graph are isomorphic although the proposed scheme is exponential in the worst case we claim that such case are unlikely to be encountered in practice to support these claim we present the result of applying our skolemisation scheme over a diverse collection of million real world rdf graph btc we also provide result for some nasty synthetic case 
latent dirichlet allocation lda is a widely used probabilistic topic modeling tool for content analysis such a web mining to handle web scale content analysis on just a single pc we propose multi core parallel expectation maximization pem algorithm to infer and estimate lda parameter in shared memory system by avoiding memory access conflict reducing the locking time among multiple thread and residual based dynamic scheduling we show that pem algorithm are more scalable and accurate than the current state of the art parallel lda algorithm on a commodity pc this parallel lda toolbox is made publicly available a open source software at mloss org 
web page consist of not only actual content but also other element such a branding banner navigational element advertisement copyright etc this noisy content is typically not related to the main subject of the webpage identifying the part of actual content or clipping web page ha many application such a high quality web printing e reading on mobile device and data mining although there are many existing method attempting to address this task most of them can either work only on certain type of web page e g article page or ha to develop different model for different website we formulate the actual content identifying problem a a dom tree node selection problem we develop multiple feature by utilizing the dom tree node property to train a machine learning model then candidate node are selected based on the learning model based on the observation that the actual content is usually located in a spatially continuous block we develop a grouping technology to further filter out noisy data and pick missing data for the candidate node we conduct extensive experiment on a real dataset and demonstrate our solution ha high quality output and outperforms several baseline method 
a b testing also known a bucket testing split testing or controlled experiment is a standard way to evaluate user engagement or satisfaction from a new service feature or product it is widely used in online website including social network site such a facebook linkedin and twitter to make data driven decision the goal of a b testing is to estimate the treatment effect of a new change which becomes intricate when user are interacting i e the treatment effect of a user may spill over to other user via underlying social connection when conducting these online controlled experiment it is a common practice to make the stable unit treatment value assumption sutva that each individual s response is affected by their own treatment only though this assumption simplifies the estimation of treatment effect it doe not hold when network interference is present and may even lead to wrong conclusion in this paper we study the problem of network a b testing in real network which have substantially different characteristic from the simulated random network studied in previous work we first examine the existence of network effect in a recent online experiment conducted at linkedin secondly we propose an efficient and effective estimator for average treatment effect ate considering the interference between user in real online experiment finally we apply our method in both simulation and a real world online experiment the simulation result show that our estimator achieves better performance with respect to both bias and variance reduction the real world online experiment not only demonstrates that large scale network a b test is feasible but also further validates many of our observation in the simulation study 
in online market a store s reputation is closely tied to it profitability seller desire to quickly achieve high reputation ha fueled a profitable underground business which operates a a specialized crowdsourcing marketplace and accumulates wealth by allowing online seller to harness human laborer to conduct fake transaction for improving their store reputation we term such an underground market a seller reputation escalation sre market in this paper we investigate the impact of the sre service on reputation escalation by performing in depth measurement of the prevalence of the sre service the business model and market size of sre market and the characteristic of seller and offered laborer to this end we have infiltrated five sre market and studied their operation using daily data collection over a continuous period of two month we identified more than online seller posting at least fake purchase task on the five sre market these transaction earned at least in revenue for the five sre market and the total value of merchandise involved exceeded our study demonstrates that online seller using sre service can increase their store reputation at least time faster than legitimate one while only of them were detected and penalized even worse we found a newly launched service that can within a single day boost a seller s reputation by such a degree that would require a legitimate seller at least a year to accomplish finally armed with our analysis of the operational characteristic of the underground economy we offer some insight into potential mitigation strategy 
when user interact with the web today they leave sequential digital trail on a massive scale example of such human trail include web navigation sequence of online restaurant review or online music play list understanding the factor that drive the production of these trail can be useful for e g improving underlying network structure predicting user click or enhancing recommendation in this work we present a general approach called hyptrails for comparing a set of hypothesis about human trail on the web where hypothesis represent belief about transition between state our approach utilizes markov chain model with bayesian inference the main idea is to incorporate hypothesis a informative dirichlet prior and to leverage the sensitivity of bayes factor on the prior for comparing hypothesis with each other for eliciting dirichlet prior from hypothesis we present an adaption of the so called trial roulette method we demonstrate the general mechanic and applicability of hyptrails by performing experiment with i synthetic trail for which we control the mechanism that have produced them and ii empirical trail stemming from different domain including website navigation business review and online music played our work expands the repertoire of method available for studying human trail on the web 
the availability of an increasing amount of user generated data is transformative to our society we enjoy the benefit of analyzing big data for public interest such a disease outbreak detection and traffic control a well a for commercial interest such a smart grid and product recommendation however the large collection of user generated data contains unique pattern and can be used to re identify individual which ha been exemplified by the aol search log release incident in this paper we propose a practical framework for data analytics while providing differential privacy guarantee to individual data contributor our framework generates differentially private aggregate which can be used to perform data mining and recommendation task to alleviate the high perturbation error introduced by the differential privacy mechanism we present two method with different sampling technique to draw a subset of individual data for analysis empirical study with real world data set show that our solution enable accurate data analytics on a small fraction of the input data reducing user privacy risk and data storage requirement without compromising the analysis result 
password continue to dominate the authentication landscape in spite of numerous proposal to replace them even though usability is a key factor in replacing password very few alternative have been subjected to formal usability study and even fewer have been analyzed using a standard metric we report the result of four within subject usability study for seven web authentication system these system span federated smartphone paper token and email based approach our result indicate that participant prefer single sign on system we report several insightful finding based on participant qualitative response transparency increase usability but also lead to confusion and a lack of trust participant prefer single sign on but wish to augment it with site specific low entropy password and participant are intrigued by biometrics and phone based authentication we utilize the system usability scale sus a a standard metric for empirical analysis and find that it produce reliable replicable result sus prof to be an accurate measure of baseline usability we recommend that new authentication system be formally evaluated for usability using sus and should meet a minimum acceptable sus score before receiving serious consideration 
sparql entailment regime are strongly influenced by the big body of work on ontology based query answering notably in the area of description logic dl however the semantics of query answering under sparql entailment regime is defined in a more naive and much le expressive way than the certain answer semantics usually adopted in dl the goal of this work is to introduce an intuitive certain answer semantics also for sparql and to show the feasibility of this approach for owl ql entailment we present algorithm for the evaluation of an interesting fragment of sparql the so called well designed sparql moreover we show that the complexity of the most fundamental query analysis task such a query containment and equivalence testing is not negatively affected by the presence of owl ql entailment under the proposed semantics 
many individual on social networking site provide trait about themselves such a interest or demographic social networking site can use this information to provide better content to match their user interest such a recommending scheduled event or various relevant product these task require accurate probability estimate to determine the correct answer to return relational machine learning rml is an excellent framework for these problem a it jointly model the user label given their attribute and the relational structure further semi supervised learning method could enable rml method to exploit the large amount of unlabeled data in network however existing rml approach have limitation that prevent their application in large scale domain first semi supervised method for rml do not fully utilize all the unlabeled instance in the network second the collective inference procedure necessary to jointly infer the missing label are generally viewed a too expensive to apply in large scale domain in this work we address each of these limitation we analyze the effect of full semi supervised rml and find that collective inference method can introduce considerable bias into prediction we correct this by implementing a maximum entropy constraint on the inference step forcing the prediction to have the same distribution a the observed label next we outline a massively scalable variational inference algorithm for large scale relational network domain we extend this inference algorithm to incorporate the maximum entropy constraint proving that it only requires a constant amount of overhead while remaining massively parallel we demonstrate our method s improvement over a variety of baseline on seven real world datasets including large scale network with over five million edge 
social medium ha led to the democratisation of opinion sharing a wealth of information about public opinion current event and author insight into specific topic can be gained by understanding the text written by user however there is a wide variation in the language used by different author in different context on the web this diversity in language make interpretation an extremely challenging task crowdsourcing present an opportunity to interpret the sentiment or topic of free text however the subjectivity and bias of human interpreter raise challenge in inferring the semantics expressed by the text to overcome this problem we present a novel bayesian approach to language understanding that relies on aggregated crowdsourced judgement our model encodes the relationship between label and text feature in document such a tweet web article and blog post accounting for the varying reliability of human labellers it allows inference of annotation that scale to arbitrarily large pool of document our evaluation using two challenging crowdsourcing datasets show that by efficiently exploiting language model learnt from aggregated crowdsourced label we can provide up to improved classification when only a small portion le than of document ha been labelled compared to the six state of the art method we reduce by up to the number of crowd response required to achieve comparable accuracy our method wa a joint winner of the crowdflower crowdscale shared task challenge at the conference on human computation and crowdsourcing hcomp 
large graph arise in a number of context and understanding their structure and extracting information from them is an important research area early algorithm on mining community have focused on the global structure and often run in time functional to the size of the entire graph nowadays a we often explore network with billion of vertex and find community of size hundred it is crucial to shift our attention from macroscopic structure to microscopic structure when dealing with large network a growing body of work ha been adopting local expansion method in order to identify the community from a few exemplary seed member very few approach can systematically demonstrate both high efficiency and effectiveness that significantly stand out amongst the divergent approach in finding community in this paper we propose a novel approach for finding overlapping community called lemon local expansion via minimum one norm different from pagerank like diffusion method lemon find the community by seeking a sparse vector in the span of the local spectrum such that the seed are in it support we show that lemon can achieve the highest detection accuracy among state of the art proposal the running time depends on the size of the community rather than that of the entire graph the algorithm is easy to implement and is highly parallelizable moreover given that network are not all similar in nature a comprehensive analysis on how the local expansion approach is suited for uncovering community in different network is still lacking we thoroughly evaluate our approach using both synthetic and real world datasets across different domain and analyze the empirical variation when applying our method to inherently different network in practice in addition the heuristic on how the quality and quantity of the seed set would affect the performance are provided 
sociolinguistic study investigate the relation between language and extra linguistic variable this requires both representative text data and the associated socio economic meta data of the subject traditionally sociolinguistic study use small sample of hand curated data and meta data this can lead to exaggerated or false conclusion using social medium data offer a large scale source of language data but usually lack reliable socio economic meta data our research aim to remedy both problem by exploring a large new data source international review website with user profile they provide more text data than manually collected study and more meta data than most available social medium text we describe the data and present various pilot study illustrating the usefulness of this resource for sociolinguistic study our approach can help generate new research hypothesis based on data driven finding across several country and language 
entity extraction is a process of identifying meaningful entity from text document in enterprise extracting entity improves enterprise efficiency by facilitating numerous application including search recommendation etc however the problem is particularly challenging on enterprise domain due to several reason first the lack of redundancy of enterprise entity make previous web based system like nell and openie not effective since using only high precision low recall pattern like those system would miss the majority of sparse enterprise entity while using more low precision pattern in sparse setting also introduces noise drastically second semantic drift is common in enterprise blue refers to window blue such that public signal from the web cannot be directly applied on entity moreover many internal entity never appear on the web sparse internal signal are the only source for discovering them to address these challenge we propose an end to end framework for extracting entity in enterprise taking the input of enterprise corpus and limited seed to generate a high quality entity collection a output we introduce the novel concept of semantic pattern graph to leverage public signal to understand the underlying semantics of lexical pattern reinforce pattern evaluation using mined semantics and yield more accurate and complete entity experiment on microsoft enterprise data show the effectiveness of our approach 
people diagnosed with a serious illness often turn to the web for their rising information need especially when decision are required we analyze the search and browsing behavior of searcher who show a surge of interest in prostate cancer prostate cancer is the most common serious cancer in men and is a leading cause of cancer related death diagnosis of prostate cancer typically involve reflection and decision making about treatment based on assessment of preference and outcome we annotated timeline of treatment related query from nearly searcher with tag indicating different phase of treatment including decision making preparation and recovery using this corpus we present a variety of analysis toward the goal of understanding search and decision making about treatment we characterize search query and the content of accessed page for different treatment phase model search behavior during the decision making phase and create an aggregate alignment of treatment timeline illustrated with a variety of visualization the experiment provide insight about how people who are engaged in intensive search about prostate cancer over an extended period of time pursue and access information from the web 
decomposing a graph into a hierarchical structure via k core analysis is a standard operation in any modern graph mining toolkit k core decomposition is a simple and efficient method that allows to analyze a graph beyond it mere degree distribution more specifically it is used to identify area in the graph of increasing centrality and connectedness and it allows to reveal the structural organization of the graph despite the fact that k core analysis relies on vertex degree k core do not satisfy a certain rather natural density property simply put the most central k core is not necessarily the densest subgraph this inconsistency between k core and graph density provides the basis of our study we start by defining what it mean for a subgraph to be locally dense and we show that our definition entail a nested chain decomposition of the graph similar to the one given by k core but in this case the component are arranged in order of increasing density we show that such a locally dense decomposition for a graph g v e can be computed in polynomial time the running time of the exact decomposition algorithm is o v e but is significantly faster in practice in addition we develop a linear time algorithm that provides a factor approximation to the optimal locally dense decomposition furthermore we show that the k core decomposition is also a factor approximation however a demonstrated by our experimental evaluation in practice k core have different structure than locally dense subgraphs and a predicted by the theory k core are not always well aligned with graph density 
personal mobile device offer a growing variety of personalized service that enrich considerably the user experience this is made possible by increased access to personal information which to a large extent is extracted from user email message and archive there are however two main issue first currently these service can be offered only by large web service company that can also deploy email service second keeping a large amount of structured personal information on the cloud raise privacy concern to address these problem we propose ln annote a new method to extract personal information from the email that is locally available on mobile device without remote access to the cloud ln annote enables third party service provider to build a question answering system on top of the local personal information without having to own the user data in addition ln annote mitigates the privacy concern by keeping the structured personal information directly on the personal device our method is based on a named entity recognizer trained in two separate step first using a common dataset on the cloud and then using a personal dataset in the mobile device at hand our contribution include also the optimization of the implementation of ln annote in particular we implemented an opencl version of the custom training algorithm to leverage the graphic processing unit gpu available on the mobile device we present an extensive set of experiment result beside proving the feasibility of our approach they demonstrate it efficiency in term of the named entity extraction performance a well a the execution speed and the energy consumption spent in mobile device 
research ethic is an important and timely topic in academia federally regulated institutional review board irbs protect participant of human subject research and offer researcher a mechanism to ass the ethical implication of their work industry research lab are not subject to the same requirement and may lack process for research ethic review we describe the creation of a new ethic framework and a research ethic submission system res within microsoft research msr this res is customized to the need of web researcher we describe our iterative development process including our assessment of the current state of web research developing a framework of method based on a survey of research paper build and evaluate our system with user to identify the benefit and pitfall of full deployment evaluate how our system match with existing federal regulation and suggest next step for supporting ethical web research 
online social network nowadays enjoy their worldwide prosperity a they have revolutionized the way for people to discover to share and to distribute information with million of registered user and the proliferation of user generated content the social network become giant likely eligible to carry on any research task however the giant do have their achilles heel extreme data sparsity compared with the massive data over the whole collection individual posting document e g a microblog le than character seem to be too sparse to make a difference under various research scenario while actually they are different in this paper we propose to tackle the achilles heel of social network by smoothing the language model via influence propagation we formulate a socialized factor graph model which utilizes both the textual correlation between document pair and the socialized augmentation network behind the document such a user relationship and social interaction these factor are modeled a attribute and dependency among document and their corresponding user an efficient algorithm is designed to learn the proposed factor graph model finally we propagate term count to smooth document based on the estimated influence experimental result on twitter and weibo datasets validate the effectiveness of the proposed model by leveraging the smoothed language model with social factor our approach obtains significant improvement over several alternative method on both intrinsic and extrinsic evaluation measured in term of perplexity ndcg and map result 
this paper first reveals the relationship between inverse document frequency idf a global term weighting scheme and information distance a universal metric defined by kolmogorov complexity we concretely give a theoretical explanation that the idf of a term is equal to the distance between the term and the empty string in the space of information distance in which the kolmogorov complexity is approximated using web document and the shannon fano coding based on our finding we propose n gram idf a theoretical extension of idf for handling word and phrase of any length by comparing weight among n gram of any n n gram idf enables u to determine dominant n gram among overlapping one and extract key term of any length from text without using any nlp technique to efficiently compute the weight for all possible n gram we adopt two string processing technique i e maximal substring extraction using enhanced suffix array and document listing using wavelet tree we conducted experiment on key term extraction and web search query segmentation and found that n gram idf wa competitive with state of the art method that were designed for each application using additional resource and effort the result exemplified the potential of n gram idf 
community based question answering platform can be rich source of information on a variety of specialized topic from finance to cooking the usefulness of such platform depends heavily on user contribution question and answer but also on respecting the community rule a a crowd sourced service such platform rely on their user for monitoring and flagging content that violates community rule common wisdom is to eliminate the user who receive many flag our analysis of a year of trace from a mature q a site show that the number of flag doe not tell the full story on one hand user with many flag may still contribute positively to the community on the other hand user who never get flagged are found to violate community rule and get their account suspended this analysis however also show that abusive user are betrayed by their network property we find strong evidence of homophilous behavior and use this finding to detect abusive user who go under the community radar based on our empirical observation we build a classifier that is able to detect abusive user with an accuracy a high a 
the rapid growth of location based service provide the potential to understand people s mobility pattern at an unprecedented level which can also enable food service industry to accurately predict consumer s dining behavior in this paper by leveraging user historical dining pattern socio demographic characteristic and restaurant attribute we aim at generating the top k restaurant for a user s next dining compared to previous study in location prediction which mainly focus on regular mobility pattern we present a novelty seeking based dining recommender system termed ndrs in consideration of both exploration and exploitation first we apply a conditional random field crf with additional constraint to infer user novelty seeking status by considering both spatial temporal historical feature and user socio demographic characteristic on the one hand when a user is predicted to be novelty seeking by incorporating the influence of restaurant contextual factor such a price and service quality we propose a context aware collaborative filtering method to recommend restaurant she ha never visited before on the other hand when a user is predicted to be not novelty seeking we then present a hidden markov model hmm considering the temporal regularity to recommend the previously visited restaurant to evaluate the performance of each component a well a the whole system we conduct extensive experiment with a large dataset we have collected covering the concerned dining related check in user demographic and restaurant attribute the result reveal that our system is effective for dining recommendation 
we present gerbil an evaluation framework for semantic entity annotation the rationale behind our framework is to provide developer end user and researcher with easy to use interface that allow for the agile fine grained and uniform evaluation of annotation tool on multiple datasets by these mean we aim to ensure that both tool developer and end user can derive meaningful insight pertaining to the extension integration and use of annotation application in particular gerbil provides comparable result to tool developer so a to allow them to easily discover the strength and weakness of their implementation with respect to the state of the art with the permanent experiment uris provided by our framework we ensure the reproducibility and archiving of evaluation result moreover the framework generates data in machine processable format allowing for the efficient querying and post processing of evaluation result finally the tool diagnostics provided by gerbil allows deriving insight pertaining to the area in which tool should be further refined thus allowing developer to create an informed agenda for extension and end user to detect the right tool for their purpose gerbil aim to become a focal point for the state of the art driving the research agenda of the community by presenting comparable objective evaluation result 
in crowdsourcing system the interest of contributing participant and system stakeholder are often not fully aligned participant seek to learn be entertained and perform easy task which offer them instant gratification system stakeholder want user to complete more difficult task which bring higher value to the crowdsourced application we directly address this problem by presenting technique that optimize the crowdsourcing process by jointly maximizing the user longevity in the system and the true value that the system derives from user participation we first present model that predict the survival probability of a user at any given moment that is the probability that a user will proceed to the next task offered by the system we then leverage this survival model to dynamically decide what task to assign and what motivating goal to present to the user this allows u to jointly optimize for the short term getting difficult task done and for the long term keeping user engaged for longer period of time we show that dynamically assigning task significantly increase the value of a crowdsourcing system in an extensive empirical evaluation we observed that our task allocation strategy increase the amount of information collected by up to we also explore the utility of motivating user with goal we demonstrate that setting specific static goal can be highly detrimental to the long term user participation a the completion of a goal e g earning a badge is also a common drop off point for many user we show that setting the goal dynamically in conjunction with judicious allocation of task increase the amount of information collected by the crowdsourcing system by up to compared to the existing baseline that use fixed objective 
everyday million of user save content item for future use on site like pinterest by pinning them onto carefully categorised personal pinboards thereby creating personal taxonomy of the web this paper seek to understand pinterest a a distributed human computation that categorises image from around the web we show that despite being categorised onto personal pinboards by individual action there is a generally a global agreement in implicitly assigning image into a coarse grained global taxonomy of category and furthermore user tend to specialise in a handful of category by exploiting these characteristic and augmenting with image related feature drawn from a state of the art deep convolutional neural network we develop a cascade of predictor that together automate a large fraction of pinterest action our end to end model is able to both predict whether a user will repin an image onto her own pinboard and also which pinboard she might choose with an accuracy of accuracy of 
